PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Data-driven adaptive optimal control for stochastic systems with unmeasurable state	NEUROCOMPUTING										Adaptive dynamic programming; Stocastic optimal control; Estimation; Linear system observers	H-INFINITY CONTROL; OUTPUT REGULATION; LINEAR-SYSTEMS; ITERATION	This paper proposes a computational data-driven adaptive optimal control strategy for a class of linear stochastic systems with unmeasurable state. First, a data-driven optimal observer is designed to obtain the optimal state estimation policy. On this basis, an off-policy data-driven ADP algorithm is further proposed, yielding the stochastic optimal control in the absence of system model. An application example of the learning mechanism of central nervous system in arm movement control is given to illustrate the effectiveness and practicality of the strategy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						1	10		10.1016/j.neucom.2019.12.001													
J								DeepPIPE: A distribution-free uncertainty quantification approach for time series forecasting	NEUROCOMPUTING										Deep learning; Uncertainty quantification; Time series; Forecasting		Time series forecasting is a challenging task as the underlying data generating process is dynamic, non-linear, and uncertain. Deep learning such as LSTM and auto-encoder can learn representations automatically and has attracted considerable attention in time series forecasting. However, current approaches mainly focus on point estimation, which leads to the inability to quantify uncertainty. Meantime, existing deep uncertainty quantification methods suffer from various limitations in practice. To this end, this paper presents a novel end-to-end framework called deep prediction interval and point estimation (DeepPIPE) that simultaneously performs multi-step point estimation and uncertainty quantification for time series forecasting. The merits of this approach are threefold: first, it requires no prior assumption on the distribution of data noise; second, it utilizes a novel hybrid loss function that improves the accuracy and stability of forecasting; third, it is only optimized by back-propagation algorithm, which is time friendly and easy to be implemented. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance on three real-world datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						11	19		10.1016/j.neucom.2020.01.111													
J								Ensemble learning based predictive framework for virtual machine resource request prediction	NEUROCOMPUTING										Cloud computing; Resource demand prediction; Extreme learning machine; Ensemble learning; Neural networks; Blackhole	WORKLOAD PREDICTION; NEURAL-NETWORK; CLOUD; MANAGEMENT; MODEL; ALLOCATION	The cloud service providers require a large number of computing resources to provide services on-demand that consume the electricity at large and leave high carbon footprints which must be minimized. A cloud system must optimally use its resources to achieve a low operational cost without degrading the quality of services. In this context, an ensemble learning based workload forecasting method is presented that uses extreme learning machines and their corresponding forecasts are weighted by a voting engine. A metaheuristic algorithm inspired by blackhole theory is used to select the optimal weights. The accuracy of the approach is tested on CPU and memory demand requests of Google cluster trace. The method is also compared with recent existing work in the literature on CPU utilization of Google cluster and PlanetLab traces. The results validate the superiority of the approach over existing methods with an improvement up to 99.20% in root mean squared error. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						20	30		10.1016/j.neucom.2020.02.014													
J								Multi-density map fusion network for crowd counting	NEUROCOMPUTING										Multi-density map; Multi-branch; Relative weights; Crowd counting		In crowed scene, its hard to get the exact number of people due to the distorted perspectives, complex backgrounds, and scale changes. People in different locations have different sizes and dimensions in an image. To deal with this problem, we propose a new multi-density map fusion method to learn the mapping from the input image to the density map. Different form previous methods, our method mainly focuses on fusing different density maps information instead of fusing multi-scale feature of the same images. The major contributions are three paralleled branches and dynamic weighting strategy. First, our network employs the first ten layers of VGG16, and the network is combined with three paralleled branches. Each branch of our network extracts image information at different scales and each branch outputs a density map. Second, to ensure the quality of the final density map, we employ learnable relative weights to fuse the three density maps. Our method has been proved more robust than many state-of-art methods. Lots of experiments have been done in the ShanghaiTech, WorldExpo10, UCSD and UCF_CC_50 dataset to show the effectiveness of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						31	38		10.1016/j.neucom.2020.02.010													
J								CBFNet: Constraint balance factor for semantic segmentation	NEUROCOMPUTING										Deep learning; Image segmentation; Semantic segmentation; Convolutional neural network		The deep neural network model based on spatial pyramid pooling for multi-scale context information has been widely adopted in image semantic segmentation. However, this structure reduces the feature resolution during the inference process, which should be recovered by bilinear interpolation. We find that the performance of boundary location heavily depends on the interpolation operation. Assuming that the foreground object has a boundary with uneven curvature changes, it will be difficult to preserve details in the lower resolution feature map. In this case, if the model achieves near-optimal fitting, the prediction of the boundary point needs to satisfy strict constraints. But this condition is very difficult to meet for the model. For this purpose, we propose a constraint balance factor (CBF) relaxation method to improve the fitting ability of the model. The constraint balance factor is learned end-to-end by a sub-network. This sub-network can be easily embedded into a semantic segmentation network that requires interpolation without too many extra parameter increments. The experimental results illustrate that the proposed CBFNet performs well on PASCAL VOC 2012 and Cityscapes datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						39	47		10.1016/j.neucom.2020.02.039													
J								Multi-path x-D recurrent neural networks for collaborative image classification	NEUROCOMPUTING										RNN; Longitudinal; Unordered image; Category-irrelevant attributes	FALSE-POSITIVE REDUCTION	With the rapid development of image acquisition and storage, multiple images per class are commonly available for computer vision tasks (e.g., face recognition, object detection, medical imaging, etc.). Recently, the recurrent neural network (RNN) has been widely integrated with convolutional neural networks (CNN) to perform image classification on ordered (sequential) data. In this paper, by permutating multiple images as multiple dummy orders, we generalize the ordered "RNN+CNN" design (longitudinal) to a novel unordered fashion, called Multi-path x-D Recurrent Neural Network (MxDRNN) for image classification. To the best of our knowledge, few (if any) existing studies have deployed the RNN framework to unordered intra-class images to leverage classification performance. Specifically, multiple learning paths are introduced in the MxDRNN to extract discriminative features by permutating input dummy orders. Eight datasets from five different fields (MNIST, 3D-MNIST, CIFAR, VGGFace2, and lung screening computed tomography) are included to evaluate the performance of our method. The proposed MxDRNN improves the baseline performance by a large margin across the different application fields (e.g., accuracy from 46.40% to 76.54% in VGGFace2 test pose set, AUC from 0.7418 to 0.8162 in NLST lung dataset). Additionally, empirical experiments show the MxDRNN is more robust to category-irrelevant attributes (e.g., expression, pose in face images), which may introduce difficulties for image classification and algorithm generalizability. The code is publicly available. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						48	59		10.1016/j.neucom.2020.02.033													
J								Adaptive neural tracking control for non-affine nonlinear systems with finite-time output constraint	NEUROCOMPUTING										Adaptive neural control; Dynamic surface control; Non-affine systems; Output constraint; Backstepping	DYNAMIC SURFACE CONTROL; BARRIER LYAPUNOV FUNCTIONS; VARYING DELAY SYSTEMS; PRESCRIBED PERFORMANCE; FEEDBACK-SYSTEMS; SERVO MECHANISMS; NETWORK CONTROL; MOTION CONTROL; DEAD-ZONE; STATE	In this paper, an adaptive neural tracking control approach is presented for a class of non-affine nonlinear systems with finite-time output constraint. The non-affine form of the original system is converted to the affine form by means of the mean value theorem. To constraint the output tracking error within a predefined boundary in finite time, a modified performance function, i.e., finite-time performance function, is introduced. During the process of controller design, the dynamic surface control is employed to handle the 'explosion of complexity' problem occurred in the conventional backstepping method. According to the approximation of radial basis functions neural networks, an adaptive neural control scheme is developed which guarantees that the output tracking error is preserved within a specified prescribed performance and all the signals in closed-loop system are bounded by appropriately choosing the design parameters. The stability of the closed-loop system is proved by Lyapunov stability analysis and the simulation results are proposed to demonstrate the effectiveness of the developed control approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						60	69		10.1016/j.neucom.2020.02.027													
J								A more general incremental inter -agent learning adaptive control for multiple identical processes in mass production	NEUROCOMPUTING										Incremental inter-agent learning; Adaptive control; Mass production	COOPERATIVE OUTPUT REGULATION; LINEAR MULTIAGENT SYSTEMS; BARRIER LYAPUNOV FUNCTIONS; FEEDBACK CONTROL; COVERAGE CONTROL	Mass production has been a trend for modern manufacturing. Information from peer processes may be used to improve the individual process control performance. The idea of 'incremental step mimicking' has been presented in our previous work [1] which resulted in 'incremental inter-agent learning' (IIAL) adaptive control. However, that work is based on a primitive adaptive control in which the control input is directly calculated from the online-identified model. This paper explores the inter-agent learning adaptive control based on a more complicated adaptive control, and proposes a more general Full-Scale IIAL (FS-IIAL) of which the previous IIAL [1] can be viewed as a special case. With the ensured robust stability, the proposed work is applied on the case when LIP formulae is a single layer RBF neural network, and simulation result validates the superior control performance of each process over the original adaptive control and the previous IIAL. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						70	93		10.1016/j.neucom.2020.02.021													
J								A dual-domain deep lattice network for rapid MRI reconstruction	NEUROCOMPUTING										Compressed sensing; Magnetic resonance imaging; Dual domain; Deep neural network	IMAGE-RECONSTRUCTION; SENSE	Compressed sensing is utilized with the aims of reconstructing an MRI using a fraction of measurements to accelerate magnetic resonance imaging called compressed sensing magnetic resonance imaging (CS-MRI). Conventional optimization-based CS-MRI methods use random under-sampling patterns and model the MRI data in the image domain as the classic CS-MRI paradigm. Instead, we design a uniform under-sampling strategy and explore the potential of modeling the MRI data directly in the measured Fourier domain. We propose a dual-domain deep lattice network (DD-DLN) for CS-MRI with variable density uniform under-sampling. We train the networks to learn the mapping between both image and frequency domains. We observe the dual networks have complementary advantages, which motivates their combination via a lattice structure. Experiments show that the proposed DD-DLN model provides promising performance in CS-MRI under the designed variable density uniform under-sampling. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						94	107		10.1016/j.neucom.2020.01.063													
J								Matrix-valued twin-multistate Hopfield neural networks	NEUROCOMPUTING										Complex-valued neural networks; Hopfield neural networks; Twin-multistate activation function	ASSOCIATIVE MEMORY; DYNAMICS	A complex-valued Hopfield neural network (CHNN) has been widely used for the storage of image data. The CHNN has been extended using hypercomplex numbers. A couple of hypercomplex-valued Hopfield neural networks employ a twin-multistate activation function to reduce the numbers of weight parameters. In this work, we propose a matrix-valued twin-multistate Hopfield neural network (MTMHNN), whose neuron states and weights are 2 x 2 matrices. Computer simulations show that the MTMHNN has better noise tolerance than the hypercomplex-valued twin-multistate Hopfield neural networks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						108	113		10.1016/j.neucom.2020.02.056													
J								Feature reduction based on semantic similarity for graph classification	NEUROCOMPUTING										Graph classification; Feature reduction; Neural language model; Semantic similarity	ALIGNMENT	Classification and recognition of graph data are crucial problems in many fields, such as bioinformatics, chemoinformatics and data mining. In graph kernel-based classification methods, the similarity among substructures is not fully considered; in addition, poorly discriminative substructures will affect the graph classification accuracy. To improve the graph classification accuracy, we propose a feature reduction algorithm based on semantic similarity for graph classification in this paper. In the algorithm, we first learn vector representations of subtree patterns using neural language models and then merge semantically similar subtree patterns into a new feature. We then provide a new feature discrimination score to select highly discriminative features. Comprehensive experiments on real datasets demonstrate that the proposed algorithm achieves a significant improvement in classification accuracy over compared graph classification methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						114	126		10.1016/j.neucom.2020.02.047													
J								Counting crowds with varying densities via adaptive scenario discovery framework	NEUROCOMPUTING										Crowd counting; Adaptive scenario discovery; Convolutional neural network		The task of crowd counting is to estimate the number of pedestrian in crowd images. Due to camera perspective and physical barriers among dense crowds, how to construct a robust counting model for varying densities and various scenarios has become a challenging problem. In this paper, we propose an adaptive scenario discovery framework for counting crowds with varying densities. The framework is structured with two parallel pathways that are trained to represent different crowd densities and present in the proper geometric configuration using different sizes of the receptive field. A third adaption branch is designed to adaptively recalibrate the pathway-wise responses by discovering and modeling the dynamic scenarios implicitly. We conduct experiments using the adaptive scenario discovery framework on five challenging crowd counting datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						127	138		10.1016/j.neucom.2020.02.045													
J								Efficient continual learning in neural networks with embedding regularization	NEUROCOMPUTING										Continual learning; Catastrophic forgetting; Embedding; Regularization; Trainable activation functions		Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						139	148		10.1016/j.neucom.2020.01.093													
J								Consensus of multi-agent systems with intermittent communications via sampling time unit approach	NEUROCOMPUTING										Consensus; Sampled-data control; Multi-agent systems; Intermittent communications; Sampling time unit	DELAYED NONLINEAR DYNAMICS; SWITCHED SYSTEMS; STABILIZATION; STABILITY	This paper investigates the consensus problem of multi-agent systems with intermittent communications under sampled-data control. A novel approach called sampling time unit (STU) approach is proposed to study such system. In such approach, the work time is described by finite number of sampling time units and the rest time is describe by several average time units. The lengths of work time and rest time depend on the convergence or divergence property of each time unit. Based on the STU approach, the stabilization property at transition instants can be derived, which can compensate the divergence during rest time to some extents. The designed control scheme can tolerate a relatively large rest time. Based on the STU approach, a corresponding sampling-dependent time-varying Lyapunov function (SDTVLF) is utilized to describe the system state and derive the computable conditions for the overall consensus with maximum admissible rest time. Finally, a numerical example with two cases is provided to illustrate the feasibility of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						149	159		10.1016/j.neucom.2020.02.055													
J								Circle formation control of second-order multi-agent systems with bounded measurement errors	NEUROCOMPUTING										Circle formation control; Multi-agent systems; Distributed control; Bounded measurement errors	COLLECTIVE CIRCULAR MOTION; ANONYMOUS MOBILE AGENTS; SUFFICIENT CONDITIONS; COLLISION-AVOIDANCE; CONNECTIVITY; VEHICLES; TARGET	In this paper, the circle formation control problem of second-order multi-agent systems which are constrained to move on a circle is considered. Each agent can only measure the positions and velocities of its neighbors with bounded measurement errors. Using these inaccurate measurements, a distributed formation control laws is developed for each mobile agent. It is shown that under the proposed control law the multi-agent system can be driven to a neighborhood of the desired circle formation. The upper bound on the agents' formation errors in the final configuration is also provided. Note that collision between mobile agents can be prevented if the agents' spatial order on the circle is always preserved. The conditions under which order preservation of the agents is guaranteed during the formation task are also given. Finally, a simulation example is provided to illustrate the effectiveness of the proposed formation control law. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						160	167		10.1016/j.neucom.2020.02.037													
J								Automatic crack distress classification from concrete surface images using a novel deep-width network architecture	NEUROCOMPUTING										Concrete surface image; Crack distress; Automatic classification; Deep-width network	RIDGE-REGRESSION; SYSTEM	The condition monitoring of concrete surface plays a significant role in civil infrastructure management system. Crack is the main threat to concrete surface of buildings, bridges, roads and pavements. This issue has been researched for several decades, however, it is still a challenge to classify crack since there are many inferior factors, e.g., intense inhomogeneity, structure complexity and background noise of concrete surface. In this paper, a novel deep-width network (DWN) architecture is used for binary and multi-label concrete surface crack classification without handcraft feature extraction. It intelligently learns cracking structures from input raw images by linear and nonlinear mapping process, flexible dynamically updates new weights and efficiently constructs the network by adding new incremental samples. The presented crack distress classification method is tested on two concrete surface crack image datasets and compared with many popular classification methods like sparse autoencoder (SAE), convolution neural network (CNN), and broad learn system (BLS). Experimental results demonstrate that it obviously outperforms those methods both in accuracy and efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						383	392		10.1016/j.neucom.2019.08.107													
J								Wind speed forecasting using deep neural network with feature selection	NEUROCOMPUTING										Wind speed forecasting; Deep neural network; Mutual information; Stacked auto-encoder; Denoising; Long short-term memory network	INPUT VARIABLE SELECTION; MUTUAL INFORMATION; SOFT SENSOR; PREDICTION; MODEL	With the rapid growth of wind power penetration into modern power grids, wind speed forecasting (WSF) becomes an increasing important task in the planning and operation of electric power and energy systems. However, WSF is quite challengeable due to its highly varying and complex features. In this paper, a novel hybrid deep neural network forecasting method is constituted. A feature selection method based on mutual information is developed in the WSF problem. With the real-time big data from the wind farm running log, the deep neural network model for WSF is established using a stacked denoising auto-encoder and long short-term memory network. The effectiveness of the deep neural network is evaluated by 10-minutes-ahead WSF. Comparing with the traditional multi-layer perceptron network, conventional long short-term memory network and stacked auto-encoder, the resulting deep neural network significantly improves the forecasting accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						393	403		10.1016/j.neucom.2019.08.108													
J								EV charging bidding by multi-DQN reinforcement learning in electricity auction market	NEUROCOMPUTING										Electric vehicle; Deep reinforcement learning; Multi-agent; Bidding		In this paper, we address the issue of optimal bidding strategy selection for Electric Vehicles (EVs) charging in an auction market. The problem of EV charging has attracted growing attention as EVs become more and more popular. We consider the scenario that EV owners submit their bids for charging to the charging station, and then charging station determines the winning EVs who are admitted to charge and the payments based on an online continuous progressive second price (OCPSP) auction mechanism. In light of this, how to formulate optimal bidding strategy and maximize the economic benefits is crucial for EV owners. To this end, we propose a Multi-Deep-Q-Network (Multi-DQN) reinforcement learning bidding strategy, in which, a value evaluation network and a target network are proposed for each agent to learn the optimal bidding strategy. The extensive experimental results show that our bidding strategy can achieve better economic benefits and help EV owners spend less time on charging compared to the Q-learning based approach and the random approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						404	414		10.1016/j.neucom.2019.08.106													
J								A novel competitive swarm optimized RBF neural network model for short-term solar power generation forecasting	NEUROCOMPUTING										Solar forecasting; Radial basis function neural network; Competitive swarm optimization; Meta-heuristic method	ARTIFICIAL BEE COLONY; ALGORITHM; PSO	Solar power is an important renewable energy resource and acts as a major contributor to replacing fossil fuel generators and reducing carbon emissions. However, the intermittent power output due to the uncertain solar irradiance significantly challenges the economic integrations of solar generation within the existing power system, which calls for effective forecasting methods to improve the solar prediction accuracy. In this paper, a novel improved radial basis function neural network model is proposed and applied in forecasting the short-term solar power generation. A recent proposed meta-heuristic approach named competitive swarm optimization is adopted to train the non-linear and linear parameters of the radial basis function neural network model. The proposed model has been validated in nonlinear benchmark functions and then employed in forecasting the solar power generation of a real-world case study in the Netherlands. Numerical results demonstrate that the proposed competitive swarm optimized radial basis function neural network model could obtain higher accuracy compared to other counterparts and thus provides a useful tool for solar power forecasting. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						415	421		10.1016/j.neucom.2019.09.110													
J								An IGAP-RBFNN-based secondary control strategy for islanded microgrid-cyber physical system considering data uploading interruption problem	NEUROCOMPUTING										Islanded microgrid-cyber physical system; Data uploading interruption; Radial basis function neural network; Prediction compensation; Neighbor transmission	HIERARCHICAL CONTROL; FREQUENCY CONTROL; NEURAL-NETWORKS; CONTROL SCHEME; VOLTAGE; COMPENSATION; AC	At present, the traditional islanded microgrid is changing to a cyber-physical system. And the data in its cyber layer is often used to solve practical problems in the physical layer through sensors and communicators. But the use of communication data also brings a lot of problems. Among them, the upload interruption problem (i.e., communicator or sensor failure) will make a heavy impact on the system. To improve the voltage and frequency stability of microgrid through secondary control considering data uploading interruption, an improved growing and pruning-radial basis function neural network-based secondary control strategy is designed in this study. The main designs can be summarized as below: (1) When the communicator cannot receive data, a prediction compensation part is designed directly in the corresponding communicator, i.e., the predicted data is used to complete the follow-up works; (2) When the sensor cannot send data, a nonlinear cyber-physical vulnerability assessment part and a neighbor transmission part are designed. In this way, a neighbor sensor is selected to send the interrupted data for the follow-up control process; (3) Based on these designs above, a novel secondary control is designed for microgrid. Finally, the simulation results show the effectiveness of the proposed strategy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						422	437		10.1016/j.neucom.2019.08.104													
J								A photovoltaic power forecasting model based on dendritic neuron networks with the aid of wavelet transform	NEUROCOMPUTING										Photovoltaic power; Very-short-term forecasting; Dendritic neural network; Wavelet transform	SOLAR-RADIATION; TERM; FUZZY; OUTPUT; IRRADIANCE; PREDICTION; SYSTEM	The ever increasing proportion of photovoltaic (PV), which is, in effect, a random and intermittent energy source, makes PV power forecasting increasingly important for power grid stability. Artificial neural networks (ANN) have become one of the commonly utilized methods in PV power prediction. Since there is no ideal theoretical guidance as yet on the determination of the number of hidden layers and hidden units, there are always abundant neurons in traditional neural networks in order to learn as many data characteristics as possible, which often results in overfitting and high computational costs. The dendritic model proposed in recent years has the characteristics of simple structure, fast convergence and better fitting ability. This paper proposes a PV power forecasting model based on the dendritic neuron networks, which seeks to improve the computational efficiency and prediction accuracy. In order to better extract characteristics of different frequencies of the input data, the approach introduces a wavelet transform. Firstly, the data is decomposed into high-frequency and low-frequency components via a wavelet transform. Thereafter, the input data of different frequencies obtained by the decomposition are transmitted respectively to different sub-models. Finally, the results of sub-models are reconstructed to obtain the final output. The proposed PV power forecasting model was tested upon actual photovoltaic datasets. Results obtained through simulation demonstrate significant improvement in terms of accuracy and efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						438	446		10.1016/j.neucom.2019.08.105													
J								Audio-based fault diagnosis for belt conveyor rollers	NEUROCOMPUTING										Stacked sparse encoders; Convolutional neural network; Fault diagnosis; Spectral clustering; Feature extraction	CLASSIFICATION; ALGORITHM; MODEL	In order to monitor the roller states online running on the belt conveyor, one class of fault diagnosis systems based on audio is studied in this paper. Firstly, the audio data is collected from the belt conveyor by sensors, which is analyzed using the stacked sparse encoders and convolutional neural network. Secondly, the fault features are extracted from the audio data by using spectral clustering algorithm. Finally, a real fault diagnosis system is applied on the belt conveyor working in the coal preparation plant. The running result shows that the fault diagnosis system works very well for rollers fault detection with the accuracy rate 96.7%. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						447	456		10.1016/j.neucom.2019.09.109													
J								BeAware: Convolutional neural network(CNN) based user behavior understanding through WiFi channel state information	NEUROCOMPUTING										User behavior analysis; WiFi channel state information (CSI); Fresnel zone	RECOGNITION	In modern informatics society, human beings are becoming more and more attached to the computer. Therefore, understanding user behavior is critical to various application fields like sedentary analysis, human-computer interaction, and affective computing. Current sensor-based and vision-based user behavior understanding approaches are either contact or obtrusive to user s, jeopardizing their availability and practicality. To this end, we present BeAware, a contactless Radio Frequency (RF) based user behavior understanding system leveraging the WiFi Channel State Information (CSI). The key idea is to visualize the channel data affected by human movements into time-series heat-map images, which are processed by a Convolutional Neural Network (CNN) to understand the corresponding user behaviors. We prototype BeAware on commodity low-cost WiFi devices and evaluate its performance in real-world environments. Experimental results have verified its effectiveness in recognizing user behaviors. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						457	463		10.1016/j.neucom.2019.09.111													
J								New telecare approach based on 3D convolutional neural network for estimating quality of life	NEUROCOMPUTING										Human agent interaction; Communication system; Deep learning; Quality of life estimation; Welfare	FACIAL EXPRESSION ANALYSIS; EMOTION RECOGNITION; MOTION	Quality of life (QoL) is an effective index of well-being, including physical health, aspect of social activity, and mental state of individuals. A new approach that uses a deep-learning architecture to estimate the score of a user's QoL is presented. This system was built using a combination of a 3D convolutional neural network and a support vector machine for multimodal data. In order to evaluate the accuracy of the estimation system, three experiments were conducted. Before these experiments, ten hours of audio and video data were collected from healthy participants during a natural-language conversation with a conversational agent we implemented. In the first experiment, the QoL question-answer estimation experiment, the accuracy of "Physical functioning," which is one of the eight scales that constitute QoL, reached 84.0%. In the second experiment, the QoL-score-regression experiment, in which the scores of each scale were directly estimated, the distribution of the difference between the actual score and the estimated results, known as error, was investigated. These results imply that the features necessary for QoL estimation can be extracted from audio and video data, except for the "Mental Health" domain. One of the reasons why it was difficult to estimate the "Mental Health" scale may be that the learning framework could not extract an appropriate feature for estimation. Therefore, we estimated "Mental Health" by focusing on eye movement. From the result, it was proven that estimation is possible, and the proposed system using multimodal data demonstrated its effectiveness for estimation for all eight scales that constitute QoL and for extracting high-dimensional information regarding the QoL of a human, including their satisfaction level towards daily life and social activities. Finally, suggestions and discussions regarding the plausible behavior of the estimation results were made from the viewpoint of human-agent interaction in the field of elderly welfare. (C) 2020 The Author(s). Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 15	2020	397						464	476		10.1016/j.neucom.2019.09.112													
J								Multi-view semantic learning network for point cloud based 3D object detection	NEUROCOMPUTING										3D object detection; LIDAR point cloud; Semantic feature; Deep learning		Point cloud based 3D objection plays a crucial role in real-world applications, such as autonomous driving. In this paper, we propose the Multi-view Semantic Learning Network (MVSLN) for 3D object detection, an approach considering the feature discrimination for LIDAR point cloud. Since the discrete and disordered nature of point cloud, most existing methods ignore the low-level information and focus more on the spatial details of point cloud. To capture the discriminative feature of objects, our MVSLN takes advantages of both spatial and low-level details to further exploit semantic information. Specifically, the Multiple Views Generator (MVG) module in our approach observes the scene from four views by projecting the 3D point cloud to planes with specific angles, which preserves much more low-level features, e.g., texture and edge. To correct the deviation brought by different projection angles, the Spatial Recalibration Fusion (SRF) operation in our approach adjusts the locations of features of these four views, enabling the interaction between different projections. Then the recalibrated features of SRF are sent to the developed 3D Region Proposal Network (RPN) to detect objects. The experimental results on challenging KITTI benchmark verify that our approach achieves a promising performance and outperforms state-of-the-art methods. Furthermore, the discriminative feature extractor brought by exploiting the conspicuous semantic information, leads to encouraging results in the hard-level difficulty of both BEV and 3D object detection tasks, without any help of camera image. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						477	485		10.1016/j.neucom.2019.10.116													
J								A hybrid energy-Aware virtual machine placement algorithm for cloud environments	EXPERT SYSTEMS WITH APPLICATIONS										Cloud computing; Server consolidation; Virtual machine placement; Permutation-based optimization	ANT COLONY SYSTEM	The high energy consumption of cloud data centers presents a significant challenge from both economic and environmental perspectives. Server consolidation using virtualization technology is widely used to reduce the energy consumption rates of data centers. Efficient Virtual Machine Placement (VMP) plays an important role in server consolidation technology. VMP is an NP-hard problem for which optimal solutions are not possible, even for small-scale data centers. In this paper, a hybrid VMP algorithm is proposed based on another proposed improved permutation-based genetic algorithm and multidimensional resource-aware best fit allocation strategy. The proposed VMP algorithm aims to improve the energy consumption rate of cloud data centers through minimizing the number of active servers that host Virtual Machines (VMs). Additionally, the proposed VMP algorithm attempts to achieve balanced usage of the multidimensional resources (CPU, RAM, and Bandwidth) of active servers, which in turn, reduces resource wastage. The performance of both proposed algorithms are validated through intensive experiments. The obtained results show that the proposed improved permutation-based genetic algorithm outperforms several other permutation-based algorithms on two classical problems (the Traveling Salesman Problem and the Flow Shop Scheduling Problem) using various standard datasets. Additionally, this study shows that the proposed hybrid VMP algorithm has promising energy saving and resource wastage performance compared to other heuristics and metaheuristics. Moreover, this study reveals that the proposed VMP algorithm achieves a balanced usage of the multidimensional resources of active servers while others cannot. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113306	10.1016/j.eswa.2020.113306													
J								SPOCC: Scalable POssibilistic Classifier Combination - toward robust aggregation of classifiers	EXPERT SYSTEMS WITH APPLICATIONS										Robust classifier combination; Agnostic aggregation; Information fusion; Classification; Possibility theory	FUSION; LOGIC	We investigate a problem in which each member of a group of learners is trained separately to solve the same classification task. Each learner has access to a training dataset (possibly with overlap across learners) but each trained classifier can be evaluated on a validation dataset. We propose a new approach to aggregate the learner predictions in the possibility theory framework. For each classifier prediction, we build a possibility distribution assessing how likely the classifier prediction is correct using frequentist probabilities estimated on the validation set. The possibility distributions are aggregated using an adaptive t-norm that can accommodate dependency and poor accuracy of the classifier predictions. We prove that the proposed approach possesses a number of desirable classifier combination robustness properties. Moreover, the method is agnostic on the base learners, scales well in the number of aggregated classifiers and is incremental as a new classifier can be appended to the ensemble by building upon previously computed parameters and structures. A python implementation can be downloaded at this link https://github.com/john-klein/SPOCC. (C) 2020ElsevierLtd. Allrightsreserved.																	0957-4174	1873-6793				JUL 15	2020	150								113332	10.1016/j.eswa.2020.113332													
J								Weighted consensus clustering and its application to Big data	EXPERT SYSTEMS WITH APPLICATIONS										Weighted consensus clustering; Big data; Utility function; Purity-based utility function; Co-association matrix	ENSEMBLE; ALGORITHM; INDEXES	The aim of this study is the development of a weighted consensus clustering that assigns weights to single clustering methods using the purity utility function. In the case of Big data that does not contain labels, the utility function based on the Davies-Bouldin index is proposed in this paper. The Banknote authentication, Phishing, Diabetic, Magic04, Credit card clients, Covertype, Phone accelerometer, and NSL-KDD datasets are used to assess the efficiency of the proposed consensus approach. The proposed approach is evaluated using the Euclidean, Minkowski, squared Euclidean, cosine, and Chebychev distance metrics. It is compared with single clustering algorithms (DBSCAN, OPTICS, CLARANS, k-means, and shared nearby neighbor clustering). The experimental results show the effectiveness of the proposed approach to the Big data clustering in comparison to single clustering methods. The proposed weighted consensus clustering using the squared Euclidean distance metric achieves the highest accuracy, which is a very promising result for Big data clustering. It can be applied to expert systems to help experts make group decisions based on several alternatives. The paper also provides directions for future research on consensus clustering in this area. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113294	10.1016/j.eswa.2020.113294													
J								A multi-objective instance-based decision support system for investment recommendation in peer-to-peer lending	EXPERT SYSTEMS WITH APPLICATIONS										P2P lending; Multi-objective optimization; Artificial neural network; Logistic regression; Non-dominated sorting genetic algorithm	CREDIT RISK-ASSESSMENT; PORTFOLIO OPTIMIZATION; CONSUMER-CREDIT; PROSPECT-THEORY; CLASSIFICATION; ALGORITHMS; MODELS; PERFORMANCE	Peer-to-peer (P2P) lending has attracted many investors and borrowers since 2005. This financial market helps investors and borrowers to invest in or get loans without a traditional financial intermediary. Investors in the P2P lending market are allowed to invest in multiple loans instead of financing one loan entirely, so investment decision-making in P2P lending can be challenging for lenders because they are not usually expert in loan investing. The goal of this paper is to propose a data-driven investment decision-making framework for this competitive market. We use the artificial neural network and logistic regression to estimate the return and the probability of default (PD) of each individual loan. The return variable is the internal rate of return (IRR). Moreover, we formulate the investment decision-making in P2P lending as a multi-objective portfolio optimization problem based on the mean-variance theory by the use of the non-dominated sorting genetic algorithm (NSGA2). To validate the proposed model, we use a real-world dataset from one of the most popular P2P lending marketplaces. In addition, our model is compared with a single-objective model and a profit-based approach. Throughout the experiment, the empirical results reveal that our multi-objective model in comparison with the single-objective model can improve a lender's investment decision based on both objectives of investments. It means that while the return increases, the risk decreases, simultaneously. On the other hand, it is concluded that the profit scoring model leads to a more profitable investment but with a high level of risk. Finally, a sensitivity analysis is done to check the sensitivity of our model to the total investment amount. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113278	10.1016/j.eswa.2020.113278													
J								Opinion mining and emotion recognition applied to learning environments	EXPERT SYSTEMS WITH APPLICATIONS										Opinion mining; Sentiment analysis; Deep learning; Evolutionary algorithms; Machine learning; Intelligent learning environments	SENTIMENT ANALYSIS	This paper presents a comparison among several sentiment analysis classifiers using three different techniques - machine learning, deep learning, and an evolutionary approach called EvoMSA - for the classification of educational opinions in an Intelligent Learning Environment called ILE-Java. To make this comparison, we develop two corpora of expressions into the programming languages domain, which reflect the emotional state of students regarding teachers, exams, homework, and academic projects, among others. A corpus called sentiTEXT has polarity (positive and negative) labels, while a corpus called eduSERE has positive and negative learning-centered emotions (engaged, excited, bored, and frustrated) labels. From the experiments carried out with the three techniques, we conclude that the evolutionary algorithm (EvoMSA) generated the best results with an accuracy of 93% for the corpus sentiTEXT, and 84% for the corpus eduSERE. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113265	10.1016/j.eswa.2020.113265													
J								Estimation of linear motion in dense crowd videos using Langevin model	EXPERT SYSTEMS WITH APPLICATIONS										Crowd flow segmentation; Crowd dynamics; Crowd behavior; Visual surveillance; Langevin equation	ANOMALY DETECTION; REPRESENTATION; SEGMENTATION; LOCALIZATION; BEHAVIORS; TRACKING; FLOW	Expert and intelligent systems are highly popular for designing cross-domain autonomous systems. Computer vision aided by expert decision-making systems are widely used to automate tasks that are normally carried out manually. For example, in the traditional way of traffic monitoring, signals are either controlled through predefined set-ups or with the help of visual observations. Even though some of the modern cities employ sensor-based surveillance at large, full automation is still far from the desirable accuracy. This is more challenging for monitoring high-density crowds that often cause unwanted situations due to sudden changes in dynamics. It has been shown in this paper that the movement of a dense crowd can be approximated using well-known physics-based models. Such a modeling can help to understand the overall crowd behavior. In accomplishing this, we have introduced a computer vision guided expert system with the help of a Langevin equation-based force model to represent the linear flow of the crowd, particularly in situations when the density is high. One of the primary contributions of our proposed model is its computational efficiency, particularly when a timely decision can help to avoid unwanted situations. Our proposed three-term force model is capable of predicting the positions of the group of key-points in a video frame leading to a significant computational gain. We have carried out several experiments on publicly available videos as well as our own videos to validate the claims in terms of accuracy as well as computational gain. It has been observed that the proposed physics-based model outperforms the existing systems with a 4 - 6% improvement in the segmentation accuracy. Moreover, we have achieved multi-fold computational gain. We believe the proposed work, when supported by appropriate post-processing, can be used to develop crowd monitoring applications. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113333	10.1016/j.eswa.2020.113333													
J								A case-based reasoning system for supervised classification problems in the medical field	EXPERT SYSTEMS WITH APPLICATIONS										Case-Based Reasoning (CBR); Case randomization; Case validation; Medical diagnosis; Supervised classification problems	RANDOMIZATION; RETRIEVAL; SELECTION; EXAMPLES; NETWORK; SUPPORT	Case-Based Reasoning (CBR) system relies on reuse for solving new problems. The system uses the experiences it previously acquired and stored into its case base to address the newly faced problems. A static and non-evolutive case base hinders the system and limits the accuracy of the CBR in problem-solving. While a massive case base can affect the resolution time. Randomization represents a way to generate data without deteriorating the spatial image of the case base and by extension the search time as well. However, the cases generated by randomization are not necessarily valid and require a thorough validation process to access their validity. This paper presents a new amplification technique based on randomization for a CBR system incorporating a structured case-base that speeds up case retrieval while supporting case retention. The generated data by randomization is validated through a three-layer validation process: coherence verification, stochastic validation, and absolute validation. Furthermore, we propose a new way to segment the case base along with new similarity functions based on features' weights to speed CBR retrieval. We carried out experiments on mammography mass and thyroid disease datasets to validate our approach, where the proposed approach is compared to several popular supervised machine-learning methods and other related works that utilize the same datasets. Experiments have shown that our approach can generate relevant data, which significantly improves the resolution accuracy and makes CBR a good competitor to classification methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113335	10.1016/j.eswa.2020.113335													
J								Solving the generalized cubic cell formation problem using discrete flower pollination algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Cell formation; Cellular manufacturing; Generalized cubic cell formation problem; Quality index; Discrete flower pollination algorithm	OPTIMIZATION ALGORITHM; MANUFACTURING SYSTEM; MACHINE RELIABILITY; GENETIC ALGORITHM; GROUP TECHNOLOGY; MODEL; ASSIGNMENT; DESIGN	The manufacturing cell formation represents one of the important stages of the construction of cellular manufacturing systems. It focuses on grouping machines, parts and workers and assigning them to the corresponding cells. This assignment is guided by multiple objectives, and is subject to many constraints. In this paper, the focus is made on a variant of the cell formation problem, which is the Generalized Cubic Cell Formation Problem (GCCFP). In this study, a mathematical model is developed for this variant of the problem. Besides the multiple objectives considered in most research works, the quality index of the produced parts is also considered in this study. To solve the problem, a Discrete Flower Pollination Algorithm (DFPA) is developed. To validate the model and the DFPA, a set of randomly generated instances were solved using B&B under LINGO software, DFPA and Simulated Annealing (SA) algorithm. The performance of DFPA, from the standpoint of the considered objectives and the time of calculation, has been tested. The experiment results show the efficiency of the developed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113345	10.1016/j.eswa.2020.113345													
J								Object traceability graph: Applying temporal graph traversals for efficient object traceability	EXPERT SYSTEMS WITH APPLICATIONS										Object traceability; Oliot EPCIS; ChronoGraph; EPCIS; Temporal graph traversal; Information diffusion	OLIOT EPCIS; INTERNET	EPC Information Services (EPCIS) is a de-facto standard for information systems for object traceability. Expert and intelligent systems complying with the standard reap benefits from its global interoperability, and thus every application can easily retrieve track and trace information of everyday-objects in an identical manner. However, existing systems are bound to go through scalability issues due to inevitable recursive queries because users are required to handle multiple transformation and aggregation of the objects instead of the systems. In the article, we propose an enhanced EPCIS system called Object Traceability Graph (OTG). The system applies a technique, temporal graph traversal, to resolve the issues. With the system, applications do not need to request recursive queries on their side. Instead, applications are able to represent their ad-hoc traceability queries in a single statement provided by the system. Then, the statement is interpreted and efficiently processed on the system side. In our evaluation, it is shown that our approach enhances the scalability of EPCIS systems by reducing the number of queries and the amount of data transmission. The proposed approach can be applied to existing expert and intelligent systems. Furthermore, we believe that an additional interface, abstracting our approach, is general enough to be included in the standard. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113287	10.1016/j.eswa.2020.113287													
J								Ranking-based instance selection for pattern classification	EXPERT SYSTEMS WITH APPLICATIONS										Instance selection; Ranking; Instance-based learning; k-nearest neighbor; Classification	PROTOTYPE SELECTION; DYNAMIC CLASSIFIER; REDUCTION; ALGORITHMS	In instance-based learning algorithms, the need to store a large number of examples as the training set results in several drawbacks related to large memory requirements, oversensitivity to noise, and slow execution speed. Instance selection techniques can improve the performance of these algorithms by selecting the best instances from the original data set, removing, for example, redundant information and noisy points. The relationship between an instance and the other patterns in the training set plays an important role and can impact its misclassification by learning algorithms. Such a relationship can be represented as a value that measures how difficult such instance is regarding classification purposes. Based on that, we introduce a novel instance selection algorithm called Ranking-based Instance Selection (RIS) that attributes a score per instance that depends on its relationship with all other instances in the training set. In this sense, instances with higher scores form safe regions (neighborhood of samples with relatively homogeneous class labels) in the feature space, and instances with lower scores form an indecision region (borderline samples of different classes). This information is further used in a selection process to remove instances from both safe and indecision regions that are considered irrelevant to represent their clusters in the feature space. In contrast to previous algorithms, the proposal combines a raking procedure with a selection process aiming to find a promising tradeoff between accuracy and reduction rate. Experiments are conducted on twenty-four real-world classification problems and show the effectiveness of the RIS algorithm when compared against other instance selection algorithms in the literature. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113269	10.1016/j.eswa.2020.113269													
J								Shop floor simulation optimization using machine learning to improve parallel metaheuristics	EXPERT SYSTEMS WITH APPLICATIONS										Simulation optimization; Parallelism; Machine learning meta-model; Metaheuristic; Shop floor resource allocation	DISCRETE-EVENT SIMULATION; GENETIC ALGORITHMS; HYPER-HEURISTICS; ALLOCATION; NETWORKS; SEQUENCE; DESIGN; SYSTEM	Simulation optimization is a tool commonly used as a decision-making support system on industrial problems in order to find the best resource allocation, which has a direct influence on costs and revenues. The present study proposed an open-source framework developed on Python, integrating different strategies for a novel optimization algorithm. The framework includes multicore parallelism (tested on two different types of computer sets), (two) population-based metaheuristics, and 33 machine learning methods. Moreover, the study tested the framework to optimize resource allocation on a theoretical shop floor case study, evaluating 12 optimization scenarios. The use of metaheuristic with parallelism reduced 88.3% the processing time compared with the serial metaheuristic, while the integration of metaheuristic with the selected machine learning generated an additional reduction of 59.0% on the necessary processing time. The combination of the optimization methods created a solution of 95.3% near the global optimum and time reduction of 95.2%. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113272	10.1016/j.eswa.2020.113272													
J								Detection of illicit accounts over the Ethereum blockchain	EXPERT SYSTEMS WITH APPLICATIONS										Blockchain; Ethereum; Fraud detection; Machine learning; XGBoost		The recent technological advent of cryptocurrencies and their respective benefits have been shrouded with a number of illegal activities operating over the network such as money laundering, bribery, phishing, fraud, among others. In this work we focus on the Ethereum network, which has seen over 400 million transactions since its inception. Using 2179 accounts flagged by the Ethereum community for their illegal activity coupled with 2502 normal accounts, we seek to detect illicit accounts based on their transaction history using the XGBoost classifier. Using 10 fold cross-validation, XGBoost achieved an average accuracy of 0.963 (+/- 0.006) with an average AUC of 0.994 (+/- 0.0007). The top three features with the largest impact on the final model output were established to be 'Time diffbetween first and last (Mins)', 'Total Ether balance' and 'Min value received'. Based on the results we conclude that the proposed approach is highly effective in detecting illicit accounts over the Ethereum network. Our contribution is multi-faceted; firstly, we propose an effective method to detect illicit accounts over the Ethereum network; secondly, we provide insights about the most important features; and thirdly, we publish the compiled data set as a benchmark for future related works. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113318	10.1016/j.eswa.2020.113318													
J								A cost-sensitive convolution neural network learning for control chart pattern recognition	EXPERT SYSTEMS WITH APPLICATIONS										Convolutional neural network; Time-series classification; Imbalanced data; Control chart pattern recognition	IMBALANCED DATA; CLASSIFICATION; SYSTEM	Abnormal control chart patterns are naturally infrequent in an industrial setting. However, such patterns may indicate manufacturing faults that, if not treated in a timely manner, can lead to significant internal and external failure costs, ultimately threatening the product reputation. Therefore, the detection of abnormalities, which is sought in the well-known control chart pattern recognition (CCPR) problem, is of utmost importance. Standard machine learning algorithms have been extensively applied to this problem. However, they often produce biased classifiers unless the inherent data imbalancedness, which originates from the scarcity of abnormal patterns, are carefully addressed. In this paper, we develop a cost-sensitive classification scheme within a deep convolutional neural network (CSCNN) for the imbalanced CCPR problem. We further investigate the performance of our algorithm on both simulated and real-world datasets to determine separable and non-separable common fault patterns in a manufacturing setting. As the contribution of this work, we particularly demonstrate that the cost weighting strategy is both robust and efficient for moderately- and severely-imbalanced cases. We further show that our method can either be fine-tuned to specific faults or trained to detect multiple faults while remaining efficient for large datasets. To the best of our knowledge, this is the first deep CSCNN designed for imbalanced CCPR problems, which presents great promise for other manufacturing applications in the presence of imbalanced datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113275	10.1016/j.eswa.2020.113275													
J								Relation chaining in binary positive-only recommender systems	EXPERT SYSTEMS WITH APPLICATIONS										Recommender systems; Data fusion; Matrix co-factorization; Relation chaining; Transitivity; Chain matrix multiplication	DATA FUSION; MATRIX	Recommender systems typically work on user-item preference relation data. Recommendations can be improved by including side relations that are indirectly linked to the target relation. Data fusion by matrix co-factorization is one such method that can integrate heterogeneous representations of objects of different types. A shared latent matrix factor model is inferred, which is then used to approximate and thus predict multiple data matrices at the same time. The factor model can also be used to infer indirect relations among objects, by multiplying the corresponding factor matrices on a chain of relations that link those objects (i.e., relation chaining). We show that recommendation in binary positive-only data can be improved by relation chaining. We can filter out less reliable relations by using the number of supporting intermediate paths as an additional parameter in a multi-objective Pareto optimization. Our method outperforms other state-of-the-art chaining methods. To speed-up the computation of relation chaining, we propose a chain matrix multiplication-based approach for chaining. To evaluate our method, we have created synthetic data on transitive relations among objects, for a varying degree of noise. Results on synthetic data show that chaining indeed works on chains containing transitive relations. Results on three real datasets show that the inclusion of the number of intermediate paths improves relation chaining predictions. Compared to the full data matrix multiplication approach, the proposed relation chaining method achieved two-fold speed-up. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113296	10.1016/j.eswa.2020.113296													
J								Learning local representations for scalable RGB-D face recognition	EXPERT SYSTEMS WITH APPLICATIONS										Face recognition; SRC; Data-driven descriptors; Convolutional neural networks; BSIF; RGB-D Sensors; Deep learning		In this article we present a novel RGB-D learned local representations for face recognition based on facial patch description and matching. The major contribution of the proposed approach is an efficient learning and combination of data-driven descriptors to characterize local patches extracted around image reference points. We explored the complementarity between both of deep learning and statistical image features as data-driven descriptors. In addition, we proposed an efficient high-level fusion scheme based on a sparse representation algorithm to leverage the complementarity between image and depth modalities and also the used data-driven features. Our approach was extensively evaluated on four well-known benchmarks to prove its robustness against known challenges in the case of face recognition. The obtained experimental results are competitive with the state-of-the-art methods while providing a scalable and adaptive RGB-D face recognition method. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113319	10.1016/j.eswa.2020.113319													
J								Generative adversarial network-based semi-supervised learning for real-time risk warning of process industries	EXPERT SYSTEMS WITH APPLICATIONS										Risk warning; Deep learning; Generative adversarial networks; Semi-supervised learning; Fuzzy HAZOP; Multizone circulating reactor	HAZOP ANALYSIS; SYSTEM; MODEL; AUTOENCODER; DESIGN	Due to the non-cognition of real-time data, rare loss-based risk warning methods can effectively respond to unexpected emergencies. Machine learning has powerful data processing capabilities and real-time computing functions and thus is suitable for offsetting the shortcomings of traditional risk methods. Risk analysis can be easily employed to perform risk-based data classification for a set of process data. However, the risk analysis process is too complicated to label risk levels for all processes, which is hard to satisfy the requirements of the amount of data for supervised learning. Therefore, the present paper focuses on developing semi-supervised learning methods for the construction of real-time risk-based early warning systems. By using fuzzy HAZOP, we estimate the risk of systems quantitatively based on the process data. With the consideration of scarce labeled data and numerous unlabeled information, we develop the generative adversarial network (GAN)-based semi-supervised learning method to identify the process risk timely. Besides, deep network architecture integrated with the convolutional neural network (CNN) is used for the codification of multi-dimensional process data to enhance the generalization of warning models. Finally, the effectiveness of the proposed method is evaluated through a comparative study with different algorithms on a case of multizone circulating reactor (MZCR). (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113244	10.1016/j.eswa.2020.113244													
J								SSDTW: Shape segment dynamic time warping	EXPERT SYSTEMS WITH APPLICATIONS										Time series data; Dynamic time warping (DTW); Shape segment dynamic time warping (SSDTW); Maximal overlap discrete wavelet transform (MODWT); Alignment path	CLASSIFICATION	In order to increase the yield of a process, it is essential to establish a process control based on manufacturing data. Process management systems mainly consist of statistical process control (SPC), fault detection and classification (FDC), and advanced process control (APC), and are modeled using time series data. However, large amounts of time series data and various distributions are collected in the process; hence, preprocessing measures, such as length adjustment, are essential for modeling. Dynamic time warping (DTW) has been widely used as an algorithm that can measure the similarity between two different time series data and adjust their length. However, owing to the complex structure and time lag of processing time series data, there are limitations in applying the traditional DTW. Therefore, to solve this problem, we propose the shape segment dynamic time warping (SSDTW) algorithm that improves DTW in consideration of the structure information of time series data. By using the maximum overlap discrete wavelet transform (MODWT), the proposed method reflects the peripheral information of the time series data and divides the time series data interval to achieve a reasonable local alignment path. SSDTW attains more accurate alignment paths than DTW, derivative dynamic time warping (DDTW), and shapeDTW. Experiments conducted using semiconductor signal data and UCR time series data sets show that the proposed method is more effective than DTW, DDTW, and shapeDTW. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113291	10.1016/j.eswa.2020.113291													
J								Improving patient-care services at an oncology clinic using a flexible and adaptive scheduling procedure	EXPERT SYSTEMS WITH APPLICATIONS										Chemotherapy; Flexible patient scheduling; Preference; Primary care delivery; Adaptive rescheduling; Integer programming	HEALTH-CARE; OPTIMIZATION	This paper studies an online scheduling problem dealing with patients' multiple requests for chemotherapy treatments at the cancer centre of a major metropolitan hospital in Canada. The proposed solution to the problem is an adaptive and flexible procedure that systematically combines two optimization models. The first model is intended to dynamically schedule incoming appointment requests, which arrive in the form of waiting lists, and the second model is used to reschedule already booked appointments with the goal of better allocating resources as new information becomes available. The performance and potential impact of the proposed procedure is assessed using historical data provided by the cancer centre. Moreover, a sensitivity analysis is carried out to draw insights that may help hospital managers to deal more efficiently with both incoming requests and unexpected events. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113267	10.1016/j.eswa.2020.113267													
J								Effects of the validation set on stock returns forecasting	EXPERT SYSTEMS WITH APPLICATIONS										Stock returns forecasting; Deep learning; Hyperparameter setting	PREDICTION; ACCURACY; ERROR; MODEL	Deep neural networks are potentially suitable tools for time series forecasting due to their ability to extract complex patterns of nonlinear data and their versatility in terms of models and applications. Even though they are powerful instruments and well-behaved approaches for certain tasks, they are sometimes surpassed by data complexity, and thus struggle to find an error that generalizes well enough on unseen data, especially in cases like times series forecasting for stock trading strategies. In this paper, the complex characteristics of time series are addressed by separating data by means of simpler yet more relevant distinctions in order to create a single model for every existing or created category, with a focus on model validation. This creates models which are trained on the same data, but validated for a particular class so that the models' hyperparameters are specifically tuned to that class. Experiments on convolutional networks applied to the DJIA, Nasdaq and S&P 500 indices using volatility as a class or category indicator, have shown that it is possible to improve predictions after validating the model, obtaining the best model per the Model Confidence Set among different regression models on all time series datasets. Even the best and only model necessary for the DJIA and S&P 500 indices can be obtained at a significance value of 5% given that the level of volatility is known. The results highlight the importance of knowing the data and how to potentially separate them into simpler yet relevant classes. The results also reveal how model validation on different data is capable of creating models that better explain information just by tuning the model's architectural hyperparameters, even though the models where trained on the very same data. This finding could be applied to any task requiring validation without modifying the training set, which is usually bigger and more expensive to obtain. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113271	10.1016/j.eswa.2020.113271													
J								Improving spherical k-means for document clustering: Fast initialization, sparse centroid projection, and efficient cluster labeling	EXPERT SYSTEMS WITH APPLICATIONS										Spherical k-means; Document clustering; k-means initialization; Sparse vector projection; Clustering labeling	MEANS ALGORITHM; QUANTIZATION	Due to its simplicity and intuitive interpretability, spherical k-means is often used for clustering a large number of documents. However, there exist a number of drawbacks that need to be addressed for much effective document clustering. Without well-dispersed initial points, spherical k-means fails to converge quickly, which is critical for clustering a large number of documents. Furthermore, its dense centroid vectors needlessly incorporate the impact of infrequent and less-informative words, thereby distorting the distance calculation between the document vectors. In this paper, we propose practical improvements on spherical k-means to overcome these issues during document clustering. Our proposed initialization method not only guarantees dispersed initial points, but is also up to 1000 times faster than previously well-known initialization method such as k-means++. Furthermore, we enforce sparsity on the centroid vectors by using a data-driven threshold that is capable of dynamically adjusting its value depending on the clusters. Additionally, we propose an unsupervised cluster labeling method that effectively extracts meaningful keywords to describe each cluster. We have tested our improvements on seven different text datasets that include both new and publicly available datasets. Based on our experiments on these datasets, we have found that our proposed improvements successfully overcome the drawbacks of spherical k-means in significantly reduced computation time. Furthermore, we have qualitatively verified the performance of the proposed cluster labeling method by extracting descriptive keywords of the clusters from these datasets. (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				JUL 15	2020	150								113288	10.1016/j.eswa.2020.113288													
J								Predicting online shopping behaviour from clickstream data using deep learning	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; e-commerce; Recurrent neural networks; Digital marketing	PURCHASE; CONVERSION; MODEL; TIME	Clickstream data is an important source to enhance user experience and pursue business objectives in e-commerce. The paper uses clickstream data to predict online shopping behavior and target marketing interventions in real-time. Such AI-driven targeting has proven to save huge amounts of marketing costs and raise shop revenue. Previous user behavior prediction models rely on supervised machine learning (SML). Conceptually, SML is less suitable because it cannot account for the sequential structure of clickstream data. The paper proposes a methodology capable of unlocking the full potential of clickstream data using the framework of recurrent neural networks (RNNs). An empirical evaluation based on real-world e-commerce data systematically assesses multiple RNN classifiers and compares them to SML benchmarks. To this end, the paper proposes an approach to measure the revenue impact of a targeting model. Estimates of revenue impact together with results of standard classifier performance metrics evidence the viability of RNN-based clickstream modeling and guide employing deep recurrent learners for campaign targeting. Given that the empirical analysis shows RNN-based and conventional classifiers to capture different patterns in clickstream data, a specific recommendation is to combine sequence and conventional classifiers in an ensemble. The paper shows such an ensemble to consistently outperform the alternative models considered in the study. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113342	10.1016/j.eswa.2020.113342													
J								Nonlinear process monitoring based on decentralized generalized regression neural networks	EXPERT SYSTEMS WITH APPLICATIONS										Generalized regression neural network; Nonlinear process monitoring; Fault detection; Residual generation	PRINCIPAL COMPONENT ANALYSIS; LATENT VARIABLE MODELS; FAULT-DETECTION; DIAGNOSIS	Given that the main task of process monitoring (i.e., fault detection) is actually a classical one-class classification problem, the generalized regression neural network (GRNN) is directly inapplicable for handling process modeling and monitoring issues. Through the selection of only one variable to be the output while the others serve as the corresponding input, a GRNN model can then be constructed to approximate the nonlinear input to output relationship. The residuals, signifying the inconsistency between the actual measurement and the predicted output from the GRNN model, could be a good indicator for online fault detection. The proposed nonlinear process monitoring approach is termed decentralized GRNN (DGRNN), which applies the GRNN in an extremely decentralized manner and utilizes the squared Mahalanobis distance for the online monitoring of the abnormalities captured by the generated residuals. The effectiveness and superiority of the DGRNN-based nonlinear process monitoring approach over other state-of-the-art nonlinear process monitoring methods are investigated by comparisons in two nonlinear processes. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113273	10.1016/j.eswa.2020.113273													
J								Fast hybrid dimensionality reduction method for classification based on feature selection and grouped feature extraction	EXPERT SYSTEMS WITH APPLICATIONS										Dimensionality Reduction; Intrinsic Dimensionality; Feature Selection; Feature Cluster; PCA	INTEGRATING FEATURE-SELECTION	Dimensionality reduction is one basic and critical technology for data mining, especially in current "big data" era. As two different types of methods, feature selection and feature extraction each have their pros and cons. In this paper, we combine multi-strategy feature selection and grouped feature extraction and propose a novel fast hybrid dimension reduction method, incorporating their advantages of removing irrelevant and redundant information. Firstly, the intrinsic dimensionality of the data set is estimated by the maximum likelihood estimation method. Fisher Score and Information Gain based feature selection are used as multi-strategy methods to remove irrelevant features. With the redundancy among the selected features as clustering criterion, they are grouped into a certain amount of clusters. In every cluster, Principal Component Analysis (PCA) based feature extraction is carried out to remove redundant information. Four classical classifiers and representation entropy are used to evaluate the classification performance and information loss of the reduced set. The runtime results of different methods show that the proposed hybrid method is consistently much faster than the other three in almost all of the sets used. Meanwhile, the proposed method shows competitive classification performance, which has no significant difference basically compared with the other methods. The proposed method reduces the dimensionality of the raw data fast and it has excellent efficiency and competitive classification performance compared with the contrastive methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113277	10.1016/j.eswa.2020.113277													
J								Non-numerical nearest neighbor classifiers with value-object hierarchical embedding	EXPERT SYSTEMS WITH APPLICATIONS										Non-numerical classification; Categorical data; Nearest neighbor classifier; Data complexity; Attribute reduction	CLASSIFICATION; PROBABILITY; ALGORITHM; BAYES; ROUGH	Non-numerical classification plays an essential role in many real-world applications such as DNA analysis, recommendation systems and expert systems. The nearest neighbor classifier is one of the most popular and flexible models for performing classification tasks in these applications. However, due to the complexity of non-numerical data, existing nearest neighbor classifiers that use the overlap measure and its variants cannot capture the inherent ordered relationship and statistic information of non-numerical data. This phenomenon leads to the classification limitation of nearest neighbor classifiers in non-numerical data environments. To overcome this challenge, we propose a novel object distance metric, i.e., value-object hierarchical metric (VOHM), which is able to capture inherent ordered relationships within non-numerical data. Then, we construct two nearest neighbor classifiers, i.e., the value-object hierarchical embedded nearest neighbor classifier (VO-kNN) and the two-stage value-object hierarchical embedded nearest neighbor classifier (TSVO-kNN), which take advantages of both VOHM and non-numerical feature selection. Experiments show that both VO-kNN and TSVO-kNN could mine more knowledge from data and achieve better performance than state-of-the-art classifiers in non-numerical data environments. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113206	10.1016/j.eswa.2020.113206													
J								An AHP-based multi-criteria model for sustainable supply chain development in the renewable energy sector	EXPERT SYSTEMS WITH APPLICATIONS										Analytic hierarchy process; Multi-criteria decision making; Photovoltaic sector; Renewable energy; Sustainable supply chain; Triple bottom line	DECISION-MAKING; SITE SELECTION; SOCIAL-RESPONSIBILITY; FUZZY TOPSIS; SOLAR PV; OPTIMIZATION; PERFORMANCE; MANAGEMENT; TECHNOLOGIES; ELECTRICITY	The aim of this paper is to provide a multi-criteria decision making framework based on the Triple Bottom Line principles and Analytic Hierarchy Process methodology for sustainable supply chain development in the renewable energy sector. The proposed framework encompasses the whole energy production supply chain, from raw materials' suppliers to disposal. In particular, the photovoltaic energy sector has been used as case study and represents the focus of this work. The framework is based on the three Triple Bottom Line dimensions such as social, economic and environmental. Furthermore, literature review and expert opinions are used to identify and assess the sub-criteria for each dimension, followed by pair-wise comparison. Finally, the proposed framework is used to evaluate the seven European countries that conjointly represent the 86.8% of the total photovoltaic installed capacity in Europe, using both logical and quantitative information. Results are in agreement with the photovoltaic development in the period 2000-2017 in these countries. The proposed framework provides the decision makers with a powerful tool for making sustainable investment decisions in the photovoltaic energy sector. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113321	10.1016/j.eswa.2020.113321													
J								An evolutionary Pentagon Support Vector finder method	EXPERT SYSTEMS WITH APPLICATIONS										Big data; Data mining; Support vector; Artificial Bee Colony (ABC); Evolutionary clustering; Fuzzy C means (FCM); Pentagon Support Vector finder (PSV)	BEE COLONY ALGORITHM; OPTIMIZATION; CLASSIFICATION; COMPUTATION; MACHINES; DESIGN	In dealing with big data, we need effective algorithms; effectiveness that depends, among others, on the ability to remove outliers from the data set, especially when dealing with classification problems. To this aim, support vector finder algorithms have been created to save just the most important data in the data pool. Nevertheless, existing classification algorithms, such as Fuzzy C-Means (FCM), suffer from the drawback of setting the initial cluster centers imprecisely. In this paper, we avoid existing shortcomings and aim to find and remove unnecessary data in order to speed up the final classification task without losing vital samples and without harming final accuracy; in this sense, we present a unique approach for finding support vectors, named evolutionary Pentagon Support Vector (PSV) finder method. The originality of the current research lies in using geometrical computations and evolutionary algorithms to make a more effective system, which has the advantage of higher accuracy on some data sets. The proposed method is subsequently tested with seven benchmark data sets and the results are compared to those obtained from performing classification on the original data (classification before and after PSV) under the same conditions. The testing returned promising results. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113284	10.1016/j.eswa.2020.113284													
J								Learning hidden Markov models with persistent states by penalizing jumps	EXPERT SYSTEMS WITH APPLICATIONS										Time series analysis; Clustering; Unsupervised learning; Regime switching; Regularization; Dynamic programming	FINANCIAL TIME-SERIES; ASSET ALLOCATION; STYLIZED FACTS; SEGMENTATION; LIKELIHOOD; MARKETS; EM	Hidden Markov models are applied in many expert and intelligent systems to detect an underlying sequence of persistent states. When the model is misspecified or misestimated, however, it often leads to unrealistically rapid switching dynamics. To address this issue, we propose a novel estimation approach based on clustering temporal features while penalizing jumps. We compare the approach to spectral clustering and the standard approach of maximizing the likelihood function in an extensive simulation study and an application to financial data. The advantages of the proposed jump estimator include that it learns the hidden state sequence and model parameters simultaneously and faster while providing control over the transition rate, it is less sensitive to initialization, it performs better when the number of states increases, and it is robust to misspecified conditional distributions. The value of estimating the true persistence of the state process is illustrated through a simple trading strategy where improved estimates result in much lower transaction costs. Robustness is particularly critical when the model is part of a system used in production. Therefore, our proposed estimator significantly improves the potential for using hidden Markov models in practical applications. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113307	10.1016/j.eswa.2020.113307													
J								An energy-efficient permutation flowshop scheduling problem	EXPERT SYSTEMS WITH APPLICATIONS										Permutation flowshop scheduling problem; Multi-objective optimization; Energy-efficient scheduling; Heuristic algorithms	TOTAL WEIGHTED TARDINESS; ITERATED GREEDY ALGORITHM; DEPENDENT SETUP TIMES; SINGLE-MACHINE; POWER-CONSUMPTION; MEMETIC ALGORITHM; LOCAL SEARCH; JOB-SHOP; MAKESPAN; HEURISTICS	The permutation flowshop scheduling problem (PFSP) has been extensively explored in scheduling literature because it has many real-world industrial implementations. In some studies, multiple objectives related to production efficiency have been considered simultaneously. However, studies that consider energy consumption and environmental impacts are very rare in a multi-objective setting. In this work, we studied two contradictory objectives, namely, total flowtime and total energy consumption (TEC) in a green permutation flowshop environment, in which the machines can be operated at varying speed levels corresponding to different energy consumption values. A bi-objective mixed-integer programming model formulation was developed for the problem using a speed-scaling framework. To address the conflicting objectives of minimizing TEC and total flowtime, the augmented epsilon-constraint approach was employed to obtain Pareto-optimal solutions. We obtained near approximations for the Pareto-optimal frontiers of small-scale problems using a very small epsilon level. Furthermore, the mathematical model was run with a time limit to find sets of non-dominated solutions for large instances. As the problem was NP-hard, two effective multi-objective iterated greedy algorithms and a multi-objective variable block insertion heuristic were also proposed for the problem as well as a novel construction heuristic for initial solution generation. The performance of the developed heuristic algorithms was assessed on well-known benchmark problems in terms of various quality measures. Initially, the performance of the algorithms was evaluated on small-scale instances using Pareto-optimal solutions. Then, it was shown that the developed algorithms are tremendously effective for solving large instances in comparison to time-limited model. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113279	10.1016/j.eswa.2020.113279													
J								Hand sign language recognition using multi-view hand skeleton	EXPERT SYSTEMS WITH APPLICATIONS										Multi-view hand skeleton; Hand sign language recognition; 3DCNN; Hand pose estimation; RGB video; Hand action recognition	3D HAND; ACCURATE; NETWORK	Hand sign language recognition from video is a challenging research area in computer vision, which performance is affected by hand occlusion, fast hand movement, illumination changes, or background complexity, just to mention a few. In recent years, deep learning approaches have achieved state-of-the-art results in the field, though previous challenges are not completely solved. In this work, we propose a novel deep learning-based pipeline architecture for efficient automatic hand sign language recognition using Single Shot Detector (SSD), 2D Convolutional Neural Network (2DCNN), 3D Convolutional Neural Network (3DCNN), and Long Short-Term Memory (LSTM) from RGB input videos. We use a CNN-based model which estimates the 3D hand keypoints from 2D input frames. After that, we connect these estimated keypoints to build the hand skeleton by using midpoint algorithm. In order to obtain a more discriminative representation of hands, we project 3D hand skeleton into three views surface images. We further employ the heatmap image of detected keypoints as input for refinement in a stacked fashion. We apply 3DCNNs on the stacked features of hand, including pixel level, multi-view hand skeleton, and heatmap features, to extract discriminant local spatio-temporal features from these stacked inputs. The outputs of the 3DCNNs are fused and fed to a LSTM to model long-term dynamics of hand sign gestures. Analyzing 2DCNN vs. 3DCNN using different number of stacked inputs into the network, we demonstrate that 3DCNN better capture spatio-temporal dynamics of hands. To the best of our knowledge, this is the first time that this multi-modal and multi-view set of hand skeleton features are applied for hand sign language recognition. Furthermore, we present a new large-scale hand sign language dataset, namely RKS-PERSIANSIGN, including 10'0 00 RGB videos of 100 Persian sign words. Evaluation results of the proposed model on three datasets, NYU, First-Person, and RKS-PERSIANSIGN, indicate that our model outperforms state-of-the-art models in hand sign language recognition, hand pose estimation, and hand action recognition. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113336	10.1016/j.eswa.2020.113336													
J								An Ontology-based approach to Knowledge-assisted Integration and Visualization of Urban Mobility Data	EXPERT SYSTEMS WITH APPLICATIONS										Data integration; Data visualization; Urban mobility; Semantic web; ontology		This paper proposes an ontology-based framework to support integration and visualization of data from Intelligent Transportation Systems. These activities may be technically demanding for transportation stakeholders, due to technical and human factors, and may hinder the use of visualization tools in practice. The existing ontologies do not provide the necessary semantics for integration of spatio-temporal data from such systems. Moreover, a formal representation of the components of visualization techniques and expert knowledge can leverage the development of visualization tools that facilitate data analysis. The proposed Visualization-oriented Urban Mobility Ontology (VUMO) provides a semantic foundation to knowledge-assisted visualization tools (KVTs). VUMO contains three facets that interrelate the characteristics of spatio-temporal mobility data, visualization techniques and expert knowledge. A built-in rule set leverages semantic technologies standards to infer which visualization techniques are compatible with analytical tasks, and to discover implicit relationships within integrated data. The annotation of expert knowledge encodes qualitative and quantitative feedback from domain experts that can be exploited by recommendation methods to automate part of the visualization workflow. Data from the city of Porto, Portugal were used to demonstrate practical applications of the ontology for each facet. As a foundational domain ontology, VUMO can be extended to meet the distinctiveness of a KVT. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113260	10.1016/j.eswa.2020.113260													
J								A scatter search method for heterogeneous fleet vehicle routing problem with release dates under lateness dependent tardiness costs	EXPERT SYSTEMS WITH APPLICATIONS										Heterogeneous vehicle routing problem; Scatter search; Vehicle routing problem with release and due date; Iterated local search; Mixed integer linear programming; Strategic oscillation	DUE-DATES; TIME; HEURISTICS; ALGORITHM	In this paper, we introduce a heterogeneous fleet vehicle routing problem with release and due dates in the presence of consolidation of customer orders and limited warehousing capacity. Tardiness cost for an order depends upon the magnitude of lateness and the type of the order. A mixed-integer programming model is proposed to minimize the sum of inventory holding, transportation, tardiness, and backorder costs. We demonstrate that customer order characteristics and limited warehousing capacity influence optimal vehicle routes significantly. As the problem is NP-hard, a scatter search (SS) method with strategic oscillation is designed to solve the large-sized instances. Problem-specific characteristics are used to customize the improvement method. Computational experiments suggest that SS finds excellent quality solutions compared to CPLEX in significantly lesser time. SS also performs better than the iterated local search used in the prior literature for the large-sized instances. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113302	10.1016/j.eswa.2020.113302													
J								Seq2Seq models for recommending short text conversations	EXPERT SYSTEMS WITH APPLICATIONS										NLP; Recommender systems; Neural learning; Microblogs		The massive amounts of data on social media networks can be overwhelming for users; for this reason, recommending relevant content becomes an essential task to avoid information overload. In this paper, we propose a new task for recommending users that might be interested in join conversations on specific domains. To that end, we introduce a new corpus that contains conversations threads from popular users on Twitter on domains such as politics, sports, or humanitarian activism. Modeling short-text conversations on microblogs can be difficult because user-generated data is unstructured and noisy. Previous works focused on recommending content to users based on latent factors models and collaborative filtering methods. We propose a state-of-the-art recommendation model based on a sequence-to-sequence neural architecture that encodes the text of users' profiles and the conversations' context using several variants of recurrent neural networks. The experimental results show that our method provides as much as 20% higher recall compared to baseline methods. Moreover, we use an end-to-end learning framework that allows downstream applications to use recommender systems (RSs) that generalize better to new content by using pre-trained embeddings, thus being useful across domains or events. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113270	10.1016/j.eswa.2020.113270													
J								Yin-Yang firefly algorithm based on dimensionally Cauchy mutation	EXPERT SYSTEMS WITH APPLICATIONS										Yin-Yang firefly algorithm; Cauchy mutation; GNS strategy; Random attraction model; CEC 2013 benchmark functions; Engineering optimization problems	OPTIMIZATION; EVOLUTIONARY; OPPOSITION; DESIGN; STRATEGY	Firefly algorithm (FA) is a classical and efficient swarm intelligence optimization method and has a natural capability to address multimodal optimization. However, it suffers from premature convergence and low stability in the solution quality. In this paper, a Yin-Yang firefly algorithm (YYFA) based on dimensionally Cauchy mutation is proposed for performance improvement of FA. An initial position of fireflies is specified by the good nodes set (GNS) strategy to ensure the spatial representativeness of the firefly population. A designed random attraction model is then used in the proposed work to reduce the time complexity of the algorithm. Besides, a key self-learning procedure on the brightest firefly is undertaken to strike a balance between exploration and exploitation. The performance of the proposed algorithm is verified by a set of CEC 2013 benchmark functions used for the single objective real parameter algorithm competition. Experimental results are compared with those of other the state-of- the-art variants of FA. Nonparametric statistical tests on the results demonstrate that YYFA provides highly competitive performance in terms of the tested algorithms. In addition, the application in constrained engineering optimization problems shows the practicability of YYFA algorithm. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113216	10.1016/j.eswa.2020.113216													
J								Orthogonally-designed adapted grasshopper optimization: A comprehensive analysis	EXPERT SYSTEMS WITH APPLICATIONS										Grasshopper optimization; Meta-heuristics; Orthogonal learning; Chaotic exploitation	PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZATION; SINE COSINE ALGORITHM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; WHALE OPTIMIZATION; FEATURE-SELECTION; INSPIRED OPTIMIZER; STRATEGY	Grasshopper optimization algorithm (GOA) is a newly proposed meta-heuristic algorithm that simulates the biological habits of grasshopper seeking for food sources. Nonetheless, some shortcomings exist in the basic version of GOA. It may quickly drop into local optima and show slow convergence rates when facing some complex basins. In this work, an improved GOA is proposed to alleviate the core shortcomings of GOA and handle continuous optimization problems more efficiently. For this purpose, two strategies, including orthogonal learning and chaotic exploitation, are introduced into the conventional GOA to find a more stable trade-offbetween the exploration and exploitation cores. Adding orthogonal learning to GOA can enhance the diversity of agents, whereas a chaotic exploitation strategy can update the position of grasshoppers within a limited local region. To confirm the efficacy of GOA, we compared it with a variety of famous classical meta-heuristic algorithms performed on 30 IEEE CEC2017 benchmark functions. Also, it is applied to feature selection cases, and three structural design problems are employed to validate its efficacy in terms of different metrics. The experimental results illustrate that the above tactics can mitigate the deficiencies of GOA, and the improved variant can reach high-quality solutions for different problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113282	10.1016/j.eswa.2020.113282													
J								Improving artificial algae algorithm performance by predicting candidate solution quality	EXPERT SYSTEMS WITH APPLICATIONS										Swarm intelligence; Artificial algae algorithm; Naive Bayes; Candidate solution prediction	PARTICLE SWARM OPTIMIZER; BEE COLONY ALGORITHM; TEXT CLASSIFICATION; NAIVE BAYES	The success of optimization algorithms is most of the time directly proportional to the number of fitness evaluations. However, not all fitness evaluations lead to successful fitness updates. Besides, the maximum number of fitness evaluations is limited and also balance of exploration and exploitation is still challenging. Best possible solution should be found in a reasonable time. Surely it can be said more fitness evaluation takes more time. Since methods are tested under fixed numbers of maximum fitness evaluation and the duration of each fitness evaluation of a problem may vary depending on the characteristic of the problem, finding best result with fewer fitness evaluations is challenging in optimization algorithms. For that reason in this study, we proposed a new method that predicts the quality of a candidate solution before evaluation of its fitness employing Gaussian-based Naive Bayes probabilistic model. If the candidate solution is predicted to generate good result then that solution is evaluated by the objective function. Otherwise new candidate solution is created as usual. The primary purpose of the proposed method is improving the performance of AAA and at the same time preventing unnecessary fitness evaluation. The proposed method is evaluated using standard benchmark functions and CEC'05 test suite. The obtained results suggests that the new method outperformed the basic AAA and other state-of-the-art meta-heuristic algorithms with fewer fitness evaluations. Thus, the new method can be extended to cost sensitive industrial problems. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 15	2020	150								113298	10.1016/j.eswa.2020.113298													
J								Processing Surface EMG Signals for Exoskeleton Motion Control	FRONTIERS IN NEUROROBOTICS										exoskeleton; gait; electromyography; volitional control; treadmill; rehabilitation	BRAIN-MACHINE INTERFACES; TREADMILL; REHABILITATION; WALKING; SPEED; NEUROREHABILITATION; ADAPTATION; STRATEGIES; STROKE; LENGTH	The surface electromyography (sEMG) signal has been used for volitional control of robotic assistive devices. There are still challenges in improving system performance accuracy and signal processing to remove systematic noise. This study presents procedures and a pilot validation of the EMG-driven speed-control of exoskeleton and integrated treadmill with a goal to provide better interaction between a user and the system. The gait cycle duration (GCD) was extracted from sEMG signals using the autocorrelation algorithm and Bayesian fusion algorithm. GCDs of various walking speeds were then programmed to control the motion speed of exoskeleton robotic system. The performance and efficiency of this sEMG-controlled robotic assistive ambulation system was tested and validated among 6 healthy volunteers. The results demonstrated that the autocorrelation algorithm extracted the GCD from individual muscle contraction. The GCDs of individual muscles had variability between different walking steps under a designated walking speed. Bayesian fusion algorithms processed the GCDs of multiple muscles yielding a final GCD with the least variance. The fused GCD effectively controlled the motion speeds of exoskeleton and treadmill. The higher amplitude of EMG signals with shorter GCD was found during a faster walking speed. The algorithms using fused GCDs and gait stride length yielded trajectory joint motion tracks in a shape of sine curve waveform. The joint angles of the exoskeleton measured by a decoder mounted on the hip turned out to be in sine waveforms. The hip joint motion track of the exoskeleton matched the angles projected by trajectory curve generated by computer algorithms based on the fused GCDs with high agreement. The EMG-driven speed-control provided the human-machine inter-limb coordination mechanisms for an intuitive speed control of the exoskeleton-treadmill system at the user's intents. Potentially the whole system can be used for gait rehabilitation of incomplete spinal cord hemispheric stroke patients as goal-directed and task-oriented training tool.																	1662-5218					JUL 14	2020	14								40	10.3389/fnbot.2020.00040													
J								Brain-Inspired Active Learning Architecture for Procedural Knowledge Understanding Based on Human-Robot Interaction	COGNITIVE COMPUTATION										Brain-inspired architecture; Procedural knowledge; Deep neural network; Reinforcement learning; Knowledge graph		Improving robots with self-learning ability is one of the critical challenges for the researchers in the area of cognitive robotics and artificial general intelligence. This robot will decide when, where, and what to learn in a continuous visual environment by itself. Here we focus on the procedural knowledge learning, which is sequential and considered harder to understand compared with declarative knowledge in the cognitive system. Inspired by the architecture of the human brain which has integrated well different kinds of cognitive functions, a Brain-inspired Active Learning Architecture (BALA) is proposed for procedural knowledge understanding based on Baxter robot and human interaction. The BALA model contains four main parts: inspired by Primary Visual Pathway, a Convolutional Neural Network (CNN) is constructed for spatial information abstraction; inspired by the Hippocampus Pathway (especially the recurrent loops in CA3 sub-region), a Recurrent Neural Network (RNN) is built for sequential information processing related with procedural knowledge; inspired by the Prefrontal Cortex, a Knowledge Graph based on Bag Of Words (BOW) is constructed for declarative knowledge generation and association; inspired by the Basal Ganglia Pathway, we select Q matrix for Reinforcement Learning (RL). The CNN and RNN parts will be firstly pre-trained on ImageNet dataset and standard Youtube Video-Scene dataset respectively. Then, the RNN, Knowledge Graph, and Q matrix will be dynamically updated in the Baxter robot's interactive learning procedure with human cooperators. The BALA could actively and incrementally recognize different kinds of procedural knowledge. In 22-type daily-life videos with procedure knowledge (e.g., opening the door, wiping the table, or taking the phone), the BALA model gets the best performance compared with standard CNN, RNN, RL, and other integrative methods. The BALA model is a small step on integrative intelligence interaction between the Baxter robot and human cooperator.																	1866-9956	1866-9964															10.1007/s12559-020-09753-1		JUL 2020											
J								Development of statistical estimators for speech enhancement using multi-objective grey wolf optimizer	EVOLUTIONARY INTELLIGENCE										Speech enhancement; Statistical estimators; Bio-inspired techniques; Quality; Intelligibility; MOGWO; Fuzzy logic	SPECTRAL AMPLITUDE ESTIMATION; NOISY; ALGORITHMS	Statistical Estimation using the SNR uncertainty technique is one of the effective Speech Enhancement (SE) algorithms. In this method, the Gain function plays a crucial role and it depends on the proper selection of the smoothing and threshold constants. In the literature, the values of these constants have been optimized by considering a single objective function of maximization of speech quality for a specific noise condition. But in practice, the noise magnitude varies and one set of optimized parameters cannot always provide consistent performance. In this paper, this problem has been addressed and solved in three steps. The first step is multi-objective optimization to find the best set of values of smoothing and threshold constants at different noise levels by considering the objectives of maximization of speech quality, intelligibility, and minimization of mean square error. The second step is the classification of the noisy speech into four SNR levels such as 0 dB, 5 dB, 10 dB, and 15 dB by using appropriate audio features. The values obtained in steps one and two are stored and in the third step, when the unknown noisy speech signal is to be enhanced the best-chosen values of the smoothing and threshold constants are selected for this task. Finally, the performance of the proposed method is evaluated in two different speech datasets. Then, comparative performance and statistical analysis are carried out using six other standard SE algorithms and it is demonstrated that the proposed approach provides superior performance than others.																	1864-5909	1864-5917															10.1007/s12065-020-00446-0		JUL 2020											
J								Bagged tree based anti-islanding scheme for multi-DG microgrids	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Anti-islanding scheme; Bagged tree classifier; Dual use line relays (DULR); Intelligent electronic devices (IEDs); Microgrid	INVERTER; PROTECTION	Microgrids are becoming a prevalent part of the grid due to its' numerous advantages on energy management system. Microgrids are perceived as the gate way for systematic inclusion of multiple renewable based distributed generations. One of the key concerns for these types of microgrids is detection of absence of the grid. This paper demonstrates a dual use line relay (DULR) based passive detection technique which uses an intelligent electronic device (IED) platform to confirm the islanding condition. Bagged tree ensemble classifier based on synchrophasor measurements has been used in IED as a decision making algorithm. Assorted range of non-islanding and islanding state of affairs have been considered to train the classifier. The measurements obtained from DULR can be used for further analysis at any level of the smart-grid communication hierarchy. The proposed scheme has been seen as fast, effective, economic, and having zero non-detection zone, compared to some of the existing techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02324-0		JUL 2020											
J								A novel scheduling approach to improve the energy efficiency in cloud computing data centers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy efficiency; Cloud data center; VM scheduling; DVFS		Cloud computing is the combination of grid computing, distributed computing and utility computing. Cloud computing provides various types of services (servers and storage facility) in the on demand basis. The main goal of the cloud computing is to preserve and organize the very huge data center or data forms. The data forms are composed of thousands of servers that absorb the giant (ample) amount of electricity in the word of energy. The decreasing the energy consumption in datacenter is the major challenge in cloud computing now a day. This research article is going to address the problem of high energy consumption at datacenter. Concentrate on virtual machine scheduling in cloud datacenter with Dynamic Voltage Frequency Scaling (DVFS) approach. We have combined shortest job first and Round Robin algorithms with Vibrant Quantum. This combination of algorithm is considered as shortest round vibrant queue (SRVQ) algorithm. SRVQ reduces the waiting time of the scheduling process and minimize the starvation. The DVFS and SRVQ worked together and produced fruitful results in the final experiments. This work reduced the server's energy consumption in the cloud data center. In the final results, our proposed framework exhibits 45% of energy efficiency compare to other previously proposed algorithms. 33% of QoS performance were enhanced by our framework.																	1868-5137	1868-5145															10.1007/s12652-020-02283-6		JUL 2020											
J								Graph based feature extraction and hybrid classification approach for facial expression recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Facial expression; Emotion recognition; Weighted visibility graph; Self-organizing map based neural network		In the current trends, face recognition has a remarkable attraction towards favorable and inquiry of an image. Several algorithms are utilized for recognizing the facial expressions, but they lack in the issues like inaccurate recognition of facial expression. To overcome these issues, a Graph-based Feature Extraction and Hybrid Classification Approach (GFE-HCA) is proposed for recognizing the facial expressions. The main motive of this work is to recognize human emotions in an effective manner. Initially, the face image is identified using the Viola-Jones algorithm. Subsequently, the facial parts such as right eye, left eye, nose and mouth are extracted from the detected facial image. The edge-based invariant transform feature is utilized to extract the features from the extracted facial parts. From this edge-based invariant features, the dimensions are optimized using Weighted Visibility Graph which produces the graph-based features. Also, the shape appearance-based features from the facial parts are extracted. From these extracted features, facial expressions are recognized and classified using a Self-Organizing Map based Neural Network Classifier. The performance of this GFE-HCA approach is evaluated and compared with the existing techniques, and the superiority of the proposed approach is proved with its increased recognition rate.																	1868-5137	1868-5145															10.1007/s12652-020-02311-5		JUL 2020											
J								Integer multiplication ranking method for cloud services selection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Analytic hierarchy process; Cloud service selection; Ranking methods	DECISION; CRITERIA; FRAMEWORK; BROKER	Several popular cloud service providers such as Microsoft, Amazon, Google and others are competing to provide fast, reliable and efficient cloud services to the customers. Cloud computing has changed the cyber world by offering infrastructure and application services with rental charges. The decision making problem arises when selection has to be done amongst a wide range of available cloud service providers. Analytical Hierarchy Process (AHP) is a decision making method for selection amongst different options with multiple criteria. Different types of ranking methods are used in AHP which have great influence on the priorities assigned to these options. In this work, we propose an efficient and simplified ranking method named as Integer Multiplication (IM) to select the best Cloud service among the available services as per the user's preference for different criteria and past experiences with the cloud service providers. The proposed method helps in making quick decisions as per customer's preferences. We have demonstrated the applicability of this mechanism using a case study and also through simulated results. The results verify that the efficient and simplified proposed ranking method-"Integer Multiplication" gives accurate results for cloud service selection with reduced calculations and hence time saving in cloud service selection.																	1868-5137	1868-5145															10.1007/s12652-020-02298-z		JUL 2020											
J								Human-autonomous devices for weak signal detection method based on multimedia chaos theory	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Humanized computing; Human-autonomous devices; Stochastic resonance; Weak signal detection	MULTIVARIATE-ANALYSIS; EXTRACTION; INFORMATION	The detection of weak signals has been widely used in communication, radar and other fields. The detection of weak signals in the background of strong noise is an important research hotspot of modern information theory, and prompts people to explore and study new theories and new methods of weak signal detection. The device takes Holmes-type Duffing mapping as the research object, and uses the Lyapunov exponent as the criterion for chaos identification. The chaotic critical value of the equation is changed from chaotic state to periodic state. Whether it contains the detection algorithm of the target signal, the signal detection for the unknown frequency and the Holmes-type Duffing system is improved by the sliding mode variable structure control method in the control theory. The simulation results show that the improved chaotic Duffing system can effectively suppress the noise and detect the frequency of the weak signal through the power spectrum of the system.																	1868-5137	1868-5145															10.1007/s12652-020-02270-x		JUL 2020											
J								GAMA: Graph Attention Multi-agent reinforcement learning algorithm for cooperation	APPLIED INTELLIGENCE										Multi-agent; Reinforcement learning; Graph network; Attention mechanism		Multi-agent reinforcement learning (MARL) is an important way to realize multi-agent cooperation. But there are still many challenges, including the scalability and the uncertainty of the environment that limit its application. In this paper, we explored to solve those problems through the graph network and the attention mechanism. Finally we succeeded in extending the existing algorithm and obtaining a new algorithm called GAMA. Specifically through the graph network, we made the environment information shared among agents. Meanwhile, the unimportant information was filtered out with the help of the attention mechanism, which helped to improve the communication efficiency. As a result, GAMA obtained the highest mean episode rewards compared to the baselines as well as excellent scalability. The reason why we choose the graph network is that understanding the relationship among agents plays a key role in solving multi-agent problems. And the graph network is very suitable for relational induction bias. Through the integration with the attention mechanism, it was shown that agents could figure out their relationship and focus on the influential environment factors in our experiment.																	0924-669X	1573-7497															10.1007/s10489-020-01755-8		JUL 2020											
J								Multi-objective decomposition optimization algorithm based on adaptive weight vector and matching strategy	APPLIED INTELLIGENCE										Discontinuous; Adaptive; Weight vector; Matching mechanism; External archive	PERFORMANCE	Multi-objective evolutionary algorithm based on decomposition (MOEA/D) uses pre-set weight vector and random matching mechanism between sub-problems and individuals, which makes the algorithm simple and efficient. However, when solving the problem of discontinuous Pareto Front, this will lead to not only the decline of the diversity of population, but also the degradation of the performance of solution set. To solve these problems, a multi-objective decomposition optimization algorithm based on adaptive weight vector and matching strategy (MOEA/D-AVM) is proposed in the article. Firstly, the algorithm finds the invalid sub-problems in the discontinuous region, and then updates these sub-problems according to the current evolutionary stage. It can reduce the possibility that the invalid sub-problems mislead evolutionary process. Secondly, the matching mechanism is established according to the value of penalty-based boundary intersection (PBI) and the Euclidean distance between the sub-problems and individuals. This mechanism can enhance the relationship between individuals and sub-problems. Finally, individuals who perform well in the neighborhood replacement operation are saved in the external archive. It can improve the diversity of the optimal solution set obtained by the algorithm. The proposed algorithm is compared with other related algorithms in the standard test problem. The result shows that the solution set obtained by MOEA/D-AVM not only can better cover the Pareto Front, but also has a competitive performance in solving the problem of discontinuous Pareto Front.																	0924-669X	1573-7497															10.1007/s10489-020-01771-8		JUL 2020											
J								A multi-objective evolutionary approach for the nonlinear scale-free level problem	APPLIED INTELLIGENCE										Videogame level design; Procedural content generation; Multi-objective optimization; Scale-free networks; Lock-and-key puzzle	GENERATION; ALGORITHM	Procedural Content Generation in games aims to produce replayable levels that might adapt to the player or designer preferences. One of the most popular approaches adopted for search-based level generation is the use of Evolutionary Algorithms. This paper addresses the context of dungeon generation and divide the problem in two steps: the first consists of generating the physical level, while the second is to create the puzzle. Although there are several approaches in the related literature for generating levels and puzzles, few are successful at generating unpredictable or diverse levels. The objective of this work is to propose a Multi-objective Evolutionary approach for generating the level and creating the puzzle providing a wide range of unpredictable and diverse solutions with different level dimensions. To achieve this goal, the level is modeled as scale-free topology with a nonlinear resolution. This model avoids the generation of linear, repetitive and grid-like levels, giving the algorithm additional freedom to explore the search space for diverse solutions. Four classical well-known evolutionary algorithms (SPEA2, PAES, NSGA-II and MOCell) were applied to both sub-problems. To analyze the results, we consider a set of quality indicators covering both convergence and diversity. Our results indicated that, the proposed multi-objective formulation was able to provide a wide range of satisfactory and diverse solutions with different level dimensions, independently of the algorithm used. Thus, the main contribution of this work is that level designers can choose between different solutions, based on solution properties and/or designer priorities.																	0924-669X	1573-7497															10.1007/s10489-020-01788-z		JUL 2020											
J								Mobile Apps as Personal Assistant Agents: the JaCa-Android Framework for programming Agents-based applications on mobile devices	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Agents; BDI; Android; JaCaMo; Personal agents	INFORMATION; MIDDLEWARE; PLATFORM	A relevant application domain for agent-based software is given by mobile and wearable applications. In this context, the impressive progress of technologies in the last decade makes it possible to explore the use of agent-oriented programming languages and frameworks based on cognitive architectures, such as the Belief-Desire-Intention (BDI) one. Accordingly, in this paper we provide a comprehensive description of the JaCa-Android approach, a framework based on the JaCaMo platform that allows for designing and programming smart mobile apps using cognitive agents based on the BDI architecture and the Agents & Artifacts environment conceptual model. In these years, the framework has been applied in real-world projects and application domains, and extended and evolved accordingly. The aim of the paper is to report our experience about designing and programming mobile appsas personal assistant agents, as well as to discuss in detail the architecture of the framework.																	1387-2532	1573-7454				JUL 14	2020	34	2							48	10.1007/s10458-020-09474-7													
J								Geolocalization with aerial image sequence for UAVs	AUTONOMOUS ROBOTS										Geolocalization; Road map mosaicking; 2-Road-intersections-tuple; GIS	EXTRACTION	The estimation of geolocation for aerial images is significant for tasks like map creating, or automatic navigation for unmanned aerial vehicles (UAV5). We propose a novel geolocalization method for the UAVs using only aerial images and reference road map. The corresponding road maps of the aerial images are firstly merged into a whole mosaic image using our newly-designed aerial image mosaicking algorithm, where the relative homography transformations between road images are firstly estimated using keypoints tracking in RGB aerial images, and then further refined with registration between detected roads. The geolocalization of the aerial mosaic image is then taken as the problem of registering observed roads in the aerial images to the reference road map under the homography transformation. The registration problem is solved with our fast search algorithm based on a novel projective-invariant feature, which consists of two road intersections augmented with their tangents. Experiments demonstrate that the proposed method can localize the aerial image sequence over an area larger than 1000 km(2) within a few seconds.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1199	1215		10.1007/s10514-020-09927-8		JUL 2020											
J								A unified kinematics modeling, optimization and control of universal robots: from serial and parallel manipulators to walking, rolling and hybrid robots	AUTONOMOUS ROBOTS										Unified kinematics; Hybrid robots; Optimization and control		The paper develops a unified kinematics modeling, optimization and control that is applicable to a wide range of autonomous and non-autonomous robots. These include hybrid robots that combine two or more modes of operations, such as combination of walking and rolling, or rolling and manipulation, as well as parallel robots in various configurations. The equations of motion are derived in compact forms that embed an optimization criterion. These equations are used to obtain various useful forms of the robot kinematics such as recursive, body and limb-end kinematic forms. Using the modeling, actuation and control equations are derived that ensure traversing a desired path while maintaining balanced operations and tip-over avoidance. Various simulation results are provided for a hybrid rolling-walking robot, which demonstrate the capabilities and effectiveness of the developed methodologies.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1233	1248		10.1007/s10514-020-09929-6		JUL 2020											
J								An XGBoost-based casualty prediction method for terrorist attacks	COMPLEX & INTELLIGENT SYSTEMS										Terrorist attack; Prediction; Extreme gradient boosting; Feature selection; Area under curve	HYPER-PARAMETER OPTIMIZATION; FEATURE-SELECTION; FUTURE PROBABILITIES; RANDOM FORESTS; MODEL; CLASSIFIERS; DEMAND	Terrorist attacks have been becoming one of the severe threats to national public security and world peace. Ascertaining whether the behaviors of terrorist attacks will threaten the lives of innocent people is vital in dealing with terrorist attacks, which has a profound impact on the resource optimization configuration. For this purpose, we propose an XGBoost-based casualty prediction algorithm, namely RP-GA-XGBoost, to predict whether terrorist attacks will cause the casualties of innocent civilians. In the proposed RP-GA-XGBoost algorithm, a novel method that incorporates random forest (RF) and principal component analysis (PCA) is devised for selecting features, and a genetic algorithm is used to tune the hyperparameters of XGBoost. The proposed method is evaluated on the public dataset (Global Terrorism Database, GTD) and the terrorist attack dataset in China. Experimental results demonstrate that the proposed algorithm achieves area under curve (AUC) of 87.00%, and accuracy of 86.33% for the public dataset, and sensitivity of 94.00%, AUC of 94.90% for the terrorist attack dataset in China, which proves the superiority and higher generalization ability of the proposed algorithm. Our study, to the best of our knowledge, is the first to apply machine learning in the management of terrorist attacks, which can provide early warning and decision support information for terrorist attack management.																	2199-4536	2198-6053				OCT	2020	6	3					721	740		10.1007/s40747-020-00173-0		JUL 2020											
J								RoCGAN: Robust Conditional GAN	INTERNATIONAL JOURNAL OF COMPUTER VISION										Conditional GAN; Unsupervised learning; Autoencoder; Robust regression; Super-resolution; Adversarial attacks; Cross-noise experiments		Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, calledRoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli).																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2665	2683		10.1007/s11263-020-01348-5		JUL 2020											
J								Fine-Grained Complexity of Safety Verification	JOURNAL OF AUTOMATED REASONING										Parameterized verification; Parameterized complexity; Fine-grained complexity; Safety verification	MODEL-CHECKING; CONCURRENT PROGRAMS	We study the fine-grained complexity of Leader Contributor Reachability (LCR) and Bounded-Stage Reachability (BSR), two variants of the safety verification problem for shared memory concurrent programs. For both problems, the memory is a single variable over a finite data domain. Our contributions are new verification algorithms and lower bounds. The latter are based on the Exponential Time Hypothesis (ETH), the problem Set Cover, and cross-compositions. LCR is the question whether a designated leader thread can reach an unsafe state when interacting with a certain number of equal contributor threads. We suggest two parameterizations: (1) By the size of the data domain D and the size of the leader L, and (2) by the size of the contributors C. We present algorithms for both cases. The key techniques are compact witnesses and dynamic programming. The algorithms run in O* ((L . (D + 1))(L.D) . D-D ) and O* (2(C)) time, showing that both parameterizations are fixed-parameter tractable. We complement the upper bounds by (matching) lower bounds based on ETH and Set Cover. Moreover, we prove the absence of polynomial kernels. For BSR, we consider programs involving t different threads. We restrict the analysis to computations where the write permission changes s times between the threads. BSR asks whether a given configuration is reachable via such an s-stage computation. When parameterized by P, the maximum size of a thread, and t, the interesting observation is that the problem has a large number of difficult instances. Formally, we show that there is no polynomial kernel, no compression algorithm that reduces the size of the data domain D or the number of stages s to a polynomial dependence on P and t. This indicates that symbolic methods may be harder to find for this problem.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1419	1444		10.1007/s10817-020-09572-x		JUL 2020											
J								Designing an adaptive production control system using reinforcement learning	JOURNAL OF INTELLIGENT MANUFACTURING										Reinforcement learning; Production control; Adaptivity; Semiconductor industry	CYBER-PHYSICAL SYSTEMS; DISPATCHING RULES; POLICY	Modern production systems face enormous challenges due to rising customer requirements resulting in complex production systems. The operational efficiency in the competitive industry is ensured by an adequate production control system that manages all operations in order to optimize key performance indicators. Currently, control systems are mostly based on static and model-based heuristics, requiring significant human domain knowledge and, hence, do not match the dynamic environment of manufacturing companies. Data-driven reinforcement learning (RL) showed compelling results in applications such as board and computer games as well as first production applications. This paper addresses the design of RL to create an adaptive production control system by the real-world example of order dispatching in a complex job shop. As RL algorithms are "black box" approaches, they inherently prohibit a comprehensive understanding. Furthermore, the experience with advanced RL algorithms is still limited to single successful applications, which limits the transferability of results. In this paper, we examine the performance of the state, action, and reward function RL design. When analyzing the results, we identify robust RL designs. This makes RL an advantageous control system for highly dynamic and complex production systems, mainly when domain knowledge is limited.																	0956-5515	1572-8145															10.1007/s10845-020-01612-y		JUL 2020											
J								Testing that a Local Optimum of the Likelihood is Globally Optimum Using Reparameterized Embeddings Applications to Wavefront Sensing	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Inverse problems; Parameter estimation; Maximum likelihood; Global optimization; Local maxima	MAXIMUM-LIKELIHOOD; NONCONVEX REGULARIZATION; PARAMETER-EXPANSION; PHASE RETRIEVAL; LEAST-SQUARES; MINIMIZERS; STABILITY; RATIO	Many mathematical imaging problems are posed as non-convex optimization problems. When numerically tractable global optimization procedures are not available, one is often interested in testing ex post facto whether or not a locally convergent algorithm has found the globally optimal solution. When the problem is formulated in terms of maximizing the likelihood function under a statistical model for the measurements, one can construct a statistical test that a local maximum is in fact the global maximum. A one-sided test is proposed for the case that the statistical model is a member of the generalized location family of probability distributions, a condition often satisfied in imaging and other inverse problems. We propose a general method for improving the accuracy of the test by reparameterizing the likelihood function to embed its domain into a higher-dimensional parameter space. We show that the proposed global maximum testing method results in improved accuracy and reduced computation for a physically motivated joint-inverse problem arising in camera-blur estimation.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		858	871		10.1007/s10851-020-00979-0		JUL 2020											
J								Efficient use of recent progresses for Real-time Semantic segmentation	MACHINE VISION AND APPLICATIONS										Semantic segmentation; Real time; Deep learning; Convolutional neural network		Different approaches were proposed to design deep CNNs for semantic segmentation. Usually, they are built upon an encoder- decoder architecture and require computationally expensive operations on high-resolution activation maps. Since for real-time segmentation the costs are critical, efficient approaches compromise spatial information to achieve real-time segmentation but with a considerable drop in accuracy. We introduce a new module based on depthwise separable, shuffled and grouped convolutions that optimize up-sampling operations by using a sizeable receptive field and preserving spatial information. Then, we designed an efficient network based on dense connectivity to achieve a remarkable trade-off accuracy and speed. We show through set of experiments that even by up-sampling with a lightweight decoder, our applied architecture scores on Cityscape 69.5% Mean IoU with 1024 x 512 inputs and 95.2 FPS on the test set.																	0932-8092	1432-1769				JUL 14	2020	31	6								10.1007/s00138-020-01095-0													
J								Some generalizations of p-semisimple BCI algebras and groups	SOFT COMPUTING										RM; aRM; aRM**; BZ; BCI algebra; Moon; Goop; p-semisimplicity	PROPERTY	We introduce and investigate the strong p-semisimple property for some generalizations of BCI algebras. For BCI algebras, the strong p-semisimple property is equivalent to the p-semisimple property. We describe the connections of strongly p-semisimple algebras and various generalizations of groups (such as, for example, involutive moons and goops). Moreover, we present some examples of proper strongly p-semisimple algebras.																	1432-7643	1433-7479				SEP	2020	24	17					12781	12787		10.1007/s00500-020-05168-0		JUL 2020											
J								A diversity introduction strategy based on change intensity for evolutionary dynamic multiobjective optimization	SOFT COMPUTING										Dynamic multiobjective optimization; Diversity introduction strategy; Inverse model; Evolutionary algorithm	PREDICTION STRATEGY; ALGORITHM; ENVIRONMENTS	Many real-world problems can be modeled as dynamic multiobjective optimization ones with several competing objectives, which requires an optimization algorithm to track the movement of Pareto front over time. This paper proposes a novel dynamic diversity introduction strategy based on change intensity to improve the performance of dynamic multiobjective optimization based on evolutionary algorithm (DMOEA). In this proposed method, the information generated during evolution is recorded in preparation for evaluating the change intensity. Then, by comparing the evaluated intensity with the inherent intensity, the introduction ratio can be determined by that greater one. Two diversity introduction strategies are utilized to keep the balance between convergence and diversity when environmental change is detected. An improved inverse modeling is used for those drastic changes, while partial solutions random initialization is utilized for these mild ones. We compare the proposed algorithm with four existing DMOEAs on a variety of test instances. The experimental results show that the proposed algorithm exhibits better search performance.																	1432-7643	1433-7479				SEP	2020	24	17					12789	12799		10.1007/s00500-020-05175-1		JUL 2020											
J								A new approach for ergonomic risk assessment integrating KEMIRA, best-worst and MCDM methods	SOFT COMPUTING										Ergonomic risk assessment; Best-worst method; KEMIRA-M; MOORA; MOOSRA; COPRAS; TPOP	LOW-BACK-PAIN; PERFORMANCE EVALUATION; SELECTION; SUSTAINABILITY; TRANSITION; EXPOSURE; EQUATION; WEIGHTS; DESIGN; TASKS	In this study, a new three-phase ergonomic risk assessment approach was proposed for manual lifting tasks to determine which worker has the highest ergonomic risk level considering two criteria sets as lifting-related criteria and human-related criteria. In the first phase, Modified Kemeny Median Indicator Ranks Accordance (KEMIRA-M) and a novel two-dimensional best-worst method (BWM) integration were proposed for weighting ergonomic risk criteria in two sets. In this way, weighting procedure of KEMIRA-M was advanced by the proposed two-dimensional BWM in a consistent manner and subjectivity in determining the best and the worst criteria in traditional BMW was prevented by using KEMIRA-M. Thus, the weaknesses of both methods have been developed. In the second phase, the rankings of workers were determined via utilizing multi-objective optimization on the basis of simple ratio analysis, multi-objective optimization by ratio analysis (MOORA) ratio, MOORA reference point and complex proportional assessment to see how worker rankings differ despite using the same advanced weighting approach based on KEMIRA-M and two-dimensional BWM integration. Finally, to aggregate these different ranking results, technique of precise order preference was applied. In this way, different viewpoints of each ranking approach can be reflected on a single worker's priority. The applicability of the proposed ergonomic risk assessment approach was demonstrated with a real application in tube manufacturing.																	1432-7643	1433-7479				OCT	2020	24	19					15093	15110		10.1007/s00500-020-05143-9		JUL 2020											
J								An unsupervised detection method for shilling attacks based on deep learning and community detection	SOFT COMPUTING										Shilling attack; Unsupervised detection; Deep learning; Community detection	NETWORKS	In the detection methods for shilling attacks, the supervised methods require labeled samples to train the classifiers. Due to lack of the labeled sample profiles in real scenarios, the applicability of supervised detection method is restricted. For unsupervised methods, the prior knowledge is often required to guarantee the detection accuracy. To break the traditional limitations, we present an unsupervised method to detect various shilling profiles from reconstructed user-user graph based on deep learning and community detection. Firstly, we construct the user-user graph, whose edge weight is calculated by the similarity of user's behaviors. Secondly, the stacked denoising autoencoders are used to extract the robust graph features at different scales with different corruption rates. Based on the features at different scales, we generate multiple clustering results and reconstruct the user-user graph by evidence accumulation method. Thirdly, the community detection is carried out by using the persistence optimization algorithm. Extensive experiments on two datasets illustrate that our proposed method has better performance than some baseline detectors for detecting the simulated attacks and actual attacks.																	1432-7643	1433-7479															10.1007/s00500-020-05162-6		JUL 2020											
J								Resilient back-propagation approach in small-world feed-forward neural network topology based on Newman-Watts algorithm	NEURAL COMPUTING & APPLICATIONS										Resilient back-propagation; Small-world network; Feed-forward artificial neural network; Newman-Watts algorithm	PERFORMANCE; DIAGNOSIS; IMPACT	The scientific researches are focused on network topologies and training algorithms fields because they reduce overfitting problem in artificial neural networks. In this context, we showed in our previous work that Newman-Watts small-world feed-forward artificial neural networks present better classification and prediction performance than conventional feed-forward artificial neural networks. In this study, we investigate the effects of the Resilient back-propagation algorithm on SW network topology and propose a Resilient Newman-Watts small-world feed-forward artificial neural network model by assuming fixed initial topological conditions. We find that Resilient small-world network further reduces overfitting and further increases the network performance when compared to the conventional feed-forward artificial neural networks. Furthermore, it is shown that the proposed network model does not increase the algorithmic complexity as per other models. The obtained results imply that the proposed model can contribute to the solving of overfitting problem encountered in both deep neural networks and conventional artificial neural networks.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16279	16289		10.1007/s00521-020-05161-6		JUL 2020											
J								A dynamic scheduling mechanism of part feeding for mixed-model assembly lines based on the modified neural network and knowledge base	SOFT COMPUTING										Dynamic scheduling; Part feeding; Tow train; Mixed-model assembly lines; General regression neural network; Knowledge base	OPTIMIZATION ALGORITHM; SEMICONDUCTOR; VEHICLES; SYSTEM	Inspired by the manufacturing costs proportion of part feeding in automotive mixed-model assembly lines (MMALs) being up to 20-35%, this paper takes the dynamic scheduling of part feeding for automotive MMALs as a crucial and complex problem. Therefore, a dynamic scheduling mechanism basing on the knowledge base (KB) and fruit fly optimization algorithm (FOA) with variable step sizes and logistic chaos (VSCFOA)-enhanced general regression neural network (VSCFOA-GRNN) is proposed to tackle the real-time part feeding scheduling problem of tow trains under the dynamic manufacturing system. A mathematical model is developed to illustrate the problem, where the throughput of the assembly line and the material delivery distance are determined as components of the objective function. Subsequently, samples of the MMAL are generated by the plant simulation software and used to train the VSCFOA-GRNN model off-line. Afterward, the trained model and KB are adopted in the real-time scheduling process to determine the optimal scheduling rule combination. Finally, the effectiveness, feasibility and accuracy of the novel scheduling mechanism are validated by computational results, especially in dynamic scheduling processes. It can cope well with changes in the dynamic environment, thus effectively realizing the higher productivity of assembly lines and better system performance.																	1432-7643	1433-7479															10.1007/s00500-020-05141-x		JUL 2020											
J								Performance enhancement of safety message communication via designing dynamic power control mechanisms in vehicular ad hoc networks	COMPUTATIONAL INTELLIGENCE										802; 11p; ad hoc network; collision avoidance application; cross-layer model; DSRC; safety application; VANET	WIRELESS ACCESS; DSRC; STANDARDS	In vehicular ad hoc networks (VANETs), transmission power is a key factor in several performance measures, such as throughput, delay, and energy efficiency. Vehicle mobility in VANETs creates a highly dynamic topology that leads to a nontrivial task of maintaining connectivity due to rapid topology changes. Therefore, using fixed transmission power adversely affects VANET connectivity and leads to network performance degradation. New cross-layer power control algorithms called (BL-TPC 802.11MAC and DTPC 802.11 MAC) are designed, modeled, and evaluated in this paper. The designed algorithms can be deployed in smart cities, highway, and urban city roads. The designed algorithms improve VANET performance by adapting transmission power dynamically to improve network connectivity. The power adaptation is based on inspecting some network parameters, such as node density, network load, and media access control (MAC) queue state, and then deciding on the required power level. Obtained results indicate that the designed power control algorithm outperforms the traditional 802.11p MAC considering the number of received safety messages, network connectivity, network throughput, and the number of dropped safety messages. Consequently, improving network performance means enhancing the safety of vehicle drivers in smart cities, highway, and urban city.																	0824-7935	1467-8640															10.1111/coin.12367		JUL 2020											
J								A hybrid grey wolf optimizer for solving the product knapsack problem	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Product knapsack problem; Grey wolf optimizer; Transfer function; Repair and optimization	BEE COLONY ALGORITHM; EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; SELECTION	The product knapsack problem (PKP) is a new variation of the knapsack problem which arises in social choice computation. Although some deterministic algorithms have been reported to handle small-scale problems, the solution to the middle and large-scale problems is still lack of progress. For efficiently solving this problem, a new ideal of solving PKP by evolutionary algorithms is proposed in the paper. Firstly, an accelerated binary grey wolf optimizer (ABGWO) is proposed by modifying the transfer function, in which the original sigmoid function is replaced by a step function to reduce the computation and accelerate convergence. Secondly, a two-phase repair and optimize algorithm based on greedy strategy is proposed, which is used to handle the infeasible solutions when using evolutionary algorithm to solve PKP. In order to validate the performance of ABGWO, we use it to solve four kinds of PKP instances and compare with the performance of genetic algorithms, discrete particle swarm optimization, discrete differential evolution, and two existed binary grey wolf optimizers. Comparison results show that ABGWO is superior to others in terms of solution quality, robustness and convergence speed, and it is most suitable for solving PKP.																	1868-8071	1868-808X															10.1007/s13042-020-01165-9		JUL 2020											
J								Discrete natural neighbour interpolation with uncertainty using cross-validation error-distance fields	PEERJ COMPUTER SCIENCE										Convex hull; Digital; Neighbor; Python; Raster; Sibson; Virtual geography experiments; Voronoi diagram	PYTHON	Interpolation techniques provide a method to convert point data of a geographic phenomenon into a continuous field estimate of that phenomenon, and have become a fundamental geocomputational technique of spatial and geographical analysts. Natural neighbour interpolation is one method of interpolation that has several useful properties: it is an exact interpolator, it creates a smooth surface free of any discontinuities, it is a local method, is spatially adaptive, requires no statistical assumptions, can be applied to small datasets, and is parameter free. However, as with any interpolation method, there will be uncertainty in how well the interpolated field values reflect actual phenomenon values. Using a method based on natural neighbour distance based rates of error calculated for data points via cross-validation, a cross-validation error-distance field can be produced to associate uncertainty with the interpolation. Virtual geography experiments demonstrate that given an appropriate number of data points and spatial-autocorrelation of the phenomenon being interpolated, the natural neighbour interpolation and cross-validation error-distance fields provide reliable estimates of value and error within the convex hull of the data points. While this method does not replace the need for analysts to use sound judgement in their interpolations, for those researchers for whom natural neighbour interpolation is the best interpolation option the method presented provides a way to assess the uncertainty associated with natural neighbour interpolations.																	2376-5992					JUL 13	2020									e282	10.7717/peerj-cs.282													
J								A novel fully convolutional network for visual saliency prediction	PEERJ COMPUTER SCIENCE										Deep learning; Convolutional neural networks; Fully Convolutional Network; Semantic Segmentation; Encoder-decoder architecture; Human eye fixation	ATTENTION; MODEL	A human Visual System (HVS) has the ability to pay visual attention, which is one of the many functions of the HVS. Despite the many advancements being made in visual saliency prediction, there continues to be room for improvement. Deep learning has recently been used to deal with this task. This study proposes a novel deep learning model based on a Fully Convolutional Network (FCN) architecture. The proposed model is trained in an end-to-end style and designed to predict visual saliency. The entire proposed model is fully training style from scratch to extract distinguishing features. The proposed model is evaluated using several benchmark datasets, such as MIT300, MIT1003, TORONTO, and DUT-OMRON. The quantitative and qualitative experiment analyses demonstrate that the proposed model achieves superior performance for predicting visual saliency.																	2376-5992					JUL 13	2020									e280	10.7717/peerj-cs.280													
J								Context, design and conveyance of information: ICT-enabled agricultural information services for rural women in Bangladesh	AI & SOCIETY										ICT4D; Information system for development; ICT in agriculture; Context specific information system; Gender and information system		ICT for development projects often focus on integrating social factors in information systems design. A well-designed ICT4D solution must be tailored to the needs of the people who will use them and subsequently, requires an extensive understanding of the context and constraints in people's lives. With an objective to explore how context-specific issues influence the conveyance of appropriate agricultural information to women, this paper uses PROTIC (participatory research and ownership with technology, information and change), a 5-year collaborative project between Monash University and Oxfam, as a case. PROTIC was implemented in three climate vulnerable ecological zones of Bangladesh with an aim to overcome the agricultural information gap for rural women. Based on a study in mixed research method and triangulation, this paper brings evidence on the significance of considering context-specific attributes in designing effective and sustainable information services for marginalized women farmers in developing countries. The paper upfolds some significant recommendations which is expected to contribute to the domain of context-specific information system design in resolving socio-economic problems to pave the way of sustainable development.																	0951-5666	1435-5655															10.1007/s00146-020-01016-9		JUL 2020											
J								ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels	DATA MINING AND KNOWLEDGE DISCOVERY										Scalable; Time series classification; Random; Convolution	STATISTICAL COMPARISONS; CLASSIFIERS	Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets. Additionally, many existing methods focus on a single type of feature such as shape or frequency. Building on the recent success of convolutional neural networks for time series classification, we show that simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods. Using this method, it is possible to train and test a classifier on all 85 'bake off' datasets in the UCR archive in < 2 h, and it is possible to train a classifier on a large dataset of more than one million time series in approximately 1 h.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1454	1495		10.1007/s10618-020-00701-z		JUL 2020											
J								Giza Pyramids Construction: an ancient-inspired metaheuristic algorithm for optimization	EVOLUTIONARY INTELLIGENCE										Metaheuristic; Optimization; Giza Pyramids Construction algorithm; GPC algorithm; Ancient-inspired; High-dimensional tests; Image segmentation; Benchmark test functions	EMPEROR PENGUINS COLONY; HEURISTIC OPTIMIZATION; SEARCH ALGORITHM	Nowadays, many optimization issues around us cannot be solved by precise methods or that cannot be solved in a reasonable time. One way to solve such problems is to use metaheuristic algorithms. Metaheuristic algorithms try to find the best solution out of all possible solutions in the shortest time possible. Speed in convergence, accuracy, and problem-solving ability at high dimensions are characteristics of a good metaheuristic algorithm. This paper presents a new population-based metaheuristic algorithm inspired by a new source of inspiration. This algorithm is called Giza Pyramids Construction (GPC) inspired by the ancient past has the characteristics of a good metaheuristic algorithm to deal with many issues. The ancient-inspired is to observe and reflect on the legacy of the ancient past to understand the optimal methods, technologies, and strategies of that era. The proposed algorithm is controlled by the movements of the workers and pushing the stone blocks on the ramp. This algorithm is compared with five standard and popular metaheuristic algorithms. For this purpose, thirty different and diverse benchmark test functions are utilized. The proposed algorithm is also tested on high-dimensional benchmark test functions and is used as an application in image segmentation. The results show that the proposed algorithm is better than other metaheuristic algorithms and it is successful in solving high-dimensional problems, especially image segmentation.																	1864-5909	1864-5917															10.1007/s12065-020-00451-3		JUL 2020											
J								The implementation to intelligent linkage service over AIoT hierarchical for material flow management	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										AIoT (artificial intelligent of thing); Flow tracking; Intelligent management mechanisms; Machine learning; Materials operation management		A total solution implemented by the techniques based on artificial intelligent of thing (AIoT) architecture, which is called as intelligent linkage devices, proposed in the article. The main implementation contents of this project have some aspects in guidance application, which includes the application of flow tracking technology to improve the efficiency of material flow management. On the other hand, the establishment of spatial distribution tools for chemical substances strengthen the autonomous management ability of factories, and the establishment of chemical substances management platform in colleges and universities to simplify the reporting management mechanism. Therefore, this project has evaluated the appropriate safety management guidelines and intelligent management mechanisms by examining the actual operation of the laboratory and combining mobile devices, labeling, measurement identification and IoT technologies. This total solution is definitely figured out provides with following safe and convenient operation process for materials operation management, and even achieve the aim of simplifying reporting. To the best of author knowledge, the presented device is complete fresh application to the research field of IoT and machine learning.																	1868-5137	1868-5145															10.1007/s12652-020-02320-4		JUL 2020											
J								Synergic deep learning based preoperative metric prediction and patient oriented payment model for total hip arthroplasty	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Finance; Payment model; Healthcare management; Deep learning	STRENGTH; AREA	In recent days, total hip arthroplasty (THP) become a very victorious process which relieve the pain and enhances its functioning. The main motive of the paper is to design and assess a synergic deep learning (SDL) for learning and predicting various metrics like duration of stay (DOS), discharge disposition (DD) and inpatient expenses for THA. The next motive is to develop a patient specific payment (PSP) model reporting the complexity level of the patient. By the use of 15 preoperative parameters from 78,335 THA patients for osteoarthritis from National Inpatient Sample and OME databases, the prediction of DOS, DS and inpatient expenses takes place. A set of two evaluation parameters namely accuracy and receiver operating characteristic curve is used for experimentation. In addition, a predictive uncertainty is employed. All patient refined comorbidity cohort for establishing the PSP model. The presented SDL model exhibited better learning with high trustworthiness, receptiveness, and validity in its prediction outcome. The presented model can be applied for the implementation of PSP model for tiered payments depending upon the case complexities.																	1868-5137	1868-5145															10.1007/s12652-020-02266-7		JUL 2020											
J								TP-PRE: threshold progressive proxy re-encryption, its definitions, construction and applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Proxy re-encryption; Threshold; Progressive; Access control; Fault-tolerance	SCHEME; MULTIHOP	Proxy re-encryption (PRE) is a public-key cryptography primitive that delegates the decryption capabilities of a user (called delegator) to another user (called delegatee) using a re-encryption key. A semi-trusted proxy uses the re-encryption key to transform a ciphertext under the delegator's public key such that it becomes a ciphertext under the delegatee's public key. If instead of "all", a delegator wishes to delegate a subset of his decryption capabilities, there is a need for elevating the level of trust in the re-encrypting proxy. As a result, PRE in multi-proxy scenario has received significant research attention in recent times. In this paper, we introduce a new PRE primitive in a multi-proxy setting called Threshold Progressive Proxy Reencryption (TP-PRE), that involves progressive transformation of ciphertext and results in production of a valid re-encrypted ciphertext if and only if at least t out of the total N distinct proxies perform re-encryption. The way these proxies are selected for re-encryption is significantly different from the existing threshold proxy cryptosystems. The TP-PRE scheme we present does not require prior knowledge about the available proxies. Proxies can take turn in any order to progressively transform the ciphertext and output of tth transformation, or any transformation(s) afterward, is the final re-encrypted ciphertext that can be successfully decrypted by the intended delegatee to obtain the correct underlying plaintext. Unlike conventional threshold cryptosystems, TP-PRE does not have a share combination phase hence it does not require any central dealer. We formally define system model and security notions for TP-PRE. We present a concrete construction for TP-PRE that satisfies indistinguishability under chosen-plaintext attacks (IND-CPA) and formally prove its security. We analyze the performance of our construction by providing theoretical bounds of the solution along with the results of practical implementation.																	1868-5137	1868-5145															10.1007/s12652-020-02285-4		JUL 2020											
J								A Learning-Based Approach to Synthesizing Invariants for Incomplete Verification Engines	JOURNAL OF AUTOMATED REASONING										Software verification; Invariant synthesis; Undecidable theories; Incomplete decision procedures; Machine learning	INFERENCE	We propose a framework for synthesizing inductive invariants for incomplete verification engines, which soundly reduce logical problems in undecidable theories to decidable theories. Our framework is based on the counterexample guided inductive synthesis principle and allows verification engines to communicate non-provability information to guide invariant synthesis. We show precisely how the verification engine can compute such non-provability information and how to build effective learning algorithms when invariants are expressed as Boolean combinations of a fixed set of predicates. Moreover, we evaluate our framework in two verification settings, one in which verification engines need to handle quantified formulas and one in which verification engines have to reason about heap properties expressed in an expressive but undecidable separation logic. Our experiments show that our invariant synthesis framework based on non-provability information can both effectively synthesize inductive invariants and adequately strengthen contracts across a large suite of programs. This work is an extended version of a conference paper titled "Invariant Synthesis for Incomplete Verification Engines".																	0168-7433	1573-0670				OCT	2020	64	7			SI		1523	1552		10.1007/s10817-020-09570-z		JUL 2020											
J								Towards PDE-Based Video Compression with Optimal Masks Prolongated by Optic Flow	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Partial differential equations; Inpainting; Laplace interpolation; Optic flow; Video reconstruction	INTERPOLATION	Lossy image compression methods based on partial differential equations have received much attention in recent years. They may yield high-quality results but rely on the computationally expensive task of finding an optimal selection of data. For the possible extension to video compression, this data selection is a crucial issue. In this context, one could either analyse the video sequence as a whole or perform a frame-by-frame optimisation strategy. Both approaches are prohibitive in terms of memory and run time. In this work, we propose to restrict the expensive computation of optimal data to a single frame and to approximate the optimal reconstruction data for the remaining frames by prolongating it by means of an optic flow field. In this way, we achieve a notable decrease in the computational complexity. As a proof-of-concept, we evaluate the proposed approach for multiple sequences with different characteristics. In doing this, we discuss in detail the influence of possible computational setups. We show that the approach preserves a reasonable quality in the reconstruction and is very robust against errors in the flow field.																	0924-9907	1573-7683															10.1007/s10851-020-00973-6		JUL 2020											
J								Fast contour detection with supervised attention learning	JOURNAL OF REAL-TIME IMAGE PROCESSING										Edge detection; Structural relationship; Attention learning; Real time		Recent advances in deep convolutional neural networks have led to significant success in many computer vision tasks, including edge detection. However, the existing edge detectors neglected the structural relationships among pixels, especially those among contour pixels. Inspired by human perception, this work points out the importance of learning structural relationships and proposes a novel real-time attention edge detection (AED) framework. Firstly, an elaborately designed attention mask is employed to capture the structural relationships among pixels at edges. Secondly, in the decoding phase of our encoder-decoder model, a new module called dense upsampling group convolution is designed to tackle the problem of information loss due to stride downsampling. And then, the detailed structural information can be preserved even it is ever destroyed in the encoding phase. The proposed relationship learning module introduces negligible computation overhead, and as a result, the proposed AED meets the requirement of real-time execution with only 0.65M parameters. With the proposed model, an optimal dataset scaleF-score of 79.5 is obtained on the BSDS500 dataset with an inference speed of 105 frames per second, which is significantly faster than existing methods with comparable accuracy. In addition, a state-of-the-art performance is achieved on the BSDS500 (81.6) and NYU Depth (77.0) datasets when using a heavier model.																	1861-8200	1861-8219															10.1007/s11554-020-00980-1		JUL 2020											
J								Ear tracking via Siamese hierarchical refinement network for local active noise control	JOURNAL OF REAL-TIME IMAGE PROCESSING										Active noise control; Ear tracking; Siamese network; Feature pyramid; Markov decision process		Active noise control (ANC) technology has been applied to reduce unwanted sound in the vehicle cabin. In this paper, a real-time ear tracking system assists ANC performance as the driver's head moves around. For long-term robust ear tracking, an offline-trained ear detector initializes target area. With precise pre-cropped image patches, a Siamese hierarchical refinement network (SHRNet) builds high-fidelity feature map based on Siamese pyramid branch. Hierarchical feature extraction with lateral refinement makes most use of all levels of feature representation. The offline matching network is trained in an augmented dataset from the self-collected in-vehicle ear database and the ear-labeled McGill face video database. Further, Q-learning is capable of learning a decision-making policy for refining tracking strategy to improve efficiency. Extensive experiment results in various scenes based on NVIDIA Jetson TX2 show the tracker performs at a real-time speed while maintaining a robust performance. In particular, the method achieves AUC score of 67.6% with 26 fps on self-collected in-vehicle ear database.																	1861-8200	1861-8219															10.1007/s11554-020-01000-y		JUL 2020											
J								Tactile discrimination of material properties: application to virtual buttons for professional appliances	JOURNAL ON MULTIMODAL USER INTERFACES										Material discrimination; Vibrotactile feedback; Auditory feedback; Virtual button; Touchscreen; Surface compliance perception	IDENTIFICATION; PERCEPTION; NORMALITY; SOUNDS	An experiment is described that tested the possibility to classify wooden, plastic, and metallic objects based on reproduced auditory and vibrotactile stimuli. The results show that recognition rates are considerably above chance level with either unimodal auditory or vibrotactile feedback. Supported by those findings, the possibility to render virtual buttons for professional appliances with different tactile properties was tested. To this end, a touchscreen device was provided with various types of vibrotactile feedback in response to the sensed pressing force and location of a finger. Different virtual buttons designs were tested by user panels who performed a subjective evaluation on perceived tactile properties and materials. In a first implementation, virtual buttons were designed reproducing the vibration recordings of real materials used in the classification experiment: mainly due to hardware limitations of our prototype and the consequent impossibility to render complex vibratory signals, this approach did not prove successful. A second implementation was then optimized for the device capabilities, moreover introducing surface compliance effects and button release cues: the new design led to generally high quality ratings, clear discrimination of different buttons and unambiguous material classification. The lesson learned was that various material and physical properties of virtual buttons can be successfully rendered by characteristic frequency and decay cues if correctly reproduced by the device.																	1783-7677	1783-8738				SEP	2020	14	3			SI		255	269		10.1007/s12193-020-00336-w		JUL 2020											
J								Exploring crossmodal perceptual enhancement and integration in a sequence-reproducing task with cognitive priming	JOURNAL ON MULTIMODAL USER INTERFACES										Crossmodal interaction; Cognitive priming; Crossmodal perception and integration; Sensory-motor performance	CORRESPONDENCES; COLOR; AWARENESS; PITCH	Crossmodal correspondence, a perceptual phenomenon which has been extensively studied in cognitive science, has been shown to play a critical role in people's information processing performance. However, the evidence has been collected mostly based on strictly-controlled stimuli and displayed in a noise-free environment. In real-world interaction scenarios, background noise may blur crossmodal effects that designers intend to leverage. More seriously, it may induce additional crossmodal effects, which can be mutually exclusive to the intended one, leading to unexpected distractions from the task at hand. In this paper, we report two experiments designed to tackle these problems with cognitive priming techniques. The first experiment examined how to enhance the perception of specific crossmodal stimuli, namely pitch-brightness and pitch-elevation stimuli. The second experiment investigated how people perceive and respond to crossmodal stimuli that were mutually exclusive. Results showed that first, people's crossmodal perception was affected by cognitive priming, though the effect varies according to the combination of crossmodal stimuli and the types of priming material. Second, when two crossmodal stimuli are mutually exclusive, priming on only the dominant one (Pitch-elevation) lead to improved performance. These results can help inform future design of multisensory systems by presenting details of how to enhance crossmodal information with cognitive priming.																	1783-7677	1783-8738															10.1007/s12193-020-00326-y		JUL 2020											
J								Binary particle swarm optimization-based T-S fuzzy predictive controller for nonlinear automotive application	NEURAL COMPUTING & APPLICATIONS										Takagi-Sugeno fuzzy system; Evolutionary computing; Binary particle swarm optimization; Constrained optimization; Fuel injection control	VELOCITY ESTIMATION; TRACKING CONTROL; FAULT ESTIMATION; CONTROL-SYSTEMS; LINEAR-SYSTEMS; LIMIT-CYCLES; DESIGN; STABILITY; RATIO	In this paper, a robust evolutionary computing-assisted Takagi-Sugeno fuzzy predictive controller (T-S FPC) has been developed for nonlinear vehicle fuel injection and emission control. To strengthen the performance of T-S FPC, we have applied an enhanced evolutionary computing algorithm named binary particle swarm optimization (BPSO) that achieves optimal control variable by performing minimization of the cost function iteratively, where the cost function signifies the mean square error between reference data and the actual predicted data. To examine the efficacy of the proposed system, a case study was performed for an automotive vehicle to control its fuel injection, throttle angle, and emission control under nonlinear conditions. The simulation results affirmed that the proposed BPSO-based T-S FPC model exhibits optimal performance by achieving target performance with low mean square error between expected functions and prediction outcomes. The efficiency of the proposed BPSO T-S FPC model enables it to be used for online nonlinear control purposes for any type of the vehicle systems.																	0941-0643	1433-3058															10.1007/s00521-020-05132-x		JUL 2020											
J								Wasserstein autoencoders for collaborative filtering	NEURAL COMPUTING & APPLICATIONS										Wasserstein autoencoder; Collaborative filtering; Implicit data	SYSTEM	The recommender systems have long been studied in the literature. The collaborative filtering is one of the most widely adopted recommendation techniques which is usually applied to the explicit data, e.g., rating scores. However, the implicit data, e.g., click data, is believed to be able to discover user's latent preferences. Consequently, a number of research attempts have been made toward this issue. To the best of our knowledge, this paper is the first attempt to adapt the Wasserstein autoencoders to collaborative filtering problem. Particularly, we propose a new loss function by introducing an L-1 regularization term to learn a sparse low-rank representation form to represent latent variables. Then, we carefully design (1) a new cost function to minimize the data reconstruction error, and (2) the appropriate distance metrics for the calculation of KL divergence between the learned distribution of latent variables and the underlying true data distribution. Rigorous experiments are performed on three widely adopted datasets. Both the state-of-the-art approaches, e.g., Mult-VAE and Mult-DAE, and the baseline models are evaluated on these datasets. The promising experimental results demonstrate that the proposed approach is superior to the compared approaches with respect to evaluation criteria Recall@R and NDCG@R.																	0941-0643	1433-3058															10.1007/s00521-020-05117-w		JUL 2020											
J								Soft computing-based fuzzy time series model for dynamic vehicle routing problem	SOFT COMPUTING										Multi-target dynamic vehicle-directing issue with fuzzy time series (MTDV-FTS); Vehicle routing problem (VRP); Fuzzy time series; Vehicle routing; Fuzzy inference; Vehicle capacity; Distance	NEIGHBORHOOD SEARCH; STOCHASTIC SERVICE; ALGORITHM	Utmost models for vehicle steering detailed in the writing accept consistent travel times. Plainly, disregarding the way that the movement time between two areas does not depend just on the separation voyaged, yet on numerous different variables including time, sways the use of the models to genuine issues. In the present research, a multi-target dynamic vehicle-directing issue with fuzzy time series is displayed. In this issue, majority of the work where information is known ahead of time, some setoff ongoing solicitations arrive arbitrarily after some time and the dispatcher does not have any deterministic or probabilistic data on the area and size of them until they arrive. The manuscript utilizes an immediate understanding of the multi-target dynamic vehicle-directing issue with fuzzy time series as a multi-target issue where the required armada measure, generally all out voyaging separation, and hold-up time forced on vehicles are limited, and the general clients' inclinations for administration are boosted. The presentation of the proposed methodology is assessed in various strides on different test issues summed up from a lot of static occasions in the writing. In the initial step, the exhibition of the proposed methodologies is checked in static conditions and after that, different presumptions and improvements are included progressively, and changes are analyzed. Computational tests on informational collections represent the productivity and adequacy of the proposed methodology.																	1432-7643	1433-7479				NOV	2020	24	22					17431	17444		10.1007/s00500-020-05111-3		JUL 2020											
J								Noise modeling of offshore platform using progressive normalized distance from worst-case error for optimal neuron numbers in deep belief network	SOFT COMPUTING										Deep learning; Noise prediction; Marine and offshore industry; Sound pressure level; Structure-borne noise; Airborne noise	DESIGN	Noise prediction is important for crew comfort in an offshore platform such as oil drilling rig. A deep neural network learning on the oil drilling rig is not widely studied. In this paper, a deep belief network (DBN) with the last layer initialized with trained DBN (named DBN-DNN) is used to model the sound pressure level (SPL) in the compartments of the oil drilling rig. The method finds an optimal number of the hidden neurons in restricted Boltzmann machine by using a normalized Euclidean distance from the worst possible error for each hidden layer progressively. The dataset used for experimental results is obtained via vibroacoustics simulation software such as VA-One and actual site measurements. The results show that output parameters such as spatial SPL, average spatial SPL, structure-borne SPL and airborne SPL improve the testing root mean square error to around 20% as compared to randomly assigning the number of neurons for each hidden layer. The testing RMSE in the output parameters has improved when compared with a multi-layer perceptron, sparse autoencoder, Softmax, self-taught learning and extreme learning machine.																	1432-7643	1433-7479															10.1007/s00500-020-05163-5		JUL 2020											
J								Apple fruit sorting using novel thresholding and area calculation algorithms	SOFT COMPUTING										Threshold; Apple sorting; Naive Bayesian; Apple defect; Defective area	CITRUS-FRUITS; IMAGE; QUALITY; IDENTIFICATION; DEFECTS; SYSTEM	The speed and capacity (kg/hr) for sorting apples are two important factors that have to be taken care of post-harvest. To increase the speed and capacity of apples being sorted, a new algorithm for segmenting the region of interest from an apple fruit image and to find the percentage of defective area after sorting process has been proposed in this paper. First, the region of interest from the acquired color image of an apple fruit was segmented using the proposed global thresholding algorithm GTA. Features were extracted from the segmented image using the coefficients obtained by applying wavelet transformation using the Haar filter. Further, the Naive Bayesian classifier was applied to sort apples as defective and sound. Second, the defective area of the defective sample was segmented using the K-means algorithm and median filter. The defective area in percentage was calculated using the proposed diseased area calculation algorithm DACA to decide about the acceptance of that defective sample. The performance of the proposed GTA was compared with Otsu's and Kapur's thresholding algorithms. With less segmentation time, it was found that the Naive Bayesian classification using GTA gave a 96.67% accuracy rate than Otsu's with 65% and Kapur's with 93.33% accuracy rate. The execution time of the proposed DACA was less when compared to the bwarea function in MATLAB.																	1432-7643	1433-7479															10.1007/s00500-020-05158-2		JUL 2020											
J								The multi-zone location-routing problem with pricing: a flow-based formulation and two heuristic approaches	SOFT COMPUTING										Pricing; Location-routing problem; Heuristic algorithms; Piecewise linearization approximation	COMPETITIVE FACILITY LOCATION; NEIGHBORHOOD SEARCH; PLANT LOCATION; SUPPLY CHAIN; HUB LOCATION; MODEL; INVENTORY; DECISIONS; OPTIMIZATION; ALGORITHM	This paper integrates the concept of pricing into the location-routing problem. The problem consists of a firm trying to optimize its multi-zone network in order to maximize its profit. It divides its big market into some zones and tries to determine the optimal zonal price of the products. It then dispatches its products from its position to each zone. After receiving the products to depots, a vehicle should distribute the products between customers. This matter is compatible with real-world distribution systems such as fruits, textile products, and leather. In other words, the main objectives of the firm are to have the optimal price, vehicle routes, and location of the depot in each zone. Therefore, a flow-based model as a mixed-integer nonlinear programming is proposed to solve the problem. In the light of this nonlinearity, we employ a piecewise linearization approximation method. In addition, to adapt with the large-scale problems, two heuristic algorithms with three combinations of operators in the local search are suggested. In order to evaluate their performance, some test instances and a case study are solved. Based on the results, the performance of the algorithms is compared with each other and the flow-based formulation. The results show the outperformance of the algorithms rather than the flow-based model. Moreover, the results show the linearization approach can efficiently approximate the nonlinear model. Furthermore, the impact of the shipping cost ratio on the depot selection, vehicles' route, and firms' profit is revealed.																	1432-7643	1433-7479															10.1007/s00500-020-05186-y		JUL 2020											
J								Software defect prediction model based on distance metric learning	SOFT COMPUTING										Software defect prediction; Software attributes; Distance metric learning; Cost-sensitive learning; Misprediction cost; Class imbalance of samples	SUPPORT VECTOR REGRESSION; SELECTION; ROBUST	Software defect prediction (SDP) is a very important way for analyzing software quality and reducing development costs. The data during software lifecycle can be used to predict software defect. Currently, many SDP models have been proposed; however, their performance was not always ideal. In many existing prediction models based on machine learning, the distance metric between samples has significant impact on the performance of the SDP model. In addition, most samples are usually class imbalanced. To solve these issues, in this paper, a novel distance metric learning based on cost-sensitive learning (CSL) is proposed for reducing the impact of class imbalance of samples, which is then applied to the large margin distribution machine (LDM) to substitute the traditional kernel function. Further, the improvement and optimization of LDM based on CSL are also studied, and the improved LDM is used as the SDP model, called as CS-ILDM. Subsequently, the proposed CS-ILDM is applied to five publicly available data sets from the NASA Metrics Data Program repository and its performance is compared to other existing SDP models. The experimental results confirm that the proposed CS-ILDM not only has good prediction performance, but also can reduce the misprediction cost and avoid the impact of class imbalance of samples.																	1432-7643	1433-7479															10.1007/s00500-020-05159-1		JUL 2020											
J								Why linear expressions in discounting and in empathy: a symmetry-based explanation	SOFT COMPUTING										Decision making; Empathy; Linearity; Preference relation; Symmetry; Utility	PROBABILITY; DELAY	People's preferences depend not only on the decision-maker's immediate gain, they are also affected by the decision-maker's expectation of future gains. A person's decisions are also affected by possible consequences for others. In decision theory, people's preferences are described by special quantities called utilities. In utility terms, the above phenomena mean that the person's overall utility of an action depends not only on the utility corresponding to the action's immediate consequences for this person, it also depends on utilities corresponding to future consequences and on utilities corresponding to consequences for others. These dependencies reflect discounting of future consequences in comparison with the current ones and to empathy (or lack of) of the person toward others. In general, many formulas involving utility are nonlinear, even formulas describing the dependence of utility on money. However, surprisingly, for discounting and for empathy, linear formulas work very well. In this paper, we show that natural symmetry requirements can explain this linearity.																	1432-7643	1433-7479															10.1007/s00500-020-05153-7		JUL 2020											
J								Weakly supervised learning in neural encoding for the position of the moving finger of a macaque	COGNITIVE COMPUTATION										Neural decoding; Macaque moving finger; Unsupervised learning; Weakly supervised learning; Kalman filter; Expectation maximization		The problem of neural decoding is essential for the realization of a neural interface. In this study, the position of the moving finger of a macaque was directly decoded through the neuron spike signals in the motor cortex, instead of relying on the synergy of the related muscle tissues around the body, also known as neural decoding. Currently, supervised learning is the most commonly employed method for this purpose. However, based on existing technologies, unsupervised learning with regression causes excessive errors. To solve this problem, weakly supervised learning (WSL) was used to correct the predicted position of the moving finger of a macaque in unsupervised training. Then, the corrected finger position was further used to train and accurately fit the weight parameters. We then utilized public data to evaluate the decoding performance of the Kalman filter (KF) and the expectation maximization (EM) algorithms in the WSL model. Unlike in previous methods, in WSL, the only available information is that the finger has moved to four areas in the plane, instead of the actual track value. When compared to the supervised models, the WSL decoding performance only differs by approximately 0.4%. This result improves by 41.3% relative to unsupervised models in the two-dimensional plane. The investigated approach overcomes the instability and inaccuracy of unsupervised learning. What's more, the method in the paper also verified that the unsupervised encoding and decoding technology of neuronal signals is related to the range of external activities, rather than having a priori specific location.																	1866-9956	1866-9964				SEP	2020	12	5					1083	1096		10.1007/s12559-020-09742-4		JUL 2020											
J								Advanced metameric dimension framework for heterogeneous industrial Internet of things	COMPUTATIONAL INTELLIGENCE										advanced machine-metameric dimension; data extraction; heterogeneous industrial IoT; optimization; sensing; trust factor	IOT; AUTHENTICATION; SECURITY	The heterogeneous industrial Internet of things (HetIoT) is a recent area of research that can change both our perception of fundamental informatics and the effectiveness of future machines. The HetIoT is rapidly used in a variety of industries, including healthcare smart cities, smart mobility, smart transportation, and advanced manufacturing. In industrial settings, outdated machines consume an inordinate amount of energy and time to produce effective output. The efficient functioning of the HetIoT machines plays a vital role in the industrial management. To demonstrate efficiency of the functioning, the continuous monitoring of machines becomes mandatory. To describe the efficiency of updated HetIoT machines, this article introduces the concept of advanced machine-metameric dimension (AmD) to analyze efficacy, efficiency, and effectiveness. This article proposes a HetIoT framework to identify the real-time resembling of continuous monitoring of a metameric dimension of the machines. This demonstration of the concept is attempted through the analysis of machine efficiency, efficacy, effectiveness, and so on. The proposed AmD with a HetIoT has achieved higher precision, recall, F1-Score, and higher purity.																	0824-7935	1467-8640															10.1111/coin.12378		JUL 2020											
J								An active safety control method of collision avoidance for intelligent connected vehicle based on driving risk perception	JOURNAL OF INTELLIGENT MANUFACTURING										Vehicle active safety; Collision avoidance; Model predictive control; Driving risk; Intelligent connected vehicle	MODEL-PREDICTIVE CONTROL; TRACKING CONTROL; FLOW; SYSTEM	As the complex driving scenarios bring about an opportunity for application of deep learning in safe driving, artificial intelligence based on deep learning has become a heatedly discussed topic in the field of advanced driving assistance system. This paper focuses on analysing vehicle active safety control of collision avoidance for intelligent connected vehicles (ICVs) in a real driving risk scenario, and driving risk perception is based on the ICV technology. In this way, trajectories of surrounding vehicles can be predicted and tracked in a real-time manner. In this paper, vehicle dynamics based state-space equations conforming to model predictive controllers are set up to primarily explore and identify a safety domain of active collision avoidance. Furthermore, the model predictive controller is also designed and calibrated, thereby implementing the active collision avoidance strategy for vehicles based on the model predictive control method. At last, functional testing is conducted for the proposed active collision avoidance control strategy in a designed complex traffic scenario. The research findings here can effectively improve automatic driving, intelligent transportation efficiency and road traffic safety.																	0956-5515	1572-8145															10.1007/s10845-020-01605-x		JUL 2020											
J								Efficient computation of the convex hull on sets of points stored in a k-tree compact data structure	KNOWLEDGE AND INFORMATION SYSTEMS										Algorithms; Data structures; Spatial databases; Computational geometry; Compact data structures; Convex hull	ALGORITHM	In this paper, we present two algorithms to obtain the convex hull of a set of points that are stored in the compact data structure called k(2)-tree. This problem consists in given a set of points P in the Euclidean space obtaining the smallest convex region (polygon) containing P. Traditional algorithms to compute the convex hull do not scale well for large databases, such as spatial databases, since the data does not reside in main memory. We use the k(2)-tree compact data structure to represent, in main memory, efficiently a binary adjacency matrix representing points over a 2D space. This structure allows an efficient navigation in a compressed form. The experimentations performed over synthetical and real data show that our proposed algorithms are more efficient. In fact they perform over four order of magnitude compared with algorithms with time complexity of O(n log n).																	0219-1377	0219-3116				OCT	2020	62	10					4091	4111		10.1007/s10115-020-01486-9		JUL 2020											
J								A derived least square fast learning network model	APPLIED INTELLIGENCE										Neural network; Extreme learning machine; Fast learning network; Least square method	MACHINE; ELM; CLASSIFICATION; REGRESSION; ALGORITHM	The extreme learning machine (ELM) requires a large number of hidden layer nodes in the training process. Thus, random parameters will exponentially increase and affect network stability. Moreover, the single activation function affects the generalization capability of the network. This paper proposes a derived least square fast learning network (DLSFLN) to solve the aforementioned problems. DLSFLN uses the inheritance of some functions to obtain various activation functions through continuous differentiation of functions. The types of activation functions were increased and the mapping capability of hidden layer neurons was enhanced when the random parameter dimension was maintained. DLSFLN randomly generates the input weights and hidden layer thresholds and uses the least square method to determine the connection weights between the output and the input layers and that between the output and the input nodes. The regression and classification experiments show that DLSFLN has a faster training speed and better training accuracy, generalization capability, and stability compared with other neural network algorithms, such as fast learning network(FLN).																	0924-669X	1573-7497															10.1007/s10489-020-01773-6		JUL 2020											
J								Optimal local dimming based on an improved greedy algorithm	APPLIED INTELLIGENCE										Local dimming; Optimization; Visual quality; IGRA	CONTRAST ENHANCEMENT; BACKLIGHT; OPTIMIZATION; SYSTEM; POWER; ADAPTATION; DISPLAYS	As a new technology appeared in recent years, the local dimming can effectively reduce the power consumption of a display system and improve its display effect. A suitable local dimming algorithm should have efficient performance and can make the displayed images have higher visual quality. However, most of the existing local dimming methods can not have both of the above advantages. In this paper, the local dimming is taken as an optimization problem. On the basis of our previous work which focuses on reducing the image distortion and power consumption, image contrast ratio which is another important factor of visual quality is also considered. To improve the running efficiency of local dimming, the Greedy Algorithm (GRA) which is one of the simplest heuristic algorithms is used to design the local dimming algorithm. In order to improve the global optimization ability of the GRA, an Improved Greedy Algorithm(IGRA) based on the strategies of Taking out-Putting in and variable search step size is proposed. Experienced in four different types of images and compared with five parameter-based algorithms, the IGRA can obtain a higher visual quality under the same or lower power consumption. It is also proved that the IGRA has more powerful search ability and higher running efficiency by the comparisons with the Improved Shuffled Frog Leaping Algorithm (ISFLA) proposed in our previous work, and two recent algorithms including the Modified Genetic Algorithm (MGA) and the Improved Particle Swarm Optimization (IPSO).																	0924-669X	1573-7497															10.1007/s10489-020-01769-2		JUL 2020											
J								Stochastic gradient-CAViaR-based deep belief network for text categorization	EVOLUTIONARY INTELLIGENCE										Text categorization; Deep belief network; Stochastic gradient descent; CAViaR; Vector space model	FEATURE-SELECTION; ALGORITHM; MODEL	Text categorization is defined as the process of assigning tags to text according to its content. Some of the text classification approaches are document organization, spam email filtering, and news groupings. This paper introduces stochastic gradient-CAViaR-based deep belief networks for text categorization. The overall procedure of the proposed approach involves four steps, such as pre-processing, feature extraction, feature selection, and text categorization. At first, the pre-processing is carried out from the input data based on stemming, stop-word removal, and then, the feature extraction is performed using a vector space model. Once the extraction is done, the feature selection is carried out based on entropy. Subsequently, the selected features are given to the text categorization step. Here, the text categorization is done using the proposed SG-CAV-based deep belief networks (SG-CAV-based DBN). The proposed SG-CAV is used to train the DBN, which is designed by combining conditional autoregressive value at risk and stochastic gradient descent. The performance of the proposed SGCAV + DBN is evaluated based on the metrics, such as recall, precision, F-measure and accuracy. Also, the performance of the proposed method is compared with the existing methods, such as Naive Bayes, K-nearest neighbours, support vector machine, and deep belief network (DBN). From the analysis, it is depicted that the proposed SGCAV + DBN method achieves the maximal precision of 0.78, the maximal recall of 0.78, maximal F-measure of 0.78, and the maximal accuracy of 0.95. Among the existing methods, DBN achieves the maximum precision, recall, F-measure and accuracy, for 20 Newsgroup database and Reuter database. The performance of the proposed system is 10.98%, 11.54%, 11.538%, and 18.33% higher than the precision, recall, F-measure, and accuracy of the DBN for 20 Newsgroup database, and 2.38%, 2.38%, 2.37%, and 0.21% higher than the precision, recall, F-measure and accuracy of the DBN for Reuter database.																	1864-5909	1864-5917															10.1007/s12065-020-00449-x		JUL 2020											
J								A novel patch selection technique in ANN B-Spline Bayesian hyperprior interpolation VLSI architecture using fuzzy logic for highspeed satellite image processing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial neural network; B-spline interpolation; Bayesian hyperprior interpolation; Image denoising; Image interpolation; Image patch based denoising; VLSI architecture for image interpolation		During recent days various research are going on Satellite image processing including remote sensing, agriculture, disaster management and various other fields. Denoising and interpolation are two major tasks in Image processing. These tasks assist screening and interpreting many scientific images such as medical images and satellite images. The proposed work named as "fuzzy logic-based Patch Selection in ANN B-Spline Bayesian hyperprior interpolation VLSI architecture" (FLABBHI) is contrived in the indent of designing a new VLSI architecture for faster satellite image denoising and interpolation. Non-native image sampling process for denoising is performed using fuzzy logic based patch selection and artificial neural network-based B-Spline Bayesian hyperprior interpolation (ABBHI) procedure is used for interpolation process. Fuzzy logic is used to improve the execution speed in the process of selecting patch images from a collection of received images to construct the image more similar toground-truth image. For experimentation, FLABBHI is designed for the target hardware Kintex, used to construct and evaluate the proposed method. Xilinx ISE is used to measure the performance metrics of the target FPGA. The interpolation is performed by ABBHI method to improve the quality of the constructed image during the interpolation process. A real-time mode is included in the proposed FLABBHI to buffer and process incoming images from a satellite lively. This real-time mode is equipped to run with the incremental rendering process which is used to get a quick scanning in the beginning and more accurate reconstructed image over the time.																	1868-5137	1868-5145															10.1007/s12652-020-02264-9		JUL 2020											
J								Probability based cluster routing protocol for wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy; Sensor; Routing; Network; Cluster head; Heterogeneous; Communication	ALGORITHM	The wireless sensor network has its applications spread in almost every domain of networking, and to improve the lifetime of the limited power network various approaches are used nowadays. The network life of wireless sensor network can be enhanced using the cluster-based routing. Routing is among the most essential and challenging task for the wireless sensor network. The balanced energy consumption and network lifetime enhancement are the most popular issues for any clustering protocol. In this paper, a heterogeneous network based cluster routing protocol named heterogeneous network based clustering (HNBC) is proposed. It focuses on solving the issues of energy consumption and network lifetime which helps in providing the enhanced network performance. The proposed protocol is based on a probability model that uses the node energy and cluster head selection probability of different heterogeneous nodes for the cluster head selection. The simulation of the proposed protocol is performed using the MATLAB simulator. The results of the simulation show that the proposed scheme has improved the throughput, energy consumption, and lifetime of the network as contrasted with the existing DHSCA, DFTR, ATEER, EBCS, and ENEFC protocols.																	1868-5137	1868-5145															10.1007/s12652-020-02307-1		JUL 2020											
J								DewMusic: crowdsourcing-based internet of music things in dew computing paradigm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Crowdsourcing; Internet of Things; Dew computing; Fog computing; Cloud computing; Music	CLOUD; ARCHITECTURE	Internet of Things is a promising paradigm that integrates a plethora of heterogeneous computational devices, incorporating the crowd, frameworks, additional system elements, and infrastructure. Information sensing, modeling, retrieval, and distribution perform an emerging role in the Internet of Things network. Dew computing is a challenging research issue, which needs to demonstrate its impact on the sensor data in the domain of parallel and distributed computing. We have presented a dew-cloud computing-based music crowdsourcing framework in this paper, to address the dew computing effectiveness in the context of the Internet of Things. The crowdsourcing paradigms are efficient to collect and analyze billions of information efficiently with a diminutive cost. In this promising paradigm, participated sound sensing devices sense acoustic information from the environment; transmit the sensor data to fog computing devices through dew repository, and eventually, cloud data center stores the processed data for providing aggregated musical information and relevant services to the end-users. This paper presents a Dew-Cloud based music crowdsourcing framework in the ambiance of the Internet of Things. We have illustrated a semantic mathematical background for the proposed crowdsourcing-based Internet of Music Things architecture in the dew-cloud computing framework. We have also discussed the system performance metrics, in terms of information transmission time, service latency, and energy dissipation in this endeavor. We have additionally illustrated a comparative analysis between the proposed paradigm and the conventional cloud computing schema in terms of data transmission time and overall system energy dissipation. The goal of this paper is to conceptualize, how the end-users can be benefitted from data analytics through data sensing, computing, and distributed scenario using a dew-cloud computational framework in the Internet of Things environment.																	1868-5137	1868-5145															10.1007/s12652-020-02309-z		JUL 2020											
J								Sentimental analysis of transliterated text in Malayalam using recurrent neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neural network; RNN-LSTM technique; Natural language processing (NLP); Waikato environment for knowledge analysis (Weka)		Usage of mobile phones, access to internet in the fingertips and increasing number of mobile applications has accelerated the generation of online content. Freedom of expression has waived the barriers of online interaction. Curiosity in knowing others viewpoint through their reviews in each and everything starting from a purchase of product to watching a movie has become a common scenario. Decision on success and failure of one's business is in the hands of public now. Humans are always comfortable, sticking to their native Language, when it comes to expressions whether it is interest, emotions, feeling or opinion. Usage of Natural Language and the trend to analyze the subjective sentiments is increasing day by day. Transliterated text of a language is the English version of spoken native language. For example, Malayalam in English we call as Manglish. Transliterated text has become the language of social media websites like WhatsApp, Facebook, Twitter. It's a kind of boon to the young generation who know to speak their native language but not to read or write in its own nominal scripts. In this paper we consider the Sentimental Analysis of Transliterated text. RNN-LSTM technique is used to derive the sentiments of transliterated text.																	1868-5137	1868-5145															10.1007/s12652-020-02305-3		JUL 2020											
J								From filters to fillers: an active inference approach to body image distortion in the selfie era	AI & SOCIETY										Body dysmorphic disorder; Body image distortions; Snapchat dysmorphia; Active inference; Self-model theory; Identity formation	FREE-ENERGY PRINCIPLE; EATING-DISORDERS; SCHEMA; MEDIA; COGNITION; GENDER; MEMORY; HABIT	Advances in artificial intelligence, as well as its increased presence in everyday life, have brought the emergence of many new phenomena, including an intriguing appearance of what seems to be a variant of body dysmorphic disorder, coined "Snapchat dysmorphia". Body dysmorphic disorder is a DSM-5 psychiatric disorder defined as a preoccupation with one or more perceived defects or flaws in physical appearance that are not observable or appear slight to others. Snapchat dysmorphia is fueled by automated selfie filters that reflect unrealistic sociocultural standard. In this paper, we discuss how body dysmorphic disorder and related body image distortions could arise, using the conceptual resources provided by the active inference framework. We suggest that these disorders involve dysfunctional self-modelling which entails maladaptive internalization of sociocultural preferences during adolescent identity formation. Identity formation is hereby described as cycles of interpersonal active inference that arbitrate between identity exploration and commitment. We propose that impaired self-modelling is unable to reduce interpersonal uncertainty during identity exploration, which, over time, degenerates into uncontrollable epistemic habits that isolate the body image from corrective sensory evidence. In light of these insights, we subsequently explore some of the consequences of image-centered social media platforms on the identity formation process. We conclude that heightened interpersonal uncertainty in this novel context could precipitate the onset of body dysmorphic disorder and related body image distortions, particularly when selfie filters are involved.																	0951-5666	1435-5655															10.1007/s00146-020-01015-w		JUL 2020											
J								Synchronization of Stochastic Complex Dynamical Networks with Mixed Time-Varying Coupling Delays	NEURAL PROCESSING LETTERS										Complex dynamical networks (CDNs); Synchronization control; Linear matrix inequality; Lyapunov-Krasovskii functional; Time-varying coupling delays	MULTIAGENT SYSTEMS; STATE ESTIMATION; NEURAL-NETWORKS; STABILITY; CONSENSUS; CRITERIA	Synchronization of complex networks with mixed time-varying coupling delays and stochastic perturbation. We constructed a novel Lyapunov functional with triple integral terms. By applying Jensen's inequality and Lyapunov stability theory stability conditions are derived to check the asymptotical stability of the concerned system. By employing the stochastic evaluation and Kronecker product delay-dependent synchronization criteria of stochastic complex dynamical networks are derived. By using the derived conditions control gain matrix is obtained. Finally, numerical results are presented to demonstrate the effectiveness and usefulness of the proposed results.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1233	1250		10.1007/s11063-020-10301-z		JUL 2020											
J								Multivariate curve resolution of multiway data using the multilinearity constraint	JOURNAL OF CHEMOMETRICS										MCR-ALS; multilinearity constraint; multivariate curve resolution; multiway data	ALTERNATING LEAST-SQUARES; MCR-ALS; DECOMPOSITION; CALIBRATION; PERFORMANCE; ALGORITHM; CATALONIA; EXTENSION; AMBIGUITY; PATTERNS	The extension of Multivariate Curve Resolution-Alternating Least Squares (MCR-ALS) to the analysis of multiway data using the multilinearity constraint is described in detail as one step forward of previous implementations of the trilinearity and quadrilinearity constraints for the analysis of three- and four-way data sets, respectively. As in previous cases, the implementation of the multilinear model for multiway data sets is done algorithmically, within the frame of the alternating least squares (ALS) optimization in the MCR-ALS method. This implementation is tested using multiway data sets of different complexity, and the obtained results have confirmed the adequacy of the proposed approach. Special advantages of the proposed methodology are that it allows for the implementation of the constraint separately for the different components in their different modes and that it also allows for the introduction of different levels of complexity of the multilinear model, including mixed multilinear models. These two features are especially relevant because they are not present in most of the most used multiway data analysis methods at present.																	0886-9383	1099-128X														e3279	10.1002/cem.3279		JUL 2020											
J								Multi-objective drone path planning for search and rescue with quality-of-service requirements	AUTONOMOUS ROBOTS										Drones; UAVs; Path planning; Coverage; Connectivity; SAR; QoS	COVERAGE; NETWORKS; UAVS	We incorporate communication into the multi-UAV path planning problem for search and rescue missions to enable dynamic task allocation via information dissemination. Communication is not treated as a constraint but a mission goal. While achieving this goal, our aim is to avoid compromising the area coverage goal and the overall mission time. We define the mission tasks as: search, inform, and monitor at the best possible link quality. Building on our centralized simultaneous inform and connect (SIC) path planning strategy, we propose two adaptive strategies: (1) SIC with QoS (SICQ): optimizes search, inform, and monitor tasks simultaneously and (2) SIC following QoS (SIC+): first optimizes search and inform tasks together and then finds the optimum positions for monitoring. Both strategies utilize information as soon as it becomes available to determine UAV tasks. The strategies can be tuned to prioritize certain tasks in relation to others. We illustrate that more tasks can be performed in the given mission time by efficient incorporation of communication in the path design. We also observe that the quality of the resultant paths improves in terms of connectivity.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1183	1198		10.1007/s10514-020-09926-9		JUL 2020											
J								An interpretable regression approach based on bi-sparse optimization	APPLIED INTELLIGENCE										Data mining; Multi-kernel learning; Sparse learning; Zero-norm regularization; Support vector regression	FEATURE-SELECTION; VARIABLE SELECTION; CLASSIFICATION; REGULARIZATION	Given the increasing amounts of data and high feature dimensionalities in forecasting problems, it is challenging to build regression models that are both computationally efficient and highly accurate. Moreover, regression models commonly suffer from low interpretability when using a single kernel function or a composite of multi-kernel functions to address nonlinear fitting problems. In this paper, we propose a bi-sparse optimization-based regression (BSOR) model and corresponding algorithm with reconstructed row and column kernel matrices in the framework of support vector regression (SVR). The BSOR model can predict continuous output values for given input points while using the zero-norm regularization method to achieve sparse instance and feature sets. Experiments were run on 16 datasets to compare BSOR to SVR, linear programming SVR (LPSVR), least squares SVR (LSSVR), multi-kernel learning SVR (MKLSVR), least absolute shrinkage and selection operator regression (LASSOR), and relevance vector regression (RVR). BSOR significantly outperformed the other six regression models in predictive accuracy, identification of the fewest representative instances, selection of the fewest important features, and interpretability of results, apart from its slightly high runtime.																	0924-669X	1573-7497				NOV	2020	50	11					4117	4142		10.1007/s10489-020-01687-3		JUL 2020											
J								A modified brain storm optimization algorithm with a special operator to solve constrained optimization problems	APPLIED INTELLIGENCE										Brain storm optimization algorithm; Constrained numerical optimization problems; Constraint-consensus method; Feasibility vectors; epsilon-constrained method	DIFFERENTIAL EVOLUTION	This paper presents a novel approach based on the combination of the Modified Brain Storm Optimization algorithm (MBSO) with a simplified version of the Constraint Consensus method as special operator to solve constrained numerical optimization problems. Regarding the special operator, which aims to reach the feasible region of the search space, the consensus vector becomes the feasibility vector computed by the hardest constraint in turn for a current infeasible solution; then the operations to mix the other feasibility vectors are avoided. This new combined algorithm, named as MBSO-R+V, solves a suit of eighteen test problems in ten and thirty dimensions. From a set of experiments related to the location and frequency of application of the constraint consensus method within MBSO, a suitable design of the combined approach is presented. This proposal shows encouraging final results while being compared against state-of-the-art algorithms, showing that it is viable to add special operators to improve the capabilities of swarm-intelligence algorithms when dealing with continuous constrained search spaces.																	0924-669X	1573-7497															10.1007/s10489-020-01763-8		JUL 2020											
J								Multi-dimensional Bayesian network classifiers: A survey	ARTIFICIAL INTELLIGENCE REVIEW										Multi-dimensional classification; Multi-label classification; Bayesian networks; Performance evaluation measures; Structural learning; Bayesian network inference complexity	MULTI-LABEL CLASSIFICATION; MARKOV BLANKET INDUCTION; FEATURE-SELECTION; BELIEF NETWORKS; CAUSAL DISCOVERY; ENSEMBLE METHOD; LOCAL CAUSAL; ALGORITHMS; FRAMEWORK; INFERENCE	Multi-dimensional classification is a cutting-edge problem, in which the values of multiple class variables have to be simultaneously assigned to a given example. It is an extension of the well known multi-label subproblem, in which the class variables are all binary. In this article, we review and expand the set of performance evaluation measures suitable for assessing multi-dimensional classifiers. We focus on multi-dimensional Bayesian network classifiers, which directly cope with multi-dimensional classification and consider dependencies among class variables. A comprehensive survey of this state-of-the-art classification model is offered by covering aspects related to their learning and inference process complexities. We also describe algorithms for structural learning, provide real-world applications where they have been used, and compile a collection of related software.																	0269-2821	1573-7462															10.1007/s10462-020-09858-x		JUL 2020											
J								DNA motif discovery using chemical reaction optimization	EVOLUTIONARY INTELLIGENCE										Motif discovery; Binding sites; Information content; Chemical reaction optimization; Meta-heuristics	BINDING SITES; DATABASE; ALGORITHM; TOOLS	DNA motif discovery means to find short similar sequence elements within a set of nucleotide sequences. It has become a compulsory need in bioinformatics for its useful applications such as compression, summarization, and clustering algorithms. Motif discovery is an NP-hard problem and exact algorithms cannot solve it in polynomial time. Many optimization algorithms were proposed to solve this problem. However, none of them can show its supremacy by overcoming all the obstacles. Chemical Reaction Optimization (CRO) is a population based metaheuristic algorithm that can easily fit for the optimization problem. Here, we have proposed an algorithm based on Chemical Reaction Optimization technique to solve the DNA motif discovery problem. The four basic operators of CRO have been redesigned for this problem to search the solution space locally as well as globally. Two additional operators (repair functions) have been proposed to improve the quality of the solutions. They have been applied to the final solution after the iteration stage of CRO to get a better one. Using the flexible mechanism of elementary operators of CRO along with the additional operators (repair functions), it is possible to determine motif more precisely. Our proposed method is compared with other traditional algorithms such as Gibbs sampler, AlignACE (Aligns Nucleic Acid Conserved Elements), MEME (Multiple Expectation Maximization for Motif Elicitation), and ACRI (Ant-Colony-Regulatory-Identification) by testing real-world datasets. The experimental results show that the proposed algorithm can give better results than other traditional algorithms in quality and in less running time. Besides, statistical tests have been performed to show the superiority of the proposed algorithm over other state-of-the-arts in this area.																	1864-5909	1864-5917															10.1007/s12065-020-00444-2		JUL 2020											
J								A weighted exponential discriminant analysis through side-information for face and kinship verification using statistical binarized image features	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Kinship verification; Face matching; Unconstrained environment; Weighting factor; SIWEDA; StatBIF; Fisher criterion	PRESERVING PROJECTIONS; TEXTURE CLASSIFICATION; RECOGNITION; ROBUST; DEEP; DESCRIPTORS; HISTOGRAMS; SUBJECT; POSE	Side-information based exponential discriminant analysis (SIEDA) is more efficient than side-information based linear discriminant analysis (SILDA) in computing the discriminant vectors because it maximizes the Fisher criterion function. In this paper, we develop a novel criterion, named side-information based weighted exponential discriminant analysis (SIWEDA), that is based on the classical SIEDA method. We reformulate and generalize the classical Fisher criterion function in order to maximize it, with the property to pull as close as possible the intra-class samples (within-class samples), and push and repulse away as far as possible the inter-class samples (between-class samples). Thus, SIWEDA selects the eigenvalues of high significance and eliminate those with less discriminative information. To reduce the feature vector dimensionality and lighten the class intra-variability, we use SIWEDA and within class covariance normalization (WCCN) using the proposed statistical binarized image features (StatBIF). Moreover, we use score fusion strategy to extract the complementarity of different weighting scales of our StatBIF descriptor. We conducted experiments to evaluate the performance of the proposed method under unconstrained environment, using five datasets namely LFW, YTF, Cornell KinFace, UB KinFace and TSKinFace datasets, in the context of matching faces and kinship verification in the wild conditions. The experiments showed that the proposed approach outperforms the current state of the art. Very interestingly, our approach showed superior performance compared to methods based on deep metric learning.																	1868-8071	1868-808X															10.1007/s13042-020-01163-x		JUL 2020											
J								Three factor nonnegative matrix factorization based HE stain unmixing in histopathological images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										HE stain image; Non-negative factorisation; PSNR; SSIM	DECOMPOSITION; SEPARATION	Histological staining facilitates the microscopic study of diseased tissues in clinical investigations. Staining by fluorescent dyes enhances the tissue contrast, highlighting the important features. Haematoxylin and Eosin staining protocol is the most common in the histopathological analysis of cells and tissues in cancer diagnosis. HE staining provides a visual representation of tissue abnormalities, distinguishing cell nuclei and acidophilic structures. Stain unmixing is an equally vital procedure which decomposes the multi-stained image into individual stain components. These components are essential for visual examination of the interaction between the dye and the specific tissues. This manuscript put forwards an original cell destaining approach based on a three factor Nonnegative Matrix Factorization which separates the H and E components from the HE stained image. The experimental results with standard datasets and performance metrics demonstrate the robustness of the method yielding better PSNR and SSIM values for stain separation compared to similar works in this context. The PSNR values around 30 dB are achieved by the proposed unmixing approach which is more than 10 dB compared to the benchmark approaches. Similarly, the SSIM values are obtained in the range 0.42-0.73, signifying strong structural elements. The proposed method can be further extended to study the interaction of other activators in the wound repair process under cancer progression.																	1868-5137	1868-5145															10.1007/s12652-020-02265-8		JUL 2020											
J								Power Spectral Clustering	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Spectral clustering; Gamma-convergence; MST-based clustering; Multiscale combinatorial grouping; Image segmentation	GRAPH; SEGMENTATION	Spectral clustering is one of the most important image processing tools, especially for image segmentation. This specializes at taking local information such as edge weights and globalizing them. Due to its unsupervised nature, it is widely applicable. However, traditional spectral clustering is O(n(3/2)). This poses a challenge, especially given the recent trend of large datasets. In this article, we propose an algorithm by using ideas from Gamma-convergence, which is an amalgamation of maximum spanning tree clustering and spectral clustering. This algorithm scales as O(nlog(n)) under certain conditions, while producing solutions which are similar to that of spectral clustering. Several toy examples are used to illustrate the similarities and differences. To validate the proposed algorithm, a recent state-of-the-art technique for segmentation-multiscale combinatorial grouping is used, where the normalized cut is replaced with the proposed algorithm and results are analyzed.																	0924-9907	1573-7683				NOV	2020	62	9					1195	1213		10.1007/s10851-020-00980-7		JUL 2020											
J								On 2S-metric spaces	SOFT COMPUTING										Soft set; Soft point; 2-metric; Soft 2-metric; Metric topology; Completeness	SOFT SET-THEORY; THEOREM	In this paper, we introduce 2-metric spaces in terms of soft points, called 2s-metric spaces, in the soft universe, which is a nonlinear generalization of soft metric spaces. Then we induce a soft topology from a given 2s-metric space and also study some of its topological structures such as open balls, open (closed) sets, completeness and etc. After that, we prove the Cantor's Intersection Theorem for complete 2s-metric spaces and use it to show that such a space cannot be expressed as a countable union of no-where dense soft sets under some general situations. At the end, we obtain some fixed point results in complete 2s-metric spaces by using Cantor's theorem.																	1432-7643	1433-7479				SEP	2020	24	17					12731	12742		10.1007/s00500-020-05134-w		JUL 2020											
J								Convexity on complete lattices	SOFT COMPUTING										Complete lattice; Convex structure; Convexity-preserving mapping; Hull space; Enclosed order	SPACES	By means of closure systems and closure operators on complete lattices, a generalized convex structure under which classical convex structures andL-convex structures are consistent with each other is established. The related convex spaces and hull spaces are investigated, and it is shown that they are isomorphic to each other from the viewpoint of category. In order to further characterize this convex structure, the notion of enclosed order spaces and their corresponding mappings are introduced. It is proved that the category of enclosed order spaces is also isomorphic to that of convex spaces we presented.																	1432-7643	1433-7479				SEP	2020	24	17					12743	12751		10.1007/s00500-020-05137-7		JUL 2020											
J								High utility itemset mining: a Boolean operators-based modified grey wolf optimization algorithm	SOFT COMPUTING										High utility itemset; Boolean operators; Grey wolf optimization algorithm	GENETIC ALGORITHM; BAT ALGORITHM; DISCOVERY	In data mining, mining high utility itemset (HUI) is one among the recent thrust area that receives several approaches for solving it in an effective manner. In the past decade, addressing optimization problems using evolutionary algorithms are an unavoidable strategy due to its convergence towards optimal solution within the stipulated time. The results of evolutionary algorithms on various optimization problems are far effective when compared to the exhaustive approaches with respect to computational time. The problem with HUI is discovering a set of items from a transactional database that possess high level of utility when compared with other distinctive sets. This problem becomes harder while addressing the count of items in the database while its higher and computational time to solve this problem using exhaustive search becomes exponential as proposition of items in transaction database increases. In this paper, an optimization model based on the biological behaviour of grey wolf is proposed; the model namely grey wolf optimization algorithm is used to solve HUI using five different Boolean operations. The proposed model is evaluated using standard performance metrics over synthetic datasets and real-world datasets. The proposed model results are then compared with recent HUIM models to show the significance.																	1432-7643	1433-7479				NOV	2020	24	21					16691	16704		10.1007/s00500-020-05123-z		JUL 2020											
J								An interval type-2 fuzzy MCDM model for work package subcontractor prequalification	SOFT COMPUTING										Work packages subcontractors prequalification; Shannon entropy weight; Technique for order performance by similarity to ideal solutions; Interval Type-2 Fuzzy Set	SELECTION; CONTRACTOR; AHP; FRAMEWORK; CRITERIA; IMPACT	The inherent role of the Main Contractors (MCs) and their responsibility to employers force them to consider an appropriate mechanism for evaluating the subcontractors (SCs). The past few decades witnessed the gradual prevalence of fuzzy set theory in different industries. The construction industry was not an exception either. It is in use now for SCs prequalification thanks to its competence in covering the flaws underlying the SC selection methods. Most previous researches employed Type-1 Fuzzy Sets (T1FSs) in SCs prequalification models. But, the result of the applying TIFSs to solve a SC prequalification could be simply misleading because it is principally a group (Multi-Criteria Decision Making) MCDM problem. An important limitation of T1FS assures the fact that it can handle the decision-making problem of only one person because its membership function is crisp, relying on his subjective viewpoint, and deterring it from revealing differing views among the experts. We aim to introduce a novel SCs prequalification method using Interval Type-2 Fuzzy Sets (IT2FSs) in a bid to rectify the linguistic flaws as well as differences of the experts' views. Therefore, an integration of two methods, namely "Shannon Entropy Weight" and "Technique for Order Performance by Similarity to Ideal Solutions," is used under IT2FS for solving the SC prequalification problem. In addition, a real case is considered to demonstrate the effectiveness of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05173-3		JUL 2020											
J								Who and where: context-aware advertisement recommendation on Twitter	SOFT COMPUTING										Triadic concept analysis; Time-aware analysis; Ads recommendation; Location-based analysis	NETWORKS	Advertising is becoming a business on social networks. Billions of people around the world use social media, and fastly, it has become one of the defining technologies of our time. Social platforms like Twitter are one of the primary means of communication and information dissemination and can capture the interest of potential customers. Therefore, it is crucial to select suitable advertisements to users in specific times and locations for capturing their attention, profitably. In this paper, we propose a context-aware advertising recommendation system that, by analyzing the users' tweets and movements along a timeline, infers the personal interests of users and provides attractive ads to users through the triadic formal concept analysis theory.																	1432-7643	1433-7479															10.1007/s00500-020-05147-5		JUL 2020											
J								Applications of contractive-likemapping principles to fuzzy fractional integral equations with the kernel psi-functions	SOFT COMPUTING										Fractional integral equations; Fuzzy fractional integral equations; The generalized kernel functions; Weakly contractive mapping	PARTIALLY ORDERED SETS; PARTIAL-DIFFERENTIAL-EQUATIONS; FIXED-POINT; EXISTENCE; INTERVAL; MATRIX	In this work, we present a new class of generalized fractional integral equations with respect to the kernel psi-function under the fuzzy concept. The results of this problem can be used to recover a wide class of fuzzy fractional integral equations by the choice of the kernel psi-function. Without the Lipschitzian right-hand side, we investigate the existence and uniqueness of the fuzzy solutions by employing the fixed point theorem of weakly contractive mappings in the partially ordered space of fuzzy numbers. The proposed approach is based on the concept of a fuzzy metric space endowed with a partial order and the altering distance functions. In addition, the continuous dependence of solutions on the order and the initial condition of the given problem is also shown. Some concrete examples are presented in order to consolidate the obtained result.																	1432-7643	1433-7479															10.1007/s00500-020-05115-z		JUL 2020											
J								Fusion of self-organizing map and granular self-organizing map for microblog summarization	SOFT COMPUTING										Microblog summarization; Unsupervised learning; Self-organizing map (SOM); Granular self-organizing map (GSOM); Word mover distance	CLASSIFICATION	In this paper, we have proposed a fusion of two architectures, self-organizing map and granular self-organizing map (SOM + GSOM), for solving the microblog summarization task where a set of relevant tweets are extracted from the available set of tweets. SOM is used to reduce the available set of tweets to a smaller subset, and GSOM is used for extracting relevant tweets. The fusion of SOM + SOM is also accomplished to illustrate the effectiveness of GSOM over SOM in the second architecture. Moreover, only SOM version is also utilized to illustrate the potentiality of fusion in our proposed approaches. As similarity/dissimilarity measures play major role in any summarization system; therefore, to measure the same between tweets, various measures like word mover distance, cosine distance and Euclidean distance are also explored. The results obtained are evaluated on four datasets related to disaster events using ROUGE measures. Experimental results demonstrate that our best-proposed approach (SOM + GSOM) has obtained 17% and 5.9% improvements in terms of ROUGE-2 and ROUGE-L scores, respectively, over the existing techniques. The results are also validated using statistical significancet-test.																	1432-7643	1433-7479															10.1007/s00500-020-05104-2		JUL 2020											
J								On soft computing with random fuzzy sets in econometrics and machine learning	SOFT COMPUTING										Copulas; Conformal prediction; Fuzzy games; Fuzzy regression discontinuity design; Fuzzy sets; Machine learning; Random sets; Statistical quality control; Soft computing		Several typical econometric analyses, namely fuzzy coalitional games, regression for causal inference, statistical quality control, and prediction in machine learning, are examined in this paper to point out that soft computing components such as fuzzy theory and random set theory have a major role to play. The examination of these statistical analyses suggests an extended use of random fuzzy sets, based upon theory of random fuzzy sets, to improve empirical research.																	1432-7643	1433-7479															10.1007/s00500-020-05154-6		JUL 2020											
J								The promise and pitfall of automated text-scaling techniques for the analysis of jurisprudential change	ARTIFICIAL INTELLIGENCE AND LAW										Judicial opinions; Text-scaling; Jurisprudential change; Case law	POLITICAL TEXTS; POLICY POSITIONS; COURT	I consider the potential of eight text-scaling methods for the analysis of jurisprudential change. I use a small corpus of well-documented German Federal Constitutional Court opinions on European integration to compare the machine-generated scores to scholarly accounts of the case law and legal expert ratings. Naive Bayes, Word2Vec, Correspondence Analysis and Latent Semantic Analysis appear to perform well. Less convincing are the performance of Wordscores, ML Affinity and lexicon-based sentiment analysis. While both the high-dimensionality of judicial texts and the validation of computer-based jurisprudential estimates pose major methodological challenges, I conclude that automated text-scaling methods hold out great promise for legal research.																	0924-8463	1572-8382															10.1007/s10506-020-09274-0		JUL 2020											
J								Constructing and Extending Description Logic Ontologies using Methods of Formal Concept Analysis A Dissertation Summary	KUNSTLICHE INTELLIGENZ										Description logic; Formal concept analysis; Axiomatization; Concept inclusion		My thesis describes how methods from Formal Concept Analysis can be used for constructing and extending description logic ontologies. In particular, it is shown how concept inclusions can be axiomatized from data in the description logics EL, M, Horn-M, and Prob-EL. All proposed methods are not only sound but also complete, i.e., the result not only consists of valid concept inclusions but also entails each valid concept inclusion. Moreover, a lattice-theoretic view on the description logic EL is provided. For instance, it is shown how upper and lower neighbors of EL concept descriptions can be computed and further it is proven that the set of EL concept descriptions forms a graded lattice with a non-elementary rank function.																	0933-1875	1610-1987				SEP	2020	34	3			SI		399	403		10.1007/s13218-020-00673-8		JUL 2020											
J								A Practical EEG-Based Human-Machine Interface to Online Control an Upper-Limb Assist Robot	FRONTIERS IN NEUROROBOTICS										EEG; human-machine interface; assist robot; online control; practicability	BRAIN-COMPUTER INTERFACE; P300; POTENTIALS; SPELLER; SYSTEMS; STROKE	Background and Objective:Electroencephalography (EEG) can be used to control machines with human intention, especially for paralyzed people in rehabilitation exercises or daily activities. Some effort was put into this but still not enough for online use. To improve the practicality, this study aims to propose an efficient control method based on P300, a special EEG component. Moreover, we have developed an upper-limb assist robot system with the method for verification and hope to really help paralyzed people. Methods:We chose P300, which is highly available and easily accepted to obtain the user's intention. Preprocessing and spatial enhancement were firstly implemented on raw EEG data. Then, three approaches- linear discriminant analysis, support vector machine, and multilayer perceptron -were compared in detail to accomplish an efficient P300 detector, whose output was employed as a command to control the assist robot. Results:The method we proposed achieved an accuracy of 94.43% in the offline test with the data from eight participants. It showed sufficient reliability and robustness with an accuracy of 80.83% and an information transfer rate of 15.42 in the online test. Furthermore, the extended test showed remarkable generalizability of this method that can be used in more complex application scenarios. Conclusion:From the results, we can see that the proposed method has great potential for helping paralyzed people easily control an assist robot to do numbers of things.																	1662-5218					JUL 10	2020	14								32	10.3389/fnbot.2020.00032													
J								On a hypergraph probabilistic graphical model	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Graphical model; Hypergraph; Chain graph; Bayesian network; Intervention; Markov property; Factorization; Structural equation model	MARKOV PROPERTIES; INDEPENDENCE; DIAGRAMS	We propose a directed acyclic hypergraph framework for a probabilistic graphical model that we callBayesian hypergraphs. The space of directed acyclic hypergraphs is much larger than the space of chain graphs. Hence Bayesian hypergraphs can model much finer factorizations than Bayesian networks or LWF chain graphs and provide simpler and more computationally efficient procedures for factorizations and interventions. Bayesian hypergraphs also allow a modeler to represent causal patterns of interaction such as Noisy-OR graphically (without additional annotations). We introduce global, local and pairwise Markov properties of Bayesian hypergraphs and prove under which conditions they are equivalent. We also extend the causal interpretation of LWF chain graphs to Bayesian hypergraphs and provide corresponding formulas and a graphical criterion for intervention.																	1012-2443	1573-7470				SEP	2020	88	9					1003	1033		10.1007/s10472-020-09701-7		JUL 2020											
J								An effective multi-level synchronization clustering method based on a linear weighted Vicsek model	APPLIED INTELLIGENCE										Divide and collect; Kuramoto model; Shrinking synchronization clustering; Linear weighted Vicsek model; Near neighbor point set	ADAPTIVE PATTERN-CLASSIFICATION; ALGORITHM	To conquer the shortcoming that general clustering methods cannot process big data in the main memory, this paper presents an effective multi-level synchronization clustering (MLSynC) method by using a framework of "divide and collect" and a linear weighted Vicsek model. We also introduce two concrete implementations of MLSynC method, a two-level framework algorithm and a recursive algorithm. MLSynC method has a different process with SynC algorithm, ESynC algorithm and SSynC algorithm. By the theoretic analysis, we find the time complexity of MLSynC method is less than SSynC. Simulation and experimental study on multi-kinds of data sets validate that MLSynC method not only gets better local synchronization effect but also needs less iterative times and time cost than SynC algorithm. Moreover, we observe that MLSynC method not only needs less time cost than ESynC and SSynC, but also almost gets the same local synchronization effect as ESynC and SSynC if the partition of the data set is proper. Further comparison experiments with some classical clustering algorithms demonstrate the clustering effect of MLSynC method.																	0924-669X	1573-7497				NOV	2020	50	11					4063	4080		10.1007/s10489-020-01767-4		JUL 2020											
J								Data-driven multi-attribute decision-making by combining probability distributions based on compatibility and entropy	APPLIED INTELLIGENCE										Multi-attribute decision-making; Normalization; Probability distribution; Compatibility weight; Entropy weight	RISK ANALYSIS; SAFETY RISK; FUZZY; NUMBER; UNCERTAINTY; SETS	Multi-attribute decision-making has many applications in different fields. How to make decisions objectively when there are many attributes is still an open issue. This paper proposes a data-driven multi-attribute decision-making method considering the compatibility and entropy. Mainly, data of different decision attributes are normalized to probability distributions. The compatibility weight and entropy weight are computed respectively and then combined to a final weight. The scores of decision objects are derived by combining weighted probability distributions. In order to verify the effectiveness of the proposed method, two examples are given to compare with the AHP method and an improved data envelopment analysis method respectively. The former results show that the proposed method can obtain more objective results and produce a low computation complexity. The latter demonstrate the proposed method focuses more on the overall performance of decision attributes while the improved data envelopment analysis emphasises more on the ecological performance.																	0924-669X	1573-7497				NOV	2020	50	11					4081	4093		10.1007/s10489-020-01738-9		JUL 2020											
J								Dynamic uncertain causality graph for computer-aided general clinical diagnoses with nasal obstruction as an illustration	ARTIFICIAL INTELLIGENCE REVIEW										Uncertainty; Causality; Probabilistic reasoning; Clinical diagnosis; Nasal obstruction	DIRECTED CYCLIC GRAPH; KNOWLEDGE REPRESENTATION; INTELLIGENT DIAGNOSIS; MEDICAL DIAGNOSIS; EXPERT-SYSTEM; MATRIX; CANCER	Many AI systems have been developed for clinical diagnoses, in which most of them lack interpretability in both knowledge representation and inference results. The newly developed Dynamic Uncertain Causality Graph (DUCG) is a probabilistic graphical model with strong interpretability. However, existing DUCG is mainly for fault diagnoses of large, complex industrial systems. In this paper, we extend DUCG for better application in general clinical diagnoses. Four extensions are introduced: (1) special logic gate and zoom function event variables to represent and quantify the influences of various risk factors on the morbidities of diseases. (2) Reversal logic gate to model the case that some diseases/causes may result in at least two simultaneous symptoms/consequences. (3) Disease-specific manifestation variable for special inference and easy understanding to diagnose a specific disease. (4) Event attention importance to count contributions of isolated state-abnormal variables in inference. To illustrate and verify the extended DUCG methodology, we performed a case study for diagnosing 25 diseases causing nasal obstruction. We tested 171 cases randomly selected from total 471 cases of discharged patients in the hospital information system of Xuanwu Hospital. The diagnosis precision of the extended DUCG was 100%. The diagnosis precision of the third-party verification performed by Suining Central Hospital was 98.86%, which exhibited the strong generalization ability of the extended DUCG.																	0269-2821	1573-7462															10.1007/s10462-020-09871-0		JUL 2020											
J								The effective SVM-based binary prediction of ground water table	EVOLUTIONARY INTELLIGENCE										SVM (support vector machine); SVM-based binary prediction; Latitude; Longitude; Depth	NETWORKS	Groundwater resources are limited, to measure the groundwater and predict the data is a challenging task. Water is the major resource for living being and nonrenewable, hence to model and predict the groundwater is great significance in the present day scenario. In this paper collected the recent datasets of longitude of and latitude of 2018, 2019 depth of groundwater level analyzed and predicted the datasets of Chittoor (Andhra Pradesh, India) and Greater Noida (Gautham Budh Nagar, Uttar Pradesh in India) regions. The proposed machine learning model, by using the Support Vector Machine (SVM)-based binary prediction, the proposed model carries out the effective results and optimized approach. The model is applied in South India region and North India region and compared. Results proved that the proposed approach is a better approach for predicting groundwater table by using SVM-based binary prediction since it also involves optimization.																	1864-5909	1864-5917															10.1007/s12065-020-00447-z		JUL 2020											
J								Hesitant Fuzzy Linguistic Correlation Coefficient and Its Applications in Group Decision Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Group decision making; correlation coefficient; collaborative filtering algorithm; hesitant fuzzy linguistic term sets; trust relationship	CONSENSUS REACHING PROCESS; SOCIAL NETWORK; TERM SETS; AGGREGATION OPERATORS; DISTANCE; INFORMATION; MODEL; ASSESSMENTS; TRUST	Hesitant fuzzy linguistic term sets (HFLTSs) are applied to deal with situations in which people are hesitant in providing linguistic evaluations and have been widely used in qualitative group decision-making processes. Considering the fact that the correlation coefficient has been widely used in many research domains, in this paper, we first present a novel correlation coefficient for HFLTSs. The significant features of the proposed correlation coefficient are that the HFLTS s need not to be extended to the same length and the correlation coefficient lies in [-1, 1]. Two examples are used to illustrate the effectiveness of the proposed correlation coefficient. Second, based on the proposed correlation coefficient and the collaborative filtering algorithm, we develop a new method to deal with incomplete hesitant fuzzy linguistic information. Furthermore, by analyzing the correlation coefficient matrix, we present a new method for determining experts' weights based on the concept of 'attenuation' and 'how much trust' they obtained. At last, a case study is used to evaluate the performance of our method.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1748	1759		10.1007/s40815-020-00876-z		JUL 2020											
J								Recommender systems as an agility enabler in supply chain management	JOURNAL OF INTELLIGENT MANUFACTURING										E-commerce; Big data; Recommender systems; Supply chain; Agility	VEHICLE-ROUTING PROBLEM; KEY PERFORMANCE INDICATORS; LOGISTICS; METAHEURISTICS	In recent years, recommender systems have become necessary in overcoming the challenges related to the incredible growth of information. They are used in a wide range of contexts and applications, mainly as prediction tools for customer interest, designed to help customers decide, compare, discover and explore products (Meyer in Recommender systems in industrial contexts, Sciences et Technologies de l'Information, Grenoble, 2012). Therefore, research in the field has focused on improving the efficiency of data processing for instant and accurate recommendations. Recommendation of products, accordingly, does not take into consideration supply chain constraints for deliveries. This can lead to recommendations for products that can be costly or too long to ship to the customer, resulting in an avoidable increase in the stress on the supply chain. This paper addresses the problem of considering delivery constraints in product recommendations. The objective is to shift demand toward products that can be delivered using the current network state without additional resources in a given time window, perimeter and with a minimum acceptable profit, in the context of e-commerce. To achieve this goal, we propose a methodology to adjust product recommendations in order to shift customers' interests towards particular products with consideration for remaining unit loads of scheduled deliveries. For this, quasireal-time information about the supply chain is taken into consideration to improve the number of shippable products in the recommendation list, resulting in a possible improvement in truck-load utilization, lower operation costs and reduced lead-times for delivery. This method works in two stages: the first stage is the computation of the recommendation with traditional recommendation systems, and the second stage is recommendation adjustments in four phases that consider the evaluation of active trucks, evaluation of physical constraints for transportation, evaluation of the profits associated with adding a pickup/delivery to a scheduled tour for each recommended item and adjustment of recommendation scores. A sensitivity analysis of the impact of the recommendation adjustment on the recommendation list has been conducted for each of the parameters considered in the proposed method: time window, perimeter radius and minimum acceptable profit. Various experimental results prove that the method permits increasing the number of recommended products that can be shipped using the available resources within a given perimeter radius, time window and minimum profit.																	0956-5515	1572-8145															10.1007/s10845-020-01619-5		JUL 2020											
J								Optimization in solving inventory control problem using nature inspired Emperor Penguins Colony algorithm	JOURNAL OF INTELLIGENT MANUFACTURING										Inventory control problem; Metaheuristic algorithm; Nature-inspired; Emperor penguins colony algorithm; Deterministic single-product model; Deterministic multi-product model; Stochastic single-product model	GENETIC ALGORITHM; SYSTEM; DISCOUNT; DEMAND	In the present day markets, it is essential for organizations that manage their supply chain efficiency to sustain their market share and improve profitability. Optimized inventory control is an integral part of supply chain management. In inventory control problems, determining the ordering times and the order quantities of products are the two strategic decisions either to minimize total costs or to maximize total profits. This paper presents three models of inventory control problems. These three models are deterministic single-product, deterministic multi-product, and stochastic single-product. Due to the high computational complexity, the presented models are solved using the Emperor Penguins Colony (EPC) algorithm as a metaheuristic algorithm and a soft computing method. EPC is a newly published metaheuristic algorithm, which has not yet been employed to solve the inventory control problem. The results of applying the proposed algorithm on the models are compared with the results obtained by nine state-of-the-art and popular metaheuristic algorithms. To justify the proposed EPC, both cost and runtime criteria are considered. To find significant differences between the results obtained by algorithms, statistical analysis is used. The results show that the proposed algorithm for the presented models of inventory control has better solutions, lower cost, and less CPU consumption than other algorithms.																	0956-5515	1572-8145															10.1007/s10845-020-01616-8		JUL 2020											
J								Study of the hinge thickness deviation for a 316L parallelogram flexure mechanism fabricated via selective laser melting	JOURNAL OF INTELLIGENT MANUFACTURING										Flexure mechanism; Metal powder; 3D printing; Effective thickness; Manufacturing error	COMPLIANT; DESIGN; STAGE	3D printing offers great potential for developing complex flexure mechanisms. Recently, thickness-correction factors (TCFs) were introduced to correct the thickness and stiffness deviations of powder-based metal 3D printed flexure hinges during design and analysis. However, the reasons for the different TCFs obtained in each study are not clear, resulting in a limited value of these TCFs for future design and fabrication. Herein, the influence of the porous layer of 3D printed flexure hinges on the hinge thickness is investigated. Samples of parallelogram flexure mechanisms (PFMs) were 3D printed using selective laser melting (SLM) and 316L stainless steel powder. A 3D manufacturing error analysis was completed for each PFM sample via 3D scanning, surface roughness measurement and morphological observation. The thickness of the porous layer of the flexure hinge was independent of the designed hinge thickness and remained close to the average powder particle diameter. The effective hinge thickness could be estimated by subtracting twice the value of the porous layer thickness from the designed value. Guidelines based on finite element analysis and stiffness experiments are proposed. The limitations of the presented method for evaluating the effective hinge thickness of flexure hinges 3D printed via SLM are also discussed.																	0956-5515	1572-8145															10.1007/s10845-020-01621-x		JUL 2020											
J								Monocular 3D Exploration using Lines-of-Sight and Local Maps	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robotics; Autonomous exploration; Monocular SLAM; Micro aerial vehicles	HUMANITARIAN ROBOTICS; ENVIRONMENT; SLAM	Nowadays, robots equipped with a single camera, such as micro aerial vehicles (MAVs), are easily found at affordable costs. They can be used in different tasks, including the building of 3D environment maps. For building such maps, monocular simultaneous localization and mapping (SLAM) methods are employed, which usually generate sparse or semi-dense representations that are ill-suited for navigation tasks. We propose a new 3D exploration approach that uses a monocular camera as the only source of information. Our approach transforms a point cloud generated by monocular SLAM into local volumetric maps. These maps are built using the lines-of-sight between points and keyframes, allowing the MAV to navigate safely through the environment. Goal poses are dynamically defined to guide the MAV to explore the environment while avoiding obstacles. Besides that, the proposed approach seeks to determine properly when the environment was entirely explored, preventing that MAV stops before cover all the environment or flies more that is needed. The effectiveness of the proposed approach is evaluated in experiments in two different indoor environments, and show that it is possible to explore an environment using only a MAV equipped with a single monocular camera.																	0921-0296	1573-0409				NOV	2020	100	2					465	481		10.1007/s10846-020-01208-x		JUL 2020											
J								Bag of biterms modeling for short texts	KNOWLEDGE AND INFORMATION SYSTEMS										Short texts; Document representation; Topic modeling; Short text classification		Analyzing texts from social media encounters many challenges due to their unique characteristics of shortness, massiveness, and dynamic. Short texts do not provide enough context information, causing the failure of the traditional statistical models. Furthermore, many applications often face with massive and dynamic short texts, causing various computational challenges to the current batch learning algorithms. This paper presents a novel framework, namely bag of biterms modeling (BBM), for modeling massive, dynamic, and short text collections. BBM comprises of two main ingredients: (1) the concept of bag of biterms (BoB) for representing documents, and (2) a simple way to help statistical models to include BoB. Our framework can be easily deployed for a large class of probabilistic models, and we demonstrate its usefulness with two well-known models: latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP). By exploiting both terms (words) and biterms (pairs of words), the major advantages of BBM are: (1) it enhances the length of the documents and makes the context more coherent by emphasizing the word connotation and co-occurrence via bag of biterms, and (2) it inherits inference and learning algorithms from the primitive to make it straightforward to design online and streaming algorithms for short texts. Extensive experiments suggest that BBM outperforms several state-of-the-art models. We also point out that the BoB representation performs better than the traditional representations (e.g., bag of words, tf-idf) even for normal texts.																	0219-1377	0219-3116				OCT	2020	62	10					4055	4090		10.1007/s10115-020-01482-z		JUL 2020											
J								A Short Survey on Inconsistency Handling in Ontology-Mediated Query Answering	KUNSTLICHE INTELLIGENZ										Inconsistency handling; Ontology-mediated query answering; Description logics		This paper provides a concise overview of the literature on inconsistency handling for ontology-mediated query answering, a topic which has grown into an active area of research over the last decade. The focus of this survey is on the case where errors are localized in the data (i.e., the ontology is deemed reliable) and where inconsistency-tolerant semantics are employed with the aim of obtaining meaningful information from inconsistent knowledge bases.																	0933-1875	1610-1987															10.1007/s13218-020-00680-9		JUL 2020											
J								A self-attention-based destruction and construction learning fine-grained image classification method for retail product recognition	NEURAL COMPUTING & APPLICATIONS										Fine-grained classification; Multi-attribute recognition; Self-attention learning		Retail products belonging to the same category usually have extremely similar appearance characteristics such as colors, shapes, and sizes, which cannot be distinguished by conventional classification methods. Currently, the most effective way to solve this problem is fine-grained classification methods, which utilize machine vision + scene to perform fine feature representations on a target local region, thereby achieving fine-grained classification. Fine-grained classification methods have been widely used for recognizing birds, cars, airplanes, and many others. However, the existing fine-grained classification methods still have some drawbacks. In this paper, we propose an improved fine-grained classification method based on self-attention destruction and construction learning (SADCL) for retail product recognition. Specifically, the proposed method utilizes a self-attention mechanism in the destruction and construction of image information in an end-to-end fashion so that to calculate a precise fine-grained classification prediction and large information areas in the reasoning process. We test the proposed method on the Retail Product Checkout (RPC) dataset. Experimental results demonstrate that the proposed method achieved an accuracy above 80% in retail commodity recognition reasoning, which is much higher than the results of other fine-grained classification methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14613	14622		10.1007/s00521-020-05148-3		JUL 2020											
J								LDA-GA-SVM: improved hepatocellular carcinoma prediction through dimensionality reduction and genetically optimized support vector machine	NEURAL COMPUTING & APPLICATIONS										Feature extraction; Genetic algorithm; Hepatocellular carcinoma; Hyperparameter optimization; Support vector machine	LINEAR DISCRIMINANT-ANALYSIS; EXPERT-SYSTEM; DIAGNOSIS; NETWORK; HYBRID	Hepatocellular carcinoma (HCC) is a common type of liver cancer worldwide. Patients with HCC have rare chances of survival. The chances of survival increase, if the cancer is diagnosed early. Hence, different machine learning-based methods have been developed by researchers for the accurate detection of HCC. However, high dimensionality (curse of dimensionality) and lower prediction accuracy are the problems in the automated detection of HCC. Dimensionality reduction-based methods have shown state-of-the-art performance on many disease detection problems, which motivates the development of machine learning models based on reduced features dimension. This paper proposes a new hybrid intelligent system that hybridizes three algorithms, i.e., linear discriminant analysis (LDA) for dimensionality reduction, support vector machine (SVM) for classification and genetic algorithm (GA) for SVM optimization. Consequently, the three models are hybridized and one black box model, namely LDA-GA-SVM, is constructed. Experimental results on publicly available HCC dataset show improvement in the HCC prediction accuracy. Apart from performance improvement, the proposed method also shows lower complexity from two aspects, i.e., reduced processing time in terms of hyperparameters optimization and training time. The proposed method achieved accuracy of 90.30%, sensitivity of 82.25%, specificity of 96.07% and Matthews Correlation Coefficient (MCC) of 0.804.																	0941-0643	1433-3058															10.1007/s00521-020-05157-2		JUL 2020											
J								Estimating cooling production and monitoring efficiency in chillers using a soft sensor	NEURAL COMPUTING & APPLICATIONS										HVAC systems; Chillers; Efficiency; Cooling production; Soft sensor; Deep learning	COMPRESSION LIQUID CHILLERS; WATER-FLOW RATE; FAULT-DETECTION; ENERGY-CONSUMPTION; POWER-CONSUMPTION; FUSED MEASUREMENT; PERFORMANCE; DIAGNOSIS; SYSTEMS; PREDICTION	Intensive use of heating, ventilation and air conditioning systems in buildings entails monitoring their efficiency. Moreover, cooling systems are key facilities in large buildings and can account up to 44% of the energy consumption. Therefore, monitoring efficiency in chillers is crucial and, for that reason, a sensor to measure the cooling production is required. However, manufacturers rarely install it in the chiller due to its cost. In this paper, we propose a methodology to build asoft sensorthat provides an estimation of cooling production and enables monitoring the chiller efficiency. The proposed soft sensor uses independent variables (internal states of the chiller and electric power) and can take advantage of current or past observations of those independent variables. Six methods (from linear approaches to deep learning ones) are proposed to develop the model for the soft sensor, capturing relevant features on the structure of data (involving time, thermodynamic and electric variables and the number of refrigeration circuits). Our approach has been tested on two different chillers (large water-cooled and smaller air-cooled chillers) installed at the Hospital of Leon. The methods to implement the soft sensor are assessed according to three metrics (MAE, MAPE and R-2). In addition to the comparison of methods, the results also include the estimation of cooling production (and the comparison of the true and estimated values) and monitoring the COP indicator for a period of several days and for both chillers.																	0941-0643	1433-3058															10.1007/s00521-020-05165-2		JUL 2020											
J								Adaptive feature fusion with attention mechanism for multi-scale target detection	NEURAL COMPUTING & APPLICATIONS										Deep learning; Target detection; Adaptive feature fusion; Attention mechanism	RECOGNITION	To detect the targets of different sizes, multi-scale output is used by target detectors such as YOLO V3 and DSSD. To improve the detection performance, YOLO V3 and DSSD perform feature fusion by combining two adjacent scales. However, the feature fusion only between the adjacent scales is not sufficient. It hasn't made advantage of the features at other scales. What is more, as a common operation for feature fusion, concatenating can't provide a mechanism to learn the importance and correlation of the features at different scales. In this paper, we propose adaptive feature fusion with attention mechanism (AFFAM) for multi-scale target detection. AFFAM utilizes pathway layer and subpixel convolution layer to resize the feature maps, which is helpful to learn better and complex feature mapping. In addition, AFFAM utilizes global attention mechanism and spatial position attention mechanism, respectively, to learn the correlation of the channel features and the importance of the spatial features at different scales adaptively. Finally, we combine AFFAM with YOLO V3 to build an efficient multi-scale target detector. The comparative experiments are conducted on PASCAL VOC dataset, KITTI dataset and Smart UVM dataset. Compared with the state-of-the-art target detectors, YOLO V3 with AFFAM achieved 84.34% mean average precision (mAP) at 19.9 FPS on PASCAL VOC dataset, 87.2% mAP at 21 FPS on KITTI dataset and 99.22% mAP at 20.6 FPS on Smart UVM dataset which outperforms other advanced target detectors.																	0941-0643	1433-3058															10.1007/s00521-020-05150-9		JUL 2020											
J								Robust finite-time H-infinity congestion control for a class of AQM network systems	NEURAL COMPUTING & APPLICATIONS										AQM network; Robust H infinity control; Congestion control; Finite-time stability	ACTIVE QUEUE MANAGEMENT; SLIDING MODE CONTROL; STABILITY CONTROL; TCP/AQM SYSTEM; TCP; DELAY; RED	This work investigates a finite-time H-infinity robust congestion control issue for transmission control protocol network systems. By means of the backstepping technique, finite-time control method and H-infinity control theory, a novel robust H-infinity finite-time controller design approach is presented to guarantee the finite-time convergence of the queue tracking error. Furthermore, the closed-loop network system has an L-2 gain less than or equal to a given positive real number. Comparative simulation results show that the proposed control method performs better than the existing control approach.																	0941-0643	1433-3058															10.1007/s00521-020-05168-z		JUL 2020											
J								Adaptive synchronization of chaotic systems with time-varying delay via aperiodically intermittent control	SOFT COMPUTING										Synchronization; Chaotic system; Adaptive intermittent control; Time-varying delay	CELLULAR NEURAL-NETWORKS; DYNAMICAL NETWORKS; STABILITY	The time-varying delay makes the state of a chaotic system more complex, which has a great influence on stability analysis and synchronization. To solve this problem, we utilize the method of adaptive intermittent control and the theory of Lyapunov stability to realize the synchronization of chaotic systems with time-varying delay. Firstly, we propose the control strategy to achieve the asymptotic exponential stability of the synchronous system under the aperiodically intermittent control. To make it easier to implement, we also propose a periodic intermittent control strategy. Finally, we choose the Lorenz and financial systems to do a numerical simulation. The experimental results verified the validity of our method.																	1432-7643	1433-7479				SEP	2020	24	17					12773	12780		10.1007/s00500-020-05161-7		JUL 2020											
J								Fuzzy alpha-cut and related mathematical structures	SOFT COMPUTING										L-fuzzy set; alpha-cut; Fuzzy alpha-cut; Fuzzy strict alpha-cut; Frame; Godel arrow	SETS	This paper deals with the notions called fuzzy alpha-cut, fuzzy strict alpha-cut and their properties. Algebraic structures arising out of the family of fuzzy alpha-cuts and fuzzy strict alpha-cuts have been investigated. Some significance and usefulness of fuzzy alpha-cuts are discussed.																	1432-7643	1433-7479															10.1007/s00500-020-05131-z		JUL 2020											
J								A self-adapting hierarchical actions and structures joint optimization framework for automatic design of robotic and animation skeletons	SOFT COMPUTING										Automatic design; Bayesian optimization; Robotics; Physics engine	ALGORITHMS	Skeleton designs are widely seen in the robotics industry and multimedia applications such as animated films and computer games. The design of skeletons is mental-labor intensive, especially axes directions of joints are difficult even for the most experienced designers to select. In the existing works, there are auto creation of skeletons from meshes, and skeleton axes optimizations from a predefined set of actions. In this work, we extend automatic construction of skeletons by proposing skeleton axes design from an objective task. A two-layered framework is proposed to optimize randomly initialized axes based on auto generated actions. First, the limit of skeleton scales that can be automatically designed is discussed. Second, a self-adaptive actions discretizer implemented by neural networks is proposed to reduce the optimization complexity. Third, actions and axes are scored by physics engine simulation, and optimizations on the score generate axes that have the best performance. Experiments include robotic designs and an animation application. Comparisons show that the proposed framework outperforms other mainstream optimizers both in speed and effectiveness.																	1432-7643	1433-7479															10.1007/s00500-020-05139-5		JUL 2020											
J								Moving Medical Image Analysis to GPU Embedded Systems: Application to Brain Tumor Segmentation	APPLIED ARTIFICIAL INTELLIGENCE											NEURAL-NETWORKS	With the growth of medical data stored as bases for researches and diagnosis tasks, healthcare providers are in need of automatic processing methods to make accurate and fast image analysis such as segmentation or restoration. Most of the existing solutions to deal with these tasks are based on Deep Learning methods that require the use of powerful dedicated hardware to be executed and address a power consumption problem that is not compatible with the aforementioned requests. There is thus a demand in the development of low-cost image analysis systems with increased performances. In this work, we address this problem by proposing a fully-automatic brain tumor segmentation method based on a Convolutional Neural Network, executed by a low-cost, Deep Learning ready GPU embedded platform. We validated our approach using the BRaTS 2015 dataset to segment brain tumors and proved that an artificial neural network can be trained and used in the medical field with limited resources by redefining some of its inner operations.																	0883-9514	1087-6545				OCT 14	2020	34	12					866	879		10.1080/08839514.2020.1787678		JUL 2020											
J								NetDAP: (delta, gamma)-approximate pattern matching with length constraints	APPLIED INTELLIGENCE										Approximate pattern matching; Gap constraints; Wildcard; Occurrence; (delta . gamma) distance	SEQUENTIAL PATTERNS; WILDCARDS; ALGORITHM; ITEMSETS	Pattern matching(PM) with gap constraints has been applied to compute the support of a pattern in a sequence, which is an essential task of the repetitive sequential pattern mining (or sequence pattern mining) Compared with exact PM, approximate PM allows data noise (differences) between the pattern and the matched subsequence. Therefore, more valuable patterns can be found. Approximate PM with gap constraints mainly adopts the Hamming distance to measure the approximation degree which only reflects the number of different characters between two sequences, but ignores the distance between different characters. Hence, this paper addresses (delta, gamma) approximate PM with length constraints which employs local-global constraints to improve the accuracy of the PM, namely, the maximal distance between two corresponding characters is less or equal to the local threshold delta, and the sum of all the delta distances is also less or equal to the global threshold gamma. To tackle the problem effectively, this paper proposes an effective online algorithm, named NetDAP, which employs a special designed data structure named approximate single-leaf Nettree. An approximate single-leaf Nettree can be created by adopting dynamic programming to determine the range of rootleaf, the minimal root, the maximal root, the range of nodes for each level, and the range of parents for each node. To improve the performance, two pruning strategies are proposed to prune the nodes and the parent-child relationships which do not satisfy the delta and gamma distance constraints respectively. Finally, extensive experimental results on real protein data sets and time series verify the performance of the proposed algorithm.																	0924-669X	1573-7497				NOV	2020	50	11					4094	4116		10.1007/s10489-020-01778-1		JUL 2020											
J								Creating rule-based agents for artificial general intelligence using association rules mining	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Production rule-based systems; Association rules mining; Artificial general intelligence; Autonomous parking		In this paper, our focus is on using a rule-based approach to develop agents with artificial general intelligence. In rule-based systems, developing effective rules is a huge challenge, and coding rules for agents requires a large amount of manual work. Association rules mining (ARM) can be used for discovering specific rules from data sets and determining relationships between data sets. In this paper, we introduce a modified ARM method and use it to discover rules that analyse the surrounding environment and determine movements for an agent-guided vehicle that has been designed to achieve autonomous parking. The rules are created by our ARM-based method from training data gained during manual training in customised parking scenarios. In this system, data are represented in terms of fuzzy symbolic elements. We have tested our system by simulation in a virtual environment to demonstrate the effectiveness of this new approach.																	1868-8071	1868-808X															10.1007/s13042-020-01166-8		JUL 2020											
J								The uncertainty and explainability in object recognition	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Uncertainty in vision task; topological knowledge representation; explainability	SHAPE	In the object recognition task, due to changes in size, colour, illumination, position, viewing angle, and environmental background, great uncertainty is caused. The invariant recognition capability required to adapt to such diverse uncertainties has also become one of the most challenging goals of artificial intelligence, as only biometric systems can do this. The approach to dealing with uncertainty in the field of pattern recognition is nothing more than training with a large number of samples and powerful machine learning algorithms, and finally generating a classifier. Among them, knowledge representation has not only been ignored, but has even been considered redundant. But research from cognitive psychology and experimental psychology suggests that humans do not use such a huge mechanism. In contrast, we use more similar methods of symbolic artificial intelligence, such as representation, induction, reasoning, interpretation, and constraint propagation, to deal with uncertainty in object recognition. In this paper, the skeleton tree is used as the basic means to form the formal representation of the topological features and geometric features of the object, and based on the generalisation framework, the knowledge extraction of a small number of similar representations is used to form a generalised explicit representation of the knowledge about the object category. This generalised form representation can cope with uncertainties such as size, colour, shape, etc., and is significantly superior to current mainstream machine learning methods in terms of explainability and computational cost.																	0952-813X	1362-3079															10.1080/0952813X.2020.1785021		JUL 2020											
J								Disruption management decision model for VRPSDP under changes of customer distribution demand	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Changes of customer distribution demand; Disruption management; VRPSDP; HQBFO	ROUTING METHOD; ALGORITHM; STRATEGY	In order to solve the disruption of vehicle routing problem with simultaneous delivery and pickup (VRPSDP), a two-stage disruption management method based on the changes of customer demand is proposed. Firstly, a mathematical model of the problem is established to minimize the distribution cost and time window deviation, then to identify the disruption caused by the change of the original distribution scheme. Next, considering the strong global searching ability of bacteria foraging algorithm a hybrid quantum bacterial foraging scheduling algorithm (HQBFO) is designed combing with the advantages of superposition and parallelism of quantum computing. At last, the validity of the model and algorithm is tested by Solomon standard example and simulation test. The comparison with the global rescheduling scheme and additional vehicle scheme verifies the effectiveness of the proposed disruption management and improvement of the quantum algorithm performance.																	1868-5137	1868-5145															10.1007/s12652-020-02304-4		JUL 2020											
J								Variational Models for Color Image Correction Inspired by Visual Perception and Neuroscience	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Variational model; Differential geometry; Color image processing; Convex; nonconvex analysis; Visual neuroscience; Visual perception	ALTERNATING LINEARIZED MINIMIZATION; GEOMETRIC MODEL; RETINEX THEORY; NONCONVEX; OPTIMIZATION; ALGORITHMS; LIGHTNESS	Reproducing the perception of a real-world scene on a display device is a very challenging task which requires the understanding of the camera processing pipeline, the display process, and the way the human visual system processes the light it captures. Mathematical models based on psychophysical and physiological laws on color vision, named Retinex, provide efficient tools to handle degradations produced during the camera processing pipeline like the reduction of the contrast. In particular, Batard and Bertalmio (in J Math Imaging Vis 60(6):849-881, 2018) described some psychophysical laws on brightness perception as covariant derivatives, included them into a variational model, and observed that the quality of the color image correction is correlated with the accuracy of the vision model it includes. Based on this observation, we postulate that this model can be improved by including more accurate data on vision with a special attention on visual neuroscience here. Then, inspired by the presence of neurons responding to different visual attributes in the area V1 of the visual cortex as orientation, color or movement, to name a few, and horizontal connections modeling the interactions between those neurons, we construct two variational models to process both local (edges, textures) and global (contrast) features. This is an improvement with respect to the model of Batard and Bertalmio as the latter cannot process local and global features independently and simultaneously. Finally, we conduct experiments on color images which corroborate the improvement provided by the new models.																	0924-9907	1573-7683				NOV	2020	62	9					1173	1194		10.1007/s10851-020-00978-1		JUL 2020											
J								A real-time critical part detection for the blurred image of infrared reconnaissance balloon with boundary curvature feature analysis	JOURNAL OF REAL-TIME IMAGE PROCESSING										Real-time detection; Boundary-elements method; Partitioned gray-scale distribution; Infrared balloon image		Detecting and striking the critical part of blurred balloon image is an important approach to counterattack the reconnaissance balloons. The existing algorithms of the image target detection task are not able to achieve the high precision and real-time performance in the meanwhile since the critical part of the balloon is tiny, weak and not easy to be segmented. In this paper, a real-time algorithm based curvature feature in the polar coordinate system is proposed to detect the critical part of reconnaissance balloons. We divide the proposed method into three steps: the image is firstly subjected to a gray-scale projection by calculating third-order post-difference, then the balloon boundary is extracted in transformed polar coordinates, and finally the boundary curvature identifies the position of the critical part. The core strategy of the proposed method is to adopt the boundary features of the balloon instead of the general time-consuming image operations (e.g. region labeling, matching) to capture the target part. The experimental results show that the proposed method obtains high precision results with a real-time detection. Our proposed method achieves a processing speed of 200 frames per second on DSP (TMS320C6678) while a state-of-the-art detection precision (>93%), which overcomes the existing comparison algorithms.																	1861-8200	1861-8219															10.1007/s11554-020-00997-6		JUL 2020											
J								A Novel Method of Curve Fitting Based on Optimized Extreme Learning Machine	APPLIED ARTIFICIAL INTELLIGENCE											ALGORITHM	In this article, we present a new method based on extreme learning machine (ELM) algorithm for solving nonlinear curve fitting problems. Curve fitting is a computational problem in which we seek an underlying target function with a set of data points given. We proposed that the unknown target function is realized by an ELM with introducing an additional linear neuron to correct the localized behavior caused by Gaussian type neurons. The number of hidden layer neurons of ELM is a crucial factor to achieve a good performance. An evolutionary computation algorithm-particle swarm optimization (PSO) technique is applied to determine the optimal number of hidden nodes. Several numerical experiments with benchmark datasets, simulated spectral data and measured data from high energy physics experiments have been conducted to test the proposed method. Accurate fitting has been accomplished for various tough curve fitting tasks. Comparing with the results of other methods, the proposed method outperforms the traditional numerical-based technique. This work clearly demonstrates that the classical numerical analysis problem-curve fitting can be satisfactorily resolved via the approach of artificial intelligence.																	0883-9514	1087-6545				OCT 14	2020	34	12					849	865		10.1080/08839514.2020.1787677		JUL 2020											
J								Relationship quality and supply chain quality performance: The effect of supply chain integration in hotel industry	COMPUTATIONAL INTELLIGENCE										hotel supply chain; relationship quality; supply chain integration; supply chain quality performance	COMPETITIVE ADVANTAGE; FIRM PERFORMANCE; IMPACT; MANAGEMENT; CAPABILITIES; COMMITMENT; STRATEGY; MODEL	This study investigates the relationship between relationship quality and supply chain quality performance in hotel supply chain, through the mediating effect of supply chain integration. A questionnaire survey is used to collect data relating to the research hypotheses. Structural equation model technique is suited for our research goals, and the SmartPLS software is implemented to test the conceptual model. The results show that relationship quality has a direct and positive impact on supply chain quality performance; but after introducing mediating variable-supply chain integration, relationship quality indirectly affects supply chain quality performance through supply chain integration. This means that a good relationship quality can promote supply chain integration and ultimately improve supply chain quality performance.																	0824-7935	1467-8640															10.1111/coin.12379		JUL 2020											
J								Retyping of triple-negative breast cancer based on clustering method	EXPERT SYSTEMS										clustering; gene selection; retyping; triple-negative breast cancer	SUBTYPES; CONSENSUS	Triple-negative breast cancer is the worst prognosis in breast cancer, accounting for 10.0-20.8% of all breast cancers. Considering that triple-negative breast cancer has great heterogeneity and very poor prognosis, clinical medication guidance is in urgent need of a more detailed classification of breast cancer itself. Although many researchers have been dedicated to the clustering of triple-negative breast cancer and have found possible targets based on typing, their results are not closely related to the prognosis. This paper utilizes three clustering methods to retype the patient data with triple-negative breast cancer, and the results show that the triple-negative breast cancer data could be classified into two categories. Eight important genes and three important clinical factors related to the prognosis of two types of triple-negative breast cancer have been obtained. These genes have the following three characteristics: co-expression, differential expression and interaction. In terms of breast cancer control, the prognosis can be controlled as much as possible by regulating gene levels, which provides new directions and ideas for related research on breast cancer prognosis.																	0266-4720	1468-0394														e12583	10.1111/exsy.12583		JUL 2020											
J								Path Planning of Mobile Robot With Improved Ant Colony Algorithm and MDP to Produce Smooth Trajectory in Grid-Based Environment	FRONTIERS IN NEUROROBOTICS										mobile robot; ant colony algorithm; Markov decision process model; motion planning; obstacle avoidance	AVOIDANCE; ASTERISK; MODEL	This approach has been derived mainly to improve quality and efficiency of global path planning for a mobile robot with unknown static obstacle avoidance features in grid-based environment. The quality of the global path in terms of smoothness, path consistency and safety can affect the autonomous behavior of a robot. In this paper, the efficiency of Ant Colony Optimization (ACO) algorithm has improved with additional assistance of A* Multi-Directional algorithm. In the first part, A* Multi-directional algorithm starts to search in map and stores the best nodes area between start and destination with optimal heuristic value and that area of nodes has been chosen for path search by ACO to avoid blind search at initial iterations. The path obtained in grid-based environment consist of points in Cartesian coordinates connected through line segments with sharp bends. Therefore, Markov Decision Process (MDP) trajectory evaluation model is introduced with a novel reward policy to filter and reduce the sharpness in global path generated in grid environment. With arc-length parameterization, a curvilinear smooth route has been generated among filtered waypoints and produces consistency and smoothness in the global path. To achieve a comfort drive and safety for robot, lateral and longitudinal control has been utilized to form a set of optimal trajectories along the reference route, as well as, minimizing total cost. The total cost includes curvature, lateral and longitudinal coordinates constraints. Additionally, for collision detection, at every step the set of optimal local trajectories have been checked for any unexpected obstacle. The results have been verified through simulations in MATLAB compared with previous global path planning algorithms to differentiate the efficiency and quality of derived approach in different constraint environments.																	1662-5218					JUL 9	2020	14								44	10.3389/fnbot.2020.00044													
J								Ergonomic clusters and displaced affordances in early lithic technology	ADAPTIVE BEHAVIOR										Stone tool; 4E cognition; evolution; ergonomics; affordance	RAW-MATERIAL SELECTIVITY; STONE TOOL PRODUCTION; WEST TURKANA; COGNITIVE IMPLICATIONS; KANJERA SOUTH; OLDUVAI GORGE; EARLY HUMANS; APES VIEW; FLK WEST; OLDOWAN	Traditional typological, technical, and cognitive approaches to early stone tools have taken an implicit Cartesian stance concerning the nature of mind. In many cases, this has led to interpretations of early technology that overemphasize its human-like features. By eschewing an epistemic mediator, 4E approaches to cognition (embodied, embedded, enactive, and extended) are in a better position to make appropriate evaluations of early hominin technical cognition that emphasize its continuity with non-human primates and ground a description of the evolution of hominin technology. This essay takes some initial steps in that direction by shifting focus away from tool types and knapping patterns toward a description based on ergonomics and Gibsonian affordances. The analysis points to the evolutionary importance of two hitherto underappreciated aspects of hominin technical systems-the emergence of ergonomic clusters instantiated in artifact form and the development of displaced affordances.																	1059-7123	1741-2633														1059712320932333	10.1177/1059712320932333		JUL 2020											
J								Experimental capabilities and limitations of a position-based control algorithm for swarm robotics	ADAPTIVE BEHAVIOR										Collective motion; decentralized control; position-based models; self-organization; swarm robotics	MOTION	Achieving efficient and reliable self-organization in groups of autonomous robots is a fundamental challenge in swarm robotics. Even simple states of collective motion, such as group translation or rotation, require nontrivial algorithms, sensors, and actuators to be achieved in real-world scenarios. We study here the capabilities and limitations in controlling experimental robot swarms of a decentralized control algorithm that only requires information on the positions of neighboring agents, and not on their headings. Using swarms of e-Puck robots, we implement this algorithm in experiments and show its ability to converge to self-organized collective translation or rotation, starting from a state with random orientations. Through a simple analytical calculation, we also unveil an essential limitation of the algorithm that produces small persistent oscillations of the aligned state, related to its marginal stability. By comparing predictions and measurements, we compute the experimental noise distributions of the linear and angular robot speeds, showing that they are well described by Gaussian functions. We then implement simulations that model this noise by adding Gaussian random variables with the experimentally measured standard deviations. These simulations are performed for multiple parameter combinations and compared to experiments, showing that they provide good predictions for the expected speed and robustness of the self-organizing dynamics.																	1059-7123	1741-2633														UNSP 1059712320930418	10.1177/1059712320930418		JUL 2020											
J								Asynchronous framework with Reptile plus algorithm to meta learn partially observable Markov decision process	APPLIED INTELLIGENCE										Meta learning; Deep reinforcement learning; Partial observable Markov decision process; Asynchronous framework; Recurrent deep deterministic policy gradient		Meta-learning has recently received much attention in a wide variety of deep reinforcement learning (DRL). In non-meta-learning, we have to train a deep neural network as a controller to learn a specific control task from scratch using a large amount of data. This way of training has shown many limitations in handling different related tasks. Therefore, meta-learning on control domains becomes a powerful tool for transfer learning on related tasks. However, it is widely known that meta-learning requires massive computation and training time. This paper will propose a novel DRL framework, which is called HCGF-R2-DDPG (Hybrid CPU/GPU Framework for Reptile+ and Recurrent Deep Deterministic Policy Gradient). HCGF-R2-DDPG will integrate meta-learning into a general asynchronous training architecture. The proposed framework will allow utilising both CPU and GPU to boost the training speed for the meta network initialisation. We will evaluate HCGF-R2-DDPG on various Partially Observable Markov Decision Process (POMDP) domains.																	0924-669X	1573-7497				NOV	2020	50	11					4050	4062		10.1007/s10489-020-01748-7		JUL 2020											
J								Orientation constraints for Wi-Fi SLAM using signal strength gradients	AUTONOMOUS ROBOTS										Wi-Fi SLAM; WiFi SLAM; Orientation; Robust pose graph SLAM	INFERENCE; ALIGNMENT	We propose the signal strength gradient (SSG) orientation constraints for simultaneous localization and mapping (SLAM) using Wi-Fi received signal strength (RSS) measurements. We show that under certain circumstances, the relative orientation between nearby trajectory segments can be recovered from the cosine similarity between their SSGs. We then show how to obtain trajectory segments and self-consistent SSGs by jointly segmenting Wi-Fi measurements and odometry. Because SSG orientation constraints inevitably contain outliers, we also evaluate the effectiveness of robust SLAM backends on the proposed constraints. Experiments show that Wi-Fi SLAM using the proposed method can correctly estimate orientations given topologically incorrect initialization on trajectories with little to no overlapping sections.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1135	1146		10.1007/s10514-020-09914-z		JUL 2020											
J								Manipulation planning under changing external forces	AUTONOMOUS ROBOTS										Manipulation planning; Forceful human-robot collaboration; Task-oriented grasping	GRASP	This paper presents a planner that enables robots to manipulate objects under changing external forces. Particularly, we focus on the scenario where a human applies a sequence of forceful operations, e.g. cutting and drilling, on an object that is held by a robot. The planner produces an efficient manipulation plan by choosing stable grasps on the object, by intelligently deciding when the robot should change its grasp on the object as the external forces change, and by choosing subsequent grasps such that they minimize the number of regrasps required in the long-term. Furthermore, as it switches from one grasp to the other, the planner solves the bimanual regrasping in the air by using an alternating sequence of bimanual and unimanual grasps. We also present a conic formulation to address force uncertainties inherent in human-applied external forces, using which the planner can robustly assess the stability of a grasp configuration without sacrificing planning efficiency. We provide a planner implementation on a dual-arm robot and present a variety of simulated and real human-robot experiments to show the performance of our planner.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1249	1269		10.1007/s10514-020-09930-z		JUL 2020											
J								Mobile intelligent terminal speaker identification for real-time monitoring system of sports training	EVOLUTIONARY INTELLIGENCE										Intelligent terminal; Mobile devices; Speaker identification; Evolutionary intelligence; Sports training		Mobile intelligent terminal speaker identification for real-time monitoring system of sports training is presented in this manuscript. The purpose of speaker recognition is to use speaker's voice characteristics to identify the speaker's identity. No two people in the world have exactly the same voice characteristics, because from the perspective of the human physiological structure, different people's vocal shapes, throat sizes, and other factors related to pronunciation are different, which makes it possible to recognize speech by speech. From the technology level, the proposed framework contains the four core aspects, namely: speaker identification, motion gesture detection, heart rate detection and breath detection. In the information collection part, we adopt the ZigBee + WIFI wireless transmission, GPRS data transmission and Bluetooth communication joint framework to capture the accurate data to rrealize the dual-network dual-pass function, the TD-SCDMA receiver must also work at the same time when the GSM radio module of the terminal is working. In the speaker identification part, we propose the novel deep neural network framework and revise the architecture. The experiment compared with the other state-of-the-art models have proven the robustness of the proposed framework.																	1864-5909	1864-5917															10.1007/s12065-020-00452-2		JUL 2020											
J								Efficient image segmentation through 2D histograms and an improved owl search algorithm	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Metaheuristic algorithms; Owl search algorithm; Image segmentation; Multilevel thresholding; 2D Histogram; Renyi entropy	MULTILEVEL THRESHOLDING METHOD; MINIMUM CROSS-ENTROPY; OPTIMIZATION ALGORITHM; RENYI; EVOLUTIONARY	Optimization is used in different fields of engineering to solve complex problems. In image processing, multilevel thresholding requires to find the optimal configuration of thresholds to obtain accurate segmented images. In this case, the use of two-dimensional histograms is helpful because they permit us to combine information from the image preserving different features. This paper introduces a new method for multilevel image thresholding segmentation based on the improved version of the owl search algorithm (iOSA) and 2D histograms. The performance of the iOSA is enhanced with the inclusion of a new strategy in the optimization process. Moreover, in the initialization step, it is applied the opposition-based learning. Meanwhile, the 2D histograms permit to maintain more information of the image. Considering such modifications, the iOSA performs a better exploration of the search space during the early iterations, preserving the exploitation of the prominent regions using a self-adaptive variable. The iOSA is employed to allocate the optimal threshold values that segment the image by using the 2D Renyi entropy as an objective function. To test the efficiency of the iOSA, a set of experiments were performed which validate the quality of the segmentation and evaluate the optimization results efficacy. Moreover, to prove that the iOSA is a promising alternative for optimization and image processing problems, statistical tests and analyses were also conducted.																	1868-8071	1868-808X															10.1007/s13042-020-01161-z		JUL 2020											
J								Multi-agent reinforcement learning for redundant robot control in task-space	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-agent; Reinforcement learning; Redundant robot	TRACKING CONTROL; TRAJECTORY TRACKING; INVERSE KINEMATICS; MANIPULATORS	Task-space control needs the inverse kinematics solution or Jacobian matrix for the transformation from task space to joint space. However, they are not always available for redundant robots because there are more joint degrees-of-freedom than Cartesian degrees-of-freedom. Intelligent learning methods, such as neural networks (NN) and reinforcement learning (RL) can learn the inverse kinematics solution. However, NN needs big data and classical RL is not suitable for multi-link robots controlled in task space. In this paper, we propose a fully cooperative multi-agent reinforcement learning (MARL) to solve the kinematic problem of redundant robots. Each joint of the robot is regarded as one agent. The fully cooperative MARL uses a kinematic learning to avoid function approximators and large learning space. The convergence property of the proposed MARL is analyzed. The experimental results show that our MARL is much more better compared with the classic methods such as Jacobian-based methods and neural networks.																	1868-8071	1868-808X															10.1007/s13042-020-01167-7		JUL 2020											
J								High energy and spectral efficiency analysis for CRAHN based spectrum aggregation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ad-hoc network; Cognitive radio; Diversity techniques; Gaming theory; Stackelberg	COGNITIVE RADIO NETWORKS	The current work proposes to conquer the issue looked by the ordinary spectrum aggregated CRAHN strategy. Co-operative routing and range spectrum aggregation are two promising strategies for cognitive radio ad-hoc networks (CRAHNS). Propose a range spectrum collection based helpful routing protocol, named as routing protocol. The basic objective of spectrum aggregation based agreeable routing protocol is to give higher imperatives profitability, upgrade throughput, and diminishes framework delay for CRAHNS. In this work Stackelberg gaming hypothesis will be actualized to accomplish high throughput and decreased delay, and this likewise improves the spectrum and energy effectiveness. Gaming method will be conducted between different channels. After doing the game, winner node among each channel is shortlisted. Now the transmission will be towards the Base station, yet again the gaming progression is done, and among the shortlisted nodes the node with high strength is being denoted and this is considered as the path with high transmitting energy and efficiency. By choosing this route, we can able to increase the effectiveness of the system. The throughput and end to end delay can be derived by using various diversity techniques. The different output power can be estimated. Over 97% efficiency achieved by using Stackelberg gaming theory.																	1868-5137	1868-5145															10.1007/s12652-020-02281-8		JUL 2020											
J								Comparative analysis of spatial interpolation with climatic changes using inverse distance method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ArcGIS; Climatic changes; Disaster; Environmental variable; Interpolation; Inverse distance method; Rainfall; Spatial parameters	PREDICTION	An ecological change on the planet the most flighty parameter is precipitation. It causes minor and real changes in the climatic changes. The time and space analyzed on the data and the spatial and temporal patterns are introduced in the required manner. For the unpredictable data to be predicted by using the spatial interpolation techniques. The major spatial interpolation methods are categorized based on the simple and complex mathematical modeling. Those studies involve interpolation techniques such as ordinary Krigging, inverse distance weighted (IDW), Spline, etc., Most of the techniques studied the hydrological modeling and deliver hydro mapping (Younghun Jung and Venkatesh Merwade2015). Various techniques are used for the hydrological mapping. This study involved comparing the statistical interpolation with precipitation and temperature. The cumulative values in the dataset taken from the year 2001 to the year 2013 used in the study. The monthly and the yearly mean square value calculated using the spatial interpolation techniques (Dhamodarn and Shruthi2016). The main objective of the study is to find the topographical features and provides high-resolution climate maps using mathematical modeling. In the result of comparing the above interpolation methods in the result will produce the high-resolution climate maps and generate the geographical patterns. Based on the study will produce climate change in the environment like disaster, flood, landslide. The comparative study of interpolation methods and mean variation can be resulted and used for the further implementation of spatial climatic condition and decision support system to the society. The area has undergone the study is Adyar river, situated in latitude 13.0012 degrees N, Longitude 80.2565 degrees E. Inverse Distance Weighted interpolation methods are analyzed by using the Geostatistical tool ArcGIS.																	1868-5137	1868-5145															10.1007/s12652-020-02296-1		JUL 2020											
J								Design of intelligent surveillance system based on wireless ad-hoc network under special conditions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent surveillance; Object detection; Convolutional neural network; Ad-hoc		For oil fields, shooting target range, military forbidden zones and other special complex scenarios, it is not only difficult to deploy communication infrastructure because of its remote location and difficult environment, but also difficult to transmit surveillance video images to command stations in real time through centralized communication networks. Furthermore, there is a waste of bandwidth resources and the use of wireless ad hoc networks to transmit surveillance video. Therefore, we aim at designing and implementing a real-time intelligent surveillance system based on wireless ad-hoc networks in special scenes in this project. The system is supposed to capture the video frames of the monitored area and perform automatic intrusion detection. Only when an intrusion is detected, the video frame is encoded and transmitted together with the alarm message to the command center. An improved Yolov2 algorithm to achieve real time object detection on a computationally limited platform was proposed first. A wireless ad-hoc communication system was designed and implemented on the basic of the Tactical Targeting Network Technology (TTNT) Data Link to transmit surveillance video stream, control signaling and alarm message. Then the extensive experiments were conducted to evaluate the system in terms of detection accuracy and communication performance. The experiments results demonstrate the effectiveness and accuracy of the proposed system, the size of the model is only 7.6% of Yolov2 and the detection speed is increased 4 times. This design transforms long-time data transmission into burst data transmission, thus saving bandwidth resources and improving bit error rate of 96.9%. It is of great significance for the future application of artificial intelligence and machine learning in the field of communication																	1868-5137	1868-5145															10.1007/s12652-020-02140-6		JUL 2020											
J								A novel multi-modal medical image fusion algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										NSCT; Medical image fusion; PCNN; Computed tomography	TRANSFORM	In order to improve the contrast of image fusion and highlight the unique characteristics of medical images, a multi-modal medical image fusion algorithm in the framework of non-subsampled contourlet transform (NSCT) is proposed in this paper. Firstly, the computed tomography images and magnetic resonance image are decomposed into low- and high-frequency sub-bands through the NSCT of multi-scale geometric transformation; secondly, for the low-frequency sub-band, the local area standard deviation method is selected or the fusion, while for the high-frequency sub-band, an adaptive pulse coupling neural network model is constructed and the fusion rules are set by the cumulative ignition times of iterative operation in the network; finally, the fusion image is obtained through image reconstruction. Experimental results show that the fusion results of the algorithm in this paper can improve the image fusion quality significantly and it has certain advantages in both visual effects and objective evaluation indexes, which provides a more reliable basis for clinical diagnosis and treatment of diseases.																	1868-5137	1868-5145															10.1007/s12652-020-02293-4		JUL 2020											
J								Link prediction in dynamic networks using time-aware network embedding and time series forecasting	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dynamic networks; Link prediction; Network embedding; Neural networks	COMPLEX NETWORKS; MODELS	As most real-world networks evolve over time, link prediction over such dynamic networks has become a challenging issue. Recent researches focus towards network embedding to improve the performance of link prediction task. Most of the network embedding methods are only applicable to static networks and therefore cannot capture the temporal variations of dynamic networks. In this work, we propose a time-aware network embedding method which generates node embeddings by capturing the temporal dynamics of evolving networks. Unlike existing works which use deep architectures, we design an evolving skip-gram architecture to create dynamic node embeddings. We use the node embedding similarities between consecutive snapshots to construct a univariate time series of node similarities. Further, we use times series forecasting using auto regressive integrated moving average (ARIMA) model to predict the future links. We conduct experiments using dynamic network snapshot datasets from various domains and demonstrate the advantages of our system compared to other state-of-the-art methods. We show that, combining network embedding with time series forecasting methods can be an efficient solution to improve the quality of link prediction in dynamic networks.																	1868-5137	1868-5145															10.1007/s12652-020-02289-0		JUL 2020											
J								A secure three factor based authentication scheme for health care systems using IoT enabled devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; Biometric; IoT; BAN Logic; Elliptic curve cryptography; ROR; AVISPA	BIOMETRICS-BASED AUTHENTICATION; KEY AGREEMENT PROTOCOL; FUZZY EXTRACTORS; EFFICIENT; PASSWORD; CRYPTANALYSIS; LIGHTWEIGHT; NETWORKS	In recent years, the Internet of Things (IoT) has gained increasing popularity due to the usage of Internet-enabled devices. However, Internet-enabled devices, also known as smart devices, share the information using an insecure channel, i.e., the Internet. Hence, the security and privacy of shared information remain the biggest concern. To ensure both security and privacy, many smart card based and biometric based schemes have been proposed for different Internet-based applications. Telecare Medical Information System (TMIS) is such an application which makes medical treatment easier by interacting with the patient and doctors. However, the transmission of the patient's private information over an insecure channel is prone to several attacks. In order to protect the medical privacy of the patient and the reliability of the system, both the patient and medical server should be mutually authenticated. In this paper, we propose a three factor-based authentication scheme for health care system using IoT enabled devices (TFASH) that are secure and more efficient than other relevant schemes. We use Elliptic Curve Cryptography (ECC) for the scheme due to its smaller key size and high level of security. The session key security and the mutual authentication of the TFASH scheme have been proved using Real-Or-Random (ROR) model and Burrows-Abadi-Needham (BAN) logic. The simulation result of the proposed scheme shows that the scheme is safe under the OFMC and CLAtSe models. Moreover, compared to the existing schemes, the TFASH scheme provides better communicational and computational cost, which makes it suitable for practical use.																	1868-5137	1868-5145															10.1007/s12652-020-02213-6		JUL 2020											
J								Trust aware similarity-based source routing to ensure effective communication using game-theoretic approach in VANETs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Game theory; Social networks; Trust; Vehicular networks		In VANETs, vehicles need to rely on other vehicles for any real-time applications. Therefore the cooperation of vehicles is needed for relaying the messages in successful manner. Most often this is not possible since there is a possibility for some vehicle to act selfishly and thereby they fail to forward messages. Moreover, during the routing process, the prior information about the neighbours will help in the selection of best data forwarding nodes. In this paper, we have proposed Similarity-based Trustworthy Routing algorithm which incorporates social features for performing trustworthy routing by choosing the best forwarder. To further improve the updating process of trust value, we have incorporated two basic approaches, one based on direct Acknowledgment during Encounter Strategy (AES) and the other one based on Game-theoretic Broadcasting Strategy (GTBS). The adaption of AES and GTBS enhances the security of the data traffic in VANETs by intelligently identifying the malicious nodes. This work identifies and overcomes the vulnerabilities of different kind of attacks with minimum overhead in terms of transmission. Moreover, the solutions provide significant performance improvement in terms of packet delivery ratio even in the presence of misbehaving nodes. Our results show, how our methods outperform others, by comparing with other existing approaches like dLife and Epidemic Routing protocols.																	1868-5137	1868-5145															10.1007/s12652-020-02306-2		JUL 2020											
J								A hierarchical parallel fusion framework for egocentric ADL recognition based on discernment frame partitioning and belief coarsening	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Egocentric activity recognition; Activity of daily living (ADL); Multimodal fusion; Hierarchical fusion; Dezert-Smarandache theory; Belief coarsening		Recently, egocentric activity recognition has become a major research area in pattern recognition and artificial intelligence due to its high significance in potential applications in medical care, rehabilitation, smart home/office, etc. In this study, we develop a hierarchical parallel multimodal fusion framework for the recognition of egocentric activities in daily living (ADL). This framework uses the Dezert-Smarandache theory and is constructed around three modalities: location, motion and vision data from a wearable hybrid sensor system. The reciprocal distance and a trained support vector machine classifier are used to form the basic belief assignments (BBA) of location and motion. For vision data composed of egocentric photo streams, a well-trained convolutional neural network is utilized to produce a set of textual tags and the entropy-based statistics for these tags are used to construct the vision BBA. Discernment partitioning and belief coarsening theory are adopted for the hierarchical fusion of the three BBA functions from different ADL levels. Experimental results show that the recognition accuracy of the proposed fusion method was significantly higher than that of the methods based on single modality or modality combinations when our method was applied to real-life multimodal egocentric activity datasets. In addition, our method also achieved higher adaptability and scalability.																	1868-5137	1868-5145															10.1007/s12652-020-02241-2		JUL 2020											
J								Sharing gaze rays for visual target identification tasks in collaborative augmented reality	JOURNAL ON MULTIMODAL USER INTERFACES										Shared gaze; Eye tracking; Eye tracking errors; Collaborative augmented reality; Target identification	TRACKING; SEE	Augmented reality (AR) technologies provide a shared platform for users to collaborate in a physical context involving both real and virtual content. To enhance the quality of interaction between AR users, researchers have proposed augmenting users' interpersonal space with embodied cues such as their gaze direction. While beneficial in achieving improved interpersonal spatial communication, suchshared gaze environmentssuffer from multiple types of errors related to eye tracking and networking, that can reduce objective performance and subjective experience. In this paper, we present a human-subjects study to understand the impact ofaccuracy,precision,latency, anddropoutbased errors on users' performance when using shared gaze cues to identify a target among a crowd of people. We simulated varying amounts of errors and the target distances and measured participants' objective performance through their response time and error rate, and their subjective experience and cognitive load through questionnaires. We found significant differences suggesting that the simulated error levels had stronger effects on participants' performance than target distance with accuracy and latency having a high impact on participants' error rate. We also observed that participants assessed their own performance as lower than it objectively was. We discuss implications for practical shared gaze applications and we present a multi-user prototype system.																	1783-7677	1783-8738				DEC	2020	14	4			SI		353	371		10.1007/s12193-020-00330-2		JUL 2020											
J								Virtual intimacy in human-embodied conversational agent interactions: the influence of multimodality on its perception	JOURNAL ON MULTIMODAL USER INTERFACES										Virtual intimacy; Human-agent interaction; Multimodal communication; Nonverbal behaviors; Attention	COGNITIVE LOAD; SOCIAL CUES; EXPRESSIONS; RAPPORT	Interacting with an embodied conversational agent (ECA) in a professional context addresses social considerations to satisfy customer-relationship. This paper presents an experimental study about the perception of virtual intimacy in human-ECA interactions. We explore how an ECA's multimodal communication affects our perception of virtual intimacy. To this end, we developed a virtual Tourism Information counselor capable of exhibiting verbal and nonverbal intimate behaviors according to several modalities (voice, chatbox, both media), and we built a corpus of videos showing interactions between the agent and a human tourist. We interrogated observers about their perception of the agent's level of intimacy. Our results confirm the human ability to perceive intimacy in an ECA displaying multimodal behaviors, although the contribution of nonverbal communication remains unclear. Our study suggests that using voice channel increases the perception of virtual intimacy and offers further evidence that human-inspired design of ECAs is needed. Finally, we demonstrate that intimate cues do not disturb the comprehension of task-related information and are valuable for an attentional focus on the agent's animation. We discuss the concept of virtual intimacy in relation to interpersonal intimacy, and we question its perception in terms of attentional mechanisms.																	1783-7677	1783-8738															10.1007/s12193-020-00337-9		JUL 2020											
J								"Let me explain!": exploring the potential of virtual agents in explainable AI interaction design	JOURNAL ON MULTIMODAL USER INTERFACES										Explainable artificial intelligence; Interpretable artificial intelligence; Virtual agents; Human-agent interaction; Deep learning; Trust	AUTOMATION; TRUST	While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user's trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants' feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.																	1783-7677	1783-8738															10.1007/s12193-020-00332-0		JUL 2020											
J								A systematic survey on collaborator finding systems in scientific social networks	KNOWLEDGE AND INFORMATION SYSTEMS										Social networks; Collaborator fining; Expert finding	OF-THE-ART; LINK-PREDICTION; RECOMMENDER SYSTEMS; RANDOM-WALK; IMPACT; RESEARCHERS; ALGORITHM; INDEX; TRUST	The increasing number of researchers and scientists participating in online communities has induced big challenges for users who are looking for researchers who are interested. As a result, finding potential collaborators among the huge amount of online information is going to be even much more important in the future. Collaborator recommendation is a kind of expert recommendation in scientific fields. A number of published papers have proposed new algorithms for an expert or a collaborator finding and tacking a narrower point of view. For instance, some of these papers have particularly considered a collaborator finding problem. New scientific social networks, such as ResearchGate, Academia, Mendeley, and so on, have provided some facilities to their users for finding new collaborators. In this paper, first of all, we review proposed models for an expert and a collaborator finding in scientific and academic social networks in a systematic manner. Next, collaborator finding facilities in online scientific social networks are evaluated. Finally, the defects and open challenges of the models are looked into and some propositions for the future works are presented.																	0219-1377	0219-3116				OCT	2020	62	10					3837	3879		10.1007/s10115-020-01483-y		JUL 2020											
J								Node influence-based label propagation algorithm for semi-supervised learning	NEURAL COMPUTING & APPLICATIONS										Semi-supervised classification; Label propagation; Node influence; Propagation sequence; Metric fusion	GRAPH CONSTRUCTION; CLASSIFICATION; ROBUST	Graph-based semi-supervised learning (GSSL) has received more and more attention due to its efficiency and accuracy. Label propagation is a critical step in GSSL that propagates label information to unlabeled data through the structure of graph. However, the traditional label propagation algorithms treat all unlabeled samples as equivalent and blindly propagate label information to all neighbors without considering their reliabilities. In this case, some unreliable samples may mislead the process of label propagation, thus greatly reducing the accuracy of classification. In order to solve this problem, this paper proposes a novel label propagation algorithm called node influence-based label propagation (NILP). Based on the structure of graph, the NILP algorithm measures the influences of nodes by calculating their degrees and local densities. In the process of label propagation, the label information is preferentially transmitted to the influential neighbors to control the propagation sequence and prevent wrong propagation. Moreover, our algorithm improves the transition matrix by integrating label information and feature information. The experimental results on both synthetic and real-world benchmark datasets show that the proposed method is superior to some existing label propagation algorithms. Especially when the number of labeled samples is very small, the advantage of NILP algorithm is more obvious.																	0941-0643	1433-3058															10.1007/s00521-020-05078-0		JUL 2020											
J								Attention-based deep convolutional neural network for spectral efficiency optimization in MIMO systems	NEURAL COMPUTING & APPLICATIONS										Attention mechanism; Cognitive radio; Convolutional neural network; Massive MIMO system; Spectral efficiency optimization	COGNITIVE RADIO; RESOURCE-ALLOCATION; MANAGEMENT; ALGORITHM; POWER	Spectral efficiency (SE) optimization in massive multiple input multiple output (MIMO) antenna cognitive systems is a challenge originated from the coexistence restrictions. Traditional power allocation can optimize the SE; however, involving deep learning can meet real-time and fairness processing requirements. In unfair allocation problem, all power is possibly assigned to one or few antennas of a particular user. In this paper, we build a mathematical optimization model considering the fairness problem such that SE is optimized for all users. To implement the model, we propose an attention-based convolutional neural network (Att-CNN), where h(0) and h(k) (i.e., cross-interference and direct channels) attention mechanisms are used to improve the SE. The convolutional neural network is applied to decrease the floating point operations (FLOPs) and number of network parameters. We conducted experiments from these aspects: Fair antenna power allocation, power allocation performance and computational performance. Heat maps with different interference thresholds show the fair allocation for all users. Analyses of SE validate the superiority of the Att-CNN compared with the equal power allocation and fully connected neural network (FNN) schemes. The analyses of the FLOPs and number of parameters show the superiority of the Att-CNN over the FNN.																	0941-0643	1433-3058															10.1007/s00521-020-05142-9		JUL 2020											
J								A combined model based on SSA, neural networks, and LSSVM for short-term electric load and price forecasting	NEURAL COMPUTING & APPLICATIONS										Power load and price forecasting; Jordan neural network; ESN network; LSSVM; Singular spectrum analysis	ECHO STATE NETWORK; HYBRID ALGORITHM; DEMAND; CLASSIFICATION; ENERGY	Electricity, a kind of clean energy, has been widely used in people's production and daily life. However, it is very difficult to estimate the electricity energy production in advance and store the rest of the electric energy due to the climate, environment, population and other factors. Based on data preprocessing and artificial intelligence optimization algorithm, this paper introduces a combined forecasting method. The proposed method contains six individual methods, and each individual method has its own usage. Singular spectrum analysis (SSA) is adopted to reduce noise from the original data; three individual forecasting methods, Jordan neural network, the echo state network, least squares support vector machine, are applied to obtain the intermediate forecasting results; two optimization algorithms, particle swarm optimization and simulated annealing, are used to optimize the parameters of the combined model. This paper not only validates the superiority of the combined model compared to the single predictive model through the simulation experiments of power load data and electricity price data. The mean absolute percent error (MAPE) of the combined power load and electricity price forecast results are 1.14% and 7.58%, respectively, which are higher than the MAPE error of the corresponding single models prediction results. It has also been verified that the process of eliminating noise by the SSA plays a positive role in the accuracy of the combined forecasting model. In addition, two series of experiments on the power load data lead to two very interesting conclusions. One of the conclusions is that as the size of the test data increases, the prediction accuracy of the model decreases; the other is that the predicted result calculated through the optimized combined weight is better than the combined result calculated using the average weight, and the average weight is used. Weighted combination does not improve the prediction accuracy of a single model.																	0941-0643	1433-3058															10.1007/s00521-020-05113-0		JUL 2020											
J								An Efficient Algorithm Combining Spectral Clustering with Feature Selection	NEURAL PROCESSING LETTERS										Spectral clustering; Sparse learning Locally preserved projection; kNN		Traditional clustering algorithms have some limitations, which are sensitive to noise and mostly applicable to convex data sets. To solve these problems, the paper proposes a novel algorithm combining spectral clustering with feature selection. Specifically, the loss item is marked with a root that can reduce the deviation value then improve the robustness of the model. And in the algorithm optimization, there is one parameter is represented by other known parameters, which can reduce the time of parameter adjustment. Then, the regular term l(2,p)-norm is applied to reduce the influence of noise and redundant features and prevent the model from overfitting. Finally, Laplace matrix is constructed by kNN algorithm which is used to learn subspace and to preserve the local structure among samples, and the data after dimension reduction is used to spectral clustering. Experimental analysis on 10 benchmark datasets show that the proposed algorithm is more outperformed than the algorithms of the state-of-the-art.																	1370-4621	1573-773X															10.1007/s11061-020-10297-6		JUL 2020											
J								Neutrosophic linear programming using possibilistic mean	SOFT COMPUTING										Neutrosophic set (NS); Neutrosophic number; Single-valued neutrosophic set (SVNS); Alpha cut; Beta cut; Gamma cut; Possibilistic mean; Possibility mean; Neutrosophic number linear programming (NNLP); Neutrosophic linear programming (NNLP); Neutrosophic optimization	DECISION-MAKING; FUZZY-SETS; AGGREGATION OPERATORS; INTERVAL; OPTIMIZATION; VARIANCE; NUMBERS; DESIGN	The paper discusses the concept of fuzzy set theory, interval-valued fuzzy set, intuitionistic fuzzy set, interval-valued intuitionistic fuzzy set, neutrosophic set and its operational laws. The paper presents the alpha, beta, gamma-cut of single-valued triangular neutrosophic numbers and introduces the arithmetic operations of triangular neutrosophic numbers using alpha, beta, gamma-cut. Then, possibilistic mean of truth membership function, indeterminacy membership function and falsity membership function is defined. The proposed approach converts each triangular neutrosophic number in linear programming problem to weighted value using possibilistic mean to determine the crisp linear programming problem. The proposed approach also considers the risk attitude of expert while deciding the parameters of linear programming model.																	1432-7643	1433-7479				NOV	2020	24	22					16847	16867		10.1007/s00500-020-04980-y		JUL 2020											
J								Rigorous reduction of partial shading condition in grid connected solar PV system using discrete time-based PSO controller	SOFT COMPUTING										Photovoltaic; Solar; Partial shading; Harmonics; PSO controller; Discrete time model	POWER POINT TRACKING	Solar industry has seen tremendous development over the past decade and in particular, the PV (Photovoltaic) system which has an imperative advancement almost in all fields of science. Envisaging various faults in the PV system will significantly enhance the efficiency, reliability, and life of the PV system. The main vulnerable element in the PV system which is subjected under different weather condition causes total damage to the system. Proper monitoring and maintenance are needed to increase the life of the system. Several investigations were made so far to predict the faults in the PV system such as the Visual method, the Thermal method, the Electrical detection method, the Machine learning techniques, the Arc fault detection technique and the Protection device-based techniques were used generally. But more effective fault diagnosis techniques are required for PV arrays. The proposes a novel method for reducing the partial shading condition in solar PV system connected to a grid which consists of a discrete time-based particle swarm optimization (PSO) controller that controls the irradiation or partial shading as well as any short circuits in PV cell. Hence, this proposed work enhances with producing efficient energy by achieving high predictive accuracy of about 99%, high efficiency of about 98.9% and low THD (0.9) under partial shading conditions as well as harmonics.																	1432-7643	1433-7479															10.1007/s00500-020-05109-x		JUL 2020											
J								Grey Wolf optimization-Elman neural network model for stock price prediction	SOFT COMPUTING										Average relative variance; Bio-inspired algorithm; Elman neural network; Grey Wolf optimization; Stock prediction		Over the past two decades, assessing future price of stock market has been a very active area of research in financial world. Stock price always fluctuates due to many variables. Thus, an accurate prediction of stock price can be considered as a tough task. This study intends to design an efficient model for predicting future price of stock market using technical indicators derived from historical data and natural inspired algorithm. The model adopts Elman neural network (ENN) because of its ability to memorize the past information, which is suitable for solving stock problems. Trial and error-based method is widely used to determine the parameters of ENN. It is a time-consuming task. To address such an issue, this study employs Grey Wolf optimization (GWO) algorithm to optimize the parameters of ENN. Optimized ENN is utilized to predict the future price of stock data in 1 day advance. To evaluate the prediction efficiency, proposed model is tested on NYSE and NASDAQ stock data. The efficacy of the proposed model is compared with other benchmark models such as FPA-ELM, PSO-MLP, PSOElman, CSO-ARMA and GA-LSTM to prove its superiority. Results demonstrated that the GWO-ENN model provides accurate prediction for 1 day ahead prediction and outperforms the benchmark models taken for comparison.																	1432-7643	1433-7479															10.1007/s00500-020-05174-2		JUL 2020											
J								Curve approximation by adaptive neighborhood simulated annealing and piecewise Bezier curves	SOFT COMPUTING										Piecewise Bezier curve; Curve approximation; Adaptive neighborhood simulated annealing; Multi-objective simulated annealing	ELECTRICAL-IMPEDANCE TOMOGRAPHY; OPTIMIZATION ALGORITHM; INTERPOLATION; CONTAINERS; PLACEMENT; SPLINES	The curve approximation problem is widely researched in CAD/CAM and geometric modelling. The problem consists in determining an approximating curve from a given sequence of points. The usual approach is the minimization of the discrepancy between the approximating curve and the given sequence of points. However, the minimization of just the discrepancy leads to the overfitting problem, in which the solution is not unique. A new approach is proposed to overcome this problem, in which the length of the approximating curve is used as a regularization increasing the algorithm stability. Another new proposal is the discrepancy determination, in which a method that has the best ratio between accuracy and processing time is proposed. A new simulated annealing (SA) approach is used to minimize the problem, in which the next candidate is determined by a probability distribution controlled by the crystallization factor. The crystallization factor is low for higher temperatures ensuring the exploration of the domain. The crystallization factor is high for lower temperatures, corresponding the refinement phase of the SA. The approximating curve is represented as a piecewise cubic Bezier curve, which is a sequence of several connected cubic Bezier curves. The piecewise Bezier curve supports a new proposed data structure that improves the proposed algorithm. A comparison is also made between the used single-objective SA and the AMOSA multi-objective SA. The results showed that the proposed single-objective SA finds a solution which is not dominated by the Pareto front determined by AMOSA. The results also showed that the regularization stabilized the algorithm, in which the increase in parameters does not lead to the overfitting problem. The proposed algorithm can process even complex curves with self-intersections and higher curvature.																	1432-7643	1433-7479															10.1007/s00500-020-05114-0		JUL 2020											
J								A comprehensive group decision-making method with interval-valued intuitionistic fuzzy preference relations	SOFT COMPUTING										Group decision making; Interval-valued intuitionistic fuzzy preference relation; Additive consistency; Cross entropy	MODELS; PRIORITIZATION; CONSISTENCY; ALGORITHM; WEIGHTS; AHP	This paper investigates the group decision making (GDM) with interval-valued intuitionistic fuzzy (IVIF) preference relations (IVIFPRs). Considering decision maker's (DM's) risk attitude, the new score and accuracy functions for an IVIF value (IVIFV) are defined and a new order relation is proposed to rank IVIFVs. By transforming an IVIFPR into the direct and indirect IFPRs, a new additive consistency of IVIFPR is defined considering the uncertainty and ambiguity. Combining the direct IFPRs and the indirect IFPRs extracted from the IVIFPR, an algorithm is designed to determine the comprehensive IVIF priority weights of IVIFPR. For GDM with IVIFPRs, a multi-objective programming model is constructed to derive DMs' weights by combining TOPSIS and cross entropy, which is solved by Lagrange function method. Based on the determination of DMs' weights, the determination of priority weights and the new order relation of IVIFVs, a comprehensive method is proposed for GDM with IVIFPRs. Finally, a ventilation system selection example is analyzed to verify the effectiveness of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05145-7		JUL 2020											
J								Correlation analysis of aeroengine operation monitoring using deep learning	SOFT COMPUTING										Artificial neural network; BP algorithm; Predictive performance; Aeroengine; Deep learning	ARTIFICIAL NEURAL-NETWORK; MODEL; PREDICTION	To analyze the reliability of aeroengine more accurately, based on the analysis of operation reliability and complex reliability, the deep learning method is adopted to deal with the nonlinear and time-varying problems between the state parameters and operation reliability of aeroengine, and the condition monitoring method and deep learning of aeroengine are discussed. The results show that, based on the deep learning integrated network, the remaining useful life of aeroengine is predicted, and the key parameters of aeroengine are fitted and predicted by the back propagation (BP) algorithm. The artificial neural network method is used to predict the aeroengine parameters. For the collected aeroengine monitoring parameters, the greedy layer by layer training algorithm is used to mine the relationship between the parameters, extract the evaluation features, and evaluate the performance degradation, which verify the statistical significance and robustness of the conclusions. The proposed algorithm is more accurate and robust than the results of back BP neural network and support vector machine. It can prevent the over-fitting of small samples in aeroengine condition monitoring and further improve its nonlinear processing and generalization ability.																	1432-7643	1433-7479															10.1007/s00500-020-05166-2		JUL 2020											
J								Predicting course achievement of university students based on their procrastination behaviour on Moodle	SOFT COMPUTING										Predication of course achievement; Prediction of grade; Higher education; Educational data mining; Procrastination behavior; Online learning; Moodle	EDUCATIONAL DATA; ACADEMIC PROCRASTINATION; PERFORMANCE; RISK; MODELS; SYSTEM	A significant amount of educational data mining (EDM) research consider students' past performance or non-academic factors to build predictive models, paying less attention to students' activity data. While procrastination has been found as a crucial indicator which negatively affects performance of students, no research has investigated this underlying factor in predicting achievement of students in online courses. In this study, we aim to predict students' course achievement in Moodle through their procrastination behaviour using their homework submission data. We first build feature vectors of students' procrastination tendencies by considering their active, inactive, and spare time for homework, along with homework grades. Accordingly, we then use clustering and classification methods to optimally sort and put students into various categories of course achievement. We use a Moodle course from the University of Tartu in Estonia which includes 242 students to assess the efficacy of our proposed approach. Our findings show that our approach successfully predicts course achievement for students through their procrastination behaviour with precision and accuracy of 87% and 84% with L-SVM outperforming other classification methods. Furthermore, we found that students who procrastinate more are less successful and are potentially going to do poorly in a course, leading to lower achievement in courses. Finally, our results show that it is viable to use a less complex approach that is easy to implement, interpret, and use by practitioners to predict students' course achievement with a high accuracy, and possibly take remedial actions in the semester.																	1432-7643	1433-7479															10.1007/s00500-020-05110-4		JUL 2020											
J								A kind of artificial intelligence model based on nature without statistic	COMPLEX & INTELLIGENT SYSTEMS										Statistics; Causality; DNA rules; Prediction; Adaptability; Intelligence	CHAOS	Simple rules can generate complexity and human can also learn fast with rules. Can machines learn in a similar way? Can artificial intelligence be independent of statistics? Machine learning is growing rapidly but models are poorly interpretable and depend on statistics. We propose a method by iteration based on causality which is the real one exists in the system. It is composed of fixed goals and basic rules called DNA rules. These DNA rules can be obtained from the definition and is not statistical rules. The causality in rules promises the process to be precise, because the potential attractor of the system is deterministic, because it is subjected to the rules although the system is complex especially under uncertain interference. Such a model not only works well in the traditional deterministic systems like the stable point and limited circle but also can work in some seemingly random and systems which are considered to be stochastic systems. The model is taken to play a game and it makes the machine learns fast and adaptively, and it is also interpretable with the causality and independent from the amount of data for it is based on causal iteration. It learns and even predicts the seemingly random interference in the game. We found such a model is adaptable, and it works well even in out-of-sample situations. The model is compared with an LSTM network in prediction a seemingly random sequence, the result shows the causality-based model also works well. We think that it may help to solve some problems hard for the traditional statistical method and become an enrichment for the current models.																	2199-4536	2198-6053															10.1007/s40747-020-00174-z		JUL 2020											
J								Optimization of a building energy performance by a multi-objective optimization, using sustainable energy combinations	EVOLVING SYSTEMS										Sustainable energy sets (SERs); Expense-optimum evaluation; Multi-criteria optimization; Grass Fibrous Root Optimization algorithm; HVAC systems	FORECAST ENGINE; CHAOS OPTIMIZATION; FEATURE-SELECTION; ALGORITHM; SYSTEMS; DEMAND; UNCERTAINTIES; STRATEGIES; EFFICIENCY; POLICY	Nowadays the application of the sustainable resources (SERs) of the energy and significant role of the building sector due to environmental effects is notable. Therefore, finding the optimum combination of SERs is an important target to be noticed by the designers to achieve sustainable buildings. Regarding the energy consumption for local warm-water, electrical appliances, and to cool and heat the building space, a new method is suggested in this study to design the optimized combination of the sustainable energy systems to unify the building energy demand. Minimization of the two objectives including the initial energy request and the investment expense is considered as the multi-criteria optimization. Also, to find the expense-optimum solution, the universal expense has been studied. Furthermore, a limitation of achieving the SER combination's minimum amount in line with the Albanian laws is regarded. The Grass Fibrous Root Optimization algorithm is applied as the optimization process using MATLAB(R)and DesignBuilder simulation software. Because the solutions should adhere to the lowest SER combination's levels recommended through present regulations, a limitation is studied, too. A regular residential case of Tirana, in Albania, is chosen to be investigated by using the proposed methodology and regarding the present laws related to the efficiency of the energy. SER systems include heat pumps (HPs), PV panels, thermal solar energy systems. It has resulted that, the initial energy demand saves up to 29.5 kWh/m(2)a and the universal expense up to 78,850 euro as provided by the methodology comparing a building composition not considering the SERs. Aiming at sustainable development, the proposed methodology can be useful regarding two aspects including economic profits for the holders and the environmental advantages.																	1868-6478	1868-6486															10.1007/s12530-020-09350-5		JUL 2020											
J								A hybrid method of blockchain and case-based reasoning for remanufacturing process planning	JOURNAL OF INTELLIGENT MANUFACTURING										Remanufacturing process planning; Blockchain; Case-based reasoning; Nearest neighbor algorithm		Remanufacturing plays a vital role in promoting the development of circular economy for its great advantages in energy saving, material saving and emission reduction. Remanufacturing process planning (RPP), which affects the performance of remanufacturing greatly, becomes increasingly important to the remanufacturing enterprises. In general, RPP is knowledge dependent. Some remanufacturing enterprises, especially small and middle-sized remanufacturing enterprises (SMREs) may have inadequate remanufacturing knowledge, which makes it difficult to implement a proper RPP. Therefore, how to share and make full use of the knowledge in different remanufacturing enterprises for RPP has become a bottleneck. To this end, a hybrid method integrating blockchain (BC) and case-based reasoning (CBR) for RPP, which can take full advantage of the remanufacturing knowledge by cross enterprises knowledge sharing, is presented in this paper. In this proposed method, a BC network was utilized to record the remanufacturing knowledge and its associated transactions to guarantee the security and reliability of knowledge sharing, and CBR was employed to retrieve and reuse the most suitable solution by analyzing the similarity between previous remanufacturing cases and new case with the nearest neighbor algorithm. Finally, a used lathe guideway was set as a case study to verify the feasibility and superiority of the proposed approach. The hybrid method has been applied in a prototype system written in HTML and JavaScript. The results indicated that the proposed approach can effectively help SMREs to obtain optimum solutions for RPP with comprehensive economic, environmental and social benefits.																	0956-5515	1572-8145															10.1007/s10845-020-01618-6		JUL 2020											
J								Evaluation on regional science and technology resources allocation in China based on the zero sum gains data envelopment analysis	JOURNAL OF INTELLIGENT MANUFACTURING										Data envelopment analysis (DEA); Efficiency evaluation; Zero sum gains (ZSG); Uniform frontier (UF); Resource allocation; Science and technology (S&T) performance evaluation	UNDESIRABLE OUTPUTS; CO2 EMISSIONS; DEA MODELS; EFFICIENCY	Regional science and technology (S&T) resource allocation is an important supporting means of Intelligent Manufacturing in the future. Research on the efficiency of S&T resource allocation is helpful to judge the potential of Intelligent Manufacturing in a specific region. S&T performance evaluation and resource allocation are critical administrative activities for a country or region. Due to resource scarcity, it is necessary to consider the constraint of limited total resources in the process of evaluation and allocation. Thus, the zero sum gains data envelopment analysis models and the associated uniform frontier (UF) method are more suitable for this issue. Comparing with the existing methods, we propose a new algorithm for solving the UF method in this article, which simplifies the procedure of calculation and extends from single to multiple resource allocation. In the empirical application, we evaluate the S&T performances and allocate R&D personnel and intramural expenditure among 31 administrative regions in China. There are 10 high-performance regions. Results can provide specific reference meanings to policy making and analysis.																	0956-5515	1572-8145															10.1007/s10845-020-01622-w		JUL 2020											
J								Fast intra-coding unit partition decision in H.266/FVC based on deep learning	JOURNAL OF REAL-TIME IMAGE PROCESSING										H266; Future video coding; Intra-coding; Convolutional neural network; Quad-tree plus binary-tree; Rate distortion optimization		In the recent Future Video Coding (FVC) standard developed by the Joint Video Exploration Team (JVET), the quad-tree binary-tree (QTBT) block partition module makes use of rectangular block forms and additional square block sizes compared to quad-tree (QT) block partitioning module proposed in the predecessor High-Efficiency Video Coding (HEVC) standard. This block flexibility, induced with the QTBT module, significantly improves compression performance while it dramatically increases coding complexity due to the brute force search for Rate Distortion Optimization (RDO). To cope with this issue, it is necessary to consider the unique characteristics of QTBT in FVC. In this paper, we propose a fast QT partitioning algorithm based on a deep convolutional neural network (CNN) model to predict coding unit (CU) partition instead of RDO which enhances considerably QTBT performance for intra-mode coding. Based on a suitable diversified CU partition patterns database, the optimization process is set up with three levels CNN structure developed to learn the split or non-split decision from the established database. Experimental results reveal that the proposed algorithm can accelerate the QTBT block partition structure by reducing the intra-mode encoding time by an average of 35% with a bit rate increase of 1.7%, allowing its application in practical scenarios.																	1861-8200	1861-8219															10.1007/s11554-020-00998-5		JUL 2020											
J								Fine-grain complexity control of HEVC intra prediction in battery-powered video codecs	JOURNAL OF REAL-TIME IMAGE PROCESSING										Complexity control; HEVC; Intra coding; Pareto optimization	ENERGY REDUCTION TECHNIQUE; MODE DECISION ALGORITHM; HARDWARE; PERFORMANCE; COMPUTATION; ENCODERS	The high-efficiency video coding (HEVC) standard improves the coding efficiency at the cost of a significantly more complex encoding process. This is an issue for a large number of video-capable devices that operate on batteries, with limited and varying processing power. A complexity controller enables an encoder to provide the best possible quality at any power quota. This paper proposes a complexity control method for HEVC intra coding, based on a Pareto-efficient rate-distortion-complexity (R-D-C) analysis. The proposed method limits the intra prediction for each block (as opposed to existing methods which limit the block partitioning), on a frame-level basis. This method consists of three steps, namely rate-complexity modeling, complexity allocation, and configuration selection. In the first step, a rate-complexity model is presented which estimates the encoding complexity according to the compression intensity. Then, according to the estimated complexity and target complexity, a complexity budget is allocated to each frame. Finally, an encoding configuration from a set of Pareto-efficient configurations is selected according to the allocated complexity and the video content, which offers the best compression performance. Experimental results indicate that the proposed method can adjust the complexity from 100 to 50%, with a mean error rate of less than 0.1%. The proposed method outperforms many state-of-the-art approaches, in terms of both control accuracy and compression efficiency. The encoding performance loss in terms of BD-rate varies from 0.06 to 3.69%, on average, for 90-60% computational complexity, respectively. The method can also be used for lower than 50% complexity if need be, with a higher BD-rate.																	1861-8200	1861-8219															10.1007/s11554-020-00996-7		JUL 2020											
J								Strength prediction of paste filling material based on convolutional neural network	COMPUTATIONAL INTELLIGENCE										compressive strength; convolution neural network; smart cities; paste filling material; prediction model	COMPRESSIVE STRENGTH; CONCRETE; BACKFILL	The common backfill mining technology in the green mining industry can be used for the secondary utilization of construction waste in smart cities. This measure has the advantages of low cost, fast results, and less environmental pollution. Over the past few decades, with the continuous advancement of global urbanization, the effective and environmentally friendly construction waste disposal and emission are very important for the development of urban green construction. Construction waste can be prepared as paste filling material, as one of the raw materials for backfill mining. This paper proposes a new method that can quickly and accurately predict the strength of paste filling materials with different compositions. A deep connected convolutional neural network (CNN) that can extract input parameters is used to build a prediction model. The coarse aggregate, fine aggregate, and cementing material are employed as the input variables of the CNN model, and five indicators which are generally used to evaluate the strength of filling material are selected as the output results. The experimental results show that the proposed prediction approach can obtain robust prediction results and high prediction accuracy and speed.																	0824-7935	1467-8640															10.1111/coin.12373		JUL 2020											
J								Service quality evaluation of satellite data distribution system based onBP-IPA	COMPUTATIONAL INTELLIGENCE										BP neural network; importance-performance analysis; satellite data distribution system; service quality	IMPORTANCE-PERFORMANCE ANALYSIS; CUSTOMER SATISFACTION; NEURAL-NETWORKS; TECHNOLOGY; DIMENSIONS; MODEL	With the remarkable advances in remote sensing technology, remote sensing data collected by satellites play an important role in many application domains, in particular the smart city. The service quality of satellite data distribution systems (SDDS) is vital for both administrators and end users. Therefore, it is of practical significance to fully understand the factors that affect the satellite data distribution service. Based on prior studies of e-service quality evaluation and the characteristics of the satellite data, a multidimensional and multi-level service quality evaluation model of the SDDS is proposed. A questionnaire was designed based on the proposed model to collect users' evaluation data. Backpropagation neural network was employed to examine whether and how factors in the proposed model affect the overall service quality. The results were interpreted using importance and performance analysis to assess the relationship between the keys of service quality of the SDDS and the overall service quality. Findings have important managerial implications for the quality management of the SDDS.																	0824-7935	1467-8640															10.1111/coin.12364		JUL 2020											
J								Machine learning approach for power consumption model based on monsoon data for smart cities applications	COMPUTATIONAL INTELLIGENCE										clustering; K-means; machine learning methods; smart city; TANGEDCO	ENERGY-CONSUMPTION; LOAD PROFILES; HEALTH-CARE; PREDICTION; IOT	In this modern world, electricity plays a vital role. It is essential for human life and also affects normal behavior of environment resulting in global warming. Recent developments in artificial intelligence (AI), in particular machine learning (ML), have been significantly advancing smart city applications. Smart infrastructure, which is an essential component of smart cities, is equipped with power systems designed for optimizing smart devices. In this article, real domestic consumption data of 500 consumers from TANGEDCO are analyzed and clustered based on different seasons (consumption rate varies upon different weather conditions) for smart city applications. An efficient clustering algorithm k-means integrates big data set for a period of 10 years and converts it into clustering graph with three seasons. By analyzing this data, the amount of consumption of electricity by humans in particular area (Pasupathikovil) of Papanasam taluk of Thanjavur district will be predicted. This article would be more useful for predicting changes in usage of electricity and take proper steps for analyzing the consumption accordingly and it will be more useful in smart city development. It gives an idea of which season needs more consumption and which needs less.																	0824-7935	1467-8640															10.1111/coin.12368		JUL 2020											
J								What makes trading strategies based on chart pattern recognition profitable?	EXPERT SYSTEMS										dynamic time warping; generic pattern recognition; stock markets; technical analysis; UCR suite	TECHNICAL ANALYSIS; CANDLESTICK PATTERNS; NEURAL-NETWORK; MARKET DATA; STOCK; OPTIMIZATION; PERFORMANCE; ALGORITHM; RULES; INDEX	Automating chart pattern recognition is a relevant issue addressed by researchers and practitioners when designing a system that considers technical analysis for trading purposes. This article proposes the design of a trading system that takes into account any generic pattern that has been proven to be profitable in the past, without restricting the search to the specific technical patterns reported in the literature, hence the termgeneric pattern recognition. A fast version of dynamic time warping, the University College Riverside subsequence search suite (called the UCR suite), is employed for the pattern recognition task in an effort to produce trading signals in realistic timescales. This article evaluates the significance of the relation between the system's profitability and (a) the pattern length, (b) the take-profit and stop-loss levels and (c) the performance consensus of past patterns. The trading system is assessed under the mean-variance perspective by using 560 NYSE stocks. The results obtained by the different parameter configurations are reported, controlling for both data-snooping and transaction costs. On average, the proposed system dominates the market index in the mean-variance sense. Although transaction costs reduce the profitability of the proposed trading system, 92.5% of the experiments are profitable if the analysis is reduced to the parameter values aligned with the technical analysis.																	0266-4720	1468-0394														e12596	10.1111/exsy.12596		JUL 2020											
J								A Novel Fuzzy Time Series Forecasting Model Based on the Hybrid Wolf Pack Algorithm and Ordered Weighted Averaging Aggregation Operator	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy time series; Wolf pack algorithm; Bacterial foraging optimization; Ordered weighted averaging aggregation operator; Hybrid wolf pack algorithm; TAIFEX	ENROLLMENTS; OPTIMIZATION; PREDICTION; FCM	The fuzzy time series has received extensive attention since it was proposed and it has been widely used in various practical applications. This study proposes a new fuzzy time series forecasting model which considers a hybrid wolf pack algorithm (HWPA) and an ordered weighted averaging (OWA) aggregation operator for fuzzy time series. The HWPA is adopted to obtain a suitable partition of the universe of discourse to promote the forecasting performance. Furthermore, the improved OWA aggregation method is applied to make the aggregation of historical information more practical. To overcome the deficiency of slow convergence speed and easy to entrap into the local extremum of the wolf pack algorithm (WPA), the chemotactic behavior and elimination-dispersal behavior of bacterial foraging optimization (BFO) are employed to optimize the scouting behavior of WPA. The actual enrollments data of the University of Alabama and Taiwan Futures Exchange (TAIFEX) are utilized as the benchmark data and the computational results of both training and testing phases all indicate that the new forecasting model outperforms other existing models. The robustness of the proposed model is also tested and the robust results can be obtained when the historical data are inaccurate.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1832	1850		10.1007/s40815-020-00906-w		JUL 2020											
J								Using the deep neural networks for normal and abnormal situation recognition in the automatic access monitoring and control system of vehicles	NEURAL COMPUTING & APPLICATIONS										Monitoring and control system; Mathematical model; Normal and abnormal situation; Recognition; Convolutional neural network; MobileNet		A new mathematical model of the intelligent access monitoring and control system based on the cybernetic approach is proposed for solving the problems of vehicle access to the territory of an organization. The distinctive feature of the mathematical model is the ability to take into account and recognize normal and abnormal situations at the protected object and develop control actions. To localize vehicles and recognize their license plates, the composition of traditional methods of image processing and two-pass classification performed by the developed modified architecture of convolutional neural network MobileNet is offered. The composition allows to define additional identification features of the object. The proposed adaptations allow to recognize the situation in real time with low computational costs and high accuracy. The natural experiment has shown that the integration of the modern hardware means and algorithms of object detection and recognition, even in the rough conditions of street closed-circuit television monitoring, provides not less than 96% accuracy, and the processing time of one frame is not more than 0.094 s based on the Nvidia GeForce 1080Ti graphic processor. High recognition accuracy without loss of speed in the real-time mode is achieved by integrating the modern hardware means and the algorithms of object detection and recognition. The program module in Python using the Tensorflow and Keras library is developed for carrying out the access control functions.																	0941-0643	1433-3058															10.1007/s00521-020-05170-5		JUL 2020											
J								A fingerprint-based coarse-to-fine algorithm for indoor positioning system using Bluetooth Low Energy	NEURAL COMPUTING & APPLICATIONS										Indoor positioning system; Bluetooth Low Energy; Fingerprinting; Coarse-to-fine estimation	KALMAN-FILTER	In this research, we propose a fingerprint-based fine-tuning algorithm to increase the accuracy of indoor positioning system using Bluetooth Low Energy fingerprint. In the fine-tuning step, Delta rule is used to update the coordinates of reference points based on the wireless environment represented in the training dataset. The combination of the coarse estimation and the proposed fine-tuning makes the coarse-to-fine algorithm. The algorithms used for coarse estimation are weighted sum andk-nearest neighbour, with three weight calculation algorithms that are compared in this experiment: Minkowski distance,k-means Gaussian mixture model, and autoencoder. Two subject rooms are used in order to measure the performance of the algorithms. The experiment showed that the fine-tuning algorithm improves the accuracy of the positioning throughout all combination of methods in both rooms, which shows its versatility. It reduces the mean positioning error by up to 11.3% and depends on what algorithm used in the weight calculation. Another benefit from the fine-tuning model is that it does not increase the complexity of the algorithm in the online phase. Overall, the best result is achieved by the combination of weighted sum andk-nearest neighbour with Minkowski distance weight calculation, together with the proposed fine-tuning. Its mean positioning error is 0.8740 m for Room 1 and 1.5385 for Room 2. The average computing time for a single online query is 0.4 ms for Room 1 and 0.7 ms for Room 2.																	0941-0643	1433-3058															10.1007/s00521-020-05159-0		JUL 2020											
J								Monitoring of volcanic ash cloud from heterogeneous data using feature fusion and convolutional neural networks-long short-term memory	NEURAL COMPUTING & APPLICATIONS										Feature fusion; Remote sensing image; Text information; Volcanic ash cloud	ERUPTIONS; TRANSPORT; BAND	Heterogeneous data have become a key issue restricting the monitoring accuracy of volcanic ash cloud and rapid application of remote sensing. In view of the characteristics of classification and heterogeneous data in volcanic ash cloud monitoring, a monitoring method of volcanic ash cloud using feature fusion, convolutional neural networks (CNN) and long short-term memory (LSTM) (FF-CNN-LSTM) is presented in this paper. In this method, firstly, the target features in image are identified by CNN and the time sequence information in text is extracted by LSTM. Secondly, the mapping relationship between text information and image features is learned by fusing image target and text high-level features, and then the identification and diffusion of volcanic ash cloud from heterogeneous data containing only image and text were performed. Finally, the presented FF-CNN-LSTM method is tested by the simulation experiment and the true heterogeneous data of Enta volcanic ash cloud case. The experimental results show that compared with the single CNN, LSTM and the simple combination between CNN and LSTM (CNN-LSTM), the presented FF-CNN-LSTM method in this paper can be fitted with less training steps and has high accuracy and low loss rate; the obtained distribution of volcanic ash cloud is clear and intuitive and has the characteristics with fewer model parameters, simple calculation and high accuracy. It also shows the feasibility and effectiveness of the presented method in volcanic ash cloud monitoring to some extent. The results reveal that the presented method can potentially contribute the monitoring of volcanic ash cloud and disaster prevention and mitigation.																	0941-0643	1433-3058															10.1007/s00521-020-05050-y		JUL 2020											
J								HAN-ReGRU: hierarchical attention network with residual gated recurrent unit for emotion recognition in conversation	NEURAL COMPUTING & APPLICATIONS										Emotion recognition in conversation; Pre-trained word embedding; Hierarchical attention network; Bidirectional gated recurrent unit; Residual connection; Position embedding		Emotion recognition in conversation aims to identify the emotion of each consistent utterance in a conversation from several pre-defined emotions. The task has recently become a new popular research frontier in natural language processing because of the increase in open conversational data and its application in opinion mining. However, most existing methods for the task cannot capture the long-range contextual information in an utterance and a conversation effectively. To alleviate this problem, we propose a novel hierarchical attention network with residual gated recurrent unit framework. Firstly, we adopt the pre-trained BERT-Large model to obtain context-dependent representation for each token of each utterance in a conversation. Then, a hierarchical attention network is proposed to capture long-range contextual information about the conversation structure. Besides, in order to better model position information of the utterances in a conversation, we add position embedding to the input of the multi-head attention. Experiments on two textual dialogue emotion datasets demonstrate that our model significantly outperforms the state-of-the-art baseline methods.																	0941-0643	1433-3058															10.1007/s00521-020-05063-7		JUL 2020											
J								Multimodal medical image fusion algorithm in the era of big data	NEURAL COMPUTING & APPLICATIONS										Multimodal medical imaging; Medical image fusion; Pulse-coupled neural network; Non-subsampled shearlet transform	INFORMATION; DECOMPOSITION	In image-based medical decision-making, different modalities of medical images of a given organ of a patient are captured. Each of these images will represent a modality that will render the examined organ differently, leading to different observations of a given phenomenon (such as stroke). The accurate analysis of each of these modalities promotes the detection of more appropriate medical decisions. Multimodal medical imaging is a research field that consists in the development of robust algorithms that can enable the fusion of image information acquired by different sets of modalities. In this paper, a novel multimodal medical image fusion algorithm is proposed for a wide range of medical diagnostic problems. It is based on the application of a boundary measured pulse-coupled neural network fusion strategy and an energy attribute fusion strategy in a non-subsampled shearlet transform domain. Our algorithm was validated in dataset with modalities of several diseases, namely glioma, Alzheimer's, and metastatic bronchogenic carcinoma, which contain more than 100 image pairs. Qualitative and quantitative evaluation verifies that the proposed algorithm outperforms most of the current algorithms, providing important ideas for medical diagnosis.																	0941-0643	1433-3058															10.1007/s00521-020-05173-2		JUL 2020											
J								Multichannel one-dimensional convolutional neural network-based feature learning for fault diagnosis of industrial processes	NEURAL COMPUTING & APPLICATIONS										Industrial process; Fault diagnosis; Wavelet transform; Convolutional neural network; Feature learning	PRINCIPAL-COMPONENT ANALYSIS; CLASSIFICATION; MODEL; CNN	In industrial processes, the noise and high dimension of process signals usually affect the performance of those methods in fault detection and diagnosis. A predominant property of a fault diagnosis model is to extract effective features from process signals. Wavelet transform is capable of extracting multiscale information that provides effective fault features in time and frequency domain of process signals. In this paper, a new deep neural network (DNN), multichannel one-dimensional convolutional neural network (MC1-DCNN), is proposed to investigate feature learning from high-dimensional process signals. Wavelet transform is used to extract multiscale components with fault features from process signals. MC1-DCNN is able to learn discriminative time-frequency features from these multiscale process signals. Tennessee Eastman process and fed-batch fermentation penicillin process are adopted to verify performance of the proposed method. The experimental results demonstrate remarkable feature extraction and fault diagnosis performance of MC1-DCNN and show prosperous possibility of applying this method to industrial processes.																	0941-0643	1433-3058															10.1007/s00521-020-05171-4		JUL 2020											
J								Diversity driven multi-parent evolutionary algorithm with adaptive non-uniform mutation	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Adaptive non-uniform mutation; multi-parent crossover; evolutionary algorithm; constraint optimisation problem	CODED GENETIC ALGORITHMS; SINE COSINE ALGORITHM; SALP SWARM ALGORITHM; GREY WOLF OPTIMIZER; WHALE OPTIMIZATION; OPERATORS	Any evolutionary algorithm tends to end up in a local optimum. A new approach based on an evolutionary algorithm named as Diversity Driven Multi-Parent Evolutionary Algorithm with Adaptive non-uniform mutation is presented. In the proposed algorithm, Non-uniform mutation is used to maintain diversity in the explored solutions. Fitness variance, which signifies solution space aggregation, is used to detect the premature convergence of the population to a local optimum. The term multi-parent is used in the context of more than two parents participating in crossover operation. After multi-parent selection for cross-over to generate new solutions, the non-uniform adaptive mutation is performed, which in turn is triggered by the diminishing value of fitness variance of candidate solutions and pushes solutions out of local optimum. Hence, it can be said that the algorithm is driven by the diversity of the population and overcomes the tendency of evolutionary algorithms to stuck in local optimum. The performance of this algorithm is tested on 23 basic benchmarks, CEC05 functions, and CEC17 functions. As CEC17 benchmark functions include constraint problems, a constraint-handling technique is proposed based on the fuzzy set theory. In the proposed constrained handling strategy, constraint violation is also taken as another objective along with the main objective. The decision to accept or discard the solution is based on the fuzzy set theory. The values of constraint violation and objective function are calculated and fuzzified by calculating membership values by considering the main objective and constraint violation as triangular fuzzy functions. The best solutions are selected based on cardinal priority ranking. The obtained results from the proposed algorithm are compared with the results available in the literature. The result indicates that this algorithm is competitive, even with a smaller number of function evaluations.																	0952-813X	1362-3079															10.1080/0952813X.2020.1785020		JUL 2020											
J								ConArgLib: an argumentation library with support to search strategies and parallel search	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Abstract Argumentation; solver; weighted edges; constraint programming	ABSTRACT ARGUMENTATION; ACCEPTABILITY; RELAXATION; ALGORITHMS; SEMANTICS; DEFENSE	We presentConArgLib, a C++ library implemented to help programmers solve some of the most important problems related to extension-based abstract Argumentation. The library is based on ConArg, which exploitsConstraint Programmingand, in particular, Gecode, a toolkit for developing constraint-based systems and applications. Given a semantics, such problems consist, for example, in enumerating all the extensions, and checking the credulous or sceptical acceptance of an argument passed as parameter. The goal is to let programmers use the library to quickly develop programs on top of it, as, for instance, implementing decision-making procedures based on the strongest arguments, or comparing two frameworks by looking at the differences between their (e.g., stable) semantics. The library features the possibility to use different branching strategies, which we all test and compare on a set of frameworks taken from theInternational Competition on Computational Models of Argumentation(ICCMA17). Moreover, for some of the tasks, it is possible to perform a parallel search using several workers at the same time: we test the speed-up between using from 1 to 16 threads on a set of ICCMA17 frameworks.																	0952-813X	1362-3079															10.1080/0952813X.2020.1789756		JUL 2020											
J								Decision Analysis Methods Combining Quantitative Logic and Fuzzy Soft Sets	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy soft set; Fuzzy information system; Soft fuzzy semantic; Soft truth degree; Soft metric; Soft decision rule		In this paper we propose a new decision analysis method combining quantitative logic and fuzzy soft set theory. Firstly, we transform a fuzzy information system into a fuzzy soft set, and then establish a formal language based on the fuzzy soft set, in which the parameters of fuzzy soft set are regarded as atomic formulas, some atomic formulas are connected by the logical connectives and then a logical formula is formed, and a implicative type of formula is interpreted as a soft decision rule (SDR). Secondly, various types of measures to evaluate the SDR are introduced and then the soft metric between two logical formulas is established. Thirdly, we apply the soft metric to the soft decision analysis, a SDR extraction algorithm for fuzzy decision information system and a corresponding recommendation algorithm are proposed. Finally, some attribute analysis examples, including the example as shown in rough sets and the practical credit card application example, are given to illustrate the newly proposed method and related concepts.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1801	1814		10.1007/s40815-020-00899-6		JUL 2020											
J								New Similarity Measures for Dual Hesitant Fuzzy Sets and Their Application	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multi-criteria decision-making; DHFS; Similarity measure; 2-Additive measure; Shapley value	GROUP DECISION-MAKING; CORRELATION-COEFFICIENT; AGGREGATION OPERATORS; PREFERENCE RELATIONS; DISTANCE	Dual hesitant fuzzy sets (DHFSs) are powerful and efficient to express hesitant preferred and non-preferred information simultaneously. This paper focuses on similarity measures for DHFSs. To do this, it first analyzes the limitations of previous similarity measures for DHFSs. Then, several new dual hesitant fuzzy similarity measures are defined that can avoid the issues of previous ones. To discriminate the importance of decision-making criteria, several weighted similarity measures are further defined in views of additive and 2-additive measures. When the weighting information is not exactly known, optimization methods for determining additive and 2-additive measures are built, respectively. Furthermore, a method for multi-criteria decision-making based on new weighted similarity measures is developed. Finally, two numerical examples are provided to show the utilization of the new method and compare with previous methods.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1851	1867		10.1007/s40815-020-00910-0		JUL 2020											
J								An Adaptive DE Algorithm Based Fuzzy Logic Anti-swing Controller for Overhead Crane Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Overhead crane systems; Fuzzy control; Fusion function; Differential evolution algorithm	DIFFERENTIAL EVOLUTION ALGORITHM; TRACKING CONTROL; OPTIMIZATION; DESIGN; PARAMETERS; OBSERVER	In this paper, aiming at the under-actuated problem of the overhead crane systems, a fuzzy logic anti-swing controller is first designed according to operator experience. Moreover, for better configuring the parameters of the controller, an adaptive differential evolution with disturbance factor algorithm (ADE-D) is proposed by introducing the adaptive scaling factor, the dynamic crossover probability and disturbance factor. By implementing numeric experiment test, the results show that the adaptive differential evolution with disturbance factor algorithm outperforms the standard differential evolution algorithm and other improved differential evolution algorithms. Finally, the adaptive differential evolution with disturbance factor algorithm-based fuzzy logic anti-swing controller is simulated under different conditions and compared with other control methods; the results exhibit excellent robustness of control performance in positioning control and damping oscillation of payload.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1905	1921		10.1007/s40815-020-00883-0		JUL 2020											
J								A Novel Approach for Probabilistic Linguistic Multiple Attribute Decision Making Based on Dual Muirhead Mean Operators and VIKOR	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multiple attribute decision making; Probabilistic linguistic term sets; PLDMM operator; PLWDMM operator; VIKOR	TERM SETS; AGGREGATION	In this study, we concentrate on multiple attribute decision-making (MADM) problems in the probabilistic linguistic preference information surroundings based on novel aggregation operators. Considering interrelationships among the multi-input arguments of probabilistic linguistic term sets (PLTSs), we extend dual Muirhead mean (DMM) operators to the probabilistic linguistic preference environment and develop a decision-making approach to deal with probabilistic linguistic MADM (PLMADM) problems. In specific, we define probabilistic linguistic dual Muirhead mean operators, i.e., probabilistic linguistic dual Muirhead mean (PLDMM) operator and probabilistic linguistic weighted dual Muirhead mean (PLWDMM) operator, and further investigate their corresponding propositions, theorems as well as properties. In the light of VIKOR method, a novel decision-making approach for PLMADM problems has been carefully explored. Finally, an application of hospitals selection can fruitfully demonstrate and signify the practicality and feasibility of the proposed decision-making approach.																	1562-2479	2199-3211															10.1007/s40815-020-00897-8		JUL 2020											
J								Joint label-specific features and label correlation for multi-label learning with missing label	APPLIED INTELLIGENCE										Missing labels; Label-specific features selections; Positive label correlations; Negative label correlations	CLASSIFICATION; SELECTION	Existing multi-label learning classification algorithms ignore that class labels may be determined by some features in the original feature space. And only a partial label of each instance can be obtained for some real applications. Therefore, we propose a novel algorithm named joint Label-Specific features and Label Correlation for multi-label learning with Missing Label (LSLC-ML) and its optimized version to solve the above-mentioned problems. First, a missing label can be recovered by the learned positive and negative label correlations from the incomplete training data sets, then the label-specific features can be selected, finally the multi-label classification task can be modeled by combining the labelspecific feature selections, missing labels and positive and negative label correlations. The experimental results show that our algorithm LSLC-ML has strong competitiveness compared with some state-of-the-art algorithms in evaluation matrices when tested on benchmark multi-label data sets.																	0924-669X	1573-7497				NOV	2020	50	11					4029	4049		10.1007/s10489-020-01715-2		JUL 2020											
J								Optimal Deep Learning based Convolution Neural Network for digital forensics Face Sketch Synthesis in internet of things (IoT)	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										5G IoT; Deep learning; Cybercrime; Forensic sketch; Convolution Neural Network	OPTIMIZATION	The rapid development in 5G cellular and IoT technologies is expected to be deployed widespread in the next few years. At the same time, crime rates are also increasing to a greater extent while the investigation officers are held responsible to deal with a broad range of cyber and internet issues in investigations. Therefore, advanced IT technologies and IoT devices can be deployed to ease the investigation process, especially, the identification of suspects. At present, only a few research works has been conducted upon deep learning-based Face Sketch Synthesis (FSS) models, concerning its success in diverse application domains including conventional face recognition. This paper proposes a new IoT-enabled Optimal Deep Learning based Convolutional Neural Network (ODL-CNN) for FSS to assist in suspect identification process. The hyper parameter optimization of the DL-CNN model was performed using Improved Elephant Herd Optimization (IEHO) algorithm. In the beginning, the proposed method captures the surveillance videos using IoT-based cameras which are then fed into the proposed ODL-CNN model. The proposed method initially involves preprocessing in which the contrast enhancement process is carried out using Gamma correction method. Then, the ODL-CNN model draws the sketches of the input images following which it undergoes similarity assessment, with professional sketch being drawn as per the directions from eyewitnesses. When the similarity between both the sketches are high, the suspect gets identified. A comprehensive qualitative and quantitative examination was conducted to assess the effectiveness of the presented ODL-CNN model. A detailed simulation analysis pointed out the effective performance of ODL-CNN model with maximum average Peak Signal to Noise Ratio (PSNR) of 20.11dB, Average Structural Similarity (SSIM) of 0.64 and average accuracy of 90.10%.																	1868-8071	1868-808X															10.1007/s13042-020-01168-6		JUL 2020											
J								Eco-driving for urban bus with big data analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Eco-driving; Urban bus; Fuel consumption; Telematics; Big data; Decision tree	FUEL-CONSUMPTION; DRIVER BEHAVIOR; MODEL; NETWORK; CONSTRUCTION; TELEMATICS; EFFICIENCY; EMISSIONS; SYSTEM; IMPACT	Fuel consumption constitutes 20-30% of the operation cost of most bus companies. Consequently, reducing fuel consumption decreases operating costs and carbon emissions. Most previous studies adopted experimental methods to collect and analyze small data and focused on the influence of a single variable on fuel consumption. Therefore, the analytical results may not have appropriately reflected the operation requirements of the bus companies. Hence, this study obtains big data comprising of Telematics and operation records from an urban bus company and selects the relevant data according to several eco-driving aspects such as driving behavior, vehicle characteristics, driver characteristics, and weather. Subsequently, a decision tree, C5.0, is adopted to explore the relevant correspondence between variables that affect fuel consumption. Observing the analytical results, the variables of bus brand, bus age, bus weight, monthly passenger load, monthly salary, monthly working days, monthly overtime, and times of high-speed have relatively high influence on fuel consumption. Based on the results, therefore, several eco-driving recommendations of fuel consumption reduction are proposed. For the case of bus purchase, the urban bus company can cautiously consider bus brand, bus age, and bus weight. The company can also provide a friendly working environment with the reasonable monthly passenger load, monthly salary, working days in a month, and overtime to reduce the times of high-speed such that the fuel efficiency can be improved.																	1868-5137	1868-5145															10.1007/s12652-020-02287-2		JUL 2020											
J								Using Grid computing architecture in computing resource allocating of IC design	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Grid computing; Load sharing; Batch scheduling; Computing resource allocating	X-ARCHITECTURE; ALGORITHM	At present, the utilization of workstation, storage, and server in an IC design house is not frequent, even the EDA license isn't, either. If these various resources on the intranet can be integrated and effectively operated, the working efficiency will be enhanced. Therefore, how to manage these critical resources on the network is a major task to enhance the competition of an IC design house.In this research, the Grid Engine structuring a model of resource distribution can integrate the resources on the intranet to build a set of system and rule about resource distribution. Taken the IC design house as an example, a user can use the technology of the Sun Grid Engine for load sharing, batch scheduling, as well as integration of resources of software/hardware on the intranet like a single workstation. Therefore, the working efficiency will be enhanced under limited resources because of complete utilization of system resources. The results of research provides a set of model and structure for management of system resources to integrate resources on the intranet in an IC design house. A resource distribution model can be established by the system and rule of resource distribution. According to the adequate management of computing resources as well as scheduling, the corresponding hardware and software can provide the computing service. Ultimately, the efficiency in software utilization increases twice and efficiency in hardware utilization increase. Besides, the purchase package expense can be reduced.																	1868-5137	1868-5145															10.1007/s12652-020-02246-x		JUL 2020											
J								Improving the security of internet of things using cryptographic algorithms: a case of smart irrigation systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Security; Cryptography; RC4; ECC; SHA-256	ATTRIBUTE-BASED ENCRYPTION; AUTHENTICATION SCHEME; NETWORK; EFFICIENT; SHA-256	Internet of Things (IoT) as a ubiquitous paradigm is a new concept in Information and Communications Technology (ICT) and has the ability to connect wireless and mobile embedded devices and things to the Internet. IoT is emerging as a key component of the Internet and a vital infrastructure for millions of smart and interconnected objects that are potentially vulnerable to different attacks. Thus, the security of resource-constrained devices in IoT is highly important. As an important solution, cryptographic algorithms are used to provide confidentiality and integrity of the transmitted data between the sender and receiver. Hence, this paper proposes a new hybrid cryptographic algorithm based on Rivest cipher (RC4), Elliptic-Curve Cryptography (ECC), and Secure Hash Algorithm (SHA-256) to protect sensitive information in IoT-based smart irrigation systems. In this paper, the RC4 key is encrypted by the ECC algorithm, and the output of this encryption process is transformed to SHA-256 for hashing and generating enigmatic data. SHA-256 algorithm encrypts RC4 based cipher text to improve data integrity. Comprehensive analysis and simulation results indicate that the proposed scheme is secure to various known attacks such as the Man-in-the-middle (MiM) attack, and has a better performance than other cryptographic algorithms. Also, the obtained results confirm the effectiveness of the proposed model and robustness in order to confidentiality based on analyzing secrecy.																	1868-5137	1868-5145															10.1007/s12652-020-02303-5		JUL 2020											
J								Design and implementation of smart manufacturing execution system in solar industry	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart manufacturing execution systems (SMES); Solar cell; Prototyping; Production indices	X-ARCHITECTURE; NEURAL-NETWORK; ALGORITHM; OPTIMIZATION; PSO	The manufacturing industry, automatic production has been a crucial resource that cannot be replaced by labors. However, with the moving-abroad of manufactures and under the pressure of continuously reduce production cost, the core of production management in the manufacturing industry has lain in to increase the yield rate, lower unnecessary waste in production, and achieve excellent quality control. Smart Manufacturing Execution Systems (SMES) is now playing the role of integration. The major MESs in the present market all place the emphasis on semi-conductor, LCD panel, and modular assembling industries. The lack of identification numbers in the process of production disables the present solar cell industry to provide support and operate, and thus an MES for solar cell industry is in need.This research adopts the MES with LCD View, as the system model base. Based on the framework of prototyping constituted by information system, we proposed an SMES for solar cell industry, which is integrated with the present solar cell operation model and designed according to the framework of LCD View system.The solar cell SMES developed in this study can raise the personnel working efficiency in production and process analysis. The contribution of this study can real time analysis on quality abnormality, together with information on management, such as real time report on machine status, analysis on raw material, and production indices, the solar cell MES can facilitate production managers to make strategic decisions regarding production and improve the production as well as yield rate of products.																	1868-5137	1868-5145															10.1007/s12652-020-02292-5		JUL 2020											
J								Recognition of face CLAHEM based on using GPP-HM	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Face recognition; CLAHEM; Gabor wavelet; Histogram methods; Similarity matrix (SM)	IMAGES	A new method is proposed which uses CLAHEM process for enhancing the image and the recognition of the face is done based on GPP-HM. The CLAHEM technique is a variant of adaptive histogram equalization in which the contrast amplification is limited to reduce the problem of noise amplification. This is a computer image processing technique used to improve contrast in the images. There are many databases available as open web source, which are in use currently and the choice varies based on the task-aging, expression and lighting. From the data set specific to the property to be tested (how algorithm behaves when given images with lighting changes / different facial expressions) the databases-Yale Face Database, PIE Database-CMU, AT&T "The Database of Faces" (formerly "The ORL Database of Faces") and Indian Face Database are chosen. The face is recognized by processing the data matrix and the training phase matrix for similarity and the matrix is formed. From this similarity, the accuracy of the face features is recognized.																	1868-5137	1868-5145															10.1007/s12652-020-02297-0		JUL 2020											
J								Behavior Models of Emotion-Featured Robots: A Survey	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Social robots; Emotions; Human-robot interaction; Behavior model; Survey	PERCEPTION; EXPRESSIONS	Emotions are important in many aspects of human behavior. Emotions are displayed on human faces, they are reflected in human memory, and they even influence human intelligence. The creation of robots that try to mimic humans arises the question of how the concept of emotion can be transferred to robots. There is no unique answer to the question, however many robots that leverage emotions exist. By summarizing the work done on these robots we try to enlighten the relations between robots and emotions from several perspectives. We first identify how artificial emotion can be defined in a robotic system. Next, we investigate the possible roles of emotions in robotic behavior models and analyze different implementations of the concept of emotion in these models. Finally, we elaborate on the evaluation of how emotions influence human-robot interaction. For this purpose, we qualitatively analyzed a selected set of robots that include emotions in their model. Considering the diversity of state-of-the-art approaches to using emotions in robots, we try to present the findings in a structured and comprehensive way that could be valuable for future researchers.																	0921-0296	1573-0409															10.1007/s10846-020-01219-8		JUL 2020											
J								Real-time wavelet transform for infinite image strips	JOURNAL OF REAL-TIME IMAGE PROCESSING										Discrete wavelet transforms; Image processing; Image compression; CCSDS 122; 0	MEMORY	This article presents a single-loop approach to a 2-D discrete wavelet transform that allows processing infinitely high-image strip-maps. The paper gradually compares several computational strategies to finally show how to deal with a multi-scale wavelet transform of infinite image streams. Besides, the transform is followed by a bit-plane encoder which also processes data in a single loop. The whole machinery is part of a CCSDS 122.0 image codec which manages to process a single pixel in about 33 ns on a contemporary desktop computer, without the contribution of any parallel computing or SIMD vectorization.																	1861-8200	1861-8219															10.1007/s11554-020-00995-8		JUL 2020											
J								Algorithm optimisation and hardware implementation of interprediction mode decision	JOURNAL OF REAL-TIME IMAGE PROCESSING										High efficiency video coding (HEVC); Interprediction; Rate-distortion optimisation (RDO); Hardware architecture	HEVC; ARCHITECTURE; INFORMATION	High efficiency video coding is the most widely used video coding standard. It has higher coding performance compared with its predecessor, H.264, but it also has higher computational complexity. Interprediction is the most computationally intensive part of the entire video encoding process. Selecting the optimal interprediction mode by the rate-distortion cost calculation function requires substantial complex calculation and memory access, thus greatly increasing the difficulty of real-time hardware encoding. This study proposes to replace the traditional complex error square sum calculation with an estimation method for distortion and rate. The estimation of distortion uses the Hadamard-transformed sum of absolute transformation difference instead of the complex calculation of the sum of squared difference, whereas the estimation of rate is obtained by weighting the number of prediction units (PUs). The experiment proves that the proposed interprediction rate-distortion cost calculation model can greatly reduce computational complexity when BD-rate is increased by 3.02%. In hardware implementation, the value of rate can be obtained by indexing the number of PUs, and the resource expenditure is small.																	1861-8200	1861-8219															10.1007/s11554-020-00985-w		JUL 2020											
J								Machine Understandable Policies and GDPR Compliance Checking	KUNSTLICHE INTELLIGENZ										GDPR; Policies; Compliance checking	SYSTEM; LOGIC	The European General Data Protection Regulation (GDPR) calls for technical and organizational measures to support its implementation. Towards this end, the SPECIAL H2020 project aims to provide a set of tools that can be used by data controllers and processors to automatically check if personal data processing and sharing complies with the obligations set forth in the GDPR. The primary contributions of the project include: (i) a policy language that can be used to express consent, business policies, and regulatory obligations; and (ii) two different approaches to automated compliance checking that can be used to demonstrate that data processing performed by data controllers/processors complies with consent provided by data subjects, and business processes comply with regulatory obligations set forth in the GDPR.																	0933-1875	1610-1987				SEP	2020	34	3			SI		303	315		10.1007/s13218-020-00677-4		JUL 2020											
J								Towards Explanatory Interactive Image Captioning Using Top-Down and Bottom-Up Features, Beam Search and Re-ranking	KUNSTLICHE INTELLIGENZ										Image captioning; Deep learning; Explainable artificial intelligence (XAI); Visual explanations; Interactive machine learning (IML); Beam search; Re-ranking		Image captioning is a challenging multimodal task. Significant improvements could be obtained by deep learning. Yet, captions generated by humans are still considered better, which makes it an interesting application for interactive machine learning and explainable artificial intelligence methods. In this work, we aim at improving the performance and explainability of the state-of-the-art method Show, Attend and Tell by augmenting their attention mechanism using additional bottom-up features. We compute visual attention on the joint embedding space formed by the union of high-level features and the low-level features obtained from the object specific salient regions of the input image. We embed the content of bounding boxes from a pre-trained Mask R-CNN model. This delivers state-of-the-art performance, while it provides explanatory features. Further, we discuss how interactive model improvement can be realized through re-ranking caption candidates using beam search decoders and explanatory features. We show that interactive re-ranking of beam search candidates has the potential to outperform the state-of-the-art in image captioning.																	0933-1875	1610-1987															10.1007/s13218-020-00679-2		JUL 2020											
J								A novel validation framework to enhance deep learning models in time-series forecasting	NEURAL COMPUTING & APPLICATIONS										Time-series; Deep learning; Spatiotemporal data; Reliability	NETWORKS	Time-series analysis and forecasting is generally considered as one of the most challenging problems in data mining. During the last decade, powerful deep learning methodologies have been efficiently applied for time-series forecasting; however, they cannot guarantee the development of reliable prediction models. In this work, we introduce a novel framework for supporting deep learning in enhancing accurate, efficient and reliable time-series models. The major novelty of our proposed methodology is that it ensures a time-series to be "suitable" for fitting a deep learning model by performing a series of transformations in order to satisfy the stationarity property. The enforcement of stationarity is performed by the application of Augmented Dickey-Fuller test and transformations based on first differences or returns, without the loss of any embedded information. The reliability of the deep learning model's predictions is guaranteed by rejecting the hypothesis of autocorrelation in the model's errors, which is demonstrated by autocorrelation function plots and Ljung-Box Q test. Our numerical experiments were performed utilizing time-series from three real-world application domains (financial market, energy sector, cryptocurrency area), which incorporate most of global research interest. The performance of all forecasting models was compared on both problems of forecasting time-series price (regression) and time-series directional movements (classification). Additionally, the reliability of the models' forecasts was evaluated by examining the existence of autocorrelation in the errors. Our numerical experiments indicate that our proposed methodology considerably improves the forecasting performance of a deep learning model, in terms of efficiency, accuracy and reliability.																	0941-0643	1433-3058															10.1007/s00521-020-05169-y		JUL 2020											
J								An adversarial semi-supervised approach for action recognition from pose information	NEURAL COMPUTING & APPLICATIONS										Action recognition; Domain adaptation; Adversarial neural networks		The collection of video data for action recognition is very susceptible to measurement bias; the equipment used, camera angle and environmental conditions are all factors that majorly affect the distribution of the collected dataset. Inevitably, training a classifier that can successfully generalize to new data becomes a very hard problem, since it is impossible to gather general enough training sets. Recent approaches in the literature attempt to solve this problem by augmenting a given training set, with synthetic data, so as to better represent the global distribution of the covariates. However, these approaches are limited because they essentially involve hand-crafted data synthesizers, which are typically hard to implement and problem specific. In this work, we propose a different approach to tackling the above issues, which relies on the combination of two techniques: pose extraction, and domain adaptation as a means to improve the generalization capabilities of classifiers. We show that adapted skeletal representations can be retrieved automatically in a semi-supervised setting and these help to generalize classifiers to new forms of measurement bias. We empirically validate our approach for generalizing across different camera angles.																	0941-0643	1433-3058															10.1007/s00521-020-05162-5		JUL 2020											
J								WisdomNet: trustable machine learning toward error-free classification	NEURAL COMPUTING & APPLICATIONS										Machine learning; Trustable learning; Classification; Neural networks		Misclassification is a critical problem in many machine learning applications. Since even the classifier models with high accuracy (e.g., > 95%) still introduce some misclassification error, it may not be possible to rely on the output of a classifier. In this paper, we introduce trustable learning, which prompts the learning model to yield only the true output, thus avoiding misclassifications. Whenever the model cannot decide the output accurately, the learning model should indicate that there could be a misclassification error if it is forced to classify, and hence, it should reject to make a decision or defer it to a human expert. Therefore, we develop a methodology for trustable learning and apply it to artificial neural networks and show that it is possible to develop a classifier with 0% misclassification error. We propose a novel neural network architecture named WisdomNet that could provide zero prediction error by introducing an additional neuron named as conjugate neuron that would indicate whether the network is able to classify the data correctly or not. The WisdomNet architecture can be applied to any previously built model, and we have evaluated WisdomNet with several network architectures such as multilayer perceptron, convolutional neural network, and deep network on different data sets. The results show that the WisdomNet is able to reduce the classification error rate to 0%, while labeling the data is difficult to classify as 'reject' at a low percentage of within around 10%.																	0941-0643	1433-3058															10.1007/s00521-020-05147-4		JUL 2020											
J								Sparse regressions and particle swarm optimization in training high-order Takagi-Sugeno fuzzy systems	NEURAL COMPUTING & APPLICATIONS										Fuzzy systems; Least squares approximation; Sparse regression; Particle swarm optimization	FUNCTION APPROXIMATION; MODEL ALGORITHM; IDENTIFICATION; PSO; SELECTION	This paper proposes a method for training Takagi-Sugeno fuzzy systems using sparse regressions and particle swarm optimization. The fuzzy system is considered with Gaussian fuzzy sets in the antecedents and high-order polynomials in the consequents of the inference rules. The proposed method can be applied in two variants: without or with particle swarm optimization. In the first variant, ordinary least squares, ridge regression, or sparse regressions (forward selection, least angle regression, least absolute shrinkage and selection operator, and elastic net regression) determine the polynomials in the fuzzy system in which the fuzzy sets are known. In the second variant, we have a hybrid method in which particle swarm optimization determines the fuzzy sets, while ordinary least squares, ridge regression, or sparse regressions determine the polynomials. The first variant is simpler to implement but less accurate, the second variant is more complex, but gives better results. A new quality criterion is proposed in which the goal is to make the validation error and the model density as small as possible. Experiments showed that: (a) the use of sparse regression and/or particle swarm optimization can reduce the validation error and (b) the use of sparse regression may simplify the model by zeroing some of the coefficients.																	0941-0643	1433-3058															10.1007/s00521-020-05133-w		JUL 2020											
J								Enhancing cloud security using crypto-deep neural network for privacy preservation in trusted environment	SOFT COMPUTING										Cloud; Security; Linear algebraic equation; Neural network		The users in communication interact over cloud for data exchange. The participants are in various levels, and their expectation varies on the nature of interactions. Though building a common platform for interactions over cloud environment is difficult, there is scope for developing security solutions that can ensure confidentiality of data exchange. In the proposed model distributed secure outsourcing scheme is enhanced using crypto-deep neural network. The proposed model has cloud server, web server, data center and cloud agent. The model mainly targets in handling impersonation attack using crypto-deep neural network cloud security (CDNNCS). The proposed framework is suitable for enhancing the level of trust among cloud users in comparison to secure linear algebraic equation scheme. The performance has been presented in terms of parameters namely Delay, Jitter, Throughput and Goodput. From the results it can be observed that with CDNNCS packet loss has been reduced by 10% and the response time has been increased by 5% in comparison to existing approach.																	1432-7643	1433-7479															10.1007/s00500-020-05122-0		JUL 2020											
J								A hybrid mine blast algorithm for feature selection problems	SOFT COMPUTING										Feature selection; Simulated annealing; Mine blast algorithm; Hybrid optimization; Classification	COLONY OPTIMIZATION; DIAGNOSIS; SEARCH	Feature selection (FS) is the process of finding the least possible number of features that are able to describe a dataset in the same way as the original features. Feature selection is a crucial preprocessing step for data mining techniques as it improves the performance of the prediction process in terms of speed and accuracy and also provides a better understanding of stored data. The success of the FS process depends on achieving a balance between two important factors, namely selecting the minimal number of features and maintaining the maximum accuracy in the results. In this paper, two methods are proposed to improve the FS process. Firstly, the mine blast algorithm (MBA) is introduced to optimize the FS process in the exploration phase. Secondly, the MBA is hybridized with simulated annealing as a local search in the exploitation phase to enhance the solutions located by the MBA. The proposed approaches (MBA and MBA-SA) are tested on 18 benchmark datasets from the UCI repository, and the comprehensive experimental results indicate that MBA-SA achieved good performance when compared with five approaches in the literature.																	1432-7643	1433-7479															10.1007/s00500-020-05164-4		JUL 2020											
J								A hybrid active contour model for ultrasound image segmentation	SOFT COMPUTING										Ultrasound image segmentation; Active contour model; Bias field; Probability score; Energy functional	LEVEL SET METHOD; EDGE-DETECTION; ALGORITHM; DRIVEN; INFORMATION; ENERGIES	Abundant noise, low contrast, intensity heterogeneity, shadows, and blurry boundaries exist in most medical images, especially for 2D ultrasound (US) image. In this paper, we propose a semiautomatic hybrid active contour model for 2D US image segmentation. The proposed method mainly uses a local bias correction function and probability score. It is well known that most region-based active contour models are based on the assumption of intensity homogeneity. It is very difficult to define a region descriptor for US images with intensity heterogeneity. Here, a bias field can account for the intensity heterogeneity of the US image. Therefore, the proposed local bias correction function is considered to integrate with respect to the neighborhood center of the US image. Besides, to segment complex ultrasound images more accurately, a probability score is constructed from the edge-based operator. Based on the estimation of the bias field and an interleaved process of probability score, minimization of the proposed energy functional is achieved. The proposed method is validated on synthetic images and real US images, with satisfactory performance in the presence of noise, intensity heterogeneity, and blurry boundaries.																	1432-7643	1433-7479															10.1007/s00500-020-05097-y		JUL 2020											
J								Developing the comparison techniques of probabilistic hesitant fuzzy elements in multiple criteria decision making	SOFT COMPUTING										Probabilistic hesitant fuzzy element; Multiplying and exponential deformation formulas; Multiple criteria decision making	DISTANCE	All the existing probabilistic hesitant fuzzy element (P-HFE) comparison techniques aredirectlybased on the possible membership degree of an element together with its probability of occurring. Here, we are going to propose a class of P-HFE comparison techniques which areindirectlybased on the ingredients of a P-HFE. Indeed, the proposed P-HFE comparison techniques are mainly related to the multiplying and exponential deformation formulas of each pair of possible membership degree and its associated probability. The proposed P-HFE comparison techniques are classified into three categories: (i) the element-based processes for comparing P-HFEs, (ii) the step-based processes for comparing P-HFEs, and (iii) the step-based processes for comparing P-HFEs. These indirect P-HFE comparison techniques provide us with more insights into the ways to increase the applicability of the P-HFE comparison algorithms, and enable us to deal with the comparison step in multiple criteria decision making problems from a different viewpoint.																	1432-7643	1433-7479															10.1007/s00500-020-05144-8		JUL 2020											
J								Parallel hybridization of series (PHOS) models for time series forecasting	SOFT COMPUTING										Series hybrid models; Parallel hybridization; Multilayer perceptrons (MLPs); Autoregressive integrated moving average (ARIMA); Time series forecasting; Weighting algorithm	WAVELET PACKET DECOMPOSITION; ECHO STATE NETWORK; HYBRID APPROACH; ELECTRICITY PRICE; ENSEMBLE; ARIMA; PREDICTION; SYSTEM; LOAD; DEMAND	Literature indicates that many efforts have been conducted toward the development of forecasting models with a high degree of accuracy. Combining different models is known as a powerful alternative to access more reliable and more accurate results than single models. Given the great importance of hybridization theory, various hybrid models have been proposed in the literature of time series forecasting. Series hybrid approaches are one of the most well study and the most widely used hybridization models, in which components are sequentially applied. However, according to the modeling essence of this hybrid structure, the performance of the series hybrid models is directly dependent on the order of using components and modeling sequence. Besides, selecting the best modeling order that can yield the best performance in all situations is a problematic theoretical as well as practical task. Thus, the main purpose of this paper is to eliminate the drawback of series models, regarding modeling order selection using a parallel hybridization schema, which is addressed for the first time. The core principle of the proposed parallel hybridization of series (PHOS) models is to improve the series hybrid model's forecasting accuracy by integrating two hybrid structures in contrast to existing hybrid models, which emphasize only the combination of individual models. The proposed model decomposes the original time series into two linear and nonlinear parts and uses the autoregressive integrated moving average (ARIMA) and multilayer perceptron neural network (MLP) models to model underlying patterns, incorporating two series models including ARIMA-MLP and MLP-ARIMA. Finally, the series models are integrated as components of the parallel hybridization scheme. Moreover, an ordinary least square algorithm is developed to determine the optimal weights of these two components. Three benchmark data sets, including the closing of the DAX index, the closing of the Nikkei 225 index (N225), and the opening of the Dow Jones Industrial Average Index, are used for empirical analysis and verifying the effectiveness of the PHOS model. The empirical analysis indicates that the PHOS model can improve the forecasting performance of both series ARIMA-MLP and MLP-ARIMA models as well as individual models and some parallel hybrid models.																	1432-7643	1433-7479															10.1007/s00500-020-05176-0		JUL 2020											
J								On hard c-means with cluster radius that makes the cluster partition homotopy equivalent to weighted alpha-complex	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Clustering; Topological data analysis; Persistent homology; Weighted alpha-complex; Hard c-means		In data mining, a combination of discovery of connected components of data by cluster analysis and capture of topological structure based on persistent homology attracted attention in recent years is effective. Especially in cluster analysis,k-means (KM) also referred to as hardc-means (HCM) and fuzzyc-means (FCM), which are representative methods and called objective-based clustering, assign data of each cluster to one representative point. These methods have an aspect of data compression. As a result, a large amount of data can be compressed and handled as representative points, so that the structure of the entire data can be known by grasping the topological structure of the representative points. Therefore, HCM and FCM have high affinity with persistent homology. In order to consider data mining methods using persistent homology, it is necessary that the mathematical property of filtration holds. However, here, there is the big problem that "topological structure made from HCM and FCM does not mathematically guarantee filtration". In this paper, we propose a new objective-based clustering method to classify a dataset into clusters with topological structure with filtration. The proposed algorithm makes the cluster partition homotopy equivalent to the weighted alpha-complex. This enables data mining using clustering and capture of the topological structure by persistent homology.																	1868-5137	1868-5145															10.1007/s12652-020-02160-2		JUL 2020											
J								Abstracting probabilistic models: Relations, constraints and beyond	KNOWLEDGE-BASED SYSTEMS											INFERENCE; COMPLEXITY; KNOWLEDGE	The Abstraction is a powerful idea widely used in science, to model, reason and explain the behavior of systems in a more tractable search space, by omitting irrelevant details. While notions of abstraction have matured for deterministic systems, the case for abstracting probabilistic models is not yet fully understood. In this paper, we provide a semantical framework for analyzing such abstractions from first principles. We develop the framework in a general way, allowing for expressive languages, including logic-based ones that admit relational, deterministic and hierarchical constructs with stochastic primitives. We motivate a definition of consistency between a high-level model and its low-level counterpart, but also treat the case when the high-level model is missing critical information present in the low-level model. We go on to prove properties of abstractions, both at the level of the parameter as well as the structure of the models. We conclude with some observations about how abstractions can be derived automatically. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105976	10.1016/j.knosys.2020.105976													
J								Multiple-source domain adaptation with generative adversarial nets	KNOWLEDGE-BASED SYSTEMS										Multi-source unsupervised domain adaptation; Deep learning; Transfer learning; Generative adversarial networks		Current unsupervised domain adaptation (UDA) methods based on GAN (Generative Adversarial Network) architectures assume that source samples arise from a single distribution. These methods have shown compelling results by finding the transformation between source and target domains to reduce the distribution divergence. However, the one-to-one assumption renders the existing GAN-based UDA methods ineffective in a more realistic scenario that source samples are typically collected from diverse sources. In this paper, we present a novel GAN-enabled framework, which we call Multi-Source Adaptation Network (MSAN), for multiple-source domain adaptation (MDA) to mitigate the domain shifts between multiple source domains and the target domain. The proposed framework consists of multiple GAN architectures to learn bidirectional transformations between the source domains and the target domain efficiently and simultaneously. Technically, we introduce a joint feature space to guide the multi-level consistency constraints across all the transformations, in order to preserve the domain-invariant pattern and endow the discriminative power for the unlabeled target samples simultaneously during the adaptation. Moreover, the proposed model can naturally be used to enlarge the target dataset by utilizing the synthetic target images (with ground-truth labels from different source domains) and the pseudo-labeled target images, thereby allowing constructing the target-specific classifier in an unsupervised manner. Experiments demonstrate that our models exceed state-of-the-art results for MDA tasks on several benchmark datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105962	10.1016/j.knosys.2020.105962													
J								Interpretable neural networks based on continuous-valued logic and multicriteria decision operators	KNOWLEDGE-BASED SYSTEMS										Explainable artificial intelligence; Continuous logic; Nilpotent logic; Neural network; Adversarial problems	OPTIMIZATION	Combining neural networks with continuous logic and multicriteria decision-making tools can reduce the black-box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future applications in deep neural networks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105972	10.1016/j.knosys.2020.105972													
J								Collaborative topological filtering with multi-hop recurrent pathological aggregation	KNOWLEDGE-BASED SYSTEMS										Recommender system; Collaborative filtering; Graph embedding; Recurrent neural network; Matrix factorization		Learning vectorial representations of users and items from their interaction data is a core approach for Collaborative Filtering. While a user's or an item's representation is commonly built upon low-hop features such as IDs and interaction history, some recent works argue the existence of higher-hop interactions, thereby motivating the use of multi-hop topological knowledge in representation learning. However, existing methods in this area explore only the local pathological connections and thus ignore the overall semantics along paths. To this end, this paper introduces a new CF approach that learns to explicitly inject the multi-hop topological features of a user or an item as a whole into its representation in an end-to-end manner. Specifically, we explore the multi-hop topology via the paths connecting a user or an item to its neighbors at different hops. To capture the entire topological information, we seamlessly integrate aggregator function with a recurrent neural network to jointly extract salient neighborhood information and detect the pathological semantics. We develop two neural network models, DF-CTF and DW-CTF, where the former focuses on modeling each individual path and the latter focuses on adapting to the path entanglement in multi-hop structures. Furthermore, we evaluate our proposed approach on three real-world benchmark datasets and demonstrate its superior performance against state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105969	10.1016/j.knosys.2020.105969													
J								An intelligent recognition model for dynamic air traffic decision-making	KNOWLEDGE-BASED SYSTEMS										Intelligent management systems; Air traffic flow; Dynamic decision-making; Intelligent rules	ALLOCATION	Air traffic flow management system (ATFMS) is becoming increasingly important due to the rapid growth of air traffic and serious flight delay nowadays. To aware the air traffic flow density and identify the heat airspace in terminal areas of large hub airports is essential for an ATFMS. Due to numerous parameters in air traffic flow, traditional methods based on one single parameter fail to reflect the true complexity relationship between these parameters. This study aims to develop an intelligent air traffic flow heat airspace recognition model using advanced data science technique for establishing a real-time cloud map in the terminal airspace of airports, which attempts to use machine learning models to represent the complex relationship among these parameters. In the proposed intelligent recognition model, high dimensions of parameters (basic parameters, additional parameters and time parameters) are processed to achieve a comprehensive and accurate situation awareness for support dynamic air traffic decision-making. An aircraft trajectories points clustering method is developed to generate a 4D heat airspace map. The basic parameters and time parameters are used to identify the heat airspaces; the changes of additional parameters which influence the heat airspaces are identified and analyzed by use of grid graphs of flight trajectories; probability fitting graphs are used to verify accuracy of 4D results in order to support air traffic decision-making. A case study on Beijing International Airport (PEK) is conducted to test our model and has obtained two main research findings: there are two areas of PEK that have the high density and there are hot peaks at two different heights; flight trajectories and speed of trajectories also effect on the heat airspace. The study realizes that the proposed 4D heat airspace model is better for detailed and accurate information construction, expression of spatial changes, and visualization of multiple parameters of temporal and spatial density and range. It can assist the decisions on airspace allocation, and also have a definite reference meaning on alleviating the contradiction between the current air traffic demand and airspace resource constraints. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				JUL 8	2020	199								105274	10.1016/j.knosys.2019.105274													
J								Biomedical-domain pre-trained language model for extractive summarization	KNOWLEDGE-BASED SYSTEMS										Extractive biomedical summarization; Document representation; Pre-trained language model; Fine-tuning	TEXT	In recent years, the performance of deep neural network in extractive summarization task has been improved significantly compared with traditional methods. However, in the field of biomedical extractive summarization, existing methods cannot make good use of the domain-aware external knowledge; furthermore, the document structural feature is omitted by existing deep neural network model. In this paper, we propose a novel model called BioBERTSum to better capture token-level and sentence-level contextual representation, which uses a domain-aware bidirectional language model pre-trained on large-scale biomedical corpora as encoder, and further fine-tunes the language model for extractive text summarization task on single biomedical document. Especially, we adopt a sentence position embedding mechanism, which enables the model to learn the position information of sentences and achieve the structural feature of document. To the best of our knowledge, this is the first work to use the pre-trained language model and fine-tuning strategy for extractive summarization task in the biomedical domain. Experiments on PubMed dataset show that our proposed model outperforms the recent SOTA (state-of-the-art) model by ROUGE-1/2/L. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105964	10.1016/j.knosys.2020.105964													
J								An evidential reasoning approach based on risk attitude and criterion reliability	KNOWLEDGE-BASED SYSTEMS										Evidential reasoning approach; Multiple criteria decision making; Risk attitude; Criterion reliability; Evidential reasoning rule	ATTRIBUTE DECISION-MAKING; VIKOR METHOD; UTILITY-THEORY; WEIGHTS; INFORMATION; SELECTION; ELECTRE; ALGORITHM; MODELS	In the evidential reasoning (ER) approach, in addition to the weight of a criterion, reliability is another important concept in connection with the criterion although it attracts little attention. Additionally, a decision maker's risk attitude is of particular importance in decision-making. To simultaneously consider these two important factors, this paper proposes a new ER approach. In the approach, with the consideration of a decision maker's risk attitude, the combinational reliability of each criterion for each alternative is constructed from the original reliability of the criterion. By following the regression idea to learn the original reliability of each criterion, a unified optimization model is constructed, in which the maximum difference between combinational reliabilities and their estimations is minimized. Within the post-optimal solution space of criterion reliabilities found by solving the unified model, another optimization model is constructed, from which the minimum and maximum expected utilities of each alternative are determined. Solutions to multi-criteria decision-making problems are then generated from the expected utilities. The proposed approach is used to analyze a material supplier selection problem for a company located in Tongling, Anhui, China. The analysis of the problem demonstrates the applicability of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105947	10.1016/j.knosys.2020.105947													
J								Decision-making and multi-objectivization for cost sensitive robust optimization over time	KNOWLEDGE-BASED SYSTEMS										Robust optimization over time; Switching cost; Decision-making; Multi-objective optimization; Multi-objectivization	DIFFERENTIAL EVOLUTION; DYNAMIC ENVIRONMENTS; SWARM OPTIMIZER; ALGORITHM; PERFORMANCE; OBJECTIVES; STRATEGY; SEARCH; MEMORY	Most existing research on dynamic optimization focuses on tracking the moving global optimum (TMO). Recently, a new paradigm for handling dynamic optimization, known as robust optimal over time (ROOT), has been proposed to avoid frequent changes in the optimal solutions. To explicitly minimize the costs incurred in switching solutions, a multi-objective ROOT algorithm has also been suggested. In practice, however, only one Pareto optimal solution can be adopted when the environment changes. To automate the decision-making process, this paper proposes a new approach that combines a ROOT/SCII algorithm with a policy to handle dynamic optimization problems. In the proposed approach, ROOT/SCII is used to simultaneously maximize the robustness and minimize the costs of switching solutions, and the policy is used to select a solution from the obtained Pareto set to be used in the new environment. In addition, multi-objectivization is introduced to enhance the efficiency in search for Pareto optimal solutions trading off between the robustness over time and the switching costs for the high dimension of decision space. Simulation results demonstrate that multi-objectivization is effective and the proposed approach is able to find a sequence of preferred solutions guided by the policy, considerably reducing the total switching costs while satisfying the user's robustness requirement, and outperforming TMO and ROOT in terms of switching cost minimization. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105857	10.1016/j.knosys.2020.105857													
J								Towards context-aware collaborative filtering by learning context-aware latent representations	KNOWLEDGE-BASED SYSTEMS										Context-aware collaborative filtering; Latent factor models; Top-N recommendation; Matrix factorization	RECOMMENDER SYSTEMS; GRAPH	Contexts have been proven to be an important source of information that can significantly improve the performance of collaborative filtering (CF), e.g., for recommendation. Most context-aware approaches that are basing on latent factor models assume that contexts share the same latent space with users and items. However such a strategy does not always make sense, e.g., the influence of contextual information may be overestimated. In this paper, we propose a generic framework to learn context-aware latent representations for context-aware collaborative filtering without imposing contexts into latent space of users and items. Contextual contents are combined via a function to produce the contextual influence factor, which is then combined with each latent factor to derive latent representations. We instantiate the generic framework using biased Matrix Factorization for rating prediction task and Bayesian Personalized Ranking (BPR) for item recommendation tasks. Stochastic Gradient Descent (SGD) based optimization procedures are developed to fit the two context-aware models by jointly learning the weight of each context and latent factors of users and items. Experiments conducted on three real-world datasets demonstrate that our context-aware CF model significantly outperforms not only the base models but also the representative context-aware models. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105988	10.1016/j.knosys.2020.105988													
J								The similarity-consensus regularized multi-view learning for dimension reduction	KNOWLEDGE-BASED SYSTEMS										Multi-view learning; Similarity consensus; Robust algorithm; Dimension reduction	CLASSIFICATION	During the last decades, learning a low-dimensional space with discriminative information for dimension reduction (DR) has gained a surge of interest. However, it is not accessible for these DR methods to achieve satisfactory performance when dealing with the features from multiple views. For multi-view learning problems, one instance can be represented by multiple heterogeneous features, which are highly relevant but sometimes look different from each other. In addition, correlations between features from multiple views always vary greatly, which challenges the capability of multi-view learning methods. Consequently, constructing a multi-view learning framework with generalization and scalability, which could take advantage of multi-view information as much as possible, is extremely necessary but challenging. To implement the above target, this paper proposes a novel multi-view learning framework based on similarity consensus, which makes full use of correlations among multi-view features while considering the scalability and robustness of the framework. It aims to straightforwardly extend those existing DR methods into multi-view learning domain by preserving the similarity consensus between different views to capture the low-dimensional embedding. Two schemes based on pairwise-consensus and centroid-consensus are separately proposed to force multiple views to learn from each other, then an iterative alternating strategy is developed to obtain the optimal solution. The proposed method is evaluated on 5 benchmark datasets and comprehensive experiments show that our proposed multi-view framework can yield comparable and promising performance with some famous methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105835	10.1016/j.knosys.2020.105835													
J								Collective disambiguation in entity linking based on topic coherence in semantic graphs	KNOWLEDGE-BASED SYSTEMS										Entity linking; Semantic annotation; Topic coherence; Named entity disambiguation	KNOWLEDGE	Entity Linking (EL) consists of determinating the entities that best represent the mentions in a document. Mentions can be very ambiguous and can refer to different entities in different contexts. In this paper, we present ABACO, a semantic annotation system for Entity Linking (EL) which addresses name ambiguity assuming that the entity that annotates a mention should be coherent with the main topics of the document. ABACO extracts a sub-graph from a knowledge base which interconnects all the candidate entities to annotate each mention in the document. Candidate entities are scored according to their degree of centrality in the knowledge graph and their textual similarity with the topics of the document, and worst candidates are pruned from the sub-graph. The approach has been validated with 13 datasets and compared with other 11 annotation systems using the GERBIL platform. Results show that ABACO outperforms the other systems for medium/large documents. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105967	10.1016/j.knosys.2020.105967													
J								Balanced scheduling of distributed workflow tasks based on clustering	KNOWLEDGE-BASED SYSTEMS										Workflow; Task clustering; Runtime balance; Dependency balance; Scheduling; Dependency correlation; Distributed computing		Distributed computing, such as Cloud, provides traditional workflow applications with completely new deployment architecture offering high performance and scalability. However, when executing the workflow tasks in a distributed computing environment, significant scheduling overheads are generated. Task clustering is a key technology to optimize process execution. Unreasonable task clustering can lead to the problems of runtime and dependency imbalance, which reduces the degree of parallelism during task execution. In order to solve the problem of runtime imbalance, we propose Runtime Balance Clustering Algorithm (RBCA), which employs the Backtracking approach to make the runtime of each cluster more balanced. In addition, to address the problem of dependency imbalance, we also propose Dependency Balance Clustering Algorithm (DBCA), which defines the dependency correlation to measure the similarity between tasks in terms of data dependencies. The tasks with high dependency correlation are clustered together so as to avoid the dependency imbalance to most extent. We conducted extensive experiments on the WorkflowSim platform and compared our algorithms with the existing task clustering algorithms. The results show that RBCA and DBCA significantly reduce the execution time of the whole workflow. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105930	10.1016/j.knosys.2020.105930													
J								A method of automatically generating initial parameters for large-scale belief rule base	KNOWLEDGE-BASED SYSTEMS										Belief rule base (BRB) model; Large-scale; Expert system; Cloud model; Evidential reasoning (ER) rule	EVIDENTIAL REASONING APPROACH; EXPERT-SYSTEM; MODEL; PREDICTION	The traditional rule-base inference methodology using evidential reasoning approach (RIMER) needs to traverse the reference values of all antecedent attributes when constructing belief rule base (BRB). Therefore, when many attributes have multiple reference levels, the scale of BRB will face the problem of combination explosion. Thus, it is inoperable to require experts to give all the parameters of each rule of BRB model, because of the limitation of expert knowledge in complex problems. If the initial BRB parameters are set unreasonably, the optimization speed and accuracy of the model will be affected. To solve this problem, a method of automatically generating initial parameters for large-scale BRB by using part of the standard rules and cloud model is proposed in this paper. Experts determine the number and parameters of standard rules through prior knowledge based on specific practical problems, and use cloud models to convert qualitative knowledge and quantitative information to automatically generate the remaining rules. A case study is established with Intel Berkeley Research lab data set to verify the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105904	10.1016/j.knosys.2020.105904													
J								Graph embedding-based approach for detecting group shilling attacks in collaborative recommender systems	KNOWLEDGE-BASED SYSTEMS										Collaborative recommender systems; Group shilling attacks; Shilling group detection; Graph embedding; Clustering	UNSUPERVISED METHOD; SPAMMER GROUPS	Over the past decade, many approaches have been presented to detect shilling attacks in collaborative recommender systems. However, these approaches focus mainly on detecting individual attackers and rarely consider the collusive shilling behaviors among attackers, i.e., a group of attackers working together to bias the output of collaborative recommender systems by injecting fake profiles. Such shilling behaviors are generally termed group shilling attacks, which are more harmful to collaborative recommender systems than traditional shilling attacks. In this paper, we propose a graph embedding-based method to detect group shilling attacks in collaborative recommender systems. First, we construct a user relationship graph by analyzing the user rating behaviors and use a graph embedding method to obtain the low-dimensional vector representation of each node in the user relationship graph. Second, we employ the k-means++ clustering algorithm to obtain candidate groups based on the generated user feature vectors. Finally, we calculate the suspicious degree of each candidate group according to the attack group detection indicators and use the Ward's hierarchical clustering method to cluster the candidate groups according to their suspicious degrees and obtain the attack groups. The experimental results on the Amazon and Netflix datasets show that the proposed method outperforms the baseline methods in detection performance. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105984	10.1016/j.knosys.2020.105984													
J								Finding structural hole spanners based on community forest model and diminishing marginal utility in large scale social networks	KNOWLEDGE-BASED SYSTEMS										Community detection; Community forest model; Diminishing marginal utility; Structural hole spanner	RESOURCES; POWER	Structural hole spanners play key role in information diffusion, community detection, epidemic diseases and rumors spreading, link prediction and viral marketing, the discovery for them is a key research work in the area of social networks. Some scholars have proposed inspired models and methods based on Mathematics, Sociology, and Economics. In this paper, we try to give a more visual and detailed definition of structural hole spanner based on the existing work, and propose a novel algorithm to identify structural hole spanner based on community forest model and diminishing marginal utility. Our work includes following four folds. Firstly we revealed the diminishing marginal utility phenomenon in the process of community reconstruction. Secondly we proved that metrics based on local or one-sided features cannot be used as a criterion for judging structural hole spanner. Thirdly we proved that the influence of SHS is not related with the distribution of SHS in the network. Fourthly we develop a novel algorithm to identify SHS. Our algorithm has slightly better performance than the state-of-the-art algorithms. It worked well on Zachary's karate club, American College Football, ground-truth samples sampled from DBLP, ground-truth samples sampled from Youtube and large-scale collaboration network DBLP. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105916	10.1016/j.knosys.2020.105916													
J								Intelligent fault diagnosis of rolling bearings based on normalized CNN considering data imbalance and variable working conditions	KNOWLEDGE-BASED SYSTEMS										Rolling bearing; Fault diagnosis; Convolutional neural network; Deep learning; Data imbalance	CONVOLUTIONAL NEURAL-NETWORK; ARTIFICIAL-INTELLIGENCE; ROTATING MACHINERY; SMOTE	Intelligent fault detection and diagnosis, as an important approach, play a crucial role in ensuring the stable, reliable and safe operation of rolling bearings, which is one of the most important components in the rotating machinery. In real industries, it is common to face that the issues of severe data imbalance and distribution difference since the number of fault data is small and the equipments frequently change the working conditions according to the production. To accurately and automatically identify the conditions of rolling bearings, a normalized convolutional neural network is proposed for the diagnosis of different fault seventies and orientations considering data imbalance and variable working conditions. First, the batch normalization is adopted as a novel application to eliminate feature distribution difference, which is the prerequisite for ensuring generalization ability under different working conditions. Then, a special model structure is established and the overall performances of the proposed model are optimized by iterative update, which combines the exponential moving average technology. Finally, the proposed model is applied to the fault diagnosis under different data imbalance cases and working conditions. The effectiveness of the proposed method is verified based on two popular experiment dataset, and the diagnosis performance is widely evaluated in different scenarios. Comparisons with other commonly used methods and related works on the same dataset demonstrate the superiority of the proposed method. The results show that the proposed method has excellent diagnosis accuracy and admirable robustness, and also has sufficient stability on the data imbalance. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105971	10.1016/j.knosys.2020.105971													
J								MTMA: Multi-task multi-attribute learning for the prediction of adverse drug-drug interaction	KNOWLEDGE-BASED SYSTEMS										Adverse drug-drug interaction; Multi-task; Multi-attribute; Supervised learning; Tensor decomposition	LARGE-SCALE PREDICTION; INTERACTION EXTRACTION; SUBSTRUCTURES; ZIDOVUDINE; ACYCLOVIR; ALGORITHM; DISCOVERY; MACHINE; GRAPH	Adverse drug-drug interaction (ADDI) is an important issue in drug developments and clinical applications, which causes a significant burden in the healthcare system and leads to serious morbidity and mortality in patients. Many methods are proposed for ADDI prediction due to the accumulation of healthcare data in a massive scale. However, these methods are insufficient in exploring the potential adverse mechanisms among drugs and incapable of revealing the leading factors of ADDIs. In this paper, we propose a Multi-Task Multi-Attribute (MTMA) learning model for ADDI prediction. In MTMA, two drug attributes, molecular structure and side effect, are adopted to model the adverse interactions among drugs and two interpretable tensors, adverse molecular structure-molecular structure interaction tensor and adverse side effect-side effect interaction tensor, are designed to uncover the adverse mechanisms among drugs. Meanwhile, we impose l(2,1)-norm on the predicted attribute matrices to explore the leading molecular substructures and side effects for each specific ADDI. The optimization problem of MTMA is solved by an alternating algorithm based on the methods of low-rank tensor decomposition and stochastic gradient descent. Experiments on the real-world dataset demonstrate the considerable performance of MTMA when compared with nine baseline methods and its three variants. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 8	2020	199								105978	10.1016/j.knosys.2020.105978													
J								Discovering sentiment potential in Twitter conversations with Hilbert-Huang spectrum	EVOLVING SYSTEMS										Opinion polarity; Functional analytics; Emotional influence; Social media analytics; Topic sampling; Signal processing for social media; Fourier spectrum; Hilbert-Huang transform	SOCIAL MEDIA	Does a tweet with specific emotional content posted by an influential account have the capability to shape or even completely alter the opinions of its readers? Moreover, can other influential accounts further enhance its original emotional potential by retweeting it and, thus, letting their followers read it? Real Twitter conversations seem to imply an affirmative answer to both questions. If this is indeed the case, then what is the key for not only successfully reaching to a large number of accounts but also for convincingly offering an alternative perspective via affective means, therefore triggering a large scale opinion change in an ongoing Twitter conversation? This work primarily focuses on determining which tweets cause multiple sentiment polarity alternations to occur based on a window segmentation approach. Moreover, an offline framework for discovering affective pivot points in a conversation based on its Hilbert-Huang spectrum, which has close ties to the Fourier transform, is introduced. Finally, given that it is highly desirable to track the sentiment shifts of a Twitter conversation while it unfolds, an adaptive scheme is presented for approximating the window sizes obtained by the offline methodology. As a concrete example, the abovementioned methodologies are applied to three recent long Twitter discussions and the results are analyzed.																	1868-6478	1868-6486															10.1007/s12530-020-09348-z		JUL 2020											
J								Geometric Affordance Perception: Leveraging Deep 3D Saliency With the Interaction Tensor	FRONTIERS IN NEUROROBOTICS										affordance; affordance detection; visual perception; learning; cognitive robotics	DEVELOPMENTAL ROBOTICS; OBJECT AFFORDANCES	Agents that need to act on their surroundings can significantly benefit from the perception of their interaction possibilities or affordances. In this paper we combine the benefits of the Interaction Tensor, a straight-forward geometrical representation that captures multiple object-scene interactions, with deep learning saliency for fast parsing of affordances in the environment. Our approach works with visually perceived 3D pointclouds and enables to query a 3D scene for locations that support affordances such as sitting or riding, as well as interactions for everyday objects like the where to hang an umbrella or place a mug. Crucially, the nature of the interaction description exhibits one-shot generalization. Experiments with numerous synthetic and real RGB-D scenes and validated by human subjects, show that the representation enables the prediction of affordance candidate locations in novel environments from a single training example. The approach also allows for a highly parallelizable, multiple-affordance representation, and works at fast rates. The combination of the deep neural network that learns to estimate scene saliency with the one-shot geometric representation aligns well with the expectation that computational models for affordance estimation should be perceptually direct and economical.																	1662-5218					JUL 7	2020	14								45	10.3389/fnbot.2020.00045													
J								Spatiotemporal saliency-based multi-stream networks with attention-aware LSTM for action recognition	NEURAL COMPUTING & APPLICATIONS										Spatiotemporal saliency; Multi-stream; Attention-aware; LSTM; Action recognition	HASHING-BASED APPROACH; SERVICE RECOMMENDATION	Human action recognition is a process of labeling video frames with action labels. It is a challenging research topic since the background of videos is usually chaotic, which will reduce the performance of traditional human action recognition methods. In this paper, we propose a novel spatiotemporal saliency-based multi-stream ResNets (STS), which combines three streams (i.e., a spatial stream, a temporal stream and a spatiotemporal saliency stream) for human action recognition. Further, we propose a novel spatiotemporal saliency-based multi-stream ResNets with attention-aware long short-term memory (STS-ALSTM) network. The proposed STS-ALSTM model combines deep convolutional neural network (CNN) feature extractors with three attention-aware LSTMs to capture the temporal long-term dependency relationships between consecutive video frames, optical flow frames or spatiotemporal saliency frames. Experimental results on UCF-101 and HMDB-51 datasets demonstrate that our proposed STS method and STS-ALSTM model obtain competitive performance compared with the state-of-the-art methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14593	14602		10.1007/s00521-020-05144-7		JUL 2020											
J								Time series forecasting with feedforward neural networks trained using particle swarm optimizers for dynamic environments	NEURAL COMPUTING & APPLICATIONS										Time series forecasting; Neural networks; Particle swarm optimization; Cooperative quantum particle swarm optimization	HYBRID GENETIC ALGORITHM; OPTIMIZATION; PREDICTION; MODEL; POWER	Several studies have applied particle swarm optimization (PSO) algorithms to train neural networks (NNs) for time series forecasting and the results indicated good performance. These studies, however, assumed static environments, making the PSO trained NNs unsuitable for forecasting many real-world time series which are generated by non-stationary processes. This study formulates training of a NN forecaster as a dynamic optimization problem, to investigate the application of a dynamic PSO algorithm to train NNs in forecasting time series in non-stationary environments. For this purpose, a set of experiments were conducted on three simulated and seven real-life time series forecasting problems under four different dynamic scenarios. Results obtained are compared to the results of NNs trained using a standard PSO and resilient backpropagation (Rprop). The results show that the NNs trained using dynamic PSO algorithms outperform the NNs trained using PSO and Rprop. These findings highlight the potential of using dynamic PSO in training NNs for real-world forecasting applications.																	0941-0643	1433-3058															10.1007/s00521-020-05163-4		JUL 2020											
J								Hilbert sEMG data scanning for hand gesture recognition based on deep learning	NEURAL COMPUTING & APPLICATIONS										Hilbert curve; Hand gesture recognition; sEMG; Electromyography; Classification; CNN; Deep learning; Multi-scale	PATTERN-RECOGNITION; ELECTRODE NUMBER; EMG SIGNALS; SURFACE EMG; ROBUST; REHABILITATION; EXTRACTION; SHIFT	Deep learning has transformed the field of data analysis by dramatically improving the state of the art in various classification and prediction tasks, especially in the area of computer vision. In biomedical engineering, a lot of new work is directed toward surface electromyography (sEMG)-based gesture recognition, often addressed as an image classification problem using convolutional neural networks (CNNs). In this paper, we utilize the Hilbert space-filling curve for the generation of image representations of sEMG signals, which allows the application of typical image processing pipelines such as CNNs on sequence data. The proposed method is evaluated on different state-of-the-art network architectures and yields a significant classification improvement over the approach without the Hilbert curve. Additionally, we develop a new network architecture (MSHilbNet) that takes advantage of multiple scales of an initial Hilbert curve representation and achieves equal performance with fewer convolutional layers.																	0941-0643	1433-3058															10.1007/s00521-020-05128-7		JUL 2020											
J								Users' Experience of Digital Wayfinding Screens: A Uses and Gratification Perspective from South Africa	ADVANCES IN HUMAN-COMPUTER INTERACTION											INTERNET USES; SIGNAGE; TELEVISION; BEHAVIOR; MEDIA; INTERACTIVITY; COMMUNICATION; PATTERNS; MALLS	Marketing and business communication researchers have neglected the wayfinding capabilities of digital out-of-home communication in the retailing landscape. The current study focuses on digital wayfinding screens in the South African shopping mall environment. The aim is understanding users' experience of digital wayfinding screens, guided by the uses and gratification theory. Shoppers were interviewed about their views and actions while engaging in the wayfinding process in large upmarket shopping malls. The in-depth semistructured interviews were recorded and then the content was analysed. The findings provide a rich and comprehensive understanding of shoppers' content gratifications and process gratifications when utilising this contemporary medium. The current study identifies four uses and gratifications for digital wayfinding screens: convenient process gratifications, interactive process gratifications, informational content gratification, and entertaining content gratifications. Understanding the gratification dimensions of digital wayfinding screens contributes to contemporary media research and forms the basis of valuable guidelines for practitioners in retail media and design.																	1687-5893	1687-5907				JUL 7	2020	2020								7636150	10.1155/2020/7636150													
J								Identifying Significance of Product Features on Customer Satisfaction Recognizing Public Sentiment Polarity: Analysis of Smart Phone Industry Using Machine-Learning Approaches	APPLIED ARTIFICIAL INTELLIGENCE											FEATURE-SELECTION; REVIEWS; LOYALTY	The reality about human behavior is that how other people think and evaluate, and have strong influences on our beliefs and thinking. Consumers get rich information from online reviews that may reduce their uncertainty regarding purchases. Besides, product-developing companies analyze user demands from online reviews to design market-driven product. In this study, a comparison among five major market share holder smart phone brands - Samsung, Apple, Huawei, Xiaomi, and Oppo is performed in different price categories - high, mid, and low range, based on sentiment polarity score. Online public reviews are extracted and sentiment scores of reviews are calculated to construct public sentiment polarity toward the famous brands. By examining both quantitative and qualitative methodologies, we identified the most important smart phone features or attributes that have great significance on consumer satisfaction. By experimenting and comparing five efficient machine-learning algorithms in predicting sentiment polarity and three feature selection algorithms in reducing attributes, an optimal set of 21 smart phone attributes was found those play major roles in determining customer satisfaction.																	0883-9514	1087-6545				SEP 18	2020	34	11					832	848		10.1080/08839514.2020.1787676		JUL 2020											
J								Estimating Forest Losses Using Spatio-temporal Pattern-based Sequence Classification Approach	APPLIED ARTIFICIAL INTELLIGENCE											SPATIAL-DISTRIBUTION; DEFORESTATION; INFORMATION; FIRES; AREA	Consistent forest loss estimates are important to enforce forest management regulations. In Tunisia, recent evidence has suggested that the deforestation rate is increasing, especially since the 2011's Revolution. However, no spatially explicit data on the extent of deforestation before and after the Revolution exists. Here, we quantify deforestation in the country for the period 2001-2014 and we propose a novel spatio-temporal pattern-based sequence classification framework for forest loss estimation. To do so, expert knowledge and spatial techniques are applied to identify deforestation drivers. Then, we adopt sequential pattern mining to extract sets of patterns sharing similar spatiotemporal behavior. The sequence miner generates multidimensional-closed sequential patterns at different time granularities. Then, a discriminative filter is employed to decide on patterns to use as relevant classification features. Lastly, the classifier is trained using random forest and shows an improved result.																	0883-9514	1087-6545				OCT 14	2020	34	12					916	940		10.1080/08839514.2020.1790247		JUL 2020											
J								CGMBE: a model-based tool for the design and implementation of real-time image processing applications on CPU-GPU platforms	JOURNAL OF REAL-TIME IMAGE PROCESSING											SCHEDULER	Processing large images in real time requires effective image processing algorithms as well as efficient software design and implementation to take full advantage of all CPU cores and GPU resources on state of the art CPU/GPU platforms. Efficiently coordinating computations on both the host (CPU) and devices (GPUs), along with host-device data transfers is critical to achieving real-time performance. However, such coordination is challenging for system designers given the complexity of modern image processing applications and the targeted processing platforms. In this paper, we present a novel model-based design tool that automates and optimizes these critical design decisions for real-time image processing implementation. The proposed tool consists of a compile-time static analyzer and a run-time dynamic scheduler. The tool automates the process of scheduling dataflow tasks (actors) and coordinating CPU-GPU data transfers in an integrated manner. The approach uses an unfolded dataflow graph representation of the application along with thread-pool-based executors, which are optimized for efficient operation on the targeted CPU-GPU platform. This approach automates the most complicated aspects of the design and implementation process for image processing system designers, while maximizing the utilization of computational power, reducing the memory footprint for both the CPU and GPU, and facilitating experimentation for tuning performance-oriented designs.																	1861-8200	1861-8219															10.1007/s11554-020-00994-9		JUL 2020											
J								Multisensory instrumental dynamics as an emergent paradigm for digital musical creation A retrospective and prospective of haptic-audio creation with physical models	JOURNAL ON MULTIMODAL USER INTERFACES										Physical modelling; Virtual musical instruments; Audio-Haptic; Artistic Creation	FORCE-FEEDBACK; SIMULATION; SOUND; FRAMEWORK; DESIGN; TOUCH	The nature of human/instrument interaction is a long-standing area of study, drawing interest from fields as diverse as philosophy, cognitive sciences, anthropology, human-computer-interaction, and artistic creation. In particular, the case of the interaction between performer and musical instrument provides an enticing framework for studying the instrumental dynamics that allow for embodiment, skill acquisition and virtuosity with (electro-)acoustical instruments, and questioning how such notions may be transferred into the realm of digital music technologies and virtual instruments. This paper offers a study of concepts and technologies allowing for instrumental dynamics with Digital Musical Instruments, through an analysis of haptic-audio creation centred on (a) theoretical and conceptual frameworks, (b) technological components-namely physical modelling techniques for the design of virtual mechanical systems and force-feedback technologies allowing mechanical coupling with them, and (c) a corpus of artistic works based on this approach. Through this retrospective, we argue that artistic works created in this field over the last 20 years-and those yet to come-may be of significant importance to the haptics community as new objects that question physicality, tangibility, and creativity from a fresh and rather singular angle. Following which, we discuss the convergence of efforts in this field, challenges still ahead, and the possible emergence of a new transdisciplinary community focused on multisensory digital art forms.																	1783-7677	1783-8738				SEP	2020	14	3			SI		235	253		10.1007/s12193-020-00334-y		JUL 2020											
J								Economic data analytic AI technique on IoT edge devices for health monitoring of agriculture machines	APPLIED INTELLIGENCE										Green IoT; Agricultural machine; Artificial neural network; Evolutionary algorithm; Edge computation; Health-monitoring	EVOLUTIONARY OPTIMIZATION; MODEL	In the era of Internet of things (IoT), network Connection of an enormous number of agriculture machines and service centers is an expectation. However, it will be with a generation of massive volume of data, thus overwhelming the network traffic and storage system especially when manufacturers give maintenance service typically by various data analytic applications on the cloud. The situation is more complex in the context of low latency applications such as health monitoring of agriculture machines, although require emergency responses. Performing the computational intelligence on edge devices is one of the best approaches in developing green communications and managing the blast of network traffic. Due to the increasing usage of smartphone applications, the edge computation on the smartphone can highly assist the network traffic management. In connection with the mentioned point, in the context of exploiting the limited computation power of smartphones, the design of an AI-based data analytic technique is a challenging task. On the other hand, the users' need for economic technology makes it not to be easily pierced. This research work aims both targets by presenting a bi-level genetic algorithm approach of an optimized data analytic AI technique for monitoring the health of the agriculture vehicles which can be economically utilized on smartphone end-devices using the built-in microphones instead of expensive IoT sensors.																	0924-669X	1573-7497				NOV	2020	50	11					3990	4016		10.1007/s10489-020-01744-x		JUL 2020											
J								Content-aware web robot detection	APPLIED INTELLIGENCE										Web robot; Crawler; Semantics; Supervised learning; Latent dirichlet allocation		Web crawlers account for more than a third of the total web traffic and they are threatening the security, privacy and veracity of web applications and their users. Businesses in finance, ticketing, and publishing, as well as websites with rich and unique content are the ones mostly affected by their actions. To deal with this problem, we present a novel web robot detection approach that takes advantage of the content of a website based on the assumption that human web users are interested in specific topics, while web robots crawl the web randomly. Our approach extends the typical user session representation of log-based features with a novel set of features that capture the semantics of the content of the requested resources. In addition, we contribute a new real-world dataset, which we make publicly available, towards alleviating the scarcity of open data in this field. Empirical results on this dataset validate our assumption and show that our approach outranks state-of-the-art methods for web robot detection.																	0924-669X	1573-7497				NOV	2020	50	11					4017	4028		10.1007/s10489-020-01754-9		JUL 2020											
J								Challenges in benchmarking stream learning algorithms with real-world data	DATA MINING AND KNOWLEDGE DISCOVERY										Data stream; Concept drift; Classification; Drift detection; Benchmark data	AEDES STEGOMYIA AEGYPTI; DETECTING CONCEPT DRIFT; CLASSIFICATION; TIME; DIPTERA; VECTOR; CLASSIFIERS; TEMPERATURE; COMPLEXITY; DISTANCE	Streaming data are increasingly present in real-world applications such as sensor measurements, satellite data feed, stock market, and financial data. The main characteristics of these applications are the online arrival of data observations at high speed and the susceptibility to changes in the data distributions due to the dynamic nature of real environments. The data stream mining community still faces some primary challenges and difficulties related to the comparison and evaluation of new proposals, mainly due to the lack of publicly available high quality non-stationary real-world datasets. The comparison of stream algorithms proposed in the literature is not an easy task, as authors do not always follow the same recommendations, experimental evaluation procedures, datasets, and assumptions. In this paper, we mitigate problems related to the choice of datasets in the experimental evaluation of stream classifiers and drift detectors. To that end, we propose a new public data repository for benchmarking stream algorithms with real-world data. This repository contains the most popular datasets from literature and new datasets related to a highly relevant public health problem that involves the recognition of disease vector insects using optical sensors. The main advantage of these new datasets is the prior knowledge of their characteristics and patterns of changes to adequately evaluate new adaptive algorithms. We also present an in-depth discussion about the characteristics, reasons, and issues that lead to different types of changes in data distribution, as well as a critical review of common problems concerning the current benchmark datasets available in the literature.																	1384-5810	1573-756X				NOV	2020	34	6					1805	1858		10.1007/s10618-020-00698-5		JUL 2020											
J								Novel Sybil attack detection using RSSI and neighbour information to ensure secure communication in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster head; Sybil attack; Neighbour; Wireless sensor network	SENSOR; NODES	Wireless sensor networks (WSN) are generally employed in unattended hostile areas such as forest monitoring, agriculture fields, military battlefields. An adversary can physically capture WSN deployed in a hostile environment. Once an adversary captures a node, the cryptographic information and the software program can be easily extracted. Also, the adversary can reprogram the software inside the node. After reprogramming, the adversary can replicate and deploy the node with multiple identities to do malicious activities. This kind of identity theft attack can be classified as Clone attack or Sybil attack. These identity theft attacks are addressed by many distributed, centralised and localised solutions. Most of these solutions utilise private/public key, and symmetric key algorithms are energy and memory demanding, while the WSN are energy and memory constrained. This paper proposes a novel Sybil attack detection protocol (NoSad) to identify and isolate the Sybil attack in WSN. This protocol is a localised method using intra-cluster communication and RSSI value to identify the Sybil node. The proposed protocol is simulated extensively with various topologies, and obtained results prove that the protocol is highly efficient in detection ratio, energy utilisation, memory usage, computation and communication requirement. This protocol may be used in any resource-constrained WSN to obtain a satisfactory result. This work provides a solution for different scenarios of Sybil node position to counter the Sybil attack in WSN.																	1868-5137	1868-5145															10.1007/s12652-020-02276-5		JUL 2020											
J								Design of ensemble classifier using Statistical Gradient and Dynamic Weight LogitBoost for malicious tumor detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ensemble classifier; Statistical Gradient; Weight based LogitBoost; probability density function; gradient vector		In medical field, the detection of abnormalities in breast is essential to find earlier stage of breast tumor. Conventional semi-supervised ensemble framework based on the normalized cut algorithm developed and it strongly improves the detection accuracy of the resulting images. However, further development of classification algorithm with minimum computational time and cost, several conventional methods limits the classification of tumor. In this paper, Statistical Gradient and Dynamic Weight based LogitBoost (SG-DWL) Ensemble approach is presented to improve the detection rate of malicious tumor. The key objective of SG-DWL Ensemble approach is to increase the malicious tumor performance with higher accuracy and lesser time consumption. The Statistical Gradient Boosting model is introduced as a numerical technique to improve feature set identification in test images. The proposed feature ensemble is formed by concatenating the probability density function, gradient vector and powerful algorithmic framework for feature selection with the best fit feature set is selected. Dynamic Weight based LogitBoost classifier (DW-LC) is applied for malicious tumor detection. This Dynamic Weight based LogitBoost classifier uses Hoeffding tree to achieve high malicious tumor detection rate by reducing the computational complexity involved in the classification of benign and malignant tumor. The performance of the proposed approach is evaluated by comparing it with the existing approaches, and the results improve the classification accuracy with minimum time period for malicious tumor detection.																	1868-5137	1868-5145															10.1007/s12652-020-02295-2		JUL 2020											
J								Normal discriminant deep convolution neural classification based web behavioral pattern mining for user identification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Web user access behavior pattern mining; Deep convolution feedforward neural learning; Normal discriminant preprocessing analysis; Tanimoto similarity; Sigmoid activation function	USAGE; MODEL	Mining the user behavior patterns from huge log files plays a major role in discovering the web user identity. Several data mining techniques describe the way of predicting the user identity using web access behavior patterns. Accurate prediction of the user identity is still a challenging issue. The proposed work introduces a normal discriminant Tanimoto similarity based deep convolution feedforward neural learning classification (NDTS-DCFNLC) technique to improve the user identifications with their web access behavior patterns. The NDTS-DCFNLC technique comprises multiple layers. Normal discriminant preprocessing is carried out in layer one to remove the unwanted patterns from the web access log files. Similarity among the relevant web pattern is calculated using Tanimoto similarity at layer two. Sigmoid activation function is used in output layer to classify the frequently accessed patterns with higher accuracy and minimum error based on similarity threshold. Experimental testing is done using apache weblog with different number of patterns and the diverse constraints such as classification accuracy, false positive rate, space requirements and execution time. Results were compared with linear temporal logic and KNN classification methods. The outcome shows NDTS-DCFNLC technique increases classification accuracy, decreases the false positives, execution time and the space requirements.																	1868-5137	1868-5145															10.1007/s12652-020-02291-6		JUL 2020											
J								Hesitant fuzzy parameterized soft sets and their applications in decision making	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hesitant fuzzy set; Soft set; Hesitant fuzzy parameterized soft set; Decision making		The fuzzy set theory plays an important role in the modeling of the problems involving uncertain data. Some extensions of the fuzzy sets are needed due to the variety of problems encountered in real life. The concept of a hesitant fuzzy set is one of these extensions. Also, soft set theory, which is free from the difficulties of determining the membership function in fuzzy sets, plays an important role in dealing with uncertainty. In this study, we introduce the concept of hesitant fuzzy parameterized soft set as a generalization of the fuzzy parameterized soft sets. Then we define set-theoretical operations of the hesitant fuzzy parameterized soft sets and obtain some of their properties. We also improve a decision-making algorithm under the hesitant fuzzy parameterized soft environment and give an example to show the process of the algorithm. Finally, we compare the proposed decision-making algorithm with methods existing in the literature.																	1868-5137	1868-5145															10.1007/s12652-020-02258-7		JUL 2020											
J								Using EGDL to represent domain knowledge for imperfect information automated negotiations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Game description language; Imperfect information; Automated negotiation; General game playing	GAME DESCRIPTION	The current work has limitations in using GDL to represent domain knowledge for Automated Negotiations, which does not support imperfect information games in negotiation scenarios. In this paper, we expand the GDL and improve the automatic negotiation model so that the framework can describe the negotiation scenarios of imperfect information, and each agent can reason according to the domain knowledge we describe. Through examples, we prove that EGDL is an effective method to represent domain knowledge for Automated Negotiations of imperfect information game, and through experiments, we prove that each agent has higher utilities after negotiations.																	1868-5137	1868-5145															10.1007/s12652-020-02274-7		JUL 2020											
J								ERTC: an Enhanced RSSI based Tree Climbing mechanism for well-planned path localization in WSN using the virtual force of Mobile Anchor Node	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile Anchor node (MAN); Path planning; Sensor nodes; Wireless sensor network; Virtal force	WIRELESS; BEACON	In this paper, an Enhanced RSSI Tree Climbing (ERTC) technique is proposed to design a well-planned path based localization using virtual force of Mobile Anchor Node (MAN) in Wireless Sensor Network (WSN). The MAN is equipped with both Omni directional and directional antennae. Since an Omni directional antenna used for broadcasting the message and directional antenna is used for receiving the messages. This proposed technique is used to identify the trajectory of the MAN with the virtual force of unknown nodes in the network. Further, the circum center algorithm is used to identify the location of unknown sensor node. The proposed technique is implemented in the NS2 simulation. Simulation results shows that an ERTC achieves lower path length and better localization accuracy with the existing trajectories HILBERT space filling curve, Z trajectory, Swarm intelligence path planning techniques Grey wolf optimizer (GWPP) and Whale Optimizer based Path Planning (WOPP). The efficacy of node coverage in ERTC is compared with the Improved Virtual Force Algorithm (IVFA). The coverage analysis of ERTC shows better results by using virtual force of unknown node than IVFA.																	1868-5137	1868-5145															10.1007/s12652-020-02286-3		JUL 2020											
J								Efficient segmentation-based methods for anomaly detection in static and streaming time series under dynamic time warping	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Anomaly detection; Static time series; Streaming time series; Dynamic time warping; Clustering; Discord; Anomaly score; Segmentation	ALGORITHM; RETRIEVAL; SEQUENCES	The problem of time series anomaly detection has attracted a lot of attention due to its usefulness in various application domains. However, most of the methods proposed so far used Euclidean distance to deal with this problem. Dynamic Time Warping (DTW) distance is more suitable than Euclidean distance because of its capability in shape-based similarity checking in many practical fields, for example those with multimedia data. In this paper, we propose two efficient anomaly detection methods, EP-Leader-DTW and SEP-Leader-DTW, for static and streaming time series under DTW, respectively. Our methods are based on time series segmentation, subsequence clustering, and anomaly scoring. For segmentation, the major extrema method is used to obtain subsequences. For clustering, we apply Leader algorithm to cluster the subsequences along with a lower bounding technique to accelerate DTW distance computation. Experimental results on several benchmark time series datasets reveal that our method for anomaly detection in static time series under DTW can perform very fast and accurately on large time series datasets. For streaming time series, our method can meet the instantaneous requirement with high accuracy. As a result, our anomaly detection methods are applicable to both static and streaming time series in practice.																	0925-9902	1573-7675															10.1007/s10844-020-00609-6		JUL 2020											
J								Error analysis based on error transfer theory and compensation strategy for LED chip visual localization systems	JOURNAL OF INTELLIGENT MANUFACTURING										Error analysis; Error compensation strategy; Error transfer theory; Visual localization; LED chip	FEEDBACK-CONTROL; ENHANCEMENT; ACCURACY	In the manufacturing process of LED chips, the accuracy of the LED chip visual localization system affects the quality of LED chip production directly. There are many errors that have impacts on positioning system accuracy. Therefore, the identification and compensation of the critical errors is key to efficiently improving the precision of the system. Based on this fact, an error analysis method and an error compensation strategy are proposed in this paper. The first step was to measure the relevant error sources that may affect the localization system. Then error model of localization system was established, and the validity of this error model was verified by comparing simulated and actual positioning results. In addition, the impact factors of each error source on localization system accuracy were obtained using the error transfer theory. According to the error analysis results, an efficient error compensation strategy was proposed, which could compensate the errors in order of impact factors, and judge whether the error compensation method is optimal. Finally, the experimental results proved that the proposed error analysis method was valid and the error compensation strategy could efficiently enhance the positioning accuracy to meet the industrial application requirements.																	0956-5515	1572-8145															10.1007/s10845-020-01615-9		JUL 2020											
J								Characterizations and uncertainty measurement of a fuzzy information system and related results	SOFT COMPUTING										Rough set theory; Fuzzy information system; Equality; Dependence; Independence; Fuzzy distance; Inclusion degree; Homomorphism; Characterization; Uncertainty; Measure	ROUGH SETS	An information system (IS) is an important model in the field of artificial intelligence. A fuzzy information system (FIS) may be regarded as an IS under fuzzy environment. This paper obtains some results on a FIS. Operators on a FIS are first researched. Then, relationships between FISs are discussed from two aspects of dependence and separability, and dependence between FISs is characterized by the inclusion degree and two kinds of fuzzy distances between FISs are proposed. Next, algebraic characterizations of a FIS are obtained and invariant characterizations of a FIS under some homomorphisms are given. Finally, measuring uncertainty of a FIS is investigated, and experimental analyses illustrates that the proposed measures are suitable.																	1432-7643	1433-7479				SEP	2020	24	17					12753	12771		10.1007/s00500-020-05138-6		JUL 2020											
J								A multi-objective decomposition-based ant colony optimisation algorithm with negative pheromone	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Ant colony optimisation; non-dominated solutions; pheromone; dominated solutions	GENETIC ALGORITHM; MOEA/D	Existing ant colony algorithms only have one kind of pheromone. They use non-dominated solutions to update it while not making use of dominated solutions, which can provide valuable information for guiding the subsequent foraging process. To make full use of dominated solutions, we create a new kind of pheromone temporarily called a negative pheromone and propose a new ant colony optimisation algorithm called NMOACO/D, which combines MOEA/D-ACO with the negative pheromone. Many experiments have been carried out in this study to compare NMOACO/D with MOEA/D-ACO and other algorithms on several bi-objective travelling salesman problems. We demonstrate that NMOACO/D outperforms the MOEA/D-ACO and six different recently proposed related algorithms on all nine test instances. We also evaluate the effect of negative pheromone on the performance of the NMOACO/D. The results in this paper show that correctly making use of the information related to dominated solutions can further improve the ant colony algorithm performance.																	0952-813X	1362-3079															10.1080/0952813X.2020.1789753		JUL 2020											
J								A memetic algorithm for restoring feasibility in scheduling with limited makespan	NATURAL COMPUTING										Scheduling; Infeasibility; Memetic algorithm; Local search	GENERATION SCHEMES; GENETIC ALGORITHM; COMPLETION-TIME; JOB; CONSTRAINT; SEARCH	When solving a scheduling problem, users are often interested in finding a schedule optimizing a given objective function. However, in some settings there can be hard constraints that make the problem unfeasible. In this paper we focus on the task of repairing infeasibility in job shop scheduling problems with a hard constraint on the makespan. In this context, earlier work addressed the problem of computing the largest subset of the jobs that can be scheduled within the makespan constraint. Herein, we face a more general weighted version of the problem, consisting in computing a feasible subset of jobs maximizing their weighted sum. To this aim, we propose an efficient memetic algorithm, that combines a genetic algorithm with a local search method, also proposed in the paper. The results from an experimental study show the practical suitability of our approach.																	1567-7818	1572-9796															10.1007/s11047-020-09796-1		JUL 2020											
J								High-definition map update framework for intelligent autonomous transfer vehicles	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										HD Map; autonomous transfer vehicles; smart factories; autonomous navigation	MOBILE ROBOT; SIMULTANEOUS LOCALIZATION	Autonomous transfer vehicles (ATVs) can be considered as one of the critical components of context-aware structured smart factories in Industry 4.0 era. Conventional mapping methods such as grid maps can provide information for navigation, but they are not enough for complex environments that require interactions. On the other hand, high-definition (HD) mapping, which is mainly used in traffic networks, includes more information about an environment to perform excellent autonomous behaviour. In order to increase the efficiency of ATVs in flexible factories, an up-to-date environmental map information is required to perform successful long-term autonomous navigation. Therefore, when there exists a change in the environment, a simultaneous update of HD-map is as important as the creation of it. In this study, we propose an HD-map update methodology for ATVs that operates in smart factories. To the best of our knowledge, HD mapping has not been applied in smart factories. The proposed method includes the object detection and localisation tool to detect objects visually and determines their positions in connection with the conventional maps of the environment. Experimental results of a simulated factory environment demonstrate that the ATV can properly update the HD-map when a predefined sign is removed from or a new sign is added to the environment.																	0952-813X	1362-3079															10.1080/0952813X.2020.1789754		JUL 2020											
J								A cyber-physical environment for detecting exceptional and dangerous human behavior in the home by sensors and its verification by computer simulation	ADAPTIVE BEHAVIOR										Human behavior; decision theory; anomaly detection; activity recognition; Markov chain; cyber-physical system; sensors	RECOGNITION	Today aging of society is becoming increasingly important. Many people want to live at home as long as possible. Very often this leads to accidents at home, which are unnoticed for a long time and therefore cause severe complications. We propose a cyber-physical system consisting of sensors for motion, light, temperature, and so on, which monitors the behavior of elderly people but is non-invasive in the sense that the persons are not observed by a camera and must not wear certain sensors on their body. Unusual behavior or accidents are registered in-time by a change point detection algorithm based on Markov chains, which does not store any data with the exception of the last datum, so that also the integrity of the elderly people is preserved. For verification of this algorithm, a cyber-physical test environment has been programmed which allows to simulate human common and uncommon behavior in house during day and night. This simulation environment is based on behavior trees and decision theory. The verification results suggest that the implemented cyber-physical system for change point and anomaly detection could be successfully used in the real environment of elderly people to give them help in-time when an accident occurs.																	1059-7123	1741-2633														1059712320930420	10.1177/1059712320930420		JUL 2020											
J								Transient search optimization: a new meta-heuristic optimization algorithm	APPLIED INTELLIGENCE										Benchmark functions; Optimization methods; Transient search optimization algorithm	PARTICLE SWARM OPTIMIZATION; NATURE-INSPIRED ALGORITHM; GREY WOLF OPTIMIZER; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; IDENTIFICATION; PARAMETERS	This article offers a new physical-based meta-heuristic optimization algorithm, which is named Transient Search Optimization (TSO) algorithm. This algorithm is inspired by the transient behavior of switched electrical circuits that include storage elements such as inductance and capacitance. The exploration and exploitation of the TSO algorithm are verified by using twenty-three benchmark, where its statistical (average and standard deviation) results are compared with the most recent 15 optimization algorithms. Furthermore, the non-parametric sign test,pvalue test, execution time, and convergence curves proved the superiority of the TSO against other algorithms. Also, the TSO algorithm is applied for the optimal design of three well-known constrained engineering problems (coil spring, welded beam, and pressure vessel). In conclusion, the comparison revealed that the TSO is promising and very competitive algorithm for solving different engineering problems.																	0924-669X	1573-7497				NOV	2020	50	11					3926	3941		10.1007/s10489-020-01727-y		JUL 2020											
J								Multi-objective traveling salesman problem: an ABC approach	APPLIED INTELLIGENCE										Multi-objective traveling salesmen problem; Artificial bee colony algorithm; Swap operation; Pareto optimal solution; Performance metric	BEE COLONY ALGORITHM; GENETIC ALGORITHM; EVOLUTIONARY ALGORITHMS; LOCAL SEARCH; OPTIMIZATION; DECOMPOSITION	Using the concept of swap operation and swap sequence on the sequence of paths of a Traveling Salesman Problem(TSP) Artificial Bee Colony (ABC) algorithm is modified to solve multi-objective TSP. The fitness of a solution is determined using a rule following the dominance property of a multi-objective optimization problem. This fitness is used for the selection process of the onlooker bee phase of the algorithm. A set of rules is used to improve the solutions in each phase of the algorithm. Rules are selected according to their performance using the roulette wheel selection process. At the end of each iteration, the parent solution set and the solution sets after each phase of the ABC algorithm are combined to select a new solution set for the next iteration. The combined solution set is divided into different non-dominated fronts and then a new solution set, having cardinality of parent solution set, is selected from the upper-level non-dominated fronts. When some solutions are required to select from a particular front then crowding distances between the solutions of the front are measured and the isolated solutions are selected for the preservation of diversity. Different standard performance metrics are used to test the performance of the proposed approach. Different sizes standard benchmark test problems from TSPLIB are used for the purpose. Test results show that the proposed approach is efficient enough to solve multi-objective TSP.																	0924-669X	1573-7497				NOV	2020	50	11					3942	3960		10.1007/s10489-020-01713-4		JUL 2020											
J								An adaptive GP-based memetic algorithm for symbolic regression	APPLIED INTELLIGENCE										Adaptive memetic algorithm; Genetic programming; Local search; Crossover; Mutation		Symbolic regression is a process to find a mathematical expression that represents the relationship between a set of explanatory variables and a measured variable. It has become a best-known problem for GP (genetic programming), as GP can use the tree representation to represent solutions as expression trees. Since the success of memetic algorithms (MAs (Memetic algorithms (MAs) can be regarded as a class of methods that combine population-based global search and local search [6,30])) has proved the importance of local search in augmenting the global search ability of GP, GP with local search is investigated to solve symbolic regression tasks in this work. An important design issue of MAs is the balance between the global exploration of GP and the local exploitation, which has a great influence on the performance and efficiency of MAs. This work proposes a GP-based memetic algorithm for symbolic regression, termed as aMeGP (adaptiveMemeticGP), which can balance global exploration and local exploitation adaptively. Compared with GP, two improvements are made in aMeGP to invoke and stop local search adaptively during evolution. The proposed aMeGP is compared with GP-based and nonGP-based symbolic regression methods on both benchmark test functions and real-world applications. The results show that aMeGP is generally better than both GP-based and nonGP-based reference methods with its evolved solutions achieving lower root mean square error (RMSE) for most test cases. Moreover, aMeGP outperforms the reference GP-based methods in the convergence ability, which can converge to lower RMSE values with faster or similar speeds.																	0924-669X	1573-7497				NOV	2020	50	11					3961	3975		10.1007/s10489-020-01745-w		JUL 2020											
J								FLGAI: a unified network embedding framework integrating multi-scale network structures and node attribute information	APPLIED INTELLIGENCE										Network embedding; Network representation learning; Nonnegative matrix factorization; Data mining; Machine learning		Network embedding is an effective method aiming to learn the low-dimensional vector representation of nodes in networks, which has been widely used in various network analytic tasks such as node classification, node clustering, and link prediction. The objective of network embedding is to capture the structural information and inherent characteristics of the network as much as possible in the low-dimensional vector representation. However, the majority of the existing network embedding methods merely exploited the microscopic proximity of the network structure to learn the node representation, which tend to generate sub-optimal network representation. In this paper, we propose a novel nonnegative matrix factorization (NMF) based network representation learning framework called FLGAI, which jointly integrates the local network structure, global network structure, and attribute information to learn the network representation. First, we employ the first-order proximity and second-order proximity jointly to preserve the local network structure. Then, the community structure is introduced to preserve the global network structure. Third, we exploit the node attribute information to capture the node characteristics. To preserve the structural information and the network node attributes simultaneously, we formulate their consensus relationships and optimize them jointly in a unified NMF framework to derive the final network representation. To evaluate the effectiveness of our model, we conduct extensive experiments on six real-world datasets and the empirical results demonstrate the superior performance of the proposed method over the state-of-the-art approaches in both node classification and node clustering tasks.																	0924-669X	1573-7497				NOV	2020	50	11					3976	3989		10.1007/s10489-020-01780-7		JUL 2020											
J								Bio-inspired VANET routing optimization: an overview A taxonomy of notable VANET routing problems, overview, advancement state, and future perspective under the bio-inspired optimization approaches	ARTIFICIAL INTELLIGENCE REVIEW										VANETs; Swarm-inspired optimization; Parameters tuning; VDTN routing; Evolutionary algorithms; Clustering algorithms	AD HOC NETWORKS; WATER DROPS ALGORITHM; CLUSTERING-ALGORITHM; GENETIC ALGORITHM; FIREFLY ALGORITHM; PROTOCOL; CLOUD; SYSTEM; METAHEURISTICS; ARCHITECTURES	This paper demonstrates a recapitulated historic evolution further to a future overview of all vehicular ad-hoc network (VANET) routing problems that concern either directly related routing tasks or targeting a set of diverse routing-related techniques with the aid of the bio-inspired approaches. In this lecture, we serialize, in a synchronous observation, the evolution and tendencies of the VANET routing problem's solving simultaneously with the emergence of different classes of nature-based meta-heuristics, by bringing a proposed taxonomy of different major VANET routing problems seen their nature, studied range and metaheuristic types used for their optimization. Then, we follow with a visionary deduction of the other appearing routing issues of VANETs that can be approached or already began to be solved by nature-inspired optimization algorithms. Noting that each spread routing problem is illustrated with notable related works, describing initially realized conventional protocols to vulgarize different routing modules, then detailing bio-inspired protocols for VANET routing to explain the utility of nature-inspired optimization techniques. The motivation of this work came from the lack of a reference classifying the VANET-related routing problems within the notion of nature-inspired optimization. That's further to giving and up-to-date literature on the context for opening out a visionary opinion on the tendencies of either emerging recent bio-inspired optimization approaches or the different metaheuristic-based combinations on specific VANET routing problems.																	0269-2821	1573-7462															10.1007/s10462-020-09868-9		JUL 2020											
J								A survey of sentiment analysis in the Portuguese language	ARTIFICIAL INTELLIGENCE REVIEW										Sentiment analysis; Opinion mining; Natural language processing; Portuguese language		Sentiment analysis is an area of study that aims to develop computational methods and tools to extract and classify the opinions and emotions expressed by people on social networks, blogs, forums, online shoppings, and others. A lot of research has been developed addressing opinions expressed in the English language. However, studies involving the Portuguese language still need to be advanced to make better use of the specificities of the language. This paper aims to survey the efforts made specifically to address sentiment analysis in the Portuguese language. It categorizes and describes state of the art works involving approaches to each of the tasks of sentiment analysis, as well as supporting language resources such as natural language processing tools, lexicons, corpora, ontologies, and datasets.																	0269-2821	1573-7462															10.1007/s10462-020-09870-1		JUL 2020											
J								Forecasting Bitcoin closing price series using linear regression and neural networks models	PEERJ COMPUTER SCIENCE										Blockchain; Bitcoin; Time Series; Forecasting; Regression; Machine Learning; Neural Networks; Cryptocurrency		In this article we forecast daily closing price series of Bitcoin, Litecoin and Ethereum cryptocurrencies, using data on prices and volumes of prior days. Cryptocurrencies price behaviour is still largely unexplored, presenting new opportunities for researchers and economists to highlight similarities and differences with standard financial prices. We compared our results with various benchmarks: one recent work on Bitcoin prices forecasting that follows different approaches, a well-known paper that uses Intel, National Bank shares and Microsoft daily NASDAQ closing prices spanning a 3-year interval and another, more recent paper which gives quantitative results on stock market index predictions. We followed different approaches in parallel, implementing both statistical techniques and machine learning algorithms: the Simple Linear Regression (SLR) model for uni-variate series forecast using only closing prices, and the Multiple Linear Regression (MLR) model for multivariate series using both price and volume data. We used two artificial neural networks as well: Multilayer Perceptron (MLP) and Long short-term memory (LSTM). While the entire time series resulted to be indistinguishable from a random walk, the partitioning of datasets into shorter sequences, representing different price "regimes", allows to obtain precise forecast as evaluated in terms of Mean Absolute Percentage Error(MAPE) and relative Root Mean Square Error (relativeRMSE). In this case the best results are obtained using more than one previous price, thus confirming the existence of time regimes different from random walks. Our models perform well also in terms of time complexity, and provide overall results better than those obtained in the benchmark studies, improving the state-of-the-art.																	2376-5992					JUL 6	2020									e279	10.7717/peerj-cs.279													
J								Identification and prioritization of DevOps success factors using fuzzy-AHP approach	SOFT COMPUTING										DevOps; Fuzzy AHP; Success factors; Systematic literature review; Empirical investigation	REQUIREMENTS CHANGE MANAGEMENT; HIERARCHY PROCESS AHP; SYSTEMATIC LITERATURE; SOFTWARE-DEVELOPMENT; PROCESS IMPROVEMENT; BARRIERS; REVIEWS	DevOps (development and operations) is a collaborative and multidisciplinary organizational effort to automate continuous delivery of a software project with an aim to improve software quality. The implementation of DevOps practices is not straightforward as there are several complexities associated with it. The aim of this study is to identify and prioritize the factors that positively influence the DevOps practices in software organizations. Using a systematic literature review, 19 factors were identified. The identified factors were further validated with experts via a questionnaire survey study. Finally, Fuzzy Analytical Hierarchy Process (FAHP) was used to prioritize the identified success factors. The results indicate that "DevOps security pipeline," "use system orchestration" and "attempt matrix organization and transparency" factors are the highest ranked success factors for the successful implementation of DevOps practices. The FAHP analysis is novel in this research area as it provides the prioritization based taxonomy of the identified factors which will assist the researchers and practitioners to focus on the critical areas that are significant for the successful adoption of DevOps practices.																	1432-7643	1433-7479															10.1007/s00500-020-05150-w		JUL 2020											
J								A novel lifetime scheme for enhancing the convergence performance of salp swarm algorithm	SOFT COMPUTING										Salp swarm algorithm; Lifetime convergence scheme; Meta-heuristic; Benchmark test functions; Engineering optimization problems	ANT COLONY OPTIMIZATION; MOTH-FLAME OPTIMIZATION; DIFFERENTIAL EVOLUTION; INSPIRED ALGORITHM; WINDING PROCESS; SEARCH; IDENTIFICATION; PARAMETERS	The performance of any meta-heuristic algorithm depends highly on the setting of dependent parameters of the algorithm. Different parameter settings for an algorithm may lead to different outcomes. An optimal parameter setting should support the algorithm to achieve a convincing level of performance or optimality in solving a range of optimization problems. This paper presents a novel enhancement method for the salp swarm algorithm (SSA), referred to as enhanced SSA (ESSA). In this ESSA, the following enhancements are proposed: First, a new position updating process was proposed. Second, a new dominant parameter different from that used in SSA was presented in ESSA. Third, a novel lifetime convergence method for tuning the dominant parameter of ESSA using ESSA itself was presented to enhance the convergence performance of ESSA. These enhancements to SSA were proposed in ESSA to augment its exploration and exploitation capabilities to achieve optimal global solutions, in which the dominant parameter of ESSA is updated iteratively through the evolutionary process of ESSA so that the positions of the search agents of ESSA are updated accordingly. These improvements on SSA through ESSA support it to avoid premature convergence and efficiently find the global optimum solution for many real-world optimization problems. The efficiency of ESSA was verified by testing it on several basic benchmark test functions. A comparative performance analysis between ESSA and other meta-heuristic algorithms was performed. Statistical test methods have evidenced the significance of the results obtained by ESSA. The efficacy of ESSA in solving real-world problems and applications is also demonstrated with five well-known engineering design problems and two real industrial problems. The comparative results show that ESSA imparts better performance and convergence than SSA and other meta-heuristic algorithms.																	1432-7643	1433-7479															10.1007/s00500-020-05130-0		JUL 2020											
J								Improving secured ID-based authentication for cloud computing through novel hybrid fuzzy-based homomorphic proxy re-encryption	SOFT COMPUTING										CCE; Cloud security; Fuzzy set theory; Risk assessment; Access control mechanism	NODES	Cloud computing environment (CCE) can empower an association to re-appropriate computing resources to increase monetary benefits. For both developers and the cloud users (CUs), CCE is transparent. Accordingly, it presents new difficulties when contrasted with precedent types of distributed computing. The precision of assessment results in CCE security risk assessment to take care of the issue of the multifaceted nature of the system and the classified fuzzy cloud method (CFCM) applied to CCE chance ID stage that captures the CCE risk factors through a complete investigation of CCE security area. Current CCE frameworks present a specific restriction on ensuring the client's INFO privacy. We offer a homomorphic proxy re-encryption (HPRE) in this paper that enables various CU to share INFO that they redistributed HPRE encrypted utilizing their PubKs with the plausibility by a close procedure such as INFO remotely. The test of giving secrecy, uprightness, and access control (AC) of INFO facilitated on cloud stages is not provided for by conventional AC models. CFCM models were created through the duration of numerous decades to satisfy the association's necessities, which accepted full authority over the physical structure of the assets. The hypothesis of the INFO proprietor, an INFO controller, and a supervisor is available in the equivalent trusted area. Besides, CCESR features like the essential unit, fuzzy set (FS) hypothesis, and EW strategy utilized to precisely measure the likelihood of CCE security risks (SR) and the subsequent damages of CCESR estimation. Eventually, the computation and authentication model specified, and the lack of CCE SECU threat evaluation examined.																	1432-7643	1433-7479															10.1007/s00500-020-05119-9		JUL 2020											
J								Visualizing image content to explain novel image discovery	DATA MINING AND KNOWLEDGE DISCOVERY										Novelty detection; Explanations; Image analysis	FEATURES; SCALE	The initial analysis of any large data set can be divided into two phases: (1) the identification of common trends or patterns and (2) the identification of anomalies or outliers that deviate from those trends. We focus on the goal of detecting observations with novel content, which can alert us to artifacts in the data set or, potentially, the discovery of previously unknown phenomena. To aid in interpreting and diagnosing the novel aspect of these selected observations, we recommend the use of novelty detection methods that generate explanations. In the context of large image data sets, these explanations should highlight what aspect of a given image is new (color, shape, texture, content) in a human-comprehensible form. We propose DEMUD-VIS, the first method for providing visual explanations of novel image content by employing a convolutional neural network (CNN) to extract image features, a method that uses reconstruction error to detect novel content, and an up-convolutional network to convert CNN feature representations back into image space. We demonstrate this approach on diverse images from ImageNet, freshwater streams, and the surface of Mars. Finally, we evaluate the utility of the visual explanations with a user study.																	1384-5810	1573-756X				NOV	2020	34	6					1777	1804		10.1007/s10618-020-00700-0		JUL 2020											
J								Fine-grained pornographic image recognition with multiple feature fusion transfer learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Pornographic image recognition; Image classification; Multiple feature fusion; Transfer learning	INTERNET PORNOGRAPHY; NEURAL-NETWORKS; IMPACT	Image has become a main medium of Internet information dissemination, makes it easy for an Internet visitor to get pornographic images with just few clicks on websites. It is necessary to build pornographic image recognition systems since uncontrolled spreading of adult content could be harm to the adolescents. Previous solutions for pornographic image recognition are usually based on hand-crafted features like human skin color. Hand-crafted feature based methods are straightforward to understand and use but limited in specific situations. In this paper, we propose a deep learning based approach with multiple feature fusion transfer learning strategy. Firstly, we obtain the training data from an open data set called NSFW with 120,000+ images. Images would be classified into different levels according to its content sensitivity. Then we employ data augment methods, train a deep convolutional neural network to extract image features and conduct the classification job, without the need for hand-crafted rules. A pre-trained model is used to initialize the network and help extract the basic features. Furthermore, we propose a fusion method that makes use of multiple transfer learning models in inference, to improve the accuracy on the test set. The experimental results prove that our method achieves high accuracy on the pornographic image recognition and inspection task.																	1868-8071	1868-808X															10.1007/s13042-020-01157-9		JUL 2020											
J								Banana disease diagnosis using computer vision and machine learning methods	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Adaptive Neuro-Fuzzy Inference System; Case-based reasoning; Fuzzy logic; Soft coring filter		As it is observed that the banana production is plagued by numerous disease conditions and inflicting large loss to the poor farmers. By using modern technology of image processing and soft computing techniques, these may be known at the sooner stage and appropriate precautions may be taken to avoid more injury and thus increase in healthy production. In this research work used identified the banana diseases in sooner stage. Through the pre-processing technique, image is input to urge standardization and soft coring filter is completed to get rid of the noise. Then colour, shape and texture feature are completed for feature extraction, followed by classification techniques. During these classification techniques, two algorithms are used, that's the Adaptive Neuro-Fuzzy Inference System and case-based reasoning. Then fuzzy logic is used for making the decision. The proposed system analysis was done using the Receiver Operating Characteristics (ROC) curve. The analysis shows Adaptive Neuro-Fuzzy Inference System is best than the case-based reasoning algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02273-8		JUL 2020											
J								Multi feature drug compound analysis model for efficient success rate prediction using fuzzy rules	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Success rate prediction; Fuzzy rules; Multi feature model; Patterns		The human society has been identified as more prone for diseases. Various drugs with different compounds are available to treat the diseases. However, the medical practitioner cannot be sure about the application of the drug and what would be exact drug should be feed to the patient. To hook this, a multi feature drug compound analysis model is presented in this paper. The method keeps track of medical records related to various patients and the details of drugs being provided to them. Using these treatment data set, the method applies machine learning techniques to generate and predict the success rate of different drugs. To perform this, the method first split the records based on the disease and for each of them the list of medicines and compounds given has been identified. Based on these data, a set of patterns are generated according to various compounds of drug provided. Further, the method estimates the success influence measure (SIM) for different drug components. Estimated success influence measure is used to generate the fuzzy rules. Based on the rule generated, the method performs success rate prediction for various drug compounds. The method produces noticeable growth in the success rate prediction.																	1868-5137	1868-5145															10.1007/s12652-020-02275-6		JUL 2020											
J								A saboteur and mutant based built-in self-test and counting threshold-based built-in self repairing mechanism for memories	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										BIST; BIRA; BISR; Saboteurs; Mutants; SOC; Counting threshold		In system on chip (SOC) design, memory occupies a large area, if any defects in the memory that will affect the SOC's total yield. To avoid this, spare rows and columns are added to the memory. When the fault cells need to be operated, spares rows/columns will be used instead of faulty cells using Built-in self-repair (BISR) logic. Repairing logic includes either row repair/column repair or a combination of both. In this paper, we propose two methods, one is on the memory built-in self-test (BIST) for checking the memory array circuit. In the second method, an optimized BISR is employed to repair the faulty memory cells based upon the Built-in redundancy analysis. In the BIST method, our proposed method injects the faults into memory by using two fault injection techniques called saboteurs and mutants with optimized test pattern logic. After the injection of the faults in the memory cell, the fault memory is repaired with the proposed counting threshold algorithm for the BISR scheme. Simulation and synthesis results are obtained using Mentor Graphics and Xilinx ISE Design Suite. From our obtained results it can be inferred that various performance measures like power, area, and timing details are reduced in the proposed methods when compared with the existing exhaustive methods. And this proves that the performance of the overall process is found to be enhanced. Thereby yield can also be enhanced with minimum time to market.																	1868-5137	1868-5145															10.1007/s12652-020-02284-5		JUL 2020											
J								3D facility layout problem	JOURNAL OF INTELLIGENT MANUFACTURING										3D configuration space; Facility layout design; Genetic algorithm; A* Search algorithm; Monte Carlo simulation	ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; SINGLE; SEARCH; MODEL	Facility layout aims to arrange a set of facilities in a site. The main objective function is to minimize the total material handling cost under production-derived constraints. This problem has received much attention during the past decades. However, these works have mainly focused on solving a 2D layout problem, dealing with the footprints of pieces of equipment. The obtained results have been then adapted to the real spatial constraints of a workshop. This research work looks to take account of spatial constraints within a 3D space from the very first steps of problem solving. The authors use a approach by combining a genetic algorithm with A*, < GA,A > research. The genetic algorithm generates possible arrangements and A* finds the shortest paths that products must travel in a restricted 3D space. The application allows to converge to a layout minimizing the total material handling cost. This approach is illustrated by its application on an example inspired by a valve assembly workshop in Tunisia and the results are discussed from two points of view. The first one consists in comparing the effect of the choice of the distance measurement technique on the handling cost. For this purpose, the results of the application of < GA,A > are compared with those obtained by combining the genetic algorithm and two of the most commonly used distance measurements in the literature of the discipline, namely the Euclidean distance, < GA,Euclidean >, and the rectilinear distance, < GA,rectilinear >. Our results show that the proposed approach offers better results than those of < GA,rectilinear > whereas they are not as good as those obtained by the < GA,Euclidean > approach. The effectiveness of the < GA,A > approach is then studied from the perspective of the effect of the algorithm used for the generation of candidate arrangements. The final results obtained from the application of < GA,A > are then compared with those of the approach combining particle swarm optimization and A*, < PSO,A >. This comparison shows that the < GA,A > approach obtains better results. Nevertheless, its convergence speed is lower than that of < PSO,A >. The paper ends with some conclusions and perspectives.																	0956-5515	1572-8145															10.1007/s10845-020-01603-z		JUL 2020											
J								Prediction of laser cutting parameters for polymethylmethacrylate sheets using random vector functional link network integrated with equilibrium optimizer	JOURNAL OF INTELLIGENT MANUFACTURING										Laser cutting; Kerf quality indices; Random vector functional link network; Equilibrium optimizer	ARTIFICIAL NEURAL-NETWORK; MULTIOBJECTIVE OPTIMIZATION; QUALITY; PMMA; INCONEL-718; COMPONENT	In this paper, an enhanced random vector functional link network (RVFL) algorithm was employed to predict kerf quality indices during CO(2)laser cutting of polymethylmethacrylate (PMMA) sheets. In the proposed model, the equilibrium optimizer (EO) is used to augment the prediction capability of RVFL via selecting the optimal values of RVFL parameters. The predicting model includes four input variables: gas pressure, sheet thickness, laser power, and cutting speed, and five kerf quality indices: rough zone ratio, widths of up and down heat affected zones, maximum surface roughness, and kerf taper angle. The experiments were designed using Taguchi L18 orthogonal array. The kerf surface contains three main zones: rough, transient, and smooth zones. The results of conventional RVFL as well as modified RVFL-EO algorithms were compared with experimental ones. Seven statistical criteria were used to assess the performance of the proposed algorithms. The results indicate that the RVFL-EO model has the predicting ability to estimate the laser-cutting characteristics of PMMA sheet.																	0956-5515	1572-8145															10.1007/s10845-020-01617-7		JUL 2020											
J								The genetic algorithm census transform: evaluation of census windows of different size and level of sparseness through hardware in-the-loop training	JOURNAL OF REAL-TIME IMAGE PROCESSING										Census transform; Stereo correspondence; Matching cost metric; Genetic algorithm; Real time; FPGA; SoC; VHDL		Stereo correspondence is a well-established research topic and has spawned categories of algorithms combining several processing steps and strategies. One core part to stereo correspondence is to determine matching cost between the two images, or patches from the two images. Over the years several different cost metrics have been proposed, one being the Census Transform (CT). The CT is well proven for its robust matching, especially along object boundaries, with respect to outliers and radiometric differences. The CT also comes at a low computational cost and is suitable for hardware implementation. Two key developments to the CT are non-centric and sparse comparison schemas, to increase matching performance and/or save computational resources. Recent CT algorithms share both traits but are handcrafted, bounded with respect to symmetry, edge lengths and defined for a specific window size. To overcome this, a Genetic Algorithm (GA) was applied to the CT, proposing the Genetic Algorithm Census Transform (GACT), to automatically derive comparison schemas from example data. In this paper, FPGA-based hardware acceleration of GACT, has enabled evaluation of census windows of different size and shape, by significantly reducing processing time associated with training. The experiments show that lateral GACT windows produce better matching accuracy and require less resources when compared to square windows.																	1861-8200	1861-8219															10.1007/s11554-020-00993-w		JUL 2020											
J								Trainable TV-L-1 model as recurrent nets for low-level vision	NEURAL COMPUTING & APPLICATIONS										Total variation; Optical flow; Recurrent network; Image decomposition		TV-L-1 is a classical diffusion- reaction model for low-level vision tasks, which can be solved by a duality-based iterative algorithm. Considering the recent success of end-to-end learned representations, we propose a TV-LSTM network to unfold the duality-based iterations of TV-L-1 into long short-term memory (LSTM) cells. In particular, we formulate the iterations as customized layers of a LSTM neural network. Then, the proposed end-to-end trainable TV-LSTMs can be naturally connected with various task-specific networks, e.g., optical flow, image decomposition and event-based optical flow estimation. Extensive experiments on optical flow estimation and structure + texture decomposition have demonstrated the effectiveness and efficiency of the proposed method.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14603	14611		10.1007/s00521-020-05146-5		JUL 2020											
J								Machine learning for total cloud cover prediction	NEURAL COMPUTING & APPLICATIONS										Ensemble calibration; Logistic regression; Multilayer perceptron; Total cloud cover	POST-PROCESSING METHODS; LOGISTIC-REGRESSION; ENSEMBLE; FORECASTS; OUTPUT; ECMWF; PERFORMANCE; SYSTEM	Accurate and reliable forecasting of total cloud cover (TCC) is vital for many areas such as astronomy, energy demand and production, or agriculture. Most meteorological centres issue ensemble forecasts of TCC; however, these forecasts are often uncalibrated and exhibit worse forecast skill than ensemble forecasts of other weather variables. Hence, some form of post-processing is strongly required to improve predictive performance. As TCC observations are usually reported on a discrete scale taking just nine different values called oktas, statistical calibration of TCC ensemble forecasts can be considered a classification problem with outputs given by the probabilities of the oktas. This is a classical area where machine learning methods are applied. We investigate the performance of post-processing using multilayer perceptron (MLP) neural networks, gradient boosting machines (GBM) and random forest (RF) methods. Based on the European Centre for Medium-Range Weather Forecasts global TCC ensemble forecasts for 2002-2014, we compare these approaches with the proportional odds logistic regression (POLR) and multiclass logistic regression (MLR) models, as well as the raw TCC ensemble forecasts. We further assess whether improvements in forecast skill can be obtained by incorporating ensemble forecasts of precipitation as additional predictor. Compared to the raw ensemble, all calibration methods result in a significant improvement in forecast skill. RF models provide the smallest increase in predictive performance, while MLP, POLR and GBM approaches perform best. The use of precipitation forecast data leads to further improvements in forecast skill, and except for very short lead times the extended MLP model shows the best overall performance.																	0941-0643	1433-3058															10.1007/s00521-020-05139-4		JUL 2020											
J								Memetic algorithms for training feedforward neural networks: an approach based on gravitational search algorithm	NEURAL COMPUTING & APPLICATIONS										Feedforward neural networks; Memetic algorithms; Gravitational search algorithm; Quasi-Newton methods	PARTICLE SWARM OPTIMIZATION; COMPUTATIONAL INTELLIGENCE; EVOLUTIONARY ALGORITHMS; TRAVEL MODE; CLASSIFIERS; DESIGN; CLASSIFICATION; PARAMETERS; GSA; PERFORMANCE	The backpropagation (BP) algorithm is a gradient-based algorithm used for training a feedforward neural network (FNN). Despite the fact that BP is still used today when FNNs are trained, it has some disadvantages, including the following: (i) it fails when non-differentiable functions are addressed, (ii) it can become trapped in local minima, and (iii) it has slow convergence. In order to solve some of these problems, metaheuristic algorithms have been used to train FNN. Although they have good exploration skills, they are not as good as gradient-based algorithms at exploitation tasks. The main contribution of this article lies in its application of novel memetic approaches based on the Gravitational Search Algorithm (GSA) and Chaotic Gravitational Search Algorithm (CGSA) algorithms, called respectively Memetic Gravitational Search Algorithm (MGSA) and Memetic Chaotic Gravitational Search Algorithm (MCGSA), to train FNNs in three classical benchmark problems: the XOR problem, the approximation of a continuous function, and classification tasks. The results show that both approaches constitute suitable alternatives for training FNNs, even improving on the performance of other state-of-the-art metaheuristic algorithms such as ParticleSwarm Optimization (PSO), the Genetic Algorithm (GA), the Adaptive Differential Evolution algorithm with Repaired crossover rate (Rcr-JADE), and the Covariance matrix learning and Bimodal distribution parameter setting Differential Evolution (COBIDE) algorithm. Swarm optimization, the genetic algorithm, the adaptive differential evolution algorithm with repaired crossover rate, and the covariance matrix learning and bimodal distribution parameter setting differential evolution algorithm.																	0941-0643	1433-3058															10.1007/s00521-020-05131-y		JUL 2020											
J								An intelligent optimization method of motion management system based on BP neural network	NEURAL COMPUTING & APPLICATIONS										BP neural network; Motion management; Simulation; Recognition rate; Recognition performance	SPORTS	Intelligent recognition sports management system applied to athletes' sports management system can effectively improve the quality of training and competition. At present, the technical bottlenecks of most sports management systems are in the sports recognition classification module. Based on BP neural network, this research has carried out experiments and analysis from feature extraction, feature selection, principal component analysis to the selection of support vector machine model functions. Moreover, this study designs a human pose recognition algorithm based on a convolutional neural network, optimizes the structure and parameters of the convolutional neural network, and designs a motion management system based on this algorithm. In addition, this study compares the performance of the proposed algorithm with other algorithms. The research shows that the method proposed in this research has certain practical significance and can provide theoretical references for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05093-1		JUL 2020											
J								Stability analysis of T-S fuzzy coupled oscillator systems influenced by stochastic disturbance	NEURAL COMPUTING & APPLICATIONS										T-S fuzzy coupled oscillator systems; Stochastic disturbance; Nonlinear T-S fuzzy control; Stochastic global asymptotic stability	MULTIAGENT SYSTEMS; NEURAL-NETWORKS; SYNCHRONIZATION; DROOP; MODEL	Coupled oscillator systems under the stochastic disturbance in practical have nonlinearity and uncertainty. To overcome the issue, the T-S fuzzy coupled oscillator system (TSFCOS) model is proposed in this paper, which provides an effective solution to coupled oscillator systems that are complex, uncertain and ill-defined. Subsequently, with the proposed nonlinear T-S fuzzy control, we give insight to the stability of the TSFCOSs with stochastic disturbance. Combined Lyapunov method and graph-theoretical technique, a systematic method to construct a global Lyapunov function for TSFCOSs is first provided, and then substantial stability criteria with the condition of the system topology property are obtained. Considering the application of our theoretical results in practical engineering, microgrids (the power networks) can be regarded as a kind of TSFCOSs with stochastic disturbance. With a numerical test of a six-generator seven-bus microgrid, the progressiveness and feasibility of our theoretical results are shown.																	0941-0643	1433-3058															10.1007/s00521-020-05116-x		JUL 2020											
J								Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware	NEURAL COMPUTING & APPLICATIONS										Truly sparse neural networks; Sparse evolutionary training (SET); Microarray gene expression; Adaptive sparse connectivity	FEATURE-SELECTION; CLASSIFICATION; INFERENCE; DNA	Artificial neural networks (ANNs) have emerged as hot topics in the research community. Despite the success of ANNs, it is challenging to train and deploy modern ANNs on commodity hardware due to the ever-increasing model size and the unprecedented growth in the data volumes. Particularly for microarray data, the very high dimensionality and the small number of samples make it difficult for machine learning techniques to handle. Furthermore, specialized hardware such as graphics processing unit (GPU) is expensive. Sparse neural networks are the leading approaches to address these challenges. However, off-the-shelf sparsity-inducing techniques either operate from a pretrained model or enforce the sparse structure via binary masks. The training efficiency of sparse neural networks cannot be obtained practically. In this paper, we introduce a technique allowing us to train truly sparse neural networks with fixed parameter count throughout training. Our experimental results demonstrate that our method can be applied directly to handle high-dimensional data, while achieving higher accuracy than the traditional two-phase approaches. Moreover, we have been able to create truly sparse multilayer perceptron models with over one million neurons and to train them on a typical laptop without GPU (), this being way beyond what is possible with any state-of-the-art technique.																	0941-0643	1433-3058															10.1007/s00521-020-05136-7		JUL 2020											
J								An energy-efficient optimization of the hard turning using rotary tool	NEURAL COMPUTING & APPLICATIONS										Rotary turning; Energy; Roughness; Machining rate	CUTTING FORCES; PREDICTION; PERFORMANCE; STEEL; MODEL	The turning operation using a self-propelled rotary tool (SPRT) is efficient manufacturing for hard machining. However, optimization-based energy saving of the rotary turning has not presented because of expensive implementation. This study addresses a parameter optimization to enhance the machining rate (MR) and decrease the energy consumption (ET) as well as the machined roughness (R) for a hard turning using SPRT. The process inputs are the inclined angle (alpha), depth of cut (a), feed rate (f), and cutting speed (V). The hard turning runs were performed using the experimental plan generated by the Taguchi approach. The adaptive neuro-fuzzy inference system (ANFIS) was used to construct the correlations between the process inputs and responses. The analytic hierarchy process technique was adopted to explore the weight values of the outputs, and the optimum solution was obtained utilizing the adaptive simulated annealing. Moreover, an integrative approach using the response surface method and utilizing the desirability approach was employed to select the optimal outcomes and compare with the proposed technique. The findings revealed that the proposed ANFIS models minimize the predictive error in comparison with the traditional one. The accurate weights may help to select reliable optimal results. The optimal values of the alpha,a,f, andVare 18 degrees, 0.15 mm, 0.40 mm/rev, and 200 mm/min, respectively. Moreover, ET and roughness are decreased by 50.29% and 19.77%, while the MR is enhanced by 33.16%, respectively, as compared to the general process.																	0941-0643	1433-3058															10.1007/s00521-020-05149-2		JUL 2020											
J								Super ensemble learning for daily streamflow forecasting: large-scale demonstration and comparison with multiple machine learning algorithms	NEURAL COMPUTING & APPLICATIONS										Combining forecasts; Ensemble learning; Hydrology; Stacking	NEURAL-NETWORKS; DATA SET; COMBINATION; MODELS; PRECIPITATION; SELECTION; EXPLAIN	Daily streamflow forecasting through data-driven approaches is traditionally performed using a single machine learning algorithm. Existing applications are mostly restricted to examination of few case studies, not allowing accurate assessment of the predictive performance of the algorithms involved. Here, we propose super learning (a type of ensemble learning) by combining 10 machine learning algorithms. We apply the proposed algorithm in one-step-ahead forecasting mode. For the application, we exploit a big dataset consisting of 10-year long time series of daily streamflow, precipitation and temperature from 511 basins. The super ensemble learner improves over the performance of the linear regression algorithm by 20.06%, outperforming the "hard to beat in practice" equal weight combiner. The latter improves over the performance of the linear regression algorithm by 19.21%. The best performing individual machine learning algorithm is neural networks, which improves over the performance of the linear regression algorithm by 16.73%, followed by extremely randomized trees (16.40%), XGBoost (15.92%), loess (15.36%), random forests (12.75%), polyMARS (12.36%), MARS (4.74%), lasso (0.11%) and support vector regression (- 0.45%). Furthermore, the super ensemble learner outperforms exponential smoothing and autoregressive integrated moving average (ARIMA). These latter two models improve over the performance of the linear regression algorithm by 13.89% and 8.77%, respectively. Based on the obtained large-scale results, we propose super ensemble learning for daily streamflow forecasting.																	0941-0643	1433-3058															10.1007/s00521-020-05172-3		JUL 2020											
J								Improved Stabilization Results for Markovian Switching CVNNs with Partly Unknown Transition Rates	NEURAL PROCESSING LETTERS										Complex-valued neural networks; Markovian switching; Partly unknown transition rates; Stochastic stability; Stabilization	RECURRENT NEURAL-NETWORKS; GLOBAL EXPONENTIAL STABILITY; FINITE-TIME SYNCHRONIZATION; JUMP LINEAR-SYSTEMS; DELAYS	In this paper, stochastic stability and stabilization problems are investigated for the Markovian switching complex-valued neural networks with mixed delays, where the transition rates (TRs) of the Markov chain are partly unknown, which might reflect more the realistic dynamical behaviors of the neural networks. On the basis of the Lyapunov stability theory and the stochastic analysis method as well as the properties of the TR matrix, several mode-dependent criteria are derived to guarantee the considered complex-valued network to be globally asymptotically stable in mean-square sense. Then, by proposing an appropriate mode-dependent controller, stabilization conditions in terms of matrix inequalities are derived to guarantee the closed-loop system to be stochastically mean-square stable. Finally, two simulation examples are presented to illustrate the effectiveness of the proposed theoretical results.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1189	1205		10.1007/s11063-020-10299-4		JUL 2020											
J								Robust decoding strategy of MIMO-STBC using one source Kurtosis based GPSO algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MIMO-STBC; BSS; Re-Im decomposition model; Kurtosis and GPSO	TIME BLOCK-CODES; CHANNEL ESTIMATION; PERFORMANCE ANALYSIS	Most research papers have referred that the performance of bit error rate (BER) for a Multiple-Input Multipl-Output Space-Time Block Code (MIMO-STBC) decoder fundamentally relies on the number of pilot symbols. This paper proposes a new strategy for decoding a MIMO-STBC using blind source separation (BSS). First, the STBC decoder is modeled as a noisy linear mixing system, which is decomposed using the technique of real-imaginary (Re-Im). Then Kurtosis based BSS algorithm is applied to the decomposition model. The method of one source extraction is also used to reduce decoding time efficiently. Finally, a global particle swarm optimization method (GPSO) is combined with the one source extraction Kurtosis based BSS to obtain a high speed/low complexity MIMO-STBC decoder. The classical PSO algorithm in this paper is modified by innovating a new updating formula, which is the combination of the swarm behavior and BSS algorithm. Although the new decoder is more complicated as compared with the conventional one, it provides superior BER performance using a fewer number of pilot symbols. Computer simulation for QPSK STBC in a quasi-static flat fading MIMO channel was implemented using MATLAB2018. The new decoder algorithm was tested using only two receivers, worst case. The important point denoted through simulation is the BER performance of the proposed decoder was significantly got better when the length of frames gets longer. The proposed strategy was also used in decoding the 8 x 8 MIMO-STBC system in an extreme noise environment. It was found the BER performance improved nine times as compared with the traditional decoder. Finally, the modified GPSO algorithm made the proposed decoder needs a very small number of iterations, even though the search space dimension is very high.																	1868-5137	1868-5145															10.1007/s12652-020-02288-1		JUL 2020											
J								Method development for in situ study of marine vanadium peroxidase based on SERS and chemometrics	JOURNAL OF CHEMOMETRICS										SERS; OPLS; T-OPLS; design of experiments; method development	ENHANCED RAMAN-SPECTROSCOPY; X-RAY ANALYSIS; DEPENDENT BROMOPEROXIDASE; SECONDARY STRUCTURE; ALGAE; PROTEINS; PURIFICATION; HALOPEROXIDASE; ACTIVATION; IMAGES	Vanadium peroxidases from marine algae are responsible for the production of ozone-depleting compounds, volatile halogenated organic compounds (VHOC). Due to the impact the release of these compounds has on the global atmospheric and biogeochemical processes, there is an interest within marine sciences in developing analytical methods for studying the various aspects of the VHOC production, particularly in situ. This study aimed to provide new methods towards the development of in situ methods within marine sciences. We demonstrate the use of design of experiments together with orthogonal partial least squares (OPLS) and transposed orthogonal partial least squares (T-OPLS) to address the qualitative spectral analysis of an enzyme-buffer system. The measurements were performed with surface-enhanced Raman spectroscopy (SERS) on vanadium bromoperoxisase from the red algaeCorallina officinalis. The chemometric tools used aimed to provide greater insights into how factors such as time, amount of gold nanoparticles and enzyme concentration influence the spectral responses and whether there was any synergy between those factors. The results acquired in this report aim to support future method development of chemometrics within in situ applications in marine sciences.																	0886-9383	1099-128X				SEP	2020	34	9							e3280	10.1002/cem.3280		JUL 2020											
J								Querying Rich Ontologies by Exploiting the Structure of Data	KUNSTLICHE INTELLIGENZ										Knowledge representation; Description logics; Ontologies; Query answering	SYSTEM	Ontology-based data access (OBDA) has emerged as a paradigm for accessing heterogeneous and incomplete data sources. A fundamental reasoning service in OBDA, the ontology mediated query (OMQ) answering has received much attention from the research community. However, there exists a disparity in research carried for OMQ algorithms for lightweight DLs which have found their way into practical implementations, and algorithms for expressive DLs for which the work has had mainly theoretical oriented goals. In the dissertation, a technique that leverages the structural properties of data to help alleviate the problems that typically arise when answering the queries in expressive settings is developed. In this paper, a brief summary of the technique along with the different algorithms developed for OMQ for expressive DLs is given.																	0933-1875	1610-1987				SEP	2020	34	3			SI		395	398		10.1007/s13218-020-00672-9		JUL 2020											
J								Data-driven dimension reduction in functional principal component analysis identifying the change-point in functional data	STATISTICAL ANALYSIS AND DATA MINING										change-point detection; functional data; functional principal component analysis	TIME-SERIES	Functional principal component analysis (FPCA) is the most commonly used technique to analyze infinite-dimensional functional data in finite lower-dimensional space for the ease of computational intensity. However, the power of a test detecting the existence of a change-point falls with the inclusion of more principal dimensions explaining a larger proportion of variability. We propose a new methodology for dynamically selecting the dimensions in FPCA that are used further for the testing of the existence of any change-point in the given data. This data-driven and efficient approach leads to a more powerful test than those available in the literature. We illustrate this method on the monthly global average anomaly of temperatures.																	1932-1864	1932-1872															10.1002/sam.11471		JUL 2020											
J								Bank CRM Optimization Using Predictive Classification Based on the Support Vector Machine Method	APPLIED ARTIFICIAL INTELLIGENCE											SEGMENTATION; MODEL; RFM	This paper proposes a predictive approach to segmenting credit card users, based on their value to the bank. The approach combines the Recency, Frequency and Monetary (RFM) method, clustering using thek-means method, and predictive classification by the Support Vector Machine (SVM) method. Clustering by non-encoded RFM attributes overcomes the subjectivity in selecting the number of segments and losing information (small differences in the values of these attributes) which are problems of classic RFM segmentation. In order to overcome the problem of class imbalance in predictive classification (which occurs due to the small number of valuable customers), the Support Vector Machine (SVM) method was applied as a pre-processor of data due to its extraordinary generalization capabilities. The end result of predictive classification should be a set of rules that describes the identified customer segments in order to tailor the offer to each segment individually. The extraction of rules from the SVM output was achieved using the Decision Tree (DT) classification method. Using a proposed approach that addresses the issue of the small class, marketing managers can more effectively target the most valuable customers, thereby increasing revenue, but also reducing unnecessary costs due to wrongly targeted valuable clients.																	0883-9514	1087-6545				OCT 14	2020	34	12					941	955		10.1080/08839514.2020.1790248		JUL 2020											
J								Comprehensive Taxonomies of Nature- and Bio-inspired Optimization: Inspiration Versus Algorithmic Behavior, Critical Analysis Recommendations	COGNITIVE COMPUTATION										Nature-inspired algorithms; Bio-inspired optimization; Taxonomy; Classification	PARTICLE SWARM OPTIMIZATION; META-HEURISTIC ALGORITHM; NUMERICAL FUNCTION OPTIMIZATION; POPULATION-BASED ALGORITHM; OF-THE-ART; GLOBAL OPTIMIZATION; SEARCH ALGORITHM; ENGINEERING OPTIMIZATION; METAHEURISTIC ALGORITHM; DIFFERENTIAL EVOLUTION	In recent algorithmic family simulates different biological processes observed in Nature in order to efficiently address complex optimization problems. In the last years the number of bio-inspired optimization approaches in literature has grown considerably, reaching unprecedented levels that dark the future prospects of this field of research. This paper addresses this problem by proposing two comprehensive, principle-based taxonomies that allow researchers to organize existing and future algorithmic developments into well-defined categories, considering two different criteria: the source of inspiration and the behavior of each algorithm. Using these taxonomies we review more than three hundred publications dealing with nature-inspired and bio-inspired algorithms, and proposals falling within each of these categories are examined, leading to a critical summary of design trends and similarities between them, and the identification of the most similar classical algorithm for each reviewed paper. From our analysis we conclude that a poor relationship is often found between the natural inspiration of an algorithm and its behavior. Furthermore, similarities in terms of behavior between different algorithms are greater than what is claimed in their public disclosure: specifically, we show that more than one-third of the reviewed bio-inspired solvers are versions of classical algorithms. Grounded on the conclusions of our critical analysis, we give several recommendations and points of improvement for better methodological practices in this active and growing research field.																	1866-9956	1866-9964				SEP	2020	12	5					897	939		10.1007/s12559-020-09730-8		JUL 2020											
J								A novel meta-heuristic search algorithm for solving optimization problems: capuchin search algorithm	NEURAL COMPUTING & APPLICATIONS										Optimization; Meta-heuristics; Bio-inspired algorithms; Capuchin search algorithm; Optimization techniques	PARTICLE SWARM OPTIMIZATION; NATURE-INSPIRED ALGORITHM; ANT COLONY OPTIMIZATION; ENGINEERING OPTIMIZATION; DIFFERENTIAL EVOLUTION; PROGRAMMING APPROACH; MIXED-INTEGER; KRILL HERD; SEGMENTATION	Meta-heuristic search algorithms were successfully used to solve a variety of problems in engineering, science, business, and finance. Meta-heuristic algorithms share common features since they are population-based approaches that use a set of tuning parameters to evolve new solutions based on the natural behavior of creatures. In this paper, we present a novel nature-inspired search optimization algorithm called the capuchin search algorithm (CapSA) for solving constrained and global optimization problems. The key inspiration of CapSA is the dynamic behavior of capuchin monkeys. The basic optimization characteristics of this new algorithm are designed by modeling the social actions of capuchins during wandering and foraging over trees and riverbanks in forests while searching for food sources. Some of the common behaviors of capuchins during foraging that are implemented in this algorithm are leaping, swinging, and climbing. Jumping is an effective mechanism used by capuchins to jump from tree to tree. The other foraging mechanisms exercised by capuchins, known as swinging and climbing, allow the capuchins to move small distances over trees, tree branches, and the extremities of the tree branches. These locomotion mechanisms eventually lead to feasible solutions of global optimization problems. The proposed algorithm is benchmarked on 23 well-known benchmark functions, as well as solving several challenging and computationally costly engineering problems. A broad comparative study is conducted to demonstrate the efficacy of CapSA over several prominent meta-heuristic algorithms in terms of optimization precision and statistical test analysis. Overall results show that CapSA renders more precise solutions with a high convergence rate compared to competitive meta-heuristic methods.																	0941-0643	1433-3058															10.1007/s00521-020-05145-6		JUL 2020											
J								How frontal is a face? Quantitative estimation of face pose based on CNN and geometric projection	NEURAL COMPUTING & APPLICATIONS										Face pose estimation; Convolutional neural networks; 3D face landmarks; Geometric projection; Face pose score	NETWORK	Face pose estimation has been widely used into various applications of human-computer interaction; however, it is yet a challenging work due to illumination, background, face orientations, appearance visibility, etc. In this paper, a novel coarse-to-fine method of face pose quantitative estimation based on convolutional neural networks (CNN) and geometric projection is proposed. In coarse classification, CNN is applied to classify the input image into a specific category and obtain a relevant weight. After that, geometric projections of 3D face landmarks projected into three planes,x-y,x-zandy-z, of 3D coordinate systems are used to perform the fine estimation of face pose, which can get the offset angles of the face in the three directions of roll, yaw, and pitch. Finally, the final score of face pose is obtained by combining the results of two stages. Experiments on standard datasets show that the proposed method can get better results than some competitive algorithms, which proves the effectiveness of the proposed method.																	0941-0643	1433-3058															10.1007/s00521-020-05167-0		JUL 2020											
J								NanoMetrix: An app for chemometric analysis from near infrared spectra	JOURNAL OF CHEMOMETRICS										NIR spectroscopy; nitrogen determination; plant tissue; smartphone	REFLECTANCE SPECTROSCOPY; QUANTIFICATION; ADULTERATION; PERFORMANCE; REGRESSION; NITROGEN	Near-infrared spectroscopy (NIR) associated with chemometric methods has been widely used in the determination of various compounds, as it is a fast, nondestructive, and analytical method with minimal sample preparation and universal application. In addition, the trend of miniaturization has allowed the development of new portable equipment, increasing the possibilities of point-of-use analysis at low cost. In this context, the objective of this study was to develop a portable analytical methodology using a low-cost DLP NIRscan Nano spectrophotometer (Texas Instruments (R)) connected to a smartphone for data storage. For this, an app called NanoMetrix was developed, created natively for Android from Android Studio IDE, according to a software design standard known as Model-View-ViewModel (MVVM). To demonstrate the applicability of the method, 36 samples of plant tissue were analyzed to determine total nitrogen content. The calibration models were compared to those obtained with the SOLO+MIA software (Eigenvector Research, Inc.), 8.6.1. The results presented root mean square error of calibration (RMSEC) 1.90 and 1.82 g kg(-1), root mean square error of prediction (RMSEP) 2.00 and 1.97 g kg(-1), andR(pred)(2)0.973 and 0.975, for the NanoMetrix application and the SOLO+MIA software, respectively. Therefore, the results indicate that the NanoMetrix integrated with the portable spectrophotometer, besides the acquisition of spectra, allows the processing of data in the field and reducing the time for the analysis and obtaining the results. Also, the chemometric associated with the NIR spectroscopy is a viable alternative to replace, or complement, the methods used for nitrogen determination.																	0886-9383	1099-128X														e3281	10.1002/cem.3281		JUL 2020											
J								A joint guidance-enhanced perceptual encoder and atrous separable pyramid -convolutions for image inpainting	NEUROCOMPUTING										Image inpainting; Perceptual encoder-decoder; Generative adversarial networks		y Satisfactory image inpainting requires visually-exquisite details and semantically-plausible structures, where encoder-decoder networks have shown their potentials but bear undesired local and global inconsistencies, such as blurry textures. To address this issue, we incorporate a perception operation in the encoder, which extracts features from known areas of the input image, to improve textured details in missing areas. We also propose an iterative guidance loss for the perception operation to guide perceptual encoding features approaching to ground-truth encoding features. The guidance-enhanced perceptual encoding features are transferred to the decoder through skip connections, mutually reinforcing the entire encoder-decoder performance. Since the inpainting task involves different levels of feature representations, we further apply atrous separable parallel-convolutions (i.e., atrous separable pyramid-convolutions or ASPC) with different receptive fields in the last guidance-enhanced perceptual encoding feature, which is used to learn high-level semantic features with multi-scale information. Experiments on public databases show that the proposed method achieves promising results in terms of visual details and semantic structures. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 5	2020	396						1	12		10.1016/j.neucom.2020.01.068													
J								Face sketch-to-photo transformation with multi-scale self-attention GAN	NEUROCOMPUTING										Image transformation; Sketch-to-photo; Divide and conquer; Multi-scale; Attention mechanism; Generative adversarial network		In this study, we investigate the sketch-to-photo problem, which currently poses a significant challenge in the field of computer vision. A large number of GAN-based encoder-decoder methods have been proposed for image transformation, inspired by the pix2pix model; however, these methods do not produce satisfactory results for photo generation, due to the fact that (1) they miss detailed information of input images because of a single-scale convolution operator in the shallow encoder layers, and (2) they fail to learn long-range dependencies in the deep encoder layers. To better handle these challenges, we present an approach that follows a "divide and conquer" strategy. Our method combines the advantages of a multi-scale convolutional neural network and an attention mechanism and applies these two modules to different encoder layers. Additionally, by optimizing a well-designed loss function, the complex correlations between the sketch and the photo can be calculated. Experimental results show that our method is able to generate high-quality photos from sketch images, and qualitative and quantitative analysis demonstrates its effectiveness and superiority over state-of-the-art models. This work paves a path to replace the traditional encoder structure with the "divide and conquer" strategy to handle image transformation tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						13	23		10.1016/j.neucom.2020.02.024													
J								Concurrent optimization of multiple base learners in neural network ensembles: An adaptive niching differential evolution approach	NEUROCOMPUTING										Niching differential evolution; Multimodal optimization; Neural network ensemble; Population size adaptation	MULTIMODAL OPTIMIZATION; GENETIC ALGORITHM; MUTATION	Neural network ensemble (NNE) exhibits improved performance when compared with a single neural network (NN) in most cases. Traditionally, each base network in an NNE is trained individually, which may result in network redundancy and expensive training overhead. This paper proposes a new adaptive niching evolutionary algorithm, which possesses promising performance in finding multiple optima in terms of good accuracy and diversity. By means of this algorithm, all NNs in an NNE can be trained simultaneously. In particular, the proposed algorithm is named adaptive niching differential evolution (ANDE), which is characterized by a heuristic clustering method to enable iteratively cluster subpopulations that track and locate multiple optima, a parameter adaptation strategy to adaptively adjust parameters according to the subpopulation states, and an auxiliary movement scheme to promote the equilibrium between exploration and exploitation. Experimental results validate the efficiency and effectiveness of the proposed ANDE on the benchmark test suite of multimodal optimization. Furthermore, ANDE is extended to concurrently train multiple base NNs for ensemble and the experiments show a promising performance of ANDE-NNE. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						24	38		10.1016/j.neucom.2020.02.020													
J								Recent advances in deep learning for object detection	NEUROCOMPUTING										Object detection; Deep learning; Deep convolutional neural networks		Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications & benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						39	64		10.1016/j.neucom.2020.01.085													
J								Hybrid method for automatic construction of 3D-ASM image intensity models for left ventricle	NEUROCOMPUTING										Statistic shape modeling; Cardiac MRI; Image intensity modeling; CNN	SHAPE MODELS; SEGMENTATION	Active Shape Models (ASMS) play an important role in model based medical image analysis. They utilize point distribution models (PDMs) in which a priori information is encoded into a template so that the objects to be detected can be represented with a fixed topology. One key element in 3D-ASM is the image intensity model (IIM), which is investigated in this work. We propose a hybrid approach to automatically construct 3D-ASM Intensity Models for the left ventricle. To train the IIM, CNN is adopted to obtain the initial shape for 3D-ASM, distance maps for endo and epicardial contours from ground truth are derived, and using PDM, training shapes can be obtained. The training shapes and cardiac images are then used to train an IIM. 1200 cardiac MRI cases from Hubei Cancer Hospital were used in this study. By comparing point-to-surface errors against a proper gold standard, it demonstrates that large-scale cardiac MRIs can be segmented by 3D models trained under this scheme with fair accuracy. Clinical parameters are calculated using the Bland-Altman analysis, and thus we yield biases of 4.8 ml, 2.19 ml, 2.59 ml, 0.96%, 0.69 g and -2.67 g for LVEDV (LV End-diastolic Volume), LVESV (LV End-systolic Volume), LVSV (LV Stroke volume), LVEF (LV Ejection Fraction), LVM-DP (LV mass in diastolic phase) and LVM-SP (LV mass in systolic phase), respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						65	75		10.1016/j.neucom.2019.10.102													
J								DRM-SLAM: Towards dense reconstruction of monocular SLAM with scene depth fusion	NEUROCOMPUTING										Depth fusion; SLAM; Monocular; Depth estimation	STRUCTURE-FROM-MOTION; IMAGE; FEATURES	Monocular visual SLAM methods can accurately track the camera pose and infer the scene structure by building sparse correspondence between two/multiple views of the scene. However, the reconstructed 3D maps of these methods are extremely sparse. On the other hand, deep learning is widely used to predict dense depth maps from single-view color images, but the results are subject to blurry depth boundaries, which severely deform the structure of 3D scene. Therefore, this paper proposes a dense reconstruction method under the monocular SLAM framework (DRM-SLAM), in which a novel scene depth fusion scheme is designed to fully utilize both the sparse depth samples from monocular SLAM and predicted dense depth maps from convolutional neural network (CNN). In the scheme, a CNN architecture is carefully designed for robust depth estimation. Besides, our approach also accounts for the problem of scale ambiguity existing in the monocular SLAM. Extensive experiments on benchmark datasets and our captured dataset demonstrate the accuracy and robustness of the proposed DRM-SLAM. The evaluation of runtime and adaptability under challenging environments also verify the practicability of our method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						76	91		10.1016/j.neucom.2020.02.044													
J								Dual-CNN: A Convolutional language decoder for paragraph image captioning	NEUROCOMPUTING										Deep learning; Language and vision; Convolutional neural networks; Image captioning	ATTENTION	The task of paragraph image captioning aims to generate a coherent paragraph describing a given image. However, due to their limited ability to capture long-term dependency, recurrent neural network or long-short term memory based decoders could hardly generate satisfactory textual descriptions with a long paragraph. In addition, the training inefficiency in the sequential decoders is significantly observed. Motivated by the advantage of convolutional neural network (i.e., CNN), in this paper, we propose a Dual-CNN decoder with long-term memory ability and parallel computation, which can produce a semantically coherent paragraph for an image. Our Dual-CNN model is evaluated on the Stanford image-paragraph dataset. Extensive experiments demonstrate that our Dual-CNN achieves comparable results compared with state-of-the-art models. Furthermore, the diversity and coherence of generated paragraphs are analyzed to show the superiority of our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						92	101		10.1016/j.neucom.2020.02.041													
J								DTC: Transfer learning for commonsense machine comprehension	NEUROCOMPUTING										Commonsense machine comprehension; Recognition textual entailment; Transfer learning; Deep learning		Commonsense Machine Comprehension (CMC) is a popular natural language understanding task. CMC enables computers to learn about causal and temporal reasoning by exploiting implicit commonsense knowledge and can be applied to Question Answering, Search Engine and Dialogue System. Previous methods for CMC limit the vision on CMC task, neglecting that Recognizing Textual Entailment(RTE) task has much similarities with CMC task. In this paper, we propose a transfer learning model, which can take advantage of commonsense knowledge in RTE task by mapping CMC examples and RTE examples to a shared feature space and comprehending in this feature space. Specifically, we first establish a transfer learning framework which has three components: (1) source and target mappings, (2) domain regularization, and (3) CMC score function. Then we make selection for each component in our transfer learning framework and propose the Domain Transfer Comprehension(DTC) model. Experiments on Story Cloze Test show that our model outperforms most previous approaches and provides competitive results with state-of-art methods. We also show each components of our model have positive effect on performance. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 5	2020	396						102	112		10.1016/j.neucom.2019.07.110													
J								Semantic deep cross-modal hashing	NEUROCOMPUTING										Cross-modal retrieval; Hashing; Semantic label information		Because an increasing number of modality data emerge on the Internet, cross-modal retrieval has become a nontrivial research topic. Furthermore, given the massive amount of cross-modal data and the high dimension of their features, hashing has been explored because it can reduce storage cost and accelerate retrieval speed. In this paper, we put forward a deep cross-modal hashing approach, dubbed semantic deep cross-modal hashing (SDCH). It can make effective use of semantic label information and generate more discriminative hash codes. Specifically, it utilizes the semantic label branches to improve the feature learning part, which can preserve semantic information of the learned features and keep the invariability of cross-modal data. Furthermore, it employs the hash codes learning branches to maintain the consistency of hash codes between different modalities in the Hamming space. Besides, it adopts inter-modal pairwise loss, cross-entropy loss and quantization loss to ensure that the ranking relevance of all similar instance pairs is higher than that of dissimilar ones. Compared to the most advanced method, attention-aware deep adversarial hashing (AADAH), SDCH averagely improves 6.14%, 4.84%, and 3.75% on three widely used datasets, IAPR TC-12, MIR-Flickr 25k, and NUS-WIDE, respectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						113	122		10.1016/j.neucom.2020.02.043													
J								Consensus of fractional-order delayed multi-agent systems in Riemann-Liouville sense	NEUROCOMPUTING										Consensus; Fractional-order delayed multi-agent system; Riemann-Liouville derivative; Lyapunov direct method	LEADER-FOLLOWING CONSENSUS; ASYMPTOTICAL STABILITY; NETWORKS	Fractional-order delayed multi-agent systems (FDMASs) in Riemann-Liouville sense are considered, where the corresponding topology is a weighted digraph. A new method is adopted to analyze consensus and some algebraic criteria are provided by applying classical Lyapunov direct method and algebraic graph theory. The main merit of our proposed approach is that the first-order derivative of the corresponding Lyapunov function can be taken. Two illustrative examples are provided to further show the validity of our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						123	129		10.1016/j.neucom.2020.02.040													
J								A classifier task based on Neural Turing Machine and particle swarm algorithm	NEUROCOMPUTING										Neural networks; Deep learning; Neural Turing Machine; Particle swarm optimization (PSO); Classification	WORKING-MEMORY; NETWORKS; ARCHITECTURES	Deep learning in artificial intelligence looks for a general-purpose computational machine to execute complex algorithms similar to humans' brain. Neural Turing Machine (NTM) as a tool to realize deep learning approach brings together Turing machine that is a general-purpose machine equipped to a long-term memory, and a neural network as a controller. NTM applies simple controllers to execute several simple/complex tasks such as copy, sort, N-gram, etc.; however, complex tasks such as classifications are neglected, and there is no control over improving the weights of NTM, either. This paper presents a framework called PSO-NTM that improves the accuracy of NTM using the LSTM deep neural network as the controller, and implements a complex classification task along with available tasks. Particle Swarm Optimization (PSO) algorithm is also applied in order to control the weights. The classifier task is compared with the basic SVM, KNN, Naive Bayesian, and Decision Tree classification methods on MNIST, ORL, letter recognition, and ionosphere datasets. The accuracy of the proposed classification tasks is 99.73%, 97.9%, 99.02%, and 97.1%, respectively. That means the NTM classification task improved Naive Bayesian 43.57%, Decision Tree 15.6%, and KNN 19.22% on average. In addition, the presented framework improves the available NTM tasks as well. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						133	152		10.1016/j.neucom.2018.07.097													
J								Minimal learning parameters-based adaptive neural control for vehicle active suspensions with input saturation	NEUROCOMPUTING										Active suspension systems; Adaptive neural network control; Input saturation; The minimal learning parameters	SLIDING-MODE CONTROL; BARRIER LYAPUNOV FUNCTIONS; FUZZY TRACKING CONTROL; NONLINEAR-SYSTEMS; BACKSTEPPING CONTROL; DISTURBANCE OBSERVER; VIBRATION ISOLATION; NETWORK; DESIGN; DELAY	In this paper, a neural network (NN) control method is developed for nonlinear quarter vehicle active suspension systems (VASSs) which have the features of parameter uncertainties, input saturation and road disturbance, whose aim is to ensure safe driving and improve the ride comfort. The NNs are employed to approximate unkown nonlinear functions that the forming reason is uncertainties by caused varied sprung mass. When the output of actuator goes beyond its maximums, an NN control scheme combined with anti-saturation is proposed to handle this problem. Furthermore, an NN controller with the minimal learning parameters is constructed to ensure that the number of adaptive learning parameters and the burden computation are largely reduced for VASSs. Meanwhile, the control objectives of VASSs are proved based on the stability analysis. Finally, the effectiveness of designed scheme is demonstrated by an example. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						153	161		10.1016/j.neucom.2018.07.096													
J								A new robust output tracking control for discrete-time switched constrained-input systems with uncertainty via a critic-only iteration learning method	NEUROCOMPUTING										Optimal tracking control; Switched system; Iteration learning; Constrained input; Adaptive dynamic programming	NONLINEAR-SYSTEMS; ADAPTIVE-CONTROL; FAULT-DETECTION; LINEAR-SYSTEMS; CONTROL SCHEME; STATE	In this paper, the control objective is driving the output of a discrete-time switched constrained system with uncertainty to track a desired output of reference by an optimal manner. A new augmented switched system with discounted cost function is constructed based on the switched and reference dynamics, which converts the complex tracking problem to a stabilizing robust control optimization problem. Combining the two stage optimization and iteration learning technique, the overall optimal hybrid policy is first achieved for the constrained switched tracking control. Instead of the general critic-actor structure, only critic neural network (NN) is applied in the algorithm to simply the architecture and manner of implementation. As the main computational burden or load in iteration learning process comes from the information transmissions of tuning NNs, the designed critic-only structure can reduce computational load with less transmissions. Then the convergence of the iteration learning process is demonstrated by theorems and the tracking objective is achieved as the output tracking errors get converged to zero. Finally, the proposed robust tracking control scheme for constrained-input switched systems is applied in the simulation, and the tracking results proved the effectiveness and applicability of the designed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						162	171		10.1016/j.neucom.2018.07.095													
J								Adaptive optimal tracking control for nonlinear continuous-time systems with time delay using value iteration algorithm	NEUROCOMPUTING										Reinforcement learning; Tracking control; Time delay; Value iteration	CONTROL SCHEME	In this paper, an integral reinforcement learning-based value iteration algorithm is developed for solving the infinite horizon optimal tracking control problem of nonlinear continuous-time systems with time delay. The main idea is using the value iteration technique to obtain the iterative control law, which optimizes the iterative performance index function. In contrast to the existing value iteration algorithms, the proposed IRL-based value iteration algorithm takes the time delay into account. Second, the convergence analysis of the proposed algorithm is given for the nonlinear continuous-time systems with time delay. Moreover, the critic neural network is utilized to approximate the performance index function and compute the optimal control law for facilitating the implementation of the iterative algorithm. Finally, the simulation results are presented to illustrate the effectiveness of the developed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						172	178		10.1016/j.neucom.2018.07.098													
J								Neural-network-based tracking Control for a Class of time-delay nonlinear systems with unmodeled dynamics	NEUROCOMPUTING										Neural networks; Time delay; Backstepping; Unmodeled dynamics	ALMOST-PERIODIC SOLUTIONS; OUTPUT-FEEDBACK STABILIZATION; BARRIER LYAPUNOV FUNCTIONS; FAULT-TOLERANT CONTROL; EXPONENTIAL STABILITY; ADAPTIVE-CONTROL; INTERCONNECTED SYSTEMS; GLOBAL STABILIZATION; VARYING DELAY; EXISTENCE	This paper is concerned with the tracking control problem of a class of non-strict-feedback nonlinear systems with unmodeled dynamics and time-delay. In the backstepping procedure, a dynamic signal is designed to handle the unmodeled dynamics and the Lyapunov-Krasovskii functions are applied to compensate for the effect of time delay. Meanwhile, a neural network-based approximator is used to approximate the unknown nonlinear functions in the system. It is proved by the theoretical analysis that the presented controller guarantees the semi-global boundedness of all signals in the closed-loop systems, and the output tracking error eventually converges to a small area around zero. Simulation results are presented to illustrate the validity of the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						179	190		10.1016/j.neucom.2018.10.091													
J								Adaptive dynamic programming based event-triggered control for unknown continuous-time nonlinear systems with input constraints	NEUROCOMPUTING										Event-triggered control; Adaptive dynamic programming; Input constraints; Neural network	ROBUST-CONTROL; EQUATION; ALGORITHMS; FEEDBACK; DESIGN	An adaptive dynamic programming (ADP) based event-triggered control method is established in this paper to solve the optimal control problem of unknown continuous-time nonlinear systems with input constraints. First, the unknown system is identified using two neural networks (NNs). Second, the threshold for event-triggering condition is designed, which guarantees the system stability. Then, a critic NN is employed to approximate the value function and the closed-loop system is proved to be uniformly ultimately bounded. Finally, the simulation results demonstrate the feasibility of the developed ADP method. The main contributions of this paper are that the developed ADP-based event-triggered control method avoids the Zeno phenomenon effectively and updates the weights of three NNs simultaneously in the process of implementation. Meanwhile, an improved critic NN weight updating criterion is adopted, which does not require an initial admissible control. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						191	200		10.1016/j.neucom.2018.09.097													
J								Asymptotically stable critic designs for approximate optimal stabilization of nonlinear systems subject to mismatched external disturbances	NEUROCOMPUTING										Adaptive dynamic programming; Adaptive critic designs; Mismatched external disturbances; Disturbance observer; Optimal control; Neural networks; Nested tuning law	ADAPTIVE OPTIMAL-CONTROL; TIME LINEAR-SYSTEMS; LEARNING ALGORITHM; TRACKING CONTROL; FEEDBACK-CONTROL; NEURAL-NETWORK; ROBUST; DYNAMICS; GAMES	This paper addresses approximate optimal stabilization problems for nonlinear systems in the presence of mismatched external disturbances via asymptotically stable critic designs. By establishing the nonlinear disturbance observer, the corresponding information is utilized to construct the online updated cost function, which reflects the real-time disturbances, regulation and control simultaneously. With the help of the proper cost function, the Hamilton-Jacobi-Bellman equation is solved by employing a critic neural network, whose weight vector is guaranteed to be asymptotically stable with nested tuning laws. The approximate optimal control is derived to guarantee the closed-loop system to be ultimately uniformly bounded based on the Lyapunov stability theorem. The effectiveness of the developed stabilization scheme is verified via simulations of two numerical examples. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						201	208		10.1016/j.neucom.2018.08.092													
J								Real-time moisture control in sintering process using offline-online NARX neural networks	NEUROCOMPUTING										Deep learning; Parameter identification; Time delay; Output feedback	MODEL	Sinter is the main raw material for blast furnace iron making. To provide high quality sinter, the moisture content of the mixture in the sintering need to be in the best range. However, most of the sintering is still artificial water adding which leads to a great variation in the moisture content of the mixture. The present work proposes a sintering parameter identification model using a nonlinear autoregressive model with exogenous (NARX). By exploiting the real-time and historical performing data, we set up a mixture adding water model involved the water and the major mixtures among sintering. Then, a combination of offline deep supervisor learning and online self-learning NARX algorithm is proposed. Finally, in the experimental stage, the results suggest the proposed method can effectively predict the moisture with an acceptable degree of accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						209	215		10.1016/j.neucom.2018.07.099													
J								Data-driven and deep learning-based detection and diagnosis of incipient faults with application to electrical traction systems	NEUROCOMPUTING										Incipient faults; Fault detection and diagnosis (FDD); Electrical drive systems; Canonical correlation analysis (CCA); Convolutional neural network CNN)		Incipient faults in electrical drive systems will evolve into faults or failures as time goes on. Successful detection and diagnosis of incipient faults can not only improve the safety and reliability but also provide optimal maintenance instructions for electrical drive systems. In this paper, an integration strategy of data-driven and deep learning-based method is proposed to deal with incipient faults. The salient advantages of the proposed method can be summarized as: (1) The moving average technique is firstly introduced into the canonical correlation analysis (CCA) framework, which makes the new residual signals more sensitive to incipient faults than the traditional CCA-based method; (2) Based on the defined residual signals, the new test statistics cooperating closely with Kullback-Leibler divergence (KLD) are proposed from the probability viewpoint, which can greatly improve the fault detectability; (3) It is of high computational efficiency because the estimation of probability density functions of residual signals is skilly avoided; (4) Based on the new developed test statistics, the fault matrices are defined and regarded as the input of convolutional neural network (CNN) whose feature extraction ability is highly improved compared with the traditional method, which helps to accurately diagnose of incipient faults; (5) The proposed method can be implemented without any priori knowledge on system information. Theoretical analysis and three sets of experiments on a practical electrical drive system demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						429	437		10.1016/j.neucom.2018.07.103													
J								A deep learning based multitask model for network-wide traffic speed prediction	NEUROCOMPUTING										Short-term traffic speed prediction; Deep learning; Multitask learning; Nonlinear Granger causality; Bayesian optimization	FLOW PREDICTION; NEURAL-NETWORK; TIME-SERIES; MACHINE	This paper proposes a deep learning based multitask learning (MTL) model to predict network-wide traffic speed, and introduces two methods to improve the prediction performance. The nonlinear Granger causality analysis is used to detect the spatiotemporal causal relationship among various links so as to select the most informative features for the MTL model. Bayesian optimization is employed to tune the hyperparameters of the MTL model with limited computational costs. Numerical experiments are carried out with taxis' GPS data in an urban road network of Changsha, China, and some conclusions are drawn as follows. The deep learning based MTL model outperforms four deep learning based single task learning (STL) models (i.e., Gated Recurrent Units network, Long Short-term Memory network, Convolutional Gated Recurrent Units network and Temporal Convolutional Network) and three other classic models (i.e., Support Vector Machine, k-Nearest Neighbors and Evolving Fuzzy Neural Network). The nonlinear Granger causality test provides a reliable guide to select the informative features from network-wide links for the MTL model. Compared with two other optimization approaches (i.e., grid search and random search), Bayesian optimization yields a better tuning performance for the MTL model in the prediction accuracy under the budgeted computation cost. In summary, the deep learning based MTL model with nonlinear Granger causality analysis and Bayesian optimization promises the accurate and efficient traffic speed prediction for a large-scale network. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						438	450		10.1016/j.neucom.2018.10.097													
J								Segmented convolutional gated recurrent neural networks for human activity recognition in ultra-wideband radar	NEUROCOMPUTING										Micro-Doppler spectrograms; Human activity recognition; Deep learning; Convolutional neural network; Recurrent neural network	MICRO-DOPPLER SIGNATURES; CLASSIFICATION	The automatic detection and recognition of human activities are valuable for physical security, gaming, and intelligent interface. Compared to an optical recognition system, radar is more robust to variations in lighting conditions and occlusions. The centimeter-wave ultra-wideband radar can even track human motion when the target is fully occluded from it. In this work, we propose a neural network architecture, namely segmented convolutional gated recurrent neural network (SCGRNN), to recognize human activities based on micro-Doppler spectrograms measured by the ultra-wideband radar. Unlike most existing approaches which treat the micro-Doppler spectrograms the same way as natural images, we extract segmented features of spectrograms via convolution operation and encode the feature maps along the time axis with gated recurrent units. Taking advantage of regularities in both the time and Doppler frequency domains in this way, our model can detect activities with arbitrary lengths. The experiments show that our method outperforms existing models in fine temporal resolution, noise robustness, and generalization performance. The radar system can thus recognize human behavior when visible light is blocked by opaque objects. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						451	464		10.1016/j.neucom.2018.11.109													
J								A deep learning interpretable classifier for diabetic retinopathy disease grading	NEUROCOMPUTING										Deep learning; Classification; Explanations; Diabetic retinopathy; Model interpretation		In this paper we present a diabetic retinopathy deep learning interpretable classifier. On one hand, it classifies retina images into different levels of severity with good performance. On the other hand, this classifier is able of explaining the classification results by assigning a score for each point in the hidden and input spaces. These scores indicate the pixel contribution to the final classification. To obtain these scores, we propose a new pixel-wise score propagation model that for every neuron, divides the observed output score into two components. With this method, the generated visual maps can be easily interpreted by an ophthalmologist in order to find the underlying statistical regularities that help to the diagnosis of this eye disease. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						465	476		10.1016/j.neucom.2018.07.102													
J								Deep multi-center learning for face alignment	NEUROCOMPUTING										Multi-center learning; Model assembling; Face alignment		Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks. Most of the existing deep learning methods only use one fully-connected layer called shape prediction layer to estimate the locations of facial landmarks. In this paper, we propose a novel deep learning framework named Multi-Center Learning with multiple shape prediction layers for face alignment. In particular, each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively. Challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to reduce the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						477	486		10.1016/j.neucom.2018.11.108													
J								Data augmentation in fault diagnosis based on the Wasserstein generative adversarial network with gradient penalty	NEUROCOMPUTING										Data augmentation; Fault diagnosis; Imbalanced data; Low-data domain; GAN; WGAN-GP	DEEP	Fault detection and diagnosis in industrial process is an extremely essential part to keep away from undesired events and ensure the safety of operators and facilities. In the last few decades various data based machine learning algorithms have been widely studied to monitor machine condition and detect process faults. However, the faulty datasets in industrial process are hard to acquire. Thus low-data of faulty data or imbalanced data distributions are common to see in industrial processes, resulting in the difficulty to accurately identify different faults for many algorithms. Therefore, in this paper, Wasserstein generative adversarial network with gradient penalty (WGAN-GP) based data augmentation approaches are researched to generate data samples to supplement low-data input set in fault diagnosis field and help improve the fault diagnosis accuracies. To verify its efficient, various classifiers are used and three industrial benchmark datasets are involved to evaluate the performance of GAN based data augmentation ability. The results show the fault diagnosis accuracies for classifiers are increased in all datasets after employing the GAN-based data augmentation techniques. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						487	494		10.1016/j.neucom.2018.10.109													
J								Deep successor feature learning for text generation	NEUROCOMPUTING										Reinforcement learning; Successor feature; Encoder-decoder; Neural machine translation; Deep learning		In this paper we present an approach to training neural network to generate sequences using successor feature learning from reinforcement learning. The model can be thought as two components, an MLE-based token generator and an estimator that predicts the future value of whole sentence. As we know, reinforcement learning has been applied to dealing with the exposure bias problem of generating sequences. Compared with other RL algorithm, successor feature(SF) can learn robust value function provided observations and reward by decomposing the value function into two components - a reward predictor and a successor map. The encoder-decoder framework with SF enables the decoder to generate outputs that receive more future reward, which means that the model pays attention on not only the current word but also the rest words. We demonstrate that the approach improves performance on two translation tasks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						495	500		10.1016/j.neucom.2018.11.116													
J								A sequential deep learning application for recognising human activities in smart homes	NEUROCOMPUTING										Smart home; Human activity recognition; Deep learning; LSTM	HUMAN ACTIVITY RECOGNITION; NEURAL-NETWORKS	The recent advancement and development of computer electronic devices has led to the adoption of smart home sensing systems, stimulating the demand for associated products and services. Accordingly, the increasingly large amount of data calls the machine learning (ML) field for automatic recognition of human behaviour. In this work, different deep learning (DL) models that learn to classify human activities were proposed. In particular, the long short-term memory (LSTM) was applied for modelling spatiotemporal sequences acquired by smart home sensors. Experimental results performed on the Center for Advanced Studies in Adaptive Systems datasets show that the proposed LSTM-based approaches outperform existing DL and ML methods, giving superior results compared to the existing literature. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						501	513		10.1016/j.neucom.2018.10.104													
J								Automatic detection of anatomical landmarks in brain MR scanning using multi-task deep neural networks	NEUROCOMPUTING										Automatic graphical prescription; Brain landmark detection; Multi-task learning; Deep neural networks; anterior commissures (AC) and posterior commissures (PC)	SEGMENTATION; LOCALIZATION; PLACEMENT; ANTERIOR	This work involves the development of a computer method to perform automatic graphical prescription in brain MR scanning. The approach is based on multi-task deep neural networks that perform automatic detection of the anterior commissures (AC) and posterior commissures (PC) in the sagittal images, and automatic determination of the symmetry line in the axial images. The proposed multi-task learning architecture solves the tasks of point and angle detection simultaneously by exploiting the commonalities and differences across these tasks. This results in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training individual models separately. After deriving the AC-PC line and symmetry line on the sagittal image and axial image, respectively, the corresponding scan coverage is then determined by using an image processing approach. Based on a study using a small-sized MR brain image dataset, three benefits are observed: Firstly, our proposed approach was able to perform the task well despite limited availability of training data. This is an advantage in view of the fact that training of single task models will typically encounter difficulty in convergence using a small training set. Secondly, it achieves better performance for landmarks detection in terms of smaller position and angulation shifts between the predicted results and the ground truth. Lastly, the inference can be performed faster since the two tasks are solved simultaneously using only one model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						514	521		10.1016/j.neucom.2018.10.105													
J								A very deep two-stream network for crowd type recognition	NEUROCOMPUTING										Crowd model; Crowd type recognition; Two-stream network; Very deep learning; Emergency alert	BEHAVIOR	Crowd type identification is a crucial task in the emergency alert. In this paper, to solve accurate identification of crowd type, the crowd type description triad C-BMO < Behavior, Mood, Organized > and a novel crowd type recognition network (CTRN): very deep two-stream network architecture are proposed, respectively. The very deep two-stream network architecture is based on the static map and motion map in the video. To early warn the emergency, the reasoning rules of the emergency alert are proposed based on joining the crowd type and the crowd characteristics. To verify the proposed method, the crowd type dataset is collected, and we experiment with the proposed plan on the crowd type dataset. The experimental results demonstrate that the proposed model is competitive compared with the state-of-the-art techniques. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 5	2020	396						522	533		10.1016/j.neucom.2018.10.106													
J								Training binary neural networks with knowledge transfer	NEUROCOMPUTING										Binary neural networks; Deep learning; Knowledge transfer		Binary Neural Networks (BNNs) use binary values for both weights and activations instead of 32 bit floating point numbers typically used in deep neural networks. This reduces the memory footprint by a factor of 32 and allows a very efficient implementation in hardware. BNNs are trained using regular gradient descent but are harder to optimise, take longer to train and generally require a more careful tuning of hyperparameters such as the learning rate decay schedule than floating point versions. We propose to use Knowledge Transfer techniques to make it easier to train BNNs. Knowledge transfer is a general technique that tries to transfer the knowledge stored in a large network (the teacher) to a smaller (student) network. In our case the teacher is a network trained with floating point weights and activations while the student is a BNN. We apply different Knowledge Transfer techniques to the task of training a BNN. We introduce a novel similarity based Knowledge Transfer algorithm and show that this technique results in a higher test accuracy on different benchmark datasets compared to training the BNN from scratch. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						534	541		10.1016/j.neucom.2018.09.103													
J								A new hybrid deep signal processing approach for bearing fault diagnosis using vibration signals	NEUROCOMPUTING										Deep learning; Signal processing; Vibration analysis; Bearing fault diagnosis	ROTATING MACHINERY	Signal processing is an important task for machine fault diagnosis. Over the recent years, many deep learning based signal processing methods have been developed for bearing fault diagnosis. However, these methods are facing some major problems when they are applied to machine fault diagnosis. In this paper, a new hybrid deep signal processing method for bearing fault diagnosis is presented. The presented method incorporates vibration analysis techniques into deep learning to form a deep learning structure embedded with time synchronous resampling mechanism. Data collected from real bearing test rig are used to validate and demonstrate the effectiveness of the presented method. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 5	2020	396						542	555		10.1016/j.neucom.2018.12.088													
J								Deep learning-based visual ensemble method for high-speed railway catenary clevis fracture detection	NEUROCOMPUTING										High-speed railway; Clevis extraction; Crack detection; Convolutional neural network; RSF model	SCALABLE FITTING ENERGY; LEVEL SET; ACTIVE CONTOURS; IMAGE; MODEL; SEGMENTATION; CLASSIFICATION; MINIMIZATION; DRIVEN; SCALE	This paper proposes an automatic visual inspection method for the fracture detection of clevises in the catenary systems of high-speed railways, using images of catenaries captured by an inspection vehicle. First, the clevises are extracted from the catenary image using a convolutional neural network based algorithm, known as the faster region-based convolutional neural network. Because the structure of catenary systems does not have many variations and the contextual information near a catenary fitting may have strong correlation with its category, the architecture of the original faster region-based convolutional neural network is modified to make use of the contextual information of the regions of interest in the images for object recognition. A crack detection process is then used to recognize the fractures of clevises. To detect the cracks, the edge map of the clevis sub-image is generated using a region-scalable fitting model. Areas where the cracks are most likely to occur are projected from a standard clevis image to the clevis sub-image by shape context matching and affine transformation matrix computation. The cracks are then recognized by calculating the wavelet entropy inside these areas followed by morphological filtering. Experimental results show that the modified faster region-based convolutional neural network architecture achieves better results in clevis extraction than the original architecture as well as some other state-of-art object detection models. The detection is not affected by the scaling, texture and grayscale changes of the clevises caused by the variation of shooting distance, shooting angle and illumination variations. The fractures of the clevises can be accurately and reliably detected using the fracture detection method proposed in this paper and the performance of this visual inspection method meets the strict requirements for catenary system maintenance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						556	568		10.1016/j.neucom.2018.10.107													
J								Scalp EEG epileptogenic zone recognition and localization based on long-term recurrent convolutional network	NEUROCOMPUTING										Seizure; LRCN; Electroencephalogram; Deep learning	SEIZURE ONSET; MODEL	The scalp electroencephalogram (EEG), a non-invasive measure of brain's electrical activity, is commonly used ancillary test to aide in the diagnosis of epilepsy. Usually, neurologists employ direct visual inspection to identify epileptiform abnormalities. Therefore, electroencephalograms have been an essential integral to the researches which aim to automatically detect epilepsy. However, it is difficult because seizure manifestations on scalp EEG are extremely variable between patients, even the same patient. In addition, scalp EEG is usually composed of large number of noise signals which might cover the real features of seizure. To this challenge, we construct an 18-layer Long-Term recurrent convolutional network (LRCN) to automatic epileptogenic zone recognition and localization on scalp EEG. As far as we know, we are the first to train a deep learning classifier to identify seizures through the EEG images, just like neurologists direct visual inspection to identify epileptiform abnormalities. Furthermore, unlike the traditionally methods extracted features from channels manually, which neglected the association of brain's epileptiform abnormalities electrical transmission, seizures is considered as a continuous brain's abnormal electrical activity in our algorithm, from produce at one or several channels, transmission between channels, to flat again after seizures. The method was evaluated in 23 patients with a total of 198 seizures. The classifier shows reasonably good results, with 84% for sensitivity, 99% for specificity, and 99% for accuracy. False Positive Rate per hours exceeds significantly previous results obtained on cross-patient classifiers, with 0.2/h. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						569	576		10.1016/j.neucom.2018.10.108													
J								Domain adaptation with SBADA-GAN and Mean Teacher	NEUROCOMPUTING										SBADA-GAN; Mean Teacher; Domain adaptation		Unsupervised domain adaptation has received much attention in the past decade, and various methods have been developed for this problem, e.g., SBADA-GAN (Symmetric Bidirectional ADAptive Generative Adversarial Network) and Mean Teacher, which achieved state-of-the-art results. However, their performance is not perfect when the source and target distributions are not well matched. By combining SBADA-GAN with Mean Teacher, we propose a powerful model for unsupervised domain adaptation, where we use Mean Teacher to replace the target classifier of SBADA-GAN and develop a bidirectional class cycle-consistency strategy to preserve the class identity of the transformed images. Furthermore, we apply gradient penalty and spectral normalization to improve the stability of the training process. The proposed method was demonstrated on six unsupervised domain adaptation benchmarks with significant and consistent improvements over the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						577	586		10.1016/j.neucom.2018.12.089													
J								A data-efficient deep learning approach for deployable multimodal social robots	NEUROCOMPUTING										Deep reinforcement learning; Deep supervised learning; Interactive robots; Multimodal perception and interaction; Board games	BEHAVIOR; LANGUAGE; GAME	The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games-and use the game of 'Noughts and Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the Pepper robot confirms that highly accurate visual perception is required for successful game play. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						587	598		10.1016/j.neucom.2018.09.104													
J								Learning to detect Android malware via opcode sequences	NEUROCOMPUTING										Android malware; Deep learning; Instruction call graph; Neural network	CLASSIFICATION; NETWORK	A large number of Android malware samples can be deployed as the variants of the previously known samples. In consequence, a classification system capable of supporting a large set of samples is required to secure Android platform. Although a large set of variants requires scalability for automatic detection and classification, it also presents a significant advantage about a richer dataset at the stage of discovering underlying malicious activities and extracting representative features. Deep Neural Networks are built by a complex structure of layers whose parameters can be tuned and trained in order to enhance classification statistical metric results. Emerging parallelization computing tools and processors reduce computation time. In this paper, we propose a deep learning Android malware detection method using features extracted from instruction call graphs. The presented method examines all possible execution paths and the balanced dataset improves deep neural learning benign execution paths versus malicious paths. Since there is not a publicly available model for Android malware detection, we train deep networks from scratch. Then, we apply a grid search method to seek the optimal parameters of the network and to discover the combination of the hyper-parameters, which maximizes the statistical metric values. To validate the effectiveness of the proposed method, we evaluate with a balanced dataset constituted by 24,650 malicious and 25,0 00 benign samples. We evaluate the deep network architecture with respect to different parameters and compare the statistical metric values including runtime with respect to baseline classifiers. Our experimental results show that the presented malware detection is reached at 91.42% level in accuracy and 91.91% in F-measure, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						599	608		10.1016/j.neucom.2018.09.102													
J								Nash Q-learning based equilibrium transfer for integrated energy management game with We-Energy	NEUROCOMPUTING										Nash Q-learning; Integrated energy management game; Interconnected multicarrier systems; Equilibrium transfer; We-Energy	REINFORCEMENT; MARKET	This paper proposes an innovative energy interacting unit ("We-Energy") with the characteristic of full duplex trading mode. In order to manage all the We-Energies in an optimal way, a new integrated energy management framework based on a noncooperative game is performed so as to allocate the energy demands of each WE such that the benefit of each WE can be maximized. To overcome the impact of the randomness and inaccurate information of renewable energy sources, Nash Q-learning algorithm is applied for computation of game equilibrium under the unknown environment. The novelty of the proposed algorithms is related to the incorporation of the continuous action space into the discrete adaptive action set and combined the equilibrium transfer to improve the efficiency of the algorithm. Simulation studies of modified IMS confirm that it has a better performance with the desired equilibrium strategy and convergence speed. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						216	223		10.1016/j.neucom.2019.01.109													
J								Correlational Convolutional LSTM for human action recognition	NEUROCOMPUTING										Human action recognition; Deep learning; Convolutional networks; LSTM; ConvLSTM	GOING DEEPER	In light of recent exponential growth of video data, the need for automated video processing has increased substantially. To learn the intrinsic structure of video data, many representation approaches have been proposed, focusing on learning the spatial features and time dependencies, while, motion features are hand-crafted and left out of the learning process. In this work, we present an extended version of the LSTM units named (CLSTM)-L-2 in which the motion data are perceived as well as the spatial features and temporal dependencies. We leverage convolution and correlation operators to credit both the spatial and motion structure of the video data. Furthermore, a deep network is designed for human action recognition using the proposed units. The network is evaluated on the two well-known benchmarks, UCF101 and HMDB51. The results confirm the potency of (CLSTM)-L-2 to capture motion as well as spatial features and time dependencies. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						224	229		10.1016/j.neucom.2018.10.095													
J								Coordinated behavior of cooperative agents using deep reinforcement learning	NEUROCOMPUTING										Deep reinforcement learning; Multi-agent systems; Cooperation; Coordination	LINEAR MULTIAGENT SYSTEMS; INTELLIGENCE; TAXONOMY	In this work, we focus on an environment where multiple agents with complementary capabilities cooperate to generate non-conflicting joint actions that achieve a specific target. The central problem addressed is how several agents can collectively learn to coordinate their actions such that they complete a given task together without conflicts. However, sequential decision-making under uncertainty is one of the most challenging issues for intelligent cooperative systems. To address this, we propose a multi-agent concurrent framework where agents learn coordinated behaviors in order to divide their areas of responsibility. The proposed framework is an extension of some recent deep reinforcement learning algorithms such as DQN, double DQN, and dueling network architectures. Then, we investigate how the learned behaviors change according to the dynamics of the environment, reward scheme, and network structures. Next, we show how agents behave and choose their actions such that the resulting joint actions are optimal. We finally show that our method can lead to stable solutions in our specific environment. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						230	240		10.1016/j.neucom.2018.08.094													
J								A self-organizing deep belief network based on information relevance strategy	NEUROCOMPUTING										Deep belief network; Information relevance strategy; Maximal information coefficient; Mutual information; Grow and prune	TIME-SERIES PREDICTION; RBF NEURAL-NETWORK; ALGORITHM; MODEL; CLASSIFICATION; NEURONS; DROPOUT; SYSTEM; IMAGES	One of the major obstacles in using deep belief network (DBN) is the structure design. Numerous studies, both empirically and theoretically, show that choosing suitable structure can improve the performance of DBN. In this paper, a self-organizing DBN (S-DBN), based on the information relevance strategy (IRS), was proposed to design the structure of DBN. For this IRS, the maximal information coefficient was designed to measure the input and output information relevance of hidden neurons. Meanwhile, the mutual information was introduced to measure the information relevance among the hidden layers. Then, a novel self-organizing strategy was developed to grow and prune both the hidden neurons and layers during the training process. Moreover, a contrastive divergence algorithm was used to adjust the parameters of S-DBN. Finally, several benchmark problems were used to illustrate the effectiveness of S-DBN. The experimental results demonstrate that the proposed S-DBN owns better performance for classification problems and modeling nonlinear systems than some existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						241	253		10.1016/j.neucom.2018.08.093													
J								Fine-grained image analysis via progressive feature learning	NEUROCOMPUTING										Fine-grained recognition; Attention model; Neural network	ALMOST-PERIODIC SOLUTIONS; NEURAL-NETWORKS; MODEL; STABILITY; EXISTENCE	Due to large intra-class variation and inter-class ambiguity, fine-grained object recognition has been a challenging task for decades. A good approach should be able to: (1) discover discriminative local details and (2) align and aggregate these local discriminative patch-level features in an effective manner to facilitate object-level classification. Toward this end, we develop a two-stage framework for fine-grained recognition. In our framework, the first stage aims to discover and align local discriminative features, and the second stage aims to aggregate these features to get classification results. In the first stage, we propose two methods to sequentially discover informative regions. In the second stage, we progressively feed these discovered and aligned regions into a recurrent neural network to obtain object-level representation. Extensive experiments are conducted on three fine-grained image benchmarks, and the results verify the effectiveness of our proposed framework. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						254	265		10.1016/j.neucom.2018.07.100													
J								Image classification with an RGB-channel nonsubsampled contourlet transform and a convolutional neural network	NEUROCOMPUTING										Nonsubsampled contourlet transform (NSCT); Convolutional neural network (CNN); The feature descriptor; RGB-channel images; Classification		In this paper, an efficient image classification method is proposed that is based on the nonsubsampled contourlet transform (NSCT) of RGB-channel images and the convolutional neural network (CNN). First, the NSCT-based coefficients of natural RGB-channel images are extracted, which are capable of capturing the statistical properties of each channel. In addition, the proposed feature descriptor is equipped with the mean-max-pooling strategy according to the characteristics of the correlated coefficients. Then, the CNN is concatenated to exaggerate the discriminative parts of the primary features. With these advantages, the proposed RGB-channel NSCT-CNN should, in general, improve the corresponding CNN-based image classification methods. Using the Food-101 and SUN Datasets, the proposed method achieves state-of-the-art classification results that are also significant for object detection. In addition, the proposed method can achieve better or comparable accuracy compared to other related methods based on these two datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						266	277		10.1016/j.neucom.2018.10.094													
J								Two birds with one stone: Transforming and generating facial images with iterative GAN	NEUROCOMPUTING										Image transformation; Image generation; Perceptual loss; GAN	ADVERSARIAL NETWORKS	Generating high fidelity identity-preserving faces with different facial attributes has a wide range of applications. Although a number of generative models have been developed to tackle this problem, there is still much room for further improvement. In particular, the current solutions usually ignore the perceptual information of images, which we argue that it benefits the output of a high-quality image while preserving the identity information, especially in facial attributes learning area. To this end, we propose to train GAN iteratively via regularizing the min-max process with an integrated loss, which includes not only the per-pixel loss but also the perceptual loss. In contrast to the existing methods only deal with either image generation or transformation, our proposed iterative architecture can achieve both of them. Experiments on the multi-label facial dataset CelebA demonstrate that the proposed model has excellent performance on recognizing multiple attributes, generating a high-quality image, and transforming image with controllable attributes. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						278	290		10.1016/j.neucom.2018.10.093													
J								Hierarchical temporal memory and recurrent neural networks for time series prediction: An empirical validation and reduction to multilayer perceptrons	NEUROCOMPUTING										HTM; LSTM; GRU; Time series prediction	ELECTRICITY PRICES; PERFORMANCE; GRADIENT	Recurrent Neural Networks such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs) are often deployed as neural network-based predictors for time series data. Recently, Hierarchical Temporal Memory (HTM), a machine learning technology attempting to simulate the human brain's neocortex, has been proposed as another approach to time series data prediction. While HTM has gained a lot of attention, little is known about the actual performance compared to the more common RNNs. The only performance comparison between the two, performed at the company behind HTM, shows they perform similarly. In this article, we present a more in-depth performance comparison, involving more extensive hyperparameter tuning and evaluation on more scenarios. Surprisingly, our results show that both LSTM and GRUs can outperform HTM by over 30% at lower runtime. Furthermore, we show that HTM requires explicitly timestamped data to recognize daily and weekly patterns, while LSTM only needs the raw sequential data to predict such time series accurately. Finally, our experiments indicate that the temporally aware components of all considered predictors contribute nothing to the prediction accuracy. We further strengthen this claim by presenting equally or better performing Multilayer Perceptrons conceptually similar to the HTM and LSTM, disregarding their temporal aspects. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						291	301		10.1016/j.neucom.2018.09.098													
J								Integrating pixels and segments: A deep-learning method inspired by the informational diversity of the visual pathways	NEUROCOMPUTING										Deep neural networks; Visual cortex modelling; Segment-based representation; Convolutional neural network; Stacked autoencoder	SHAPE; AUTOENCODERS; PERCEPTION; STREAM	Visual cortex is able to process information in multiple pathways and integrate various forms of representations. This paper proposed a bio-inspired method that utilizes the line-segment-based representation to perform a dedicated channel for the geometric feature learning process. The extracted geometric information can be integrated with the original pixel-based information and implemented on both the convolutional neural networks (SegCNN) and the stacked autoencoders (SegSAE). Segment-based operations such as segConvolve and segPooling are designed to further process the extracted geometric features. The proposed models are verified on the MNIST dataset, Caltech 101 dataset and QuickDraw dataset for image classification. According to the experimental results, the proposed models can facilitate the classification accuracies especially when the sizes of the training set are limited. Particularly, the method based on multiple representations is found to be effective for classifying the hand-drawn sketches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						314	323		10.1016/j.neucom.2018.10.096													
J								Joint blur kernel estimation and CNN for blind image restoration	NEUROCOMPUTING										Blind image restoration; Blur kernel; Blur support parameter estimation; Blur type identification; CNN	REGULARIZATION APPROACH; MOTION; IDENTIFICATION; SIMILARITY; PARAMETERS	Convolutional neural networks (CNN) have shown its excellent performance in computer vision fields. Recently, they are successfully applied to image restoration. This paper proposes a joint blur kernel estimation and CNN method for blind image restoration. The blur kernel estimation is based on both blur support parameter estimation and blur type identification. An automatic feature line detection algorithm is presented for blur support parameter estimation and a dictionary learning algorithm is presented for the blur type identification. Once the blur kernel estimate is obtained, we use an effective CNN for iterative non-blind deconvolution, which is able to automatically learn image priors. Compared with current blind image restoration methods, the proposed joint method can obtain restored images under three types of unknown blur kernels. The experimental result shows that the proposed blur kernel estimation algorithm can provide high accuracy results. Furthermore, the proposed joint blur kernel estimation and CNN algorithm is superior to conventional blind image restoration algorithms in terms of restoration quality and computation time. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						324	345		10.1016/j.neucom.2018.12.083													
J								Coupled ensembles of neural networks	NEUROCOMPUTING										Neural net architectures; Ensemble learning; Deep learning		We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as "coupled ensembles". The approach is generic and can be applied to almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25 M, we obtain error rates of 2.92%, 15.68% and 1.50% on CIFAR-10, CIFAR-100 and SVHN, respectively. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8%, respectively. With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42%, respectively on these tasks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						346	357		10.1016/j.neucom.2018.10.092													
J								Multi-branch convolutional neural network for built-up area extraction from remote sensing image	NEUROCOMPUTING										Built-up area extraction; Convolutional neural network; Remote sensing; Feature learning; Graph model	SEGMENTATION; INDEX	Built-up area is one of the most important objects of remote sensing images analysis, therefore extracting built-up area from remote sensing image automatically has attracted wide attention. It is common to treat built-up area extraction as image segmentation task. However, it's hard to devise a handcrafted feature to describe built-up area since it contains many non-built-up elements, such as trees, grasslands, and small ponds. Besides, built-up area corresponds to large size local region without precise boundary in remote sensing image so that the precision of segmentation in pixel level is not reliable. To cope with the problem of built-up area extraction, a segmentation framework based on deep feature learning and graph model is proposed. The segmentation procedure comprises of three steps. Firstly, the image is divided into small patches whose deep features are extracted by the devised lightweight multi-branch convolutional neural network (LMB-CNN). Secondly, a patch-wise graph model is constructed according to the learnt features, and then is optimized to segment built-up area with patch-level precision in full frame of remote sensing image. At last, post-processing step is also adopted to make the segmentation result visually intact. The experiments verify that the proposed method shows excellent performance by achieving high overall accuracy over 98.6% on Gaofen-2 remote sensing image data with size of 10,240 x10,240. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						358	374		10.1016/j.neucom.2018.09.106													
J								Deep quality-related feature extraction for soft sensing modeling: A deep learning approach with hybrid VW-SAE	NEUROCOMPUTING										Deep learning; Soft sensor; Feature representation; Stacked autoencoder (SAE); Hybrid variable-wise weighted SAE (HVW-SAE)	REGRESSION-MODEL; PROCESS INDUSTRY; SENSOR; FRAMEWORK	Soft sensors have been extensively used to predict difficult-to-measure quality variables for effective modeling, control and optimization of industrial processes. To construct accurate soft sensors, it is significant to carry out feature extraction for massive high-dimensional process data. Recently, deep learning has been introduced for feature representation in process data modeling. However, most of them cannot capture deep quality-related features for output prediction. In this paper, a hybrid variable-wise weighted stacked autoencoder (HVW-SAE) is developed to learn quality-related features for soft sensor modeling. By measuring the linear Pearson and nonlinear Spearman correlations for variables at the input layer with the quality variable at each encoder, a corresponding weighted reconstruction objective function is designed to successively pretrain the deep networks. With the constraint of preferential reconstruction for more quality-related variables, it can ensure that the learned features contain more information for quality prediction. Finally, the effectiveness of the proposed HVW-SAE based soft sensor method is validated on an industrial debutanizer column process. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						375	382		10.1016/j.neucom.2018.11.107													
J								Multilayer probability extreme learning machine for device-free localization	NEUROCOMPUTING										Device-free localization; Extreme learning machine; Extreme learning machine autoencoder; Multilayer probability extreme learning machine	ALGORITHM; UNCERTAINTY; STRATEGY; MODEL	Device-free localization (DFL) is becoming one of the new techniques in wireless localization field, due to its advantage that the target to be localized does not need to attach any electronic device. One of the key issues of DFL is how to characterize the influence of the target on the wireless links, such that the target's location can be accurately estimated by analyzing the changes of the signals of the links. Most of the existing related research works usually extract the useful information from the links through manual approaches, which are labor-intensive and time-consuming. Deep learning approaches have attempted to automatically extract the useful information from the links, but the training of the conventional deep learning approaches are time-consuming, because a large number of parameters need to be fine-tuned multiple times. Motivated by the fast learning speed and excellent generalization performance of extreme learning machine (ELM), which is an emerging training approach for generalized single hidden layer feed-forward neural networks (SLFNs), this paper proposes a novel hierarchical ELM based on deep learning theory, named multilayer probability ELM (MP-ELM), for automatically extracting the useful information from the links, and implementing fast and accurate DFL. The proposed MP-ELM is stacked by ELM autoencoders, so it also keeps the very fast learning speed of ELM. In addition, considering the uncertainty and redundant links existing in DFL, MP-ELM outputs the probabilistic estimation of the target's location instead of the deterministic output. The validity of the proposed MP-ELM-based DFL is evaluated both in the indoor and the outdoor environments, respectively. Experimental results demonstrate that the proposed MP-ELM can obtain better performance compared with classic ELM, multilayer ELM (ML-ELM), hierarchical ELM (H-ELM), deep belief network (DBN), and deep Boltzmann machine (DBM). (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						383	393		10.1016/j.neucom.2018.11.106													
J								The Random Neural Network with Deep Learning Clusters in Smart Search	NEUROCOMPUTING										Intelligent search; World wide web; Random Neural Network; Clusters; Deep learning; Management clusters		This paper proposes a Neurocomputing application that reorders the Web results obtained from different Web Search Engines emulating the way our brain takes decisions. The proposed application is based on the Random Neural Network with Deep Learning Clusters that evaluates and adapts Web result relevance by associating independently each Deep Learning Cluster to a specific Web Search Engine. In addition, this paper presents a Deep Learning Cluster to perform as a Management Cluster that decides the final result relevance based on the inputs from each independent Deep Learning cluster. The performance of the proposed Management Cluster is evaluated when included as an additional layer to the Deep Learning Clusters. On average; the proposed Deep Learning cluster structure improves Smart Search performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						394	405		10.1016/j.neucom.2018.05.134													
J								Deep Learning Clusters in the Cognitive Packet Network	NEUROCOMPUTING										Random Neural Network; Deep Learning Clusters; Cognitive Packet Network; QoS; Cybersecurity; Routing	RANDOM NEURAL-NETWORKS	The Cognitive Packet Network (CPN) bases its routing decisions and flow control on the Random Neural Network (RNN) Reinforcement Learning algorithm; this paper proposes the addition of a Deep Learning (DL) Cluster management structure to the CPN for Quality of Service metrics (Delay Loss and Bandwidth), Cyber Security keys (User, Packet and Node) and Management decisions (QoS, Cyber and CEO). The RNN already models how neurons transmit information using positive and negative impulsive signals whereas the proposed additional Deep Learning structure emulates the way the brain learns and takes decisions; this paper presents a brain model as the combination of both learning algorithms, RNN and DL. The proposed model has been simulated under different network sizes and scenarios and it has been validated against the CPN itself without DL clusters. The simulation results are promising; the presented CPN with DL clusters as a mechanism to transmit, learn and make packet routing decisions is a step closer to emulate the way the brain transmits information, learns the environment and takes decisions. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 5	2020	396						406	428		10.1016/j.neucom.2018.07.101													
J								Modeling learner's dynamic knowledge construction procedure and cognitive item difficulty for knowledge tracing	APPLIED INTELLIGENCE										Knowledge tracing; Learner modeling; Knowledge construction procedure; Cognitive item difficulty; Learning and forgetting	SYSTEM; TUTORS; HARD	Knowledge tracing (KT) is essential for adaptive learning to obtain learners' current states of knowledge for the purpose of providing adaptive service. Generally, the knowledge construction procedure is constantly evolving because students dynamically learn and forget over time. Unfortunately, to the best of our knowledge most existing approaches consider only a fragment of the information that relates to learning or forgetting, and the problem of making use of rich information during learners' learning interactions to achieve more precise prediction of learner performance in KT remains under-explored. Moreover, existing work either neglects the problem difficulty or assumes that it is constant, and this is unrealistic in the actual learning process as problem difficulty affects performance undoubtedly and also varies overtime in terms of the cognitive challenge it presents to individual learners. To this end, we herein propose a novel model, KTM-DLF (Knowledge Tracing Machine by modeling cognitive item Difficulty and Learning and Forgetting), to trace the evolution of each learner's knowledge acquisition during exercise activities by modeling his or her dynamic knowledge construction procedure and cognitive item difficulty. Specifically, we first specify the concept of cognitive item difficulty and propose a method to model the cognitive item difficulty adaptively based on learners' learning histories. Then, based on two classical theories (the learning curve theory and the Ebbinghaus forgetting curve theory), we propose a method for modeling learners' learning and forgetting over time. Finally, the KTM-DLF model is proposed to incorporate learners' abilities, the cognitive item difficulty, and the two dynamic procedures (learning and forgetting) together. We then use the factorization machine framework to embed features in high dimensions and model pairwise interactions to increase the model's accuracy. Extensive experiments have been conducted on three public real-world datasets, and the results confirm that our proposed model outperforms the other state-of-the-art educational data mining models.																	0924-669X	1573-7497				NOV	2020	50	11					3894	3912		10.1007/s10489-020-01756-7		JUL 2020											
J								Artificial bee colony algorithm based on knowledge fusion	COMPLEX & INTELLIGENT SYSTEMS										Artificial bee colony (ABC); Knowledge fusion; Exploration and exploitation; Opposition-based learning; Optimization	PARTICLE SWARM OPTIMIZATION	Artificial bee colony (ABC) algorithm is one of the branches of swarm intelligence. Several studies proved that the original ABC has powerful exploration and weak exploitation capabilities. Therefore, balancing exploration and exploitation is critical for ABC. Incorporating knowledge in intelligent optimization algorithms is important to enhance the optimization capability. In view of this, a novel ABC based on knowledge fusion (KFABC) is proposed. In KFABC, three kinds of knowledge are chosen. For each kind of knowledge, the corresponding utilization method is designed. By sensing the search status, a learning mechanism is proposed to adaptively select appropriate knowledge. Thirty-two benchmark problems are used to validate the optimization capability of KFABC. Results show that KFABC outperforms nine ABC and three differential evolution algorithms.																	2199-4536	2198-6053															10.1007/s40747-020-00171-2		JUL 2020											
J								Discrete convolutional CRF networks for depth estimation from monocular infrared images	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Infrared images; Depth estimation; Discrete conditional random field; Order information		Predicting the depth of a scene from monocular infrared images, which plays a crucial role in understanding three-dimensional structures, is one of the challenging tasks in machine learning and computer vision. Considering the lack of texture and color information in infrared images, a novel discrete convolutional conditional random field network is proposed for depth estimation. The proposed method inherits several merits of conditional random fields and deep learning. First, the pairwise features are automatically extracted and optimized through deep architectures. Second, the monocular-images-based depth regression is converted into a multi-class classification, in which the order information of different levels of depths is considered in the loss function. Our experiments demonstrate that this conversion achieves much higher accuracy and faster conversion. Third, to obtain fine-grained level details, we have further proposed a multi-scale discrete convolutional conditional random field network that computes the pairwise features of the discrete conditional random field at different spatial levels. Extensive experiments on the infrared image dataset NUSTMS demonstrate that the proposed method outperforms other depth estimation methods. Specifically, for the proposed method, the mean relative error is 0.181, the mean log10 error is 0.072, and the accuracy with a threshold (t = 1.25(3)) is 95.3%.																	1868-8071	1868-808X															10.1007/s13042-020-01164-w		JUL 2020											
J								Association rule generation and classification with fuzzy influence rule based on information mass value	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Association rules; Classification; Fuzzy influence rules; Mass value; Rule generation; Disease prediction	SELECTION; PREDICTION	The association rule based classification is imperative in the disease prediction owing to its high predictability. To deal with the sensitive data, we propose an algorithm using fuzzy inference set. The association rule mining is improved further by generating an associative rules for each item of the data set. The ranking of the item in the data set is based on the information mass value estimated. The mass value represents the depth of the item in the data set and its class. Selection of the certain item set is done based on the mass value of different associated items. According to the associative items selected, the association rule mining is performed. For each association rule generated, this method calculates the impact of each object from the rules based on how fuzzy rules are generated. Fuzzy impact rules indicate symptoms and diagnostic labels. A class of disease posses disease influence measure that predicts each class of disease has changed. The proposed algorithm improves the classification efficiency and reduces the error rates.																	1868-5137	1868-5145															10.1007/s12652-020-02280-9		JUL 2020											
J								A hybrid model-based method for leak detection in large scale water distribution networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WDN; Leak; Flow; Pressure; Machine learning	WIRELESS SENSOR NETWORKS; LOCALIZATION; PIPELINES	During the past decades, the problem of finding leaks in Water Distribution Networks (WDN) has been challenging. The quicker detection of leaks, on one hand prevents water loss and helps avoiding economic and environmental leakage consequences. On the other hand, increasing the speed of leak detection increases the false leak detection that imposes high costs. In this paper, we propose a fast hybrid method using AI algorithms and hydraulic relations for detecting and locating leaks and identifying the volume of losses material. The proposed method relies on simple and cost-effective flow sensors installed on each junction in the pipeline network. We demonstrate how influential features for leak detection would be generated by using hydraulic equations like Hazen-Williams, Darcy-Weisbach and pressure drop. Through exploiting Decision Tree, KNN, random forest, and Bayesian network we build predictive models and based on the pipeline topology, we locate leaks and their pressure. Comparing the results of applying the proposed method on various leak scenarios shows that the proposed method in this paper, outperforms other existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02233-2		JUL 2020											
J								Modified SVPWM based three phase to nine phase matrix converter for nine phase induction motor with closed loop speed control	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										3-9CYRILLIC CAPITAL LETTER EF matrix converter; Induction motor; Space vector PWM; PI controller; PR controller; Fuzzy controller	VECTOR-MODULATED 3-PHASE; IMPLEMENTATION	A new design procedure is presented in this paper to simulate the implementation of 3-9 CYRILLIC CAPITAL LETTER EF power conversion using matrix converter whose output is fed to the Induction motor to control speed under closed loop configuration. Using mathematical derivations, the converter is modeled and its performance is evaluated using nonlinear load. The overall matrix converter circuit is simulated in MATLAB. The numerical equations which relate the input and outcome of the 9CYRILLIC CAPITAL LETTER EF matrix converter is executed by utilizing Simulation blocks. The duty cycle of the bidirectional switches is estimated using PI/PR/Fuzzy controller for optimum voltage transfer ratio. Due to the low on-state resistance and conduction losses, and better power handling capacity in addition to the ability of switching at higher voltages at frequency below 20 kHz, the power circuit was implemented using bidirectional IGBT switches to achieve high speed switching operation (10 kHz) and to minimize on state conduction misfortunes with less THD. The duty cycle of the bidirectional switches are controlled by the space vector PWM controller. Finally the performance of the controllers is compared with regards to THD.																	1868-5137	1868-5145															10.1007/s12652-020-02178-6		JUL 2020											
J								Radical and Stroke-Enhanced Chinese Word Embeddings Based on Neural Networks	NEURAL PROCESSING LETTERS										Word embeddings; Internal structure; Neural networks		The internal structural information of words has proven to be very effective for learning Chinese word embeddings. However, most previous attempts made a single form extraction of internal feature to learn representations, ignoring the comprehensive combination of such information. And they focused only on explicit feature of internal structures, even though these structures still have the implicit semantics of words. In this paper, we propose Radical and Stroke-enhanced Word Embeddings (RSWE), a novel method based on neural networks for learning Chinese word embeddings with joint guidance from semantic and morphological internal information. RSWE enables an embedding model to learn simultaneously from (1) implicit semantic information that is exploited from the radicals, and (2) stroke n-grams information that can be explicitly obtained from Chinese words. In the learning process, RSWE uses stroke n-grams to capture the local structural feature of words, and integrates the implicit information exploited from radicals to enhance the semantic of embeddings. Through this combination procedure, semantics of Chinese words are effectively transferred into the learned embeddings. We evaluate the effectiveness of RSWE on word similarity computation, word analogy reasoning, performance over dimensions, performance over learning corpus size, and named entity recognition tasks, the experimental results show that our model outperforms existing state-of-the-art approaches.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1109	1121		10.1007/s11063-020-10289-6		JUL 2020											
J								A hybrid two-stage financial stock forecasting algorithm based on clustering and ensemble learning	APPLIED INTELLIGENCE										Clustering; Ensemble learning; Stock price forecasting	EFFICIENT MARKET HYPOTHESIS; PREDICTION; INDEX	This paper investigates the problem of the stock closing price forecasting for the stock market. Based on existing two-stage fusion models in the literature, two new prediction models based on clustering have been proposed, where k-means clustering method is adopted to cluster several common technical indicators. In addition, ensemble learning has also been applied to improve the prediction accuracy. Finally, a hybrid prediction model, which combines both the k-means clustering and ensemble learning, has been proposed. The experimental results on a number of Chinese stocks demonstrate that the hybrid prediction model obtains the best predicting accuracy of the stock price. The k-means clustering on the stock technical indicators can further enhance the prediction accuracy of the ensemble learning.																	0924-669X	1573-7497				NOV	2020	50	11					3852	3867		10.1007/s10489-020-01766-5		JUL 2020											
J								A deceptive detection model based on topic, sentiment, and sentence structure information	APPLIED INTELLIGENCE										Deceptive detection; Topic model; Multiple-classifier; Feature extraction		Deceptive reviews on Web are a common phenomenon and how to detect them has a very important impact on products, services, and even business policies. In order to filter out deceptive reviews more accurately, a new model called Sentence Joint Topic Sentiment Model (SJTSM) is presented in this paper, which incorporates the sentence structure of reviews and the sentiment label information of words based on Latent Dirichlet Allocation (LDA) model to extract the review features. The proposed model employs Gibbs algorithm to estimate the maximum likelihood parameters and takes the vector of topic-sentiment distribution as the review features. Then a voting system of multiple-classifier, which takes the extracted review feature vector as its input is designed to realize the classification of deceptive review detection. The comparative experiments on different public datasets with other existing methods based on LDA model show that the new classifying system based on SJTSM model can achieve more satisfying classification results on deceptive review detection.																	0924-669X	1573-7497				NOV	2020	50	11					3868	3881		10.1007/s10489-020-01779-0		JUL 2020											
J								Using prior knowledge in the inference of gene association networks	APPLIED INTELLIGENCE										Gene-gene association networks; Ontology; Semantic similarity measure; Information fusion; Microarray data analysis	IDENTIFICATION; ANNOTATION	Traditional computational techniques are recently being improved with the use of prior biological knowledge from open-access repositories in the area of gene expression data analysis. In this work, we propose the use of prior knowledge as heuristic in an inference method of gene-gene associations from gene expression profiles. In this paper, we use Gene Ontology, which is an open-access ontology where genes are annotated using their biological functionality, as a source of prior knowledge together with a gene pairwise Gene-Ontology-based measure. The performance of our proposal has been compared to other benchmark methods for the inference of gene networks, outperforming in some cases and obtaining similar and competitive results in others, but with the advantage of providing simple and interpretable models, which is a desired feature for the Artificial Intelligence Health related models as stated by the European Union.																	0924-669X	1573-7497				NOV	2020	50	11					3882	3893		10.1007/s10489-020-01705-4		JUL 2020											
J								Bounded Evaluation: Querying Big Data with Bounded Resources	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Bounded evaluation; resource-bounded query processing; effective syntax; access schema; boundedness		This work aims to reduce queries on big data to computations on small data, and hence make querying big data possible under bounded resources. A query Q is boundedly evaluable when posed on any big dataset D, there exists a fraction D-Q of D such that Q(D) = Q(D-Q), and the cost of identifying D-Q is independent of the size of D. It has been shown that with an auxiliary structure known as access schema, many queries in relational algebra (RA) are boundedly evaluable under the set semantics of RA. This paper extends the theory of bounded evaluation to RA(aggr), i.e., RA extended with aggregation, under the bag semantics. (1) We extend access schema to bag access schema, to help us identify D-Q for RA(aggr) queries Q. (2) While it is undecidable to determine whether an RA(aggr) query is boundedly evaluable under a bag access schema, we identify special cases that are decidable and practical. (3) In addition, we develop an effective syntax for bounded RA(aggr), queries, i.e., a core subclass of boundedly evaluable RA(aggr) queries without sacrificing their expressive power. (4) Based on the effective syntax, we provide efficient algorithms to check the bounded evaluability of RA(aggr) queries and to generate query plans for bounded RA(aggr) queries. (5) As proof of concept, we extend PostgreSQL to support bounded evaluation. We experimentally verify that the extended system improves performance by orders of magnitude.																	1476-8186	1751-8520				AUG	2020	17	4					502	526		10.1007/s11633-020-1236-1		JUL 2020											
J								Integrated Extremal Control and Explicit Guidance for Quadcopters	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Guidance; Optimal control; Trajectory; Unmanned aerial vehicle	VEHICLE	The research study aims to create a framework for autonomous control technology for unmanned aerial vehicles with real-time target-relative guidance capabilities, which leverages onboard decision-making to provide targeting and re-targeting solutions. Thus, this paper aims to develop extremal control and guidance functions in the context of the optimal control problem and their integration for applications. Solving the optimal control problem leads to a constant motor thrust case and trivial and nontrivial cases for the variable motor thrust case. As illustrative examples, two quadcopter maneuvers use integrated extremal control and explicit guidance. The first maneuver is the quadcopter taking off to the desired altitude using maximum and then intermediate thrust. The second maneuver has the quadcopter traveling to a waypoint over an agricultural field. The DJI Onboard Software Development Kit provides a method to implement the proposed integration of extremal control and explicit guidance onboard a Raspberry Pi connected to the DJI M100 quadcopter. Simulated and experimental flight tests demonstrate that the integration of extremal control and explicit guidance allows the DJI M100 to reach the desired locations and velocities for both maneuvers.																	0921-0296	1573-0409															10.1007/s10846-020-01211-2		JUL 2020											
J								Computed Tomography Reconstruction with Uncertain View Angles by Iteratively Updated Model Discrepancy	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Computed Tomography; Uncertain view angles; Model error; Model discrepancy	INVERSE PROBLEMS; IMAGE-RECONSTRUCTION; MARGINALIZATION; IMPERFECT; REDUCTION; ERRORS	We propose a new model and a corresponding iterative algorithm for Computed Tomography (CT) when the view angles are uncertain. The uncertainty is described by an additive model discrepancy term which is included in the data fidelity term of a total variation regularized variational model. We approximate the model discrepancy with a Gaussian distribution. Our iterative algorithm alternates between updating the CT reconstruction and parameters of the model discrepancy. By assuming that the uncertainties in the view angles are independent we achieve a covariance matrix structure that we can take advantage of in a stochastic primal dual method to greatly reduce the computational work compared to classical primal dual methods. Using simulations with 2D problems we demonstrate that our method is able to reduce the reconstruction error and improve the visual quality, compared to methods that ignore the uncertainties in the angles.																	0924-9907	1573-7683															10.1007/s10851-020-00972-7		JUL 2020											
J								Reasoning in Description Logic Ontologies for Privacy Management	KUNSTLICHE INTELLIGENZ												This work is initially motivated by a privacy scenario in which the confidential information about persons or its properties formulated in description logic (DL) ontologies should be kept hidden. We investigate procedures to detect whether this confidential information can be disclosed in a certain situation by using DL formalisms. If it is the case that this information can be deduced from the ontologies, which implies certain privacy policies are not fulfilled, then one needs to consider methods to repair these ontologies in a minimal way such that the modified ontologies complies with the policies. However, privacy compliance itself is not enough if a possible attacker can also obtain relevant information from other sources, which together with the modified ontologies might violate the privacy policy. This article provides a summary of studies and results from Adrian Nuradiansyah's Ph.D. dissertation that are corresponding to the addressed problem above with a special emphasis on the investigations on the worst-case complexities of those problems as well as the complexity of the procedures and algorithms solving the problems.																	0933-1875	1610-1987				SEP	2020	34	3			SI		411	415		10.1007/s13218-020-00681-8		JUL 2020											
J								A novel empirical correlation for waterflooding performance prediction in stratified reservoirs using artificial intelligence	NEURAL COMPUTING & APPLICATIONS										Empirical correlation; Regression; Artificial neural network; Adaptive neuro-fuzzy inference system; Waterflooding; Performance prediction	NEURAL-NETWORK; SYSTEMS	Water has been used as an injected fluid for decades to improve oil recovery, commonly known as waterflooding. Simulating this process is very expensive, especially for the post-water breakthrough analysis in stratified oil reservoirs. The existing correlations do not predict waterflooding performance in heterogeneous reservoirs accurately. Most of the methods do not account for pattern flooding and consider piston-like displacement with non-communicative layers. In this study, a model has been developed using artificial neural networks (ANNs) for predicting the recovery performance of a layered reservoir undergoing a five-spot-pattern waterflood. In addition to the ANN model, a mathematical equation is presented based on ANN to predict the oil recovery in pattern waterflooding with and without crossflow between the layers for different rock wettabilities. A novel parameter-wettability indicator (WI)-has also been introduced that can be used to quantify the rock's wettability based only on the relative permeability curves. The results showed that the introduction of the new term (WI) significantly decreased the simulation runs in comparison with existing relative permeability models. ANN approach was compared with non-linear regression (NLR) and adaptive neuro-fuzzy inference system (ANFIS). The ANN model outperformed NRL and ANFIS in terms of least mean absolute percentage error (MAPE) and highest coefficient of determination (R-2). The new correlation was tested with an unseen data set, two different real field cases, an analytical model, and a semi-analytical model. The training and testing data show good match and accuracy withR(2)of 0.9973 and 0.997, respectively. MAPE of the predicted recovery efficiency using a blind data set was around 7%. The developed correlation can be a useful tool for a quick estimate of the waterflood oil recovery before a large simulation model is built and ran.																	0941-0643	1433-3058															10.1007/s00521-020-05158-1		JUL 2020											
J								Time-series forecasting of Bitcoin prices using high-dimensional features: a machine learning approach	NEURAL COMPUTING & APPLICATIONS										Time-series forecasting; Deep learning; Machine learning; Blockchain	ATTENTION	Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh-ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.																	0941-0643	1433-3058															10.1007/s00521-020-05129-6		JUL 2020											
J								Probabilistic collaborative representation on Grassmann manifold for image set classification	NEURAL COMPUTING & APPLICATIONS										Grassmann manifolds; Probabilistic collaborative representation; Image set classification; Sparse coding	SPARSE REPRESENTATION; RECOGNITION; EIGENFACES; FUSION	For image-set based classification, sparse coding and collaborative representation have gained a lot of attention due to their robustness and effectiveness. However, most existing methods focus on collaborative representation in Euclidean space. It still remains a research gap to handle this problem from Geometry-Aware perspective and interpret the mechanism of collaborative representation on nonlinear manifold. In this paper, we propose a novel method named probabilistic collaborative representation on Grassmann manifold for image set classification, which is interpreted from a probabilistic viewpoint. Specifically, we regard each image set as a point on Grassmann manifold inspired by its non-Euclidean geometry and then perform collaborative representation on the space of symmetric matrices, which enables us to explain the internal mechanism of classification and derive a closed form solution. Moreover, classification criterion is designed to further improve the performance of the proposed method. Experimental results on four databases (i.e. Honda/UCSD, YaleB, Youtube Celebrities and ETH-80) for face recognition task and object recognition task demonstrate the robustness and effectiveness of our proposed method.																	0941-0643	1433-3058															10.1007/s00521-020-05089-x		JUL 2020											
J								An FMEA-based TOPSIS approach under single valued neutrosophic sets for maritime risk evaluation: the case of ship navigation safety	SOFT COMPUTING										Risk assessment; Neutrosophic sets; FMEA; TOPSIS; Maritime industry; Ship navigation	GROUP DECISION-MAKING; FAILURE MODE; COLLISION-AVOIDANCE; FUZZY TOPSIS; PROBABILISTIC RISK; AHP METHODS; SELECTION; SYSTEM; TRANSPORTATION; PRIORITIZATION	As in terrestrial facilities, safety is one of the most important issue in ships. Vessels navigating in many parts of the world face many different, tough and dangerous navigational risks. In this context, twenty-three fundamental risks which are frequently encountered in ship navigation were considered in this study and examined by an FMEA-based TOPSIS approach under single valued neutrosophic sets. Because of the lack of data in the literature, the opinions of the experts (Masters) who have many years of experience in the sector were taken. As a result of the study, extreme weather conditions, injury of crew, loss of input of sensory equipment (depth, gyro, speed etc.), struck by ropes, exposure to high speed machineries under high pressures, loss of maneuverability are very important among these risks. Considering these risks, corrective-preventive action plans and managerial implications for ship navigation have been presented. Consequently, the results of this study have an important warning and solution recommendation regarding ship navigation risks.																	1432-7643	1433-7479															10.1007/s00500-020-05108-y		JUL 2020											
J								An approach for optimizing multi-objective problems using hybrid genetic algorithms	SOFT COMPUTING										Genetic algorithms; Particle swarm optimization; Hybrid genetic algorithm; Multi-objective optimization	OPTIMIZATION; SELECTION; TIME	Optimization problems can be found in many aspects of our lives. An optimization problem can be approached as searching problem where an algorithm is proposed to search for the value of one or more variables that minimizes or maximizes an optimization function depending on an optimization goal. Multi-objective optimization problems are also abundant in many aspects of our lives with various applications in different fields in applied science. To solve such problems, evolutionary algorithms have been utilized including genetic algorithms that can achieve decent search space exploration. Things became even harder for multi-objective optimization problems when the algorithm attempts to optimize more than one objective function. In this paper, we propose a hybrid genetic algorithm (HGA) that utilizes a genetic algorithm (GA) to perform a global search supported by the particle swarm optimization algorithm (PSO) to perform a local search. The proposed HGA achieved the concept of rehabilitation of rejected individuals. The proposed HGA was supported by a modified selection mechanism based on the K-means clustering algorithm that succeeded to restrict the selection process to promising solutions only and assured a balanced distribution of both the selected to survive and selected for rehabilitation individuals. The proposed algorithm was tested against 4 benchmark multi-objective optimization functions where it succeeded to achieve maximum balance between search space exploration and search space exploitation. The algorithm also succeeded in improving the HGA's overall performance by limiting the average number of iterations until convergence.																	1432-7643	1433-7479															10.1007/s00500-020-05149-3		JUL 2020											
J								Online estimation of state of health for the airborne Li-ion battery using adaptive DEKF-based fuzzy inference system	SOFT COMPUTING										Li-ion battery; State of health; Extended Kalman filter; Adaptive dual extended Kalman filter-based fuzzy inference system	PREDICTION; CHARGE; PACK	The quick and accurate estimation of the state of health (SOH) of Li-ion battery is a technical difficulty in battery management system research. For the low accuracy of Li-ion battery SOH estimation under complex stress conditions, an estimation method of SOH for Li-ion battery using the adaptive dual extended Kalman filter-based fuzzy inference system (ADEKF-FIS) is proposed. First, Li-ion battery SOH is online estimated by dual extended Kalman filter. Then the Sage-Husa adaptive algorithm and the fuzzy controller are used to correct the state noise covariance and the observed noise covariance, respectively. The algorithm is flat on the state variance and the noise variance. The recursive estimation of the square root ensures the symmetry and nonnegative nature of the state and noise variance. In the end, this paper performing the dynamic stress test condition experiment for confirmation. Experimental results show that, compared with the EKF algorithm, ADEKF-FIS algorithm can obtain state of charge estimation with higher accuracy, which further improves the prediction accuracy of SOH and makes this algorithm have higher accuracy and better convergence.																	1432-7643	1433-7479															10.1007/s00500-020-05101-5		JUL 2020											
J								Three-way decisions with decision-theoretic rough sets based on Pythagorean fuzzy covering	SOFT COMPUTING										Decision-theoretic rough sets; Pythagorean fuzzy beta-covering; Pythagorean fuzzy beta-covering decision-theoretic rough sets; Three-way decisions	SYSTEMS; MODELS; TOPSIS	Three-way decisions (3WDs) with decision-theoretic rough sets (DTRSs) are a new method for solving problems of risky decision. In DTRSs, it is crucial to the determination of the loss function. Pythagorean fuzzy (PF) sets are a more powerful mathematical tool than intuitionistic fuzzy sets for dealing with uncertainty and inaccuracy. Although the researchers have introduced Pythagorean fuzzy numbers (PFNs) into the loss function, study on the combination of 3WDs and PF covering is still blank. In view of this, we develop 3WDs with DTRS based on PF covering. Firstly, by using the concepts of PF beta-covering and PF beta-neighborhood, we construct a Pythagorean fuzzy beta-covering decision-theoretic rough set (PFCDTRS) model as per Bayesian decision procedure. Then, some of interesting properties of the expected loss related to the model are investigated. Secondly, based on the membership degree and non-membership degree of PFNs, four methods to address the expected loss expressed in the form of PFNs are established and the corresponding 3WDs are also derived. Finally, we develop a corresponding algorithm for deriving 3WDs with PFCDTRS, and then, an example is provided to validate the feasibility and reliability of 3WDs with PFCDTRS. Compared the proposed methods with the existing methods, we conclude that the proposed Methods 3 and 4 are superior to the existing methods.																	1432-7643	1433-7479															10.1007/s00500-070-05102-4		JUL 2020											
J								Weighted linear programming discriminant analysis for high-dimensional binary classification	STATISTICAL ANALYSIS AND DATA MINING										alternating direction method of multipliers; binary classification; feature screening; linear discriminant analysis; linear programming; high dimensional data	ADAPTIVE LASSO; MODELS	Linear discriminant analysis (LDA) is widely used for various binary classification problems. In contrast to the LDA that estimates the precision matrix Omega and the mean difference vector delta in the classification rule separately, the linear programming discriminant (LPD) rule estimates the product Omega delta directly through a constrained l(1) minimization. The LPD rule has very good classification performance on many high-dimensional binary classification problems. However, to estimate beta* = Omega delta, the LPD rule uses equal weights for all the elements of beta* in the constrained l(1) minimization. It may not deliver the optimal estimate of beta*, and therefore the estimated discriminant direction can be suboptimal. In order to obtain better estimates of beta* and the discriminant direction, we can heavily penalize beta(j) in the constrained l(1) minimization if we suspect the jth feature is useless for the classification while moderately penalize beta(j) if we suspect the jth feature is useful. In this paper, based on the LPD rule and some popular feature screening methods, we propose a new weighted linear programming discriminant (WLPD) rule for the high-dimensional binary classification problem. The screening statistics used in the marginal two-sample t-test screening, Kolmogorov-Smirnov filter, and the maximum marginal likelihood screening will be used to construct appropriate weights for different elements of beta* flexibly. Besides the linear programming algorithm, we develop a new alternating direction method of multipliers algorithm to solve the high-dimensional constrained l(1) minimization problem efficiently. Our numerical studies show that our proposed WLPD rule can outperform LPD and serve as an effective binary classification tool.																	1932-1864	1932-1872				OCT	2020	13	5					437	450		10.1002/sam.11473		JUL 2020											
J								MEvo: a framework for effective macro sets evolution	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Automated planning; domain model reformulation; sets evolution	OPERATORS; GENERATION	In Automated Planning, generating macro-operators (macros) is a well-known reformulation approach that is used to speed-up the planning process. Nowadays, given the number of existing techniques, a large number of macros is already available or can be easily extracted. Most of the macro generation techniques aim for using the same set of generated macros for each planner and every problem instance in a given domain. Although they provide 'general improvement', the effect of macros might vary a lot for different planners. Moreover, the impact of macros on structurally different problem instances than the training ones can be potentially very detrimental. Evidently, this limits the exploitation of macros in real-world planning applications, where the structure of problem instances can often change as well as the exploited planning engine can change from time to time. In this paper, we propose the Macro sets Evolution (MEvo) approach. MEvo has been designed for overcoming the aforementioned issues in order to improve the performance of domain-independent planners by dynamically selecting promising macros - taken from a given pool - while solving continuous streams of problem instances. Our extensive empirical study, involving more than 1,000 planning problem instances and 8 state-of-the-art planning engines, demonstrates effectiveness and efficiency of MEvo.																	0952-813X	1362-3079				JUL 3	2020	32	4					685	703		10.1080/0952813X.2019.1672796													
J								Data-Driven Optimal Assistance Control of a Lower Limb Exoskeleton for Hemiplegic Patients	FRONTIERS IN NEUROROBOTICS										walking assistance control; reinforcement learning; leader-follower multi-agent system; lower limb exoskeleton; hemiplegic patients; actor-critic neural network	ANKLE-FOOT ORTHOSIS; HUMAN-ROBOT INTERACTION; MULTIAGENT SYSTEMS; TRACKING CONTROL; GAIT; ALGORITHM	More recently, lower limb exoskeletons (LLE) have gained considerable interests in strength augmentation, rehabilitation, and walking assistance scenarios. For walking assistance, the LLE is expected to control the affected leg to track the unaffected leg's motion naturally. A critical issue in this scenario is that the exoskeleton system needs to deal with unpredictable disturbance from the patient, and the controller has the ability to adapt to different wearers. To this end, a novel data-driven optimal control (DDOC) strategy is proposed to adapt different hemiplegic patients with unpredictable disturbances. The interaction relation between two lower limbs of LLE and the leg of patient's unaffected side are modeled in the context of leader-follower framework. Then, the walking assistance control problem is transformed into an optimal control problem. A policy iteration (PI) algorithm is utilized to obtain the optimal controller. To improve the online adaptation to different patients, an actor-critic neural network (AC/NN) structure of the reinforcement learning (RL) is employed to learn the optimal controller on the basis of PI algorithm. Finally, experiments both on a simulation environment and a real LLE system are conducted to verify the effectiveness of the proposed walking assistance control method.																	1662-5218					JUL 3	2020	14								37	10.3389/fnbot.2020.00037													
J								A Dynamic multi-sensor data fusion approach based on evidence theory and WOWA operator	APPLIED INTELLIGENCE										Evidence theory; Multi-sensor data fusion; Weighted ordered weighted averaging operator; Dynamic; Preference	DEMPSTER-SHAFER THEORY; DIVERGENCE MEASURE; COMBINATION; FRAMEWORK	Multi-sensor data fusion (MSDF) problems have attracted widespread attention recently. However, it is still an open issue about how to make the fusion process effectively even if the collected data conflict due to several unpredictable reasons. Moreover, most existing approaches mainly concentrated on the distinction of evidence sources, which cannot well consider the feature of individual belief degree and the associated preference of decision-makers. To address such an issue, a dynamic MSDF method based on evidence theory and weighted ordered weighted averaging (WOWA) operator is proposed in this study. A numerical example is analyzed to demonstrate its whole calculation procedure. Two simulation experiments, composed of a motor rotor fault diagnosis and an insulator string target recognition application, are also mentioned to illustrate its effectiveness and applied value. The results show that the proposed methodology can enhance the fusion accuracy in the constrained scenarios with the consideration of preference relation.																	0924-669X	1573-7497				NOV	2020	50	11					3837	3851		10.1007/s10489-020-01739-8		JUL 2020											
J								Bone age estimation from carpal radiography images using deep learning	EXPERT SYSTEMS										carpal radiographs; CNN; deep learning; image processing; ossification centre segmentation	SEGMENTATION	Bone age estimation has been used in medicine to verify whether the bone structure development degree of a person corresponds to their chronological age. Such estimate is useful for prognosis about the development of children and adolescents, as well as for the diagnosis of endocrinological diseases. This work proposes a fully automated methodology for bone age estimation from carpal radiography images. The methodology comprises two steps, the preprocessing of the image and the classification using a convolutional neural network. The system accuracy for different types of preprocessing is evaluated. We compare the accuracy achieved using the full radiography image as input for the neural network and using only parts of the image corresponding to the Phalangeal region, the Epiphyseal region, and the concatenation of these parts with a crop around the wrist. Digital image processing techniques are employed to segment these regions. Experiments are performed using radiography images from the California University Database. The impact of using different pre-trained neural networks for transfer learning is evaluated.																	0266-4720	1468-0394															10.1111/exsy.12584		JUL 2020											
J								Data Access With Horn Ontologies: Where Description Logics Meet Existential Rules	KUNSTLICHE INTELLIGENZ										Data access; Ontology-mediated query answering; Existential rules; Horn description logics	DATA EXCHANGE; QUERY	Two main families of ontology languages are considered in the context of data access, namely Horn description logics and existential rules. In this paper, we review the semantic relationships between these families in the light of the ontology-mediated query answering problem. To this end, we rely on the standard translation of description logics in first-order logic and on the notion of semantic emulation. We focus on description logics and classes of existential rules for which the conjunctive query answering problem has polynomial data complexity.																	0933-1875	1610-1987															10.1007/s13218-020-00678-3		JUL 2020											
J								Attention-based convolutional neural network for Bangla sentiment analysis	AI & SOCIETY										Bangla sentiment analysis (BSA); Polarity prediction; Attention; CNN; NLP; Deep learning (DL)		With the accelerated evolution of the internet in the form of web-sites, social networks, microblogs, and online portals, a large number of reviews, opinions, recommendations, ratings, and feedback are generated by writers or users. This user-generated sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, and individuals. While this content is meant to be useful, a bulk of this writer-generated content requires using text mining techniques and sentiment analysis. However, there are several challenges facing the sentiment analysis and evaluation process. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting suitable sentiment polarity specifically in the Bangla language. Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text. This paper presents how the attention mechanism could be incorporated effectively and efficiently in analyzing the Bangla sentiment or opinion.																	0951-5666	1435-5655															10.1007/s00146-020-01011-0		JUL 2020											
J								Analysis of real-time heartbeat monitoring using wearable device Internet of Things system in sports environment	COMPUTATIONAL INTELLIGENCE										ECG; Internet of Things (IoT); probabilistic neural network (PNN); radio-basis function network (RBFN); wearable devices	HEALTH-CARE-SYSTEM; ANALYTICS	Technology in the field of Internet of Things (IoT) with smartphones is enormously growing at a rapid pace for assisting people with their health conditions. Wearable sensors can provide real time data in the field of sports for monitoring the heartbeat of the athletes which can assist in physical activities. Heartbeat rate of the players change during different positions while playing sports and heartbeat monitoring will help the players to know the health condition thus improving the health of an individual. In this research, we propose a new method of wearable sensor device for collecting real time data of athletes using IoT-based system for monitoring electrocardiogram (ECG) patterns along with acceleration of body using smart phone and classify the obtained data using Radial-basis Function Network and Levenberg-Marquardt with Probabilistic Neural Network. The experimental setup of the proposed model performed using 100 persons and effectively classifies the data and predicts the heart rate with the precision of validation and training sample being 73.58% and 73.45 respectively. Thus the proposed IoT-based prediction system can be used to monitor health data of the athletes in real time as an alternate solution for monitoring physical health of the athletes.																	0824-7935	1467-8640															10.1111/coin.12337		JUL 2020											
J								Credible seed identification for large-scale structural network alignment	DATA MINING AND KNOWLEDGE DISCOVERY										Network alignment; Seed identification; Edge consistency; Mapping credibility	PROTEIN-INTERACTION NETWORKS; GLOBAL ALIGNMENT; ALGORITHM; REGISTRATION	Structural network alignment utilizes the topological structure information to find correspondences between nodes of two networks. Researchers have proposed a line of useful algorithms which usually require a prior mapping of seeds acting as landmark points to align the rest nodes. Several seed-free algorithms are developed to solve the cold-start problem. However, existing approaches suffer high computational cost and low reliability, limiting their applications to large-scale network alignment. Moreover, there is a lack of useful metrics to quantify the credibility of seed mappings. To address these issues, we propose a credible seed identification framework and develop a metric to assess the reliability of a mapping. To tackle the cold-start problem, we employ graph embedding techniques to represent nodes by structural feature vectors in a latent space. We then leverage point set registration algorithms to match nodes algebraically and obtain an initial mapping of nodes. Besides, we propose a heuristic algorithm to improve the credibility of the initial mapping by filtering out mismatched node pairs. To tackle the computational problem in large-scale network alignment, we propose a divide-and-conquer scheme to divide large networks into smaller ones and then match them individually. It significantly improves the recall of mapping results. Finally, we conduct extensive experiments to evaluate the effectiveness and efficiency of our new approach. The results illustrate that the proposed method outperforms the state-of-the-art approaches in terms of both effectiveness and efficiency.																	1384-5810	1573-756X				NOV	2020	34	6					1744	1776		10.1007/s10618-020-00699-4		JUL 2020											
J								Causative label flip attack detection with data complexity measures	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Adversarial learning; Causative attack detection; Label flip attack; Data complexity	FEATURE-SELECTION; TRAINING DATA; CLASSIFIERS; SECURITY	A causative attack which manipulates training samples to mislead learning is a common attack scenario. Current countermeasures reduce the influence of the attack to a classifier with the loss of generalization ability. Therefore, the collected samples should be analyzed carefully. Most countermeasures of current causative attack focus on data sanitization and robust classifier design. To our best knowledge, there is no work to determinate whether a given dataset is contaminated by a causative attack. In this study, we formulate a causative attack detection as a 2-class classification problem in which a sample represents a dataset quantified by data complexity measures, which describe the geometrical characteristics of data. As geometrical natures of a dataset are changed by a causative attack, we believe data complexity measures provide useful information for causative attack detection. Furthermore, a two-step secure classification model is proposed to demonstrate how the proposed causative attack detection improves the robustness of learning. Either a robust or traditional learning method is used according to the existence of causative attack. Experimental results illustrate that data complexity measures separate untainted datasets from attacked ones clearly, and confirm the promising performance of the proposed methods in terms of accuracy and robustness. The results consistently suggest that data complexity measures provide the crucial information to detect causative attack, and are useful to increase the robustness of learning.																	1868-8071	1868-808X															10.1007/s13042-020-01159-7		JUL 2020											
J								Attribute reduction in formal decision contexts and its application to finite topological spaces	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Attribute reduction; Formal decision contexts; Subbases; Topological spaces	ROUGH SET; KNOWLEDGE REDUCTION; RULE ACQUISITION; CONCEPT LATTICE; DISCERNIBILITY MATRIX; CONNECTIONS; GRANULATION; DISCOVERY; 3-WAY	Attribute reduction in formal decision contexts has become one of the key issues in the research and development of formal concept analysis (FCA) and its applications. As far as we know, however, most of the existing reduction methods for formal decision contexts are time-consuming especially for the large-scale data. This paper investigates the attribute reduction method for large-scale formal decision contexts. The computation of a discernibility matrix is an important step in the development of the corresponding reduction method. A simple and powerful method to efficiently calculate the discernibility matrix of formal decision contexts is first presented. In addition, a heuristic algorithm for searching the optimal reduct is then proposed. Thirdly, as an application of the new results, we discuss the problem of finding the minimal subbases of finite topological spaces. It has shown that the method of attribute reduction in formal decision contexts can be used to obtain all the minimal subbases of a finite topological space. Furthermore, we present an algorithm for computing the minimal subbase of a topological space, based on the attribute reduction method proposed in this paper. Finally, two groups of experiments are carried out on some large-scale data sets to verify the effectiveness of the proposed algorithms.																	1868-8071	1868-808X															10.1007/s13042-020-01147-x		JUL 2020											
J								A survey on instance segmentation: state of the art	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Instance segmentation; Object detection; Convolutional neural networks; Deep learning	DEEP; IMAGE; RECOGNITION; NETWORKS; MODELS	Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.																	2192-6611	2192-662X				SEP	2020	9	3					171	189		10.1007/s13735-020-00195-x		JUL 2020											
J								A stream position performance analysis model based on DDoS attack detection for cluster-based routing in VANET	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VANET; DDoS attack; Cluster; Routing; Detection	INTRUSION-DETECTION; SECURITY CHALLENGES; MITIGATION; MECHANISM	The strength of Vehicular Ad hoc Networks (VANETs) and the rapid deployment capability, can be used in many situations where the network should be arranged in a short time and there is a need to collect sensitive information. We consider cluster-based attack detection in data compilation wherever the neighbor nodes give the important information to the cluster head. Moreover, evidence is obtainable in the cluster head may possibly be accumulated by some vehicular nodes and executes numerous responsibilities such as decision making about delivering information. The existence of malicious nodes threatens determination making through transmitting malevolent information, which is not appropriate to the VANET categorized data and might send a substantial number of packets to the vehicles or Road Side Unit (RSU). To overcome this issue, we have proposed a Stream Position Performance Analysis (SPPA) approach. This approach monitors the position of any field station in sending the information to perform a Distributed Denial of Service (DDoS) attack. The method computes various factors like Conflict field, Conflict data and Attack signature sample rate (CCA). Using all these factors, the method identifies the trustworthiness of the packet and includes it in decision making. The proposed approach increases the performance of a Distributed Denial of Service (DDoS) attack detection in a VANET environment.																	1868-5137	1868-5145															10.1007/s12652-020-02279-2		JUL 2020											
J								An approach to identify solutions of interest from multi and many-objective optimization problems	NEURAL COMPUTING & APPLICATIONS										Multiobjective optimization; Solutions of interest; Preferences articulation	INCOMPLETE WEIGHT INFORMATION; ATTRIBUTE DECISION-MAKING	The result of a multiobjective or a many-objective optimization problem is a large set of non-dominated solutions. Once the Pareto Front (or a good approximation of it) has been found, then providing the decision maker with a smaller set of "interesting solutions" is a key step. Here, the focus is on how to select such a set of solutions of interest which, in contrast to previous approaches that relied on geometrical features, is carried out considering the decision maker's preferences. The proposeda posterioriapproach consists in assigning an interval of potential scores to every solution, where such scores depend on the decision maker's preferences. The solutions are then compared and filtered according to their corresponding intervals, using a recently proposed possibility degree formula. Three examples, with two, three and many objectives are used to show the benefits of the proposal.																	0941-0643	1433-3058															10.1007/s00521-020-05140-x		JUL 2020											
J								Bipartite finite time synchronization for general Caputo fractional-order impulsive coupled networks	NEURAL COMPUTING & APPLICATIONS										Impulsive control; Bipartite finite time synchronization; Fractional-order signed network	DELAYED NEURAL-NETWORKS; GLOBAL SYNCHRONIZATION; SYSTEMS; STABILITY; CONSENSUS; GRAPH	This briefly investigates the bipartite finite time synchronization in fractional-order impulsive signed networks (FISNs), where there exist antagonistic communication links between neighboring nodes. Firstly, some new judgmentconditions about finite time stability of FISNs aregiven on generalized Caputo fractional-order derivative. Secondly, by using Dirac function and the grapy theory, FISNs are transformed to fractional-order impulsive differential equations, which shows that the impulsive effect on signed networks is dependent on the order of the addressed networks and impulsive function. Thirdly, to provide novel criteria for bipartite finite time synchronization of FISNs by using a low-dimensional linear matrix inequality, pinning impulsive control strategy is designed. Fourthly, an upper bound on setting time for synchronization is obtained, and the influence of order on setting time is analyzed. Finally, numerical simulation is provided for illustration.																	0941-0643	1433-3058															10.1007/s00521-020-05135-8		JUL 2020											
J								Fixing state change inconsistency with self regulating particle swarm optimization	SOFT COMPUTING										Consistency; State diagram; Artificial intelligence; Self regulating particle swarm optimization	MODEL; CONSISTENCY	Software has made a profound influence in all walks of life. Developing quality software is a major challenge, and the consistency and completeness of the design has a prime role in the development of quality software. Many a times, the process of consistency checking in industries is manual. Artificial intelligence techniques can replace many of these manual efforts to make the development of software easier and cost-effective. Software developers use state diagrams to represent the dynamic behavior in the design stage. We propose a novel application of self regulating particle swarm optimization (SRPSO) algorithm to ensure consistency of state diagrams during the design phase of software development. Inconsistency management is modeled as an optimization problem. In this work, we detect two types of state change inconsistency, incompatible behavior inconsistency and disconnected model inconsistency. A fitness function is defined to detect inconsistency. We make use of the SRPSO algorithm to resolve inconsistency. Detecting inconsistencies in the early stages of software development enables phase containment of errors and prevents errors from being propagated to the code. The proposed approach generates consistent and complete state diagrams leading to accurate code generation, meeting time deadlines, reducing cost of production and easy system maintenance.																	1432-7643	1433-7479															10.1007/s00500-020-05124-y		JUL 2020											
J								Robust controller design for systems with probabilistic uncertain parameters using multi-objective genetic programming	SOFT COMPUTING										Genetic programming; Robust controller; Pareto; Monte Carlo simulation; Uncertainty	DIFFERENTIAL EVOLUTION ALGORITHM; REACTIVE POWER DISPATCH; OPTIMIZATION	Optimal design of controllers without considering uncertainty in the plant dynamics can induce feedback instabilities and lead to obtaining infeasible controllers in practice. This paper presents a multi-objective evolutionary algorithm integrated with Monte Carlo simulations (MCS) to perform the optimal stochastic design of robust controllers for uncertain time-delay systems. Each potential optimal solution represents a controller in the form of a transfer function with the optimal numerator and denominator polynomials. The proposed methodology uses genetic programming to evolve robust controllers. Using GP enables the algorithm to optimize the structure of the controller and tune the parameters in a holistic approach. The proposed methodology employs MCS to apply robust optimization and uses a new adaptive operator to balance exploration and exploitation in the search space. The performance of controllers is assessed in the closed-loop system with respect to three objective functions as (1) minimization of mean integral time absolute error (ITAE), (2) minimization of the standard deviation of ITAE and (3) minimization of maximum control effort. The new methodology is applied to the first-order and second-order systems with dead time. We evaluate the performance of obtained robust controllers with respect to the upper and lower bounds of step responses and control variables. We also perform a post-processing analysis considering load disturbance and external noise; we illustrate the robustness of the designed controllers by cumulative distribution functions of objective functions for different uncertainty levels. We show how the proposed methodology outperforms the state-of-the-art methods in the literature.																	1432-7643	1433-7479															10.1007/s00500-020-05133-x		JUL 2020											
J								Develop and implement unsupervised learning through hybrid FFPA clustering in large-scale datasets	SOFT COMPUTING										Unsupervised clustering; Classification; Large datasets; Dimensionality reduction; Feature extraction	FEATURE-SELECTION; CLASSIFICATION; FEATURES; MACHINE	Clustering is extensively realistic and considered in computer vision that follows unsupervised learning principles. In this, the performance of a clustering process mainly depends on the feature representation. Generally, the clustering process may have an error rate, and this affects the feature representation. To avoid this, unsupervised Learning (USL) provides an alternative path to obtain the best clusters from the dataset through the optimum features. In this proposed work, the clustering process is done by using a hybrid firefly-based flower pollination algorithm (FFPA). So this clustering process removes the complexity in USL. The better performance is obtained by identifying an essential group from the data to avoid the problems obtained by a USL. In the standard USL, the PCA method is used to minimize a large amount of original data. Here, the features are extracted based on RGB features and Zernike moments, and this is given to the input for the hybrid cluster. Finally, the hybrid convolutional neural network classifier, along with the datasets that are trained from a similar patch manifold, is used to create a label for several datasets. The performance of this proposed method portrays that the local features are effectively clustered from the various datasets by an unsupervised FFPA algorithm. In this work, the unsupervised clustering process with a hybrid classification for the object recognition application is used. In this work, the average accuracy, error rate, and run time are nearly 95%, 73%, and 26 s, respectively.																	1432-7643	1433-7479															10.1007/s00500-020-05140-y		JUL 2020											
J								Cycling environment investigation and optimization of urban central road in Qingdao	COMPUTATIONAL INTELLIGENCE										cycling environment optimization; IPA analysis; urban central road in Qingdao		In the context of the current construction of smart cities, cycling activities have grown rapidly, and the optimization of road cycling environments has become increasingly important. In order to explore the role of the optimization of cycling environment in improving the health of cyclists and shaping the overall sustainable environment of the city, this article takes the coastal road of Qingdao as the research object to consult the relevant literature and current environment investigation. The IPA analysis method is used to analyze and evaluate the safety, continuity, convenience, interest, and comfort of the cycling environment, to plan and construct the bicycle lanes to ensure the personal safety of cyclists, and to improve the original road facilities. In order to make suggestions for the optimization of the cycling environment in Qingdao, five aspects of developing the characteristic cycling route of coastal tourism are put forward. That is to ensure smooth cycling activities, to add bicycles parking facilities to maintain a good appearance of the city, to increase bicycles related services facilities, to make rational use of the vertical slope of the road, and to make use of the advantages of natural landscape resources.																	0824-7935	1467-8640															10.1111/coin.12363		JUL 2020											
J								Extending emotional lexicon for improving the classification accuracy of Chinese film reviews	CONNECTION SCIENCE										Emotion analysis; emotional lexicon; film reviews; distance of word; point mutual information; semantic similarity	SENTIMENT ANALYSIS; AUTOMATIC CONSTRUCTION; SEMANTIC SIMILARITY	It is challenging to build domain-specific emotional lexicon for film reviews, due to its unique characteristics, such as massive data, endless new login words, and others. To improve the accuracy of film reviews classification, this article proposes a method for extending emotional lexicon based on word distance and point mutual information. First, using the improved K-means++ algorithm to cluster and select seed words with obvious emotional tendencies. Next, the Distance of Word and Point Mutual Information (DW-PMI) algorithm is presented to determine the emotional polarity of emotional words in the domain of film reviews. Four types of vocabulary, including degree adverb, negation, emoticon and emotion dictionary in the film reviews domain are added to the basic emotion dictionary to extend the film reviews emotional lexicon. From the experimental results, the expanded emotional lexicon of the Chinese film reviews can improve the accuracy and preciseness of the film reviews emotion analysis.																	0954-0091	1360-0494															10.1080/09540091.2020.1782839		JUL 2020											
J								Use of acoustic emission in combination with machine learning: monitoring of gas-liquid mixing in stirred tanks	JOURNAL OF INTELLIGENT MANUFACTURING										Acoustic emission; Gas-liquid mixing; Stirred tank; Machine learning	VECTOR MACHINE; EXPERT SYSTEM; HOLD-UP; MODEL; PRECIPITATION; DISPERSION; VESSELS	Operations involving gas-liquid agitated vessels are common in the biochemical and chemical industry; ensuring good contact between the two phases is essential to process performance. In this work, a methodology to compute acoustic emission data, recorded using a piezoelectric sensor, to evaluate the gas-liquid mixing regime within gas-liquid and gas-solid-liquid mixtures was developed. The system was a 3L stirred tank equipped with a Rushton Turbine and a ring sparger. Whilst moving up through the vessel, gas bubbles collapse, break or coalesce generating sound waves transmitted through the wall to the acoustic transmitter. The system was operated in different flow regimes (non-gassed condition, loaded and complete dispersion) achieved by varying impeller speed and gas flow rate, with the objective to feed machine learning algorithms with the acoustic spectrum to univocally identify the different conditions. The developed method allowed to successfully recognise the operating regime with an accuracy higher than 90% both in absence and presence of suspended particles.																	0956-5515	1572-8145															10.1007/s10845-020-01611-z		JUL 2020											
J								Gaussian bandwidth selection for manifold learning and classification	DATA MINING AND KNOWLEDGE DISCOVERY										Dimensionality reduction; Kernel methods; Diffusion maps; Classification	INTRINSIC DIMENSIONALITY ESTIMATOR; DISCRIMINATION; EARTHQUAKES; EXPLOSIONS; ALGORITHM; KERNELS	Kernel methods play a critical role in many machine learning algorithms. They are useful in manifold learning, classification, clustering and other data analysis tasks. Setting the kernel's scale parameter, also referred to as the kernel's bandwidth, highly affects the performance of the task in hand. We propose to set a scale parameter that is tailored to one of two types of tasks: classification and manifold learning. For manifold learning, we seek a scale which is best at capturing the manifold's intrinsic dimension. For classification, we propose three methods for estimating the scale, which optimize the classification results in different senses. The proposed frameworks are simulated on artificial and on real datasets. The results show a high correlation between optimal classification rates and the estimated scales. Finally, we demonstrate the approach on a seismic event classification task.																	1384-5810	1573-756X				NOV	2020	34	6					1676	1712		10.1007/s10618-020-00692-x		JUL 2020											
J								Introducing time series snippets: a new primitive for summarizing long time series	DATA MINING AND KNOWLEDGE DISCOVERY										Time series; Summarization; Motifs; Sampling; Diversification		The first question a data analyst asks when confronting a new dataset is often, "Show me some representative/typical data." Answering this question is simple in many domains, with random samples or aggregate statistics of some kind. Surprisingly, it is difficult for large time series datasets. The major difficulty is not time or space complexity, but defining what it means to berepresentativedata for this data type. In this work, we show that the obvious candidate definitions: motifs, shapelets, cluster centers, random samples etc., are all poor choices. We introducetime series snippets, a novel representation of typical time series subsequences. Informally, time series snippets can be seen as the answer to the following question. If a user, which could be a human or a higher-level algorithm, only has resources (including human time) to inspectksubsequences of a long time series, whichksubsequences should be chosen? Beyond their utility for visualizing and summarizing massive time series collections, we show that time series snippets have utility for high-level comparison of large time series collections.																	1384-5810	1573-756X				NOV	2020	34	6					1713	1743		10.1007/s10618-020-00702-y		JUL 2020											
J								A taxonomy of human-machine collaboration: capturing automation and technical autonomy	AI & SOCIETY										Human-machine collaboration; Taxonomy; Automation; Autonomy	RESPONSIBILITY; ALGORITHMS; FAIRNESS; AGENCY	Due to the ongoing advancements in technology, socio-technical collaboration has become increasingly prevalent. This poses challenges in terms of governance and accountability, as well as issues in various other fields. Therefore, it is crucial to familiarize decision-makers and researchers with the core of human-machine collaboration. This study introduces a taxonomy that enables identification of the very nature of human-machine interaction. A literature review has revealed that automation and technical autonomy are main parameters for describing and understanding such interaction. Both aspects must be carefully evaluated, as their increase has potentially far-reaching consequences. Hence, these two concepts comprise the taxonomy's axes. Five levels of automation and five levels of technical autonomy are introduced below, based on the assumption that both automation and autonomy are gradual. The levels of automation were developed from existing approaches; those of autonomy were carefully derived from a review of the literature. The taxonomy's use is also explained, as are its limitations and avenues for further research.																	0951-5666	1435-5655															10.1007/s00146-020-01004-z		JUL 2020											
J								Optimal subset selection for causal inference using machine learning ensembles and particle swarm optimization	COMPLEX & INTELLIGENT SYSTEMS										Analytics; Evolutionary computing; Swarm optimization; Machine learning	OF-THE-ART; PERMUTATION FLOWSHOP; RANDOM FORESTS; BALANCE; ALGORITHMS; MAKESPAN	We suggest and evaluate a method for optimal construction of synthetic treatment and control samples for the purpose of drawing causal inference. The balance optimization subset selection problem, which formulates minimization of aggregate imbalance in covariate distributions to reduce bias in data, is a new area of study in operations research. We investigate a novel metric, cross-validated area under the receiver operating characteristic curve (AUC) as a measure of balance between treatment and control groups. The proposed approach provides direct and automatic balancing of covariate distributions. In addition, the AUC-based approach is able to detect subtler distributional differences than existing measures, such as simple empirical mean/variance and count-based metrics. Thus, optimizing AUCs achieves a greater balance than the existing methods. Using 5 widely used real data sets and 7 synthetic data sets, we show that optimization of samples using existing methods (Chi-square, mean variance differences, Kolmogorov-Smirnov, and Mahalanobis) results in samples containing imbalance that is detectable using machine learning ensembles. We minimize covariate imbalance by minimizing the absolute value of the distance of the maximum cross-validated AUC on M folds from 0.50, using evolutionary optimization. We demonstrate that particle swarm optimization (PSO) outperforms modified cuckoo swarm (MCS) for a gradient-free, non-linear noisy cost function. To compute AUCs, we use supervised binary classification approaches from the machine learning and credit scoring literature. Using superscore ensembles adds to the classifier-based two-sample testing literature. If the mean cross-validated AUC based on machine learning is 0.50, the two groups are indistinguishable and suitable for causal inference.																	2199-4536	2198-6053															10.1007/s40747-020-00169-w		JUL 2020											
J								Learning the spatiotemporal variability in longitudinal shape data sets	INTERNATIONAL JOURNAL OF COMPUTER VISION										Longitudinal data; Statistical shape analysis; Large deformation diffeomorphic metric mapping; Medical imaging; Disease progression modeling	RIEMANNIAN-MANIFOLDS; TRAJECTORIES; MORPHOMETRY; FRAMEWORK; VERSION; MODEL	In this paper, we propose a generative statistical model to learn the spatiotemporal variability in longitudinal shape data sets, which contain repeated observations of a set of objects or individuals over time. From all the short-term sequences of individual data, the method estimates a long-term normative scenario of shape changes and a tubular coordinate system around this trajectory. Each individual data sequence is therefore (i) mapped onto a specific portion of the trajectory accounting for differences in pace of progression across individuals, and (ii) shifted in the shape space to account for intrinsic shape differences across individuals that are independent of the progression of the observed process. The parameters of the model are estimated using a stochastic approximation of the expectation-maximization algorithm. The proposed approach is validated on a simulated data set, illustrated on the analysis of facial expression in video sequences, and applied to the modeling of the progressive atrophy of the hippocampus in Alzheimer's disease patients. These experiments show that one can use the method to reconstruct data at the precision of the noise, to highlight significant factors that may modulate the progression, and to simulate entirely synthetic longitudinal data sets reproducing the variability of the observed process.																	0920-5691	1573-1405				DEC	2020	128	12					2873	2896		10.1007/s11263-020-01343-w		JUL 2020											
J								Incorporating Side Information by Adaptive Convolution	INTERNATIONAL JOURNAL OF COMPUTER VISION										Convolutional neural network (CNN); Deep learning; Crowd counting		Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in deep learning based counting systems. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned "filter manifold" sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters and achieves similar or better than state-of-the-art performance on crowd counting task. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information. We also perform ablation experiments, mainly for crowd counting, to study the helpfulness of the side information, and the effect of the placement of the adaptive convolutional layers in order to get insight about ACNNs.																	0920-5691	1573-1405				DEC	2020	128	12					2897	2918		10.1007/s11263-020-01345-8		JUL 2020											
J								Fuzzy rule based ontology reasoning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ontology; Fuzzy rule; Reasoner; Description logic; Inference; Image retrieval	IMAGE; RECOGNITION; EXTRACTION	Constructing a domain specific ontology is tedious commitment. Through reasoner the created ontology can be evaluated. The reasoner checks the consistency of the classes and evaluates the occurrence of any obvious errors. The ontology entities are expected to be consistent with intuitions. The ontology instance has to be minimal redundant. Thus to maintain the high quality ontology, the designed ontology should be meaningful, correct, minimally redundant, and richly axiomatised. The main objective of this paper is to create a logical entailment between the domain specific ontology and entities using fuzzy rule.																	1868-5137	1868-5145															10.1007/s12652-020-02163-z		JUL 2020											
J								Double random forest	MACHINE LEARNING										Classification; Ensemble; Random forest; Bootstrap; Decision tree	CLASSIFICATION TREES; ALGORITHMS; ENSEMBLES	Random forest (RF) is one of the most popular parallel ensemble methods, using decision trees as classifiers. One of the hyper-parameters to choose from for RF fitting is the nodesize, which determines the individual tree size. In this paper, we begin with the observation that for many data sets (34 out of 58), the best RF prediction accuracy is achieved when the trees are grown fully by minimizing the nodesize parameter. This observation leads to the idea that prediction accuracy could be further improved if we find a way to generate even bigger trees than the ones with a minimum nodesize. In other words, the largest tree created with the minimum nodesize parameter may not be sufficiently large for the best performance of RF. To produce bigger trees than those by RF, we propose a new classification ensemble method called double random forest (DRF). The new method uses bootstrap on each node during the tree creation process, instead of just bootstrapping once on the root node as in RF. This method, in turn, provides an ensemble of more diverse trees, allowing for more accurate predictions. Finally, for data where RF does not produce trees of sufficient size, we have successfully demonstrated that DRF provides more accurate predictions than RF.																	0885-6125	1573-0565				AUG	2020	109	8					1569	1586		10.1007/s10994-020-05889-1		JUL 2020											
J								Development of intelligent healthcare system based on ambulatory blood pressure measuring device	NEURAL COMPUTING & APPLICATIONS										Intelligent healthcare system; Ambulatory blood pressure monitoring; TOAST classification; Signal similarity calculation	EUROPEAN-SOCIETY; RECOMMENDATIONS; RECOGNITION; SENSOR; MARKET; STATE	Currently, the market size of blood pressure monitors both in domestic and overseas is gradually increasing due to the increase in hypertension patients resulting from aging population. In addition, the necessity of developing systems and devices for the healthcare of hypertension patients is also increasing. Moreover, the determination of health normality in respect to the management of hypertension patients is possible, but it is essentially important to incorporate preventive healthcare. Thus, further studies on deep learning-based prediction technology using previous data are needed. This paper proposes the development of an intelligent healthcare management system that can help to manage the health of hypertensive patients. The system includes a wrist-worn ambulatory blood pressure monitoring device that can analyze the normality of measured blood pressures. The performance evaluation results of the proposed system verified the reliability of data acquisition as compared with the existing equipment as well as the efficiency of the intelligent healthcare system.																	0941-0643	1433-3058															10.1007/s00521-020-05114-z		JUL 2020											
J								Interval-valued probabilistic hesitant fuzzy set-based framework for group decision-making with unknown weight information	NEURAL COMPUTING & APPLICATIONS										Bayesian approximation; COPRAS method; Deviation method; Evidence theory; Hesitant fuzzy set; Maclaurin symmetric mean	PROPORTIONAL ASSESSMENT METHOD; SELECTION; MODEL; RISK; RANKING; PROJECT; ART	This paper aims at presenting a new decision framework under an interval-valued probabilistic hesitant fuzzy set (IVPHFS) context with fully unknown weight information. At first, the weights of the attributes are determined by using the interval-valued probabilistic hesitant deviation method. Later, the DMs' weights are determined by using a recently proposed evidence theory-based Bayesian approximation method under the IVPHFS context. The preferences are aggregated by using a newly extended generalized Maclaurin symmetric mean operator under the IVPHFS context. Further, the alternatives are prioritized by using an interval-valued probabilistic hesitant complex proportional assessment method. From the proposed framework, the following significances are inferred; for example,it uses a generalized preference structure that provides ease and flexibility to the decision-makers(DMs)during preference elicitation; weights are calculated systematically to mitigate inaccuracies and subjective randomness; interrelationship among attributes are effectively captured; and alternatives are prioritized from different angles by properly considering the nature of the attributes. Finally, the applicability of the framework is validated by using green supplier selection for a leading bakery company, and from the comparison, it is observed that the framework isuseful, practical and systematicfor rational decision-makingand robust and consistentfrom sensitivity analysis of weights and Spearman correlation of rank values, respectively.																	0941-0643	1433-3058															10.1007/s00521-020-05160-7		JUL 2020											
J								Metric learning-guidedknearest neighbor multilabel classifier	NEURAL COMPUTING & APPLICATIONS										Metric learning; kNearest neighbor; Multilabel classification	LABEL; LIBRARY	Multilabel classification deals with the problem where each instance belongs to multiple labels simultaneously. The algorithm based on large margin loss withknearest neighbor constraints (LM-kNN) is one of the most prominent multilabel classification algorithms. However, due to the use of square hinge loss, LM-kNN needs to iteratively solve a constrained quadratic programming at a high computational cost. To address this issue, we propose a novel metric learning-guidedknearest neighbor approach (MLG-kNN) for multilabel classification. Specifically, we first transform the original instance into the label space by least square regression. Then, we learn a metric matrix in the label space, which makes the predictions of an instance in the learned metric space close to its true class values while far away from others. Since our MLG-kNN can be formulated as an unconstrained strictly (geodesically) convex optimization problem and yield a closed-form solution, the computational complexity is reduced. An analysis of generalization error bound indicates that our MLG-kNN converges to the optimal solutions. Experimental results verify that the proposed approach is more effective than the existing ones for multilabel classification across many benchmark datasets.																	0941-0643	1433-3058															10.1007/s00521-020-05134-9		JUL 2020											
J								Design of stochastic numerical solver for the solution of singular three-point second-order boundary value problems	NEURAL COMPUTING & APPLICATIONS										Singular systems; Genetic algorithms; Hybrid approach; Artificial neural networks; Interior point algorithm	INTERIOR-POINT METHOD; INSPIRED HEURISTICS; DYNAMICS; ALGORITHM; SYSTEMS; HYBRID; FLOW	In this paper, a novel meta-heuristic computing solver is presented for solving the singular three-point second-order boundary value problems using artificial neural networks (ANNs) optimized by the combined strength of global and local search ability of genetic algorithms (GAs) and interior point algorithm (IPA), i.e., ANN-GA-IPA. The inspiration for presenting this numerical work comes from the intention of introducing a consistent framework that combines the effective features of neural networks optimized with the contexts of soft computing to handle with such challenging systems. Three numerical variants of singular second-order system have been taken to examine the proficiency, robustness, and stability of the designed approach. The comparison of the proposed results of ANN-GA-IPA from available exact solutions shows the good agreement with 5 to 7 decimal places of the accuracy which established worth of the methodology through performance analyses on a single and multiple executions.																	0941-0643	1433-3058															10.1007/s00521-020-05143-8		JUL 2020											
J								A distributed WND-LSTM model on MapReduce for short-term traffic flow prediction	NEURAL COMPUTING & APPLICATIONS										Big data analytics; Deep learning; LSTM neural network; Traffic flow prediction; MapReduce	DEEP BELIEF NETWORKS; SVR	Building data-driven intelligent transportation is a significant task for establishing data-centric smart cities, and exceptionally efficient and accurate traffic flow prediction (TFP) is a crucial technology in constructing intelligent transportation systems (ITSs). To address the computation and storage problems of processing traffic flow big data with the centralized model on a traditional mining platform, we propose a distributed long short-term memory weighted model combined with a time window and normal distribution based on a MapReduce parallel processing framework in this paper, named as WND-LSTM. More specifically, under the Hadoop distributed computing platform, a distributed modeling framework of forecasting traffic flow on MapReduce is developed to solve the existing issues of storage and calculation in handling large-scale traffic flow data with the stand-alone learning model. Moreover, a distributed WND-LSTM model is presented on the MapReduce-based distributed modeling framework to enhance the accuracy, efficiency, and scalability of short-term TFP. Finally, we forecast the traffic flow on the Sanlihe East Road of Beijing in China using the proposed WND-LSTM model with the real-world taxi trajectory big data. In particular, the extensively experimental results from a case study demonstrate that the MAPE value of WND-LSTM is 88.48%, 65.79%, 70.46%, 68.21%, 66.95%, 68.43%, and 70.41% lower than that of the autoregressive integrated moving average (ARIMA), logistical regression (LR), support vector regression (SVR),k-nearest neighbor (KNN), stacked autoencoders (SAEs), gated recurrent unit (GRU), and long short-term memory (LSTM), respectively, and achieves 71.25% accuracy improvement on average.																	0941-0643	1433-3058															10.1007/s00521-020-05076-2		JUL 2020											
J								Bifurcation Mechanisation of a Fractional-Order Neural Network with Unequal Delays	NEURAL PROCESSING LETTERS										Unequal delays; Stability; Fractional order; Hopf bifurcation; Neural networks	HOPF-BIFURCATION; QUASI-SYNCHRONIZATION; TIME-DELAY; MODEL; STABILITY; SYSTEM; DISCRETE	The theme of bifurcation for a class of fractional-order neural networks (FONNs) with unique delay has been incalculably elucidated. It exhibits that multiple delays are capable of increasing the complicacy of realistic FONNs, but this has been insufficiently probed into. This paper attempts to conduct a research on the stability and bifurcation for a FONN with two unequal delays. By intercalating one delay and taking remnant delay as a bifurcation parameter, the incongruent critical values of diverse delays-induced bifurcations are exactly gained. Eventually, confirmation experiments are offered to endorse the procured theory.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1171	1187		10.1007/s11063-020-10293-w		JUL 2020											
J								Feature-Based Learning in Drug Prescription System for Medical Clinics	NEURAL PROCESSING LETTERS										Feature vector; Similarity ratio; Word embedding; Adverse network model; Personalised drug prescription	INTERACTION EXTRACTION; INFORMATION	Rapid increases in data volume and variety pose a challenge to safe drug prescription for health professionals like doctors and dentists. This is addressed by our study, which presents innovative approaches in mining data from drug corpus and extracting feature vectors to combine this knowledge with individual patient medical profiles. Within our three-tiered framework-the prediction layer, the knowledge layer and the presentation layer-we describe multiple approaches in computing similarity ratios from the feature vectors, illustrated with an example of applying the framework in a typical medical clinic. Experimental evaluation shows that the word embedding model performs better than the adverse network model, with aFscore of 0.75. TheFscore is a common metrics used for evaluating the performance of classification algorithms. Similarity to a drug the patient is allergic to or is taking are important considerations for the suitability of a drug for prescription. Hence, such an approach, when integrated within the clinical work-flow, will reduce prescription errors thereby increasing patient health outcomes.																	1370-4621	1573-773X															10.1007/s11063-020-10296-7		JUL 2020											
J								An Improved Mean Imputation Clustering Algorithm for Incomplete Data	NEURAL PROCESSING LETTERS										Incomplete data; Mean imputation; K-means; Validity index	K-MEANS	There are many incomplete data sets in all fields of scientific studies due to random noise, data lost, limitations of data acquisition, data misunderstanding etc. Most of the clustering algorithms can not be used for incomplete data sets directly because objects with missing values need to be preprocessed. For this reason, this paper presents an improved mean imputation clustering algorithm for incomplete data based on partition clustering algorithm. In the proposed method, we divide the universe into two sets: the set of objects with non-missing values and the set of objects with missing values. Firstly, the objects with non-missing values are clustered by traditional clustering algorithm. For each object with missing values, we use the mean attribute's value of each cluster to fill the missing attribute's value based on the cluster results of the objects with non-missing values, respectively. Perturbation analysis of cluster centroid is applied to search the optimal imputation. The experimental clustering results on some UCI data sets are evaluated by several validity indexes, which proves the effectiveness of the proposed algorithm.																	1370-4621	1573-773X															10.1007/s11063-020-10298-5		JUL 2020											
J								Stability analysis for a class of fractional-order nonlinear systems with time-varying delays	SOFT COMPUTING										Nonlinear systems; Fractional-order systems; Time-varying delay; Stability analysis		This paper presents the stability analysis problem of fractional-order nonlinear systems with time-varying delay. After formulating the problem and selecting the nonlinear model as the system under study, stability analysis and expression of the sufficient conditions for fractional-order nonlinear systems with time-varying delay are obtained using two different methods. In these methods, sufficient conditions for stability of fractional-order nonlinear systems are found in the form of satisfying some inequalities based on norms of nonlinear functions in the system and in terms of linear matrix inequality according fractional-order and nonlinear functions. In each case, despite the presence of time-varying delay, the system stability is ensured by meeting the stability sufficient conditions in terms of an inequality of functions and system parameters. Finally, numerical examples are given to determine the effectiveness of the proposed theorem.																	1432-7643	1433-7479				NOV	2020	24	22					17445	17453		10.1007/s00500-020-05118-w		JUL 2020											
J								Spatial segregative behaviors in robotic swarms using differential potentials	SWARM INTELLIGENCE										Swarm robotics; Segregation; Artificial potential fields; Differential adhesion; Control	AGGREGATION; FLOCKING; TAXONOMY	Segregative behaviors, in which individuals with common characteristics are placed together and set apart from other groups, are commonly found in nature. In swarm robotics, these behaviors can be important in different tasks that require a heterogeneous group of robots to be divided in homogeneous sets according to their physical (sensors, actuators) or logical (algorithms) capabilities. In this paper, we propose a controller that can spatially segregate a swarm of robots in two specific ways: clusters and concentric rings. By segregation, we mean that the swarm is partitioned in groups, with similar robots belonging to a same group, and these groups are spatially separated from each other. We achieve this by adapting and extending the differential potential concept, an abstraction of the mechanism by which cells achieve segregation. We present stability analysis and perform simulated experiments in 2D and 3D spaces in order to show the robustness of the proposed controller. Experiments with a limited number of real robots are also presented as a proof of concept. Results show that our approach allows a swarm of heterogeneous robots to segregate in a stable, compact, and collision-free fashion.																	1935-3812	1935-3820															10.1007/s11721-020-00184-0		JUL 2020											
J								Sustainable achievement efficiency of transport energy consumption based on indicator analysis	COMPUTATIONAL INTELLIGENCE										DEA; energy achievement efficiency; indicator; transport energy consumption	CARBON-DIOXIDE EMISSIONS; DECOMPOSITION ANALYSIS; LMDI DECOMPOSITION; CHINA; URBAN; PANEL; SECTOR; ROAD	Energy issue is one of the crucial problems in the corresponding research of sustainability because it is strongly related to the environmental dimension and economic dimension. For evaluating the transport sustainability level for sustainability for regions, the concept of sustainable achievement efficiency in transport energy consumption is initially suggested in this article. And then, on the quantitative analysis to calculate the transport energy achievement efficiency of the regions, the indicators that can represent the achievement of transport energy consumption are convincingly found out by indicator theory. Next, concentration is focused on the transport related indicators and proper indicators are picked up from the candidate indicators, which were the affecting factors to this issue. After that, using the selected indicators, we introduce the method of data envelopment analysis to do quantitative analysis, which helps to get the sustainable achievement efficiency of transport energy among cities all over the world. The analysis result shows the efficient regions and the inefficient regions respectively. Furthermore, the detailed efficiency value of each region is also laid out clearly. Consequently, the achievement efficiency in transport sector can be discussed quantitatively and further policy implications can be suggested for improvement for the inefficient regions to reach high level sustainability.																	0824-7935	1467-8640															10.1111/coin.12366		JUL 2020											
J								Monitoring of industrial processes using robust global-local preserving projection	JOURNAL OF CHEMOMETRICS										fault detection; global-local preserving projection; outlier identification; process monitoring; robust methods	PRINCIPAL COMPONENT ANALYSIS; OUTLIERS	Usual multivariate statistical analysis (MSA) techniques are highly sensitive to outliers because they are based on the least squares fitting or the empirical mean and covariance of the data. Data-driven process monitoring methods based on usual MSA techniques may be unreliable when outliers are present in the training data. To overcome this deficiency, a robust global-local preserving projection (RGLPP) method is proposed for dimension reduction of the high-dimensional data contaminated by outliers, and then it is applied to the robust monitoring of industrial processes. Despite the presence of outliers, RGLPP yields robust projection directions that can preserve both global and local structures of the high-dimensional data. Moreover, based on the robust projection directions of RGLPP, a method is developed for identifying outliers in a multivariate data set. An RGLPP-based robust process monitoring method is also developed to achieve high-performance monitoring when the training data of industrial processes contain outliers. The effectiveness and advantages of the proposed method are illustrated using an industrial case study.																	0886-9383	1099-128X				SEP	2020	34	9							e3278	10.1002/cem.3278		JUL 2020											
J								Artificial Intelligence in Purchasing: Facilitating Mechanism Design-based Negotiations	APPLIED ARTIFICIAL INTELLIGENCE											GAME-THEORY; SOURCING LEVERS; PROCUREMENT; IMPACT; SUPPLIERS; NETWORKS; CUSTOMER; MODEL	Negotiations are central to reach consensus between supply chain partners while, simultaneously, meeting internal cost and quality targets. Purchasing prices can be improved by inducing competition in the supply base. In this context, the application of mechanism design theory in negotiations gained enhanced attention. While such approaches can result in high cost reductions, mechanism design-based negotiations are very complex. The paper aims at answering the question whether artificial intelligence (AI) can facilitate the execution of mechanism design-based negotiations. To this end, a World Cafe has been conducted at an automotive original equipment manufacturer. A group of 20 experts from the fields of purchasing and AI discussed the potentials of AI for the purchasing function. The results indicate that the application of AI can indeed facilitate the execution of mechanism design-based negotiations and help overcoming bounded rationality problems. Even more, AI might be a game changer for the purchasing function.																	0883-9514	1087-6545				JUL 2	2020	34	8					618	642		10.1080/08839514.2020.1749337													
J								A Hybrid Ant Colony Optimization and Simulated Annealing Algorithm for Multi-Objective Scheduling of Cellular Manufacturing Systems	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Ant Colony Optimization; Cellular Manufacturing Systems; Production Planning; Simulated Annealing; Uncertain Market Demands	GENETIC ALGORITHM; DESIGN; DEMAND; CELLS; MACHINES; MODEL; RISK	During the last 2 decades, there have been many manufacturing companies in various industries that used the advantages of cellular manufacturing layouts. However, determining the best schedule for cellular layouts considering uncertain product demands is a big concern for scientists. In this research, a multi-objective decision-making model is proposed in the process of dynamic cellular production planning where the market demands are uncertain. In this regard, a non-linear mixed integer programming model is developed. The complexity of the model is high to consider the model as NP-hard. Therefore, a hybrid Ant colony Optimization and Simulated Annealing Algorithms are proposed to solve the problem. Then, the Taguchi method is used to estimate appropriate sets of parameters of the proposed algorithm. The results demonstrated that the proposed algorithm can generate the best part-routes of products in terms of time, cost and load variance in a reasonable time. The algorithm is then used for a cellular production plant which is the producer of heavy vehicles parts.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					1	40		10.4018/IJAMC.2020070101													
J								Quantum-Behaved Bat Algorithm for Solving the Economic Load Dispatch Problem Considering a Valve-Point Effect	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Economic Load Dispatch; Non-Convex; Optimization; Quantum-Behaved Bat Algorithm; Valve-Point Effect	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; SEARCH; NEWTON; UNITS	Quantum computing-inspired metaheuristic algorithms have emerged as a powerful computational tool to solve nonlinear optimization problems. In this paper, a quantum-behaved bat algorithm (QBA) is implemented to solve a nonlinear economic load dispatch (ELD) problem. The objective of ELD is to find an optimal combination of power generating units in order to minimize total fuel cost of the system, while satisfying all other constraints. To make the system more applicable to the real-world problem, a valve-point effect is considered here with the ELD problem. QBA is applied in 3-unit, 10-unit, and 40-unit power generation systems for different load demands. The obtained result is then presented and compared with some well-known methods from the literature such as different versions of evolutionary programming (EP) and particle swarm optimization (PSO), genetic algorithm (GA), differential evolution (DE), simulated annealing (SA) and hybrid ABC_PSO. The comparison of results shows that QBA performs better than the above-mentioned methods in terms of solution quality, convergence characteristics and computational efficiency. Thus, QBA proves to be an effective and a robust technique to solve such nonlinear optimization problem.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					41	57		10.4018/IJAMC.2020070102													
J								Optimized Path Planning for Electric Vehicle Routing and Charging Station Navigation Systems	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Ant Colony Optimization; Ant System Algorithm; Driving Assistance; Electric Vehicles Routing; Smart Multi-Agent System; Swarm Intelligence; Vehicle Routing Optimization; Vehicle Routing System		With the increase in the number of electric vehicles (EV) on the street in the last years, the drivers of EVs are suffering from the problem of guiding themselves toward the nearest charging stations for recharging their batteries or finding the shortest routes toward their destinations. Although, the electric vehicle planning problem (EPP) is designed to achieve several transactions such as battery energy restrictions and the challenge of finding the nearest charging stations to the position of the electric vehicle. In this work, a new distributed system for electric vehicle routing is based on a novel driving strategy using a distributed Ant system algorithm (AS). The distributed architecture minimizes the total travelling path for the EV to attain the destination by proposing a set of the nearest charging stations that can be visited for recharging during his travels. Simulation result proved that our prototype is able to prepare optimal solutions within a reasonable time and forwarding EVs toward the nearest charging stations during their trips.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					58	78		10.4018/IJAMC.2020070103													
J								A Two Stage Method for the Multiple Traveling Salesman Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Ant Colony Optimization (ACO); Distribution; Marketing; Multiple Traveling Salesman Problem (m-TSP); p-Median Problem (PMP); Swarm Intelligence; Transport	ANT COLONY OPTIMIZATION; SEARCH ALGORITHM; BRANCH	The variation of the traveling salesman problem (TSP) with multiple salesmen (m-TSP) has been studied for many years resulting in diverse solution methods, both exact and heuristic. However, the high difficulty level on finding optimal (or acceptable) solutions has opposed the many efforts of doing so. The proposed method regards a two stage procedure which implies a modified version of the p-Median Problem (PMP) alongside the TSP, making a partition of the nodes into subsets that will be assigned to each salesman, solving it with Branch & Cut (B&C), in the first stage. This is followed by the routing, applying an Ant Colony Optimization (ACO) metaheuristic algorithm to solve a TSP for each subset of nodes. A case study was reviewed, detailing the positioning of five vehicles in strategic places in the Mexican Republic.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					79	91		10.4018/IJAMC.2020070104													
J								A Nearest Neighbor Algorithm to Optimize Recycling Networks	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Capacitated Vehicle Routing Problem; Cumulative Capacitated Vehicle Routing Problem with Multiple Trips with Time Windows; Green Logistics; Heuristics; Optimization; Recycling; Simulation; Vehicle Routing	VEHICLE-ROUTING PROBLEM	This article analyses the processes of collecting used non-returnable packaging to improve the recycling of material. A collection system is proposed by applying a profitable visit algorithm based on the widely-known Nearest Neighbor Algorithm. A comparative study is performed to achieve a higher volume of recycled material while decreasing the cost of collection. The proposed algorithm shows a much better performance than the reference. The developed algorithm was evaluated in a real scenario and confirmed by a simulation runs. Savings in material sourcing processes can be achieved in real operations. The proposed algorithm shows some advantage.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					92	107		10.4018/IJAMC.2020070105													
J								A Metaheuristic Approach and Mathematical Programming for Packing Objects in a Rectangular Container	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Approximate Packing; Circle Packing; Evolutionary Computation; Gurobi Solver; Heuristic; Integer Programming; Large-Scale Optimization; Monkey Algorithm	CIRCLES	The problem of packing non-congruent circles within bounded regions is considered. The aim is to maximize the number of circles placed into a rectangular container or minimize the waste. The circle is considered as a set of points that are all the same distance (not necessarily Euclidean) from a given point. An integer programming model is proposed using a dotted-board approximating the container and considering the dots as potential positions for assigning centers of the circles. The packing problem is then stated as a large scale linear 0-1 optimization problem. Binary decision variables are associated with each discrete point of the board (a dot) and with each object. Then, the same grid is used to prove a population-based metaheuristic. This metaheuristic is inspired by the monkeys' behavior. The resulting binary problem is then solved by using Gurobi Solver and Python Programming Language as Interface																	1947-8283	1947-8291				JUL-AUG	2020	11	3					108	119		10.4018/IJAMC.2020070106													
J								Comparison of Integer Linear Programming and Dynamic Programming Approaches for ATM Cash Replenishment Optimization Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Cash Replenishment Problem; Dynamic Programming; Efficient Solution; Interest Cost; Linear Programming; Loading Cost; Optimization; Replenishment Schedule	MANAGEMENT; LOGISTICS; DEMAND; MODEL	With the automated teller machine (ATM) cash replenishment problem, banks aim to reduce the number of out-of-cash ATMs and duration of out-of-cash status. On the other hand, they want to reduce the cost of cash replenishment, as well. The problem conventionally involves forecasting ATM cash withdrawals, and then cash replenishment optimization based on the forecast. The authors assume that reliable forecasts are already obtained for the amount of cash needed in ATMs. The focus of the article is cash replenishment optimization. After introducing linear programming-based solutions, the authors propose a solution based on dynamic programming. Experiments conducted on real data reveal that the proposed approach can find the optimal solution more efficiently than linear programming.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					120	132		10.4018/IJAMC.2020070107													
J								An Integer-Order Transfer Function Estimation Algorithm for Fractional-Order PID Controllers	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Fractional-Order Differentiator/Integrator; Fractional-Order PID Controller; Integer-Order Estimation; Matsuda Approach; Oustaloup Approximation	APPROXIMATION METHOD; SYSTEMS	Fractional-order systems and controllers have been extensively used in many control applications to achieve robust modeling and controlling performance. To implement these systems, curve fitting based integer-order transfer function estimation techniques namely Oustaloup and Matsuda are most widely used. However, these methods are failed to achieve the best approximation due to the limitation of the desired frequency range. Thus, this article presents a simple curve fitting based integer-order transfer function estimation method for fractional-order differentiator/integrator using frequency response. The advantage of this technique is that it is simple and can fit the entire desired frequency range. Using the approach, an approximation table for fractional-order differentiator has also been obtained which can be used directly to obtain the approximation of fractional-order systems. A simulation study on fractional systems shows that the proposed approach produced better parameter approximation for the desired frequency as compared to Oustaloup, refined Oustaloup and Matsuda techniques.																	1947-8283	1947-8291				JUL-AUG	2020	11	3					133	150		10.4018/IJAMC.2020070108													
J								Data reduction and data visualization for automatic diagnosis using gene expression and clinical data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Classification; Gene expression; Heatmap; Hot-spot map; Convolutional neural networks	BREAST-CANCER; CLASSIFICATION; SELECTION	Accurate diagnoses of specific diseases require, in general, the review of the whole medical history of a patient. Currently, even though many advances have been made for disease monitoring, domain experts are still requested to perform direct analyses in order to get a precise classification, thus implying significant efforts and costs. In this work we present a framework for automated diagnosis based on high-dimensional gene expression and clinical data. Given that high-dimensional data can be difficult to analyze and computationally expensive to process, we first perform data reduction to transform high-dimensional representations of data into a lower dimensional space, yet keeping them meaningful for our purposes. We used then different data visualization techniques to embed complex pieces of information in 2-D images, that are in turn used to perform diagnosis relying on deep learning approaches. Experimental analyses show that the proposed method achieves good performance, featuring a prediction Recall value between 91% and 99%.																	0933-3657	1873-2860				JUL	2020	107								101884	10.1016/j.artmed.2020.101884													
J								Fully convolutional attention network for biomedical image segmentation	ARTIFICIAL INTELLIGENCE IN MEDICINE										Biomedical image; Segmentation; Dilated fully convolutional network; Attention modules; Long-range and short-range distance		In this paper, we embed two types of attention modules in the dilated fully convolutional network (FCN) to solve biomedical image segmentation tasks efficiently and accurately. Different from previous work on image seg-mentation through multiscale feature fusion, we propose the fully convolutional attention network (FCANet) to aggregate contextual information at long-range and short-range distances. Specifically, we add two types of attention modules, the spatial attention module and the channel attention module, to the Res2Net network, which has a dilated strategy. The features of each location are aggregated through the spatial attention module, so that similar features promote each other in space size. At the same time, the channel attention module treats each channel of the feature map as a feature detector and emphasizes the channel dependency between any two channel maps. Finally, we weight the sum of the output features of the two types of attention modules to retain the feature information of the long-range and short-range distances, to further improve the representation of the features and make the biomedical image segmentation more accurate. In particular, we verify that the proposed attention module can seamlessly connect to any end-to-end network with minimal overhead. We perform comprehensive experiments on three public biomedical image segmentation datasets, i.e., the Chest X-ray col-lection, the Kaggle 2018 data science bowl and the Herlev dataset. The experimental results show that FCANet can improve the segmentation effect of biomedical images. The source code models are available at https:// github.com/luhongchun/FCANet																	0933-3657	1873-2860				JUL	2020	107								101899	10.1016/j.artmed.2020.101899													
J								Learning personalized ADL recognition models from few raw data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Few-shot learning; Matching networks; Activity of daily living; EHealth; Inertial measurement unit; Gated recurrent units	NEURAL-NETWORKS	Recognition of activities of daily living (ADL) is an essential component of assisted living systems based on actigraphy. This task can nowadays be performed by machine learning models which are able to automatically extract and learn relevant features but, most of time, need to be trained with large amounts of data collected on several users. In this paper, we propose an approach to learn personalized ADL recognition models from few raw data based on a specific type of neural network called matching network. The interest of this few-shot learning approach is three-fold. Firstly, people perform activities their own way and general models may average out important individual characteristics unlike personalized models that could thus achieve better performance. Secondly, gathering large quantities of annotated data from one user is time-consuming and threatens privacy in a medical context. Thirdly, matching networks are by nature weakly dependent on the classes they are trained on and can generalize easily to new activities without needing extra training, thus making them very versatile for real applications. Our results show the effectiveness of the proposed approach compared to general neural network models, even in situations with few training data.																	0933-3657	1873-2860				JUL	2020	107								101916	10.1016/j.artmed.2020.101916													
J								A shape context fully convolutional neural network for segmentation and classification of cervical nuclei in Pap smear images	ARTIFICIAL INTELLIGENCE IN MEDICINE										Liquid-based cytology; Pap smear; Fully convolutional neural network; Segmentation; Classification	LIQUID-BASED CYTOLOGY; ACCURATE SEGMENTATION; CYTOPLASM; OPTIMIZATION; DIAGNOSIS; CELLS	Pap smear is often employed as a screening test for diagnosing cervical pre-cancerous and cancerous lesions. Accurate identification of dysplastic changes amongst the cervical cells in a Pap smear image is thus essential for rapid diagnosis and prognosis. Manual pathological observations used in clinical practice require exhaustive analysis of thousands of cell nuclei in a whole slide image to visualize the dysplastic nuclear changes which make the process tedious and time-consuming. Automated nuclei segmentation and classification exist but are chal-lenging to overcome issues like nuclear intra-class variability and clustered nuclei separation. To address such challenges, we put forward an application of instance segmentation and classification framework built on an Unet architecture by adding residual blocks, densely connected blocks and a fully convolutional layer as a bottleneck between encoder-decoder blocks for Pap smear images. The number of convolutional layers in the standard Unet has been replaced by densely connected blocks to ensure feature reuse-ability property while the introduction of residual blocks in the same attempts to converge the network more rapidly. The framework provides simultaneous nuclei instance segmentation and also predicts the type of nucleus class as belonging to normal and abnormal classes from the smear images. It works by assigning pixel-wise labels to individual nuclei in a whole slide image which enables identifying multiple nuclei belonging to the same or different class as individual distinct instances. Introduction of a joint loss function in the framework overcomes some trivial cell level issues on clustered nuclei separation. To increase the robustness of the overall framework, the proposed model is preceded with a stacked auto-encoder based shape representation learning model. The proposed model outperforms two state-of-the-art deep learning models Unet and Mask_RCNN with an average Zijdenbos simi-larity index of 97 % related to segmentation along with binary classification accuracy of 98.8 %. Experiments on hospital-based datasets using liquid-based cytology and conventional pap smear methods along with benchmark Herlev datasets proved the superiority of the proposed method than Unet and Mask_RCNN models in terms of the evaluation metrics under consideration.																	0933-3657	1873-2860				JUL	2020	107								101897	10.1016/j.artmed.2020.101897													
J								Combining analysis of multi-parametric MR images into a convolutional neural network: Precise target delineation for vestibular schwannoma treatment planning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Magnetic resonance imaging; Vestibular schwannoma; gamma knife; Convolutional neural network	SEGMENTATION; PREDICTION	Manual delineation of vestibular schwannoma (VS) by magnetic resonance (MR) imaging is required for diagnosis, radiosurgery dose planning, and follow-up tumor volume measurement. A rapid and objective automatic segmentation method is required, but problems have been encountered due to the low through-plane resolution of standard VS MR scan protocols and because some patients have non-homogeneous cystic areas within their tumors. In this study, we retrospectively collected multi-parametric MR images from 516 patients with VS; these were extracted from the Gamma Knife radiosurgery planning system and consisted of T1-weighted (T1W), T2-weighted (T2W), and T1W with contrast (T1W + C) images. We developed an end-to-end deep-learning-based method via an automatic preprocessing pipeline. A two-pathway U-Net model involving two sizes of convolution kernel (i.e., 3 x 3 x 1 and 1 x 1 x 3) was used to extract the in-plane and through-plane features of the anisotropic MR images. A single-pathway model that adopted the same architecture as the two-pathway model, but used a kernel size of 3 x 3 x 3, was also developed for comparison purposes. In addition, we used multi-parametric MR images with different image contrasts as the model training input in order to effectively segment tumors with solid as well as cystic parts. The results of the automatic segmentation demonstrated that (1) the two-pathway model outperformed single-pathway model in terms of dice scores (0.90 +/- 0.05 versus 0.87 +/- 0.07); both of them having been trained using the T1W, T1W + C and T2W anisotropic MR images, (2) the optimal single-parametric two-pathway model (dice score: 0.88 +/- 0.06) was then trained using the T1W + C images, and (3) the two-pathway models trained using bi-parametric (T1W + C and T2W) and tri-parametric (T1W, T2W, and T1W + C) images outperformed the model trained using the single-parametric (T1W + C) images (dice scores: 0.89 +/- 0.05 and 0.90 +/- 0.05, respectively, larger than 0.88 +/- 0.06) because it showed improved segmentation of the non-homogeneous parts of the tumors. The proposed two-pathway U-Net model outperformed the single-pathway U-Net model when segmenting VS using anisotropic MR images. The multi-parametric models effectively improved on the defective segmentation obtained using the single-parametric models by separating the non-homogeneous tumors into their solid and cystic parts.																	0933-3657	1873-2860				JUL	2020	107								101911	10.1016/j.artmed.2020.101911													
J								CCAE: Cross-field categorical attributes embedding for cancer clinical endpoint prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Clinical endpoint prediction; Electronic health records; Categorical variables embedding	PALLIATIVE CARE	Patients with advanced cancer are burdened physically and psychologically, so there is an urgent need to pay more attention to their health-related quality of life (HRQOL). With an expected clinical endpoint prediction, over-treatment can be effectively eliminated by the means of palliative care at the right time. This paper de-velops a deep learning based approach for cancer clinical endpoint prediction based on patient's electronic health records (EHR). Due to the pervasive existence of categorical information in EHR, it brings unavoidably obstacles to the effective numerical learning algorithms. To address this issue, we propose a novel cross-field categorical attributes embedding (CCAE) model to learn a vectorized representation for cancer patients in attribute-level by orders, in which the strong semantic coupling among categorical variables are well exploited. By transforming the order-dependency modeling into a sequence learning task in an ingenious way, recurrent neural network is adopted to capture the semantic relevance among multi-order representations. Experimental results from the SEER-Medicare EHR dataset have illustrated that the proposed model can achieve competitive prediction performance compared with other baselines.																	0933-3657	1873-2860				JUL	2020	107								101915	10.1016/j.artmed.2020.101915													
J								Detection of early stages of Alzheimer's disease based on MEG activity with a randomized convolutional neural network	ARTIFICIAL INTELLIGENCE IN MEDICINE										Alzheimer 's disease detection; Deep learning; Convolutional neural network; Ensemble model; Magnetoencephalography	MILD COGNITIVE IMPAIRMENT; CLASSIFICATION; MACHINE; MAGNETOENCEPHALOGRAPHY; NOISE	The early detection of Alzheimer's disease can potentially make eventual treatments more effective. This work presents a deep learning model to detect early symptoms of Alzheimer's disease using synchronization measures obtained with magnetoencephalography. The proposed model is a novel deep learning architecture based on an ensemble of randomized blocks formed by a sequence of 2D-convolutional, batch-normalization and pooling layers. An important challenge is to avoid overfitting, as the number of features is very high (25755) compared to the number of samples (132 patients). To address this issue the model uses an ensemble of identical submodels all sharing weights, with a final stage that performs an average across sub-models. To facilitate the exploration of the feature space, each sub-model receives a random permutation of features. The features correspond to magnetic signals reflecting neural activity and are arranged in a matrix structure interpreted as a 2D image that is processed by 2D convolutional networks. The proposed detection model is a binary classifier (disease/non-disease), which compared to other deep learning architectures and classic machine learning classifiers, such as random forest and support vector machine, obtains the best classification performance results with an average F1-score of 0.92. To perform the comparison a strict validation procedure is proposed, and a thorough study of results is provided.																	0933-3657	1873-2860				JUL	2020	107								101924	10.1016/j.artmed.2020.101924													
J								Automated detection of dynamical change in EEG signals based on a new rhythm measure	ARTIFICIAL INTELLIGENCE IN MEDICINE										Change detection; EEG rhythm; Graph modeling; Time-frequency analysis	FEATURE-EXTRACTION; CLASSIFICATION; SINGLE; WAVELET; EPILEPSY	Automated detection of dynamical change in EEG signals has been a long-standing problem in a wide range of clinic applications. It is essential to extract an effective and accurate EEG rhythm indicator that can reflect the dynamical behavior of a given EEG signal. Time-frequency analysis is a promising method to achieve this end, but existing methods still have limitations in real implementation making this kind of methods still progressive until the present day. In this paper, along the line of ongoing research on time-frequency methods, we present a new method based on graph-based modeling. By virtue of this method, an effective and accurate EEG rhythm indicator can be extracted to characterize the dynamical EEG time series. Together with the extracted EEG rhythm indicator, an automatic analysis of continuous monitoring of EEG signal, is developed by means of a null hypothesis testing to inspect whether an EEG change occurs or not during a monitoring period. The proposed framework is applied to both simulated data and real signals respectively to validate its effectiveness. Experimental results, together with theoretical interpretation and discussions, suggest its promising potentials in practice.																	0933-3657	1873-2860				JUL	2020	107								101920	10.1016/j.artmed.2020.101920													
J								Bayesian networks in healthcare: Distribution by medical condition	ARTIFICIAL INTELLIGENCE IN MEDICINE										Bayesian networks; Healthcare; Medical conditions	DECISION-SUPPORT-SYSTEM; ARTIFICIAL-INTELLIGENCE; RISK-MANAGEMENT; DIAGNOSIS; MODEL; DEPRESSION; PREDICTION; KNOWLEDGE; CANCER	Bayesian networks (BNs) have received increasing research attention that is not matched by adoption in practice and yet have potential to significantly benefit healthcare. Hitherto, research works have not investigated the types of medical conditions being modelled with BNs, nor whether there are any differences in how and why they are applied to different conditions. This research seeks to identify and quantify the range of medical conditions for which healthcare-related BN models have been proposed, and the differences in approach between the most common medical conditions to which they have been applied. We found that almost two-thirds of all healthcare BNs are focused on four conditions: cardiac, cancer, psychological and lung disorders. We believe there is a lack of understanding regarding how BNs work and what they are capable of, and that it is only with greater understanding and promotion that we may ever realise the full potential of BNs to effect positive change in daily healthcare practice.																	0933-3657	1873-2860				JUL	2020	107								101912	10.1016/j.artmed.2020.101912													
J								The four dimensions of contestable AI diagnostics - A patient-centric approach to explainable AI	ARTIFICIAL INTELLIGENCE IN MEDICINE										AI diagnostics; Contestability; Explainability; Health data; Bias; Performance; Organisation of diagnostic labour	DECISION-SUPPORT; AUTOMATION BIAS; ARTIFICIAL-INTELLIGENCE; HEALTH-CARE; BLACK-BOX; PERFORMANCE; ALGORITHM; MEDICINE	The problem of the explainability of AI decision-making has attracted considerable attention in recent years. In considering AI diagnostics we suggest that explainability should be explicated as 'effective contestability'. Taking a patient-centric approach we argue that patients should be able to contest the diagnoses of AI diagnostic systems, and that effective contestation of patient-relevant aspect of AI diagnoses requires the availability of different types of information about 1) the AI system's use of data, 2) the system's potential biases, 3) the system performance, and 4) the division of labour between the system and health care professionals. We justify and define thirteen specific informational requirements that follows from 'contestability'. We further show not only that contestability is a weaker requirement than some of the proposed criteria of explainability, but also that it does not introduce poorly grounded double standards for AI and health care professionals' diagnostics, and does not come at the cost of AI system performance. Finally, we briefly discuss whether the contestability requirements introduced here are domain-specific.																	0933-3657	1873-2860				JUL	2020	107								101901	10.1016/j.artmed.2020.101901													
J								Multi-stage domain-specific pretraining for improved detection and localization of Barrett's neoplasia: A comprehensive clinically validated study	ARTIFICIAL INTELLIGENCE IN MEDICINE										Computer-aided detection; Barrett's Esophagus; Deep learning; Clinical validation	COMPUTER-AIDED DIAGNOSIS; SYSTEM; ADENOCARCINOMA; ESOPHAGUS	Patients suffering from Barrett's Esophagus (BE) are at an increased risk of developing esophageal adenocarci-noma and early detection is crucial for a good prognosis. To aid the endoscopists with the early detection for this preliminary stage of esophageal cancer, this work concentrates on the development and extensive evaluation of a state-of-the-art computer-aided classification and localization algorithm for dysplastic lesions in BE. To this end, we have employed a large-scale endoscopic data set, consisting of 494,355 images, in combination with a novel semi-supervised learning algorithm to pretrain several instances of the proposed neural network architecture. Next, several Barrett-specific data sets that are increasingly closer to the target domain with significantly more data compared to other related work, were used in a multi-stage transfer learning strategy. Additionally, the algorithm was evaluated on two prospectively gathered external test sets and compared against 53 medical professionals. Finally, the model was also evaluated in a live setting without interfering with the current biopsy protocol. Results from the performed experiments show that the proposed model improves on the state-of-the-art on all measured metrics. More specifically, compared to the best performing state-of-the-art model, the speci-ficity is improved by more than 20% points while simultaneously preserving high sensitivity and reducing the false positive rate substantially. Our algorithm yields similar scores on the localization metrics, where the in-tersection of all experts is correctly indicated in approximately 92% of the cases. Furthermore, the live pilot study shows great performance in a clinical setting with a patient level accuracy, sensitivity, and specificity of 90%. Finally, the proposed algorithm outperforms each individual medical expert by at least 5% and the average assessor by more than 10% over all assessor groups with respect to accuracy.																	0933-3657	1873-2860				JUL	2020	107								101914	10.1016/j.artmed.2020.101914													
J								Missing data imputation and synthetic data simulation through modeling graphical probabilistic dependencies between variables (ModGraProDep): An application to breast cancer survival	ARTIFICIAL INTELLIGENCE IN MEDICINE										Breast cancer; Survival; Graphical models; Missing data; Oversampling; Simulation	COVARIATE DATA; SPAIN; STAGE; DISCRETE; SMOTE	Background: Two common issues may arise in certain population-based breast cancer (BC) survival studies: I) missing values in a survivals' predictive variable, such as "Stage" at diagnosis, and II) small sample size due to "imbalance class problem" in certain subsets of patients, demanding data modeling/simulation methods. Methods: We present a procedure, ModGraProDep, based on graphical modeling (GM) of a dataset to overcome these two issues. The performance of the models derived from ModGraProDep is compared with a set of frequently used classification and machine learning algorithms (Missing Data Problem) and with oversampling algorithms (Synthetic Data Simulation). For the Missing Data Problem we assessed two scenarios: missing completely at random (MCAR) and missing not at random (MNAR). Two validated BC datasets provided by the cancer registries of Girona and Tarragona (northeastern Spain) were used. Results: In both MCAR and MNAR scenarios all models showed poorer prediction performance compared to three GM models: the saturated one (GM.SAT) and two with penalty factors on the partial likelihood (GM.K1 and GM.TEST). However, GM.SAT predictions could lead to non-reliable conclusions in BC survival analysis. Simulation of a "synthetic" dataset derived from GM.SAT could be the worst strategy, but the use of the remaining GMs models could be better than oversampling. Conclusion: Our results suggest the use of the GM-procedure presented for one-variable imputation/prediction of missing data and for simulating "synthetic" BC survival datasets. The "synthetic" datasets derived from GMs could be also used in clinical applications of cancer survival data such as predictive risk analysis.																	0933-3657	1873-2860				JUL	2020	107								101875	10.1016/j.artmed.2020.101875													
J								A generic approach for cell segmentation based on Gabor filtering and area-constrained ultimate erosion	ARTIFICIAL INTELLIGENCE IN MEDICINE										Cell segmentation; Gradient detection; Gabor filter; Threshold selection; Area constrained ultimate erosion	BOTTLENECK DETECTION; NUCLEI	Nowadays, the demand for segmenting different types of cells imaged by microscopes is increased tremendously. The requirements for the segmentation accuracy are becoming stricter. Because of the great diversity of cells, no traditional methods could segment various types of cells with adequate accuracy. In this paper, we aim to propose a generic approach that is capable of segmenting various types of cells robustly and counting the total number of cells accurately. To this end, we utilize the gradients of cells instead of intensity for cell segmentation because the gradients are less affected by the global intensity variations. To improve the segmentation accuracy, we utilize the Gabor filter to increase the intensity uniformity of the gradient image. To get the optimal segmentation, we utilize the slope difference distribution based threshold selection method to segment the Gabor filtered gradient image. At last, we propose an area-constrained ultimate erosion method to separate the connected cells robustly. Twelve types of cells are used to test the proposed approach in this paper. Experimental results showed that the proposed approach is very promising in meeting the strict accuracy requirements for many applications.																	0933-3657	1873-2860				JUL	2020	107								101929	10.1016/j.artmed.2020.101929													
J								Causal inference and counterfactual prediction in machine learning for actionable healthcare	NATURE MACHINE INTELLIGENCE											PROPENSITY SCORE; BIAS; DIAGRAMS; MODEL	Big data, high-performance computing, and (deep) machine learning are increasingly becoming key to precision medicine-from identifying disease risks and taking preventive measures, to making diagnoses and personalizing treatment for individuals. Precision medicine, however, is not only about predicting risks and outcomes, but also about weighing interventions. Interventional clinical predictive models require the correct specification of cause and effect, and the calculation of so-called counterfactuals, that is, alternative scenarios. In biomedical research, observational studies are commonly affected by confounding and selection bias. Without robust assumptions, often requiring a priori domain knowledge, causal inference is not feasible. Data-driven prediction models are often mistakenly used to draw causal effects, but neither their parameters nor their predictions necessarily have a causal interpretation. Therefore, the premise that data-driven prediction models lead to trustable decisions/interventions for precision medicine is questionable. When pursuing intervention modelling, the bio-health informatics community needs to employ causal approaches and learn causal structures. Here we discuss how target trials (algorithmic emulation of randomized studies), transportability (the licence to transfer causal effects from one population to another) and prediction invariance (where a true causal model is contained in the set of all prediction models whose accuracy does not vary across different settings) are linchpins to developing and testing intervention models. Machine learning models are commonly used to predict risks and outcomes in biomedical research. But healthcare often requires information about cause-effect relations and alternative scenarios, that is, counterfactuals. Prosperi et al. discuss the importance of interventional and counterfactual models, as opposed to purely predictive models, in the context of precision medicine.																		2522-5839				JUL	2020	2	7					369	375		10.1038/s42256-020-0197-y													
J								Deep learning decodes the principles of differential gene expression	NATURE MACHINE INTELLIGENCE											RNA-BINDING PROTEINS; DATABASE; TRANSCRIPTOME; MOUSE; DNA	A goal of biology is to identify the molecular mechanisms that control differential gene expression. Tasaki et al. have developed a framework that integrates genomic data into a deep learning model of transcriptome regulations to predict multiple transcriptional effects in tissue- and person-specific transcriptomes. Identifying the molecular mechanisms that control differential gene expression (DE) is a major goal of basic and disease biology. Here, we develop a systems biology model to predict DE and mine the biological basis of the factors that influence predicted gene expression to understand how it may be generated. This model, called DEcode, utilizes deep learning to predict DE based on genome-wide binding sites on RNAs and promoters. Ranking predictive factors from DEcode indicates that clinically relevant expression changes between thousands of individuals can be predicted mainly through the joint action of post-transcriptional RNA-binding factors. We also show the broad potential applications of DEcode to generate biological insights, by predicting DE between tissues, differential transcript usage, and drivers of ageing throughout the human lifespan, of gene co-expression relationships on a genome-wide scale, and of frequently differentially expressed genes across diverse conditions. DEcode is freely available to researchers to identify influential molecular mechanisms for any human expression data.																		2522-5839				JUL	2020	2	7					376	+		10.1038/s42256-020-0201-6													
J								Gaussian embedding for large-scale gene set analysis	NATURE MACHINE INTELLIGENCE											PROTEIN-INTERACTION NETWORKS; ENRICHMENT ANALYSIS; FUNCTIONAL-ANALYSIS; PATHWAYS; ONTOLOGY; CANCER	Gene sets, including protein complexes and signalling pathways, have proliferated greatly, in large part as a result of high-throughput biological data. Leveraging gene sets to gain insight into biological discovery requires computational methods for converting them into a useful form for available machine learning models. Here, we study the problem of embedding gene sets as compact features that are compatible with available machine learning codes. We present Set2Gaussian, a novel network-based gene set embedding approach, which represents each gene set as a multivariate Gaussian distribution rather than a single point in the low-dimensional space, according to the proximity of these genes in a protein-protein interaction network. We demonstrate that Set2Gaussian improves gene set member identification, accurately stratifies tumours, and finds concise gene sets for gene set enrichment analysis. We further show how Set2Gaussian allows us to identify a clinical prognostic and predictive subnetwork around neurofilament medium in sarcoma, which we validate in independent cohorts. Gene sets can provide valuable information for gaining insight into disease mechanisms and cellular functions. In this paper, the authors use a Gaussian approach to represent gene sets and gene networks in a low-dimensional space, allowing for accurate prediction and decreased computational complexity.																		2522-5839				JUL	2020	2	7					387	395		10.1038/s42256-020-0193-2													
J								Quantum approximate Bayesian computation for NMR model inference	NATURE MACHINE INTELLIGENCE											HIGH-RESOLUTION H-1-NMR; SPECTROSCOPY; SPECTRA; METABOLOMICS; PLASMA	Currently available quantum hardware is limited by noise, so practical implementations often involve a combination with classical approaches. Sels et al. identify a promising application for such a quantum-classic hybrid approach, namely inferring molecular structure from NMR spectra, by employing a range of machine learning tools in combination with a quantum simulator. Recent technological advances may lead to the development of small-scale quantum computers that are capable of solving problems that cannot be tackled with classical computers. A limited number of algorithms have been proposed and their relevance to real-world problems is a subject of active investigation. Analysis of many-body quantum systems is particularly challenging for classical computers due to the exponential scaling of the Hilbert space dimension with the number of particles. Hence, solving the problems relevant to chemistry and condensed-matter physics is expected to be the first successful application of quantum computers. In this Article, we propose another class of problems from the quantum realm that can be solved efficiently on quantum computers: model inference for nuclear magnetic resonance (NMR) spectroscopy, which is important for biological and medical research. Our results are based on three interconnected studies. First, we use methods from classical machine learning to analyse a dataset of NMR spectra of small molecules. We perform stochastic neighbourhood embedding and identify clusters of spectra, and demonstrate that these clusters are correlated with the covalent structure of the molecules. Second, we propose a simple and efficient method, aided by a quantum simulator, to extract the NMR spectrum of any hypothetical molecule described by a parametric Heisenberg model. Third, we propose a simple variational Bayesian inference procedure for estimating the Hamiltonian parameters of experimentally relevant NMR spectra.																		2522-5839				JUL	2020	2	7					396	402		10.1038/s42256-020-0198-x													
J								Actor neural networks for the robust control of partially measured nonlinear systems showcased for image propagation through diffuse media	NATURE MACHINE INTELLIGENCE											TRANSMISSION; LIGHT; INFORMATION; MICROSCOPY; HOLOGRAPHY; DESIGN	The output of physical systems, such as the scrambled pattern formed by shining the spot of a laser pointer through fog, is often easily accessible by direct measurements. However, selection of the input of such a system to obtain a desired output is difficult, because it is an ill-posed problem; that is, there are multiple inputs yielding the same output. Information transmission through scattering media is an example of this problem. Machine learning approaches for imaging have been implemented very successfully in photonics to recover the original input phase and amplitude objects of the scattering system from the distorted intensity diffraction pattern outputs. However, controlling the output of such a system, without having examples of inputs that can produce outputs in the class of the output objects the user wants to produce, is a challenging problem. Here, we propose an online learning approach for the projection of arbitrary shapes through a multimode fibre when a sample of intensity-only measurements is taken at the output. This projection system is nonlinear, because the intensity, not the complex amplitude, is detected. We show an image projection fidelity as high as similar to 90%, which is on par with the gold-standard methods that characterize the system fully by phase and amplitude measurements. The generality and simplicity of the proposed approach could potentially provide a new way of target-oriented control in real-world applications when only partial measurements are available.																		2522-5839				JUL	2020	2	7					403	+		10.1038/s42256-020-0199-9													
J								High-accuracy prostate cancer pathology using deep learning	NATURE MACHINE INTELLIGENCE											IMAGES; BIOPSIES; SYSTEM	Deep learning methods can be a powerful part of digital pathology workflows, provided well-annotated training datasets are available. Tolkach and colleagues develop a deep learning model to recognize and grade prostate cancer, based on a convolution neural network and a dataset with high-quality labels at gland-level precision. Deep learning (DL) is a powerful methodology for the recognition and classification of tissue structures in digital pathology. Its performance in prostate cancer pathology is still under intensive investigation. Here we develop DL-based models for the detection of prostate cancer tissue in whole-slide images based on a large high-quality annotated training dataset and a modern state-of-the-art convolutional network architecture (NASNetLarge). The overall accuracy of our model for tumour detection in two validation cohorts is comparable to that of pathologists and reaches 97.3% in a native version and more than 98% using the suggested DL-based augmentation strategies. As a second step, we suggest a new biologically meaningful DL-based algorithm for Gleason grading of prostatic adenocarcinomas with high, human-level performance in prognostic stratification of patients when tested in several well-characterized validation cohorts. Furthermore, we determine the optimal minimal tumour size (real size of approximately 560 x 560 mu m) for robust Gleason grading representative of the whole tumour focus. Our approach is realized in the unified digital pathology pipeline, which delivers all the relevant tumour metrics for a pathology report.																		2522-5839				JUL	2020	2	7					411	+		10.1038/s42256-020-0200-7													
J								An Integration of Train Timetabling, Platforming and Routing-Based Cooperative Adjustment Methodology for Dealing with Train Delay	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Train delay; railway timetabling; train platforming; train routing; optimization models and algorithms	OPTIMIZATION	Train delay is a serious issue that can spread rapidly in the railway network leading to further delay of other trains and detention of passengers in stations. However, the current practice in the event of the trail delay usually depends on train dispatcher's experience, which cannot manage train operation effectively and may have safety risks. The application of intelligent railway monitor and control system can improve train operation management while increasing railway safety. This paper presents a methodology in which train timetabling, platforming and routing models are combined by studying the real-time adjustment and optimization of highspeed railway in the case of the train delay in order to produce a cooperative adjustment algorithm so that the train operation adjustment plan can be obtained. MATLAB computer programs have been developed based on the proposed methodology and adjustment criteria have been established from knowledge data bases in order to calculate optimized solutions. A case study is used to demonstrate the proposed methodology. The results show that the proposed method can quickly adjust the train operation plan in the case of the train delay, restore the normal train operation order, and reduce the impact of train delay on railway network effectively and efficiently.																	0218-1940	1793-6403				JUL	2020	30	7					901	919		10.1142/S0218194020400112													
J								A Data-Driven Two-Stage Prediction Model for Train Primary-Delay Recovery Time	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Train delay; recovery prediction; high-speed railway; random forest model; data-driven	REGRESSION	Accurate prediction of train delay recovery is critical for railway incident management and providing passengers with accurate journey time. In this paper, a two-stage prediction model is proposed to predict the recovery time of train primary-delay based on the real records from High-Speed Railway (HSR). In Stage 1, two models are built to study the influence of feature space and model framework on the prediction accuracy of buffer time in each section or station. It is found that explicitly inputting the attribute features of stations and sections to the model, instead of implicit simulation, will improve the prediction accuracy effectively. For validation purpose, the proposed model has been compared with several alternative models, namely, Logistic Regression (LR), Artificial Neutral Network (ANN), Support Vector Machine (SVM) and Gradient Boosting Tree (GBT). The results show that its remarkable performance is better than other schemes. Specifically, when the error is extended to 3min, the proposed model can achieve up to the accuracy of 94.63%. It proves that our method has high value in practical engineering application. Considering the delay propagation of trains is a complex process, our future study will focus on building delay propagation knowledge base and dispatcher experience knowledge base.																	0218-1940	1793-6403				JUL	2020	30	7					921	940		10.1142/S0218194020400124													
J								An Improved Faster R-CNN for UAV-Based Catenary Support Device Inspection	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Catenary support device; improved Faster R-CNN; UAV image; fasteners; automatic defect detection	DEFECT DETECTION	The catenary support device inspection is of crucial importance for ensuring safety and reliability of railway systems. At present, visual detection tasks of catenary support devices defect are performed by trained personnel based on the images taken periodically by industrial cameras installed on inspection vehicle in a limited period of time at midnight. However, the inspection mean is inappropriate for low efficiency and high cost. This paper presents a novel network based on unmanned aerial vehicle (UAV) images for catenary support device inspection and focuses on small object detection and the imbalanced dataset. With regards to the first aspect, based on a pyramid network structure, the improved Faster R-CNN consists of a top-down-top feature pyramid fusion structure, which heavily fuses high-level semantic information and low-level detail information. The feature map fusions of three different pooling scales are employed for improving detection accuracy of predicted bounding boxes. With regards to the second, we copy and paste the small proportion objects of dataset for avoiding category imbalance. Finally, quantitative and qualitative evaluations illustrate that the improved Faster-RCNN achieves better performance over the classic methods, yet remains convenient and efficient.																	0218-1940	1793-6403				JUL	2020	30	7					941	959		10.1142/S0218194020400136													
J								The Turnout Abnormality Diagnosis Based on Semi-Supervised Learning Method	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Railway transportation; turnout abnormality diagnosis; semi-supervised learning method; SVM; unmarked sampling		In China, the turnout abnormality very easily causes traffic accident or affects the efficiency due to the operating environment of railway transportation. The existing monitoring means are relatively backward, and mature automatic diagnosis method is lacking. In this study, a method based on semi-supervised learning algorithm for abnormal state diagnosis of turnout action curve is proposed, which is used to analyze and extract the electrical characteristics of the turnout by using the turnout action curve and the static and dynamic properties collected by the railway centralized monitoring system. The support vector machine model is used to construct the initial classifier with a small number of labeled samples, and the labeled samples are expanded from a large number of unlabeled samples. The switch curve is analyzed and diagnosed by using unlabeled data with a small amount of labeled data. The experimental results show that the method can automatically diagnose turnout electrical characteristics with high accuracy. While the cost of this method is relatively low compared with supervised learning, it can achieve higher accuracy and improve the practicability of fault diagnosis of turnout.																	0218-1940	1793-6403				JUL	2020	30	7					961	976		10.1142/S0218194020400148													
J								Impact Analysis About Response Time Considering Deployment Change of SaaS Software	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										SaaS software; deployment scheme; performance; queueing theory	PERFORMANCE ANALYSIS; MODEL; OPTIMIZATION	The deployment change of SaaS (Software as a Service) software will influence its response time, which is an important performance metric. Therefore, studying the impact of deployment change on the response time of SaaS software could contribute to performance improvement of the software. However, there are few performance analysis methods which can directly analyze the relationship between deployment change and response time of SaaS software. In this paper, we propose an approach which provides the impact analysis of specific deployment change operations on response time of SaaS software explicitly. Specifically, we present an evaluation method for the response time of SaaS software in specific deployment scheme by leveraging queueing theory. With mathematical derivation based on the proposed evaluation method, we qualitatively analyze the variation trend of response time with respect to deployment change. Furthermore, we study the relationship between two specific types of deployment change operations and response time variation of SaaS software, which is used to propose a response time improvement method based on deployment change. Finally, the effectiveness of the analysis conclusions and the proposed method in this paper is validated by practical cases, which indicates that adjusting deployment scheme according to the conclusions obtained in this paper can be helpful in improving the response time of SaaS software.																	0218-1940	1793-6403				JUL	2020	30	7					977	1004		10.1142/S0218194020500266													
J								An Automated Hybrid Approach for Generating Requirements Trace Links	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Requirements traceability; information retrieval; vector space model; biterm topic model; genetic algorithm; short-text artifacts	TRACEABILITY	Trace links between requirements and software artifacts provide available traceability information and in-depth insights for different stakeholders. Unfortunately, establishing requirements trace links is a tedious, labor-intensive and fallible task. To alleviate this problem, Information Retrieval (IR) methods, such as Vector Space Model (VSM), Latent Semantic Indexing (LSI), and their variants, have been widely used to establish trace links automatically. But with the widespread use of agile development methodology, artifacts that can be used to generate automatic tracing links are getting shorter and shorter, which decreases the effects of traditional IR-based trace link generation methods. In this paper, Biterm Topic Model-Genetic Algorithm (BTM-GA), which is effective in managing short-text artifacts and configuring initial parameters, is introduced. A hybrid method VSM+BTM-GA is proposed to generate requirements trace links. Empirical experiments conducted on five real and frequently-used datasets indicate that (1) the hybrid method VSM+BTM-GA outperforms the others, and its results can achieve the "Good" level, where recall and precision are no less than 70% and 30%, respectively; (2) the performance of the hybrid method is stable and (3) BTM-GA can provide a number of "hard-to-find" trace links that complement the candidate trace links of VSM.																	0218-1940	1793-6403				JUL	2020	30	7					1005	1048		10.1142/S0218194020500278													
J								On the Implication of Properties of Related Systems: a Method for Obtaining Implication Conditions and Application Examples	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											QUALITATIVE-ANALYSIS; DYNAMIC-SYSTEMS; REDUCTION	We propose a method for obtaining the implication conditions of the properties of a mathematical model of a system from the structurally close properties of a model of another system related to the former one by cross-model linking objects. This method is applicable to various models and their properties. Any implication conditions for the properties are not required to be given a priori. They are formed during the solving corresponding logical equation with the possibility to broadly vary the cross-model linking objects. Usually, implication theorems obtained by this method are new or modifications of known ones. In other cases, they are corollaries or generalizations of known theorems. We provide detailed application examples for this method and interpret the conditions of the obtained theorems.																	1064-2307	1555-6530				JUL	2020	59	4					479	493		10.1134/S1064230720040140													
J								Game-Theoretic Control of the Object's Random Jump Structure in the Class of Pure Strategies	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The optimal control problem of the random-jump structure of an object under counteraction conditions is considered. The counteracting parties observe any changes in the object's structure using special indicators that operate with errors. The optimality criterion of the controls is a certain functional of the object's state. One of the opponents seeks to minimize this criterion, whereas the other seeks to maximize. The players control the object's structure in the class of pure strategies by applying a finite number of admissible strategies. The optimal controls are found in the class of deterministic dependences on the results of observations preceding the current time instant. As an illustrative example, the optimal control of the object's structure with two states is obtained using the design methods of the theory of systems with a random jump structure (RJS) in the game-theoretic formulation.																	1064-2307	1555-6530				JUL	2020	59	4					494	503		10.1134/S1064230720040024													
J								Modeling and Optimizing the Turn of a Loaded Elastic Rod Controlled by an Electric Drive	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											SYSTEMS	In this study, we consider a flat rotation of an electric rectilinear rod controlled by an electric drive, loaded at the free end by a point mass. The problem is to optimally control the voltage in the motor's winding, which transfers the system to the final state with the damping of the elastic vibrations. A generalized formulation of the equations of the rod's state and a method for approximating unknown displacements, momentum density, and bending moment are proposed. The procedure for minimizing the objective functional and regularizing the numerical error is described.																	1064-2307	1555-6530				JUL	2020	59	4					504	517		10.1134/S1064230720040103													
J								Optimal Energy-Efficient Programmed Control of Distributed Parameter Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A constructive solution of the optimal energy-efficient programmed control problem for distributed parameter systems with a given-precision uniform approximation of the space distribution of the controlled variable with respect to the desired state is proposed. The computational algorithm developed below involves a special-form preliminary parametrization procedure for control actions on finite-dimensional subsets of the terminal values of conjugate variables in the boundary-value problem of Pontryagin's maximum principle, in combination with the subsequent reduction to a semi-infinite optimization problem, which is solved with respect to the requisite parameter vector using the alternance method suggested earlier. An example of optimal energy-efficient control of transient heat conduction, which is of independent interest, is given.																	1064-2307	1555-6530				JUL	2020	59	4					518	532		10.1134/S1064230720030120													
J								Unstable Oscillating Systems with Hysteresis: Problems of Stabilization and Control	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											INVERTED PENDULUM	The work is devoted to studying the dynamics of unstable oscillating systems (in the form of an inverted pendulum) controlled by the action of a hysteretic type. The results for different types of motion of a suspension point are presented, in particular, for vertical and horizontal motion. A mathematical model of the inverted pendulum with an oscillating suspension is considered. For this pendulum the explicit criteria of stability are obtained using the linearized equations of motion. The dependences between the initial conditions and the value of the control parameters providing periodic oscillations of the pendulum are described. A mathematical model of the inverted pendulum with feedback control is given under the conditions of the horizontal motion of the suspension point. The conditions that guarantee the stabilization of the considered system are obtained; the conditions are formulated in terms of constraints on the initial conditions. The solution to the problem of the optimal control of an oscillating system is presented in the sense of minimization of a quadratic goal functional. The stabilization problem for an unstable system with distributed parameters, the flexible inverted pendulum, is also considered, and the stabilization conditions are formulated. Fulfilment of these conditions ensures the boundedness of the phase coordinates in the infinite interval of time. The optimal parameters (in the sense of minimization of a quadratic goal functional) corresponding to stabilization of the distributed system are identified.																	1064-2307	1555-6530				JUL	2020	59	4					533	556		10.1134/S1064230720030090													
J								Planning Calculations in a Multiprocessor System with Unspecified Moments of Operational Readiness	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											FIXED PARAMETERS; NETWORK MODELS	The problem of compiling an acceptable multiprocessor schedule without interruptions and switching is considered for the case when the set of partial relations is specified on the set of operations, all operations have a common deadline, and the distribution of tasks on the processors is specified. At some undetermined times, requests may be made to perform additional, higher priority operations, for which some processors are freed for a certain time. As a result, the execution of the initial set of tasks is postponed to a later time and thereby violates the schedule built for it. A strategy is developed for constructing an acceptable schedule in which the probability of its violation due to requests for additional operations is minimal.																	1064-2307	1555-6530				JUL	2020	59	4					557	564		10.1134/S1064230720040048													
J								Method for the Functional Diagnosis of Nondeterministic Finite State Machines	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The problem of the functional diagnosis of critical systems, described by a nondeterministic finite state machine model, is considered. A new method for solving the problem is proposed, the distinguishing feature of which is the use of the mathematical apparatus of the algebra's covers. The features and advantages of the method are illustrated by the example of the problem of monitoring the errors of IT system operators.																	1064-2307	1555-6530				JUL	2020	59	4					565	574		10.1134/S1064230720040152													
J								Training a Multimodal Neural Network to Determine the Authenticity of Images	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											IDENTIFICATION	The identification of attempts to substitute images plays an important role in protecting biometric systems (authorization in mobile devices, access control systems for premises, terminals with automatic access by face recognition, etc.). This study presents a new method for detecting falsified images based on processing the multimodal data from a camera. A new neural network architecture is developed that aggregates the features from different modalities at all levels of the model. The separation of the training sample for different types of attacks and the initialization of the model with attributes trained in other tasks that are associated with facial images are considered. Numerical experiments on real data are performed, showing the successful performance of the system. The proposed model won first place in the CASIA-SURF competition for the recognition of falsified facial images.																	1064-2307	1555-6530				JUL	2020	59	4					575	582		10.1134/S1064230720040073													
J								Optimization of a Tracking System Based on a Network of Cameras	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											OBJECT DETECTION; IDENTIFICATION	Tracking the motion of objects in video sequences is an important problem of computer vision that has a wide range of applications. The key points in tracking systems is the detection of an object and, if it was detected repeatedly, its reidentification. A fast correctly working tracking system that uses a number of cameras is described. The system includes detection and segmentation of objects in images, construction of their appearance descriptors, comparison of each new object with earlier collected objects, and making a decision about their reidentification. The basic system configuration is implemented in which the state-of-the art detection algorithms and models for constructing the appearance descriptors are used as the constituent parts. Based on this, the system as a whole and some of its modules are modified. A computational experiment that quantitatively confirms the advantages of the modified system over the basic system is performed.																	1064-2307	1555-6530				JUL	2020	59	4					583	597		10.1134/S1064230720040127													
J								Refining the Earth Orientation Parameters Onboard Spacecraft: Concept and Information Technologies	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We consider the problem of refining the evolving Earth Orientation Parameters (EOP) used during the converting from the inertial coordinate system to the Earth's coordinate system, including ephemerides computations onboard spacecraft (SC). We discuss the approaches and technologies used to refine these parameters. We propose a concept to refine EOP by ground stations and SC, based on processing the measurements of the distance between the ground station and SC by the least squares method (LSM). We provide mathematical models, refining algorithms, and the results of their application in experiments simulating refining processes for parameters of the EOP onboard SC.																	1064-2307	1555-6530				JUL	2020	59	4					598	608		10.1134/S1064230720040061													
J								Hybrid Simulation of Spacecraft Berthing	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A hybrid simulation method applied for developing and testing the process of spacecraft berthing (i.e., docking using a space manipulator) is described. In this method, the manipulator's dynamics and the relative motion of the objects to be docked are computed using a mathematical model and the docking units are real-life copies. The relative motion of these units is reproduced on a 6-degrees of freedom test facility controlled by a computer.																	1064-2307	1555-6530				JUL	2020	59	4					609	621		10.1134/S106423072004005X													
J								Objects Changing the Spatial Orientation of a Solid Body by Using Mobile Mass	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Formulas are obtained that make it possible to realize a predetermined motion of a rigid body with respect to its center of mass in a coordinate system with this center and translating axes. It is shown that there are two significantly different cases. In the first case, the point should always be in a certain plane, which not only makes it possible to implement any required movement from this class but also allows us to use the ambiguity of the solution so that, for example, the point always moves along the same path in the specified plane. In the second case, the solution turns out to be unique and the material point should have, generally speaking, a separate spatial trajectory for each given program of reorientation of a rigid body. Moreover, in this case only those motions that satisfy the found condition can be realized.																	1064-2307	1555-6530				JUL	2020	59	4					622	629		10.1134/S1064230720040139													
J								Wind Turbine of the Savonius-Magnus Type with Conical Blades: Dynamics and Control	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We consider a mathematical model of a horizontal-axis wind-energy unit in which Savonius rotors are used instead of classical blades. The Magnus force formed due to the autorotation of Savonius rotors creates a momentum supporting the rotation of the central turbine shaft. The main difference of this study from earlier investigations in this area is as follows: we take into account the variation of the blade width along the radius. In our model, the conical Savonius rotor is replaced by a pair of cylindrical rotors with different diameters, which provides the possibility to use the experimental force-momentum characteristics, taking into account the substantial variations of the velocity field along the blade's radius. In the model, we consider the possibility to control the value of the external electric resistance in the local circuit of the unit generator. We describe the dependence of the mechanical power on the parameters of the model and construct a control strategy providing the possibility to maintain the power close to the maximum possible value under changes in the wind velocity.																	1064-2307	1555-6530				JUL	2020	59	4					630	638		10.1134/S1064230720040085													
J								Classification of Trajectories of Hidden Motion of an Airborne Object in the Detection Zone of an Onboard Doppler Radar Station	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												It is shown that due to the peculiarities of the detection zone of an airborne radar station of a long-range radar detection system operating in the pulse-Doppler mode, there are hidden trajectories in which airborne objects (AOs) are not detected by the airborne radar station. Solutions to differential equations describing hidden trajectories are found. The hidden trajectories are classified. Phase portraits corresponding to various types of hidden trajectories are constructed. The conditions are found on the motion parameters of an AO and aircraft of long-range radar detection, upon fulfillment of which a hidden trajectory is realized.																	1064-2307	1555-6530				JUL	2020	59	4					639	646		10.1134/S1064230720040097													
J								A novel method for predicting the progression rate of ALS disease based on automatic generation of probabilistic causal chains	ARTIFICIAL INTELLIGENCE IN MEDICINE										Causal chain; Amyotrophic Lateral Sclerosis; Entropy; Temporal dataset; Prediction	AMYOTROPHIC-LATERAL-SCLEROSIS; DYNAMIC BAYESIAN NETWORK; MODELS; TRIAL; INFERENCE; SYSTEM; GENES	Causal discovery is considered as a major concept in biomedical informatics contributing to diagnosis, therapy, and prognosis of diseases. Probabilistic causality approaches in epidemiology and medicine is a common method for finding relationships between pathogen and disease, environment and disease, and adverse events and drugs. Bayesian Network (BN) is one of the common approaches for probabilistic causality, which is widely used in health-care and biomedical science. Since in many biomedical applications we deal with temporal dataset, the temporal extension of BNs called Dynamic Bayesian network (DBN) is used for such applications. DBNs define probabilistic relationships between parameters in consecutive time points in the form of a graph and have been successfully used in many biomedical applications. In this paper, a novel method was introduced for finding probabilistic causal chains from a temporal dataset with the help of entropy and causal tendency measures. In this method, first, Causal Features Dependency (CFD) matrix is created on the basis of parameters changes in consecutive events of a phenomenon, and then the probabilistic causal graph is constructed from this matrix based on entropy criteria. At the next step, a set of probabilistic causal chains of the corresponding causal graph is constructed by a novel polynomial-time heuristic. Finally, the causal chains are used for predicting the future trend of the phenomenon. The proposed model was applied to the Pooled Resource Open-Access Clinical Trials (PRO-ACT) dataset related to Amyotrophic Lateral Sclerosis (ALS) disease, in order to predict the progression rate of this disease. The results of comparison with Bayesian tree, random forest, support vector regression, linear regression, and multivariate regression show that the proposed algorithm can compete with these methods and in some cases outperforms other algorithms. This study revealed that probabilistic causality is an appropriate approach for predicting the future states of chronic diseases with unknown cause.																	0933-3657	1873-2860				JUL	2020	107								101879	10.1016/j.artmed.2020.101879													
J								Integrating expert's knowledge constraint of time dependent exposures in structure learning for Bayesian networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Dynamic Bayesian network; Graphical structure learning; VAR model; Time dependent exposure	COMBINATION; ALGORITHMS; SEARCH; MODELS	Learning a Bayesian network is a difficult and well known task that has been largely investigated. To reduce the number of candidate graphs to test, some authors proposed to incorporate a priori expert knowledge. Most of the time, this a priori information between variables influences the learning but never contradicts the data. In addition, the development of Bayesian networks integrating time such as dynamic Bayesian networks allows identifying causal graphs in the context of longitudinal data. Moreover, in the context where the number of strongly correlated variables is large (i.e. oncology) and the number of patients low; if a biomarker has a mediated effect on another, the learning algorithm would associate them wrongly and vice versa. In this article we propose a method to use the a priori expert knowledge as hard constraints in a structure learning method for Bayesian networks with a time dependant exposure. Based on a simulation study and an application, where we compared our method to the state of the art PC-algorithm, the results showed a better recovery of the true graphs when integrating hard constraints a priori expert knowledge even for small level of information.																	0933-3657	1873-2860				JUL	2020	107								101874	10.1016/j.artmed.2020.101874													
J								Internet of things-inspired healthcare system for urine-based diabetes prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Internet of things; Diabetic monitoring system; Recurrent Neural Network (RNN); SOM visualization	IOT; CHALLENGES; SECURITY; EDGE	Healthcare industry is the leading domain that has been revolutionized by the incorporation of Internet of Things (IoT) technology resulting in smart medical applications. Conspicuously, this study presents an effective system of home-centric Urine-based Diabetes (UbD) monitoring system. Specifically, the proposed system comprises of 4-layers for predicting and monitoring diabetes-oriented urine infection. The system layers including Diabetic Data Acquisition (DDA) layer, Diabetic Data Classification (DDC) layer, Diabetic-Mining and Extraction (DME) layer, and Diabetic Prediction and Decision Making (DPDM) layer allow an individual not exclusively to track his/her diabetes measure on regular basis but the prediction procedure is also accomplished so that prudent steps can be taken at early stages. Additionally, probabilistic measurement of UbD monitoring in terms of Level of Diabetic Infection (LoDI), which is cumulatively quantified as Diabetes Infection Measure (DIM) has been performed for predictive purposes using Recurrent Neural Network (RNN). Moreover, the existence of UbD is visualized based on the Self-Organized Mapping (SOM) procedure. To validate the proposed system, numerous experimental simulations were performed on datasets of 4 individuals. Based on the experimental simulation, enhanced results in terms of temporal delay, classification efficiency, prediction efficiency, reliability and stability were registered for the proposed system in comparison to state-of-the-art decision-making techniques.																	0933-3657	1873-2860				JUL	2020	107								101913	10.1016/j.artmed.2020.101913													
J								Pulmonary nodule detection on chest radiographs using balanced convolutional neural network and classic candidate detection	ARTIFICIAL INTELLIGENCE IN MEDICINE										Computer-aided detection (CADe); Transfer learning; Chest radiograph (CXR); Pulmonary nodule	COMPUTER-AIDED DIAGNOSIS; LUNG NODULES; IMAGE DATABASE; SEGMENTATION; PERFORMANCE; REDUCTION; FEATURES; SYSTEM; SCHEME	Computer-aided detection (CADe) systems play a crucial role in pulmonary nodule detection via chest radiographs (CXRs). A two-stage CADe scheme usually includes nodule candidate detection and false positive reduction. A pure deep learning model, such as faster region convolutional neural network (faster R-CNN), has been successfully applied for nodule candidate detection via computed tomography (CT). The model is yet to achieve a satisfactory performance in CXR, because the size of the CXR is relatively large and the nodule in CXR has been obscured by structures such as ribs. In contrast, the CNN has proved effective for false positive reduction compared to the shallow method. In this paper, we developed a CADe scheme using the balanced CNN with classic candidate detection. First, the scheme applied a multi-segment active shape model to accurately segment pulmonary parenchyma. The grayscale morphological enhancement technique was then used to improve the conspicuity of the nodule structure. Based on the nodule enhancement image, 200 nodule candidates were selected and a region of interest (ROI) was cropped for each. Nodules in CXR exhibit a large variation in density, and rib crossing and vessel tissue usually present similar features to the nodule. Compared to the original ROI image, the nodule enhancement ROI image has potential discriminative features from false positive reduction. In this study, the nodule enhancement ROI image, corresponding segmentation result, and original ROI image were encoded into a red-green-blue (RGB) color image instead of the duplicated original ROI image as input of the CNN (GoogLeNet) for false positive reduction. With the Japanese Society of Radiological Technology database, the CADe scheme achieved high performance of the published literatures (a sensitivity of 91.4 % and 97.1 %, with 2.0 false positives per image (FPs/image) and 5.0 FPs/image, respectively) for nodule cases.																	0933-3657	1873-2860				JUL	2020	107								101881	10.1016/j.artmed.2020.101881													
J								A case-based ensemble learning system for explainable breast cancer recurrence prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Ensemble learning; Case-based reasoning; Breast cancer; Recurrence prediction; Case-based interpretation	ARTIFICIAL-INTELLIGENCE; NEURAL-NETWORKS; DIAGNOSIS; CLASSIFICATION; PERFORMANCE; NUCLEI	Significant progress has been achieved in recent years in the application of artificial intelligence (AI) for medical decision support. However, many AI-based systems often only provide a final prediction to the doctor without an explanation of its underlying decision-making process. In scenarios concerning deadly diseases, such as breast cancer, a doctor adopting an auxiliary prediction is taking big risks, as a bad decision can have very harmful consequences for the patient. We propose an auxiliary decision support system that combines ensemble learning with case-based reasoning to help doctors improve the accuracy of breast cancer recurrence prediction. The system provides a case-based interpretation of its prediction, which is easier for doctors to understand, helping them assess the reliability of the system's prediction and make their decisions accordingly. Our application and evaluation in a case study focusing on breast cancer recurrence prediction shows that the proposed system not only provides reasonably accurate predictions but is also well-received by oncologists.																	0933-3657	1873-2860				JUL	2020	107								101858	10.1016/j.artmed.2020.101858													
J								A framework to shift basins of attraction of gene regulatory networks through batch reinforcement learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Reinforcement learning; Gene regulatory network; Basin of attraction	EXTERNAL CONTROL; INTERVENTION; INFERENCE; SYSTEMS	A major challenge in gene regulatory networks (GRN) of biological systems is to discover when and what interventions should be applied to shift them to healthy phenotypes. A set of gene activity profiles, called basin of attraction (BOA), takes this network to a specific phenotype; therefore, a healthy BOA leads the GRN to a healthy phenotype. However, without the complete observability of the genes, it is not possible to identify whether the current BOA is healthy. In this article we investigate external interventions in GRN with partial observability aiming to bring it to healthy BOAS. We propose a new batch reinforcement learning method (BRL), called mSFQI, to define intervention strategies based on the probabilities of the gene activity profiles being in healthy BOAS, which are calculated from a set of previous observed experiences. BRL uses approximation functions and repeated applications of previous experiences to accelerate learning. Results demonstrate that our proposal can quickly shift a partially observable GRN to healthy BOAS, while reducing the number of interventions. In addition, when observability is poor, mSFQI produces better results when the probabilities for a greater amount of previous observations are available.																	0933-3657	1873-2860				JUL	2020	107								101853	10.1016/j.artmed.2020.101853													
J								Predicting postoperative non-small cell lung cancer prognosis via long short-term relational regularization	ARTIFICIAL INTELLIGENCE IN MEDICINE										Prognostic prediction; Non-small cell lung cancer; Risk factor recognition; Long short-term relational regularization	STAGE-I; INTERNATIONAL-ASSOCIATION; SURVIVAL PREDICTION; CLASSIFICATION; REGRESSION; DIAGNOSIS; SELECTION; MODELS; SYSTEM	Objectives: Lung cancer is the leading cause of cancer death worldwide. Prognosis of lung cancer plays a crucial role in the clinical decision-making process to optimize the treatment for patients. Most of the existing data-driven prognostic prediction models explore the relations between patient's characteristics and outcomes at a specific time interval. Although valuable, they neglect the relations between long-term and short-term prognoses and thus may limit the prediction performance. Methods: In this study, we present a novel prognostic prediction approach for postoperative NSCLC patients. Specifically, we formulate the learning objective function by exploiting the relations between long-term and short-term prognoses via a long short-term relational regularization. The regularization term is composed of two parts, i.e., the similarities between prognoses measured by patients' outcomes and the L-2 -norms between the corresponding prognoses' weight vectors. Based on this regularization, the proposed method can extract critical risk factors that comprehensively consider the long-term and short-term prognoses to facilitate the estimation of clinical risks. Results: We evaluate the proposed model on a clinical dataset containing 693 consecutive postoperative NSCLC patients with more than 5-year follow-up from 2006 to 2015. Our best models achieve 0.743, 0.709, and 0.746 AUCs for 1-year, 3-year, and 5-year survival prediction, 0.696, 0.724, and 0.736 AUCs for 1-year, 3-year, and 5-year recurrence prediction, respectively. The experimental results show the efficiency of our proposed model in improving the performances on 1-year prognostic prediction in comparison with benchmark models. By comparing with the model without the long short-term relational regularization, the proposed model extracts more consistent critical risk factors for both long-term and short-term prognoses and contains fewer unreasonable risk factors under the clinician's review. Conclusions: We conclude that the proposed model can effectively exploit the relations between long-term and short-term prognoses. And the risk factors recognized by the proposed model have the potentials for further prognostic prediction of postoperative non-small cell lung cancer patients.																	0933-3657	1873-2860				JUL	2020	107								101921	10.1016/j.artmed.2020.101921													
J								Temporal matrix completion with locally linear latent factors for medical applications	ARTIFICIAL INTELLIGENCE IN MEDICINE										Temporal matrix completion; Missing data; Medical records		Regular medical records are useful for medical practitioners to analyze and monitor patient's health status especially for those with chronic disease. However, such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based models have been developed for missing data imputation in recent papers. This approach makes use of the low-rank tensor assumption for highly correlated data in a short-time interval. Nevertheless, when the time intervals are long, data correlation may not be high between consecutive time stamps so that such assumption is not valid. To address this problem, we propose to decompose matrices with missing data over time into their latent factors. Then, the locally linear constraint is imposed on the latent factors for temporal matrix completion. By using three publicly available medical datasets and two medical datasets collected from Prince of Wales Hospital in Hong Kong, experimental results show that the proposed algorithm achieves the best performance compared with state-of-the-art methods.																	0933-3657	1873-2860				JUL	2020	107								101883	10.1016/j.artmed.2020.101883													
J								Enhanced prediction of hemoglobin concentration in a very large cohort of hemodialysis patients by means of deep recurrent neural networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Anemia; Chronic kidney disease; Erythropoetin stimulating agents; Deep learning; Recurrent neural networks	CHRONIC KIDNEY-DISEASE; ANEMIA MANAGEMENT; RENAL REPLACEMENT; LEARNING APPROACH; COST; INDIVIDUALIZATION; THERAPY; ESA	Erythropoiesis Stimulating Agents (ESAs) have become a standard anemia management tool for End Stage Renal Disease (ESRD) patients. However, dose optimization constitutes an extremely challenging task due to huge inter and intra-patient variability in the responses to ESA administration. Current data-based approaches to anemia control focus on learning accurate hemoglobin prediction models, which can be later utilized for testing competing treatment choices and choosing the optimal one. These methods, despite being proven effective in practice, present several shortcomings which this paper intends to tackle. Namely, they are limited to a small cohort of patients and, even then, they fail to provide suggestions when some strict requirements are not met (such as having a three month history prior to the prediction). Here, recurrent neural networks (RNNs) are used to model whole patient histories, providing predictions at every time step since the very first day. Furthermore, an unprecedented amount of data (similar to 110,000 patients from many different medical centers in twelve countries, without exclusion criteria) was used to train it, thus allowing it to generalize for every single patient. The resulting model outperforms state-of-the-art Hemoglobin prediction, providing excellent results even when tested on a prospective dataset. Simultaneously, it allows to bring the benefits of algorithmic anemia control to a very large group of patients.																	0933-3657	1873-2860				JUL	2020	107								101898	10.1016/j.artmed.2020.101898													
J								Near-optimal insulin treatment for diabetes patients: A machine learning approach	ARTIFICIAL INTELLIGENCE IN MEDICINE											GLUCOSE; HYPOGLYCEMIA; MANAGEMENT; ADHERENCE	Blood glycemic control is crucial for minimizing severe side effects in diabetes mellitus. Currently, two opposing treatment approaches exist: in formulaic methods, insulin care is calculated by parameter-based computation (i.e., correction factor, insulin-to-carb ratio, and absorption duration), which are fixed by the medical team based on the history of a tested patient blood glucose levels (BGLs). Alternatively, closed-loop methods test glycemic level via sensors and provide insulin boluses based on sensor data thus ignoring other medical information. Unlike the body, both these systems are reactive - chasing insulin dosage based on fluctuating BGL - resulting in significant fluctuations of glucose values, rather than the relatively flat profile normal to the body's glycemic control. Extended periods of these fluctuations - particularly high BGLs (hyperglycemia) result in vascular and organ epithelial damage, which increases comorbidities and is ultimately life-threatening. We propose an individualized treatment scheme based on machine learning artificial intelligence, which combines the best of both approaches and is tailored to the individual. We model patient reaction to insulin treatment as Markov decision process (MDP) thus allowing the system to find a unique, individualized and dynamically updating insulin care policy that would lead to flat blood glucose profiles in target areas. We incorporate an individualized "health reward function", preferably from the medical team, describing a grading scheme of BGL tailored to the patient for even more precise glycemic control. The solution to MDP is found via reinforcement learning, which yields an individualized, optimal insulin care policy. This policy can prevent hypoglycemia, minimize high glucose duration and glycemic fluctuations. It can be further updated as the patient undergoes environmental changes. Significantly, our method provides the care team a constantly updated patient model, allowing them to better understand and support the patient.																	0933-3657	1873-2860				JUL	2020	107								101917	10.1016/j.artmed.2020.101917													
