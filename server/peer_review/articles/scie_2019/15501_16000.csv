PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Aesthetic product design combining with rough set theory and fuzzy quality function deployment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Product design; customer satisfaction; rough set theory; fuzzy quality function deployment; Blender	SERVICE QUALITY; FRAMEWORK; MODEL; SYSTEM; QFD	The emotional value of products, especially aesthetic qualities are conducive to increasing the market competitiveness along with the improvement of living standards. Therefore, the main purpose of this study is to combine the rough set theory and fuzzy quality function deployment design matrix to construct an innovative model, thus developing an aesthetic product design for customer satisfaction. Taking Blender as an example, the author divides this paper into three phases. Firstly, the author summarizes seven aesthetic qualities through the literature discussions and determines the core aesthetic qualities of Blender by making rough set theory attribute reduction and importance calculation. Secondly, the results are imported into the fuzzy quality function deployment needs facet, and the correlation matrix is established by consulting the expert's opinions to obtain the optimal combination of design features. Finally, the combination of the features produces a bio-conceptual shape via a bionics step. The results show that four out of seven aesthetic attributes (ie, concise, original, elegant, and comfortable) are found to be more significant. The optimal combination of product features are integrated with the bio-inspired method to generate three design solutions, in which the styling of butterfly concept effectively enhances the products' emotional value and customers' aesthetic satisfaction degree.																	1064-1246	1875-8967					2020	39	1					1131	1146		10.3233/JIFS-192032													
J								Aircraft fleet route optimization based on cost and low carbon emission in aviation line alliance network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Aircraft fleet route optimization; multi-airbases stochastic; time and capacity constraints; improved NSGA-II algorithm		This paper studies the flight path optimization problem of air cargo companies in aviation line alliance. There are two limitations in this paper. One is to limit of the number and location of airbases and capacity in the air network. The other is to limit of flight time and airspace capacity of full cargo aircraft in actual operation. Considering the influence of alliance on operation, the selection probability of air alliance is introduced. It is assuming that all cargo aircraft is one type, the unit transportation cost of every aviation line is the same as each other, the queuing problem of aircraft landing is not considered, and the network transportation demand of itself must be completed by an airline. It proposes a directed aircraft fleet routing problem optimization model (SMDDDAAAFRPTW) with multi-airbase stochastic and time constraints to minimize total operating cost and flight distance. Using the multi-objective optimization algorithm NSGA-II by most scholars, and improving the initial solution generation step, introducing Genetic engineering into cross-mutation to solve the optimal number and location of air bases and fleet routing of multiple aircraft. Comparing with the weighted method and ant colony algorithm, it shows that the improved NSGA-II algorithm is effective and has better computational efficiency. The results show that the more segments are selected for outsourcing, the lowest cost of network and the lowest carbon emission. This kind of decision-making behavior is only suitable for the initial operation phase of the enterprise.																	1064-1246	1875-8967					2020	39	1					1163	1182		10.3233/JIFS-192041													
J								On the rectifying multiple deferred state plan in the presence of uncertain parameter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Statistical quality control; fuzzy multiple deferred state sampling plan; average total inspection; fuzzy numbers arithmetic	SAMPLING PLAN; INSPECTION; DESIGN	The quality of manufactured products plays a very important role in increasing consumer satisfaction. One approach to improving outgoing lot quality is applying screening method. In this paper, a multiple deferred state sampling plan is presented for a unilateral-univariate normal process with imprecise process quality in the presence of the rectifying inspection. To assess the performance of the proposed plan, a mathematical model is derived for calculating the fuzzy average total inspection ((ATI) over tilde) under the operation of the proposed plan. The obtained conclusions indicate that the proposed plan is more economical than the existing plans in terms of (ATI) over tilde measure. A numerical example is given to demonstrate how to apply the introduced plan in the real world.																	1064-1246	1875-8967					2020	39	1					1197	1211		10.3233/JIFS-192097													
J								Double-weighted neighborhood standardization method with applications to multimode-process fault detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										multimode process; double-weighted neighborhood standardization; principal component analysis; fault detection	PRINCIPAL COMPONENT ANALYSIS; MULTIPLE OPERATING MODES; DIAGNOSIS	As modern industrial processes often have multiple production modes, multimode-process monitoring has become an important issue. In multimode processes, the operating condition may often switch among different modes. As a result, popular process monitoring methods such as principal component analysis (PCA) and partial least squares (PLS) method should not be directly applied because they are based on a fundamental assumption that the process only has one stable operating condition. In this paper, a novel multimode-process data-standardization approach called double-weighted neighborhood standardization (DWNS) is proposed to solve the problem of multimode characteristics. This approach can transform multimode data into approximately single-mode data, which follow a Gaussian distribution. By analyzing a concrete example, this study indicates that the DWNS strategy is effective for multimode data preprocessing. Moreover, a novel fault detection method called DWNS-PCA is proposed for multimode processes. Finally, a numerical example and the penicillin fermentation process are used to test the validity and effectiveness of the DWNS-PCA. The results demonstrate that the proposed data-standardization method is suitable for multimode data, and the DWNS-PCA process monitoring method is effective for detecting faults in multimode processes.																	1064-1246	1875-8967					2020	39	1					1243	1256		10.3233/JIFS-192158													
J								CC-CSA: A culture&chaos-inspired clonal selection algorithm for abnormal detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial immune system; clonal selection algorithm; culture algorithm; chaos mechanism; abnormal detection	SEARCH	The clonal selection algorithm(CSA) is a core method in artificial immune system, which is famous for its intelligent evolution in artificial intelligence application. However, There are some shortcomings in the algorithm, such as local optima and low convergence speed, which make its practical effects not ideal. Culture algorithm(CA) is driven by knowledge, which can significantly improve the evolutionary efficiency. Chaos mechanism can make the algorithm have better problem space coverage ability. Therefore, a culture&chaos-inspired CSA(CC-CSA) is proposed in this paper to deal with the problems mentioned before. CC-CSA adopts the double-layer evolutionary framework of CA to extract knowledge and guide the crossover and chaotic mutation operation to complete the evolution process. The implicit knowledge is used to adaptively control the chaotic mutation scale, guide the individuals to jump out of the local optima, and realize the accurate search in the latter evolution cycle to gradually approach the optimal solution. It can be seen from the mathematical model analysis that CC-CSA can converge to the global optimal solution. Compared with the experimental results of the original CSA and its representative, up-to-date improved methods, CC-CSA has the fastest convergence speed and the best detection performances. It is also proved that CC-CSA can solve the problems of local optima and slow convergence speed by using the knowledge guidance of CA's double-layer framework and good coverage ability of chaos mechanism to the problem space.																	1064-1246	1875-8967					2020	39	1					1289	1301		10.3233/JIFS-192188													
J								Multi-criteria fuzzy comprehensive evaluation in interval environment with dual preferences	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Aggregation operators; evaluation; information fusion; multi-criteria decision making; weights adjustment	WEIGHTED AVERAGING AGGREGATION; LINGUISTIC DECISION-MAKING; OPERATORS	This work firstly proposes some weight adjusting and preference interfering methods to generate more suitable weight vector in two-tier multi-criteria decision making. The proposed models simultaneously consider the original weight information and subjective preferences of decision makers under interval numbers based evaluation environments. A recently proposed weights allocation method based on convex poset is applied to determine the weight vectors from subjective preferences. With well adjusted and melted weight information, some fuzzy comprehensive evaluations are realized by applying Shilkret Integrals with melted preferences. A numerical example with corresponding decision rules for online shop evaluation problem is also presented for practitioners to refer to.																	1064-1246	1875-8967					2020	39	1					1361	1369		10.3233/JIFS-200123													
J								A novel method to derive the intuitionistic multiplicative priority vector for the intuitionistic multiplicative preference relation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic multiplicative preference relation; consistency; intuitionistic multiplicative priority vector	GROUP DECISION-MAKING; MODELS	Consistency is related with reasonableness of the priority vector derived from a preference relation. In this paper, it is pointed out by an example that the existing consistency for the intuitionistic multiplicative preference relations (IMPR) is weak that the ranking or the optimal alternative could not always be derived from the given consistent IMPR. We provide a novel consistency for the IMPRs by the score function and accuracy function and characterize it with the S-normalized and A-normalized intuitionistic multiplicative priority vectors (IMPV). Then, we propose methods to check and reach the S-normalization, the acceptable consistency of the IMPR by its local IMPVs. We also give some examples to show how the proposed methods work and make comparisons with the existing methods to demonstrate the advantages and disadvantages of the proposed methods.																	1064-1246	1875-8967					2020	39	1					1371	1380		10.3233/JIFS-200128													
J								Dimensionality reduction of tensor based on subspace learning and local homeomorphism	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Tensors; dimensionality reduction; subspace learning; local homeomorphism	DECOMPOSITIONS	In this paper we address the problem of dimensionality reduction of tensor data. There are three contributions in this paper. Local Homeomorphism is the intrinsic mathematical feature of manifolds and the basis of many manifold learning algorithms. However, these algorithms are developed for vector data, not suitable for tensor data. Our first contribution is to derive a tensor version of dimensionality reduction based on local homeomorphism. Tucker decomposition is widely used in dimensionality reduction of tensor data. However, Tucker decomposition without any regularization is actually a traditional subspace learning problem. Our second contribution is to propose a local homeomorphism regularized Tucker decomposition and applies it to dimensionality reduction of tensor data, called dimensionality reduction of tensor data based on subspace learning and homeomorphism, SLLH for short. As far as dimensionality reduction is concerned, only the core tensor in Tucker decomposition is the target, while the mode product matrices are only by-products. Therefore, many algorithms absorb all these mode product matrices into a big matrix by using the conversion theorem of tensor algebra. However, in Tucker decomposition, each mode product matrix represents dimensionality reduction for a specific dimension of tensor. Our third contribution is to propose an iterative solution method for SLLH, in which each mode product matrix of the current iteration is calculated from other mode product matrices and the core tensor of the previous iteration. The core tensor is evolved iteratively from the iteratively-calculated mode product matrices. The experimental results presented in this paper show that the proposed SLLH outperforms many of the state-of-the-art algorithms.																	1064-1246	1875-8967					2020	39	1					1391	1405		10.3233/JIFS-200182													
J								A Data-Driven Model-Based Regression Applied to Panchromatic Sharpening	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multivariate linear regression; injection models; pansharpening; image fusion; remote sensing	IMAGE FUSION TECHNIQUE; SPARSE REPRESENTATION; INTENSITY MODULATION; PAN; DECOMPOSITION; ENHANCEMENT; TRANSFORM; MS	Image fusion is growing interest in recent years, thanks to the huge amount of data acquired everyday by sensors on board of satellite platforms. The enhancement of the spatial resolution of a multispectral (MS) image through the use of a panchromatic (PAN) image, usually called pansharpening, is getting more and more relevant. In this work, we focus on the problem of the estimation of the injection coefficients that rule the enhancement of the spatial resolution of the MS image by properly adding the PAN details. In particular, a statistical analysis of the residuals coming from the linear multivariate regression between details extracted from the PAN image and the MS image is performed. A novel hybrid model is introduced for accurately describing the statistical distribution of these residuals, together with a procedure for efficiently estimating both the parameters of the residual distribution and the injection coefficients. The improvements achieved by the proposed approach are assessed using two very high resolution datasets acquired by the WorldView-3 and Worldview-4 satellites. The benefits of the proposed approach are particularly clear when vegetated areas are involved in the fusion process.																	1057-7149	1941-0042					2020	29						7779	7794		10.1109/TIP.2020.3007824													
J								Mutual Context Network for Jointly Estimating Egocentric Gaze and Action	IEEE TRANSACTIONS ON IMAGE PROCESSING										Gaze prediction; egocentric video; action recognition	ACTIVITY RECOGNITION; EYE-MOVEMENTS; VISION; MEMORY	In this work, we address two coupled tasks of gaze prediction and action recognition in egocentric videos by exploring their mutual context: the information from gaze prediction facilitates action recognition and vice versa. Our assumption is that during the procedure of performing a manipulation task, on the one hand, what a person is doing determines where the person is looking at. On the other hand, the gaze location reveals gaze regions which contain important and information about the undergoing action and also the non-gaze regions that include complimentary clues for differentiating some fine-grained actions. We propose a novel mutual context network (MCN) that jointly learns action-dependent gaze prediction and gaze-guided action recognition in an end-to-end manner. Experiments on multiple egocentric video datasets demonstrate that our MCN achieves state-of-the-art performance of both gaze prediction and action recognition. The experiments also show that action-dependent gaze patterns could be learned with our method.																	1057-7149	1941-0042					2020	29						7795	7806		10.1109/TIP.2020.3007841													
J								An Object Context Integrated Network for Joint Learning of Depth and Optical Flow	IEEE TRANSACTIONS ON IMAGE PROCESSING										Joint learning; deep neural network; depth estimation; optical flow estimation; object context		Supervised depth prediction and optical flow estimation have achieved promising performance due to the advanced deep network architectures. Since the ground truths are difficult to be collected, many recent works try to learn the depth and flow in an unsupervised manner. However, existing methods only use features from convolutional layers or a simple aggregation of multi-level features to predict the depth and flow maps, which is insufficient to exploit context information. In this paper, we attempt to exploit object contextual information and investigate the effect of the object context for joint learning of depth and optical flow. Specifically, we present a novel combination of object context and the framework of joint learning depth and optical flow. Our proposed network can exploit and integrate the object context for both tasks by aggregating the context according to pair-wise similarities. Furthermore, we adopt the existing spatial pyramid network (SPN) to estimate the depth and flow in a coarse-to-fine strategy effectively. Given temporally adjacent stereo pairs, our network can be trained end-to-end in an unsupervised manner and can predict the depth and flow maps simultaneously. We conduct experiments on two publicly available datasets, KITTI2012 and KITTI2015. Our proposed approach yields comparable performance on both depth and flow tasks, compared to the recent deep learning-based approaches. Experimental results demonstrate that exploiting object contextual information is useful and beneficial for depth and optical flow estimation.																	1057-7149	1941-0042					2020	29						7807	7818		10.1109/TIP.2020.3007843													
J								End-to-End Single Image Fog Removal Using Enhanced Cycle Consistent Adversarial Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Single image defogging; cycleGAN; unpaired training; image restoration	QUALITY ASSESSMENT; VISIBILITY	Single image defogging is a classical and challenging problem in computer vision. Existing methods towards this problem mainly include handcrafted priors based methods that rely on the use of the atmospheric degradation model and learning-based approaches that require paired fog-fogfree training example images. In practice, however, prior-based methods are prone to failure due to their own limitations and paired training data are extremely difficult to acquire. Moreover, there are few studies on the unpaired trainable defogging network in this field. Thus, inspired by the principle of CycleGAN network, we have developed an end-to-end learning system that uses unpaired fog and fogfree training images, adversarial discriminators and cycle consistency losses to automatically construct a fog removal system. Similar to CycleGAN, our system has two transformation paths; one maps fog images to a fogfree image domain and the other maps fogfree images to a fog image domain. Instead of one stage mapping, our system uses a two stage mapping strategy in each transformation path to enhance the effectiveness of fog removal. Furthermore, we make explicit use of prior knowledge in the networks by embedding the atmospheric degradation principle and a sky prior for mapping fogfree images to the fog images domain. In addition, we also contribute the first real world nature fog-fogfree image dataset for defogging research. Our multiple real fog images dataset (MRFID) contains images of 200 natural outdoor scenes. For each scene, there is one clear image and corresponding four foggy images of different fog densities manually selected from a sequence of images taken by a fixed camera over the course of one year. Qualitative and quantitative comparison against several state-of-the-art methods on both synthetic and real world images demonstrate that our approach is effective and performs favorably for recovering a clear image from a foggy image.																	1057-7149	1941-0042					2020	29						7819	7833		10.1109/TIP.2020.3007844													
J								In AI we trust? Perceptions about automated decision-making by artificial intelligence	AI & SOCIETY										Automated decision-making; Artificial intelligence; Algorithmic fairness; Algorithmic appreciation; User perceptions	FAIRNESS	Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research aboutalgorithmic appreciationand algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluatedon paror evenbetterthan human experts for specific decisions. Theoretical and societal implications about these findings are discussed.																	0951-5666	1435-5655				SEP	2020	35	3					611	623		10.1007/s00146-019-00931-w		JAN 2020											
J								The mediator role of robot anxiety on the relationship between social anxiety and the attitude toward interaction with robots	AI & SOCIETY										Attitude; Interaction; Robots; Robot anxiety; Social anxiety	NEGATIVE ATTITUDES; BEHAVIOR	Robots that can communicate with people are one of the goals reached by the technology developed for automation in work life. Experts aim to improve the communication skills of these robots further in the near future. Besides, various studies emphasize that people may interact with robots in a similar way as they interact with other people. In line of this idea, this study examines the possible causal chain in which the social anxiety affects the robot anxiety which in turn affects the attitude toward interacting with robots. Data obtained from university students were analyzed in a simple and parallel mediation model. The results showed that robot anxiety and in particular two of its sub-dimensions mediate the relationship between social anxiety and negative attitudes toward interaction with robots. Researchers should carry out new studies about the common structural characteristics of the anxiety felt by people due to interacting with humans and robots.																	0951-5666	1435-5655															10.1007/s00146-019-00933-8		JAN 2020											
J								Optimal feature selection through a cluster-based DT learning (CDTL) in heart disease prediction	EVOLUTIONARY INTELLIGENCE										Classification; Machine learning; Heart disease; Support vector machine; Random forest	DECISION-SUPPORT-SYSTEM; NEURAL-NETWORK; DIAGNOSIS; ALGORITHM	In the rural side, there is the absence of centers for cardiovascular ailment. Due to this, around 12 million people passing worldwide reported by WHO. The principal purpose of coronary illness is a propensity for smoking. ML classifiers are applied to predict the risk of cardiovascular disease. However, the ML model has some inherent problems like it's serene to feature selection, splitting attribute, and imbalanced datasets prediction. Most of the mass datasets have multi-class labels, but their combinations are in different proportions. In this paper, we experiment with our system with Cleveland's heart samples from the UCI repository. Our cluster-based DT learning (CDTL) mainly includes five key stages. At first, the original set has partitioned through target label distribution. From the high distribution samples, the other possible class combination has made. For each class-set combination, the significant features have identified through entropy. With the significant critical features, an entropy-based partition has made. At last, on these entropy clusters, RF performance is made through significant and all features in the prediction of heart disease. From our CDTL approach, the RF classifier achieves 89.30% improved prediction accuracy from 76.70% accuracy (without CDTL). Hence, the error rate of RF with CDTL has significantly reduced from 23.30 to 9.70%.																	1864-5909	1864-5917															10.1007/s12065-019-00336-0		JAN 2020											
J								Improving mass discrimination in mammogram-CAD system using texture information and super-resolution reconstruction	EVOLVING SYSTEMS										Classification; Mammograms; Pattern recognition; Super-resolution; Texture features	COMPUTER-AIDED DETECTION; NEURAL-NETWORK; SCREENING MAMMOGRAPHY; DIAGNOSIS SYSTEM; CLASSIFICATION; ACCURACY; SENSITIVITY; IMAGES; SEGMENTATION; EXTRACTION	Screening helps to reduce mortality in the breast cancers. Mammography is a screening procedure used to detect breast cancer at an early stage. Computer-aided detection (CAD) systems can help in mammograms examination. Automatic differentiation between benign and malignant mammographic masses is a challenging task, due to high variability in mass structures. That is why, CAD systems frequently misdiagnose breast cancer. This paper presents a new CAD approach for mass detection in digital mammograms. The purpose of the proposed approach is to improve the discrimination between benign mass and malignant mass by reinforcing their statistics texture features. To achieve this aim, a new step based on super-resolution reconstruction is added to multistage CAD system. The proposed approach gives very good results comparing to other recent works. It achieves 96.7% classification accuracy using the MIAS (Mammography Image Analysis Society) dataset. This work shows that a super-resolution based approach improves the performance of the evaluated texture methods and thus outperforms benign/malignant mass classification for digital mammograms.																	1868-6478	1868-6486				DEC	2020	11	4					697	706		10.1007/s12530-019-09322-4		JAN 2020											
J								Real-Time Quality Assessment of Pediatric MRI via Semi-Supervised Deep Nonlocal Residual Neural Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Training; Semisupervised learning; Standards; Quality assessment; Annotations; Deep learning; Kernel; Image quality assessment; nonlocal residual networks; semi-supervised learning; self-training	CLASSIFICATION; ARTIFACTS; NOISE	In this paper, we introduce an image quality assessment (IQA) method for pediatric T1- and T2-weighted MR images. IQA is first performed slice-wise using a nonlocal residual neural network (NR-Net) and then volume-wise by agglomerating the slice QA results using random forest. Our method requires only a small amount of quality-annotated images for training and is designed to be robust to annotation noise that might occur due to rater errors and the inevitable mix of good and bad slices in an image volume. Using a small set of quality-assessed images, we pre-train NR-Net to annotate each image slice with an initial quality rating (i.e., pass, questionable, fail), which we then refine by semi-supervised learning and iterative self-training. Experimental results demonstrate that our method, trained using only samples of modest size, exhibit great generalizability, capable of real-time (milliseconds per volume) large-scale IQA with near-perfect accuracy.																	1057-7149	1941-0042					2020	29						7697	7706		10.1109/TIP.2020.2992079													
J								Double Graph Regularized Double Dictionary Learning for Image Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Class-specific information; class-shared information; double graph regularization; double dictionary learning; image classification; similarity preserving	DISCRIMINATIVE DICTIONARY; SPARSE REPRESENTATION; FACE RECOGNITION; LOW-RANK; SHARED DICTIONARY; MODELS	In this paper, we present a novel double graph regularized double dictionary learning (DGRDDL) method for image classification. The proposed method jointly constructs a number of class-specific sub-dictionaries to capture the most discriminative features (class-specific information) of each class, and a class-shared dictionary to model the common patterns (class-shared information) shared by the images from different classes. A novel double graph regularization is proposed to correctly represent and differentiate these two types of information. Specifically, an intra-class similarity graph constraint is imposed on the representation coefficients over the class-specific dictionaries, and an inter-class similarity graph constraint is applied on the representation coefficients over the class-shared dictionary. In this way, the representations learned by the proposed DGRDDL method can correctly model the local similarity relationships of the class-specific and the class-shared information in images, respectively. Moreover, due to the differences between the intra-class and inter-class similarity graphs, the two types of information can be appropriately separated and captured by the learned dictionaries. We evaluate the performance of the proposed method on six public datasets and compared against those of seven benchmark methods. The experimental results demonstrate the effectiveness and superiority of the proposed method in image classification over the benchmark dictionary learning methods.																	1057-7149	1941-0042					2020	29						7707	7721		10.1109/TIP.2020.3004246													
J								Bag of Color Features for Color Constancy	IEEE TRANSACTIONS ON IMAGE PROCESSING										Color constancy; illumination estimation; bag of features; attention mechanism	CHROMATICITY	In this paper, we propose a novel color constancy approach, called Bag of Color Features (BoCF), building upon Bag-of-Features pooling. The proposed method substantially reduces the number of parameters needed for illumination estimation. At the same time, the proposed method is consistent with the color constancy assumption stating that global spatial information is not relevant for illumination estimation and local information (edges, etc.) is sufficient. Furthermore, BoCF is consistent with color constancy statistical approaches and can be interpreted as a learning-based extension of many statistical approaches. To further improve the illumination estimation accuracy, we propose a novel attention mechanism for the BoCF model with two variants based on self-attention. BoCF approach and its variants achieve competitive, compared to the state of the art, results while requiring much fewer parameters on three benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.																	1057-7149	1941-0042					2020	29						7722	7734		10.1109/TIP.2020.3004921													
J								Image Restoration Using Joint Patch-Group-Based Sparse Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Sparse representation; JPG-SR; nonlocal self-similarity; image restoration; ADMM	K-SVD; TRANSFORM; RECONSTRUCTION; ARTIFACTS; DCT; REDUCTION; ALGORITHM; RECOVERY	Sparse representation has achieved great success in various image processing and computer vision tasks. For image processing, typical patch-based sparse representation (PSR) models usually tend to generate undesirable visual artifacts, while group-based sparse representation (GSR) models lean to produce over-smooth effects. In this paper, we propose a new sparse representation model, termed joint patch-group based sparse representation (JPG-SR). Compared with existing sparse representation models, the proposed JPG-SR provides an effective mechanism to integrate the local sparsity and nonlocal self-similarity of images. We then apply the proposed JPG-SR to image restoration tasks, including image inpainting and image deblocking. An iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is developed to solve the proposed JPG-SR based image restoration problems. Experimental results demonstrate that the proposed JPG-SR is effective and outperforms many state-of-the-art methods in both objective and perceptual quality.																	1057-7149	1941-0042					2020	29						7735	7750		10.1109/TIP.2020.3005515													
J								SABER: A Systems Approach to Blur Estimation and Reduction in X-Ray Imaging	IEEE TRANSACTIONS ON IMAGE PROCESSING										Blur; deblur; optimization; algorithm; radiography; tomography; source blur; detector blur; motion blur; deconvolution; model estimation; high resolution	RADIOGRAPHY; MODEL; SPOT; RECONSTRUCTION; ALGORITHM; SIZE	Blur in X-ray radiographs not only reduces the sharpness of image edges but also reduces the overall contrast. The effective blur in a radiograph is the combined effect of blur from multiple sources such as the detector panel, X-ray source spot, and system motion. In this paper, we use a systems approach to model the point spread function (PSF) of the effective radiographic blur as the convolution of multiple PSFs, where each PSF models one of the various sources of blur. In particular, we model the combined contribution of X-ray source and detector blurs while assuming negligible contribution from other forms of blur. Then, we present a numerical optimization algorithm for estimating the source and detector PSFs from multiple radiographs acquired at different X-ray source to object (SOD) and object to detector distances (ODD). Finally, we computationally reduce blur in radiographs using deblurring algorithms that use the estimated PSFs from the previous step. Our approach to estimate and reduce blur is called SABER, which is an acronym for systems approach to blur estimation and reduction.																	1057-7149	1941-0042					2020	29						7751	7764		10.1109/TIP.2020.3006339													
J								DID: Disentangling-Imprinting-Distilling for Continuous Low-Shot Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; low-shot learning; continuous learning; deep learning; transfer learning		Practical applications often face a challenging continuous low-shot detection scenario, where a target detection task only has a few annotated training images, and a number of such new tasks come in sequence. To address this challenge, we propose a generic detection scheme via Disentangling-Imprinting-Distilling (DID). DID can leverage delicate transfer insights into the main development flow of deep learning, i.e., architecture design (Disentangling), model initialization (Imprinting), and training methodology (Distilling). This allows DID to be a simple but effective solution for continuous low-shot detection. In addition, DID can integrate the supervision from different detection tasks into a progressive learning procedure. As a result, one can efficiently adapt the previous detector for a new low-shot task, while maintaining the learned detection knowledge in the history. Finally, we evaluate our DID on a number of challenging settings in continuous/incremental low-shot detection. All the results demonstrate that our DID outperforms the recent state-of-the-art approaches. The code and models are available at https://github.com/chenxy99/DID.																	1057-7149	1941-0042					2020	29						7765	7778		10.1109/TIP.2020.3006397													
J								Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization	JOURNAL OF MACHINE LEARNING RESEARCH										Stochastic optimization; conditional gradient methods; convex minimization; submodular maximization; gradient averaging; Frank-Wolfe algorithm; greedy algorithm	ALGORITHMS	This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is computationally costly. Instead, stochastic conditional gradient algorithms are proposed as an alternative solution which rely on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction. The gradient averaging technique reduces the noise of gradient approximations as time progresses, and replacing projection step in proximal methods by a linear program lowers the computational complexity of each iteration. We show that under convexity and smoothness assumptions, our proposed stochastic conditional gradient method converges to the optimal objective function value at a sublinear rate of O(1/t(1/3)). Further, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that our proposed method achieves a ((1-1/e)OPT-epsilon) guarantee (in expectation) with O(1/epsilon(3)) stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. Additionally, we achieve ((1/e)OPT-epsilon) guarantee after operating on O(1/epsilon(3)) stochastic gradients for the case that the objective function is continuous DR-submodular but non-monotone and the constraint set is a down-closed convex body. By using stochastic continuous optimization as an interface, we also provide the first (1-1/e) tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint and (1/e) approximation guarantee for the non-monotone case.																	1532-4435						2020	21						1	49															
J								AI-Toolbox: A C plus plus library for Reinforcement Learning and Planning (with Python Bindings)	JOURNAL OF MACHINE LEARNING RESEARCH										MDP; POMDP; multiagent; reinforcement learning; software; open-source	APPROXIMATIONS	This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality.																	1532-4435						2020	21																						
J								Stochastic Nested Variance Reduction for Nonconvex Optimization	JOURNAL OF MACHINE LEARNING RESEARCH										Nonconvex Optimization; Finding Local Minima; Variance Reduction		We study nonconvex optimization problems, where the objective function is either an average of n nonconvex functions or the expectation of some stochastic function. We propose a new stochastic gradient descent algorithm based on nested variance reduction, namely, Stochastic Nested Variance-Reduced Gradient descent (SNVRG). Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K 1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, SNVRG converges to an 6-approximate first-order stationary point within O (n A 6-2 + 63 A n1/262)1 number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG 0(n+n2/36-2) and that of SCSG 0(n A 6-2 +6-1 /3 A n2/36-2). For gradient dominated functions, SNVRG also achieves better gradient complexity than the state-of-the-art algorithms. Based on SNVRG, we further propose two algorithms that can find local minima faster than state-of-the-art algorithms in both finite-sum and general stochastic (online) nonconvex optimization. In particular, for finite-sum optimization problems, the proposed SNVRG Neon2finite algorithm achieves o(n1/26-2 n3/46H7/2) gradient complexity to converge to an (6, 6H) -second-order stationary point, which outperforms SVRG+Neon2fin'te (Allen-Zhu and Li, 2018), the best existing algorithm, in a wide regime. For general stochastic optimization problems, the proposed SNVRG Neon2'nlmne achieves 6(6-3 + 6H-5 +6-26H-3) gradient complexity, which is better than both SVRG Neon2'1' (Allen-Zhu and Li, 2018) and Natasha2 (Allen-Zhu, 2018a) in certain regimes. Thorough experimental results on different nonconvex optimization problems back up our theory.																	1532-4435						2020	21																						
J								An ILP Model for Multi-Label MRFs With Connectivity Constraints	IEEE TRANSACTIONS ON IMAGE PROCESSING										Computational modeling; Optimization; Semantics; Computer vision; Machine learning; Labeling; Particle separators; Image segmentation; integer programming; Markov random fields	MARKOV RANDOM-FIELDS; ENERGY MINIMIZATION	Integer Linear Programming (ILP) formulations of multi-label Markov random fields (MRFs) models with global connectivity priors were investigated previously in computer vision. In these works, only Linear Programming (LP) relaxations [1] or simplified versions [2] of the problem were solved. This paper investigates the ILP of MRF with exact connectivity priors via a branch-and-cut method, which provably finds globally optimal solutions. It enforces connectivity priors iteratively by a cutting plane method, and provides feasible solutions with a guarantee on sub-optimality even if we terminate it earlier. The proposed ILP can be applied as a post-processing method on top of any existing multi-label segmentation approach. As it provides globally optimal solution, it can be used off-line to serve as quality check for any fast on-line algorithm. Furthermore, the scribble based model presented in this paper could be potentially used to generate ground-truth proposals for any deep learning based segmentation. We demonstrate the power and usefulness of our model by extensive experiments on the BSDS500 and PASCAL VOC dataset. The experiments show that our proposed model achieves great performance, yielding provably global optimum in most instances and that provably good optimization solutions also provide good segmentation accuracy, even with the limited computing time of few seconds.																	1057-7149	1941-0042					2020	29						6909	6917		10.1109/TIP.2020.2995056													
J								Fast and Accurate Tensor Completion With Total Variation Regularized Tensor Trains	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tensor completion; tensor train decomposition; total variation; image restoration	ALTERNATING LINEAR SCHEME; FACTORIZATION; OPTIMIZATION; FRAMEWORK; MATRICES	We propose a new tensor completion method based on tensor trains. The to-be-completed tensor is modeled as a low-rank tensor train, where we use the known tensor entries and their coordinates to update the tensor train. A novel tensor train initialization procedure is proposed specifically for image and video completion, which is demonstrated to ensure fast convergence of the completion algorithm. The tensor train framework is also shown to easily accommodate Total Variation and Tikhonov regularization due to their low-rank tensor train representations. Image and video inpainting experiments verify the superiority of the proposed scheme in terms of both speed and scalability, where a speedup of up to 155x is observed compared to state-of-the-art tensor completion methods at a similar accuracy. Moreover, we demonstrate the proposed scheme is especially advantageous over existing algorithms when only tiny portions (say, 1%) of the to-be-completed images/videos are known.																	1057-7149	1941-0042					2020	29						6918	6931		10.1109/TIP.2020.2995061													
J								Few-Shot Text Style Transfer via Deep Feature Similarity	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Rendering (computer graphics); Gallium nitride; Image color analysis; Generative adversarial networks; Task analysis; Painting; Few-shot; deep similarity; character content; text style transfer; discriminative network		Generating text to have a consistent style with only a few observed highly-stylized text samples is a difficult task for image processing. The text style involving the typography, i.e., font, stroke, color, decoration, effects, etc., should be considered for transfer. In this paper, we propose a novel approach to stylize target text by decoding weighted deep features from only a few referenced samples. The deep features, including content and style features of each referenced text, are extracted from a Convolutional Neural Network (CNN) that is optimized for character recognition. Then, we calculate the similarity scores of the target text and the referenced samples by measuring the distance along the corresponding channels from the content features of the CNN when considering only the content, and assign them as the weights for aggregating the deep features. To enforce the stylized text to be realistic, a discriminative network with adversarial loss is employed. We demonstrate the effectiveness of our network by conducting experiments on three different datasets which have various styles, fonts, languages, etc. Additionally, the coefficients for character style transfer, including the character content, the effect of similarity matrix, the number of referenced characters, the similarity between characters, and performance evaluation by a new protocol are analyzed for better understanding our proposed framework.																	1057-7149	1941-0042					2020	29						6932	6946		10.1109/TIP.2020.2995062													
J								Dehazing Evaluation: Real-World Benchmark Datasets, Criteria, and Baselines	IEEE TRANSACTIONS ON IMAGE PROCESSING										Measurement; Benchmark testing; Indexes; Image restoration; Distortion; Image quality; Reliability; Benchmark dataset; dehazing evaluation metrics; dehazing baselines; FR-IQA	IMAGE QUALITY ASSESSMENT; COLOR; SIMILARITY; VISIBILITY	On benchmark images, modern dehazing methods are able to achieve very comparable results whose differences are too subtle for people to qualitatively judge. Thus, it is imperative to adopt quantitative evaluation on a vast number of hazy images. However, existing quantitative evaluation schemes are not convincing due to a lack of appropriate datasets and poor correlations between metrics and human perceptions. In this work, we attempt to address these issues, and we make two contributions. First, we establish two benchmark datasets, i.e., the BEnchmark Dataset for Dehazing Evaluation (BeDDE) and the EXtension of the BeDDE (exBeDDE), which had been lacking for a long period of time. The BeDDE is used to evaluate dehazing methods via full reference image quality assessment (FR-IQA) metrics. It provides hazy images, clear references, haze level labels, and manually labeled masks that indicate the regions of interest (ROIs) in image pairs. The exBeDDE is used to assess the performance of dehazing evaluation metrics. It provides extra dehazed images and subjective scores from people. To the best of our knowledge, the BeDDE is the first dehazing dataset whose image pairs were collected in natural outdoor scenes without any simulation. Second, we provide a new insight that dehazing involves two separate aspects, i.e., visibility restoration and realness restoration, which should be evaluated independently; thus, to characterize them, we establish two criteria, i.e., the visibility index (VI) and the realness index (RI), respectively. The effectiveness of the criteria is verified through extensive experiments. Furthermore, 14 representative dehazing methods are evaluated as baselines using our criteria on BeDDE. Our datasets and relevant code are available at https://github.com/xiaofeng94/BeDDE-for-defogging.																	1057-7149	1941-0042					2020	29						6947	6962		10.1109/TIP.2020.2995264													
J								Learning Recurrent 3D Attention for Video-Based Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Feature extraction; Robustness; Learning (artificial intelligence); Optical imaging; Adaptation models; Aggregates; Person re-identification; 3D attention; reinforcement learning; recurrent model	NETWORK	In this paper, we propose to learn recurrent 3D attention (A3D) for video-based person re-identification. Attention model plays a key role in both spatial and temporal domains for video representation. Most existing methods apply spatial attention model to extract feature from a single image and aggregate image features with attentive temporal pooling or RNN. However, the inherent consistencies and correlations between spatial and temporal clues are not leveraged. Our A3D method aims to utilize the joint constraints of temporal and spatial attentions to enhance the robustness of attention model. Towards this goal, we treat the pedestrian video as a unified 3D bin where the temporal domain is denoted as an additional dimension. Then we develop an attention agent to iteratively select the locations of the salient spatial-temporal parts in the 3D bin. In addition, we formulate our sequential 3D attention learning as a Markov Decision Process and train the representation network and attention detector with the policy gradient method in an end-to-end manner. We evaluate the proposed method on three challenging datasets including iLIDS-VID, PRID-2011 and the large-scale MARS dataset, and consistently improve the performance in comparison with the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						6963	6976		10.1109/TIP.2020.2995272													
J								Multi-Modal Recurrent Attention Networks for Facial Expression Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face recognition; Image color analysis; Videos; Emotion recognition; Benchmark testing; Databases; Task analysis; Multi-modal facial expression recognition; dimensional (continuous) emotion recognition; attention mechanism	DATABASE; EMOTION	Recent deep neural networks based methods have achieved state-of-the-art performance on various facial expression recognition tasks. Despite such progress, previous researches for facial expression recognition have mainly focused on analyzing color recording videos only. However, the complex emotions expressed by people with different skin colors under different lighting conditions through dynamic facial expressions can be fully understandable by integrating information from multi-modal videos. We present a novel method to estimate dimensional emotion states, where color, depth, and thermal recording videos are used as a multi-modal input. Our networks, called multi-modal recurrent attention networks (MRAN), learn spatiotemporal attention volumes to robustly recognize the facial expression based on attention-boosted feature volumes. We leverage the depth and thermal sequences as guidance priors for color sequence to selectively focus on emotional discriminative regions. We also introduce a novel benchmark for multi-modal facial expression recognition, termed as multi-modal arousal-valence facial expression recognition (MAVFER), which consists of color, depth, and thermal recording videos with corresponding continuous arousal-valence scores. The experimental results show that our method can achieve the state-of-the-art results in dimensional facial expression recognition on color recording datasets including RECOLA, SEWA and AFEW, and a multi-modal recording dataset including MAVFER.																	1057-7149	1941-0042					2020	29						6977	6991		10.1109/TIP.2020.2996086													
J								Progressive Feature Matching: Incremental Graph Construction and Optimization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Optimization; Approximation algorithms; Computational modeling; Clustering algorithms; Markov processes; Computational efficiency; Feature matching; correspondences; local features; Markov random field (MRF); progressive optimization	MODEL; SCALE	We present a novel feature matching algorithm that systematically utilizes the geometric properties of image features such as position, scale, and orientation, in addition to the conventional descriptor vectors. In challenging scenes, in which repetitive structures and large view changes are present, it is difficult to find correct correspondences using conventional approaches that only use descriptors, as the descriptor distances of correct matches may not be the least among the candidates. The feature matching problem is formulated as a Markov random field (MRF) that uses descriptor distances and relative geometric similarities together. Assuming that the layout of the nearby features does not considerably change, we propose the bidirectional transfer measure to gauge the geometric consistency between the pairs of feature correspondences. The unmatched features are explicitly modeled in the MRF to minimize their negative impact. Instead of solving the MRF on the entire features at once, we start with a small set of confident feature matches, and then progressively expand the MRF with the remaining candidate matches. The proposed progressive approach yields better feature matching performance and faster processing time. Experimental results show that the proposed algorithm provides better feature correspondences in many challenging scenes, i.e., more matches with higher inlier ratio and lower computational cost than those of the state-of-the-art algorithms. The source code of our implementation is open to the public.																	1057-7149	1941-0042					2020	29						6992	7005		10.1109/TIP.2020.2996092													
J								Bi-Modal Progressive Mask Attention for Fine-Grained Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Image recognition; Feature extraction; Annotations; Task analysis; Semantics; Streaming media; Fine-grained visual recognition; multi-modal analysis; deep neural networks; language modality	NETWORKS	Traditional fine-grained image recognition is required to distinguish different subordinate categories (e.g., birds species) based on the visual cues beneath raw images. Due to both small inter-class variations and large intra-class variations, it is desirable to capture the subtle differences between these sub-categories, which is crucial but challenging for fine-grained recognition. Recently, language modality aggregation has been proved as a successful technique to improve visual recognition in the experience. In this paper, we introduce an end-to-end trainable Progressive Mask Attention (PMA) model for fine-grained recognition by leveraging both visual and language modalities. Our Bi-Modal PMA model can not only stage-by-stage capture the most discriminative part in the visual modality by our mask-based fashion, but also explore the out-of-visual-domain knowledge from the language modality in an interactional alignment paradigm. Specifically, at each stage, a self-attention module is proposed to attend to the key patch from images or text descriptions. Besides, a query-relational module is designed to seize the key words/phrases of texts and further bridge the connection between two modalities. Later, the learned representations of bi-modality from multiple stages are aggregated as the final features for recognition. Our Bi-Modal PMA model only needs raw images and raw text descriptions, without requiring bounding boxes/part annotations in images or key word annotations in texts. By conducting comprehensive experiments on fine-grained benchmark datasets, we demonstrate that the proposed method achieves superior performance over the competing baselines, on either vision and language bi-modality or single visual modality.																	1057-7149	1941-0042					2020	29						7006	7018		10.1109/TIP.2020.2996736													
J								Unsupervised Multi-View Constrained Convolutional Network for Accurate Depth Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Estimation; Training; Feature extraction; Geometry; Computer vision; Cameras; Unsupervised learning; Unsupervised learning; DenseDepthNet; multi-view geometry constraint; depth consistency		Accurate depth estimation from images is a fundamental problem in computer vision. In this paper, we propose an unsupervised learning based method to predict high-quality depth map from multiple images. A novel multi-view constrained DenseDepthNet is designed for this task. Our DenseDepthNet can effectively leverage both the low-level and high-level features of input images and generate appealing results, especially with sharp details. We employ the public datasets KITTI and Cityscapes for training in an end-to-end unsupervised fashion. A novel depth consistency loss based on multi-view geometry constraint is also applied to the corresponding points across pairwise images, which helps to improve the quality of predicted depth maps significantly. We conduct comprehensive evaluations on our DenseDepthNet and our depth consistency loss function. Experiments validate that our method outperforms the state-of-the-art unsupervised methods and produce comparable results with supervised methods.																	1057-7149	1941-0042					2020	29						7019	7031		10.1109/TIP.2020.2997247													
J								A New Hybrid Level Set Approach	IEEE TRANSACTIONS ON IMAGE PROCESSING										Level set; Image segmentation; Computational modeling; Image edge detection; Nonhomogeneous media; Optimization; Active contours; Image segmentation; active contour model; level set; hybrid; energy weight constraint	ACTIVE CONTOUR MODEL; IMAGE SEGMENTATION; FITTING ENERGY; VECTOR FLOW; DRIVEN; EVOLUTION; FEATURES; ROBUST	Hybrid active contour models with the combination of region and edge information have attracted great interests in image segmentation. To the best of our knowledge, however, the theoretical foundation of these hybrid models with level set evolution is insufficient and limited. More specifically, the weighting factors of their energy terms are difficult to select and are often empirically determined without definite theoretical basis. This problem is particularly prominent in the case of multi-object segmentation when more level set functions must be computed simultaneously. To cope with these challenges, this paper proposes a new level set approach for constructing hybrid active contour models with reliable energy weights, where the weights of region and edge terms can be constrained by the optimization condition deduced from the proposed method. It can be regarded as a general approach since many existing region-based models can be easily used to construct new hybrid models using their equivalent two-phase formulations. Some representative as well as state-of-the-art models are taken as examples to demonstrate the generality of our method. The respective comparative studies validate that under the guidance of the optimization condition, segmentation accuracy, robustness, and computational efficiency can be improved compared with the original models which are used to construct the new hybrid ones.																	1057-7149	1941-0042					2020	29						7032	7044		10.1109/TIP.2020.2997331													
J								Remove Cosine Window From Correlation Filter-Based Visual Trackers: When and How	IEEE TRANSACTIONS ON IMAGE PROCESSING										Contamination; Microsoft Windows; Training; Computational efficiency; Adaptation models; Correlation; Convolution; Visual tracking; correlation filters; cosine window; spatial regularization	TRACKING	Correlation filters (CFs) have been continuously advancing the state-of-the-art tracking performance and have been extensively studied in the recent few years. Nonetheless, the existing CF trackers adopt a cosine window to spatially reweight base image to alleviate boundary discontinuity. However, cosine window emphasizes more on the central regions of base image and has the risk of contaminating negative training samples during model learning. On the other hand, spatial regularization deployed in many recent CF trackers plays a similar role as cosine window by enforcing spatial penalty on CF coefficients. Therefore, we in this paper investigate the feasibility to remove cosine window from CF trackers with spatial regularization. When simply removing cosine window, CF with spatial regularization still suffers from small degree of boundary discontinuity. To tackle this issue, binary and Gaussian shaped mask functions are further introduced for eliminating boundary discontinuity while reweighting the estimation error of each training sample, and can be incorporated with multiple CF trackers with spatial regularization. In comparison to the baseline methods with cosine window, our methods are effective in handling boundary discontinuity and sample contamination, thereby benefiting tracking performance. Extensive experiments on four benchmarks show that our methods perform favorably against the state-of-the-art trackers using either handcrafted or deep CNN features.																	1057-7149	1941-0042					2020	29						7045	7060		10.1109/TIP.2020.2997521													
J								The TVp Regularized Mumford-Shah Model for Image Labeling and Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image labeling; segmentation; piecewise smooth; Mumford-Shah model; l(P) quasi-norm	AUGMENTED LAGRANGIAN METHOD; LEVEL SET METHOD; GLOBAL MINIMIZATION; ACTIVE CONTOURS; RESTORATION; ALGORITHM; FRAMEWORK; RECOVERY	The Mumford-Shah model is an important tool for image labeling and segmentation, which pursues a piecewise smooth approximation of the original image and the boundaries with the shortest length. In contrast to previous efforts, which use the total variation regularization to measure the total length of the boundaries, we build up a novel piecewise smooth Mumford-Shah model by utilizing a non-convex l(P) regularity term for p is an element of (0,1), which can well preserve sharp edges and eliminate geometric staircasing effects. We present optimization algorithms with convergence verification, where all subproblems can be solved by either the closed-form solution or fast Fourier transform (FFT). The method is compared to piecewise constant labeling algorithm and several state-of-the-art piecewise smooth Mumford-Shah models based on image decomposition approximations. Roth labeling and segmentation results on synthetic and real images confirm the robustness and efficiency of the proposed method.																	1057-7149	1941-0042					2020	29						7061	7075		10.1109/TIP.2020.2997524													
J								A New Polarization Image Demosaicking Algorithm by Exploiting Inter-Channel Correlations With Guided Filtering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Guided filter; image demosaicking; degree of linear polarization; polarization image; image denoising	GRADIENT-BASED INTERPOLATION; FOCAL-PLANE SENSORS; DIVISION	This paper presents a fast and effective polarization image demosaicking algorithm, which explores inter-channel dependency of Stokes parameters for the minimization of residual aliasing artifacts after cubic spline interpolation. A guided filtering approach is used for denoising. An optimization based on the confidence level of the aforementioned guided filtering, the correlations between the demosaicked image and input, as well as the total intensity, angle and degree of linear polarization, is constructed and solved with Newton's method. Experimental results demonstrate that the proposed algorithm can surpass the existing methods in terms of both objective root mean squared error and structural similarity index by at least 36.0% and 3.4%, respectively, and by close visual inspection of the clarity of objects in the angle and degree of linear polarization images. The proposed algorithm consists of only convolutions and element-wise operations, making it fast and parallelizable for efficient CPU acceleration. An image of size 512 x 612 x 4 can be processed within 10 s on i7-6700k CPU, and gains further 5 times speedup with M4000M GPU.																	1057-7149	1941-0042					2020	29						7076	7089		10.1109/TIP.2020.2998281													
J								Long-Term Video Prediction via Criticization and Retrospection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Predictive models; Feature extraction; Training; Adaptive optics; Optical imaging; Image reconstruction; Video prediction; generative adversarial networks		Video prediction refers to predicting and generating future video frames given a set of consecutive frames. Conventional video prediction methods usually criticize the discrepancy between the ground-truth and predictions frame by frame. As the prediction error accumulates recursively, these methods would easily become out of control and are often confined to the short-term horizon. In this paper, we introduce a retrospection process to rectify the prediction errors beyond criticizing the future prediction. The introduced retrospection process is designed to look back what have been learned from the past and rectify the prediction deficiencies. To this end, we build a retrospection network to reconstruct the past frames given the currently predicted frames. A retrospection loss is introduced to push the retrospection frames being consistent with the observed frames, so that the prediction error is alleviated. On the other hand, an auxiliary route is built by reversing the flow of time and executing a similar retrospection. These two routes interact with each other to boost the performance of retrospection network and enhance the understanding of dynamics across frames, especially for the long-term horizon. An adversarial loss is employed to generate more realistic results in both prediction and retrospection process. In addition, the proposed method can be used to extend many state-of-the-art video prediction methods. Extensive experiments on the natural video dataset demonstrate the advantage of introducing the retrospection process for long-term video prediction.																	1057-7149	1941-0042					2020	29						7090	7103		10.1109/TIP.2020.2998297													
J								Learning to Align via Wasserstein for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Heating systems; Measurement; Learning systems; Training; Estimation; Feature extraction; Person re-identification; deep metric learning; convolutional neural network; Wasserstein distance	NETWORK	Existing successful person re-identification (Re-ID) models often employ the part-level representation to extract the fine-grained information, but commonly use the loss that is particularly designed for global features, ignoring the relationship between semantic parts. In this paper, we present a novel triplet loss that emphasizes the salient parts and also takes the consideration of alignment. This loss is based on the crossing-bing matching metric that also known as Wasserstein Distance. It measures how much effort it would take to move the embeddings of local features to align two distributions, such that it is able to find an optimal transport matrix to re-weight the distance of different local parts. The distributions in support of local parts is produced via a new attention mechanism, which is calculated by the inner product between high-level global feature and local features, representing the importance of different semantic parts w.r.t. identification. We show that the obtained optimal transport matrix can not only distinguish the relevant and misleading parts, and hence assign different weights to them, but also rectify the original distance according to the learned distributions, resulting in an elegant solution for the mis-alignment issue. Besides, the proposed method is easily implemented in most Re-ID learning system with end-to-end training style, and can obviously improve their performance. Extensive experiments and comparisons with recent Re-ID methods manifest the competitive performance of our method.																	1057-7149	1941-0042					2020	29						7104	7116		10.1109/TIP.2020.2998931													
J								Model-Guided Multi-Path Knowledge Aggregation for Aerial Saliency Prediction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Predictive models; Visualization; Computational modeling; Drones; Solid modeling; Prediction algorithms; Adaptation models; Multi-path CNNs; knowledge transfer; visual saliency; aerial video; eye-tracking	VISUAL-ATTENTION; VIDEO	As an emerging vision platform, a drone can look from many abnormal viewpoints which brings many new challenges into the classic vision task of video saliency prediction. To investigate these challenges, this paper proposes a large-scale video dataset for aerial saliency prediction, which consists of ground-truth salient object regions of 1,000 aerial videos, annotated by 24 subjects. To the best of our knowledge, it is the first large-scale video dataset that focuses on visual saliency prediction on drones. Based on this dataset, we propose a Model-guided Multi-path Network (MM-Net) that serves as a baseline model for aerial video saliency prediction. Inspired by the annotation process in eye-tracking experiments, MM-Net adopts multiple information paths, each of which is initialized under the guidance of a classic saliency model. After that, the visual saliency knowledge encoded in the most representative paths is selected and aggregated to improve the capability of MM-Net in predicting spatial saliency in aerial scenarios. Finally, these spatial predictions are adaptively combined with the temporal saliency predictions via a spatiotemporal optimization algorithm. Experimental results show that MM-Net outperforms ten state-of-the-art models in predicting aerial video saliency.																	1057-7149	1941-0042					2020	29						7117	7127		10.1109/TIP.2020.2998977													
J								Fast Learning of Spatially Regularized and Content Aware Correlation Filter for Visual Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Target tracking; Training; Correlation; Visualization; Object tracking; Optimization; Benchmark testing; Object tracking; correlation filter; boundary effects; fast spatial regularization; temporal variations	SATELLITE VIDEOS; OBJECT TRACKING	With a good balance between accuracy and speed, correlation filter (CF) has become a popular and dominant visual object tracking scheme. It implicitly extends the training samples by circular shifts of a given target patch, which serve as negative samples for fast online learning of the filters. Since all these shifted patches are not real negative samples of the target, CF tracking scheme suffers from the annoying boundary effects that can greatly harm the tracking performance, especially under challenging situations, like occlusion and fast temporal variation. Spatial regularization is known as a potent way to alleviate such boundary effects, but with the cost of highly increased time complexity, caused by complex optimization imported by spatial regularization. In this paper, we propose a new fast learning approach to content-aware spatial regularization, namely weighted sample based CF tracking (WSCF). In WSCF, specifically, we present a simple yet effective energy function that implicitly weighs different training samples by spatial deviations. With the energy function, the learning of correlation filters is composed of two subproblems with closed-form solution and can be efficiently solved in an alternate way. We further develop a content-aware updating strategy to dynamically refine the weight distribution to well adapt to the temporal variations of the target and background. Finally, the proposed WSCF is used to enhance two state-of-the-art CF trackers to significantly boost their tracking accuracy, with little sacrifice on the tracking speed. Extensive experiments on five benchmarks validate the effectiveness of the proposed approach.																	1057-7149	1941-0042					2020	29						7128	7140		10.1109/TIP.2020.2998978													
J								Convexity Shape Prior for Level Set-Based Image Segmentation Method	IEEE TRANSACTIONS ON IMAGE PROCESSING										Shape; Level set; Image segmentation; Mathematical model; Computational modeling; Numerical models; Minimization; Convexity shape prior; image segmentation; level set method; Chan-Vese model	ACTIVE CONTOURS; MODEL; EFFICIENT; ALGORITHM	In this paper, we propose an image segmentation model that incorporates convexity shape priori using level set representations. In the past decade, several discrete and continuous methods have been developed to solve this problem. Our method comes from the observation that the signed distance function of a convex region must be a convex function. Based on this observation, we transfer the complicated geometrical convexity shape priori into some simple constraints on the signed distance function. We propose a simple algorithm to keep these constraints exactly. The proposed method could be easily applied to level set based segmentation models, such as the well-known Chan-Vese mode and the active contour models. By setting some good initial curves, the proposed method can easily segment convex objects from images with complicated background. We demonstrate the performance of the proposed methods on both synthetic images and real images, as well as the comparison to some state-of-the-art methods.																	1057-7149	1941-0042					2020	29						7141	7152		10.1109/TIP.2020.2998981													
J								Learning Spatial and Spatio-Temporal Pixel Aggregations for Image and Video Denoising	IEEE TRANSACTIONS ON IMAGE PROCESSING										Noise reduction; Noise measurement; Neural networks; Kernel; Heuristic algorithms; Image denoising; Aggregates; Image denoising; video denoising; pixel aggregation; neural network	ENHANCEMENT; ALGORITHM	Existing denoising methods typically restore clear results by aggregating pixels from the noisy input. Instead of relying on hand-crafted aggregation schemes, we propose to explicitly learn this process with deep neural networks. We present a spatial pixel aggregation network and learn the pixel sampling and averaging strategies for image denoising. The proposed model naturally adapts to image structures and can effectively improve the denoised results. Furthermore, we develop a spatio-temporal pixel aggregation network for video denoising to efficiently sample pixels across the spatio-temporal space. Our method is able to solve the misalignment issues caused by large motion in dynamic scenes. In addition, we introduce a new regularization term for effectively training the proposed video denoising model. We present extensive analysis of the proposed method and demonstrate that our model performs favorably against the state-of-the-art image and video denoising approaches on both synthetic and real-world data.																	1057-7149	1941-0042					2020	29						7153	7165		10.1109/TIP.2020.2999209													
J								Constrained Design of Deep Iris Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Iris recognition; Computer architecture; Computational modeling; Network architecture; Optimization; Search problems; Neural networks; Iris recognition; deep learning; iris network design; constrained deep network design	RECOGNITION	Despite the promise of recent deep neural networks to provide more accurate and efficient iris recognition compared to traditional techniques, there are vital properties of the classic IrisCode which are almost unable to be achieved with current deep iris networks: the compactness of model and the small number of computing operations (FLOPs). This paper casts the iris network design process as a constrained optimization problem which takes model size and computation into account as learning criteria. On one hand, this allows us to fully automate the network design process to search for the optimal iris network architecture with the highest recognition accuracy confined to the computation and model compactness constraints. On the other hand, it allows us to investigate the optimality of the classic IrisCode and recent deep iris networks. It also enables us to learn an optimal iris network and demonstrate state-of-the-art performance with less computation and memory requirements.																	1057-7149	1941-0042					2020	29						7166	7175		10.1109/TIP.2020.2999211													
J								Confidence-Based Large-Scale Dense Multi-View Stereo	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-view stereo; confidence; large-scale; interpolation; static and dynamic guidance; refinement	RECONSTRUCTION	Albeit remarkable progress has been made to improve the accuracy and completeness of multi-view stereo (MVS), existing methods still suffer from either sparse reconstructions of low-textured surfaces or heavy computational burden. In this paper, we propose a Confidence-based Large-scale Dense Multi-view Stereo (CLD-MVS) method for high resolution imagery. Firstly, we formulate MVS as a multi-view depth estimation problem, and employ a normal-aware efficient PatchMatch stereo to estimate the initial depth and normal map for each reference view. A self-supervised deep learning method is then developed to predict the spatial confidence for multi-view depth maps, which is combined with cross-view consistency to generate the ground control points. Subsequently, a confidence-driven and boundary-aware interpolation scheme using static and dynamic guidance is adopted to synthesize dense depth and normal maps. Finally, a refinement procedure which leverages synthesized depth and normal as prior is conducted to estimate cross-view consistent surface. Experiments show that the proposed CLD-MVS method achieves high geometric completeness while preserving fine-scale details. In particular, it has ranked No. 1 on the ETH3D high-resolution MVS benchmark in terms of F-1-score.																	1057-7149	1941-0042					2020	29						7176	7191		10.1109/TIP.2020.2999853													
J								OFF-eNET: An Optimally Fused Fully End-to-End Network for Automatic Dense Volumetric 3D Intracranial Blood Vessels Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Three-dimensional displays; Biomedical imaging; Convolution; Blood vessels; Computer architecture; Feature extraction; Convolution neural network; computed tomography angiography; dilated convolution; inception module; up-skip connection; intracranial vessels segmentation		Intracranial blood vessels segmentation from computed tomography angiography (CTA) volumes is a promising biomarker for diagnosis and therapeutic treatment in cerebrovascular diseases. These segmentation outputs are a fundamental requirement in the development of automated decision support systems for preoperative assessment or intraoperative guidance in neuropathology. The state-of-the-art in medical image segmentation methods are reliant on deep learning architectures based on convolutional neural networks. However, despite their popularity, there is a research gap in the current deep learning architectures optimized to address the technical challenges in blood vessel segmentation. These challenges include: (i) the extraction of concrete brain vessels close to the skull; and (ii) the precise marking of the vessel locations. We propose an Optimally Fused Fully end-to-end Network (OFF-eNET) for automatic segmentation of the volumetric 3D intracranial vascular structures. OFF-eNET comprises of three modules. In the first module, we exploit the up-skip connections to enhance information flow, and dilated convolution for detailed preservation of spatial feature map that are designed for thin blood vessels. In the second module, we employ residual mapping along with inception module for speedy network convergence and richer visual representation. For the third module, we make use of the transferred knowledge in the form of cascaded training strategy to gradually optimize the three segmentation stages (basic, complete, and enhanced) to segment thin vessels located close to the skull. All these modules are designed to be computationally efficient. Our OFF-eNET, evaluated using 70 CTA image volumes, resulted in 90.75% performance in the segmentation of intracranial blood vessels and outperformed the state-of-the-art counterparts.																	1057-7149	1941-0042					2020	29						7192	7202		10.1109/TIP.2020.2999854													
J								MEF-GAN: Multi-Exposure Image Fusion via Generative Adversarial Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Gallium nitride; Image fusion; Generative adversarial networks; Generators; Dynamic range; Feature extraction; Training; Image fusion; multi-exposure; generative adversarial network; self-attention	QUALITY ASSESSMENT; MODEL	In this paper, we present an end-to-end architecture for multi-exposure image fusion based on generative adversarial networks, termed as MEF-GAN. In our architecture, a generator network and a discriminator network are trained simultaneously to form an adversarial relationship. The generator is trained to generate a real-like fused image based on the given source images which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused images from the ground truth. The adversarial relationship makes the fused image not limited to the restriction of the content loss. Therefore, the fused images are closer to the ground truth in terms of probability distribution, which can compensate for the insufficiency of single content loss. Moreover, aiming at the problem that the luminance of multi-exposure images varies greatly with spatial location, the self-attention mechanism is employed in our architecture to allow for attention-driven and long-range dependency. Thus, local distortion, confusing results, or inappropriate representation can be corrected in the fused image. Qualitative and quantitative experiments are performed on publicly available datasets, where the results demonstrate that MEF-GAN outperforms the state-of-the-art, in terms of both visual effect and objective evaluation metrics. Our code is publicly available at https://github.com/jiayi-ma/MEF-GAN.																	1057-7149	1941-0042					2020	29						7203	7216		10.1109/TIP.2020.2999855													
J								A Local Flatness Based Variational Approach to Retinex	IEEE TRANSACTIONS ON IMAGE PROCESSING										Lighting; Image color analysis; Computational modeling; Adaptation models; Visualization; Dispersion; Image reconstruction; Illumination correction; retinex; localized deviation; vatiational approach	QUALITY ASSESSMENT; COLOR; MODEL; LIGHTNESS; IMPLEMENTATION; FRAMEWORK; IMAGES	A topic of continued interest in Retinex over the years has been finding ways to implement it with computational models of improved accuracy and efficiency. We have devised a new approach to digitally implementing the Retinex using a local deviation based variational model. The new model leads to improvements in the computed image quality with respect to illumination correction and image enhancement. Several contributions are made: 1) a new prior constraint, which we call local flatness, is proposed, and a new measure of Local Deviation (LD) is developed to quantify the degree of local illumination flatness; 2) a variational problem is defined and the solution is found by a logical sequence of steps; 3) discrete implementation of the variational solution is shown to effectively estimate and remove uneven illumination, yielding an accurate recovered image. Unlike other physical prior based variational Retinex models, which use the L2 norm of the illumination gradient to enforce smoothness of illumination, our LD prior selectively imposes local flatness on illumination by calculating the deviation between the estimated illumination surface to a reference plane. In the experiments, pseudo ground truth images are created by superimposing uneven illumination on real scenes, providing an effective way to objectively assess algorithm performance. The experimental results show that our method can reconstruct more accurate recovered images than other state-of-the-art methods, while maintaining good contrast.																	1057-7149	1941-0042					2020	29						7217	7232		10.1109/TIP.2020.2999858													
J								Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor Completion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tensors; Discrete Fourier transforms; Electron tubes; Matrix decomposition; Magnetic resonance imaging; Videos; Tensor nuclear norm; framelet; alternating direction method of multipliers (ADMM); tensor completion; tensor robust principal component analysis	REMOTE-SENSING IMAGES; MATRIX FACTORIZATION; DECOMPOSITION; RECOVERY; MODEL	The main aim of this paper is to develop a framelet representation of the tensor nuclear norm for third-order tensor recovery. In the literature, the tensor nuclear norm can be computed by using tensor singular value decomposition based on the discrete Fourier transform matrix, and tensor completion can be performed by the minimization of the tensor nuclear norm which is the relaxation of the sum of matrix ranks from all Fourier transformed matrix frontal slices. These Fourier transformed matrix frontal slices are obtained by applying the discrete Fourier transform on the tubes of the original tensor. In this paper, we propose to employ the framelet representation of each tube so that a framelet transformed tensor can be constructed. Because of framelet basis redundancy, the representation of each tube is sparsely represented. When the matrix slices of the original tensor are highly correlated, we expect the corresponding sum of matrix ranks from all framelet transformed matrix frontal slices would be small, and the resulting tensor completion can be performed much better. The proposed minimization model is convex and global minimizers can be obtained. Numerical results on several types of multi-dimensional data (videos, multispectral images, and magnetic resonance imaging data) have tested and shown that the proposed method outperformed the other testing methods.																	1057-7149	1941-0042					2020	29						7233	7244		10.1109/TIP.2020.3000349													
J								Sparse Projection Oblique Randomer Forests	JOURNAL OF MACHINE LEARNING RESEARCH										Ensemble Learning; Random Forests; Decision Trees; Random Projections; Classification; Regression; Feature Extraction; Sparse Learning		Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called "Sparse Projection Oblique Randomer Forests" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with > 100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. Very sparse random projections can be incorporated into gradient boosted trees to obtain potentially similar gains.																	1532-4435						2020	21						1	39															
J								MFE: Towards reproducible meta-feature extraction	JOURNAL OF MACHINE LEARNING RESEARCH										Machine Learning; AutoML; Meta-Learning; Meta-Features	SELECTION	Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.																	1532-4435						2020	21																						
J								GluonTS: Probabilistic and Neural Time Series Modeling in Python	JOURNAL OF MACHINE LEARNING RESEARCH										time series; deep learning; Python; scientific toolkit; benchmarking		We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms.																	1532-4435						2020	21																						
J								A General Framework for Consistent Structured Prediction with Implicit Loss Embeddings	JOURNAL OF MACHINE LEARNING RESEARCH										Structured Prediction; Statistical Learning Theory; Kernel Methods	CLASSIFICATION; CONVERGENCE; GRADIENT; OBJECTS; KERNELS; RATES	We propose and analyze a novel theoretical and algorithmic framework for structured prediction. While so far the term has referred to discrete output spaces, here we consider more general settings, such as manifolds or spaces of probability measures. We define structured prediction as a problem where the output space lacks a vectorial structure. We identify and study a large class of loss functions that implicitly defines a suitable geometry on the problem. The latter is the key to develop an algorithmic framework amenable to a sharp statistical analysis and yielding efficient computations. When dealing with output spaces with infinite cardinality, a suitable implicit formulation of the estimator is shown to be crucial.																	1532-4435						2020	21																						
J								Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Space	JOURNAL OF MACHINE LEARNING RESEARCH										Nonparametric estimation; latent variable models; deconvolution	MAXIMUM-LIKELIHOOD; OPTIMAL RATES; DECONVOLUTION; CONVERGENCE; IDENTIFICATION; INFERENCE; ERROR	This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.																	1532-4435						2020	21																						
J								Regularized Gaussian Belief Propagation with Nodes of Arbitrary Size	JOURNAL OF MACHINE LEARNING RESEARCH										belief propagation; Gaussian distributions; regularization; inference quality; higher-dimensional marginals	GRAPHICAL MODELS; CONVERGENCE	Gaussian belief propagation (GaBP) is a message-passing algorithm that can be used to perform approximate inference on a pairwise Markov graph (MG) constructed from a multivariate Gaussian distribution in canonical parameterization. The output of GaBP is a set of approximate univariate marginals for each variable in the pairwise MG. An extension of GaBP (labeled GaBP-m), allowing for the approximation of higher-dimensional marginal distributions, was explored by Kamper et al. (2019). The idea is to create an MG in which each node is allowed to receive more than one variable. As in the univariate case, the multivariate extension does not necessarily converge in loopy graphs and, even if convergence occurs, is not guaranteed to provide exact inference. To address the problem of convergence, we consider a multivariate extension of the principle of node regularization proposed by Kamper et al. (2018). We label this algorithm slow GaBP-m (sGaBP-m), where the term "slow" relates to the damping effect of the regularization on the message passing. We prove that, given sufficient regularization, this algorithm will converge and provide the exact marginal means at convergence, regardless of the way variables are assigned to nodes. The selection of the degree of regularization is addressed through the use of a heuristic, which is based on a tree representation of sGaBP-m. As a further contribution, we extend other GaBP variants in the literature to allow for higher-dimensional marginalization. We show that our algorithm compares favorably with these variants, both in terms of convergence speed and inference quality.																	1532-4435						2020	21								21														
J								Bayesian Model Selection with Graph Structured Sparsity	JOURNAL OF MACHINE LEARNING RESEARCH										spike-and-slab prior; graph laplacian; variational inference; expectation maximization; sparse linear regression; biclustering	VARIABLE SELECTION; STOCHASTIC SEARCH; REGRESSION; ALGORITHM	We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm (Rockova and George, 2014) as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as l(0) or l(1) penalization.																	1532-4435						2020	21																						
J								Quadratic Decomposable Submodular Function Minimization: Theory and Practice	JOURNAL OF MACHINE LEARNING RESEARCH										Submodular functions; Lovasz extensions; Random coordinate descent; Frank-Wolfe method; PageRank		We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization (QDSFM), which allows to model a number of learning tasks on graphs and hypergraphs. The problem exhibits close ties to decomposable submodular function minimization (DSFM) yet is much more challenging to solve. We approach the problem via a new dual strategy and formulate an objective that can be optimized through a number of double-loop algorithms. The outer-loop uses either random coordinate descent (RCD) or alternative projection (AP) methods, for both of which we prove linear convergence rates. The inner-loop computes projections onto cones generated by base polytopes of the submodular functions via the modified min-norm-point or Frank-Wolfe algorithms. We also describe two new applications of QDSFM: hypergraph-adapted PageRank and semi-supervised learning. The proposed hypergraph-based PageRank algorithm can be used for local hypergraph partitioning and comes with provable performance guarantees. For hypergraph-adapted semi-supervised learning, we provide numerical experiments demonstrating the efficiency of our QDSFM solvers and their significant improvements on prediction accuracy when compared to state-of-the-art methods.																	1532-4435						2020	21																						
J								Regularized Estimation of High-dimensional Factor-Augmented Vector Autoregressive (FAVAR) Models	JOURNAL OF MACHINE LEARNING RESEARCH										Model Identifiability; Compactness; Low-rank plus Sparse Decomposition; Finite-Sample Bounds	MATRIX COMPLETION; NOISY; ERRORS; RATES	A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables X and latent factors F, and a calibration equation that relates another set of observed variables Y with F and X. The latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the VAR system. The FAVAR model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the VAR equation. However, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. Hence, in this paper we investigate the FAVAR model under high-dimensional scaling. We introduce an appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. Further, we address a number of technical challenges introduced by the fact that estimates of the VAR system model parameters are based on estimated rather than directly observed quantities. The performance of the proposed estimators is evaluated on synthetic data. Further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators.																	1532-4435						2020	21																						
J								Joint Causal Inference from Multiple Contexts	JOURNAL OF MACHINE LEARNING RESEARCH										causal discovery; causal modeling; causal inference; observational and experimental data; interventions; randomized controlled trials	CONDITIONAL-INDEPENDENCE; MARKOV EQUIVALENCE; BAYESIAN NETWORKS; PATH DIAGRAMS; DISCOVERY; MODELS; ALGORITHM; GRAPHS	The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.																	1532-4435						2020	21																						
J								NEVAE: A Deep Generative Model for Molecular Graphs	JOURNAL OF MACHINE LEARNING RESEARCH										Drug design; Molecule discovery; Deep generative models; Variational autoencoders; Geometric deep learning		Deep generative models have been praised for their ability to learn smooth latent representations of images, text, and audio, which can then be used to generate new, plausible data. Motivated by these success stories, there has been a surge of interest in developing deep generative models for automated molecule design. However, these models face several difficulties due to the unique characteristics of molecular graphs-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes' labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given any arbitrary molecule, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning.																	1532-4435						2020	21																						
J								High-dimensional Linear Discriminant Analysis Classifier for Spiked Covariance Model	JOURNAL OF MACHINE LEARNING RESEARCH										Linear Discriminant Analysis; Spiked Covariance Models; High-Dimensional Data; Random Matrix Theory	SHRINKAGE; MATRIX; ASYMPTOTICS; COMPONENTS; ESTIMATOR; NUMBER	Linear discriminant analysis (LDA) is a popular classifier that is built on the assumption of common population covariance matrix across classes. The performance of LDA depends heavily on the quality of estimating the mean vectors and the population covariance matrix. This issue becomes more challenging in high-dimensional settings where the number of features is of the same order as the number of training samples. Several techniques for estimating the covariance matrix can be found in the literature. One of the most popular approaches are estimators based on using a regularized sample covariance matrix, giving the name regularized LDA (R-LDA) to the corresponding classifier. These estimators are known to be more resilient to the sample noise than the traditional sample covariance matrix estimator. However, the main challenge of the regularization approach is the choice of the optimal regularization parameter, as an arbitrary choice could lead to severe degradation of the classifier performance. In this work, we propose an improved LDA classifier based on the assumption that the covariance matrix follows a spiked covariance model. The main principle of our proposed technique is the design of a parametrized inverse covariance matrix estimator, the parameters of which are shown to be easily optimized. Numerical simulations, using both real and synthetic data, show that the proposed classifier yields better classification performance than the classical R-LDA while requiring lower computational complexity.																	1532-4435						2020	21																						
J								General Latent Feature Models for Heterogeneous Datasets	JOURNAL OF MACHINE LEARNING RESEARCH											DEPRESSION; DISORDERS; CHOICE; VIEW; SEX	Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.																	1532-4435						2020	21								21														
J								Change Point Estimation in a Dynamic Stochastic Block Model	JOURNAL OF MACHINE LEARNING RESEARCH										stochastic block model; Erdos-Renyi random graph; change point; edge probability matrix; community detection; estimation; clustering algorithm; convergence rate	COMMUNITY DETECTION; NETWORK MODELS; REGULARIZATION; CONSISTENCY	We consider the problem of estimating the location of a single change point in a network generated by a dynamic stochastic block model mechanism. This model produces community structure in the network that exhibits change at a single time epoch. We propose two methods of estimating the change point, together with the model parameters, before and after its occurrence. The first employs a least-squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises the following two steps: in the first one, a least-squares function is used and evaluated at each time point, but ignoring the community structure and only considering a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. The first method, since it requires knowledge of the community structure and hence clustering at every point in time, is significantly more computationally expensive than the second one. On the other hand, it requires a significantly less stringent identifiability condition for consistent estimation of the change point and the model parameters than the second method; however, it also requires a condition on the misclassification rate of misallocating network nodes to their respective communities that may fail to hold in many realistic settings. Despite the apparent stringency of the identifiability condition for the second method, we show that networks generated by a stochastic block mechanism exhibiting a change in their structure can easily satisfy this condition under a multitude of scenarios, including merging/splitting communities, nodes joining another community, etc. Further, for both methods under their respective identifiability and certain additional regularity conditions, we establish rates of convergence and derive the asymptotic distributions of the change point estimators. The results are illustrated on synthetic data. In summary, this work provides an in-depth investigation of the novel problem of change point analysis for networks generated by stochastic block models, identifies key conditions for the consistent estimation of the change point, and proposes a computationally fast algorithm that solves the problem in many settings that occur in applications. Finally, it discusses challenges posed by employing clustering algorithms in this problem, that require additional investigation for their full resolution.																	1532-4435						2020	21																						
J								Prediction regions through Inverse Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Inverse regression; Prediction regions; Confidence regions; High-dimension; Asymptotic distribution	CONFIDENCE-INTERVALS; ASYMPTOTIC THEORY; CALIBRATION; SHRINKAGE; SELECTION	Predicting a new response from a covariate is a challenging task in regression, which raises new question since the era of high-dimensional data. In this paper, we are interested in the inverse regression method from a theoretical viewpoint. Theoretical results for the well-known Gaussian linear model are well-known, but the curse of dimensionality has increased the interest of practitioners and theoreticians into generalization of those results for various estimators, calibrated for the high-dimension context. We propose to focus on inverse regression. It is known to be a reliable and efficient approach when the number of features exceeds the number of observations. Indeed, under some conditions, dealing with the inverse regression problem associated to a forward regression problem drastically reduces the number of parameters to estimate, makes the problem tractable and allows to consider more general distributions, as elliptical distributions. When both the responses and the covariates are multivariate, estimators constructed by the inverse regression are studied in this paper, the main result being explicit asymptotic prediction regions for the response. The performances of the proposed estimators and prediction regions are also analyzed through a simulation study and compared with usual estimators.																	1532-4435						2020	21																						
J								ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization	JOURNAL OF MACHINE LEARNING RESEARCH										Stochastic proximal gradient descent; variance reduction; composite nonconvex optimization; finite-sum minimization; expectation minimization		We propose a new stochastic first-order algorithmic framework to solve stochastic composite nonconvex optimization problems that covers both finite-sum and expectation settings. Our algorithms rely on the SARAH estimator introduced in Nguyen et al. (2017a) and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. The algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. They work with both constant and dynamic step-sizes, while allowing single sample and mini-batches. In all these cases, we prove that our algorithms can achieve the best-known complexity bounds in terms of stochastic first-order oracle. One key step of our methods is the new constant and dynamic step-sizes resulting in the desired complexity bounds while improving practical performance. Our constant step-size is much larger than existing methods including proximal SVRG scheme in the single sample case. We also specify our framework to the non-composite case that covers existing state-of-the-arts in terms of oracle complexity bounds. Our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. We test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known data sets.																	1532-4435						2020	21																						
J								Tslearn, A Machine Learning Toolkit for Time Series Data	JOURNAL OF MACHINE LEARNING RESEARCH										time series; clustering; classification; pre-processing; data mining		tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects.																	1532-4435						2020	21																						
J								ThunderGBM: Fast GBDTs and Random Forests on GPUs	JOURNAL OF MACHINE LEARNING RESEARCH										Gradient Boosting Decision Trees; Random Forests; GPUs; Efficiency	DECISION TREE	Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail.																	1532-4435						2020	21																						
J								Convergence of the Time Discrete Metamorphosis Model on Hadamard Manifolds	SIAM JOURNAL ON IMAGING SCIENCES										shape space; metamorphosis; variational time discretization; Hadamard manifolds; manifold-valued images; image morphing	FLOWS; FUNCTIONALS; MAPPINGS; GEOMETRY; IMAGES; LIE	Continuous image morphing is a classical task in image processing. The metamorphosis model proposed by Trouve, Younes, and coworkers [M. I. Miller and L. Younes, Int. J. Comput. Vis., 41 (2001), pp. 61-84; A. Trouve and L. Younes, Found. Comput. Math., 5 (2005), pp. 173-198] casts this problem in the frame of Riemannian geometry and geodesic paths between images. The associated metric in the space of images incorporates dissipation caused by a viscous flow transporting image intensities and its variations along motion paths. In many applications, images are maps from the image domain into a manifold (e.g., in diffusion tensor imaging (DTI), the manifold of symmetric positive definite matrices with a suitable Riemannian metric). In this paper, we propose a generalized metamorphosis model for manifold-valued images, where the range space is a finite-dimensional Hadamard manifold. A corresponding time discrete version was presented in [S. Neumayer, J. Persch, and G. Steidl, SIAM J. Imaging Sci., 11 (2018), pp. 1898-1930] based on the general variational time discretization proposed in [B. Berkels, A. Effland, and M. Rumpf, SIAM J. Imaging Sci., 8 (2015), pp. 1457-1488]. Here, we prove the Mosco-convergence of the time discrete metamorphosis functional to the proposed manifold-valued metamorphosis model, which implies the convergence of time discrete geodesic paths to a geodesic path in the (time continuous) metamorphosis model. In particular, the existence of geodesic paths is established. In particular, the existence of geodesic paths is established. In fact, images as maps into Hadamard manifold are not only relevant in applications, but it is also shown that the joint convexity of the distance function-which characterizes Hadamard manifolds-is a crucial ingredient to establish existence of the metamorphosis model.																	1936-4954						2020	13	2					557	588		10.1137/19M1247073													
J								Explicit Inversion Formulas for the Two-Dimensional Wave Equation from Neumann Traces	SIAM JOURNAL ON IMAGING SCIENCES										wave equation; inverse problems; computed tomography; inversion formula; back-projection; Neumann trace	PHOTOACOUSTIC TOMOGRAPHY; THERMOACOUSTIC TOMOGRAPHY; SPHERICAL MEANS; RECONSTRUCTION; TRANSFORM; ALGORITHM; OPERATOR; FAMILY	In this article we study the problem of recovering the initial data of the two-dimensional wave equation from Neumann measurements on a convex domain Omega subset of R-2 with smooth boundary. We derive an explicit inversion formula of a so-called back-projection type and deduce exact inversion formulas for circular and elliptical domains. In addition, for circular domains, we show that the initial data can also be recovered from any linear combination of its solution and its normal derivative on the boundary. Numerical results of our implementation of the derived inversion formulas are presented demonstrating their accuracy and stability.																	1936-4954						2020	13	2					589	608		10.1137/19M1260517													
J								Fixed Point Analysis of Douglas-Rachford Splitting for Ptychography and Phase Retrieval	SIAM JOURNAL ON IMAGING SCIENCES										fixed point; Douglas-Rachford splitting; phase retrieval; ptychography	ALGORITHM; APPROXIMATION; CONVEX; ILLUMINATION; CONVERGENCE	Douglas-Rachford splitting (DRS) methods based on the proximal point algorithms for the Poisson and Gaussian log-likelihood functions are proposed for ptychography and phase retrieval. Fixed point analysis shows that the DRS iterated sequences are always bounded explicitly in terms of the step size and that the fixed points are attracting if and only if the fixed points are regular solutions. This alleviates two major drawbacks of the classical Douglas-Rachford algorithm: slow convergence when the feasibility problem is consistent and divergent behavior when the feasibility problem is inconsistent. Fixed point analysis also leads to a simple, explicit expression for the optimal step size in terms of the spectral gap of an underlying matrix. When applied to the challenging problem of blind ptychography, which seeks to recover both the object and the probe simultaneously, alternating minimization with the DRS inner loops, even with a far from optimal step size, converges geometrically under the nearly minimum conditions established in the uniqueness theory.																	1936-4954						2020	13	2					609	650		10.1137/19M128781X													
J								A Variational Image Segmentation Model Based on Normalized Cut with Adaptive Similarity and Spatial Regularization	SIAM JOURNAL ON IMAGING SCIENCES										normalized cut; Parzen-Rosenblatt window; EM algorithm; regularization; convex optimization; adaptive similarity; duality	ACTIVE CONTOURS; FRAMEWORK; MUMFORD; MINIMIZATION; ALGORITHMS; TV	Image segmentation is a fundamental research topic in image processing and computer vision. In recent decades, researchers developed a large number of segmentation algorithms for various applications. Among these algorithms, the normalized cut (Ncut) segmentation method is widely applied due to its good performance. The Ncut segmentation model is an optimization problem whose energy is defined on a specifically designed graph. Thus, the segmentation results of the existing Ncut method are largely dependent on a preconstructed similarity measure on the graph since this measure is usually given empirically by users. This flaw will lead to some undesirable segmentation results. In this paper, we propose an Ncut-based segmentation algorithm by integrating an adaptive similarity measure and spatial regularization. The proposed model combines the Parzen-Rosenblatt window method, nonlocal weights entropy, Ncut energy, and regularizer of phase field in a variational framework. Our method can adaptively update the similarity measure function by estimating some parameters. This adaptive procedure enables the proposed algorithm to find a better similarity measure for classification than the Ncut method. We provide some mathematical interpretation of the proposed adaptive similarity from multiple viewpoints, such as statistics and convex optimization. In addition, the regularizer of phase field can guarantee that the proposed algorithm has a robust performance in the presence of noise, and it can also rectify the similarity measure with a spatial priori. The well-posed theory such as the existence of the minimizer for the proposed model is given in the paper. Compared with some existing segmentation methods such as the traditional Ncutbased model and the classical Chan-Vese model, the numerical experiments show that our method can provide promising segmentation results.																	1936-4954						2020	13	2					651	684		10.1137/18M1192366													
J								Reduced Order Model Approach to Inverse Scattering	SIAM JOURNAL ON IMAGING SCIENCES										inverse scattering; model reduction; Galerkin approximation	MIGRATION	We study an inverse scattering problem for a generic hyperbolic system of equations with an unknown coefficient called the reflectivity. The solution of the system models waves (sound, electromagnetic, or elastic), and the reflectivity models unknown scatterers embedded in a smooth and known medium. The inverse problem is to determine the reflectivity from the time resolved scattering matrix (the data) measured by an array of sensors. We introduce a novel inversion method, based on a reduced order model (ROM) of an operator called a wave propagator, because it maps the wave from one time instant to the next, at an interval corresponding to the discrete time sampling of the data. The wave propagator is unknown in the inverse problem, but the ROM can be computed directly from the data. By construction, the ROM inherits key properties of the wave propagator, which facilitate the estimation of the reflectivity. The ROM was introduced previously and was used for two purposes: (1) to map the scattering matrix to that corresponding to the single scattering (Born) approximation and (2) to image, i.e., obtain a qualitative estimate of the support of the reflectivity. Here we study further the ROM and show that it corresponds to a Galerkin projection of the wave propagator. The Galerkin framework is useful for proving properties of the ROM that are used in the new inversion method which seeks a quantitative estimate of the reflectivity.																	1936-4954						2020	13	2					685	723		10.1137/19M1296355													
J								Semisupervised Dictionary Learning with Graph Regularized and Active Points	SIAM JOURNAL ON IMAGING SCIENCES										dictionary learning; semi-supervised learning; manifold learning	K-SVD; SPARSE; ALGORITHM	Supervised dictionary learning has gained much interest in the recent decade and has shown significant performance improvements in image classification. However, in general, supervised learning needs a large number of labelled samples per class to achieve an acceptable result. In order to deal with databases which have just a few labelled samples per class, semisupervised learning, which also exploits unlabelled samples in training phase is used. Indeed, unlabelled samples can help to regularize the learning model, yielding an improvement of classification accuracy. In this paper, we propose a new semisupervised dictionary learning method based on two pillars: on one hand, we enforce manifold structure preservation from the original data into sparse code space using locally linear embedding, which can be considered a regularization of sparse code; on the other hand, we train a semisupervised classifier in sparse code space. We show that our approach provides an improvement over state-of-the-art semisupervised dictionary learning methods.																	1936-4954						2020	13	2					724	745		10.1137/19M1285469													
J								Microlocal Analysis of a Compton Tomography Problem	SIAM JOURNAL ON IMAGING SCIENCES										microlocal analysis; Compton scattering; tomography	MONOCHROMATIC X-RAYS; LINE INTEGRALS; RECONSTRUCTION; REPRESENTATION	Here we present a novel microlocal analysis of a new toric section transform which describes a twodimensional image reconstruction problem in Compton scattering tomography and airport baggage screening. By an analysis of two separate limited data problems for the circle transform and using microlocal analysis, we show that the canonical relation of the toric section transform is 21. This implies that there are image artifacts in the filtered backprojection reconstruction. We provide explicit expressions for the expected artifacts and demonstrate these by simulations. In addition, we prove injectivity of the forward operator for L-infinity functions supported inside the open unit ball. We present reconstructions from simulated data using a discrete approach and several regularizers with varying levels of added pseudorandom noise.																	1936-4954						2020	13	2					746	774		10.1137/19M1251035													
J								Reconstruction of the High Resolution Phase in a Closed Loop Adaptive Optics	SIAM JOURNAL ON IMAGING SCIENCES										image improvement; adaptive optics; astronomical imaging	WAVE-FRONT RECONSTRUCTION; ALGORITHM; SUPERRESOLUTION; MODEL	Adaptive optics is a commonly used technique to correct the phase distortions caused by the Earth's atmosphere to improve the image quality of the ground-based imaging systems. However, the observed images still suffer from the blur caused by the adaptive optics residual wavefront. In this paper, we propose a model for reconstructing the residual phase in high resolution from a sequence of deformable mirror data. Our model is based on the turbulence statistics and the Taylor frozen flow hypothesis with knowledge of the wind velocities in atmospheric turbulence layers. A tomography problem for the phase distortions from different altitudes is solved in order to get a high quality phase reconstruction. We also consider inexact tomography operators resulting from the uncertainty in the wind velocities. The wind velocities are estimated from the deformable mirror data and, additionally, by including them as unknowns in the objective function. We provide a theoretical analysis on the existence of a minimizer of the objective function. To solve the associated joint optimization problem, we use an alternating minimization method which results in a high resolution reconstruction algorithm with adaptive wind velocities. Numerical simulations are carried out to show the effectiveness of our approach.																	1936-4954						2020	13	2					775	806		10.1137/19M1258426													
J								Low-Frequency Electromagnetic Imaging Using Sensitivity Functions and Beamforming	SIAM JOURNAL ON IMAGING SCIENCES										imaging; Maxwell's equations; low frequency; VLF/ELF; computational imaging	MAGNETIC INDUCTION TOMOGRAPHY; SPATIOTEMPORAL INVERSE FILTER; NUMERICAL-SOLUTION; RECONSTRUCTION; MODELS; MEDIA	We present a computational technique for low-frequency electromagnetic imaging in inhomogeneous media that provides superior three-dimensional resolution over existing techniques. The method is enabled through large-scale, fast (low-complexity) algorithms that we introduce for simulating electromagnetic wave propagation. We numerically study the performance of the technique on various problems including the imaging of a strong finite scatterer located within a thick conductive box.																	1936-4954						2020	13	2					807	843		10.1137/19M1279502													
J								A Gray Level Indicator-Based Regularized Telegraph Diffusion Model: Application to Image Despeckling	SIAM JOURNAL ON IMAGING SCIENCES										speckle noise; nonlinear diffusion; telegraph diffusion equation; gray level indicator; weak solution; Schauder fixed point theorem; despeckling	NONLINEAR DIFFUSION; VARIATIONAL MODEL; GEOMETRIC FILTER; SCALE-SPACE; SPECKLE; NOISE; EQUATION; ALGORITHM; REMOVAL	In this work, a gray level indicator-based nonlinear telegraph diffusion model is presented for image despeckling. Most of the researchers focus only on diffusion equation-based filter for multiplicative noise removal process. The proposed technique uses the benefit of the combined effect of diffusion equation as well as the wave equation. The wave nature of the system preserves the high oscillatory and texture patterns in an image. In this model, the diffusion coefficient depends not only on the image gradient but also on the gray level of the image, which controls the diffusion process better than only gradient-based diffusion approaches. Moreover, we prove the well-posedness of the present system using the Schauder fixed point theorem. Furthermore, we show the superiority of the proposed method over three recently developed methods on a set of gray level test images corrupted by speckle noise and check the noise removal capability of the present technique over some real SAR images corrupted by speckle noise with different noise levels.																	1936-4954						2020	13	2					844	870		10.1137/19M1283033													
J								Convexification for a Three-Dimensional Inverse Scattering Problem with the Moving Point Source	SIAM JOURNAL ON IMAGING SCIENCES										coefficient inverse scattering problem; point sources; Carleman weight function; globally convergent numerical method; data completion; Fourier truncation	CONVERGENT NUMERICAL-METHOD; CONVEXITY; RECOVERY	For the first time, we develop in this paper the globally convergent convexification numerical method for a coefficient inverse problem for the three-dimensional Helmholtz equation for the case when the backscattering data are generated by a point source running along an interval of a straight line and the wavenumber is fixed. Thus, by varying the wavenumber, one can reconstruct the dielectric constant depending not only on spatial variables but on the wavenumber (i.e., frequency) as well. Our approach relies on a new derivation of a boundary value problem for a system of coupled quasi-linear elliptic partial differential equations. This is done via an application of a special truncated Fourier-like method. First, we prove the Lipschitz stability estimate for this problem via a Carleman estimate. Next, using the Carleman weight function generated by that estimate, we construct a globally strictly convex cost functional and prove the global convergence to the exact solution of the gradient projection method. Finally, our theoretical finding is verified via several numerical tests with computationally simulated data. These tests demonstrate that we can accurately recover all three important components of targets of interest: locations, shapes, and dielectric constants. In particular, large target/background contrasts in dielectric constants (up to 10:1) can be accurately calculated.																	1936-4954						2020	13	2					871	904		10.1137/19M1303101													
J								Accelerating Proximal Markov Chain Monte Carlo by Using an Explicit Stabilized Method	SIAM JOURNAL ON IMAGING SCIENCES										mathematical imaging; inverse problems; Bayesian inference; Markov chain Monte Carlo methods; proximal algorithms	MEAN-SQUARE; LANGEVIN; REGULARIZATION; OPTIMIZATION; ALGORITHM; NOISE; STIFF	We present a highly efficient proximal Markov chain Monte Carlo methodology to perform Bayesian computation in imaging problems. Similarly to previous proximal Monte Carlo approaches, the proposed method is derived from an approximation of the Langevin diffusion. However, instead of the conventional Euler-Maruyama approximation that underpins existing proximal Monte Carlo methods, here we use a state-of-the-art orthogonal Runge-Kutta-Chebyshev stochastic approximation [A. Abdulle, I. Aimuslimani, and G. Vilmart, SIAM/ASA J. Uncertain. Quantif., 6 (2018), pp. 937-964] that combines several gradient evaluations to significantly accelerate its convergence speed, similarly to accelerated gradient optimization methods. The proposed methodology is demonstrated via a range of numerical experiments, including non-blind image deconvolution, hyperspectral unmixing, and tomographic reconstruction, with total-variation and l(1)-type priors. Comparisons with Euler-type proximal Monte Carlo methods confirm that the Markov chains generated with our method exhibit significantly faster convergence speeds, achieve larger effective sample sizes, and produce lower mean-square estimation errors at equal computational budget.																	1936-4954						2020	13	2					905	935		10.1137/19M1283719													
J								A Wasserstein-Type Distance in the Space of Gaussian Mixture Models	SIAM JOURNAL ON IMAGING SCIENCES										optimal transport; Wasserstein distance; Gaussian mixture model; multimarginal optimal transport; barycenter; image processing applications	BARYCENTERS	In this paper we introduce a Wasserstein-type distance on the set of Gaussian mixture models. This distance is defined by restricting the set of possible coupling measures in the optimal transport problem to Gaussian mixture models. We derive a very simple discrete formulation for this distance, which makes it suitable for high dimensional problems. We also study the corresponding multi-marginal and barycenter formulations. We show some properties of this Wasserstein-type distance, and we illustrate its practical use with some examples in image processing.																	1936-4954						2020	13	2					936	970		10.1137/19M1301047													
J								On Decomposition Models in Imaging Sciences and Multi-time Hamilton-Jacobi Partial Differential Equations	SIAM JOURNAL ON IMAGING SCIENCES										image processing; Hamilton-Jacobi equations; convex analysis; Hopf-Lax formulas	TOTAL VARIATION MINIMIZATION; TOTAL VARIATION REGULARIZATION; VARIATION PENALTY METHODS; BOUNDED VARIATION; VISCOSITY SOLUTIONS; TEXTURE; CONVEX; ALGORITHMS; COMPONENT; FORMULAS	This paper provides new theoretical connections between multi-time Hamilton-Jacobi partial differential equations and variational image decomposition models in imaging sciences. We show that the minimal values of these optimization problems are governed by multi-time Hamilton-Jacobi partial differential equations. The minimizers of these optimization problems can be represented using the momentum in the corresponding Hamilton-Jacobi partial differential equation. Moreover, variational behaviors of both the minimizers and the momentum are investigated as the regularization parameters approach zero. In addition, we provide a new perspective from convex analysis to prove the uniqueness of convex solutions to Hamilton-Jacobi equations. Finally, we consider image decomposition models that do not have unique minimizers, and we propose a regularization approach to perform the analysis using multi-time Hamilton-Jacobi partial differential equations.																	1936-4954						2020	13	2					971	1014		10.1137/19M1266332													
J								Spectral Embedding Norm: Looking Deep into the Spectrum of the Graph Laplacian	SIAM JOURNAL ON IMAGING SCIENCES										spectral clustering; graph Laplacian; spectral theory; outlier detection; calcium imaging	GEOMETRY; ENTROPY	The extraction of clusters from a dataset which includes multiple clusters and a significant background component is a nontrivial task of practical importance. In image analysis this manifests for example in anomaly detection and target detection. The traditional spectral clustering algorithm, which relies on the leading K eigenvectors to detect K clusters, fails in such cases. In this paper we propose the spectral embedding norm which sums the squared values of the first I normalized eigenvectors, where I can be significantly larger than K. We prove that this quantity can be used to separate clusters from the background in unbalanced settings, including extreme cases such as outlier detection. The performance of the algorithm is not sensitive to the choice of I, and we demonstrate its application on synthetic and real-world remote sensing and neuroimaging datasets.																	1936-4954						2020	13	2					1015	1048		10.1137/18M1283160													
J								AF-Net: A Convolutional Neural Network Approach to Phase Detection Autofocus	IEEE TRANSACTIONS ON IMAGE PROCESSING										Phase detection autofocus; supervised learning; focus profile; phase shift		It is important for an autofocus system to accurately and quickly find the in-focus lens position so that sharp images can be captured without human intervention. Phase detectors have been embedded in image sensors to improve the performance of autofocus; however, the phase shift estimation between the left and right phase images is sensitive to noise. In this paper, we propose a robust model based on convolutional neural network to address this issue. Our model includes four convolutional layers to extract feature maps from the phase images and a fully-connected network to determine the lens movement. The final lens position error of our model is five times smaller than that of a state-of-the-art statistical PDAF method. Furthermore, our model works consistently well for all initial lens positions. All these results verify the robustness of our model.																	1057-7149	1941-0042					2020	29						6386	6395		10.1109/TIP.2019.2947349													
J								NPSA: Nonorthogonal Principal Skewness Analysis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Coskewness tensor; eigenpairs; feature extraction; Kronecker product; nonorthogonality; principal skewness analysis; subspace	JOINT DIAGONALIZATION; TENSOR DECOMPOSITIONS; COMPONENT ANALYSIS; TARGET DETECTION; ALGORITHM; RECOGNITION	Principal skewness analysis (PSA) has been introduced for feature extraction in hyperspectral imagery. As a third-order generalization of principal component analysis (PCA), its solution of searching for the local maximum skewness direction is transformed into the problem of calculating the eigenpairs (the eigenvalues and the corresponding eigenvectors) of a coskewness tensor. By combining a fixed-point method with an orthogonal constraint, the new eigenpairs are prevented from converging to the same previously determined maxima. However, in general, the eigenvectors of the supersymmetric tensor are not inherently orthogonal, which implies that the results obtained by the search strategy used in PSA may unavoidably deviate from the actual eigenpairs. In this paper, we propose a new nonorthogonal search strategy to solve this problem and the new algorithm is named nonorthogonal principal skewness analysis (NPSA). The contribution of NPSA lies in the finding that the search space of the eigenvector to be determined can be enlarged by using the orthogonal complement of the Kronecker product of the previous eigenvector with itself, instead of its orthogonal complement space. We also give a detailed theoretical proof on why we can obtain the more accurate eigenpairs through the new search strategy by comparison with PSA. In addition, after some algebraic derivations, the complexity of the presented algorithm is also greatly reduced. Experiments with both simulated data and real multi/hyperspectral imagery demonstrate its validity in feature extraction.																	1057-7149	1941-0042					2020	29						6396	6408		10.1109/TIP.2020.2984849													
J								Fast Optical Flow Extraction From Compressed Video	IEEE TRANSACTIONS ON IMAGE PROCESSING										Adaptive optics; Optical imaging; Optimization; Motion estimation; Video coding; Image edge detection; Optical filters; Optical flow; inverse problems; edge-preserving filtering; HEVC	MOTION; IMAGE; ALGORITHM; ROAD	We propose the fast optical flow extractor, a filtering method that recovers artifact-free optical flow fields from HEVC-compressed video. To extract accurate optical flow fields, we form a regularized optimization problem that considers the smoothness of the solution and the pixelwise confidence weights of an artifact- ridden HEVC motion field. Solving such an optimization problem is slow, so we first convert the problem into a confidence-weighted filtering task. By leveraging the already-available HEVC motion parameters, we achieve a 100-fold speed-up in the running times compared to similar methods, while producing subpixel-accurate flow estimates. The fast optical flow extractor is useful when video frames are already available in coded formats. Our method is not specific to a coder, and works with motion fields from video coders such as H.264/AVC and HEVC.																	1057-7149	1941-0042					2020	29						6409	6421		10.1109/TIP.2020.2985866													
J								Interweaved Prediction for Video Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video coding; Image coding; Complexity theory; Adaptation models; Tools; Motion compensation; Encoding; Video coding; sub-block; affine; AMC; sbTMVP; interweaved prediction; VVC	MOTION COMPENSATION; EFFICIENCY	In the emerging next generation video coding standard Versatile Video Coding (VVC) developed by the Joint Video Exploration Team (JVET), sub-block-based inter-prediction plays a key role in promising coding tools such as Affine Motion Compensation (AMC) and sub-block-based Temporal Motion Vector Prediction (sbTMVP). With sub-block-based inter-prediction, a coding block is divided into sub-blocks, and the motion information of each sub-block is derived individually. Although sub-block-based inter-prediction can provide a higher quality prediction benefiting from a finer motion granularity, it still suffers two problems: uneven prediction quality and boundary discontinuity. In this paper, we present a method of interweaved prediction to further improve sub-block-based inter-prediction. With interweaved prediction, a coding block with AMC or sbTMVP mode is divided into sub-blocks with two different dividing patterns, so that a corner position of a sub-block in one dividing pattern coincides with the central position of a sub-block in the other dividing pattern. Then two auxiliary predictions are generated by AMC or sbTMVP with the two dividing patterns, independently. The final prediction is calculated as a weighted-sum of the two auxiliary predictions. Theoretical analysis and statistical data prove that interweaved prediction can significantly mitigate the two problems in sub-block-based inter-prediction. Simulation results show that the proposed methods can achieve 0.64% BD-rate saving on average with the random access configurations. On sequences with rich affine motions, the average BD-rate saving can be up to 2.54%.																	1057-7149	1941-0042					2020	29						6422	6437		10.1109/TIP.2020.2987432													
J								PiCANet: Pixel-Wise Contextual Attention Learning for Accurate Saliency Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Saliency detection; Object detection; Context modeling; Convolution; Semantics; Computational modeling; saliency detection; attention network; global context; local context; semantic segmentation; object detection	MODEL	Existing saliency models typically incorporate contexts holistically. However, for each pixel, usually only part of its context region contributes to saliency prediction, while other parts are likely either noise or distractions. In this paper, we propose a novel pixel-wise contextual attention network (PiCANet) to selectively attend to informative context locations at each pixel. The proposed PiCANet generates an attention map over the contextual region of each pixel and construct attentive contextual features via selectively incorporating the features of useful context locations. We present three formulations of the PiCANet via embedding the pixel-wise contextual attention mechanism into the pooling and convolution operations with attending to global or local contexts. All the three models are fully differentiable and can be integrated with convolutional neural networks with joint training. In this work, we introduce the proposed PiCANets into a U-Net model for salient object detection. The generated global and local attention maps can learn to incorporate global contrast and regional smoothness, which help localize and highlight salient objects more accurately and uniformly. Experimental results show that the proposed PiCANets perform effectively for saliency detection against the state-of-the-art methods. Furthermore, we demonstrate the effectiveness and generalization ability of the PiCANets on semantic segmentation and object detection with improved performance.																	1057-7149	1941-0042					2020	29						6438	6451		10.1109/TIP.2020.2988568													
J								Learning From Synthetic Images via Active Pseudo-Labeling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Data models; Training; Visualization; Adaptation models; Neural networks; Predictive models; Deep learning; domain adaptation; style transfer; pseudo-labeling; semantic segmentation; object detection	OBJECT DETECTION; DEEP	Synthetic visual data refers to the data automatically rendered by the mature computer graphic algorithms. With the rapid development of these techniques, we can now collect photo-realistic synthetic images with accurate pixel-level annotations without much effort. However, due to the domain gaps between synthetic data and real data, in terms of not only visual appearance but also label distribution, directly applying models trained on synthetic images to real ones can hardly yield satisfactory performance. Since the collection of accurate labels for real images is very laborious and time-consuming, developing algorithms which can learn from synthetic images is of great significance. In this paper, we propose a novel framework, namely Active Pseudo-Labeling (APL), to reduce the domain gaps between synthetic images and real images. In APL framework, we first predict pseudo-labels for the unlabeled real images in the target domain by actively adapting the style of the real images to source domain. Specifically, the style of real images is adjusted via a novel task guided generative model, and then pseudo-labels are predicted for these actively adapted images. Lastly, we fine-tune the source-trained model in the pseudo-labeled target domain, which helps to fit the distribution of the real data. Experiments on both semantic segmentation and object detection tasks with several challenging benchmark data sets demonstrate the priority of our proposed method compared to the existing state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						6452	6465		10.1109/TIP.2020.2989100													
J								Shearlet Enhanced Snapshot Compressive Imaging	IEEE TRANSACTIONS ON IMAGE PROCESSING										Snapshot compressive imaging; sparsity; shearlet transform; SeSCI; ADMM	VIDEO	Snapshot compressive imaging (SCI) is a promising approach to capture high-dimensional data with low dimensional sensors. With modest modifications to off-the-shelf cameras, SCI cameras encode multiple frames into a single measurement frame. These correlated frames can then be retrieved by reconstruction algorithms. Existing reconstruction algorithms suffer from low speed or low fidelity. In this paper, we propose a novel reconstruction algorithm, namely, Shearlet enhanced Snapshot Compressive Imaging (SeSCI), which exploits the sparsity of the image representation in both frequency domain and shearlet domain. Towards this end, we first derive our SeSCI algorithm under the alternating direction method of multipliers (ADMM) framework. We then propose an efficient solution of SeSCI algorithm. Moreover, we prove that the improved SeSCI algorithm converges to a fixed point. Experimental results on both synthetic data and real data captured by SCI cameras demonstrate the significant advantages of SeSCI, which outperforms the conventional algorithms by more than 2dB in PSNR. At the same time, the SeSCI achieves a speed-up more than 100x over the state-of-the-art algorithm.																	1057-7149	1941-0042					2020	29						6466	6481		10.1109/TIP.2020.2989550													
J								OSLNet: Deep Small-Sample Classification With an Orthogonal Softmax Layer	IEEE TRANSACTIONS ON IMAGE PROCESSING										Deep neural network; Orthogonal softmax layer; overfitting; small-sample classification	NEURAL-NETWORKS	A deep neural network of multiple nonlinear layers forms a large function space, which can easily lead to overfitting when it encounters small-sample data. To mitigate overfitting in small-sample classification, learning more discriminative features from small-sample data is becoming a new trend. To this end, this paper aims to find a subspace of neural networks that can facilitate a large decision margin. Specifically, we propose the Orthogonal Softmax Layer (OSL), which makes the weight vectors in the classification layer remain orthogonal during both the training and test processes. The Rademacher complexity of a network using the OSL is only k, where K is the number of classes, of that of a network using the fully connected classification layer, leading to a tighter generalization error bound. Experimental results demonstrate that the proposed OSL has better performance than the methods used for comparison on four small-sample benchmark datasets, as well as its applicability to large-sample datasets. Codes are available at: https://github.com/dongliangchang/OSLNet.																	1057-7149	1941-0042					2020	29						6482	6495		10.1109/TIP.2020.2990277													
J								No-Reference Image Quality Assessment: An Attention Driven Approach	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Distortion; Image restoration; Computational modeling; Feature extraction; Image quality; Visualization; No-reference image quality assessment; attention model	FREE-ENERGY PRINCIPLE; VISUAL-ATTENTION; FRAMEWORK	In this paper, we tackle no-reference image quality assessment (NR-IQA), which aims to predict the perceptual quality of a distorted image without referencing its pristine-quality counterpart. Inspired by the free-energy principle, we assume that, while perceiving a distorted image, the human visual system (HVS) tends to predict the pristine image then estimates the perceptual quality based on the distorted-restored pair. Furthermore, the perceptual quality depends heavily on the way how human beings attend to distorted images, namely, the cooperation of foveal vision and the eye movement mechanism. Inspired by these properties of the HVS, given the distorted-restored pair, we implement an attention-driven NR-IQA method with reinforcement learning (RL). The model learns a policy to attend to several regions parallelly. The observations of the fixation regions are aggregated in a weighted average way, which is inspired by the robust averaging strategy. For policy learning, the rewards are derived from two tasks-classifying the distortion type and estimating the perceptual score. The goal of policy learning is to maximize the expectation of the accumulated rewards. Extensive experiments on LIVE, TID2008, TID2013 and CSIQ demonstrate the superiority of our methods.																	1057-7149	1941-0042					2020	29						6496	6506		10.1109/TIP.2020.2990342													
J								Segmentation of MR Brain Images Through Hidden Markov Random Field and Hybrid Metaheuristic Algorithm	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Hidden Markov models; Optimization; Adaptation models; Brain; Particle swarm optimization; Task analysis; Image segmentation; hidden Markov random field; Cuckoo search; particle swarm optimization	CUCKOO SEARCH ALGORITHM; LEVEL SET METHOD; STATISTICAL-ANALYSIS; INTENSITY; MODEL; OPTIMIZATION; SIMULATION; THRESHOLD; ACCURATE	Image segmentation is one of the most critical tasks in Magnetic Resonance (MR) images analysis. Since the performance of most current image segmentation methods is suffered by noise and intensity non-uniformity artifact (INU), a precise and artifact resistant method is desired. In this work, we propose a new segmentation method combining a new Hidden Markov Random Field (HMRF) model and a novel hybrid metaheuristic method based on Cuckoo search (CS) and Particle swarm optimization algorithms (PSO). The new model uses adaptive parameters to allow balancing between the segmented components of the model. In addition, to improve the quality of searching solutions in the Maximum a posteriori (MAP) estimation of the HMRF model, the hybrid metaheuristic algorithm is introduced. This algorithm takes into account both the advantages of CS and PSO algorithms in searching ability by cooperating them with the same population in a parallel way and with a solution selection mechanism. Since CS and PSO are performing exploration and exploitation in the search space, respectively, hybridizing them in an intelligent way can provide better solutions in terms of quality. Furthermore, initialization of the population is carefully taken into account to improve the performance of the proposed method. The whole algorithm is evaluated on benchmark images including both the simulated and real MR brain images. Experimental results show that the proposed method can achieve satisfactory performance for images with noise and intensity inhomogeneity, and provides better results than its considered competitors.																	1057-7149	1941-0042					2020	29						6507	6522		10.1109/TIP.2020.2990346													
J								Task-Oriented Network for Image Dehazing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Image color analysis; Atmospheric modeling; Image restoration; Distortion; Convolutional neural networks; Recurrent neural networks; Task-oriented network; multi-stage dehazing algorithm; image dehazing; image restoration		Haze interferes the transmission of scene radiation and significantly degrades color and details of outdoor images. Existing deep neural networks-based image dehazing algorithms usually use some common networks. The network design does not model the image formation of haze process well, which accordingly leads to dehazed images containing artifacts and haze residuals in some special scenes. In this paper, we propose a task-oriented network for image dehazing, where the network design is motivated by the image formation of haze process. The task-oriented network involves a hybrid network containing an encoder and decoder network and a spatially variant recurrent neural network which is derived from the hazy process. In addition, we develop a multi-stage dehazing algorithm to further improve the accuracy by filtering haze residuals in a step-by-step fashion. To constrain the proposed network, we develop a dual composition loss, content-based pixel-wise loss and total variation constraint. We train the proposed network in an end-to-end manner and analyze its effect on image dehazing. Experimental results demonstrate that the proposed algorithm achieves favorable performance against state-of-the-art dehazing methods.																	1057-7149	1941-0042					2020	29						6523	6534		10.1109/TIP.2020.2991509													
J								Semantic Neighborhood-Aware Deep Facial Expression Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Predictive models; Feature extraction; Training; Perturbation methods; Face recognition; Task analysis; Expression recognition; basic emotion; deep learning; autoencoder	REPRESENTATION; PATTERNS	Different from many other attributes, facial expression can change in a continuous way, and therefore, a slight semantic change of input should also lead to the output fluctuation limited in a small scale. This consistency is important. However, current Facial Expression Recognition (FER) datasets may have the extreme imbalance problem, as well as the lack of data and the excessive amounts of noise, hindering this consistency and leading to a performance decreasing when testing. In this paper, we not only consider the prediction accuracy on sample points, but also take the neighborhood smoothness of them into consideration, focusing on the stability of the output with respect to slight semantic perturbations of the input. A novel method is proposed to formulate semantic perturbation and select unreliable samples during training, reducing the bad effect of them. Experiments show the effectiveness of the proposed method and state-of-the-art results are reported, getting closer to an upper limit than the state-of-the-art methods by a factor of 30% in AffectNet, the largest in-the-wild FER database by now.																	1057-7149	1941-0042					2020	29						6535	6548		10.1109/TIP.2020.2991510													
J								Deep Ranking for Image Zero-Shot Multi-Label Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Testing; Training; Predictive models; Semantics; Correlation; Visualization; Training data; Multi-label classification; zero-shot learning; visual-semantic embedding; transductive learning	SIMILARITY	During the past decade, both multi-label learning and zero-shot learning have attracted huge research attention, and significant progress has been made. Multi-label learning algorithms aim to predict multiple labels given one instance, while most existing zero-shot learning approaches target at predicting a single testing label for each unseen class via transferring knowledge from auxiliary seen classes to target unseen classes. However, relatively less effort has been made on predicting multiple labels in the zero-shot setting, which is nevertheless a quite challenging task. In this work, we investigate and formalize a flexible framework consisting of two components, i.e., visual-semantic embedding and zero-shot multi-label prediction. First, we present a deep regression model to project the visual features into the semantic space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors and makes label prediction possible. Then, we formulate the label prediction problem as a pairwise one and employ Ranking SVM to seek the unique multi-label correlations in the embedding space. Furthermore, we provide a transductive multi-label zero-shot prediction approach that exploits the testing data manifold structure. We demonstrate the effectiveness of the proposed approach on three popular multi-label datasets with state-of-the-art performance obtained on both conventional and generalized ZSL settings.																	1057-7149	1941-0042					2020	29						6549	6560		10.1109/TIP.2020.2991527													
J								A Novel Deep Learning Pipeline for Retinal Vessel Detection In Fluorescein Angiography	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pipelines; Deep learning; Retinal vessels; Angiography; Training; Annotations; Fluorescein angiography; generative adversarial networks; vessel detection; retinal image analysis; deep learning	BLOOD-VESSELS; SEGMENTATION; MORPHOLOGY; IMAGES	While recent advances in deep learning have significantly advanced the state of the art for vessel detection in color fundus (CF) images, the success for detecting vessels in fluorescein angiography (FA) has been stymied due to the lack of labeled ground truth datasets. We propose a novel pipeline to detect retinal vessels in FA images using deep neural networks (DNNs) that reduces the effort required for generating labeled ground truth data by combining two key components: cross-modality transfer and human-in-the-loop learning. The cross-modality transfer exploits concurrently captured CF and fundus FA images. Binary vessels maps are first detected from CF images with a pre-trained neural network and then are geometrically registered with and transferred to FA images via robust parametric chamfer alignment to a preliminary FA vessel detection obtained with an unsupervised technique. Using the transferred vessels as initial ground truth labels for deep learning, the human-in-the-loop approach progressively improves the quality of the ground truth labeling by iterating between deep-learning and labeling. The approach significantly reduces manual labeling effort while increasing engagement. We highlight several important considerations for the proposed methodology and validate the performance on three datasets. Experimental results demonstrate that the proposed pipeline significantly reduces the annotation effort and the resulting deep learning methods outperform prior existing FA vessel detection methods by a significant margin. A new public dataset, RECOVERY-FA19, is introduced that includes high-resolution ultra-widefield images and accurately labeled ground truth binary vessel maps.																	1057-7149	1941-0042					2020	29						6561	6573		10.1109/TIP.2020.2991530													
J								A Unified Deep Model for Joint Facial Expression Recognition, Face Synthesis, and Face Alignment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face; Task analysis; Face recognition; Geometry; Feature extraction; Training; Generators; Facial expression recognition; facial image synthesis; generative adversarial network; facial landmarks	GAUSSIAN-PROCESSES; MULTIVIEW; POSE	Facial expression recognition, face synthesis, and face alignment are three coherently related tasks and can be solved in a joint framework. To achieve this goal, in this paper, we propose a novel end-to-end deep learning model by exploiting the expression code, geometry code and generated data jointly for simultaneous pose-invariant facial expression recognition, face image synthesis, and face alignment. The proposed deep model enjoys several merits. First, to the best of our knowledge, this is the first work to address these three tasks jointly in a unified deep model to complement and enhance each other. Second, the proposed model can effectively disentangle the global and local identity representation from different expression and geometry codes. As a result, it can automatically generate facial images with different expressions under arbitrary geometry codes. Third, these three tasks can further boost their performance for each other via our model. Extensive experimental results on three standard benchmarks demonstrate that the proposed deep model performs favorably against state-of-the-art methods on the three tasks.																	1057-7149	1941-0042					2020	29						6574	6589		10.1109/TIP.2020.2991549													
J								FADE: Feature Aggregation for Depth Estimation With Multi-View Stereo	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Feature extraction; Estimation; Image reconstruction; Cameras; Visualization; Computational modeling; Multi-view stereo; depth estimation; feature aggregation; attention mechanism; homography; plane sweep algorithm		Both structural and contextual information is essential and widely used in image analysis. However, current multi-view stereo (MVS) approaches usually use a single common pre-trained model as pixel descriptor to extract features, which mix structural and contextual information together and thus increase the difficulty of matching correspondence. In this paper, we propose FADE (feature aggregation for depth estimation), which treats spatial and context information separately and focuses on aggregating features for efficient learning of the MVS problem. Spatial information includes image details such as edges and corners, whereas context information comprises object features such as shapes and traits. To aggregate these multi-level features, we use an attention mechanism to select important features for matching. We then build a plane sweep volume by using a homography backward warping method to generate match candidates. Furthermore, we propose a novel cost volume regularization network aims to minimize the noise in the matching candidates. Finally, we take advantage of 3D stacked hourglass and regression to produces high-quality depth maps. With these well-aggregated features, FADE can efficiently perform dense depth reconstruction, achieving state-of-the-art performance in terms of accuracy and requiring the least amount of model parameters.																	1057-7149	1941-0042					2020	29						6590	6600		10.1109/TIP.2020.2991883													
J								Polarimetric SAR Image Semantic Segmentation With 3D Discrete Wavelet Transform and Markov Random Field	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Feature extraction; Semantics; Multiresolution analysis; Task analysis; Speckle; PolSAR image segmentation; three-dimensional discrete wavelet transform (3D-DWT); support vector machine (SVM); Markov random field (MRF)	SCATTERING MODEL; LAND-COVER; CLASSIFICATION; DECOMPOSITION	Polarimetric synthetic aperture radar (PolSAR) image segmentation is currently of great importance in image processing for remote sensing applications. However, it is a challenging task due to two main reasons. Firstly, the label information is difficult to acquire due to high annotation costs. Secondly, the speckle effect embedded in the PolSAR imaging process remarkably degrades the segmentation performance. To address these two issues, we present a contextual PolSAR image semantic segmentation method in this paper. With a newly defined channel-wise consistent feature set as input, the three-dimensional discrete wavelet transform (3D-DWT) technique is employed to extract discriminative multi-scale features that are robust to speckle noise. Then Markov random field (MRF) is further applied to enforce label smoothness spatially during segmentation. By simultaneously utilizing 3D-DWT features and MRF priors for the first time, contextual information is fully integrated during the segmentation to ensure accurate and smooth segmentation. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on three real benchmark PolSAR image data sets. Experimental results indicate that the proposed method achieves promising segmentation accuracy and preferable spatial consistency using a minimal number of labeled pixels.																	1057-7149	1941-0042					2020	29						6601	6614		10.1109/TIP.2020.2992177													
J								Robust Estimation of Absolute Camera Pose via Intersection Constraint and Flow Consensus	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cameras; Pose estimation; Three-dimensional displays; Two dimensional displays; Gravity; Reliability; Structure from motion; absolute camera pose; outliers; 3D-to-2D correspondences; points and; or lines	LINE CORRESPONDENCES	Estimating the absolute camera pose requires 3D-to-2D correspondences of points and/or lines. However, in practice, these correspondences are inevitably corrupted by outliers, which affects the pose estimation. Existing outlier removal strategies for robust pose estimation have some limitations. They are only applicable to points, rely on prior pose information, or fail to handle high outlier ratios. By contrast, we propose a general and accurate outlier removal strategy. It can be integrated with various existing pose estimation methods originally vulnerable to outliers, and is applicable to points, lines, and the combination of both. Moreover, it does not rely on any prior pose information. Our strategy has a nested structure composed of the outer and inner modules. First, our outer module leverages our intersection constraint, i.e., the projection rays or planes defined by inliers intersect at the camera center. Our outer module alternately computes the inlier probabilities of correspondences and estimates the camera pose. It can run reliably and efficiently under high outlier ratios. Second, our inner module exploits our flow consensus. The 2D displacement vectors or 3D directed arcs generated by inliers exhibit a common directional regularity, i.e., follow a dominant trend of flow. Our inner module refines the inlier probabilities obtained at each iteration of our outer module. This refinement improves the accuracy and facilitates the convergence of our outer module. Experiments on both synthetic data and real-world images have shown that our method outperforms state-of-the-art approaches in terms of accuracy and robustness.																	1057-7149	1941-0042					2020	29						6615	6629		10.1109/TIP.2020.2992336													
J								Light Field Synthesis by Training Deep Network in the Refocused Image Domain	IEEE TRANSACTIONS ON IMAGE PROCESSING										Light fields; Image quality; Feature extraction; Cameras; Image resolution; Measurement; Training; Light field; view synthesis; CNN; image refocusing		Light field imaging, which captures spatial-angular information of light incident on image sensors, enables many interesting applications such as image refocusing and augmented reality. However, due to the limited sensor resolution, a trade-off exists between the spatial and angular resolutions. To increase the angular resolution, view synthesis techniques have been adopted to generate new views from existing views. However, traditional learning-based view synthesis mainly considers the image quality of each view of the light field and neglects the quality of the refocused images. In this paper, we propose a new loss function called refocused image error (RIE) to address the issue. The main idea is that the image quality of the synthesized light field should be optimized in the refocused image domain because it is where the light field is viewed. We analyze the behavior of RIE in the spectral domain and test the performance of our approach against previous approaches on both real (INRIA) and software-rendered (HCI) light field datasets using objective assessment metrics such as MSE, MAE, PSNR, SSIM, and GMSD. Experimental results show that the light field generated by our method results in better refocused images than previous methods.																	1057-7149	1941-0042					2020	29						6630	6640		10.1109/TIP.2020.2992354													
J								SAR Image Speckle Filtering With Context Covariance Matrix Formulation and Similarity Test	IEEE TRANSACTIONS ON IMAGE PROCESSING										Covariance matrices; Speckle; Synthetic aperture radar; Microwave filters; Optical filters; Scattering; Information filtering; Synthetic aperture radar (SAR); speckle filter; context covariance matrix; similarity test; very high resolution (VHR)	NOISE; DECOMPOSITION; REDUCTION; DETECTOR; MODEL	Speckle filtering of synthetic aperture radar (SAR) image is a necessary pre-processing for many subsequent applications. The challenge lies in how to adaptively select a sufficient number of similar pixels for an unbiased estimator generation. A novel SAR speckle filter is proposed and the core idea contains two aspects. Firstly, a context covariance matrix representation is developed within a local neighborhood to characterize the contexture information. Then, the Wishart statistic test is extended to examine the similarity of context covariance matrices. The extended similarity test indicator derived from context covariance matrices is verified to be sensitive for similar pixel localization. Thereafter, a sample averaging estimator is adopted based on the similar samples determined by the context covariance matrices similarity test (the proposed method is named as the CCM+SimiTest). Furthermore, a fast similarity test computation scheme is established which can handle large images smoothly even with a normal laptop. Intensive experimental studies with Radarsat-2, MiniSAR and ALOS-2 datasets are carried out. Comparisons with several state-of-the-art methods from both subjective and objective viewpoints demonstrate the superiority of the proposed method.																	1057-7149	1941-0042					2020	29						6641	6654		10.1109/TIP.2020.2992883													
J								Textual-Visual Reference-Aware Attention Network for Visual Dialog	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visual dialog; attention network; textual reference; visual reference; multimodal semantic interaction		Visual dialog is a challenging task in multimedia understanding, which requires the dialog agent to answer a series of questions that are based on an input image. The critical issue to produce an exact answer is how to model the mutual semantic interaction among feature representations of the image, question-answer history, and current question. In this study, we propose a textual-visual Reference-Aware Attention Network (RAA-Net), which aims to effectively fuse Q (question), H (history), V-l (local vision), and V-g (global vision) to infer the exact answer. In the multimodal feature flows, RAA-Net first learns the textual context through multi-head attention between Q and H and then guides the textual reference semantics to the image to capture visual reference semantics by self- and cross-reference-aware attention in and between V-l and V-g. In the proposed RAA-Net, we exploit the two-stage (intra- and inter-) visual reasoning mechanism on V-l and V-g. Extensive experiments on the VisDial v0.9 and v1.0 datasets show that RAA-Net achieves state-of-the-art performance. Visualization results on both visual and textual attention maps further validate the remarkable interpretability achieved by our solution.																	1057-7149	1941-0042					2020	29						6655	6666		10.1109/TIP.2020.2992888													
J								Three Dimensional Root CT Segmentation Using Multi-Resolution Encoder-Decoder Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Computed tomography; Soil; Image resolution; Decoding; Three-dimensional displays; Biomedical imaging; X-ray computed tomography; image segmentation; deep learning; root system analysis; plant phenotyping	ARCHITECTURE; SOFTWARE	We address the complex problem of reliably segmenting root structure from soil in X-ray Computed Tomography (CT) images. We utilise a deep learning approach, and propose a state-of-the-art multi-resolution architecture based on encoder-decoders. While previous work in encoder-decoders implies the use of multiple resolutions simply by downsampling and upsampling images, we make this process explicit, with branches of the network tasked separately with obtaining local high-resolution segmentation, and wider low-resolution contextual information. The complete network is a memory efficient implementation that is still able to resolve small root detail in large volumetric images. We compare against a number of different encoder-decoder based architectures from the literature, as well as a popular existing image analysis tool designed for root CT segmentation. We show qualitatively and quantitatively that a multi-resolution approach offers substantial accuracy improvements over a both a small receptive field size in a deep network, or a larger receptive field in a shallower network. We then further improve performance using an incremental learning approach, in which failures in the original network are used to generate harder negative training examples. Our proposed method requires no user interaction, is fully automatic, and identifies large and fine root material throughout the whole volume.																	1057-7149	1941-0042					2020	29						6667	6679		10.1109/TIP.2020.2992893													
J								Structured Dictionary Learning for Image Denoising Under Mixed Gaussian and Impulse Noise	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image denoising; image enhancement; image restoration; noise; Gaussian noise; impulse noise	SPARSE REPRESENTATION; REMOVAL; ALGORITHM; MINIMIZATION; MATRIX; DECOMPOSITION; NONCONVEX; FRAMEWORK; MODEL; SALT	Although image denoising as a basic task of image restoration has been widely studied in the past decades, there are not many studies on mixed noise denoising. In this paper, we propose two structured dictionary learning models to recover images corrupted by mixed Gaussian and impulse noise. These two models can be merged as l(p)-norm fidelity plus l(q)-norm regularization. The fidelity term is used to fit image patches and the regularization term is employed for sparse coding. Particularly, we utilize proximal (and proximal linearized) alternating minimization methods as the main solvers to deal with these two models. We remove the Gaussian noise under the assumption that the uncorrupted image can be approximated with a linear representation under an appropriate orthogonal basis. We use different ways to remove impulse noise for these two models. The experimental results are reported to compare the existing methods and demonstrate the performance of the proposed denoising model is better than the other existing methods in terms of some quality assessment metrics.																	1057-7149	1941-0042					2020	29						6680	6693		10.1109/TIP.2020.2992895													
J								Long-Term Tracking With Deep Tracklet Association	IEEE TRANSACTIONS ON IMAGE PROCESSING										Target tracking; Trajectory; Radar tracking; Detectors; Machine learning; Robustness; Multi-object tracking (MOT); tracking-by-tracklet; multiple hypothesis tracking (MHT); deep association	MULTITARGET; ALGORITHM	Recently, most multiple object tracking (MOT) algorithms adopt the idea of tracking-by-detection. Relevant research shows that the performance of the detector obviously affects the tracker, while the improvement of detector is gradually slowing down in recent years. Therefore, trackers using tracklet (short trajectory) are proposed to generate more complete trajectories. Although there are various tracklet generation algorithms, the fragmentation problem still often occurs in crowded scenes. In this paper, we introduce an iterative clustering method that generates more tracklets while maintaining high confidence. Our method shows robust performance on avoiding internal identity switch. Then we propose a deep association method for tracklet association. In terms of motion and appearance, we construct motion evaluation network (MEN) and appearance evaluation network (AEN) to learn long-term features of tracklets for association. In order to explore more robust features of tracklets, a tracklet-based training mechanism is also introduced. Tracklet groups are used as the input of the networks instead of discrete detections. Experimental results show that our training method enhances the performance of the networks. In addition, our tracking framework generates more complete trajectories while maintaining the unique identity of each target as the same time. On the latest MOT 2017 benchmark, we achieve state-of-the-art results.																	1057-7149	1941-0042					2020	29						6694	6706		10.1109/TIP.2020.2993073													
J								Interpret Neural Networks by Extracting Critical Subnetworks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Predictive models; Logic gates; Neural networks; Machine learning; Feature extraction; Robustness; Visualization; Model interpretability; model pruning; adversarial robustness		In recent years, deep neural networks have achieved excellent performance in many fields of artificial intelligence. The requirements for the interpretability and robustness of neural networks are also increasing. In this paper, we propose to understand the functional mechanism of neural networks by extracting critical subnetworks. Specifically, we denote the critical subnetworks as a group of important channels across layers such that if they were suppressed to zeros, the final test performance would deteriorate severely. This novel perspective can not only reveal the layerwise semantic behavior within the model but also present more accurate visual explanations appearing in the data through attribution methods. Moreover, we propose two adversarial example detection methods based on the properties of sample-specific and class-specific subnetworks, which provides the possibility for increasing the model robustness.																	1057-7149	1941-0042					2020	29						6707	6720		10.1109/TIP.2020.2993098													
J								Multi-Exponential Transverse Relaxation Times Estimation From Magnetic Resonance Images Under Rician Noise and Spatial Regularization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Magnetic resonance imaging; Rician channels; Minimization; Parameter estimation; Signal to noise ratio; Maximum likelihood estimation; Multi-exponential decay; rician noise; maximum likelihood estimation; majoration-minimization; Levenberg-Marquardt; T2~relaxation times	LIKELIHOOD-ESTIMATION; T-2 RELAXATION; IRON OVERLOAD; MRI; ALGORITHM; MAGNITUDE; RECONSTRUCTION; RELAXOMETRY; SEQUENCE; PHANTOM	Relaxation signal inside each voxel of magnetic resonance images (MRI) is commonly fitted by a multi-exponential decay curve. The estimation of a discrete multi-component relaxation model parameters from magnitude MRI data is a challenging nonlinear inverse problem since it should be conducted on the entire image voxels under non-Gaussian noise statistics. This paper proposes an efficient algorithm allowing the joint estimation of relaxation time values and their amplitudes using different criteria taking into account a Rician noise model, combined with a spatial regularization accounting for low spatial variability of relaxation time constants and amplitudes between neighboring voxels. The Rician noise hypothesis is accounted for either by an adapted nonlinear least squares algorithm applied to a corrected least squares criterion or by a majorization-minimization approach applied to the maximum likelihood criterion. In order to solve the resulting large-scale non-negativity constrained optimization problem with a reduced numerical complexity and computing time, an optimization algorithm based on a majorization approach ensuring separability of variables between voxels is proposed. The minimization is carried out iteratively using an adapted Levenberg-Marquardt algorithm that ensures convergence by imposing a sufficient decrease of the objective function and the non-negativity of the parameters. The importance of the regularization alongside the Rician noise incorporation is shown both visually and numerically on a simulated phantom and on magnitude MRI images acquired on fruit samples.																	1057-7149	1941-0042					2020	29						6721	6733		10.1109/TIP.2020.2993114													
J								An Unordered Image Stitching Method Based on Binary Tree and Estimated Overlapping Area	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image stitching; unordered images; binary tree; overlapping area estimation	STRUCTURE-FROM-MOTION; FUSION; FEATURES	Aiming at the complex computation and time-consuming problem during unordered image stitching, we present a method based on the binary tree and the estimated overlapping areas to stitch images without order in this paper. For image registration, the overlapping areas between input images are estimated, so that the extraction and matching of feature points are only performed in these areas. For image stitching, we build a model of the binary tree to stitch each two matched images without sorting. Compared to traditional methods, our method significantly reduces the computational time of matching irrelevant image pairs and improves the efficiency of image registration and stitching. Moreover, the stitching model of the binary tree proposed in this paper further reduces the distortion of the panorama. Experimental results show that the number of extracted feature points in the estimated overlapping area is approximately 0.3 similar to 0.6 times of that in the entire image by using the same method, which greatly reduces the computational time of feature extraction and matching. Compared to the exhaustive image matching method, our approach only takes about 1/3 of the time to find all matching images.																	1057-7149	1941-0042					2020	29						6734	6744		10.1109/TIP.2020.2993134													
J								Self-Supervised Feature Augmentation for Large Image Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Object detection; Task analysis; Image resolution; Pipelines; Convolution; Detectors; Self-supervise; feature augmentation; object detection; large image		Input scale plays an important role in modern detection frameworks, and an optimal training scale for images exists empirically. However, the optimal one usually cannot be reached in facing extremely large images under the memory constraint. In this study, we explore the scale effect inside the object detection pipeline and find that feature upsampling with the introduction of high-resolution information benefits the detection. Compared with direct input upscaling, feature upsampling trades a small performance loss for a large amount of memory savings. From these observations, we propose a self-supervised feature augmentation network, which takes downsampled images as inputs and aims to generate comparable features with the ones when feeding upscaled images to networks. We present a guided feature upsampling module, which takes downsampled images as inputs, to learn upscaled feature representations with the supervision of real large features acquired from upscaled images. In a self-supervised learning manner, we can introduce detailed information of images to the network. For an efficient feature upsampling, we design a residualized sub-pixel convolution block based on a sub-pixel convolution layer, which involves considerable information in upsampling process. Experiments on Mapillary Vistas Dataset (MVD), Cityscapes, and COCO are conducted to demonstrate the effectiveness of our method. On the MVD and Cityscapes detection benchmarks, in which the images are extremely large, our method surpasses current approaches. On COCO, the proposed method obtains comparable results to existing methods but with higher efficiency.																	1057-7149	1941-0042					2020	29						6745	6758		10.1109/TIP.2020.2993403													
J								Removing Arbitrary-Scale Rain Streaks via Fractal Band Learning With Self-Supervision	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rain; Feature extraction; Training; Machine learning; Biological system modeling; Fractals; Degradation; Rain streak removal; varied scale; frequency band learning; self-supervision; deep network	QUALITY ASSESSMENT; IMAGE; NETWORK; MODEL	Data-driven rain streak removal methods, most of which rely on synthesized paired data, usually come across the generalization problem when being applied in real scenarios. In this paper, we propose a novel deep-learning based rain streak removal method injected with self-supervision to obtain the capacity of removing more varied-scale rain streaks in practical applications. To this end, in this work, efforts are made from two perspectives. First, considering that rain streak removal is highly correlated with texture characteristics, we create a fractal band learning (FBL) network based on frequency band recovery. It integrates commonly seen band feature operations as neural forms and effectively improves the capacity to capture discriminative features for deraining. Second, to further improve the generalization ability of FBL to remove rain streaks of varied scales, we incorporate scale-robust self-supervision to regularize the network training. The constraint forces the extracted features of an input rain image at different scales to be equivalent after rescaling operations. Therefore, our method can offer similar responses based on solely image content without the interference of scale change and is capable to remove varied-scale rain streaks. Extensive experiments in quantitative and qualitative evaluations demonstrate the superiority of our method for rain streak removal, especially for the real cases where very large rain streaks exist, and prove the effectiveness of each component.																	1057-7149	1941-0042					2020	29						6759	6772		10.1109/TIP.2020.2993406													
J								PMHLD: Patch Map-Based Hybrid Learning DehazeNet for Single Image Haze Removal	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Distortion; Generative adversarial networks; Prediction algorithms; Atmospheric modeling; Gallium nitride; Generators; Haze removal; end-to-end hybrid learning system; dark channel prior; patch map; bi-attentive generative adversarial network	FRAMEWORK; NETWORK	Images captured in a hazy environment usually suffer from bad visibility and missing information. Over many years, learning-based and handcrafted prior-based dehazing algorithms have been rigorously developed. However, both algorithms exhibit some weaknesses in terms of haze removal performance. Therefore, in this work, we have proposed the patch-map-based hybrid learning DehazeNet, which integrates these two strategies by using a hybrid learning technique involving the patch map and a bi-attentive generative adversarial network. In this method, the reasons limiting the performance of the dark channel prior (DCP) have been analyzed. A new feature called the patch map has been defined for selecting the patch size adaptively. Using this map, the limitations of the DCP (e.g., color distortion and failure to recover images involving white scenes) can be addressed efficiently. In addition, to further enhance the performance of the method for haze removal, a patch-map-based DCP has been embedded into the network, and this module has been trained with the atmospheric light generator, patch map selection module, and refined module simultaneously. A combination of traditional and learning-based methods can efficiently improve the haze removal performance of the network. Experimental results show that the proposed method can achieve better reconstruction results compared to other state-of-the-art haze removal algorithms.																	1057-7149	1941-0042					2020	29						6773	6788		10.1109/TIP.2020.2993407													
J								Group Feedback Capsule Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Routing; Neurons; Transforms; Heuristic algorithms; Convolution; Nose; Electronic mail; Capsule networks; network architecture design		In capsule networks (CapsNets), the capsule is made up of collections of neurons. Their adjacent capsule layers are connected using routing-by-agreement mechanisms in an unsupervised way. The routing-by-agreement mechanisms have two main drawbacks: a) too many parameters and high computation complexity; b) the cluster distribution assumptions of these routing mechanisms may not hold in some complex real-world data. In this paper, we propose a novel Group Feedback Capsule Network (GF-CapsNet) which adopts a supervised routing strategy called group-routing. Compared with the previous routing strategies which globally transform each capsule, Group-routing equally splits capsules into groups where capsules locally share the same transformation weights, reducing routing parameters. To address the second drawback, we devise a distance network to directly predict capsules in a supervised way without making distribution assumptions. Our proposed group-routing captures local information of low-level capsules by group-wise transformation and supervisedly predicts high-level ones in a feedback way to address two drawbacks respectively. We conduct experiments on CIFAR-10/100 and SVHN datasets and the results show that our method can perform better against state-of-the-arts.																	1057-7149	1941-0042					2020	29						6789	6799		10.1109/TIP.2020.2993931													
J								Crowd Counting Via Cross-Stage Refinement Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Convolution; Decoding; Clutter; Benchmark testing; Cameras; Network architecture; Crowd counting; recurrent network; image refinement		Crowd counting is challenging due to unconstrained imaging factors, e.g., background clutters, non-uniform distribution of people, large scale and perspective variations. Dealing with these problems using deep neural networks requires rich prior knowledge and multi-scale contextual representations. In this paper, we propose a Cross-stage Refinement Network (CRNet) that can refine predicted density maps progressively based on hierarchical multi-level density priors. In particular, CRNet is composed of several fully convolutional networks. They are stacked together recursively with the previous output as the next input, and each of them serves to utilize previous density output to gradually correct prediction errors of crowd areas and refine the predicted density maps at different stages. Cross-stage multi-level density priors are further exploited in our recurrent framework by the cross-stage skip layers based on ConvLSTM. To cope with different challenges of unconstrained crowd scenes, we explore different crowd-specific data augmentation methods to mimic real-world scenarios and enrich crowd feature representations from different aspects. Extensive experiments show the proposed method achieves superior performances against state-of-the-art methods on four widely-used challenging benchmarks in terms of counting accuracy and density map quality. Code and models are available at this https://github.com/lytgftyf/Crowd-Counting-via-Cross-stage-Refinement-Networks.																	1057-7149	1941-0042					2020	29						6800	6812		10.1109/TIP.2020.2994410													
J								Hyperspectral Image Compressive Sensing Reconstruction Using Subspace-Based Nonlocal Tensor Ring Decomposition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tensile stress; Correlation; Image coding; Image reconstruction; TV; Computational efficiency; Dictionaries; Compressive sensing; hyperspectral image; subspace; nonlocal self-similarity; tensor ring decomposition	MINIMIZATION; RESTORATION; CONVERGENCE; RECOVERY	Hyperspectral image compressive sensing reconstruction (HSI-CSR) can largely reduce the high expense and low efficiency of transmitting HSI to ground stations by storing a few compressive measurements, but how to precisely reconstruct the HSI from a few compressive measurements is a challenging issue. It has been proven that considering the global spectral correlation, spatial structure, and nonlocal self-similarity priors of HSI can achieve satisfactory reconstruction performances. However, most of the existing methods cannot simultaneously capture the mentioned priors and directly design the regularization term to the HSI. In this article, we propose a novel subspace-based nonlocal tensor ring decomposition method (SNLTR) for HSI-CSR. Instead of designing the regularization of the low-rank approximation to the HSI, we assume that the HSI lies in a low-dimensional subspace. Moreover, to explore the nonlocal self-similarity and preserve the spatial structure of HSI, we introduce a nonlocal tensor ring decomposition strategy to constrain the related coefficient image, which can decrease the computational cost compared to the methods that directly employ the nonlocal regularization to HSI. Finally, a well-known alternating minimization method is designed to efficiently solve the proposed SNLTR. Extensive experimental results demonstrate that our SNLTR method can significantly outperform existing approaches for HSI-CSR.																	1057-7149	1941-0042					2020	29						6813	6828		10.1109/TIP.2020.2994411													
J								MV-GNN: Multi-View Graph Neural Network for Compression Artifacts Reduction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Neural networks; Three-dimensional displays; Transform coding; Feature extraction; Image reconstruction; Visualization; Compression artifacts reduction; graph neural network; multi-view video compression	VIDEO; INFORMATION; EXTENSIONS; DEBLOCKING	Inevitable compression artifacts in multi-view video (MVV) can clearly degrade the quality of experience in many interaction-oriented 3D visual applications. Under the framework of asymmetric coding, low-quality images can be enhanced with high-quality images from the neighboring viewpoints considering the similarity among different views. However, compression artifacts and warping error cause different cross-view quality gaps for various sequences, and thus the contribution of cross-view priors can hardly be located and considered in previous works. In this paper, we propose a multi-view graph neural network (MV-GNN) to reduce compression artifacts in multi-view compressed images. We dedicate to design a fusion mechanism which can exploit contributions from neighboring viewpoints and meanwhile suppress the misleading information. In our method, a GNN-based fusion mechanism is designed to fuse the cross-view information under the aggregation and update mechanism of GNN. Experiments show that 1.672 dB and 0.0242 average gains on PSNR and SSIM metrics can be obtained, respectively. For the subjective evaluations, blocking effect in the compressed images are clearly suppressed and the damaged object boundary are better recovered. The experimental results demonstrate that our MV-GNN outperforms the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						6829	6840		10.1109/TIP.2020.2994412													
J								Integrating Neural Networks Into the Blind Deblurring Framework to Compete With the End-to-End Learning-Based Methods	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; Neural networks; Kernel; Optimization; Learning systems; Estimation; Training; Image deblurring; image motion analysis; image restoration; muti-layer neural network; supervised learning		Recently, the end-to-end learning-based methods have been proven effective for the blind image deblurring. Without human-made assumptions or numerical algorithms, they are able to restore images with fewer artifacts and better perceptual quality. However, in practice, these methods suffer from limited performance under complex motion scenario and produces unnatural results sometimes. In this paper, in order to overcome their limitations, we propose to integrate deep convolution neural networks into a conventional deblurring framework. Specifically, we propose Stacked Estimation Residual Net (SEN) to estimate the motion flow map and Recurrent Prior Generative and Adversarial Net (RP-GAN) to learn the implicit image prior for the optimization. Comparing with the state-of-the-art end-to-end learning-based methods, the proposed method restores image content more naturally and shows better generalization ability.																	1057-7149	1941-0042					2020	29						6841	6851		10.1109/TIP.2020.2994413													
J								Single Image Deraining Using Bilateral Recurrent Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rain; Convolutional neural networks; Feature extraction; Machine learning; Electronic mail; Task analysis; Degradation; Image deraining; convolutional neural network; recurrent network; LSTM	REMOVAL	Single image deraining has received considerable progress based on deep convolutional neural network (CNN). In existing deep deraining methods, CNNs are deployed to extract rain streaks while failing in learning direct mapping from rainy image to clean background image, and their architectures become more and more complicated. In this work, we first propose a single recurrent network (SRN) by recursively unfolding a shallow residual network, where a recurrent layer is adopted to propagate deep features across multiple stages. This simple SRN is effective not only in learning residual mapping for extracting rain streaks, but also in learning direct mapping for predicting clean background image. Furthermore, two SRNs are coupled to simultaneously exploit rain streak layer and clean background image layer. Instead of naive combination, we propose bilateral LSTMs, which not only can respectively propagate deep features of rain streak layer and background image layer across stages, but also bring the interplay between these two SRNs, finally forming bilateral recurrent network (BRN). The experimental results demonstrate that our BRN notably outperforms state-of-the-art deep deraining networks on synthetic datasets quantitatively and qualitatively. The proposed methods also perform more favorably in terms of generalization performance on real-world rainy dataset. All the source code and pre-trained models are available at https://github.com/csdwren/RecDerain.																	1057-7149	1941-0042					2020	29						6852	6863		10.1109/TIP.2020.2994443													
J								Multi-Atlas Brain Parcellation Using Squeeze-and-Excitation Fully Convolutional Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Brain parcellation; fully convolutional networks; squeeze-and-excitation module; brain atlas selection	DIFFEOMORPHIC IMAGE REGISTRATION; LABEL FUSION; SEGMENTATION	Multi-atlas parcellation (MAP) is carried out on a brain image by propagating and fusing labelled regions from brain atlases. Typical nonlinear registration-based label propagation is time-consuming and sensitive to inter-subject differences. Recently, deep learning parcellation (DLP) has been proposed to avoid nonlinear registration for better efficiency and robustness than MAP. However, most existing DLP methods neglect using brain atlases, which contain high-level information (e.g., manually labelled brain regions), to provide auxiliary features for improving the parcellation accuracy. In this paper, we propose a novel multi-atlas DLP method for brain parcellation. Our method is based on fully convolutional networks (FCN) and squeeze-and-excitation (SE) modules. It can automatically and adaptively select features from the most relevant brain atlases to guide parcellation. Moreover, our method is trained via a generative adversarial network (GAN), where a convolutional neural network (CNN) with multi-scale l(1) loss is used as the discriminator. Benefiting from brain atlases, our method outperforms MAP and state-of-the-art DLP methods on two public image datasets (LPBA40 and NIREP-NAO).																	1057-7149	1941-0042					2020	29						6864	6872		10.1109/TIP.2020.2994445													
J								Sparse Domain Gaussianization for Multi-Variate Statistical Modeling of Retinal OCT Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Transforms; Hidden Markov models; Correlation; Data models; Probability density function; Noise reduction; Computational modeling; Statistical model; multivariate probability density function; sparse; Gaussianization; optical coherence tomography; denoising	OPTICAL COHERENCE TOMOGRAPHY; SPECKLE NOISE-REDUCTION; MIXTURE-MODELS; TRANSFORM; SEGMENTATION; COMPRESSION; SUPPRESSION; ALGORITHM; FILTER	In this paper, a multivariate statistical model that is suitable for describing Optical Coherence Tomography (OCT) images is introduced. The proposed model is comprised of a multivariate Gaussianization function in sparse domain. Such an approach has two advantages, i.e. 1) finding a function that can effectively transform the input - which is often not Gaussian - into normally distributed samples enables the reliable application of methods that assume Gaussianity, 2) although multivariate Gaussianization in spatial domain is a complicated task and rarely results in closed-form analytical model, by transferring data to sparse domain, our approach facilitates multivariate statistical modeling of OCT images. To this end, a proper multivariate probability density function (pdf) which considers all three properties of OCT images in sparse domains (i.e. compression, clustering, and persistence properties) is designed and the proposed sparse domain Gaussianization framework is established. Using this multivariate model, we show that the OCT images often follow a 2-component multivariate Laplace mixture model in the sparse domain. To evaluate the performance of the proposed model, it is employed for OCT image denoising in a Bayesian framework. Visual and numerical comparison with previous prominent methods reveals that our method improves the overall contrast of the image, preserves edges, suppresses background noise to a desirable amount, but is less capable of maintaining tissue texture. As a result, this method is suitable for applications where edge preservation is crucial, and a clean noiseless image is desired.																	1057-7149	1941-0042					2020	29						6873	6884		10.1109/TIP.2020.2994454													
J								Dark and Bright Channel Prior Embedded Network for Dynamic Scene Deblurring	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; Kernel; Network architecture; Optimization; Machine learning; Training; Convolutional neural networks; Dynamic scene deblurring; convolutional neural network; dark and bright channel priors; multi-scale strategy	DECONVOLUTION	Recent years have witnessed the significant progress on convolutional neural networks (CNNs) in dynamic scene deblurring. While most of the CNN models are generally learned by the reconstruction loss defined on training data, incorporating suitable image priors as well as regularization terms into the network architecture could boost the deblurring performance. In this work, we propose a Dark and Bright Channel Priors embedded Network (DBCPeNet) to plug the channel priors into a neural network for effective dynamic scene deblurring. A novel trainable dark and bright channel priors embedded layer (DBCPeL) is developed to aggregate both channel priors and blurry image representations, and a sparse regularization is introduced to regularize the DBCPeNet model learning. Furthermore, we present an effective multi-scale network architecture, namely image full scale exploitation (IFSE), which works in both coarse-to-fine and fine-to-coarse manners for better exploiting information flow across scales. Experimental results on the GoPro and Kohler datasets show that our proposed DBCPeNet performs favorably against state-of-the-art deep image deblurring methods in terms of both quantitative metrics and visual quality.																	1057-7149	1941-0042					2020	29						6885	6897		10.1109/TIP.2020.2995048													
J								Efficient Low-Resolution Face Recognition via Bridge Distillation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face recognition in the wild; two-stream architecture; knowledge distillation; CNNs		Face recognition in the wild is now advancing towards light-weight models, fast inference speed and resolution-adapted capability. In this paper, we propose a bridge distillation approach to turn a complex face model pretrained on private high-resolution faces into a light-weight one for low-resolution face recognition. In our approach, such a cross-dataset resolution-adapted knowledge transfer problem is solved via two-step distillation. In the first step, we conduct cross-dataset distillation to transfer the prior knowledge from private high-resolution faces to public high-resolution faces and generate compact and discriminative features. In the second step, the resolution-adapted distillation is conducted to further transfer the prior knowledge to synthetic low-resolution faces via multi-task learning. By learning low-resolution face representations and mimicking the adapted high-resolution knowledge, a light-weight student model can be constructed with high efficiency and promising accuracy in recognizing low-resolution faces. Experimental results show that the student model performs impressively in recognizing low-resolution faces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed reaches up to 14,705, 934 and 763 faces per second on GPU, CPU and mobile phone, respectively.																	1057-7149	1941-0042					2020	29						6898	6908		10.1109/TIP.2020.2995049													
J								Supply chain risk assessment model based on cloud model with subjective preference weight allocation algorithm	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Risk evaluation; cloud model; preference weight; supply chain	DISRUPTIONS; MITIGATION; SELECTION	The supply chain management philosophy has often been used by organizations to achieve a competitive advantage, but this has led to increased vulnerability of these supply chains (SC) to certain risks. This paper proposes a supply chain risk evaluation model based on the analysis of existing risk evaluation systems. This paper accomplishes the following goals. First, it integrates key indicators to compensate for the shortcomings of traditional risk indices. Second, the proposed model also measures the risk of a company?s supply chain effectively. We propose a subjective group preference weight allocation algorithm for a comprehensive evaluation. The simulation experiment shows that the proposed model can measure quantitative and qualitative indices in the supply chain effectively.																	1872-4981	1875-8843					2020	14	2					133	142		10.3233/IDT-180001													
J								Modeling and parameters extraction of photovoltaic cell and modules using the genetic algorithms with lambert W-function as objective function	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Photovoltaic solar cell; parameter extraction; genetic algorithm; optimization; Lambert function	APPROXIMATE ANALYTICAL SOLUTION; SOLAR-CELL; DIODE; IDENTIFICATION; SINGLE	In this paper, a method based on genetic algorithms is proposed for improving the accuracy of solar cell parameters extracted using novel technique. We propose a computational based binary-coded genetic algorithm (GA) to extract the parameters (I-o, I-ph, n, R-s and R-sh) for a single diode model of solar cell from its current-voltage (I-V) characteristic. The algorithm was implemented using Matlab as a programming tool and validated by applying it to the I-V curve synthesized from the literature using reported values. The characterization, current-voltage data used was generated by simulating a one-diode solar cell model of specified parameters. The new approach is based on formulating I-V equation of solar cell, with Lambert function, the parameter extraction as a search and optimization problem. Compared with other optimization techniques in literatures, the approach proposed for the determination of parameters are in good agreement.																	1872-4981	1875-8843					2020	14	2					143	151		10.3233/IDT-180015													
J								A robust approach to detect gas bubbles through images analysis	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Bubble detection; image processing; image segmentation; graph cut algorithm; Haar wavelet transform	RESTORATION	Bubble detection is a challenging problem in automatic process control in the power and energy industry, medical and pharmaceutical industry and many other fields. Computer vision methods applications for bubble detection and measurement is the principal step of robust bubbles monitoring systems development. In various applications the input image may include a diverse and image background, especially in different environments. This paper presents a new and effective bubble detection approach. The main steps of this proposed approach are as follows: image preprocessing, background subtraction, and contour detection. The graph cut algorithm is used for image segmentation. The Haar wavelet transform is applied to collect bubble component points. The developed approach is evaluated based on the real data set.																	1872-4981	1875-8843					2020	14	2					153	158		10.3233/IDT-180130													
J								Improving task scheduling by using a fuzzy reasoner	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Task scheduling; fuzzy logic; rule-based fuzzy reasoner; SRTF; EDF		In this paper a rule-based fuzzy scheduling method is presented. Aim of this method is to handle the uncertainty and vagueness that characterize the complicated process of task scheduling. It uses fuzzy sets to describe both a task's deadline and a task's remaining execution time. The operation of the presented scheduling method is based on a rule-based reasoner that decides dynamically about the priority of the tasks that wait to be executed. This reasoner is triggered each time a change occurs (e.g. the execution of a task ends, a new task arrives etc.). It has been compared with Earlied Deadline First (EDF) scheduling algorithm. The results showed that the presented fuzzy scheduling method improves the scheduling process, since it minimizes both, the mean waiting time and the mean turnaround time.																	1872-4981	1875-8843					2020	14	2					165	170		10.3233/IDT-190110													
J								Fuzzy IDS as a service on the cloud for malicious TCP port scanning traffic detection	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										IDS; fuzzy IDS as a service; port scanning; fuzzy logic controller; mamdani inference; PSCL	INTRUSION DETECTION SYSTEM; RULES	Port scanning is a first common discovering step which allows cyber malicious actors to gather valuable information about target hosts namely defense, governmental and banks servers by trying to identify instantly open ports, which correspond to specific services on the cloud, such as HTTP, DNS, and email. This paper aims to introduce a detection and evaluation approach for port scanning attacks in various contexts and levels of criticity based on fuzzy reasoning method. A new fuzzy logic controller, which uses fuzzy rules base and the Mamdani inference method is proposed as Intrusion Detection System as a Service, which dynamically detect and evaluate the criticity of port scanning. This SaaS enables network administrators and cyber security specialists to follow in real time the network traffic behavior, i.e., the Port Scanning Criticity Level (PSCL). A Dynamic dashboard is implemented to quickly and efficiently identify malicious port scanning activities. Experimentations and evaluations showed the efficiency of the proposed system in multilevel port scanning detection compared to Snort and the related IDS systems.																	1872-4981	1875-8843					2020	14	2					171	180		10.3233/IDT-180050													
J								Fuzzy-inspired decision making for dependability recommendation in e-commerce industry	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Recommender system; social network analysis; context-specific information; item-dependability; fuzzy number set	SYSTEM; INFORMATION; LOCATION; DESIGN	Recommender System has become one of the most effective tools for provisioning user-interest based decision-making services. With its capability to generate efficient recommendations, users are directed towards items that are optimal with compliance to their needs, and preferences. Inspired from these aspects, this paper presents a novel recommendation technique based on context-specific information and social network analysis for determining dependable items. Context specific information provides a quantifiable measure of user interest for dependability whereas social network analysis determines the degree of similarity among other users. Both types of information are acquired and analyzed in the form of linguistic terms. This fuzzy-based quantification provides an effective way to evaluate social-ratings and social-similarity. For validation, it is evaluated in the on-line mobile purchase scenario. Based on the numerous simulations performed on different data sets, performance estimators in the form of Temporal Delay, Statistical Analysis and System Stability are estimated. It is concluded that the proposed mechanism of recommendation is effective and efficient in comparison to state-of-the-art recommender systems.																	1872-4981	1875-8843					2020	14	2					181	197		10.3233/IDT-190143													
J								Fuzzy controller and emotional model for evacuation of virtual crowd behaviors	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Fuzzy logic; decision making; emotional model; perception; belief; collision avoidance; panic	COLLISION-AVOIDANCE; LOGIC; GENERATION; SYSTEM; PATH	This paper presents the optimal path planning from an initial position to a final position of virtual crowd movement in an emergency evacuation. We have used hybrid architecture based on reinforced fuzzy inference system to control dynamic obstacles and A* algorithm for path finding which are widely used in artificial intelligence. The proposed contribution is to model the collision avoidance problem using a logic inference system. This fuzzy logic addresses cognitive behavior that introduces the uncertainty and imprecision during decision-making, for this the emotional model of personality (OCEAN) is integrated for BDI agents (beliefs, desires and intentions) and perception of agents in the environment. We will demonstrate the implemented approach by developing application with Java and Netlogo platform and tested with several simulation examples, that show how a crowd can be evacuated without collision and generate macroscopic emergence behavior by microscopic interaction. This application can be used as a framework to simulate real situations of complex systems (stadium evacuation, airports, hospitals).																	1872-4981	1875-8843					2020	14	2					199	214		10.3233/IDT-190092													
J								Reconceptualizing examination debar criteria using fuzzy logic	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Attendance; fuzzy logic; fuzzy expert system; academics; performance evaluation; fuzzy computing; students perfor-; mance; FIS; Mamdani Inference; centroid method; principal of maximal belongingness; machine learning	CLASS ATTENDANCE; STUDENT ATTENDANCE; PANEL-DATA; PERFORMANCE	Strict requirements regarding student attendance have always been a debated topic in academic institutions. Numerous studies carried out to relate class attendance with a student's overall performance have reported positive as well as negative results; thereby not resulting in a clear overall conclusion. Therefore, this paper presents a fuzzy logic based attendance evaluation system for higher educational institutions. The proposed fuzzy system considers four attributes: student attendance in the current course, overall performance, performance in the current course and faculty's assessment for deciding if the student should be debarred from examination, allowed taking the examination or be given reconsideration. Since the considered attributes are relevant to any course, it results in a generalized model which may be adapted according to the specific requirements of courses at different universities. The proposed model is implemented using the fuzzy logic toolkit of OCTAVE. The application of the system to actual students' data has yielded an accuracy of 95.25%. Further, for performance analysis, three classification algorithms, namely Naive Bayes, Support Vector Machine and Neural Networks are also applied on the same dataset.																	1872-4981	1875-8843					2020	14	2					215	225		10.3233/IDT-190012													
J								An intelli AFM: An intelligent association based fuzzy rule miner to predict high blood pressure using bio-psychological factors	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Association; blood pressure; fuzzy based systems; classification; apriori		High Blood Pressure (HBP) is one of the major triggering factors for many health-related issues such as brain stroke, heart stroke, kidney failure, eye damage, etc. The victims of HBP are drastically increasing day by day across the globe. The prediction of HBP in advance is more beneficial to control the Blood Pressure (BP) rather than using BP control medications. So this paper focused on an intelligent fuzzy classification model called Association based Fuzzy rule Miner (AFM) to predict HBP. Although they are numerous parameters that contribute to HBP, the impact of Bio-Psychological factors on HBP is always worth noting. This paper considered biological factors obesity level, cholesterol level, age, and Psychological factors anxiety level and anger level of a person for experimental analysis. The proposed Model initially converts the crisp data set into the fuzzified data set. Later, the association rules are extracted using apriori algorithm based on conditions imposed as constraints. In the final step the extracted association rules for each decision class separately together constructs AFM, which predicts whether a person is a victim of HBP or not. The experiments are conducted on a real-time dataset of 1000 records, where 600 records are used for training and 400 records are used for testing. The AFM has shown 90.75% accuracy, which is for better than the accuracy of existing classifiers such as Random Forest, Naive Bayes, Simple logistic regression, J48, and PART.																	1872-4981	1875-8843					2020	14	2					227	237		10.3233/IDT-190156													
J								Multiclass classification of EEG signal for epilepsy detection using DWT based SVD and fuzzy kNN classifier	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										EEG; epilepsy; DWT; SVD; Eigen value; F k NN	AUTOMATIC SEIZURE DETECTION; FEATURE-EXTRACTION; WAVELET TRANSFORM; BINARY PATTERN; ALGORITHM; DIAGNOSIS; FEATURES	Epileptic seizures happen because of neuronal disorder that produces an unusual pattern of brain signals. Automatic seizure detection has proved to be a challenging task, for both long terms monitoring as well as epilepsy diagnosis. In this work, the proposed discrete wavelet transform (DWT) based singular value decomposition fuzzy k-nearest neighbor classifier (SVD-FkNN) technique, is one of the most effective methods in supervised learning, which provides good accuracy with fast learning speed in comparison to several other conventional techniques. In this work, both feature extraction and classification of EEG signals have been done for epilepsy detection of the human brain using the Bonn University dataset. The proposed method is based on the multi-scale eigenspace analysis of the matrices, generated from the sub-band signals of all EEG channels using DWT by SVD at a substantial scale and are classified using extracted singular value features and FkNN classifier with different 'k' values to obtain better accuracy. The proposed DWT based SVD-FkNN technique has been applied for the first time on the EEG signal for epilepsy detection (using five-class classifications). The experimental results of the proposed method give an overall accuracy of 100% for two and three class classification and 93.33% (p < 0.001) for five class classification.																	1872-4981	1875-8843					2020	14	2					239	252		10.3233/IDT-190043													
J								Application of expert system with fuzzy logic and artificial intelligence (AI) for recognizing Alzheimer	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Expert system; fuzzy logic; analytical heirarchy process (AHP); Alzheimer disease (AD)	AHP	The paper shows the adaptation of Fuzzy Logic and AHP to recognize Alzheimer's disease at early stages. Alzheimer disease (AD) is a neurological disease which is mostly seen in the age of 65 years and above, when they have problems like memory loss, decline in cognitive abilities and changes in mood and personality. We all know that human behavior is mostly based on qualitative facts, which are hard to be measured and cannot be judged easily. Fuzzy logic and AI are certain latest methods to approximate and to bring out a decision or an inference. Through this method, certain linguistic terms are used like very high, high, low, very low etc. and the weights are calculated accordingly, via the response. During final interpretation, the identified parameters are diagnosed through mapping and framing a hierarchy of major subareas to arrive at a subsequent decision. A set of governing parameters are framed and then subparametrs been identified, creating a hierarchy. Thus this tool can be used for final diagnosis of the Alzheimer disease as it is Yes Or No for the concerned patient.																	1872-4981	1875-8843					2020	14	2					253	258		10.3233/IDT-180108													
J								Neuro-fuzzy based fusion of LiDAR and ultrasonic sensors to minimize error in range estimation for the navigation of mobile robots	INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS										Adaptive neuro fuzzy inference systems; data fusion; LiDAR sensor; ten-fold cross-validation; ultrasonic sensor	SYSTEM; ANFIS; ALGORITHMS; LOGIC	Multi sensor data fusion plays a significant role in addressing the research problems for mobile robot navigation. In this paper, a fusion procedure was developed, based on an Adaptive Neuro Fuzzy Inference Systems (ANFIS), which fuses the data from ultrasonic and LiDAR sensors for a better range estimation as well as environment perception. The root mean square error analysis between the measured and actual distance across different experiment trials indicates that, ultrasonic sensor could provide the data with a root mean square error (RMSE) which varies between 0.3118 cm and 4.9953 cm, whereas the LiDAR sensor could provide the same between 5.1503 cm and 10.4773 cm over 0-4 m range. The RMSE of the proposed fusion algorithm varies between 3.4831 cm and 6.1471 cm. It can be observed that, the fusion process could reduce the mean square error present with the high cost LiDAR sensing system by one-half, while fusing it with the low cost ultrasonic sensing system. The fusion algorithm discussed in this paper provides a guidance to define various operating/safety zones for initiating necessary control action during the navigation of the mobile robots.																	1872-4981	1875-8843					2020	14	2					259	267		10.3233/IDT-180109													
J								Symmetric sum based aggregation operators for spherical fuzzy information: Application in multi -attribute group decision making problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											SETS																		1064-1246	1875-8967					2020	38	4			SI		5241	5255															
J								Minimax Nonparametric Parallelism Test	JOURNAL OF MACHINE LEARNING RESEARCH										asymptotic distribution; minimax optimality; nonparametric inference; parallelism test; penalized least squares; smoothing spline ANOVA; Wald test	CHRONIC LYMPHOCYTIC-LEUKEMIA; SMOOTHING SPLINE ANOVA; ALZHEIMERS-DISEASE; DNA-METHYLATION; REGRESSION; HIPPOCAMPAL; EXPRESSION; ATROPHY; GYRUS	Testing the hypothesis of parallelism is a fundamental statistical problem arising from many applied sciences. In this paper, we develop a nonparametric parallelism test for inferring whether the trends are parallel in treatment and control groups. In particular, the proposed nonparametric parallelism test is a Wald type test based on a smoothing spline ANOVA (SSANOVA) model which can characterize the complex patterns of the data. We derive that the asymptotic null distribution of the test statistic is a Chi-square distribution, unveiling a new version of Wilks phenomenon. Notably, we establish the minimax sharp lower bound of the distinguishable rate for the nonparametric parallelism test by using the information theory, and further prove that the proposed test is minimax optimal. Simulation studies are conducted to investigate the empirical performance of the proposed test. DNA methylation and neuroimaging studies are presented to illustrate potential applications of the test. The software is available at https://github.com/BioAlgs/Parallelism.																	1532-4435						2020	21																						
J								Adaptive Variational Model for Contrast Enhancement of Low-Light Images	SIAM JOURNAL ON IMAGING SCIENCES										contrast enhancement; image enhancement; adaptive variational model; nonuniform illumination; low-light images	HISTOGRAM EQUALIZATION; MEAN BRIGHTNESS; RETINEX; ALGORITHM; FRAMEWORK; ISSUES	Contrast enhancement plays an important role in image/video processing and computer vision applications. Its main purpose is to adjust the image intensity to enhance the quality and features of the image. In this paper, we propose a simple and efficient adaptive variational model for contrast enhancement for partially shaded low-light images. The key idea of this adaptive approach is to employ the maximum image of the RGB color channels as a classifier to divide the image domain into the relatively bright and dim parts, and then use different fitting terms for each part such that the bright pixels are preserved as close as possible to the original ones while the dim pixels are boosted with brightness and contrast-level parameters to adjust the degree of the strength. With this adaptivity, one can find that the proposed model considerably improves upon the existing variational models in the literature. In this paper, the existence and uniqueness of the minimizer for the variational minimization problem is established. The split Bregman method is used to accomplish an efficient numerical implementation of the adaptive variational model. Moreover, a number of numerical experiments and comparisons with other popular enhancement methods are conducted to demonstrate the high performance of the newly proposed method.																	1936-4954						2020	13	1					1	28		10.1137/19M1245499													
J								Bayesian Inference and Uncertainty Quantification for Medical Image Reconstruction with Poisson Data	SIAM JOURNAL ON IMAGING SCIENCES										Poisson distribution; Bayesian inference; image reconstruction; uncertainty quantification; Markov chain Monte Carlo; positron emission tomography	INVERSE PROBLEMS; ALGORITHM; MODEL; MCMC	We provide a complete framework for performing infinite dimensional Bayesian inference and uncertainty quantification for image reconstruction with Poisson data. In particular, we address the following issues to make the Bayesian framework applicable in practice. We first introduce a positivity-preserving reparametrization, and we prove that under the reparametrization and a hybrid prior, the posterior distribution is well-posed in the infinite dimensional setting. Second, we provide a dimension-independent Markov chain Monte Carlo algorithm, based on the preconditioned Crank-Nicolson Langevin method, in which we use a primal-dual scheme to compute the offset direction. Third, we give a method combining the model discrepancy method and maximum likelihood estimation to determine the regularization parameter in the hybrid prior. Finally we propose to use the obtained posterior distribution to detect artifacts in a recovered image. We provide an example to demonstrate the effectiveness of the proposed method.																	1936-4954						2020	13	1					29	52		10.1137/19M1248352													
J								Computation of Circular Area and Spherical Volume Invariants via Boundary Integrals	SIAM JOURNAL ON IMAGING SCIENCES										spherical volume invariant; PCA on local neighborhoods; integral invariants; curvature; computational geometry; boundary integral methods	BONE FRAGMENTATION; IMAGE; CLASSIFICATION; 3D	We show how to compute the circular area invariant of planar curves, and the spherical volume invariant of surfaces, in terms of line and surface integrals, respectively. We use the divergence theorem to express the area and volume integrals as line and surface integrals, respectively, against particular kernels; our results also extend to higher-dimensional hypersurfaces. The resulting surface integrals are computable analytically on a triangulated mesh. This gives a simple computational algorithm for computing the spherical volume invariant for triangulated surfaces that does not involve discretizing the ambient space. We discuss potential applications to feature detection on broken bone fragments of interest in anthropology.																	1936-4954						2020	13	1					53	77		10.1137/19M1260803													
J								Automatically Controlled Morphing of 2D Shapes with Textures	SIAM JOURNAL ON IMAGING SCIENCES										metamorphosis; transfinite color interpolation; space-time bounded blending; signed distance fields	TRANSFINITE INTERPOLATION; DISTANCE; TRANSPORT	This paper deals with 2D image transformations from a perspective of a 3D heterogeneous shape modeling and computer animation. Shape and image morphing techniques have attracted a lot of attention in artistic design, computer animation, and interactive and streaming applications. We present a novel method for morphing between two topologically arbitrary 2D shapes with sophisticated textures (raster color attributes) using a metamorphosis technique called space-time blending (STB) coupled with space-time transfinite interpolation. The method allows for a smooth transition between source and target objects by generating in-between shapes and associated textures without setting any correspondences between boundary points or features. The method requires no preprocessing and can be applied in 2D animation when position and topology of source and target objects are significantly different. With the conversion of given 2D shapes to signed distance fields, we have detected a number of problems with directly applying STB to them. We propose a set of novel and mathematically substantiated techniques, providing automatic control of the morphing process with STB and an algorithm of applying those techniques in combination. We illustrate our method with applications in 2D animation and interactive applications.																	1936-4954						2020	13	1					78	107		10.1137/19M1241581													
J								Nonlinear Iterative Hard Thresholding for Inverse Scattering	SIAM JOURNAL ON IMAGING SCIENCES										iterative hard thresholding; nonlinear inverse scattering; mutual coherence; compressive imaging	RECONSTRUCTION	We consider the inverse scattering problem for sparse scatterers. An image reconstruction algorithm is proposed that is based on a nonlinear generalization of iterative hard thresholding. The convergence and error of the method was analyzed by means of coherence estimates and compared to numerical simulations.																	1936-4954						2020	13	1					108	140		10.1137/19M1251928													
J								Dictionary Learning for Two-Dimensional Kendall Shapes	SIAM JOURNAL ON IMAGING SCIENCES										Kendall's shape space; sparse dictionary learning; 2D shape analysis; interpolating curves; splines	SPARSE; REPRESENTATION; ALGORITHMS; SNAKES	We propose a novel sparse dictionary learning method for planar shapes in the sense of Kendall, namely configurations of landmarks in the plane considered up to similitudes. Our shape dictionary method provides a good trade-off between algorithmic simplicity and faithfulness with respect to the nonlinear geometric structure of Kendall's shape space. Remarkably, it boils down to a classical dictionary learning formulation modified using complex weights. Existing dictionary learning methods extended to nonlinear spaces map the manifold either to a reproducing kernel Hilbert space or to a tangent space. The first approach is unnecessarily heavy in the case of Kendall's shape space and causes the geometrical understanding of shapes to be lost, while the second one induces distortions and theoretical complexity. Our approach does not suffer from these drawbacks. Instead of embedding the shape space into a linear space, we rely on the hyperplane of centered configurations, including preshapes from which shapes are defined as rotation orbits. In this linear space, the dictionary atoms are scaled and rotated using complex weights before summation. Furthermore, our formulation is more general than Kendall's original one: it applies to discretely defined configurations of landmarks as well as continuously defined interpolating curves. We implemented our algorithm by adapting the method of optimal directions combined to a Cholesky-optimized order recursive matching pursuit. An interesting feature of our shape dictionary is that it produces visually realistic atoms, while guaranteeing reconstruction accuracy. Its efficiency can mostly be attributed to a clear formulation of the framework with complex numbers. We illustrate the strong potential of our approach for the characterization of datasets of shapes up to similitudes and the analysis of patterns in deforming two-dimensional shapes.																	1936-4954						2020	13	1					141	175		10.1137/19M126044X													
J								N-Dimensional Tensor Completion for Nuclear Magnetic Resonance Relaxometry	SIAM JOURNAL ON IMAGING SCIENCES										nuclear magnetic resonance; tensor completion; compressed sensing; multidimensional inverse problems; nonuniform sampling; restricted isometry property	MULTICOMPONENT T-2 RELAXATION; BAYESIAN-ANALYSIS; MYELIN WATER; RECOVERY; FACTORIZATION; BRAIN; DECAY; MRI	This paper deals with tensor completion for the solution of multidimensional inverse problems arising in nuclear magnetic resonance (NMR) relaxometry. We study the problem of reconstructing an approximately low-rank tensor from a small number of noisy linear measurements. New recovery guarantees, numerical algorithms, nonuniform sampling strategies, and parameter selection methods are developed in this context. In particular, we derive a fixed point continuation algorithm for tensor completion and prove its convergence. A restricted isometry property-based tensor recovery guarantee is proved. Probabilistic recovery guarantees are obtained for sub-Gaussian measurement operators and for measurements obtained by nonuniform sampling from a Parseval tight frame. The proposed algorithm is then applied to the setting of nuclear magnetic resonance relaxometry, for both simulated and experimental data. We compare our results with basis pursuit as well as with the state-of-the-art nonsubsampled data acquisition and reconstruction approach. Our experiments indicate that tensor recovery promises to significantly accelerate N-dimensional NMR relaxometry and related experiments, enabling previously impractical experiments to be performed. Our methods could also be applied to other similar inverse problems arising in machine learning, signal and image processing, and computer vision.																	1936-4954						2020	13	1					176	213		10.1137/18M1193037													
J								Multigrid Optimization for Large-Scale Ptychographic Phase Retrieval	SIAM JOURNAL ON IMAGING SCIENCES										phase retrieval; multigrid optimization; inverse problems; ptychography; coherent diffraction imaging	ALGORITHMS; RECONSTRUCTION	Ptychography is a popular imaging technique that combines diffractive imaging with scanning microscopy. The technique consists of a coherent beam that is scanned across an object in a series of overlapping positions, leading to reliable and improved reconstructions. Ptychographic microscopes allow for large fields to be imaged at high resolution at the cost of additional computational expense. In this work, we propose a multigrid-based optimization framework to reduce the computational burdens of large-scale ptychographic phase retrieval. Our proposed method exploits the inherent hierarchical structures in ptychography through tailored restriction and prolongation operators for the object and data domains. Our numerical results show that our proposed scheme accelerates the convergence of its underlying solver and outperforms the ptychographical iterative engine, a workhorse in the optics community.																	1936-4954						2020	13	1					214	233		10.1137/18M1223915													
J								Fisher Information Matrix for Single Molecules with Stochastic Trajectories	SIAM JOURNAL ON IMAGING SCIENCES										object tracking; single molecule microscopy; stochastic differential equation; maximum likelihood estimation; Fisher information matrix; Cramer-Rao lower bound	PARTICLE TRACKING; PARAMETER-ESTIMATION; LOCALIZATION	Tracking of objects in cellular environments has become a vital tool in molecular cell biology. A particularly important example is single molecule tracking, which enables the study of the motion of a molecule in cellular environments by locating the molecule over time and provides quantitative information on the behavior of individual molecules in cellular environments, which were not available before through bulk studies. Here, we consider a dynamical system where the motion of an object is modeled by stochastic differential equations (SDEs), and measurements are the detected photons, emitted by the moving fluorescently labeled object, that occur at discrete time points, corresponding to the arrival times of a Poisson process, in contrast to equidistant time points, which have been commonly used in the modeling of dynamical systems. The measurements are distributed according to the optical diffraction theory, and therefore, they would be modeled by different distributions, e.g., an Airy profile for an in-focus and a Born and Wolf profile for an out-of-focus molecule with respect to the detector. For some special circumstances, Gaussian image models have been proposed. In this paper, we introduce a stochastic framework in which we calculate the maximum likelihood estimates of the biophysical parameters of the molecular interactions, e.g., diffusion and drift coefficients. More importantly, we develop a general framework to calculate the Cramer-Rao lower bound (CRLB), given by the inverse of the Fisher information matrix, for the estimation of unknown parameters and use it as a benchmark in the evaluation of the standard deviation of the estimates. There exists no established method, even for Gaussian measurements, to systematically calculate the CRLB for the general motion model that we consider in this paper. We apply the developed methodology to simulated data of a molecule with linear trajectories and show that the standard deviation of the estimates matches well with the square root of the CRLB. We also show that equally sampled and Poisson distributed time points lead to significantly different Fisher information matrices.																	1936-4954						2020	13	1					234	264		10.1137/19M1242562													
J								Multimodal 3D Shape Reconstruction under Calibration Uncertainty Using Parametric Level Set Methods	SIAM JOURNAL ON IMAGING SCIENCES										3D shape reconstruction; parametric level sets; dip transform; joint reconstruction; shape from silhouettes; point clouds; compactly supported radial basis functions	WAVE-FORM INVERSION; SURFACE RECONSTRUCTION; VOLUME RECONSTRUCTION; OPTIMIZATION; REGISTRATION; ALIGNMENT; POINT; JULIA	We consider the problem of 3D shape reconstruction from multimodal data, given uncertain calibration parameters. Typically, 3D data modalities can come in diverse forms such as sparse point sets, volumetric slices, and 2D photos. To jointly process these data modalities, we exploit a parametric level set method that utilizes ellipsoidal radial basis functions. This method not only allows us to analytically and compactly represent the object; it also confers on us the ability to overcome calibration-related noise that originates from inaccurate acquisition parameters. This essentially implicit regularization leads to a highly robust and scalable reconstruction, surpassing other traditional methods. In our results we first demonstrate the ability of the method to compactly represent complex objects. We then show that our reconstruction method is robust both to a small number of measurements and to noise in the acquisition parameters. Finally, we demonstrate our reconstruction abilities from diverse modalities such as volume slices obtained from liquid displacement (similar to CT scans and X-rays) and visual measurements obtained from shape silhouettes as well as point clouds.																	1936-4954						2020	13	1					265	290		10.1137/19M1257895													
J								High-Resolution Interferometric Synthetic Aperture Imaging in Scattering Media	SIAM JOURNAL ON IMAGING SCIENCES										synthetic aperture; random media; imaging; interferometric	COMPLEX-VALUED OBJECT; PHASE RETRIEVAL; RECONSTRUCTION; MODULUS	The goal of synthetic aperture imaging is to estimate the reflectivity of a remote region of interest by processing data gathered with a moving sensor which emits periodically a signal and records the backscattered wave. We introduce and analyze a high-resolution interferometric method for synthetic aperture imaging through an unknown scattering medium which distorts the wave. The method builds on the coherent interferometric approach which uses empirical cross-correlations of the measurements to mitigate the distortion, at the expense of a loss of resolution of the image. The new method shows that, while mitigating the wave distortion, it is possible to obtain a robust and sharp estimate of the modulus of the Fourier transform of the reflectivity function. A high-resolution image can then be obtained by a phase retrieval algorithm.																	1936-4954						2020	13	1					291	316		10.1137/19M1272470													
J								Trading Beams for Bandwidth: Imaging with Randomized Beamforming	SIAM JOURNAL ON IMAGING SCIENCES										array imaging; broadband imaging; subspace model; randomized linear algebra	SPHEROIDAL WAVE-FUNCTIONS; FOURIER-ANALYSIS; PHASED SUBARMYS; UNCERTAINTY; ALGORITHMS	We study the problem of actively imaging a range-limited far-field scene using an antenna array. We describe how the range limit imposes structure in the measurements across multiple wavelengths. This structure allows us to introduce a novel trade-off: the number of spatial array measurements (i.e., beams that have to be formed) can be reduced to be significantly lower than the number array elements if the scene is illuminated with a broadband source. To take advantage of this trade-off, we use a small number of "generic" linear combinations of the array outputs, instead of the phase offsets used in conventional beamforming. We provide theoretical justification for the proposed trade-off without making any strong structural assumptions on the target scene (such as sparsity) except that it is range-limited. In proving our theoretical results, we take inspiration from the sketching literature. We also provide simulation results to establish the merit of the proposed signal acquisition strategy. Our proposed method results in a reduction in the number of required spatial measurements in an array imaging system and hence can directly impact their speed and cost of operation.																	1936-4954						2020	13	1					317	350		10.1137/19M1242045													
J								A Variational Model Dedicated to Joint Segmentation, Registration, and Atlas Generation for Shape Analysis	SIAM JOURNAL ON IMAGING SCIENCES										segmentation; registration; nonlinear elasticity; Ogden materials; Potts model; atlas generation; asymptotic results; D-m -splines; geometric PCA	WEIGHTED TOTAL VARIATION; IMAGE SEGMENTATION; SEGMENTATION/REGISTRATION MODEL; LARGE-DEFORMATION; MOTION; TRANSFORMATION; RECONSTRUCTION; INTENSITY; SNAKES; ENERGY	In medical image analysis, constructing an atlas, i.e., a mean representative of an ensemble of images, is a critical task for practitioners to estimate variability of shapes inside a population, and to characterize and understand how structural shape changes have an impact on health. This involves identifying significant shape constituents of a set of images, a process called segmentation, and mapping this group of images to an unknown mean image, a task called registration, making a statistical analysis of the image population possible. To achieve this goal, we propose treating these operations jointly to leverage their positive mutual influence, in a hyperelasticity setting, by viewing the shapes to be matched as Ogden materials. The approach is complemented by novel hard constraints on the L-infinity norm of both the Jacobian and its inverse, ensuring that the deformation is a bi-Lipschitz homeomorphism. Segmentation is based on the Potts model, which allows for a partition into more than two regions, i.e., more than one shape. The connection to the registration problem is ensured by the dissimilarity measure that aims to align the segmented shapes. A representation of the deformation field in a linear space equipped with a scalar product is then computed in order to perform a geometry-driven Principal Component Analysis (PCA) and to extract the main modes of variations inside the image population. Theoretical results emphasizing the mathematical soundness of the model are provided, among which are existence of minimizers, analysis of a numerical method, asymptotic results, and a PCA analysis, as well as numerical simulations demonstrating the ability of the model to produce an atlas exhibiting sharp edges, high contrast, and a consistent shape.																	1936-4954						2020	13	1					351	380		10.1137/19M1271907													
J								Novel Proximal Gradient Methods for Nonnegative Matrix Factorization with Sparsity Constraints	SIAM JOURNAL ON IMAGING SCIENCES										nonnegative matrix factorization; sparsity constraints; nonconvex nonsmooth composite minimization; proximal gradient algorithms; non-Euclidean Bregman distance; Kurdyka-Losiajewicz property; essentially cyclic block proximal gradient; global convergence	ALTERNATING LINEARIZED MINIMIZATION; HIERARCHICAL ALS ALGORITHMS; 1ST-ORDER METHODS; LEAST-SQUARES; CONVERGENCE; NONCONVEX; CONTINUITY	We consider the nonnegative matrix factorization (NMF) problem with sparsity constraints formulated as a nonconvex composite minimization problem. We introduce four novel proximal gradient based algorithms proven globally convergent to a critical point and which are applicable to sparsity constrained NMF models. Our approach builds on recent results allowing one to lift the classical global Lipschitz continuity requirement through the use of a non-Euclidean Bregman based distance. Since under the proposed framework we are not restricted by the gradient Lipschitz continuity assumption, we can consider new decomposition settings of the NMF problem. Two of the derived schemes are genuine non-Euclidean proximal methods that tackle nonstandard decompositions of the NMF problem. The two other schemes are novel extensions of the well-known state-of-the-art methods (the multiplicative and hierarchical alternating least squares), thus allowing one to significantly broaden the scope of these algorithms. Numerical experiments illustrate the performance of the proposed methods.																	1936-4954						2020	13	1					381	421		10.1137/19M1271750													
J								Extended-Sampling-Bayesian Method for Limited Aperture Inverse Scattering Problems	SIAM JOURNAL ON IMAGING SCIENCES										Bayesian inversion; inverse scattering; limited aperture; extended sampling method	UNIQUE CONTINUATION; ACOUSTIC SCATTERING; LINE	Limited aperture inverse scattering problems arise in many important applications. In this paper, we propose a new method combining the extended sampling method (ESM) and the Bayesian approach for the inverse acoustic scattering problem to reconstruct the shape of a sound-soft obstacle using the limited aperture data. The problem is formulated as a statistical model using the Bayes formula. The well-posedness is proved in the sense of the Hellinger metric. A modified ESM is proposed to obtain the obstacle location, which is critical to the convergence of the MCMC algorithm. An extensive numerical study is presented to illustrate the performance of the method.																	1936-4954						2020	13	1					422	444		10.1137/19M1270501													
J								Simplifying Transforms for General Elastic Metrics on the Space of Plane Curves	SIAM JOURNAL ON IMAGING SCIENCES										elastic shape analysis; statistical shape analysis; infinite-dimensional geometry; Sobolev metrics; curve matching	SHAPE SPACE; HYPERSURFACES; REGISTRATION; TRANSPORT	In the shape analysis approach to computer vision problems, one treats shapes as points in an infinite-dimensional Riemannian manifold, thereby facilitating algorithms for statistical calculations such as geodesic distance between shapes and averaging of a collection of shapes. The performance of these algorithms depends heavily on the choice of the Riemannian metric. In the setting of plane curve shapes, attention has largely been focused on a two-parameter family of first order Sobolev metrics, referred to as elastic metrics. They are particularly useful due to the existence of simplifying coordinate transformations for particular parameter values, such as the well-known square-root velocity transform. In this paper, we extend the transformations appearing in the existing literature to a family of isometries, which take any elastic metric to the flat L-2 metric. We also extend the transforms to treat piecewise linear curves and demonstrate the existence of optimal matchings over the diffeomorphism group in this setting. We conclude the paper with multiple examples of shape geodesics for open and closed curves. We also show the benefits of our approach in a simple classification experiment.																	1936-4954						2020	13	1					445	473		10.1137/19M1265132													
J								Higher-Order Total Directional Variation: Analysis	SIAM JOURNAL ON IMAGING SCIENCES										anisotropic total variation; higher-order total variation; variational model		We analyze a new notion of total anisotropic higher-order variation which, differently from total generalized variation in [K. Bredies, K. Kunisch, and T. Pock, SIAM J. Imaging Sci., 3 (2010), pp. 492-526], quantifies for possibly nonsymmetric tensor fields their variations at arbitrary order weighted by possibly inhomogeneous, smooth elliptic anisotropies. We prove some properties of this total variation and of the associated spaces of tensors with finite variations. We show the existence of solutions to a related regularity-fidelity optimization problem. We also prove a decomposition formula which appears to be helpful for the design of numerical schemes, as shown in a companion paper, where several applications to image processing are studied.																	1936-4954						2020	13	1					474	496		10.1137/19M1239210													
J								A High-Order Scheme for Image Segmentation via a Modified Level-Set Method	SIAM JOURNAL ON IMAGING SCIENCES										image segmentation; level-set method; Hamilton-Jacobi equations; filtered scheme; smoothness indicators	ESSENTIALLY NONOSCILLATORY SCHEMES; FILTERED SCHEMES; ACTIVE CONTOURS; CURVATURE; ENO; MODEL	In this paper, we propose a high-order accurate scheme for image segmentation based on the level-set method. In this approach, the curve evolution is described as the 0-level set of a representation function, but we modify the velocity that drives the curve to the boundary of the object in order to obtain a new velocity with additional properties that are extremely useful to develop a more stable high-order approximation with a small additional cost. The approximation scheme proposed here is the first 2D version of an adaptive "filtered" scheme recently introduced and analyzed by the authors in one dimension. This approach is interesting since the implementation of the filtered scheme is rather efficient and easy. The scheme combines two building blocks (a monotone scheme and a high-order scheme) via a filter function and smoothness indicators that allow one to detect the regularity of the approximate solution adapting the scheme in an automatic way. Some numerical tests on synthetic and real images confirm the accuracy of the proposed method and the advantages given by the new velocity.																	1936-4954						2020	13	1					497	534		10.1137/18M1231432													
J								Numerical Reconstruction of Radiative Sources in an Absorbing and Nondiffusing Scattering Medium in Two Dimensions	SIAM JOURNAL ON IMAGING SCIENCES										transport equation; inverse problems; numerical source reconstruction; attenuated x-ray transform; attenuated Radon transform; scattering; A-analytic maps; Hilbert transform; Bukhgeim-Beltrami equation; optical molecular imaging	INVERSE SOURCE PROBLEM; ATTENUATED RADON-TRANSFORM; EFFICIENT TENSOR TOMOGRAPHY; OPTICAL TOMOGRAPHY; FORMULA; RANGE; FULL	We consider the two dimensional quantitative imaging problem of recovering a radiative source inside an absorbing and scattering medium from knowledge of the outgoing radiation measured at the boundary. The medium has an anisotropic scattering property that is neither negligible nor large enough for the diffusion approximation to hold. We present the numerical realization of the authors' recently proposed reconstruction method. For scattering kernels of finite Fourier content in the angular variable, the solution is exact. The feasibility of the proposed algorithms is demonstrated in several numerical experiments, including simulated scenarios for parameters meaningful in optical molecular imaging.																	1936-4954						2020	13	1					535	555		10.1137/19M1282921													
B								Data as the New Driving Gears of Urbanization	CITIES AND THE DIGITAL REVOLUTION: ALIGNING TECHNOLOGY AND HUMANITY										Urban planning; Data; Artificial intelligence; Urbanization; Technology; Smart Cities	BIG DATA; ECONOMIC-GROWTH; SMART CITIES; CHALLENGES; TECHNOLOGIES; FUTURE; CITY; INFRASTRUCTURE; GLOBALIZATION; INFORMATION	While there have been a slow rural-urban transition which highlighted the role that cities are the centre for sustaining economies of regions, and even countries, it was the advent of the Internet that has drastically changed the way they are planned, operate and seen. A resulting rise in data, fuelled by a heavy technological revolution, showed that there are new ways of increasing urban efficiency and productivity. This has even reflected in reforms at governance levels and has proved how the digital layer can provide stronger networks. However, while this reinforces economies, it brings substantial changes in urban lifestyle that has for centuries and decades remained unchanged. This disruption in lifestyle is happening at faster speed and impacting not only on the social strata, but also reflecting in its physical form, the urban morphology. While the primary notion of increasing efficiency of cities is understood, the question remains of how to allow change while still catering for the liveability of cities.																			978-3-030-29800-5; 978-3-030-29799-2				2020							1	29		10.1007/978-3-030-29800-5_1	10.1007/978-3-030-29800-5												
B								Urban Chaos and the AI Messiah	CITIES AND THE DIGITAL REVOLUTION: ALIGNING TECHNOLOGY AND HUMANITY										Cities; Climate change; Overpopulation; Big Data; Robots; Internet of Things (IoT)	ARTIFICIAL-INTELLIGENCE; CLIMATE-CHANGE; SECURITY; CITIES; SUSTAINABILITY; URBANIZATION; CHALLENGES; DRIVERS; FOOD	While there are predictions that the future will be highly urbanized, there are others stating that the urban world will be increasingly faced with the impacts of climate change, and cities are being pressured from various angles. Faced with this, the role of technology is being hailed and the possibilities that Artificial Intelligence (AI) brings are getting more pronounced as the technology gets more accurate and efficient. Indeed, its applicability in various fields is making a way and the results are promising. However, while AI stands as a potential saviour and as its role is being accentuated in urban planning, governance and management, there are increasing concerns that its practical implications are successful and its planning principles are disconnected with sensibilities linked to the dimensions of liveability.																			978-3-030-29800-5; 978-3-030-29799-2				2020							31	60		10.1007/978-3-030-29800-5_2	10.1007/978-3-030-29800-5												
B								Digital Urban Networks and Social Media	CITIES AND THE DIGITAL REVOLUTION: ALIGNING TECHNOLOGY AND HUMANITY										Cities; Internet of things (IoT); Social media; Branding; Placemaking; Technology; Information Communication Technology (ICT)	ECONOMIC-GROWTH; SMART CITY	The new gold rush in today's day and age is that of the urban mining of data for commercial usage. In the aim of monetizing on this, ICT corporations are actively, and aggressively, offering services, often at the expense of the general population, which are then disguised to increase public acceptance. Along with the Smart City, the safe city concept is an example of this and can be argued to stand as a data mining strategy for the enrichment of ICT Corporations. However, those dimensions can be recalibrated, in particular the former, so that they include dimensions of liveability and contribute to building safer, more inclusive, sustainable cities as prescribed by the Sustainable Development Goal 11 by the United Nations and through the New Urban Agenda.																			978-3-030-29800-5; 978-3-030-29799-2				2020							61	83		10.1007/978-3-030-29800-5_3	10.1007/978-3-030-29800-5												
B								Privatization and Privacy in the Digital City	CITIES AND THE DIGITAL REVOLUTION: ALIGNING TECHNOLOGY AND HUMANITY										Smart cities; Privacy; Privatization; Big Data; Public-Private Partnerships (PPP); Intellectual property	SMART CITIES; INNOVATION; CITIZENS; SECURITY	The concept of Smart Cities is accentuated as ICT corporations engage in market monopolies under the umbrella of proprietary technology and thus further negates the possibilities of technology transfer and knowledge transfer. While technologically inclined urban solutions are seen as being tailored in accordance to the city's need and financial capabilities, the main objective remain the selling of a product while ensuring large profit margins. This has been often contested as this gives rise to a number of issues relating to privacy and intellectual property catalysed through Public-Private Partnerships. This paper discusses this dichotomy and outlines that there are emerging areas that need consideration for the thematic of public data when coupling technology providers with cities.																			978-3-030-29800-5; 978-3-030-29799-2				2020							85	106		10.1007/978-3-030-29800-5_4	10.1007/978-3-030-29800-5												
B								On Culture, Technology and Global Cities	CITIES AND THE DIGITAL REVOLUTION: ALIGNING TECHNOLOGY AND HUMANITY										Diversity; Global cities; Migration; Homogeneity; Autonomous cities; Culture	SMART CITY	The role of technology becomes more pronounced and advances in various fields emerge and are made to impact on the lifestyle of people. One such notable impact has been on that of the transportation and tourism sector which are seen to witness an incremental rise due to the substantial rise in middle-income groups and through newly renovated and increasing access networks, hence moulding our interaction with cities. This gives new ways of mannerisms and interesting patterns, as seen through the youth and their intimate relationship with social media when travelling. However, as the implementation of technologically inclined solutions gain ground, we are made away of its risks that can reverberate on the urban form as well as on governance decisions and the innate identity of place.																			978-3-030-29800-5; 978-3-030-29799-2				2020							107	124		10.1007/978-3-030-29800-5_5	10.1007/978-3-030-29800-5												
J								Probabilistic Symmetries and Invariant Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH										probabilistic symmetry; convolutional neural networks; exchangeability; neural architectures; invariance; equivariance; sufficiency; adequacy; graph neural networks	THEOREM; MODELS; DISTRIBUTIONS; GENERALIZE; GRAPHS	Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.																	1532-4435						2020	21																						
J								Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients	JOURNAL OF MACHINE LEARNING RESEARCH											INEQUALITIES; ALGORITHMS; SAMPLER; BOUNDS	Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the Stormer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of step-size and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds of Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution.																	1532-4435						2020	21																						
J								Effective Ways to Build and Evaluate Individual Survival Distributions	JOURNAL OF MACHINE LEARNING RESEARCH										Survival analysis; risk model; patient-specific survival prediction; calibration; discrimination	PROGNOSTIC INDEX; BREAST-CANCER; PREDICTION; REGRESSION; MODELS; RISK; PERFORMANCE; LIFE; ACCURACY; SCALE	An accurate model of a patient's individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (for example from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (for instance the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients, meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model that provides an individual survival distribution for each subject, which gives survival probabilities across all times, such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such "individual survival distribution" (ISD) models, and explains how they differ from standard models. It then discusses ways to evaluate such models namely Concordance, 1-Calibration, Inte- grated Brier score, and versions of L1-loss then motivates and defines a novel approach, "D-Calibration", which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several ISD prediction tools over a range of survival data sets. We also provide a code base for all of these survival models and evaluation measures, at https : //github. com/haiderstats/ISDEvaluation.																	1532-4435						2020	21																						
J								Constrained Dynamic Programming and Supervised Penalty Learning Algorithms for Peak Detection in Genomic Data	JOURNAL OF MACHINE LEARNING RESEARCH										Non-convex; constrained; optimization; changepoint; segmentation	CHANGE-POINTS	Peak detection in genomic data involves segmenting counts of DNA sequence reads aligned to different locations of a chromosome. The goal is to detect peaks with higher counts, and filter out background noise with lower counts. Most existing algorithms for this problem are unsupervised heuristics tailored to patterns in specific data types. We propose a supervised framework for this problem, using optimal changepoint detection models with learned penalty functions. We propose the first dynamic programming algorithm that is guaranteed to compute the optimal solution to changepoint detection problems with constraints between adjacent segment mean parameters. Implementing this algorithm requires the choice of penalty parameter that determines the number of segments that are estimated. We show how the supervised learning ideas of Rigaill et al. (2013) can be used to choose this penalty. We compare the resulting implementation of our algorithm to several baselines in a benchmark of labeled ChIP-seq data sets with two different patterns (broad H3K36me3 data and sharp H3K4me3 data). Whereas baseline unsupervised methods only provide accurate peak detection for a single pattern, our supervised method achieves state-of-the-art accuracy in all data sets. The log-linear timings of our proposed dynamic programming algorithm make it scalable to the large genomic data sets that are now common. Our implementation is available in the PeakSegOptimal R package on CRAN.																	1532-4435						2020	21																						
J								Causal Discovery from Heterogeneous/Nonstationary Data	JOURNAL OF MACHINE LEARNING RESEARCH										causal discovery; heterogeneous/nonstationary data; independent-change principle; kernel distribution embedding; driving force estimation; confounder	COMPONENT ANALYSIS; INFERENCE	It is commonplace to encounter heterogeneous or nonstationary data, of which the underlying generating process changes across domains or over time. Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper, we develop a framework for causal discovery from such data, called Constraint-based causal Discovery from heterogeneous/NOnstationary Data (CD-NOD), to find causal skeleton and directions and estimate the properties of mechanism changes. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a method to determine causal orientations by making use of independent changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. After learning the causal structure, next, we investigate how to efficiently estimate the "driving force" of the nonstationarity of a causal mechanism. That is, we aim to extract from data a low-dimensional representation of changes. The proposed methods are nonparametric, with no hard restrictions on data distributions and causal mechanisms, and do not rely on window segmentation. Furthermore, we find that data heterogeneity benefits causal structure identification even with particular types of confounders. Finally, we show the connection between heterogeneity/nonstationarity and soft intervention in causal discovery. Experimental results on various synthetic and real-world data sets (task-fMRI and stock market data) are presented to demonstrate the efficacy of the proposed methods.																	1532-4435						2020	21																						
J								Loss Control with Rank-one Covariance Estimate for Short-term Portfolio Optimization	JOURNAL OF MACHINE LEARNING RESEARCH										rank-one covariance estimate; short-term portfolio optimization; undersampled condition; loss control; downside risk	REVERSION STRATEGY; MATRIX; MARKOWITZ; SELECTION	In short-term portfolio optimization (SPO), some financial characteristics like the expected return and the true covariance might be dynamic. Then there are only a small window size w of observations that are sufficiently close to the current moment and reliable to make estimations. w is usually much smaller than the number of assets d, which leads to a typical undersampled problem. Worse still, the asset price relatives are not likely subject to any proper distributions. These facts violate the statistical assumptions of the traditional covariance estimates and invalidate their statistical efficiency and consistency in risk measurement. In this paper, we propose to reconsider the function of covariance estimates in the perspective of operators, and establish a rank-one covariance estimate in the principal rank-one tangent space at the observation matrix. Moreover, we propose a loss control scheme with this estimate, which effectively catches the instantaneous risk structure and avoids extreme losses. We conduct extensive experiments on 7 real-world benchmark daily or monthly data sets with stocks, funds and portfolios from diverse regional markets to show that the proposed method achieves state-of-the-art performance in comprehensive downside risk metrics and gains good investing incomes as well. It offers a novel perspective of rank-related approaches for undersampled estimations in SPO.																	1532-4435						2020	21																						
J								Distributed Kernel Ridge Regression with Communications	JOURNAL OF MACHINE LEARNING RESEARCH										learning theory; distributed learning; kernel ridge regression; communication	GRADIENT; RATES	This paper focuses on generalization performance analysis for distributed algorithms in the framework of learning theory. Taking distributed kernel ridge regression (DKRR) for example, we succeed in deriving its optimal learning rates in expectation and providing theoretically optimal ranges of the number of local processors. Due to the gap between theory and experiments, we also deduce optimal learning rates for DKRR in probability to essentially reflect the generalization performance and limitations of DKRR. Furthermore, we propose a communication strategy to improve the learning performance of DKRR and demonstrate the power of communications in DKRR via both theoretical assessments and numerical experiments.																	1532-4435						2020	21																						
J								Convergence Rate of Optimal Quantization and Application to the Clustering Performance of the Empirical Measure	JOURNAL OF MACHINE LEARNING RESEARCH										clustering performance; convergence rate of optimal quantization; distortion function; empirical measure; optimal quantization	LOCALLY OPTIMAL QUANTIZER; UNIQUENESS; ALGORITHM; THEOREM	We study the convergence rate of the optimal quantization for a probability measure sequence (mu(n))(n is an element of N)* on R-d converging in the Wasserstein distance in two aspects: the first one is the convergence rate of optimal quantizer x((n)) is an element of (R-d)(K) of mu(n) at level K; the other one is the convergence rate of the distortion function valued at x((n)), called the "performance" of x((n)). Moreover, we also study the mean performance of the optimal quantization for the empirical measure of a distribution mu with finite second moment but possibly unbounded support. As an application, we show an upper bound with a convergence rate O(log n/root n) of the mean performance for the empirical measure of the multidimensional normal distribution N (m, Sigma) and of distributions with hyper-exponential tails. This extends the results from Biau et al. (2008) obtained for compactly supported distribution. We also derive an upper bound which is sharper in the quantization level K but suboptimal in n by applying results in Fournier and Guillin (2015).																	1532-4435						2020	21								21														
J								pyDML: A Python Library for Distance Metric Learning	JOURNAL OF MACHINE LEARNING RESEARCH										Distance Metric Learning; Classification; Mahalanobis Distance; Dimensionality; Python		pyDML is an open-source python library that provides a wide range of distance metric learning algorithms. Distance metric learning can be useful to improve similarity learning algorithms, such as the nearest neighbors classifier, and also has other applications, like dimensionality reduction. The pyDML package currently provides more than 20 algorithms, which can be categorized, according to their purpose, in: dimensionality reduction algorithms, algorithms to improve nearest neighbors or nearest centroids classifiers, information theory based algorithms or kernel based algorithms, among others. In addition, the library also provides some utilities for the visualization of classifier regions, parameter tuning and a stats website with the performance of the implemented algorithms. The package relies on the scipy ecosystem, it is fully compatible with scikit-learn, and is distributed under GPLv3 license. Source code and documentation can be found at https://github.com/j1suarezdiaz/pyDML.																	1532-4435						2020	21																						
J								Target-Aware Bayesian Inference: How to Beat Optimal Conventional Estimators	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian inference; Monte Carlo methods; importance sampling; adaptive sampling; amortized inference	MONTE-CARLO; NORMALIZING CONSTANTS; TUMOR-GROWTH; FREE-ENERGY; MIXTURE; RATIOS	Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions a computational pipeline that is inefficient when the target function(s) are known upfront. We address this inefficiency by introducing a framework for target-aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard O(1/N) Monte Carlo rate, potentially producing rates as fast as O(1/N-2). We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target aware inference scheme and demonstrate the substantial benefits this can yield.																	1532-4435						2020	21																						
J								Cornac: A Comparative Framework for Multimodal Recommender Systems	JOURNAL OF MACHINE LEARNING RESEARCH										comparison; multimodality; recommendation algorithms; software		Cornac is an open-source Python framework for multimodal recommender systems. In addition to core utilities for accessing, building, evaluating, and comparing recommender models, Cornac is distinctive in putting emphasis on recommendation models that leverage auxiliary information in the form of a social network, item textual descriptions, product images, etc. Such multimodal auxiliary data supplement user-item interactions (e.g., ratings, clicks), which tend to be sparse in practice. To facilitate broad adoption and community contribution, Cornac is publicly available at https://github.com/PreferredAI/cornac, and it can be installed via Anaconda or the Python Package Index (pip). Not only is it well-covered by unit tests to ensure code quality, but it is also accompanied with a detailed documentation(1), tutorials, examples, and several built-in benchmarking data sets.																	1532-4435						2020	21																						
J								Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching	JOURNAL OF MACHINE LEARNING RESEARCH										generalized score matching; high-dimensional inference; probabilistic graphical models; simultaneous inference	INVERSE COVARIANCE ESTIMATION; CONFIDENCE-INTERVALS; LINEAR-REGRESSION; MATRIX ESTIMATION; SELECTION; NETWORKS; REGIONS; PARAMETERS; TESTS; LASSO	Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is root n-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.																	1532-4435						2020	21																						
J								Generalized belief function in complex evidence theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complex evidence theory; generalized dempster-Shafer evidence theory; generalized belief function; generalized plausibility function; complex basic belief assignment; complex mass function; uncertainty modelling; fuzzy measure; decision theory	DIVERGENCE MEASURE; SELECTION; FUSION	The complex-value-based generalized Dempster-Shafer evidence theory, also called complex evidence theory is a useful methodology to handle uncertainty problems of decision-making on the framework of complex plane. In this paper, we propose a new concept of belief function in complex evidence theory. Furthermore, we analyze the axioms of the proposed belief function, then define a plausibility function in complex evidence theory. The newly defined belief and plausibility functions are the generalizations of the traditional ones in Dempster-Shafer (DS) evidence theory, respectively. In particular, when the complex basic belief assignments are degenerated from complex numbers to classical basic belief assignments (BBAs), the generalized belief and plausibility functions in complex evidence theory degenerate into the traditional belief and plausibility functions in DS evidence theory, respectively. Some special types of the generalized belief function are further discussed as well as their characteristics. In addition, an interval constructed by the generalized belief and plausibility functions can be utilized for fuzzy measure, which provides a promising way to express and model the uncertainty in decision theory.																	1064-1246	1875-8967					2020	38	4			SI		3665	3673		10.3233/JIFS-179589													
J								A new method to classify malicious domain name using Neutrosophic sets in DGA botnet detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										DGA domain detection; Neutrosophic clustering; classifying		In Botnet Detection, Domain generation algorithms are the most effective method to intercept and analyze captured package. In this article, we propose a new method to classify harmful domain names using Neutrosophic Sets. Data of domain name, after being selected featured and fuzzed into Neutrosophic Sets will be used to classify benign domain names, malicious domain names and indeterminacy domain names, minimizing false detection of benign domain names. The proposed model is going to be tested and evaluated with other malicious domain detection models in the aspects of accuracy points, Accuracy, Revocation, and F1, all of which show that our proposed model has good results.																	1064-1246	1875-8967					2020	38	4			SI		4223	4236		10.3233/JIFS-190681													
J								Existence of positive solutions of nonlocal p (x)-Kirchhoff hyperbolic systems via sub-super solutions concept	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Positive solutions; sub-super solutions method; p (x)-Kirchhoff systems	BOUNDARY-VALUE-PROBLEMS; KIRCHHOFF-TYPE; ELLIPTIC EQUATION; SUPERSOLUTION METHOD; ASYMPTOTIC-BEHAVIOR; GLOBAL SOLVABILITY; WEAK SOLUTIONS; (P X; EXPONENT	The paper deals with the existence of positive solutions of nonlocal p (x)-Kirchhoff hyperbolic systems with zero Dirichlet boundary conditions in bounded domain Omega subset of R-N by using sub-super solutions method combined with a comparison principle. Moreover, the numerical example is presented to illustrate the stationary case.																	1064-1246	1875-8967					2020	38	4			SI		4301	4313		10.3233/JIFS-190884													
J								Another view on generalized interval valued intuitionistic fuzzy soft set and its applications in decision support system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued intuitionistic fuzzy set; interval-valued intuitionistic fuzzy soft set; generalized interval-valued intuitionistic fuzzy soft set; multi-attribute decision making	DISCERNIBILITY MATRIX; ALGORITHMS	As a combination of an interval-valued intuitionistic fuzzy soft set (IVIFSS) and interval-valued intuitionistic fuzzy set (IVIFS), the existing notion of the generalized interval-valued intuitionistic fuzzy soft set (GIVIFSS) is clarified and reformulated. We define two types of containment in GIVIFSS. With this new perspective, the g-union, g-intersection, g*-union, g*-intersection, OR, AND, g-necessity and g-possibility operations are defined for GIVIFSSs. The properties and relations between operations of GIVIFSSs are investigated. An algorithm is proposed for solving MADM problems using GIVIFSS. A descriptive example is presented to see the applicability of the proposed method. Results indicate that the proposed technique is more effective and generalize over the previous models of interval-valued fuzzy sets.																	1064-1246	1875-8967					2020	38	4			SI		4327	4341		10.3233/JIFS-190944													
J								A study of m-polar neutrosophic graph with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multipolar information; neutrosophic set; m-polar neutrosophic graph; union; direct product	SPANNING TREE	Information in many real life problems collect from multi agents, i.e., "multipolar information" exists. This multipolar information cannot be properly modeled by m-polar fuzzy graph or intutionistic fuzzy graph. An m-polar neutrosophic model is very much efficient for such real word problems which can construct more precise, flexible, and comparable system as compared to the classical, fuzzy and neutrosophic graph models. In this paper, we present the definition of m-polar neutrosophic graph model. Some new operations, such as union, join, composition and ring sum of two m-polar neutrosophic graph are defined here. We define six new products on m-polar neutrosophic graphs namely strong product, semi strong product, complete product, direct product, cartesian product and lexicographic product. Some idea of complement, isomorphism, weak and co weak isomorphism on m-polar neutrosophic graph are introduced here. We also present several associated properties and theorems of m-polar neutrosophic graph. We introduce a model of m-polar neutrosophic graph, which is applied in evaluating the teacher's performance of a college. The performances of the teachers are computed based on the response score (feedback) of the students of the college. We also present a numerical example to illustrate our proposed model.																	1064-1246	1875-8967					2020	38	4			SI		4809	4828		10.3233/JIFS-191520													
J								Cloud model-based PROMETHEE method under 2D uncertain linguistic environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										2D uncertain linguistic variable; cloud; possibility degree; possibility degree index; improved PROMETHEE	GROUP DECISION-MAKING; PERSONALIZED INDIVIDUAL SEMANTICS; OPTIMAL SITE SELECTION; AGGREGATION OPERATORS; POWER-PLANT; INFORMATION; CONSENSUS; MECHANISM; TRUST	This paper proposes a cloud model-based Preference Ranking Organisation Method for Enrichment Evaluation (PROMETHEE) method with 2D uncertain linguistic variables (2DULVs). 2DULVs are adopted by decision makers (DMs) to evaluate each alternative under the criteria because they can provide extra evaluation information. Cloud model is adopted to depict randomness and fuzziness. The possibility degree and possibility degree index are defined to develop an improved PROMETHEE II method for sorting alternatives. Entropy weight method is used to calculate the weight of each criterion. A renewable energy performance sample is used to illustrate the applicability of the proposed method. Sensitivity analysis and four comparative experiments demonstrate the stability and accuracy of the proposed approach.																	1064-1246	1875-8967					2020	38	4			SI		4869	4887		10.3233/JIFS-191546													
J								A novel stochastic deep conviction network for emotion recognition in speech signal	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Stochastic deep conviction network; restricted Boltzmann machine; particle swarm optimization; support vector machine; whale optimization	RECURSIVE DIGITAL INTEGRATORS; NEURAL-NETWORK; CLASSIFICATION; DESIGN	Deep learning is far and wide considered to be the most powerful method in computer vision fields, which has a lot of applications such as image recognition, robot navigation systems, and self-driving cars. Recent developments in neural networks have led to an efficient end-to-end architecture to human activity representation and classification. In light of these recent events in deep learning, there is now much considerable concern about developing less expensive computation and memory-wise methods. This paper presents an optimized end-to-end approach named stochastic deep conviction network (SDCN) formulated using the deep learning method. It comprises of deep learning method namely deep belief network (DBN), two supervised machine learning algorithm support vector machine (SVM) and decision tree (DT) with optimization capability for speech emotion identification. In the beginning, pre-processing is performed and the features are automatically extracted from the input speech signal by the DBN. Since speech signal features loses most of the information and the performance cannot be guaranteed because dynamic interactions can generate uncountable emotion-specific experiences that have the same core feeling state but different perceptual inclinations so DBN provides more robust features. The next step is to classify the emotions in the training phase; here the SVM classifier is chosen which performs dual classification. In order to enhance this classification process, defects must be reduced and the best discrimination of the extracted features should be obtained hence particle swarm optimization (PSO) technique is being added along with SVM classifier in the training phase. To reduce the over fitting problem and risks of a single classifier a DT is being used in the testing phase for the exact identification of emotions (anger, disgust, fear, happiness, neutral and sadness) and therefore it obtains better performance than a single classifier. The complication of the decision tool is that it can increase the computation time. Thus to eliminate this defect whale optimization (WO) technique is being added to the decision tree to reduce the complexity of the system, which in turn lessens the time taken for recognizing the emotion of the speech signal. This formulated proposed SDCN system improves the recognition rate accurately. In this work, the MATLAB environment is being preferred to perform speech emotion recognition. Using the proposed technique the achieved accuracy of emotion detection is above 95% and the identification of various emotions exceeds 98% recognition rate with a computation time of 23 seconds, which has not been achieved so far by any other existing techniques.																	1064-1246	1875-8967					2020	38	4			SI		5175	5190		10.3233/JIFS-191753													
B								The biological basis of vision: the retina	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										HORIZONTAL CELLS; GANGLION-CELLS; COLOR; PHOTORECEPTORS; DIVERSITY																				978-0-12-813895-3; 978-0-12-813894-6				2020							11	46		10.1016/B978-0-12-813894-6.00007-7													
B								The biological basis of vision: LGN, visual cortex and L plus NL models	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										COLOR; ORIENTATION																				978-0-12-813895-3; 978-0-12-813894-6				2020							47	63		10.1016/B978-0-12-813894-6.00008-9													
B								Adaptation and efficient coding	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										NATURAL IMAGES; CONTRAST ADAPTATION; LIGHT ADAPTATION; VISUAL-ADAPTATION; RETINA; STATISTICS; PRIMATE; PHYSIOLOGY; CIRCUITRY; VISION																				978-0-12-813895-3; 978-0-12-813894-6				2020							65	93		10.1016/B978-0-12-813894-6.00009-0													
B								Brightness perception and encoding curves	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										DYNAMIC-RANGE; GRAY-SCALE; LIGHTNESS; CONTRAST; MODEL; DISCRIMINATION; REFLECTANCE; FECHNER																				978-0-12-813895-3; 978-0-12-813894-6				2020							95	129		10.1016/B978-0-12-813894-6.00010-7													
B								Colour representation and colour gamuts	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										BRIGHTNESS; LUMINANCE; PERCEPTION; SATURATION; HUE																				978-0-12-813895-3; 978-0-12-813894-6				2020							131	155		10.1016/B978-0-12-813894-6.00011-9													
B								Histogram equalisation and vision models	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										RETINEX THEORY; CONTRAST; ADAPTATION; LIGHTNESS																				978-0-12-813895-3; 978-0-12-813894-6				2020							157	184		10.1016/B978-0-12-813894-6.00012-0													
B								Vision models for gamut mapping in cinema	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										COLOR; BRIGHTNESS; LUMINANCE; CONTRAST; EXTENSION; HUE																				978-0-12-813895-3; 978-0-12-813894-6				2020							185	213		10.1016/B978-0-12-813894-6.00013-2													
B								Vision models for tone mapping in cinema	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										DYNAMIC-RANGE; VISUAL-ADAPTATION; CONE PHOTORECEPTORS; BRIGHTNESS; APPEARANCE; LUMINANCE																				978-0-12-813895-3; 978-0-12-813894-6				2020							215	246		10.1016/B978-0-12-813894-6.00014-4													
B								Extensions and applications	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										COLOR; TIME																				978-0-12-813895-3; 978-0-12-813894-6				2020							247	293		10.1016/B978-0-12-813894-6.00015-6													
B								Open problems: an argument for new vision models rather than new algorithms	VISION MODELS FOR HIGH DYNAMIC RANGE AND WIDE COLOUR GAMUT IMAGING: TECHNIQUES AND APPLICATIONS	Computer Vision and Pattern Recognition Series										RETINA																				978-0-12-813895-3; 978-0-12-813894-6				2020							295	300		10.1016/B978-0-12-813894-6.00016-8													
J								Image Restoration by Combined Order Regularization With Optimal Spatial Adaptation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; Minimization; TV; Image reconstruction; Magnetic resonance imaging; Microscopy; Magnetic force microscopy; Total variation; image restoration; multi-order regularization; Hessian-Schatten norm; spatially adaptive regularization; magnetic resonance imaging; total internal reflection fluorescence microscopy	TOTAL VARIATION MINIMIZATION; ALGORITHM; RECONSTRUCTION; NETWORKS; RECOVERY; SIGNAL; MRI	Total Variation (TV) and related extensions have been popular in image restoration due to their robust performance and wide applicability. While the original formulation is still relevant after two decades of extensive research, its extensions that combine derivatives of first and second orders are now being explored for better performance, with examples being Combined Order TV (COTV) and Total Generalized Variation (TGV). As an improvement over such multi-order convex formulations, we propose a novel non-convex regularization functional which adaptively combines Hessian-Schatten (HS) norm and first order TV (TV1) functionals with spatially varying weight. This adaptive weight itself is controlled by another regularization term; the total cost becomes the sum of this adaptively weighted HS-TV1 term, the regularization term for the adaptive weight, and the data-fitting term. The reconstruction is obtained by jointly minimizing w.r.t. the required image and the adaptive weight. We construct a block coordinate descent method for this minimization with proof of convergence, which alternates between minimization w.r.t. the required image and the adaptive weights. We derive exact computational formula for minimization w.r.t. the adaptive weight, and construct an ADMM algorithm for minimization w.r.t. to the required image. We compare the proposed method with existing regularization methods, and a recently proposed Deep GAN method using image recovery examples including MRI reconstruction and microscopy deconvolution.																	1057-7149	1941-0042					2020	29						6315	6329		10.1109/TIP.2020.2988146													
J								Viewport-Adaptive Scalable Multi-User Virtual Reality Mobile-Edge Streaming	IEEE TRANSACTIONS ON IMAGE PROCESSING										Mobile virtual reality; scalable 360 degrees video tiling; mobile edge computing and streaming; resource allocation; 5G small cell systems; statistical VR navigation analysis; multiple knapsack problem with multiple constraints; branch-and-prune fully-polynomial time approximation method	MULTIVIEW VIDEO	Virtual reality (VR) holds tremendous potential to advance our society, expected to make impact on quality of life, energy conservation, and the economy. To bring us closer to this vision, the present paper investigates a novel communications system that integrates for the first time scalable multi-layer 360 degrees video tiling, viewport-adaptive rate-distortion optimal resource allocation, and VR-centric edge computing and caching, to enable next generation high-quality untethered VR streaming. Our system comprises a collection of 5G small cells that can pool their communication, computing, and storage resources to collectively deliver scalable 360 degrees video content to mobile VR clients at much higher quality. The major contributions of the paper are the rigorous design of multi-layer 360 degrees tiling and related models of statistical user navigation, analysis and optimization of edge-based multi-user VR streaming that integrates viewport adaptation and server cooperation, and base station 360 degrees video packet scheduling. We also explore the possibility of network coded data operation and its implications for the analysis, optimization, and system performance we pursue in this setting. The advances introduced by our framework over the state-of-the-art comprise considerable gains in delivered immersion fidelity, featuring much higher 360 degrees viewport peak signal to noise ratio (PSNR) and VR video frame rates and spatial resolutions.																	1057-7149	1941-0042					2020	29						6330	6342		10.1109/TIP.2020.2986547													
J								Coupled Real-Synthetic Domain Adaptation for Real-World Deep Depth Enhancement	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pipelines; Training; Deep learning; Sensors; Degradation; Adaptation models; Depth enhancement; real-world; denoising; RGBD sensor; domain adaptation; deep learning	POSE ESTIMATION; RGB; RECOGNITION; SIMULATION; IMAGES	Advances in depth sensing technologies have allowed simultaneous acquisition of both color and depth data under different environments. However, most depth sensors have lower resolution than that of the associated color channels and such a mismatch can affect applications that require accurate depth recovery. Existing depth enhancement methods use simplistic noise models and cannot generalize well under real-world conditions. In this paper, a coupled real-synthetic domain adaptation method is proposed, which enables domain transfer between high-quality depth simulators and real depth camera information for super-resolution depth recovery. The method first enables the realistic degradation from synthetic images, and then enhances degraded depth data to high quality with a color-guided sub-network. The key advantage of the work is that it generalizes well to real-world datasets without further training or fine-tuning. Detailed quantitative and qualitative results are presented, and it is demonstrated that the proposed method achieves improved performance compared to previous methods fine-tuned on the specific datasets.																	1057-7149	1941-0042					2020	29						6343	6356		10.1109/TIP.2020.2988574													
J								The Fourier-Argand Representation: An Optimal Basis of Steerable Patterns	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convolution; Two dimensional displays; Image edge detection; Approximation error; Approximation algorithms; Pattern recognition; Radon; Fourier-Argand representation; rotation-covariant function; Radon transform; ridge; edge detection; pattern matching	VESSEL SEGMENTATION; INVARIANT; RECOGNITION; FILTERS; DESIGN; IMAGES; SCALE	Computing the convolution between a 2D signal and a corresponding filter with variable orientations is a basic problem that arises in various tasks ranging from low level image processing (e.g. ridge/edge detection) to high level computer vision (e.g. pattern recognition). Through decades of research, there still lacks an efficient method for solving this problem. In this paper, we investigate this problem from the perspective of approximation by considering the following problem: what is the optimal basis for approximating all rotated versions of a given bivariate function? Surprisingly, solely minimising the L-2-approximation-error leads to a rotation-covariant linear expansion, which we name Fourier-Argand representation. This representation presents two major advantages: 1) rotation-covariance of the basis, which implies a "strong steerability" - rotating by an angle alpha corresponds to multiplying each basis function by a complex scalar e(-ik alpha); 2) optimality of the Fourier-Argand basis, which ensures a few number of basis functions suffice to accurately approximate complicated patterns and highly direction-selective filters. We show the relation between the Fourier-Argand representation and the Radon transform, leading to an efficient implementation of the decomposition for digital filters. We also show how to retrieve accurate orientation of local structures/patterns using a fast frequency estimation algorithm.																	1057-7149	1941-0042					2020	29						6357	6371		10.1109/TIP.2020.2990483													
J								Utilising Low Complexity CNNs to Lift Non-Local Redundancies in Video Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Redundancy; Decoding; Neural networks; Complexity theory; Video codecs; Noise reduction; Video coding; convolutional neural networks; compression; machine learning	EFFICIENCY	Digital media is ubiquitous and produced in ever-growing quantities. This necessitates a constant evolution of compression techniques, especially for video, in order to maintain efficient storage and transmission. In this work, we aim at exploiting non-local redundancies in video data that remain difficult to erase for conventional video codecs. We design convolutional neural networks with a particular emphasis on low memory and computational footprint. The parameters of those networks are trained on the fly, at encoding time, to predict the residual signal from the decoded video signal. After the training process has converged, the parameters are compressed and signalled as part of the code of the underlying video codec. The method can be applied to any existing video codec to increase coding gains while its low computational footprint allows for an application under resource-constrained conditions. Building on top of High Efficiency Video Coding, we achieve coding gains similar to those of pretrained denoising CNNs while only requiring about 1% of their computational complexity. Through extensive experiments, we provide insights into the effectiveness of our network design decisions. In addition, we demonstrate that our algorithm delivers stable performance under conditions met in practical video compression: our algorithm performs without significant performance loss on very long random access segments (up to 256 frames) and with moderate performance drops can even be applied to single frames in high-resolution low delay settings.																	1057-7149	1941-0042					2020	29						6372	6385		10.1109/TIP.2020.2991525													
J								Memoryless Sequences for General Losses	JOURNAL OF MACHINE LEARNING RESEARCH										algorithmic randomness; property elicitation; prediction markets	PROBABILITY; PREDICTION	One way to define the randomness of a fixed individual sequence is to ask how hard it is to predict relative to a given loss function. A sequence is memoryless if, with respect to average loss, no continuous function can predict the next entry of the sequence from a finite window of previous entries better than a constant prediction. For squared loss, memoryless sequences are known to have stochastic attributes analogous to those of truly random sequences. In this paper, we address the question of how changing the loss function changes the set of memoryless sequences, and in particular, the stochastic attributes they possess. For convex differentiable losses we establish that the statistic or property elicited by the loss determines the identity and stochastic attributes of the corresponding memoryless sequences. We generalize these results to convex non-differentiable losses, under additional assumptions, and to non-convex Bregman divergences. In particular, our results show that any Bregman divergence has the same set of memoryless sequences as squared loss. We apply our results to price calibration in prediction markets.																	1532-4435						2020	21																						
J								Quantile Graphical Models: a Bayesian Approach	JOURNAL OF MACHINE LEARNING RESEARCH										Graphical model; Quantile regression; Variational Bayes	FIELD VARIATIONAL BAYES; VARIABLE-SELECTION; CONVERGENCE-RATES; REGRESSION; NETWORKS	Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset where-in multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.																	1532-4435						2020	21																						
J								Sequential change-point detection in high-dimensional Gaussian graphical models	JOURNAL OF MACHINE LEARNING RESEARCH										Sequential change-point detection; Gaussian graphical models; Pseudo-likelihood; Mini-batch update; Asymptotic analysis		High dimensional piecewise stationary graphical models represent a versatile class for modelling time varying networks arising in diverse application areas, including biology, economics, and social sciences. There has been recent work in offline detection and estimation of regime changes in the topology of sparse graphical models. However, the online setting remains largely unexplored, despite its high relevance to applications in sensor networks and other engineering monitoring systems, as well as financial markets. To that end, this work introduces a novel scalable online algorithm for detecting an unknown number of abrupt changes in the inverse covariance matrix of sparse Gaussian graphical models with small delay. The proposed algorithm is based upon monitoring the conditional log-likelihood of all nodes in the network and can be extended to a large class of continuous and discrete graphical models. We also investigate asymptotic properties of our procedure under certain mild regularity conditions on the graph size, sparsity level, number of samples, and pre-and post-changes in the topology of the network. Numerical works on both synthetic and real data illustrate the good performance of the proposed methodology both in terms of computational and statistical efficiency across numerous experimental settings.																	1532-4435						2020	21																						
J								Harmless Overfitting: Using Denoising Autoencoders in Estimation of Distribution Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										denoising autoencoder; estimation of distribution algorithm; overfitting; combinatorial optimization; neural networks	MODEL; BIAS	Estimation of Distribution Algorithms (EDAs) are metaheuristics where learning a model and sampling new solutions replaces the variation operators recombination and mutation used in standard Genetic Algorithms. The choice of these models as well as the corresponding training processes are subject to the bias/variance tradeoff, also known as under- and overfitting: simple models cannot capture complex interactions between problem variables, whereas complex models are susceptible to modeling random noise. This paper suggests using Denoising Autoencoders (DAEs) as generative models within EDAs (DAE-EDA). The resulting DAE-EDA is able to model complex probability distributions. Furthermore, overfitting is less harmful, since DAEs overfit by learning the identity function. This overfitting behavior introduces unbiased random noise into the samples, which is no major problem for the EDA but just leads to higher population diversity. As a result, DAE-EDA runs for more generations before convergence and searches promising parts of the solution space more thoroughly. We study the performance of DAE-EDA on several combinatorial single-objective optimization problems. In comparison to the Bayesian Optimization Algorithm, DAE-EDA requires a similar number of evaluations of the objective function but is much faster and can be parallelized efficiently, making it the preferred choice especially for large and difficult optimization problems.																	1532-4435						2020	21																						
J								Discerning the Linear Convergence of ADMM for Structured Convex Optimization through the Lens of Variational Analysis	JOURNAL OF MACHINE LEARNING RESEARCH										Convex programming; variational analysis; alternating direction method of multipliers; linear convergence; calmness; metric subregularity; machine learning; statistics	ALTERNATING DIRECTION METHOD; PROXIMAL POINT ALGORITHM; PRIMAL-DUAL ALGORITHMS; VARIABLE SELECTION; MATHEMATICAL PROGRAMS; OPTIMALITY CONDITIONS; MULTIPLIERS; RACHFORD; REGRESSION; 1ST-ORDER	Despite the rich literature, the linear convergence of alternating direction method of multipliers (ADMM) has not been fully understood even for the convex case. For example, the linear convergence of ADMM can be empirically observed in a wide range of applications arising in statistics, machine learning, and related areas, while existing theoretical results seem to be too stringent to be satisfied or too ambiguous to be checked and thus why the ADMM performs linear convergence for these applications still seems to be unclear. In this paper, we systematically study the local linear convergence of ADMM in the context of convex optimization through the lens of variational analysis. We show that the local linear convergence of ADMM can be guaranteed without the strong convexity of objective functions together with the full rank assumption of the coefficient matrices, or the full polyhedricity assumption of their subdifferential; and it is possible to discern the local linear convergence for various concrete applications, especially for some representative models arising in statistical learning. We use some variational analysis techniques sophisticatedly; and our analysis is conducted in the most general proximal version of ADMM with Fortin and Glowinski's larger step size so that all major variants of the ADMM known in the literature are covered.																	1532-4435						2020	21																						
J								A novel approach to improve the bank ranking process: an empirical study in Spain	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Financial performance; fuzzy numbers; fuzzy AHP; fuzzy TOPSIS; CAMELS rating system	DECISION-MAKING TECHNIQUES; FUZZY AHP; PERFORMANCE EVALUATION; EFFICIENCY; QUALITY; TOPSIS; DEA; TECHNOLOGIES; INTEGRATION; MANAGEMENT	In this paper, a novel approach to the bank ranking process based on the possibilistic theory is proposed. Through this new method, the sensitivity of the results can be improved. Several methods are applied in order to rank the financial performance of Spanish Banks. Methods such as the Fuzzy Analytic Hierarchy Process (FAHP) and fuzzy TOPSIS are integrated in the proposed model. Criteria and sub-criteria weights are computed based on the judgments of experts using FAHP. These weights and financial indicators are inputs of the fuzzy TOPSIS methods for ranking the banks. The financial ratios are based on the CAMEL rating system criteria. Moreover, the results from the application of several distance measurements (Vertex, Hamming and Euclidean) in fuzzy TOPSIS as well as a new measure based on the possibilistic theory are compared. Finally, the results obtained applying fuzzy TOPSIS show that they vary depending on the separate measure, so it is necessary to have different measures to be able to correct decision making.																	1064-1246	1875-8967					2020	38	5					5323	5331		10.3233/JIFS-179626													
J								Fuzzy logic in economic models	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy logic; economic models; national income; fuzzy arithmetic; extension principle		Models of economic behavior are based on the search for and establishing of relationships between the various economic variables included in the model. Generally speaking, those coefficients that appear in relationships between the model variables are specific values that are determined when the model is to be used to make a given prediction. In this article we propose incorporating fuzzy logic into the study of economic models via the incorporation of fuzzy numbers to express the coefficients relating the different variables. To develop this idea, we analyze a simplified model for determining national income in which it is assumed that, for the sake of equilibrium, said value is composed of consumption and investment. Also, by relating the consumption function to income, we analyze a relationship model between the variables. To obtain broader and more real information than that resulting from the application of classical models, we incorporate fuzzy logic by assuming the parameters that establish the degree of dependence between the variables to be fuzzy numbers with a known membership function. Depending on their form, we determine their respective membership function for national income.																	1064-1246	1875-8967					2020	38	5					5333	5342		10.3233/JIFS-179627													
J								Forgotten effects of worth-creating activities in hybrid business management models in non-profit organizations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Worth-creating activities; non-profit organizations; hybrid business; forgotten effects	CORPORATE SOCIAL-RESPONSIBILITY; ENTERPRISE; FRAMEWORK	This research aims to explore the impact of the principal worth-creating initiatives in hybrid business management models. The methodology applied is the theory of the forgotten effects, which assesses some direct and indirect relationships to determine causes and effects in the value-creation in non-profit organizations. The main contribution of this study shows that the non-foreseen causes, at least in the first instance, affect remarkably to the generation of value in non-profit organizations. A case study has also been provided to solve decision making problems under partial information.																	1064-1246	1875-8967					2020	38	5					5343	5353		10.3233/JIFS-179628													
J								A citation analysis of fuzzy research by universities and countries	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bibliometrics; fuzzy research; citation analyses; universities; countries	INFORMATION SCIENCES; VISUALIZATION; JOURNALS; FIELD	This paper presents the results of a citation analysis focusing on the universities and countries represented by publications in 22 highly oriented fuzzy research journals using bibliometric techniques. Bibliometric studies have recently gained significant relevance, this fact is mainly due to the flexibility and dynamism that new information technologies provide. In our case, the structured gathered materials are thoroughly refined and offer a wide view of the influence that the selected journals have on territories and institutions around the globe. The results show the clear influence of countries located in the Asian continent and the high impact of institutions in Spain, the United States of America and China. This study sheds light on the visualization of the citation scope of these highly oriented fuzzy research journals with the aim of finding possible common ground for synergy and collaboration.																	1064-1246	1875-8967					2020	38	5					5355	5367		10.3233/JIFS-179629													
J								Knowledge, innovation, and outcomes in craft beer: Theoretical framework and fuzzy-set qualitative comparative analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Craft beer industry; tacit knowledge; explicit knowledge; formal learning; fuzzy-set qualitative comparative analysis (FsQCA)	BOUNDARIES; MANAGEMENT	This paper explores the relationship between knowledge, innovation, and profit-making in the craft beer industry in Baja California, Mexico. The research highlights the cultural nature of this industry, in which the depth of culture and tradition bolster the capacity for innovation. One source of interest in the study of cultural industries is the importance of businesses in regional development, as is happening in the Baja California region with craft beer. At its core, this study draws on the SECI model as a reference to highlight the different ways in which knowledge and learning combine to produce new forms of processes or products or break into new market segments. This empirical study is based on the fuzzy-set qualitative comparative analysis method (FsQCA), which serves to identify sequences or combinations of knowledge and learning that lead to innovation and profit.																	1064-1246	1875-8967					2020	38	5					5369	5378		10.3233/JIFS-179630													
J								The use of fuzzy mathematical tools for local public services outsourcing according to typology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Outsourcing; public sector; fuzzy logic; theory of expertons; experton's expectation; expected value of the experton's expectation	EXPERTONS	This paper addresses the need for a tool that makes use of expert opinion to determine whether individual public administration services should be outsourced. It begins by analyzing the specific characteristics of the public sector in relation to outsourcing and continues with a description of the main services, both mandatory and otherwise, provided by a local corporation, classifying them according to whether they are aimed at people, the territory or infrastructure. Fuzzy math tools are then used, by means of an endecadarian scale, to allow diverse experts to state the degree to which they agree on the possible outsourcing of a public service. In order to facilitate their decision-making, experts can offer a confidence interval of that degree. From the different intervals obtained through this process, a decision method is presented that uses the fuzzy logic tool "experton" as a basis. Several examples clarify and exemplify the method.																	1064-1246	1875-8967					2020	38	5					5379	5389		10.3233/JIFS-179631													
J								Digital and programmable economy applications: A smart cities congestion case by fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Programmable economy; blockchain; smart cities; urban congestion; fuzzy sets	INTERNET; LOGIC	Currently, cities are facing great challenges such as the population growing, citizen wellbeing, externalities management or environmental deterioration. The search for solutions are making significant inroads into the incorporation of ICT in them and subsequent large-scale digitalization such as programmable economy (PE) applications, offering the possibility to develop new approaches over these issues, in particular which related to sustainability management. Operating under a fuzzy numbers methodology and FIS (Fuzzy Inference System), the present exploratory work shows a new approach to city urban congestion management by deploying PE applications, which include some disruptive inputs such as the Internet of value, blockchain/DLT (distributed ledger technology), smarts contracts, digital assets and the monetization, all of this combined with the human motivation.																	1064-1246	1875-8967					2020	38	5					5391	5404		10.3233/JIFS-179632													
J								Causal relationships between economic activity and the mining industry in Chile	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft innovation; mining industry; forgotten effects theory; monte carlo method	OIL PRICE; MARKET ORIENTATION; INNOVATION; TECHNOLOGY; CAPABILITIES; IMPACT	The aim of this study is to examine the incidence of economic activity on soft innovation in the mining sector. Through a global analysis of indirect incidents and using the theory of forgotten effects. Thus, the case of the mining industry in Chile was analyzed, given that it is one of the industries that contributes the most to GDP in Chile. The empirical study was carried out through the application of a structured survey towards three experts from the mining sector. In addition, the study concludes with evidence that oil price has a direct incidence on investments in mining and economic expectations, and indirectly in average income middle managers, market share of the company in the mining sector, and growth of imports and exports in the mining sector.																	1064-1246	1875-8967					2020	38	5					5405	5412		10.3233/JIFS-179633													
J								Fifty years of fuzzy research: A bibliometric analysis and a long-term comparative overview	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy logic; bibliometrics; scientometrics; web of science; research indicators; H-index; CAGR	CITATION ANALYSIS; MANAGEMENT; ECONOMICS; OPERATORS; EVOLUTION; JOURNALS; SCIENCE; IMPACT; FIELD	This paper presents a general overview and a long-term comparison in fuzzy logic research published between 1965 and 2017, obtained via Web of Science. The paper analyzes the growth, impact, trends and regional localization of fuzzy research. Conventional, sophisticated among others bibliometric indicators have applies. It aggregates the information according to different levels and criteria including researchers, publications, institutions, or countries. A global perspective have been provided through comparisons of regional aggregates and compound annual growth rates that strengthen the indicators applied in this article. The results permit to visualize the influence, importance, evolution and performance of the fuzzy research as well its contribution to, and transversality with other fields. The findings show that China continues to be a leader in number of contributions. There has been a recent relative decline in the United States contributions overall. Asian and African contributions to scientific literature have grown noticeably. The results also provide a framework for the use of indicators adjusted to specific contexts and relevant information for future research.																	1064-1246	1875-8967					2020	38	5					5413	5425		10.3233/JIFS-179634													
J								OWA operators in the calculation of the average green-house gases emissions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Green-house gases emission; aggregation operators; decision making; ordered weighted average	ORDERED WEIGHTED AVERAGE; FUZZY DECISION-MAKING; AGGREGATION OPERATORS; DISTANCE MEASURES; DNDC MODEL; IMPACT	This study proposes, through weighted averages and ordered weighted averaging operators, a new aggregation system for the investigation of average gases emissions. We present the ordered weighted averaging operators gases emissions, the induced ordered weighted averaging operators gases emissions, the weighted ordered weighted averaging operators gases emissions and the induced probabilistic weighted ordered weighted averaging operators gases emissions. These operators represent a new way of analyzing the average gases emissions of different variables like countries or regions. The work presents further generalizations by using generalized and quasi-arithmetic means. The article also presents an illustrative example with respect to the calculations of the average gases emissions in the European region.																	1064-1246	1875-8967					2020	38	5					5427	5439		10.3233/JIFS-179635													
J								Bibliometrics in computer science: An institution ranking	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Computer science; bibliometrics; institutions; web of science; citations; ranking	MANAGEMENT LITERATURE; JOURNALS; INDICATORS; UNIVERSITY; HOSPITALITY; ECONOMICS; TOURISM; IMPACT; FIELD	Computer Science degrees are very popular currently among institutions worldwide. The proliferation of these programs in different universities has led to the creation of rankings for classifying programs according to their prestige and quality. However, these rankings do not specify the quality of research. This study develops a bibliometric overview of all the journals that are currently indexed in the Web of Science (WoS) database in any of the seven categories connected to Computer Science research. These categories include Artificial Intelligence, Cybernetics, Hardware and Architecture, Information Systems, Interdisciplinary Applications, Software Engineering and Theory and Methods. This study aims to identify the leading institutions over the last 25 years (1991-2015) in each area selected according to a wide range of bibliometric indicators. The results indicate that American universities are the most influential in Computer Science research. This study concludes that Computer Science traverses many institutions.																	1064-1246	1875-8967					2020	38	5					5441	5453		10.3233/JIFS-179636													
J								Early warning models for European banks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Early warning systems; CAMEL models; distance to default; expected default frequency; banks	PREDICTION	The study analyses the effectiveness of the Early Warning System (EWS) for forecasting bank defaults during the recent financial crisis based on Moody's KMV Expected Default Frequency (EDF) measure and accounting ratios using a sample of European listed banks. The Bank Financial Strength ratings D+ and below are used as a bank default indicator. Independent variables include 1-year and 5-year EDFs, one for the adverse selection effect and another for accounting ratios. Our results show that EDF metrics combined with four CAMEL covariates and the variable capturing adverse selection are able to predict the defaults of European banks up to 8 quarters before an event. When comparing the model with another only including the EDF indicator, the statistical significance improves considerably, suggesting that added variables provide additional information and power to the model. This study proposes possible improvements to the EWS which could be useful to identify inputs to incorporate in intelligent models.																	1064-1246	1875-8967					2020	38	5					5455	5462		10.3233/JIFS-179637													
J								The most influential journals and authors in digital business research	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Digital Business; information systems and technologies; bibliometrics; web of science	E-COMMERCE; BIBLIOMETRIC ANALYSIS; ELECTRONIC COMMERCE; INNOVATION; ANTECEDENTS; COMMUNITIES; TECHNOLOGY; ADOPTION; LOYALTY; SYSTEMS	In the last few decades, the number of academic articles focused on digital business has grown exponentially. The aim of this study is to present the evolution of academic research in digital business between 1990 and 2015. The analysis concentrates on identifying the most productive and influential journals in this field of research, as well as the leading authors. The results show a strong increase in digital business research during that period due to the development of an important number of specialized information systems journals, but also with publications in several disciplines of the management field. The latter reflects the multidisciplinary nature of this field of research, including not only studies with a focus on management information systems, but also on strategy, marketing, operations and more.																	1064-1246	1875-8967					2020	38	5					5463	5474		10.3233/JIFS-179638													
J								A bibliometric overview of how critical success factors influence on enterprise resource planning implementations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CSF; ERP; bibliometrics; Web of Science	SUPPLY CHAIN MANAGEMENT; INFORMATION SCIENCES; INNOVATION; JOURNALS; IMPACT	This work conducts bibliometric research into publications during the period 1999 to early 2018. The aim of this study is to help gain a better understanding of the publications covering CSF and ERP implementations all over the world. The study includes the most cited articles, most cited authors and most influential institutions as well as the most prolific countries. A database of 301 articles from 86 different institutions and 48 countries has been documented and analyzed. The results indicate that this field is growing significantly over time and a small number of US institutions are currently the most productive in this field.																	1064-1246	1875-8967					2020	38	5					5475	5487		10.3233/JIFS-179639													
J								Classifying Spanish mutual funds according to their survival capacity using SOM	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mutual funds; self-organizing maps; spanish market; survival capacity; surviving funds; disappeared funds	SELF-ORGANIZING MAPS; NEURAL-NETWORKS; SURVIVORSHIP BIAS; MORNINGSTAR RATINGS; SHAREHOLDER WEALTH; PERFORMANCE; BANKRUPTCY; SEGMENTATION; DETERMINANTS; PREDICTION	The main purpose of this article is to analyze the survival capacity of mutual funds based on their characteristics. The methodology used to meet this objective is Self-Organizing Maps (SOM), a type of Artificial Neural Network that allows patterns to be grouped together according to their similarity, enabling us to classify mutual funds into surviving funds and disappeared funds. We consider the following variables: age, size, investment flows, performance, volatility, and Morningstar rating. We use a sample of 1,617 Spanish mutual funds, of which 943 had disappeared between 2004 and 2016. The results obtained indicate that SOM accurately classifies 80% of mutual funds. Therefore, SOMs are effective instruments for classifying the disappearance of Spanish mutual funds and the variables used to define them can explain their survival capacity.																	1064-1246	1875-8967					2020	38	5					5489	5495		10.3233/JIFS-179640													
J								Penalized logistic regression to improve predictive capacity of rare events in surveys	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Survey data; sampling design; uncommon events; weighting; pseudo-likelihood	ROBUST	Logistic regression as a modelling technique of rare binary dependent variables with much fewer events (ones) than non-events (zeros) tends to underestimate their probability of occurrence. The vast literature devoted to the prediction of rare binary data identifies several ways to improve predictive performance by making modifications to the likelihood estimation. We propose two weighting mechanisms for incorporation in a pseudo-likelihood estimation that improve the predictive capacity of rare binary responses in data collected in complex surveys. We multiply sampling weights by specific correctors that lead to lower root mean square errors for event observations in almost all deciles. A case study is discussed where this method is implemented to predict the probability of suffering a workplace accident in a logistic regression model that is estimated with data from a survey conducted in Ecuador.																	1064-1246	1875-8967					2020	38	5					5497	5507		10.3233/JIFS-179641													
J								Induced OWA operators in linear regression	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										IOWA operator; linear regression; aggregation operator	ORDERED WEIGHTED AVERAGE; DECISION-MAKING; AGGREGATION OPERATORS; VARIANCE; MODELS; INCLUSION; TIME	The induced ordered weighted average (IOWA) is an aggregation operator that provides a parameterized family of operators between the minimum and the maximum. This work presents a new application that uses the simple linear regression (LR) and the IOWA operator in the same formulation. We study some of its main properties and particular cases. The main advantage of the linear regression IOWA operator is that it unifies the IOWA operator with the linear regression in the same formulation considering the degree of optimism and pessimism of the decision maker. Thus, we can under- or overestimate the regression according to complex attitudes that the decision may have in the analysis. The work ends analyzing the applicability of this new approach in a problem regarding exchange rate forecasting. The objective of the new approach is to analyze the information in a more complete way.																	1064-1246	1875-8967					2020	38	5					5509	5520		10.3233/JIFS-179642													
J								Sincerity is a dangerous thing: On how appropriability regimes shape innovation strategies	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Innovation; knowledge management; appropriability regime; imitation threat	RESEARCH-AND-DEVELOPMENT; PERFORMANCE; OPENNESS; ORIENTATION	Since the pioneeringwork of David J. Teece ("Profiting from technological innovation: Implications for integration, collaboration, licensing and public policy", Research Policy 15, 1986), the concept of "appropriability regime" -defined as the level of threat stemming from potential imitators in the firm's environment- has been useful in describing specific sectoral patterns at the aggregate level. However, there have been few attempts to formally prove the impact of different regimes on knowledge management strategies at the firm-level. Using firm-level longitudinal data from software and hardware sectors in Spain, we show that differences in appropriability regimes condition the profitability of open innovation strategies, and also induce specific patterns in firms' knowledge management strategies. More specifically, we show that the appearance of diminishing returns from an open innovation strategy is more accelerated in firms immersed in weaker regimes (i.e. firms facing more threats from potential imitators). We also show that this is due to the fact that such firms must resort to more complex (and costly) practices that go far beyond the legal protection of intellectual property, internalizing other management practices related to productive capacity and the commercialization of innovations.																	1064-1246	1875-8967					2020	38	5					5521	5528		10.3233/JIFS-179643													
J								GPS trajectory clustering method for decision making on intelligent transportation systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Segmentation; clustering; GPS trajectories; intelligent transportation systems	PERSPECTIVE; DISTANCE	Technological progress facilitates recording and collecting information on vehicles' GPS trajectories on public roads. The intelligent analysis of this data leads to the identification of extremely useful patterns when making decisions in situations related to urbanism, traffic and road congestion, among others. This article presents a GPS trajectory clustering method that uses angular information to segment the trajectories and a similarity function guided by a pivot. In order to initialize the process, it is proposed to segment the region to be analyzed in a uniform way forming a grid. The obtained results after applying the proposed method on a real trajectories database are satisfactory and show significant improvement in comparison with the methods published in the bibliography.																	1064-1246	1875-8967					2020	38	5					5529	5535		10.3233/JIFS-179644													
J								A bibliometric analysis of the Base/Bottom of the Pyramid research	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Base of the Pyramid; Bottom of the Pyramid; BoP; bibliometrics; business research; VOS viewer	BASE; BUSINESS; INNOVATION; MARKETS; SUSTAINABILITY; MANAGEMENT; COCITATION; IMPACT; FIELD	The concept of the Base/Bottom of the pyramid (BoP), since its first use in the early 2000 s, has been used by researchers and practitioners alike. The past 16 years has witnessed a significant increase in output of published research on the subject. This study aims to analyze the main contributions in this field, using a bibliometric approach. It considers key bibliometric indicators, such as leading authors, journals, institutions, sources, countries, and the most common keywords. A graphical visualization in bibliometric maps has also been developed, using the VOSviewer software. As expected, the results indicate a sharp increase in BoP research over the last 5 years. The most influential research is from the USA, although there has been a considerable wave of production from the global south. The results may be of interest for those hoping to gain an overview of the current state of BoP research.																	1064-1246	1875-8967					2020	38	5					5537	5551		10.3233/JIFS-179645													
J								A comparison of the wine sectors in Catalonia, La Rioja, Languedoc-Roussillon and Emilia-Romagna	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wine sector; Catalonia; La Rioja; Llenguadoc-Roussillon; Emilia-Romagna; Economic financial analysis; profitability	PERFORMANCE; PROFITABILITY; DETERMINANTS; PERSISTENCE; STRATEGY	This article presents an economic and financial analysis of the big wine companies in four of the sector's leading territories: Catalonia, La Rioja, Languedoc-Roussillon and Emilia Romagna. The different sectors are compared over a sevenyear period from 2008-2016. Financial indicators are used which are supported by the literature and built using information from the SABI and AMADEUS databases. The article first characterizes the areas under study and provides a review of the literature before going on to present the empirical study with the appropriate constrasts to explain the economic and financial health of these companies representative of the sector and especially their profitability.																	1064-1246	1875-8967					2020	38	5					5553	5563		10.3233/JIFS-179646													
J								Tourism research: A bibliometric and country analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bibliometric analysis; web of science; country; journals; research productivity; tourism	HOSPITALITY MANAGEMENT; JOURNALS; CITATION; UNIVERSITY; PUBLICATION; PERFORMANCE; COCITATION; ECONOMICS; AUTHOR; IMPACT	This paper aims to present a current overview of the main productive and influential countries around the world in the tourism, leisure and hospitality field. The methodology includes a bibliometric analysis in all research journals that are indexed in the Web of Science in the tourism, leisure and hospitality field in 2014 respect to the national contribution that these countries made in these journals. The study shows that USA is the country leadership in the tourism, leisure and hospitality research. Other countries, such as UK and People's Republic of China, also get a main position in the ranking. There are not previous studies examining all the journals indexed in theWeb of Science in the tourism, leisure and hospitality field during a period as wide as this one nor including as many countries as those analyzed in this paper (Top 50).																	1064-1246	1875-8967					2020	38	5					5565	5577		10.3233/JIFS-179647													
J								Document summarization using a structural metrics based representation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Text summarization; extractive summaries; sentence scoring; feature selection; neural networks		Currently, each person produces 1.7MB of information every second in different formats. However, the vast majority of information is text. This has increased the interest to study techniques to automate the identification of the relevant portions of text documents in order to offer as a result an automatic summary. This article presents a technique to extract the most representative sentences of a document taking into account by the user's criteria. These criteria are learned using a neural network, from a minimum set of documents whose sentences have been rated by the user in terms of importance. To verify the performance of the proposed methodology, we used 220 scientific articles from the PLOS Medicine journal published between 2004 and 2016. The results obtained have been very satisfactory.																	1064-1246	1875-8967					2020	38	5					5579	5588		10.3233/JIFS-179648													
J								Entrepreneurship on family business: Bibliometric overview (2005-2018)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Entrepreneurship; bibliometrics; family business; VoSviewer; WoS	MAPPING SOFTWARE; MANAGEMENT; DYNAMICS; JOURNALS; ARTICLES; BEHAVIOR; CITATION; TRENDS	This paper presents an overview of entrepreneurship in the family business. We used detailed bibliometric analysis to map terms and analyze relations and tendencies between documents, keywords, authors, universities, organizations, and countries. The purpose is to understand better the phenomenon of Entrepreneurship, their relationship, and implications related to causes and consequences derived from a family business on the first stage of their life. Scholars with interests in entrepreneurship may find relevant and pertinency information about patterns of research between Universities, authors, countries, keywords, and the co-citations and co-occurrences of them. We used descriptive bibliometric analysis, to show in this study bibliometric characteristics reporting publication and citation trends from 2005 to 2018, combining bibliometric analysis and mapping, with thematic analysis, usingWeb of Science and VoSviewer software. We based this research on the Web of Sciences(WoS) Core Collection including: Science Citation Index Expanded (Sci-Expanded), Social Sciences Citation Index (SSCI), Arts & Humanities Citation Index (A&HCI), Emerging Sources Citation Index (ESCI), in order to analyze the most productive authors, institutions and countries; besides we look for the most cited papers and articles. Bibliometric indicators represent bibliographic data, including the total number of publications and citations between 2005 and 2018 found inWoS. We created, based on VOSviewer software, graphical visualization of the bibliographic material, developing the map of different terms: journals, keywords, institutions, besides bibliographic coupling and co-citation analysis.																	1064-1246	1875-8967					2020	38	5					5589	5604		10.3233/JIFS-179649													
J								Optimizing biomedical ontology alignment in lexical vector space	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Biomedical ontology matching; lexical vector space; compact evolutionary algorithm	EVOLUTIONARY ALGORITHM; MEMETIC ALGORITHM; SIMILARITY; STRATEGY; MODEL	Biomedical ontology matching dedicates to find two heterogeneous ontologies' alignment and address their heterogeneity problem. Typically, a biomedical ontology has various biomedical concepts that are described with various labels and datatype property names, which forms a lexical space where each label or datatype property represents one dimension. Therefore, it is an effective way to present two biomedical concepts in a vector space, and use the cosine distance to measure their similarity. In this work, we present two biomedical concepts in a lexical vector space which is constructed with their inner and context concepts' lexical information, and then utilize two vector's cosine distance to measure similarity value. Then, we propose a compact Evolutionary Algorithm (cEA) to find the concept correspondences. The experiment uses Ontology Alignment Evaluation Initiative (OAEI)'s testing cases, and the expeirmental results with Vector space Based Ontology Matcher (VBOM), Genetic Algorithm based Ontology Matcher (GAOM) and OAEI's participants show the effectiveness of our proposal.																	1064-1246	1875-8967					2020	38	5					5609	5614		10.3233/JIFS-179650													
J								Convolutional neural network for human behavior recognition based on smart bracelet	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; human behavior recognition; fall detection	MODEL	In this paper, we designed a smart bracelet to realize human behavior recognition, remote monitoring, fall detection, alarm and other functions. We use a six-axis inertial measurement module to collect a large amount of human body posture data. The Kalman filter is used to preprocess the data of acceleration and gyroscope, and then the processed series data is segmented by sliding window method. At last these data are put into a convolutional neural network(CNN) to identify human activities. Furthermore, we designed a two-level fall behavior judgment, the first-level threshold determination is performed through smart bracelet, when a suspected fall behavior occurred, the sequence data are uploaded to the cloud platform, the second CNN discriminant model is triggered to accurately determine whether a fall behavior occurs.																	1064-1246	1875-8967					2020	38	5					5615	5626		10.3233/JIFS-179651													
J								PrivGRU: Privacy-preserving GRU inference using additive secret sharing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Privacy-preserving; MLaaS; gated recurrent unit; additive secret sharing; UC framework		Gated Recurrent Unit (GRU) has wide application fields, such as sentiment analysis, speech recognition, and other sequential data processing. For efficient prediction, a growing number of model owners choose to deploy the trained GRU models through the machine-learning-as-a-service method (MLaaS). However, deploying a GRU model in cloud generates privacy issues for both model owners and prediction clients. This paper presents the architecture of PrivGRU and designs the privacy-preserving protocols to complete the secure inference. The protocols include base protocols and principal protocols. Base protocols define basic linear and non-linear computations, while principal protocols construct the gating mechanisms of GRUs. The main benefit of PrivGRU is to address privacy problems while enjoying the efficiency and convenience of MLaaS. The overall secure inference is performed on shares, which retain two properties of security: correctness and privacy. To prove the security, this work adopts Universal Composability (UC) framework with the honest-but-curious corruption model. As each protocol is proved to UC-realize the ideal functionality, it can be arbitrarily composed in any manner. This strong security feature makes PrivGRU more flexible and practical in future implementation.																	1064-1246	1875-8967					2020	38	5					5627	5638		10.3233/JIFS-179652													
J								A framework of deep reinforcement learning for stock evaluation functions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep reinforcement learning; convolution neural networks; evaluation function; position sizing; money management	FUTURES; STRATEGY	Quantitative trading is a crucial aspect of money management; however, conventional trading strategies are based on indicators and signals, despite the fact that position sizing is arguably the most important issue. In this study, we present a stock evaluation function that outputs the size of the stock in each fixed period as well as the consequences of increasing or decreasing the size of one's position. The difficulties involved in using machine learning to adjust stock weighting can be attributed to difficulties in obtaining definite answers via supervised learning. We therefore train our evaluation function using reinforcement learning via CNN within the EIIE network architecture and have the agent adjust the size of the position with the purpose of maximizing profits. Back testing was performed using the top 50 stocks in Taiwan, based on market capitalization. In experiments, most of the stock returns outperformed conventional strategies in terms of cumulative stock value.																	1064-1246	1875-8967					2020	38	5					5639	5649		10.3233/JIFS-179653													
J								Embedded draw-down constraint using ensemble learning for stock trading	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Kelly criterion; ensemble learning; Monte Carlo simulation; money managemen	PORTFOLIO; MARKOWITZ; STRATEGY; FUTURES	The objective in using the Kelly criterion for money management is to maximize returns; however, in many cases, the risk level exceeds that which the investor can bear. In this study, we present an algorithm to calculate the bidding fraction, while taking into account the level of risk (i.e., the maximum drawdown). The proposed algorithm is based on ensemble learning with a combination of bagging and subset resampling. Our assessment results obtained using the FF48 (i.e., Fama-French-48) dataset revealed that when the maximum drawdown was 5% and 10%, ensemble learning outperformed the conventional approach by 2% and 4%, respectively.																	1064-1246	1875-8967					2020	38	5					5651	5659		10.3233/JIFS-179654													
J								A parameter adaptive differential evolution based on depth information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Differential evolution; depth information; global optimization; real-parameter optimization	ALGORITHM	Differential Evolution (DE) was an easy-coding and efficient stochastic algorithm for global optimization, and the whole optimization process simulates biological evolution. Superior individuals of the population that were suitable for the environment were retained during the evolution, and consequently the tolerable solutions could be obtained in the end. Despite the excellent performance of DE algorithm, there were still some shortcomings. For example, the general performance of DE depended largely on mutation strategy and control parameters, how to design the appropriate control parameters and mutation strategy were difficult tasks. Here a novel DE variant was proposed to overcome these shortcomings. By incorporating the depth information of previous generations of populations, a better diversity of trial vector candidates could be secured during the evolution process. Moreover, the thought that successful parameters should be retained to guide the update of themselves during the evolution was also incorporated into the novel algorithm. The optimization performance of the new proposed DE variant was verified under CEC 2013 test suit containing 28 benchmarks, and the results showed its competitiveness with several state-of-the-art DE variants.																	1064-1246	1875-8967					2020	38	5					5661	5671		10.3233/JIFS-179655													
J								Internal search of the evolution matrix in QUasi-Affine TRansformation Evolution (QUATRE) algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internal search; QUATRE; single-objective optimization; stochastic optimization; real-parameter optimization	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; PARAMETERS	Optimization demands exist everywhere in the real world especially in science studies and engineering practices, and it is important that the method to deal with intricacy optimization problems should itself be relative simple. Particle Swarm Optimization (PSO) and Differential Evolution (DE) both are simple evolutionary algorithms (EAs) which are proposed for single-objective optimization and both of them have been proved to be efficient methods for optimizing applications, however, there are still some weakness existing within them. A innovative evolutionary method named QUasi-Affine TRansformation Evolutionary (QUATRE) algorithm which is derived from, also tackles some weaknesses of PSO and DE algorithm, and obtains better performance on commonly used test suites. The key characteristic of QUATRE is that an automatically generated matrix named evolution matrix M is implemented in evolutionary process, which is taken as an alternative of employing the crossover rate CR. Here in this paper, we present a novel QUATRE variant, named IS-QUATRE, which can explore the search area in a better way comparison with the previous method, and relatively good optimization ability can be obtained by our proposed IS-QUATRE algorithm under CEC2013 test suit. And the conducted experimental results validate that our proposed IS-QUATRE is competitive with some other famous PSO and DE variants.																	1064-1246	1875-8967					2020	38	5					5673	5684		10.3233/JIFS-179656													
J								An automatic generation method of cross-modal fuzzy creativity	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Generation of fuzzy creativity; cross-modal; graph neural network; creative works		Digital creativity is creative expression derived from cultural creativity and information technology. In order to overcome the problem in the creative generation in the condition of fuzzy and uncertain ideas, an automatic generation method of cross-modal fuzzy creativity (AGMCFC) is proposed. In this subject, fuzzy creative data sets and learning retrieval network are constructed for the sake of extracting original creative data effectively. And the logical correlations between creative objects are acquired dynamically based on the graph neural network. Creative objects and creative styles are generated by using generative adversarial nets technology and style transfer technology, respectively. Then, the projectiles, boundary markers and location words of the creative scene objects are generated by analyzing related attributes of each entity. After adjusting the layout, creative works are automatically generated. A fuzzy creative generating environment is implemented. Experimental results show that the screened number of AGMCFC method is about twice as much as that of manual method, and the accuracy rate of AGMCFC method is improved compared with the manual method. AGMCFC method performs well at creative generation of fuzzy ideas automatically.																	1064-1246	1875-8967					2020	38	5					5685	5696		10.3233/JIFS-179657													
J								Sensitivity evaluation of soil erosion based on land use types: A case study of Minjiang River Basin	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soil erosion; sensitivity assessment; Minjiang River Basin; land use		Soil erosion is one of the main environmental problems in the world, and also an important branch of natural environment protection. Taking Minjiang River Basin as the research area, based on the supervised classification of land use status map, the comprehensive soil erosion modulus map was generated by referring to the soil erosion evaluation index and the revised soil loss equation (RUSLE). The results showed that the Minjiang River Basin is dominated by mild eroded area and middle eroded area, accounting for 41.50% and 54.81% of the total area respectively. High eroded area accounted for only 3.69%, mainly in the north and west, and there was no extremely sensitive area. From the perspective of land use types, the construction land and water body are mainly slightly sensitive areas, and forest land, garden land and cultivated land are mainly mild or middle sensitive areas, while the unused lands are shown as middle sensitive areas or highly sensitive areas. Through the discussion of the sensitivity evaluation of soil erosion in Minjiang River Basin, it can provide some references for the relevant departments to the soil and water conservation work in Minjiang River Basin.																	1064-1246	1875-8967					2020	38	5					5697	5705		10.3233/JIFS-179658													
J								A new approach of user-level intrusion detection with command sequence-to-sequence model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										User behavior; recurrent neural networks; anomaly intrusion detection; attacks and defenses	ANOMALY DETECTION; PCA	It is not foolproof for intrusion detection to focus only on the network level and the program level. Internal security and external security of information systems should be given equal attention. User-level intrusion detection can deter and curtail attackers from damaging information systems. Even if the mimic attacker has gained and enhanced the host user privileges that he illegally obtained. In this paper, a novel method based on recurrent neural networks (RNNs) is used to predict user command sequences and prophesy user behaviors. The experimental results show that our command sequence-to-sequence model is robust and effective for solving long sequential problem on three different data sets including Purdue University data set, SEA data set and self-collected data set.																	1064-1246	1875-8967					2020	38	5					5707	5716		10.3233/JIFS-179659													
J								Concentration control of SMB system in parameter space of region velocity based on adjusted fuzzy control	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Chromatographic; SMB; fuzzy; control	SIMULATED MOVING-BED; MODEL-PREDICTIVE CONTROL; SEPARATION; CHROMATOGRAPHY; PRODUCT	Because of high efficiency and cleanness, the simulated moving bed (SMB) is the most important technology in chromatographic separation. SMB system, which contains several sectors of flow rate, the switching time of valves and many other possible influencing variables, is complex, highly sensitive and difficult to control. This paper proposes adjusted fuzzy control; the adjusted fuzzy controller is applied to the SMB system to control the separation concentration by establishing a variable direction correction formula. Compared with the traditional fuzzy controller, the adjusted fuzzy controller does not need to analyze the dynamic direction of the control force in advance, moreover, the control accuracy is high and the fluctuation is small for SMB system.																	1064-1246	1875-8967					2020	38	5					5717	5729		10.3233/JIFS-179660													
J								A novel feature extraction approach based on neighborhood rough set and PCA for migraine rs-fMRI	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Resting state functional magnetic resonance imaging; migraine; correlation coefficient matrix; neighborhood rough set; principal component analysis; classification	COMPLEXITY; RESONANCE	As a common disease, migraine has a high incidence but the pathogenesis is still not clear. Resting-state Functional Magnetic Resonance Imaging (rs-fMRI) is an important research topic in the field of brain medicine, which can classify rs-fMRI data to automatically diagnose brain diseases. However, the original features of the rs-fMRI data are difficult to be extracted and the high-dimensional characteristics, which make the data analysis an extremely complicated task. Those have also plagued many researchers and bring great challenges to the existing pattern classification methods. Aiming at the high dimensionality of rs-fMRI data, in this paper, we propose a feature extraction approach based on the combination of neighborhood rough set and PCA, thereby improving the accuracy of migraine identification. Firstly, Resting-State fMRI Data Analysis Toolkit plus was applied for preprocessing, calculating three characteristic indices: Amplitude of Low Frequency Fluctuation (ALFF), Regional Homogeneity (ReHo) and Functional Connectivity (FC. The inter-group difference analysis was performed by two-sample T test and GRF correction. Then, correlation coefficient matrix original features extraction was performed by means of automatic anatomical label template (AAL). Finally, the original features were trained by the traditional classification algorithm in machine learning. The experimental results show that the propose approach can obtain good performance in predicting migraine.																	1064-1246	1875-8967					2020	38	5					5731	5741		10.3233/JIFS-179661													
J								Timetable optimization of high-speed railway hub based on passenger transfer	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										High-speed railway; hub; transfer; timetable optimization	TIME; ALGORITHM; SERVICE; DESIGN	As one kind of highest hierarchy node on the network, the transfer scheme of high-speed railway hub timetable should be studied at priority. After defining the problem that optimizing transfer scheme of timetable at high-speed railway hub, this paper proposes time adjusting strategy and platform adjusting strategy to optimize the problem, of which the first strategy introduces a FUZZY set of reasonable time range to reduce the possible train conflicts at adjacent stations on the network, and the second strategy helps to use different transfer time to match the arrival and departure of trains. Then, an optimization model of timetable based on passenger transfer is established with the minimized invalid transfer waiting time for passenger and train conflicts at adjacent stations as the objective function. The model is solved by the above two strategies in MATLAB software. Finally, the rationality and effectiveness of this model are verified, taking Shanghai Hongqiao Station as an example.																	1064-1246	1875-8967					2020	38	5					5743	5752		10.3233/JIFS-179662													
J								Exploring passenger train occupation rate influencing factors using association rules: A case study in China	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Passenger train occupation rate; association rules; train service planning; Beijing-Shanghai high-speed railway		Passenger train occupation rate directly affects Railway Company's revenue and capacity utilization of railway infrastructure, and measures to improve passenger train occupation rate is always the focus of railway companies' attention. However, researches on what, and howfactors influence passenger train occupation rate remain sparse. Based on the collective data of 363 passenger trains on the Beijing-Shanghai High-speed Railway in November 2015, this paper establishes a data-driven analysis framework to explore the influencing factors of passenger train occupation rate. Specifically, we first analyze the possible factors that influence train occupation rate from the perspective of train service planning. Then, the approach of association rules is applied to analyze the potential relationship between train occupation rate and its influencing factors. And a total of 6711 and 8133 association rules were generated for high and low train occupation rate of passenger trains, respectively. Further analysis found that train departure and arrival times, train departure and arrival station class and the type of trains are the main factors influencing the level of train occupation rate. These findings can provide reference for train service planning.																	1064-1246	1875-8967					2020	38	5					5753	5761		10.3233/JIFS-179663													
J								Research on trans-region integrated traffic emergency dispatching technology based on multi-agent	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-agent; trans-region; emergency dispatch; bidding		With the increasingly close links between provinces and municipalities, the complexity and social connectivity of emergencies have gradually increased. How to realize the rapid and scientific dispatch of emergency resources based on the comprehensive transportation network has become a realistic need. However, due to barriers in management and communication between different administrative regions and different modes of transportation, emergency resource scheduling can only be carried out in the region, and the utilization of integrated traffic channels is insufficient to form a good multimodal transport system. The multi-agent system has strong self-organization ability, learning ability and reasoning ability, which solves problems in the fields of dynamic decision-making and micro-simulation and provides ideas for solving the problem of comprehensive traffic emergency dispatching across regions. In this paper, the multi-agent system is introduced into the research of the task assignment problem of trans-regional comprehensive traffic emergency materials dispatching, based on the traditional bidding rules, the emergency dispatch task assignment rules are proposed and multi-agent trans-regional comprehensive traffic emergency dispatching model based on improved bidding rules is established. The simulation results show that the proposed method can break the regional barrier and form a multimodal transport scheme for emergency materials from the reserve point to the demand point under the minimum generalized time cost, providing decision support for emergency dispatch.																	1064-1246	1875-8967					2020	38	5					5763	5774		10.3233/JIFS-179664													
J								A parameter adaptive DE algorithm on real-parameter optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Adaptive update mechanism; differential evolution; real parameter optimization; stochastic optimization	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION	Differential Evolution (DE) algorithm generates a population of individuals by encoding with a floating point vector, and it is a simple and effective population-based stochastic optimization algorithm for global optimization of continuous space. Because of its excellent performance, DE variants can be applied in a wide range of applications in science and engineering. However, the performance of DE is sensitive to the choice of trial vector generation strategy and the associated control parameters. Therefore, it is necessary to choose appropriate mutation strategy and control parameters when tackling optimization applications. In this paper, an adaptive update mechanism is proposed to update control parameters F and Cr. The experimental results are verified on the CEC 2013 test suite which contains 28 benchmark functions for the evaluation of single objective real parameter optimization. The proposed algorithm is compared with jDE, iwPSO and ccPSO, and experiment results show its good performance.																	1064-1246	1875-8967					2020	38	5					5775	5786		10.3233/JIFS-179665													
J								An efficient algorithm for fuzzy frequent itemset mining	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; fuzzy frequent itemset mining; type-2 fuzzy-set theory; list-based structure	TREE	Association-rule mining (ARM) has concerned as an important and critical research issue in the field of data analytics and mining that aims at finding the correlations among the items in binary databases. However, the conventional algorithms considered the frequency of the item(set) in binary databases for ARM, which is not sufficient in real-life situations. Mining of useful information is not an easy task especially if the item(set) consists of the added values. Moreover, the discovered knowledge is not easy to understand if you are not the domain experts. For the past decades, several intelligent systems involved the fuzzy-set theory for many domains and applications due to it is interpretable for human reasoning. Before, the Apriori-based method for discovering fuzzy frequent itemsets (FFIs) based on the type-2 fuzzy-set theory was proposed, which requires the amount of computations with enormous candidates. In this study, we then first present a fast list-based multiple fuzzy frequent itemset mining (named as LFFT2)algorithm under type-2 fuzzy-set theory. It is developed by the type-2 membership functions to retrieve the multiple fuzzy frequent itemsets for presenting more useful and meaningful knowledge for making the efficient strategies or decisions. From the results shown in the experiments, it is clear to see that the developed LFFT2 outperforms the conventional Apriori-based approach regarding the execution time and the number of examined nodes in the search space.																	1064-1246	1875-8967					2020	38	5					5787	5797		10.3233/JIFS-179666													
J								Design and implementation of intelligent tracking car based on machine vision	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine vision; intelligent car; tracking; obstacle avoidance		The intelligent tracking car can realize self-perception, behavioral decision-making, automatic driving, etc. for environmental information, and has a wide application in our lives. In this paper, a new intelligent car is designed based on STM32F407ZGT6 MCU. The track information is collected by the OV7725 camera. The fast OTSU adaptive threshold algorithm is used to obtain the path guiding center line, which can realize image collection, image analysis and sensor data fusion, blocked path identification and intelligent judgment, automatic tracking function. The test results show that the algorithm is effective and feasible. When there are obstacles or partial loss on the track, it can intelligently identify the effective and obstacles and realize the independent decision making. It has good dynamic and robustness.																	1064-1246	1875-8967					2020	38	5					5799	5810		10.3233/JIFS-179667													
J								A hybridized parallel bats algorithm for combinatorial problem of traveling salesman	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Transportation applications; metaheuristic algorithms; hybridized parallel bats algorithm	DIFFERENTIAL EVOLUTION; OPTIMIZATION	Metaheuristic algorithms have been applied widely for real-world problems in many fields, e.g., engineering, financial, healthcare. Bats algorithm (BA) is a recent metaheuristic algorithm with considering as a robust optimization method that can outperform existing algorithms. However, when dealing with complicated combinatorial problems such as traveling salesman problems (TSP), the BA can be fallen in a local optimum. This paper proposes a new hybridizing Parallel BA (HPBA) with a mutation in local-search to escape such its drawback scenario for TSP. A graph theory mutation method is used to embed for hybridizing BA with exploiting similarities among individuals. The proposed method is extensively evaluated in TSP with series instances of the benchmark from TSPLIB to test its performance. The compared experimental result with the previous method and the best-known solutions (B.K.S) in the literature shows that the proposed approach offers competitive results.																	1064-1246	1875-8967					2020	38	5					5811	5820		10.3233/JIFS-179668													
J								An improved architecture for urban building extraction based on depthwise separable convolution	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										U-net; depthwise separable convolution; semantic segmentation; urban building extraction	NETWORK	Accurate extraction of urban buildings is a key problem in urban remote sensing image processing. It can be applied to many kinds of urban problems, such as data statistics of urban management and smart cities. In recent years, the deep learning model based on convolutional neural network is widely used in the field of target recognition and semantic segmentation. In this paper, based on U-Net for urban building extraction from remote sensing image, we propose a neural network architecture for urban building extraction from remote sensing image. We use depth separable convolution to improve it and adjust the process of network super parametric optimization according to the characteristics of building. We call this new architecture XU-Net. We evaluate the performance of XU-Net through experiments with INRIA aerial image data set. The result shows that XU-Net is not only feasible but also efficient. Moreover, XU-Net reduces number of parameters 89%, from 18.8M to 2.13M, compared to classical architecture U-Net, at the same time, it guarantees the accuracy can reach 97.5%.																	1064-1246	1875-8967					2020	38	5					5821	5829		10.3233/JIFS-179669													
J								Updating high average-utility itemsets with pre-large concept	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										pre-large; high average-utility itemset mining; dynamic database; incremental; transaction insertion	ALGORITHM; DISCOVERY; PATTERNS; MODEL	HAUIM (High Average-Utility Itemset Mining) is a variation of HUIM (High-Utility Itemset Mining) that provides a reliable measure to reveal utility patterns in light of the length of the mined pattern. Several works have been studied to improve mining efficiency by designing multiple pruning strategies and efficient frameworks, but fewer studies have centered on the sophisticated database maintenance algorithm. Existing works still have to rescan the databases multiple times when it is necessary. We first use the pre-large principle in this paper to efficiently update the newly discovered HAUIs. For further updates and maintenance on the basis of the two thresholds, the Pre-large Average Utility Itemset (PAUI) can be maintained to increase the mining performance. Experiments will then be performed to compare the batch model, the Fast-Updated (FUP)-based model, and the Apriori-like HAUIM (APHAUIM) model designed in respect of the number of maintenance patterns, scalability, runtime, and memory usage.																	1064-1246	1875-8967					2020	38	5					5831	5840		10.3233/JIFS-179670													
J								Manchurian artificial intelligence in autonomous vehicles	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Manchurian AI; self-driving car; autonomous vehicle; external manipulation		The expectation of spreading autonomous vehicles lies in the hope of significantly decreasing the 1,3 million death toll accidents worldwide, which are caused by human factor 90% of the time. In policies of insurance companies the reaction time of a human realizing any dangerous situation, reacting to it and putting the breaks into action is two seconds. The reaction time would be reduced by the power of AI that can process the huge amount of data coming from sensors and with the information regarding the situation could make decision much faster than men. The aim of the research is to denote several situations and possibilities that are capable of deceiving, diverting, capturing a self-driving car or even turning it against the other vehicles by influencing the decision-making of the artificial intelligence. In this paper I will discuss several situations that might be able to confuse the artificial intelligence of the autonomous vehicles or to make them come to an inadequate decision. You can see that safe decision-making depends on the teaching method of the artificial intelligence as well as the correctness of the data uploaded. The other aim of the research is to demonstrate how could work a Manchurian Artificial Intelligence in autonomous vehicles. I will introduce the idea of Manchurian artificial intelligence which can be activated by a certain event and can pose a threat to the passengers of the vehicles. If it is present in the software of several vehicles, a chain of worldwide accidents can be induced at a certain time.																	1064-1246	1875-8967					2020	38	5					5841	5845		10.3233/JIFS-179671													
J								Radiomic feature selection for lung cancer classifiers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Quantitative imaging features; radiomic features; nodule classification; machine learning; feature selection algorithms		Machine learning methods with quantitative imaging features integration have recently gained a lot of attention for lung nodule classification. However, there is a dearth of studies in the literature on effective features ranking methods for classification purpose. Moreover, optimal number of features required for the classification task also needs to be evaluated. In this study, we investigate the impact of supervised and unsupervised feature selection techniques on machine learning methods for nodule classification in Computed Tomography (CT) images. The research work explores the classification performance of Naive Bayes and Support Vector Machine(SVM) when trained with 2, 4, 8, 12, 16 and 20 highly ranked features from supervised and unsupervised ranking approaches. The best classification results were achieved using SVM trained with 8 radiomic features selected from supervised feature ranking methods and the accuracy was 100%. The study further revealed that very good nodule classification can be achieved by training any of the SVM or Naive Bayes with a fewer radiomic features. A periodic increment in the number of radiomic features from 2 to 20 did not improve the classification results whether the selection was made using supervised or unsupervised ranking approaches.																	1064-1246	1875-8967					2020	38	5					5847	5855		10.3233/JIFS-179672													
J								Fabrication and fuzzy analysis of AAO membrane with manipulated pore diameter for applications in biotechnology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AAO; fuzzy analysis; pore diameter; etching time; voltage	ANODIC ALUMINUM-OXIDE; POROUS ALUMINA; SIMULATION; SYSTEM	Nano porous anodized aluminum oxide is fabricated in acidic electrolyte using two step anodization process with varied potential and etching time. Pore diameter of the fabricated membrane increases with increasing the voltage and time of etching. The rate of pore opening of the membrane is established and optimized. Morphology of the membrane is studied by SEM micrographs and quantitative analysis is done by EDX. The pore size was in the range of 85-140 nm. Also the simulation and analysis for varied parameters is done using Fuzzy Logic Controller and it was observed that the simulated and value of pore diameter calculated using Mamdani's model are approximately equal with minute percentage error. The AAO membrane have potential applications in biotechnology.																	1064-1246	1875-8967					2020	38	5					5857	5864		10.3233/JIFS-179673													
J								Parametric estimation of Group II element doped zinc oxide nanostructures using fuzzy logic	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Zinc oxide; fuzzy analysis; magnesium; calcium; beryllium; strontium	ZNO; FABRICATION	Fuzzy logic has been considered as a viable method for every field of study because of its large number of applications and advantages. In nano-materials, structural knowledge of material and parametric estimation can be studied using fuzzy logic. The main objective of this work is to perform fuzzy logic analyis for parametric estimation of Zinc oxide (ZnO) nano-rods based structures. The zinc oxide nano-rods when prepared with doped metal that results in change in properties of Zinc oxide nano-rods. These properties include structural, optical, mechanical and electrical properties of ZnO nano-rods. The enhancement in properties make the doped ZnO based material suitable for energy harvesting, bio-medical, energy and electronics application. Literature depicts the effect of 2nd group elements on ZnO nano-particles which directly shows the effect of change of parameters due to change in doping concentration. In this work, the analysis of effect on bandgap and rod diameter due to change in doping concentration and synthesis time is performed on ZnO nano-rods with doping 2nd group elements. The authors concluded that the synthesis time increase the rod diameter which directly decreases the bandgap. However, the doping concentration of 2nd group elements results in increase in band gap and decrease in rod diameter. However this effect is negligible for Mg and Be due to there small atomic size. The comparison between fuzzy logic simulation and mamdani model were also analysed which shows an error of less than 1% between the value. The 2nd group doped ZnO nano-rods can be used for various application due to adjustable band-gap and rod diameter with change in doping concentration.																	1064-1246	1875-8967					2020	38	5					5865	5875		10.3233/JIFS-179674													
J								Sentiment analysis of Romanized Sindhi text	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Sentiment analysis; natural language processing (NLP); dataset; Romanized Sindhi; Python		Sentiment Analysis have also an important role in natural language processing to evaluate and analyzing the public opinion, sentiments and views about social activities such as product, services, Academic institutes, organizations etc. Lot of work has been done on English language in natural language processing. However, it is found out from the literature that still huge research gap is available for the Romanized Sindhi and there sentiment analysis in the field of natural language processing and also no any trained data is available for the testing. Classification of sentiment of Romanized Sindhi text is very difficult task. For the evaluation of sentiment of Romanized Sindhi text easily available online Python tool were used. In this research work thousand words of Romanized Sindhi text/data were used for the sentiment classification. Also discussed issues in sentiment classification in Python tool on Romanized Sindhi text.																	1064-1246	1875-8967					2020	38	5					5877	5883		10.3233/JIFS-179675													
J								Skin insertion analysis of microneedle using ANSYS and fuzzy logic	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy logic; ANSYS; microneedle; skin insertion	HOLLOW MICRONEEDLES; FABRICATION; DELIVERY; SYSTEM; ARRAY	Computers have been used in different areas of medical technology and applications. Innovation in the field of tehnology has been considered as a fundamental consitutent of medical discipline. Advancement in the field of medical and health care results in an ease for disease diagnostic, reduction in risk of diseases and lessen pain which is eventually beneficial to human life. In this work, the designing and effect of skin puncturing of micro-needle was studied. The simulations for skin puncturing was performed in ANSYS and MATLAB fuzzy logic tool. The skin puncturing using needle based on human skin coatings including dermis, stratum corneum and viable epidermis was studied. Fuzzy logic analysis was use to study the effect of effect of applied stress and tip diameter of the needle on the three layers of skin. A 3D model of human skin layer and needle was created in ANSYS and studied for an applied force of 0.4 to 0.9 N. Thinner the tip diameter of the needle, more penetration and puncturing of skin will occur. Similarly, for applied skin, more stress is required for proper puncturing of stratum corneum layer of human skin. The microfluidic analysis performed in the CFX environment of ANSYS shows that at the driving pressure of 140 kPa, 415 mu L/min flow rate has been achieved.																	1064-1246	1875-8967					2020	38	5					5885	5895		10.3233/JIFS-179676													
J								Fault tolerance of neural networks in adversarial settings	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Trustworthy machine learning; differential privacy; fault tolerance; adversarial robustness; deep learning	NOISE	Applications using Artificial Intelligence techniques demand a thorough assessment of different aspects of trust, namely, data and model privacy, reliability, robustness against adversarial attacks, fairness, and interpretability. While each of these aspects has been extensively studied in isolation, an understanding of the trade-offs between different aspects of trust is lacking. In this work, the trade-off between fault tolerance, privacy, and adversarial robustness is evaluated for Deep Neural Networks, by considering two adversarial settings under security and a privacy threat model. Specifically, this work studies the impact of training the model with input noise (Adversarial Robustness) and gradient noise (Differential Privacy) on Neural Network's fault tolerance. While adding noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness lowers fault tolerance due to increased overfitting. On the other hand, (epsilon(dp), delta(dp))-Differentially Private models enhance the fault tolerance, measured using generalisation error, which theoretically has an upper bound of e(epsilon dp) - 1 + delta(dp). This novel study of the trade-offs between different aspects of trust is pivotal for training trustworthy Machine Learning models.																	1064-1246	1875-8967					2020	38	5					5897	5907		10.3233/JIFS-179677													
J								Prediction of air-pollutant concentrations using hybrid model of regression and genetic algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Concentration of gases; genetic algorithm; polynomial regression; air quality	UNITED-STATES	Air pollution is one of the major environmental concerns in recent time. The majority of the population in the developed world live in urban area, hence air pollution concern is even more in cities. The worst gaseous pollutants are Caron Monoxide (CO), Nitrogen dioxide (NO2) and OZONE (O3). In this paper, we propose two predictive models for estimation of concentration of gases in the air, namely Carbon Monoxide (CO), Nitrogen dioxide (NO2) and OZONE (O3). The first proposed model is a combination of linear regression and Genetic Algorithm (GA). The second proposed model estimates concentration of gasses using Multivariable Polynomial Regression. First model uses a linear regression for prediction of concentration of gases, whereby errors like MAPE, R2 obtained by linear regression are optimized using a genetic algorithm (GA). Multivariable Polynomial Regression is adopted as a second proposed method for the prediction of concentration of same gases. A detailed comparative study has been carried out on the performances of GA and Multivariable Polynomial Regression. In addition, predictive equations are formed for CO, O3 and NO2 based on temperature, relative humidity, benzene and Nox (oxides of nitrogen).																	1064-1246	1875-8967					2020	38	5					5909	5919		10.3233/JIFS-179678													
J								Optimized deep neural network for cryptanalysis of DES	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cryptography; encryption; decryption; plaintext; cipher text; DES		Cryptography is the study of techniques which used to transforms the original text (plain text) to cipher text (non understandable text). Due to recent progress on digitized data exchange in electronic way, information security has become crucial in data storage and transmission. Some of the cryptographic algorithm has provided a promising solution which not only protects the data but also authenticates the systems and its participants, so the threat of various attacks is minimized. Nonetheless in the advancement of computing resources the cryptanalysis techniques also emerged and performing competitively in the field of information security with good results. In this paper, we have proposed the optimized deep neural network approach for cryptanalysis of symmetric encryption algorithm 64-bit DES (Data encryption standard). Our approach has used back propagation technique with multiple hidden layers and advanced activation function also we have addressed the problem of vanishing gradient. Further, the implementation results show that we have achieved 90% accuracy which is significantly higher as compared to previous approaches. We have also compared the proposed technique with the existing ones against three parameters i.e. time, loss, accuracy.																	1064-1246	1875-8967					2020	38	5					5921	5931		10.3233/JIFS-179679													
J								An analysis of the application of fuzzy logic in cloud computing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; fuzzy logic; scheduling algorithms; mobile cloud computing; fuzzy logic applications	SERVICE SELECTION; QOS OPTIMIZATION; TRUST; SYSTEM; MODEL; MANAGEMENT; FRAMEWORK; BEHAVIOR	Fuzzy logic has wide adoption in every field of study due to its immense advantages and techniques. In cloud computing, there are many challenges that can be resolved with the help of fuzzy logic. The core objective of this paper is to analyze the application of fuzzy logic in most demanding research areas of cloud computing. We also analyzed the fuzzy methods that were used in the solving of problems relates to cloud computing. A systematic literature review was conducted to enlist the all the challenging areas of research relates to cloud computing, categorized the most critical and challenging areas of cloud research, studied existing problem-solving techniques of each challenging cloud area, and finally studied the application of fuzzy logic in each aforementioned areas to redress different problems. The authors concluded that fuzzy logic can be used in every area of research including cloud computing to solve the problems and optimized the performance, as well as fuzzy logic techniques, were opted by many cloud computing researchers to conduct their study to optimize the performance of the system.																	1064-1246	1875-8967					2020	38	5					5933	5947		10.3233/JIFS-179680													
J								A deep learning based hybrid framework for stock price prediction	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										SVM; LSTM; back propagation; RNN; machine learning		Stock market analysis or stock price prediction is aimed at predicting firm's profitability based on current as well as historical data. From recent studies it is observed that machine learning approaches have outperformed traditional statistical methods in predictive analysis task. In our work we have analyzed time series data as prediction of stock price depends on historical variation in prices of stocks. To enhance the prediction accuracy, we have proposed a hybrid approach which is based on the concept of support vector machines (SVM) and Long Short-Term Memory (LSTM) as these algorithms are performing better in time series problem. On applying proposed approach onto the TATA Global Beverages stock dataset, we have observed prediction accuracy of ninety seven percent which is outperforming, along with this to enhance the performance author have presented some observation like relative importance of the input financial variables and differences of determining factors in market comparative predictive analysis onto the experimentation dataset.																	1064-1246	1875-8967					2020	38	5					5949	5956		10.3233/JIFS-179681													
J								A grey relational projection method for multi attribute decision making based on three trapezoidal fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-criteria decision making (MCDM); three trapezoidal fuzzy number (TT2FN); Entropy method (EM); grey correlation projection method (GRPM)	AGGREGATION; OPERATORS; TYPE-2; RISK; ENTROPY; SET	Multi-criteria decision making (MCDM) problems have been solved involving various types of fuzzy sets. We know that interval type-2 fuzzy sets (IT2FSs) are the most representative known fuzzy sets since they have the ability to capture both type of linguistic uncertainties associated with a word namely, the intra-personal and inter-personal uncertainties respectively. Here for MCDM problems, we will use the three trapezoidal fuzzy numbers (TT2FNs) which are more effective in capturing the uncertainty than IT2FSs, just like triangular fuzzy numbers has a better representational power than simple interval numbers. Moreover, Entropy method is employed for evaluating the values of unknown attribute weights. The ranking method employed here is the grey correlation projection method (GRPM), obtained by joining grey relational method (GRM) and projection method (PM) respectively. Lastly an example will be given to check the productivity of the suggested method.																	1064-1246	1875-8967					2020	38	5					5957	5967		10.3233/JIFS-179682													
J								Evaluation model for manufacturing plants with linguistic information in terms of three trapezoidal fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										MCDM; IT2 fuzzy set (IT2FS); maximizing deviation; generalized fuzzy number; three trapezoidal fuzzy numbers (TTFNs)	GROUP DECISION-MAKING; TYPE-2; DESIGN; LOGIC; AGGREGATION; SELECTION	Here we have employed three trapezoidal fuzzy numbers (TT2FNs) to deal with a multi-criteria decision making (MCDM) problems. The introduced technique takes into consideration the left and right areas of the three types of membership memberships involved in TT2FNs and also considers the risk attitude of decision maker. The presented method is more generalized since we have used TT2FNs, which are more effective in capturing uncertainty than IT2FSs, just like triangular fuzzy numbers has a better representational power than simple interval numbers. We have considered the unknown attribute environment where maximizing deviation method has been employed to evaluate the attribute weights. Moreover, evaluation model for manufacturing plants with linguistic information has been provided as an illustrative example for the justification of the proposed technique.																	1064-1246	1875-8967					2020	38	5					5969	5978		10.3233/JIFS-179683													
J								Multi-attribute decision making using grey relational projection method based on interval type-2 trapezoidal fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-attribute decision making (MADM); interval type-2 trapezoidal fuzzy number (IT2TFN); Information Entropy method (IEM); grey relational projection method (GRPM)	SELECTION; OPERATORS; MODELS; RISK; SETS	Here we present a method to deal with multi-attribute decision making(MADM) problems when the attribute values are modeled in the form of interval type-2 trapezoidal fuzzy numbers (IT2FNs), and the attribute weights are completely unknown. Grey Relation Projection Method (GRPM), which is a combination of "grey relational analysis method" and the "projection method" is employed for ranking the alternatives. The linguistic information is modeled in the form of "interval type-2 trapezoidal fuzzy numbers" (IT2TFN) which are able to capture both the intra personal and inter personal uncertainties associated with a linguistic term. Information Entropy Method (IEM) is used for calculating unknown attribute weights. Lastly, an illustrative example is provided as a verification of the developed approach.																	1064-1246	1875-8967					2020	38	5					5979	5986		10.3233/JIFS-179684													
J								Biogeography-based meta-heuristic optimization for resource allocation in cloud for E-health services	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										BBO; cloud infrastructure; meta-heuristic; resource allocation		Technology has enabled us to carry the world on our tips. Cloud computing has majorly contributed to this by providing infrastructure services on the go using pay per use model and with high quality of services. Cloud services provide resources through various distributed datacenters and client requests been fulfilled over these datacenters which act as resources. Therefore, resource allocation plays an important role in providing a high quality of service like utilization, network delay and finish time. Biogeography-based optimization (BBO) is an optimization algorithm that is an evolutionary algorithm used to find optimized solution. In this work BBO algorithm is been used for resource optimization problem in cloud environment at infrastructure as a service level. In past several task scheduling algorithms are being proposed to find a global best schedule to achieve least execution time and high performance like genetic algorithm, ACO and many more but as compared to GA, BBO has high probability to find global best solution. Existing solutions aim toward improving performance in term of power execution time, but they have not considered network performance and utilization of the systems performance parameters. Therefore, to improve the performance of cloud in network-aware environment we have proposed an efficient nature inspired BBO algorithm. Further, the proposed approach takes network overhead and utilization of the system into consideration to provide improved performance as compared to ACO, Genetic algorithm as well as with PSO.																	1064-1246	1875-8967					2020	38	5					5987	5997		10.3233/JIFS-179685													
J								Identifying emotion pattern from physiological sensors through unsupervised EMDeep model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; emotion pattern; hybrid model; physiological sensors; unsupervised learning		Pattern of emotion identification is one of the improvised research application regarding facial expression as major concern, in those cases, conventional facial expressions for patterns identification. The present model is based on signal collected from physiological sensors followed by consecutive deployment of unsupervised machine learning model. The proposed model is unsupervised in following aspects: firstly, it introduces Expectation Maximization problem with respect to unknown emotion labels to be derived from the measures. Correlation of physiological signal and individual emotion labels can be identified. This follows a considerable emotion classification method. However, the output of EM model doesn't ensure the correct identification of emotion class, if any. We introduce Support Vector Regression (SVR) as output module of this model. Hence, we try to forecast the probable classes of emotion after investigating the ranges of values and appropriate standard threshold values of physiological signal with respect to respective emotion class e.g. angry, frustration and joy. This should be noted that, the proposed model doesn't envisage facial expression analysis. However, after successful implementation of Gaussian behaviors of mixed physiological signal, we can enhance the accuracy of identification. Significant emotional context exists in output with more precise results of emotion identification phases.																	1064-1246	1875-8967					2020	38	5					5999	6017		10.3233/JIFS-179686													
J								Automatic detection of blood vessels and evaluation of retinal disorder from color fundus images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Retinal vascular disorder; matched filter; inpainting; F-score; wavelet decomposition	DIABETIC-RETINOPATHY; MACULAR DEGENERATION; SEGMENTATION; EXTRACTION; DIAGNOSIS	Here, an unique approach is presented for automatic detection of blood vessels and estimation of retinal disorder from color fundus image. This technique can be used to determine the progression of retinal disorders due to diabetic retinopathy, which can help in better evaluation and treatment for clinical purposes. The proposed method combines the Gaussian based matched filter with Kirsch for extraction of blood vessels, and inpainting technique to determine the pathologically affected region. This is tested on various databases (such as: DRIVE, Aria and Glaucoma etc.,). Various performance measures (such as: accuracy, sensitivity, specificity and F-score etc.,) are used to estimate the quality of blood vessels detection. Here, we have applied the segmentation technique to the subband-2 image in 5-level wavelet decomposition by db4 mother wavelet. This reduces the computational time for inpainting. Comparing the blood vessels and the pathologies, index for blood vessel damage is calculated. This index is proportional to retinal damage in case of diabetic retinopathy. Higher index corresponds to significant amount of blood vessel damage. From the index, progression of the disease and condition of the retina can be assessed. The index for blood vessel damage for Im-24 is 2.98%, whereas for Im-18 is 68.78%. This indicates that in Im-18 more blood vessels are affected by pathologies. It also indicates that maximum portion of the retina is affected by pathology.																	1064-1246	1875-8967					2020	38	5					6019	6030		10.3233/JIFS-179687													
J								Online identification of nonlinear systems using neo-fuzzy supported brain emotional learning network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										System identification; brain emotional learning; neo-fuzzy neurons; MATLAB		This paper proposes an online algorithm for identifying the nonlinear dynamical systems and is termed as neo-fuzzy based brain emotional learning plant identifier (NFBELPI). As the name suggests, the proposed identifier is a combination of brain emotional learning network and neo-fuzzy neurons. The integration of these two networks is realized in a way that retains the characteristics of both the networks while an enhanced performance is achieved at the same time. Precisely, the orbitofrontal cortex section of the brain emotional learning network is fused with neo-fuzzy neurons with a view to equip it with more knowledge than does the amygdala section possesses. The proposed identifier accepts n-input and m-output samples to generate an estimate of the plant output and employs a brain emotional learning algorithm to lower the estimation error by adjusting a total of ((n + m + 1) x p) + ( n + m + 2) weights, with p being the number of neo-fuzzy neurons. The proposal is validated in aMATLAB programming environment using a simulated Narendra dynamical plant as well as against the data recorded from real forced duffing oscillator. Comparison with a brain emotional learning plant identifier (BELPI) and some other state-of-the art identifiers in terms of root mean squared error (RMSE) criterion reveals the improved performance of the proposed identifier.																	1064-1246	1875-8967					2020	38	5					6045	6051		10.3233/JIFS-179689													
J								A monotonic optimization approach for solving strictly quasiconvex multiobjective programming problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multiobjective programming; monotonic optimization; strictly quasiconvex; outcome space; outer approximation	GLOBAL OPTIMIZATION; POINTS; SET	In this article, we use a monotonic optimization approach to propose an outcome-space outer approximation by copolyblocks for solving strictly quasiconvex multiobjective programming problems which include many classes of captivating problems, for example when the criterion functions are nonlinear fractional. After the algorithm is terminated, with any given tolerance, an approximation of the weakly efficient solution set is obtained containing the whole weakly efficient solution set of the problem. The algorithm is proved to be convergent and it is suitable to be implemented in parallel using convex programming tools. Some computational experiments are reported to show the accuracy and efficiency of the algorithm.																	1064-1246	1875-8967					2020	38	5					6053	6063		10.3233/JIFS-179690													
J								Intelligent decision making device for residue incorporation in soil or biomass power plants	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Agricultural residue; Biomass energy; Soil incorporation; Centralized basis	FEASIBILITY; ENERGY	For decentralized biomass plants using farm residue as input, the economic viability study are available in literature. The study calculates the different aspects of the cost of biomass power plants such as the cost of capital investment, running and maintenance of plants, residue processing costs, and the economic value equivalent of embedded nutrients (NPK) that are lost from residues. A comparison is drawn between the costs and revenue from the plant's power generation. The word modest is used for giving analogy to the power plant which has capacity higher than decentralized power plants and lower than centralized power plants. In Gujarat there exists total four biomass power plants which should be considered in the modest category. The aim is to establish the cost efficiency of adding soil nutritious chemical fertilizers and using agricultural residues for the purposes of energy-building biomass (rather than incorporating soil). To study economic viability, the discounted rate method is used. In our paper, a modest / centralized biomass power plant has been studied for economic feasibility. For this, we have selected the state of Gujarat and followed district-wise analysis. We have used the penalty method given in optimization research in order to calculate transportation and other costs. Also, we studied a soil nutrition index for measuring soil health and the proportions of N, P and K in the soil. For annual crop production, the data used is district-wise for Gujarat state for the year 2011-12.																	1064-1246	1875-8967					2020	38	5					6065	6074		10.3233/JIFS-179691													
J								Combination of AHP and TOPSIS methods for the ranking of information security controls to overcome its obstructions under fuzzy environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Information security; Analytical Hierarchy Process; TOPSIS; fuzzy logic; MCDM; MADM	SELECTION; MODEL	The organizations utilizing the cloud computing services are required to select suitable Information Security Controls (ISCs) to maintain data security and privacy. Many organizations bought popular products or traditional tools to select ISCs. However, selecting the wrong information security control without keeping in view severity of the risk, budgetary constraints, measures cost, and implementation and mitigation time may lead to leakage of data and resultantly, organizations may lose their user's information, face financial implications, even reputation of the organization may be damaged. Therefore, the organizations should evaluate each control based on certain criteria like implementation time, mitigation time, exploitation time, risk, budgetary constraints, and previous effectiveness of the control under review. In this article, the authors utilized the methodologies of the Multi Criteria Decision Making (MCDM), Analytic Hierarchy Process (AHP) and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) to help the cloud organizations in the prioritization and selection of the best information security control. Furthermore, a numerical example is also given, depicting the step by step utilization of the method in cloud organizations for the prioritization of the information security controls.																	1064-1246	1875-8967					2020	38	5					6075	6088		10.3233/JIFS-179692													
J								A new fuzzy time series forecasting method based on clustering and weighted average approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy set; fuzzy c-means clustering; information granules; fuzzy time series; forecasting	C-MEANS; LOGICAL RELATIONSHIPS; INFORMATION GRANULES; MODEL; ENROLLMENTS; INTERVALS; OPTIMIZATION; LENGTHS	Time series is a classification of data series of variables, orderly arranged with respect to time. In time series analysis, forecasting is the vital area of study besides other meaningful characteristics of the data. It has vast application in decision-making and prediction in the domain of economics, agriculture, medicine, industries, energy sector and other sciences. Fuzzy time series emerged as a robust tool to cater for historical data in linguistic values. This paper proposes the new method of fuzzy time series forecasting based on the approach of fuzzy clustering and information granules integrated with the weighted average approach to deal with the uncertainty in data series. To distinguish the power of modeling and prediction, the strategies of Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are utilized as a criterion. Findings illustrate that proposed fuzzy based time series approach is vigorous to compute the accurate estimates.																	1064-1246	1875-8967					2020	38	5					6089	6098		10.3233/JIFS-179693													
J								New delay-range-dependent stability condition for fuzzy Hopfield neural networks via Wirtinger inequality	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Delay-range-dependent stability; takagi-sugeno fuzzy model; Hopfield neural networks; LKF; LMI	DISSIPATIVITY ANALYSIS; EXPONENTIAL STABILITY; STATE ESTIMATION; SYSTEMS	This paper deals with global delay-range-dependent (DRD) stability analysis for Takagi-Sugeno (T-S) fuzzy Hopfield neural network with time-varying delays. First, we proposes a new Lyapunov-Krasovskii functional (LKF) by incorporating the information of activation function, the lower and upper bound of time delay. Further, to achieve the larger delay bound results and approximating the derivatives of LKFs, Wirtinger based inequality (WBI) together with reciprocal convex lemma (RCL) is being utilized. As a result some DRD global stability conditions for the system under consideration with less conservatism are derived in an linear matrix inequality (LMI) framework. Three numerical examples are presented in this work to exhibit the efficacy of the proposed stability criterion over the recent existing results.																	1064-1246	1875-8967					2020	38	5					6099	6109		10.3233/JIFS-179694													
J								Evaluating and prioritizing municipal solid waste management-related factors in Romania using fuzzy AHP and TOPSIS	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Municipal solid waste management (MSWM); selective collection; recycle; fuzzy analytic hierarchy process	MULTICRITERIA DECISION-MAKING; SUSTAINABILITY; ENERGY; PERFORMANCE; ECONOMY; SYSTEM; CITIES	The present study aims at identifying the main drivers underlying the development of municipal solid waste management (MSWM) so as to ensure an effective enhancement of the current waste management system and significantly improved recycling rates. Based on the factors identified in the qualitative evaluation of the deployed statistics and using fuzzy multi-criteria decision-making (MCDM), these factors are hierarchized and the competitive strategic alternative is selected/customized. The judgment of eight experts from the eight major regions of Romania has been used in the applied fuzzy AHP and TOPSIS methodologies. The robustness of the results is analyzed against the sensitivity analysis. Following the sensitivity analysis, the alternatives retained their rank so that the eight experts' assessments have been validated. By developing a sustainable MSWM, it is claimed that the recycling rate in Romania will increase.																	1064-1246	1875-8967					2020	38	5					6111	6127		10.3233/JIFS-179695													
J								Adaptive prediction-based control for an ecological cruise control system on curved and hilly roads	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ecological cruise control (ECC) system; continuous curve; up-down slope; driving safety; fuel efficiency; approximate dynamic programming (ADP)	ELECTRIC VEHICLE; MODEL; STRATEGY; ACC	This paper presents an adaptive prediction-based control scheme for developing an ecological cruise control (ECC) system when simultaneous running on the roads with the continuous curves and up-down slopes, which aims to guarantee the driving safety, riding comfort of a vehicle and enhance its fuel efficiency as well. This study is mainly divided into two parts: vehicle longitudinal speed control and vehicle lateral stability control. Firstly, based on GPS data, topographical information, and a fuel consumption model, the prediction based on approximate dynamic programming (ADP) using adaptive fuzzy neural networks (FNNs) is employed to calculate appropriate control signal required for ecological driving. Secondly, to maintain the vehicle stable on the curved road, the vehicle lateral stability control is presented utilizing sliding mode control approach. The effectiveness of the proposed ECC system, in view of stability, improved riding comfort and fuel efficiency, has been validated on the CarSim environment.																	1064-1246	1875-8967					2020	38	5					6129	6144		10.3233/JIFS-179696													
J								Optimization of communication in VANETs using fuzzy logic and artificial Bee colony	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Communication; vehicles; optimization; fuzzy logic; ad-hoc network; artificial bee colony	ROUTING PROTOCOL; ANT	Vehicular Ad-hoc Networks (VANETs) have unique capabilities and are associated with Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications to reduce traffic dynamics and provide support to the providers and the drivers to rely on security and communication alerts. Our proposed method is only supported by V2V and V2I communications and neighbourhood localization records, which are summarized in an adaptive adjustment of the forwarded information related to the messages(data communication). We optimize these message information, and only the relevant information is transmitted to the relevant vehicles, and all other unnecessary information is terminated. Because, irrelevant information causes many problems, like unnecessary resource utilization and energy consumption. In this case, the driver will be promptly reminded to take appropriate measures. A swarm intelligence algorithm such as the Artificial Bee Colony (ABC) is utilized to optimize the communication mechanism. In this paper, we present a high-level framework for ABC based on the fuzzy logic for the VANETs. We prove the validity and efficiency of the proposed method. Our proposed method achieves higher optimization accuracy in information broadcasting with minimum delay and with the lowest energy consumption.																	1064-1246	1875-8967					2020	38	5					6145	6157		10.3233/JIFS-179697													
J								The impact of big data market segmentation using data mining and clustering techniques	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Market segmentation; data mining; customer lifetime value (CLTV); RFM model (recency frequency monetary)	RFM	Targeted marketing strategy is a prominent topic that has received substantial attention from both industries and academia. Market segmentation is a widely used approach in investigating the heterogeneity of customer buying behavior and profitability. It is important to note that conventional market segmentation models in the retail industry are predominantly descriptive methods, lack sufficient market insights, and often fail to identify sufficiently small segments. This study also takes advantage of the dynamics involved in the Hadoop distributed file system for its ability to process vast dataset. Three different market segmentation experiments using modified best fit regression, i.e., Expectation-Maximization (EM) and K-Means++ clustering algorithms were conducted and subsequently assessed using cluster quality assessment. The results of this research are twofold: i) The insight on customer purchase behavior revealed for each Customer Lifetime Value (CLTV) segment; ii) performance of the clustering algorithm for producing accurate market segments. The analysis indicated that the average lifetime of the customer was only two years, and the churn rate was 52%. Consequently, a marketing strategy was devised based on these results and implemented on the departmental store sales. It was revealed in the marketing record that the sales growth rate up increased from 5% to 9%.																	1064-1246	1875-8967					2020	38	5					6159	6173		10.3233/JIFS-179698													
J								Identifier based intelligent blood glucose concentration regulation for type 1 diabetic patients: An adaptive fuzzy approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Adaptive control; glucose-insulin; diabetes; FNN; Backpropagation	MODEL-PREDICTIVE CONTROL; INSULIN SENSITIVITY	This paper presents an identifier based intelligent adaptive fuzzy control scheme with regulating blood glucose concentration in normoglycemic level of 70 mg/dl for type 1 diabetic patients. The identifier is built with fuzzy neural network (FNN) to predict the blood glucose concentration of the diabetic patient. The fuzzy based controller with generic operating regimes which cluster all the adaptive control rules is designed to robustly reject the multiple meal disturbances resulting from food intake and deal with the parametric uncertainties in model and measurement noise. All the parameters of the FNN and of the fuzzy logic system are tuned by backpropagation (BP), to achieve the control objectives. The numerical simulations are performed to show that the set point tracking, meal disturbances and measurement noise rejection can be realized within this method.																	1064-1246	1875-8967					2020	38	5					6175	6184		10.3233/JIFS-179699													
J								Solving word sense disambiguation problem using combinatorial PSO	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Word sense disambiguation; particle swarm optimization; knowledge-based approach; combinatorial PSO	ALGORITHM	In natural language processing, the problem of finding the intended meaning or "sense" of a word which is activated by the use of that word in a particular context is generally known as word sense disambiguation (WSD) problem. The solution to this problem impacts many other fields of natural language processing including sentiment analysis and machine translation. Here, WSD problem is modelled as a combinatorial optimization problem where the goal is to find a sequence of meanings or senses that maximizes the semantic meaning among the targeted words. In this work, an algorithm is proposed that uses a combinatorial version of particle swarm optimization algorithm for solving WSD problem. The test results show that the algorithm performs better than existing methods.																	1064-1246	1875-8967					2020	38	5					6193	6200		10.3233/JIFS-179701													
J								Extractive multi-document summarization using relative redundancy and coherence scores	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-document summarization; topic model; support vector regression; entity grid; rouge		Most extractive multi-document summarization (MDS) methods relies on extraction of content relevant sentences ignoring sentence relationships. In this work, we propose a unified framework for extractive MDS that also considers sentence relationships. We argue that adding a sentence to the summary increases summary score by relevance score of the new sentence plus some additional score which depends on the relationships of new sentence with other summary sentences. The quantification of additional score depends on how coherent the new sentence is with respect to the existing sentences in the summary. Simultaneously, some score is decreased from the summary score due to the redundancy which depends on overlap between new and existing summary sentences. To find the exact solution, sentence extraction problem is modeled as integer linear problem. The sentence relevance score is found using content and surface features of the sentence using topic model and regression framework. To find the relative coherence score, transition probabilities in the entity grid model are used. Redundancy between sentences is found using support vector regression that uses sentence overlapping features. The proposed method is evaluated on DUC datasets over query based multi-document summarization task. DUC 2006 dataset is used as training and development set for tuning parameters. Experimental results produce ROUGE score comparable to the state-of-the-art methods demonstrating the effectiveness of the proposed method.																	1064-1246	1875-8967					2020	38	5					6201	6210		10.3233/JIFS-179702													
J								Empirical study on imbalanced learning of Arabic sentiment polarity with neural word embedding	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Social network; sentiment analysis; polarity detection; word embedding; machine learning; imbalanced dataset; Arabic tweets	CLASSIFICATION; SMOTE	With the proliferation of social media and mobile technology, huge amount of unstructured data is posted daily online. Consequently, sentiment analysis has gained increasing importance as a tool to understand the opinions of certain groups of people on contemporary political, cultural, social or commercial issues. Unlike western languages, the research on sentiment analysis for dialectical Arabic language is still in its early stages with several challenges to be addressed. The main goal of this study is twofold. First, it compares the performance of core machine learning algorithms for detecting the polarity in imbalanced Arabic tweet datasets using neural word embedding as a feature extractor rather than hand-crafted or traditional features. Second, it examines the impact of using various oversampling techniques to handle the highly-imbalanced nature of the sentiment data. Intensive empirical analysis of nine machine learning methods and six oversampling methods has been conducted and the results have been discussed in terms of a wide range of performance measures.																	1064-1246	1875-8967					2020	38	5					6211	6222		10.3233/JIFS-179703													
J								Automated genre-based multi-domain sentiment lexicon adaptation using unlabeled data	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lexicon adaptation; sentiment lexicon; domain adaptation; multiple source; transfer learning; sentiment analysis	DOMAIN ADAPTATION; CONTEXT	Sentiment analysis research has evolved over the years to extract relevant information from opinionated raw text. Sentiment lexicon is a compiled list of sentiment words and a core component of sentiment analysis tasks. These words play a key role in domain adaptation. Domain adaptation is challenging due to variation in sentiments across the domains. We propose a solution to this research problem by presenting a genre-level sentiment lexicon adaptation approach. The model uses a language domain sense to represent the genre pertaining to the distinct characteristics of the communicated text. The approach addresses the generalization of knowledge at the genre level by learning the multi-source domain lexicon for the selected source domains. The novelty of our approach lies in the genre level relevancy of the source lexicon to the target domains. The model uses unlabeled training data for the source and target domain sentiment lexicon learning. The lexicon adaptation is demonstrated on a long list of target domains that address the three domain adaptation challenges. Experimental results have proved that the model learns the relevant scores and polarities of sentiment words, in addition, it identifies new domain-based sentiment words. The model is evaluated in comparison with standard baselines.																	1064-1246	1875-8967					2020	38	5					6223	6234		10.3233/JIFS-179704													
J								Combining trust and reputation as user influence in cross domain group recommender system (CDGRS)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommender system; cross domain; group recommender system; multi-agent system; user influence	MODELS	Group recommender system provides suggestions for a group of users by exploring the choices of individual users of the group. Popularity of group recommender systems is increasing because many activities such as listening to music, watching movies, traveling, etc. are normally performed in groups rather individually. Group recommender systems like personal recommender systems also suffer from cold start and sparsity issues. The cold start and sparsity issues result into inaccurate recommendation computation which degrades the recommendation quality. To handle the cold start and sparsity issues in a Group Recommender System (GRS), this paper proposes to use cross domain approach and introduces Cross Domain Group Recommender System (CDGRS). The recommendations provided by trustworthy and reputed users in the group enhance the acceptance towards the presented recommendations as compared to the other individuals in the group. We have combined the social factors e.g. trust and reputation to get influential user in the group recommendation. A prototype of the system is developed for tourism domain that incorporates four sub-domains i.e. restaurants, hotels, tourist places and shopping places. The performance of CDGRS is compared with GRS. Spearman's Correlation Coefficient, MAE, RMSE, Precision, Recall and F-measure are used to find the accuracy of the generated recommendations.																	1064-1246	1875-8967					2020	38	5					6235	6246		10.3233/JIFS-179705													
J								A computationally efficient method for Human Activity Recognition based on spatio temporal cuboid and Super Normal vector	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Motion energy; depth videos; frame selection; boundary curves; polynormal; dictionary learning		For the last three decades human activity recognition has shown a huge technological advancement due to less expensive RGB-D cameras and the increase in the large volume of video data. As a result of the increase in number of surveillance cameras, manual annotation becomes difficult and need for automatic recognition and annotation of video arises. In this paper, we introduce a computationally and storage efficient method for recognizing human activities from depth videos and a new frame selection method based on the mean value of motion energy. We extract normal vectors from the points in the boundary curve. Then polynormals are obtained by sequentially attaching the normals from a neighborhood of each of the points in the boundary curve. These polynormals from a spatio-temporal cuboid constructed from the input video and it is pooled to form the Super Normal vectors. These Super Normal vectors are the final feature vectors, which are given as input to the classifier. The classifier used is lib-linear SVM. The results on MSRAction3D dataset show that the algorithm we put forward is fast and the accuracy obtained is comparable with the existing methods. The method which we proposed here gives an accuracy of 88% while taking whole frames and 89.82% when frame selection method is applied. The proposed method is also tested on UTD-MHAD dataset.																	1064-1246	1875-8967					2020	38	5					6247	6255		10.3233/JIFS-179706													
J								An intelligent technique for the characterization of coal microscopic images using ensemble learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Coal; HSV; GLCM; image processing; machine learning	CLASSIFICATION; SYSTEM	Coal is a primary natural resource of fuel that is efficiently used for electricity generation, steel or iron production, and household usage. Characterization is needed for industries to understand the quality of coal before shipping. Currently, industries follow chemical, microscopical, and machine-based analysis as the gold standard for coal characterization. These conventional analyses of coal are an indispensable method over the years and have tested by high skilled petrologists. Though, these types of optical or machine-dependent recognition of coal category samples are quite slow, expensive, and restricted by subjective analyses with less accuracy. The main aim of this research is to propose an accurate, time, and cost-effective machine learning-based automated characterization system by analyzing coal color and textural features. This paper comes up with a quantitative approach toward the characterization of dissimilar types of coal for better utilization in industries. The proposed ensemble learning coal characterization method provides an accuracy of around 97% and takes less computational time than conventional methods. Hence, the proposed automated coal characterization system provides support to industries in the development of computer-aided assessment of coal category samples.																	1064-1246	1875-8967					2020	38	5					6257	6267		10.3233/JIFS-179707													
J								A fully convolutional neural network approach for the localization of optic disc in retinopathy of prematurity diagnosis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Optic disc; deep learning; convolutional neural network; retinal images; ROP diagnosis	RETINAL IMAGES; FUNDUS IMAGES; CLASSIFICATION; SEGMENTATION; NERVE; CUP	The identification of landmark features such as optic disc is of high prognostic significance in diagnosing various ophthalmic diseases. A retinal fundus photograph provides a non-invasive observation of the optic disc. The wide variability present in fundus images poses difficulties in its detection and further analysis. The reported work is a part of the fundus image screening for the diagnosis of Retinopathy of Prematurity (ROP), a sight threatening disorder seen in preterm infants. The diagnostic procedure for this disease estimates blood vessel tortuosity in a pre-defined area around the optic disc. Hence accurate optic disc localization is very important for the disease diagnosis. In this paper, we present an optic disc localization technique using a deep neural network based framework. The proposed system relies on the underlying architecture of YOLOv3, a fully convolutional neural network pipeline for object detection and localization. The new approach is tested in 10 different data sets and has achieved an overall accuracy of 99.25%, outperforming other deep learning-based OD detection methods. The test results guarantees the robustness of the proposed technique, and hence may be deployed to assist medical experts for disease diagnosis.																	1064-1246	1875-8967					2020	38	5					6269	6278		10.3233/JIFS-179708													
J								Evolutionary intelligence for breast lesion detection in ultrasound images: A wavelet modulus maxima and SVM based approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Breast ultrasound; Shearlet transform; tumour detection; wavelet modulus maxima; SVM classifier	COMPUTER-AIDED DIAGNOSIS; AUTOMATED SEGMENTATION; DOMAIN KNOWLEDGE; TUMOR-DETECTION; BLOB DETECTION; LOW-LEVEL; MAMMOGRAPHY; ALGORITHM; BENIGN; MASSES	Intelligent lesion detection system for medical ultrasound images are aimed at reducing physicians' effort during cancer diagnosis process. Automatic separation and classification of tumours in ultrasound images is challenging owing to the low contrast and noisy behavior of the image. A Computer aided detection (CAD) system that automatically segment and classify breast tumours in ultrasound (US) images is proposed in this paper. The proposed method is invariant to scale changes and does not require an operator defined initial region of interest. Wavelet modulus maxima points of the US image are analyzed to extract the tumour seed point. The lesions segmented using a region-based approach are classified using a support vector machine (SVM) classifier. Evaluation of various performance measures show that the performance of the proposed CAD system is promising.																	1064-1246	1875-8967					2020	38	5					6279	6290		10.3233/JIFS-179709													
J								A layered approach to detect elephants in live surveillance video streams using convolution neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Human elephant conflict; machine learning; convolutional neural network; support vector machine		Human-Elephant Conflict (HEC) and its mitigation have always been a serious conservation issue in India. It occurs mainly due to the encroachment of forests by humans as part of societal development. Consequently, these human settlements are highly affected by the intrusion of wild elephants as they cause extensive crop-raiding, injuries and even death in many cases. HEC is a growing problem in rural areas of India which shares a border with forests and other elephant habitats. Based on the studies, it is very explicit that HEC is an important conservation issue which affects the peaceful co-existence of both humans and elephants near the forest areas. The desirable solution for this problem would be to facilitate co-existence among humans and elephants, but this often fails because of technical difficulties. Hence, this paper presents an end-to-end technological solution to facilitate smoother coexistence of humans and elephants. The proposed work deploys a live video surveillance system along with deep learning strategies to effectively detect the presence of elephants. From the numerical analysis, it is revealed that the post-training accuracy of the deep learning model used in the proposed approach is evaluated at 98.7% and outperforms an an out-of-the-box image detector. The layered approach used in the proposed work improves resource management which is a major bottleneck in real-time deployment scenarios.																	1064-1246	1875-8967					2020	38	5					6291	6298		10.3233/JIFS-179710													
J								Intelligent gaze tracking approach for trail making test	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Eye tracking; cognitive impairment; trail making test; area of interest; scanpath; fixation; adaptive neuro fuzzy inference system; k-means clustering	EYE; PERFORMANCE	Trail making test is a cognitive impairment test used for understanding the visual attention during the visual search task. The classical paper pencil method measures the completion time of the participant and there was no mechanism for comparison across the participant with similar feature. The psychologist has to observe the reactions of the participants during the trial process and there is no mechanism to capture it. This study made an attempt to resolve the above problem and tried to infer additional parameters which can support psychologist to understand the participant performance in trail making test. The insight provided by the approach is to extract various features which helps a psychologist by providing individual profiling and group profiling of a person and can understand the group of people who show similar cognitive impairment while performing trail Making Test. The proposed Intelligent Gaze Tracking approach could classify the participant into three different groups like low, high and medium cognitive impairment based on the extracted gaze features. The proposed approach has been compared across existing literature survey to significantly show the advantage of the system in terms of identifying the people with similar characteristics in terms of cognitive impairment.																	1064-1246	1875-8967					2020	38	5					6299	6310		10.3233/JIFS-179711													
J								A fuzzy based system for target search using top-down visual attention	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Visual attention; saliency; regions of interest; fuzzy system; computer vision	MODEL	Top-down influences play a major role in the primate's visual attention mechanism. Design of top-down influences for target search problems is the recommended approach to develop better computational models. Existing top down computational visual attention models mainly exploit three factors namely the context information, target information and task demands. Here in this paper we propose a Fuzzy based System for Target Search (FSTS) which makes use of target information as the top-down factor. The system uses Fuzzy logic to predict the salient locations in an image based on the prior information about a target object to be detected in a scene or frame. The performance of the system was analysed using multiple evaluation parameters and is found to have a better average hit number, number of first hits and elapsed CPU time than the existing system. The saliency map comparison is performed with human eye fixation map and is found to predict the human fixations with better accuracy than existing systems.																	1064-1246	1875-8967					2020	38	5					6311	6323		10.3233/JIFS-179712													
J								An improved hyper smoothing function based edge detection algorithm for noisy images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Edge detection; digital radiography images; real images; noise images	MULTIRESOLUTION GRAY-SCALE	This paper presents an improved hyper smoothing function based methodology for efficient edge detection. The main aim of this work is to obtain localized edges of noisy and blurred images without duplicate ones and integrating them into meaningful object boundaries. Therefore, logarithmic hyper-smoothing function is introduced in local binary pattern leading to improved hyperfunction based local binary pattern (IHLBP) algorithm. The proposed technique uses an improved counting scheme to correctly evaluate the number of image points having pixel value greater than or equal to the central pixel. The IHLBP algorithm is tested on synthetic images, radiography images, real-life pictures from USC-SIPL and BSDS database. Improved local binary pattern (ILBP), hyper local binary pattern (HLBP), Canny and Sobel methods are also used for comparative analysis. The results reveal that the proposed algorithm performs well on all synthetic and real images in the presence of blur and salt & pepper noise. Thus IHLBP proves to be an effective approach for edge detection in comparison to conventional methods.																	1064-1246	1875-8967					2020	38	5					6325	6335		10.3233/JIFS-179713													
J								A vital neurodegenerative disorder detection using speech cues	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Best first selector; correlation based feature selection; feature reduction; parkinson; random forest; support vector machine	PARKINSONS-DISEASE; CLASSIFICATION	The main motive of this work is to discriminate a vital neurodegenerative condition of Parkinson Disease (PD) affected patients from individuals with no history of such a disorder. Excitation source features, voice quality features and prosodic features are the speech constituents considered. Voice samples of PD patients are extracted from the University of California-Irvine (UCI) Machine Learning Parkinson's database. Random Forest (RF) decision trees and Support Vector Machine (SVM) are considered for classification. Feature reduction is applied with the Correlation based Feature Selection (CFS) attribute selector classifier that utilizes Best First Selector (BFS) as a search algorithm. The work involves recognizing a PD patient from a healthy individual using only two speech sounds of /a/ and /o/. The speech sounds are extracted without the association of a certified clinician, that adds novelty. The proposed algorithm is non-invasive and accomplished 94.77% accuracy with feature selection process and applying RF classifier.																	1064-1246	1875-8967					2020	38	5					6337	6345		10.3233/JIFS-179714													
J								Foreign accent classification using deep neural nets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mel-spectrogram; deep neural networks; foreign accent classification; recurrent neural network		Speech analysis for extracting attributes such as the speaker, gender, accent and like has been a field of great interest and has been widely studied. The paper presents a novel architecture for accent identification by using a cascade of two deep-learning architecture. We design and test our proposed architecture on common voice dataset. The architecture consists of a cascade of Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN). It is trained on Mel-spectrogram of the audios. We consider five of the most popular English accents groups namely India, Australia, US, England, Canada in this study. The proposed model has an accuracy of 78.48% using CNN and 83.21% using CRNN.																	1064-1246	1875-8967					2020	38	5					6347	6352		10.3233/JIFS-179715													
J								Random fourier feature based music-speech classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Music/speech; random kitchen sink; feature vector; GTZAN database; S&S database; spectral features	DISCRIMINATION	The present paper proposes Random Kitchen Sink based music/speech classification. The temporal and spectral features such as spectral centroid, Spectral roll-off, spectral flux, Mel-frequency cepstral coefficients, entropy, and Zerocrossing rate are extracted from the signals. In order to show the competence of the proposed approach, experimental evaluations and comparisons are performed. Even though both speech and music signals differ in their production mechanisms, those share many common characteristics such as a common spectrum of frequency and are comparatively non-stationary which makes the classification difficult. The proposed approach explicitly maps the data to a feature space where it is linearly separable. The evaluation results shows that the proposed approach provides competing scores with the methods in the available literature.																	1064-1246	1875-8967					2020	38	5					6353	6363		10.3233/JIFS-179716													
J								Brain computer interface for measuring the impact of yoga on concentration levels in engineering students	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Brain Computer Interface; EEG signals; yoga; wavelets; Probabilistic Neural Network	EEG; ACTIVATION; STRESS; HEALTH; WAVES	Background: Students have to manage the strain of rising education level and their future career, accompanying the hormonal changes during their pubescence. This creates a great impact on their education as well as personal life. In this paper, an analysis has been made to study the impact of yoga on engineering students. To understand the impact. Brain-Computer Interface (BCI) approaches have been utilized. An EEG based BCI is used which will give a direct view of whats going on in the students' brains. Methodology: In this work, an experiment has been performed on engineering students and their brain activity is recorded before and after practicing yoga. In the experimental procedure, EEG signals are acquired from 8 electrodes which are associated with the cognitive and memory-related tasks of the brain. During each trial, participants solve the set of mathematical questionnaire. EEG signals are acquired during test trials before and after the yoga session. A bandpass filter is applied to preprocess the EEG signals. A discrete wavelet transform is implemented for feature extraction of the preprocessed signals. Results: Different classification algorithms are applied to classify the EEG signals before and after the yoga session. To measure the classification performance, measures such as accuracy, sensitivity, and specificity are presented in the paper. The highest accuracy of 95 % is achieved with Probabilistic Neural Network. Classification concluded the variations in signals before and after yoga. Further, in this work analysis of frequency bands, accuracy and score of the subjects before and after the yoga session are also done.																	1064-1246	1875-8967					2020	38	5					6365	6376		10.3233/JIFS-179717													
J								Enhancement of hydrophone sensitivity by varying mandrel parameters for detection of acoustic waves in underwater environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fiber optic-hydrophone; sensitivity; poissons ratio; Young's modulus; mandrel; finite element analysis		Increasingly challenging problems have been addressed in the field of underwater acoustics. The critical topics of research have been increasing the sensitivity of Sensors, measurement of sound intensity, locating target from the source, measurement of radiating power, etc. In this proposed research work, the fiber optic hydrophone sensitivity is increased by varying different parameters like dimensions of the materials, Poisson's Ratio and Young's Modulus of the mandrel. The fiber optic hydrophone is composed of different materials- Nylon, Aluminum, Polystyrene, Fiber, and Polyurethane. The design of the hydrophone is carried out using finite element analysis tools. The parameters of the hydrophone (mandrel) have been varied, and the analytical result shows that there is a considerable increase in sensitivity. These results demonstrate that there is an improvement in the hydrophone sensitivity by around 20 db in contrast with the existing hydrophone. From this result, we are now focusing on customizing the design and further validating the design, in the future.																	1064-1246	1875-8967					2020	38	5					6377	6382		10.3233/JIFS-179718													
J								Hybrid architecture for multiple transforms for signal processing applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hybrid architecture; transforms implementation; triple matrix product method	IMPLEMENTATION	Ahybrid architecture for transforms such as N-point Discrete Fourier Transform (DFT), Discrete Cosine Transform (DCT), Discrete Sine Transform (DST) and Discrete Wavelet Transform (DWT) has been proposed and implemented using triple matrix product method. There is limited work reported on efficient single architecture that can perform multiple transforms simultaneously or serially depending on the application requirement. The Hybrid architecture implemented, can compute various transforms efficiently. A controller is designed which can perform different transforms using the hybrid architecture based on the input provided. The implemented systolic array can be used for computing the diagonal elements of triple-matrix product. The designed architecture produces the output of transform sequence in order, which avoids reordering at output. The implemented architecture can be used to handle large sized transforms by repeatedly using fixed size architecture for a large number of points without increasing the number of Processing Elements (PEs). The proposed architecture has been validated with a watermarking algorithm that uses DCT and DWT transforms and its performance analyzed. The proposed hybrid architecture is implemented on Spartan-7 xc7s100fgga676-1. The simulation results are given and analyzed against standalone architecture.																	1064-1246	1875-8967					2020	38	5					6383	6390		10.3233/JIFS-179719													
J								Image forgery detection using deep textural features from local binary pattern map	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; rotation invariant-local binary pattern; pretrained convolutional neural etworks; deep textual features; image forgery detection	AUTHENTICATION; CLASSIFICATION	Nowadays the manipulations of digital images are common due to easy access of many online photo editing applications and image editing softwares. Forged images are widely used in social media for creating deceitful propaganda of an individual or a particular event and for cooking up fake evidences even in court proceedings. Hence ensuring the integrity of digital images is of prime significance and it has become a hot research area. In this paper, a novel technique for image forgery detection is proposed. The method utilizes the layer activation of inception-ResNet-v2, a pretrained Convolutional Neural Network(CNN)to extract the deep textural features from Rotation Invariant - Local Binary Pattern (RI-LBP) map of the chrominance image. Non-negative Matrix Factorization (NMF) technique is used to reduce the dimensionality of the extracted features. The dimensionality reduced features are used to train a quadratic Support Vector Machine(SVM) classifier to classify images into forged or authentic. The method is assessed on four benchmark datasets (CASIA ITDE v1.0, CASIA ITDE v2.0, CUISDE and IFS-TC). Extensive experimental analysis is done and the results show an improved detection accuracy compared to the state-of-the-art methods.																	1064-1246	1875-8967					2020	38	5					6391	6401		10.3233/JIFS-179720													
J								A high payload separable reversible data hiding in cipher image with good decipher image quality	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Reversible data hiding; pixel expansion; embedding capacity; imperceptibility; SRDHIEI	DIFFERENCE; STEGANOGRAPHY	Reversible Data Hiding (RDH) is a technique that is used to protect the secret by using digital cover media to hide it and to retrieve the cover after extracting the secret. The Reversible Data Hiding In an Encrypted Image (RDHIEI) protects the privacy of secret information and also the cover by hiding confidential information in a cipher image. Some of the algorithms in RDHIEI can extract the information if and only if the cipher image is already decrypted and some algorithms can decrypt the image if only if the data has already been extracted from the cipher image. In those algorithms, the extraction and decryption cannot be separated. But some applications like healthcare and army image processing require that image recovery and information extraction to be separate processes; this new technique is called Separable Reversible Data Hiding In an Encrypted Image (SRDHIEI). In this paper, a novel SRDHIEI is suggested with high payload and good quality decipher image by embedding information in a cipher image on two levels. In the first level data is embedded by the Least Significant Bit(LSB) substitution method and in the second level data is embedded by using Pixel Expansion (PE) method. For image confidentiality, the cover image is encrypted by using an additive homomorphism technique. The benefits of the proposed method is to transfer the cover image in an extremely secure manner with PSNR of 8.6813 dB, -0.0077 correlation and 7.998 entropy. The average embedding capacity of the proposed method is 217579 bits, and the decrypted image PSNR is 29.5 dB. 100% restoration of the host image and 100% lossless secret information extraction can be achieved.																	1064-1246	1875-8967					2020	38	5					6403	6414		10.3233/JIFS-179721													
J								Tongue print identification using deep CNN for forensic analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Tongue print; biometric; identification; CNN; support vector machine; forensics		The need of newer biometric traits is increasing, as the conventional biometric systems are found to be vulnerable to forging. Nowadays, tongue print is gaining importance as a biometric trait, especially in the area of forensics. Tongue is a well protected vital organ which exhibits rich structural patterns. Success of tongue print as a biometric tool depends on how well the discriminating features are extracted from it. Advancements in the field of deep neural network and availability of high-end computing environments facilitate remarkable progress in the area of image recognition. CNN follows a hierarchical learning to extract feature maps that highly characterize the training data. However, obtaining a tongue print dataset large enough to train a CNN for recognition poses a huge challenge. Alternatively, two techniques can be used to successfully employ CNN for recognition: fine-tuning pre-trained CNN models, to use as a classifier, with the new input dataset and class labels to perform tongue-print image recognition. Another effective method is to use a pre-trained CNN model as a feature extractor, to extract features from the input tongue dataset and then use a state-of-the-art classifier to perform image recognition. In this paper, we addressed three important factors regarding the deployment of tongue-print as a biometric tool. Since, a tongue-print dataset is not publicly available, our first objective to create a challenging tongue-print dataset. We then explored and evaluated different state-of-the-art CNN architectures for image recognition. These models are varied in their architecture and contain 5 million to 144 million parameters. Finally, we analyzed different approaches to use the pre-trained CNN models for the tongue-print identification task.																	1064-1246	1875-8967					2020	38	5					6415	6422		10.3233/JIFS-179722													
J								Occluded face recognition using NonCoherent dictionary	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Face recognition; sparse representation; occlusion; dictionary		Today's world is facing threats from terrorism, for safety concerns system needs to strengthen security. Security is a challenging task and it can be strengthened by technology such as biometric and surveillance cameras. These technologies are deployed everywhere but it is the need of the days a strong automatic face recognition applications so they can be used to recognize the person in an unconstrained environment. In an unconstrained environment, images are affected by occlusion such as a scarf, goggle, random but these variations decrease the performance of face recognition. Also, the accuracy of face recognition depends on the number of labeled samples and variation available in the training dataset. But some applications of face recognition such as passport verification, identification of these applications have fewer training samples without or with very less occlusion hence, it is not enough to solve the issue of unconstrained conditions. This problem has been targeted by many researchers using an occlusion based training dataset where common variation exists in both training and testing datasets. This paper tackles the occlusion issues by designing a NonCoherent dictionary. The proposed dictionary is designed by two steps firstly it extracts the occlusion from the face image and secondly creates NonCoherent samples. The extensive experimentation is done on benchmark face databases and compared the results on state-of-the-art SRC methods by using NonCoherent and normal dictionary also compared the sparse coefficients of each method. The results show the effectiveness of proposed model.																	1064-1246	1875-8967					2020	38	5					6423	6435		10.3233/JIFS-179723													
J								Multi-biometric cryptosystem using graph for secure cloud authentication	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud security; template security; biometric authentication; cloud database security; secure cloud authentication; biometric cloud security; feature extraction; thinning	FINGERPRINT; MINUTIAE	A recent trend of the information technology is cloud computing technology where many complex tasks are simplified with increased speed and low cost. However, cloud authentication plays a crucial role once all the data's are uploaded in the cloud. In this paper, multi-biometric template security based on generation of unique graph is proposed to ensure a safe and secured cloud authentication mechanism. To overcome the vulnerability issues of traditional password and token based authentication methods, in this work, a multi-biometric system is proposed. The left, right fingerprint and palm print multi modal traits are given as input to the system. After preprocessing, all the features are combined to generate a weighted graph called as branching factor graph. In the end, the node and edge values of the branching factor graph will be stored in the cloud database. Experimental study shows that the proposed method achieved a very low equal error rate than the other existing works.																	1064-1246	1875-8967					2020	38	5					6437	6444		10.3233/JIFS-179724													
J								A novel publicly delegable secure outsourcing algorithm for large-scale matrix multiplication	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Matrix multiplication; cloud computing; secure outsourcing; security; verifiability; Cipher-Text Only Attack (COA); Chosen Cipher-Text Attack (CCA) and Known Plain-Text Attack (KPA)	COMPUTATION; SYSTEMS; CLOUD	Computation of complex mathematical problems are always a challenge of resource constrained clients. A client can outsource the computations to resource abundant cloud server for execution. But this arrangement brings many security and privacy challenges. In this paper, we have presented a secure and efficient algorithm for general computation and scientific problem i.e. matrix multiplication. The proposed algorithm is inspired by the existing algorithm, but we believe that it is imperative to improve the algorithm to enable secure outsourcing of computation. The previous state-of-the art algorithm for matrix multiplication is vulnerable to the Cipher-Text Only Attack (COA) along with Chosen Cipher-Text Attack (CCA) and Known Plain-Text Attack (KPA) and reveal information about the client's data. Hence fails the security requirements of the outsourcing algorithm. The proposed work retains the efficiency benefit of state-of-the-art algorithm, additionally defended the client data against (COA) along with (CCA) and Known Plain-Text Attack (KPA).																	1064-1246	1875-8967					2020	38	5					6445	6455		10.3233/JIFS-179725													
J								Biological inheritance on fuzzy hyperlattice ordered group	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hyperlattice; fuzzy hyperlattice; lattice ordered group; fuzzy lattice ordered group; fuzzy hyperlattice ordered group		In this manuscript we proposed the concept of fuzzy hyperlattice ordered group. Algebraic hyperstructures represent a natural extension of classical algebraic structures. In a classical algebraic structure, the composition of two elements is an element, while in an algebraic hyperstructure, the composition of two elements is a set. Algebraic hyperstructure theory has many applications in other disciplines. The foremost intendment of the manuscript is to contribute some properties of fuzzy hyperlattice ordered group and also an application of fuzzy hyperlattice ordered group on inheritance.																	1064-1246	1875-8967					2020	38	5					6457	6464		10.3233/JIFS-179726													
J								Combination of neural network and advanced encryption and decryption technique is used for digital image watermarking	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Digital image watermarking; attacks; AES; encryption; decryption; neural network		The computer system represents data in various format viz (i) Audio, (ii) Video, (iii) Text, (iv) Message, and (v) Image format. There are many ways through which, data can be easily transferred. The process of transferring digital bank cheque image is done with the help of cheque truncation system. It transfers cheque from home branch to clearing bank branch for faster clearance of customer cheque. This helps the banking system to keep transparency of transaction. During the flow of digital bank cheque image, there may be possibility of various attacks like, Cropping, JPEG compression, Median filtering, Gaussian Blur noise, Rotation, Salt & Pepper noise, etc. This arise the issues of copyright protection and security for digital bank cheque image. In this research work, The Combination of Digital Image Watermarking Using Neural Network and Advanced Encryption and Decryption Technique is Used for Providing Copyright Protection & Security Technique to Digital Bank Cheque Image.																	1064-1246	1875-8967					2020	38	5					6465	6474		10.3233/JIFS-179727													
J								Energy measure cluster based concealed aggregation for confidentiality and integrity in WSN	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										WSNs; concealed data aggregation; clustering; end-to-end encryption; homomorphic encryption		Wireless sensor networks (WSNs) is a network of resource constrained sensors deployed in unattended region for environmental monitoring. The resource constrained and ad-hoc nature of WSN stances lot of challenges to the research community when designing protocols for such environments. Now a days WSN is widely deployed from environmental monitoring to military applications. So secure data transmission is mandated in WSNs when it is used for mission critical applications. Data aggregation is a widely used method in WSNs for reducing communication overhead by mitigating unwanted data transmissions. But upholding accuracy of such aggregated data and providing security for the same is a challenging task. In this paper we propose Cluster based Concealed data Aggregation for Confidentiality and Integrity(C-CASIN) in WSN. It uses Elliptic Curve Cryptography based Elgamal additive homomorphic encryption scheme for providing Confidentiality and Integrity.EC-Elgamal Signature algorithm supports for authenticity. By supporting end-to-end encryption proposed method provides security with reduced computation and communication overheads. Results show that proposed method defend against various possible attacks and malicious behavior with the extended network lifetime of 15 to 20 percentage when comparing with basic secure model.																	1064-1246	1875-8967					2020	38	5					6475	6482		10.3233/JIFS-179728													
J								Nonce reuse/misuse resistance authentication encryption schemes for modern TLS cipher suites and QUIC based web servers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AEAD; GCM_SIV; AES_GCM_SIV; TLS; QUIC	SECURITY; GCM	Authentication Encryption with Associated Data (AEAD) is a scheme that preserves the integrity of both the cipher text and authenticated data. In AEAD, cipher suites like GCM_SIV and AES_GCM_SIV provides the message integrity through nonce-based authentication encryption technique. The problem of nonce-based authentication encryption is the repetition of nonce in two different messages that violates message integrity property when the number of message blocks is maximized to 2(32). This paper verifies the maximum limit of nonce usefulness and proves better security bounds attained in GCM_SIV and AES_GCM_SIV using nonce-reuse/misuse resistance authentication encryption (NRMR-AE) technique. The NRMR-AE resistance property achieves better security bounds and performance even when the nonces are repeated in different messages. But nonce repetition in NRMR-AE property reduces the number of message encryption and message length (in blocks) in GCM_SIV and AES_GCM_SIV_AEAD methods used in QUIC(Quick UDP Internet Communications) and TLS Cipher suites which is found to be a greater drawback. This paper increases the number of messages encrypted even with maximum number of nonce repetition ensuring that the message length in AES_GCM_SIV meets the standard NIST bound 2(-32).																	1064-1246	1875-8967					2020	38	5					6483	6493		10.3233/JIFS-179729													
J								Homomorphic encryption approach for exploration of sensitive information retrieval	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud database; homomorphic encryption; privacy; sensitive information retrieval; information security		This paper presents the core algorithms behind DB-Query-Encryption, a proposal that supports private information retrieval (PIR) explorations. DB-Query-Encryption permits users for selectively retrieve information from a cloud database whereas keeping sensitive data terms secretive. As an example use case, a medical research institute may, as part of a sensitive data exploration, requisite to look up facts about an individual person from a cloud database deprived of reveling the person's identity. The basic idea behind DB-Query-Encryption is to uses homomorphic encryption, which allows the cloud server to fulfill this request, whereas making it infeasible for the database owner (or a hacker who might compromised) to conclude the name being explored for, either which records are retrieved. The query, which retrieved the information, still secretive even if the spectator be able to search all the data over the cloud server and all the actions as they are being executed. Within that period, the query response produced by the cloud server is considerable smaller than the whole cloud database, making it more convenient when it is not feasible or appropriate for the user to transfer the entire database.																	1064-1246	1875-8967					2020	38	5					6495	6505		10.3233/JIFS-179730													
J								Hypnotic computer interface (hypCI) using GEORGIE: An approach to design discrete voice user interfaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial intelligence; cognitive science; defense; hypnotic triggers; user experience design; voice user interfaces	HYPNOSIS	Voice User Interfaces have become popular with the advent of Alexa, Google Home, Cortana and other commercial speech recognition interfaces; however, the privacy of the end users is compromised while using these interfaces in public. In addition, users can feel a bit awkward while using these interfaces with loud voice while they are outside their homes. Contextually, 'Hypnosis' and 'Hypnotherapy' have not been frequently applied as a way of human communications although the principle of suggestion induced behavior changes in an interesting approach to interact with machines. In this paper, GEORGIE a prototype AI was used to achieve a novel means of interaction inspired from the principles of hypnotherapy, which is a discrete interface ensuring that end-users' privacy is not compromised. It is envisaged that people who prefer secret communication and interaction might love to use this hypnotic computer interface (hypCI). The hypCI would be the novel means of human robot interface (HRI) or human computer interface (HCI).																	1064-1246	1875-8967					2020	38	5					6507	6516		10.3233/JIFS-179731													
J								A preliminary investigation into automatically evolving computer viruses using evolutionary algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Anti-malware research; cyber security; evolutionary algorithms; malware; malware creation; virus	MALWARE; CLASSIFICATION	This paper attempts to employ Evolutionary Algorithm(EA) techniques to evolve variants of a computer virus(Timid) that successfully evades popular antivirus scanners. Generating authentic variants of a specific malware results in a valid database of malware variants, which is sought by anti-malware scanners, so as to identify the variants before they are released by malware developers. This preliminary investigation applies EAs to mutate the Timid virus with a simple code evasion strategy, i.e., insertion and deletion(if available) of a specific assembly code instruction directly into the virus source code. Starting with a database of over 60 popular antivirus scanners, this EA based approach for malware variant generation successfully evolves Timid variants that evade more than 97% of the antivirus scanners. The results from these preliminary investigations demonstrate the potential for EA based malware generation and also opens up avenues for further analysis.																	1064-1246	1875-8967					2020	38	5					6517	6526		10.3233/JIFS-179732													
J								S-DDoS: Apache spark based real-time DDoS detection system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Distributed denial of service (DDoS); K-means clustering algorithm; big data; entropy; network security; apache spark	ATTACKS	A Distributed Denial of Service (DDoS) attack is the biggest threat to Internet-based applications and consumes victim service by sending a massive amount of attack traffic. In the literature, numerous approaches are available to protect the victim from the DDoS attacks. However, the attack incidents are increasing year by year. Further, several issues exist in the traditional framework based detection system such as itself becoming a victim, slow detection, no real-time response, etc. Therefore, the traditional framework based system is not capable of processing live traffic in the big data environment. This paper proposes a novel Spark streaming-based distributed and real-time DDoS detection system called S-DDoS. The proposed S-DDoS system employs the K-Means clustering algorithm to recognize the DDoS attack traffic in real-time. The proposed detection model designed on the Apache Hadoop framework using highly scalable H2O sparkling water. The detection model deployed on the Spark framework to classify live traffic flows. The results show that the proposed S-DDoS detection system efficiently detects the DDoS attack from network traffic flows with higher detection accuracy (98%).																	1064-1246	1875-8967					2020	38	5					6527	6535		10.3233/JIFS-179733													
J								Improved PSO for optimizing the performance of intrusion detection systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intrusion detection system; receiver operating characteristics; particle swarm optimization; pareto front; multi-objective optimization; non-linear programming	MULTIOBJECTIVE OPTIMIZATION	Intrusion detection system is a second layer of defence in a secured network environment. When comes to an IoT platform, the role of IDS is very critical since it is highly vulnerable to security threats. For a trustworthy intrusion detection system in a network, it is necessary to improve the true positives with minimum false positives. Research reveals that the true positive and false positive are conflicting objectives that are to be simultaneously optimized and hence their trade-off always exists as a major challenge. This paper presents a method to solve the tradeoff among these conflicting objectives using multi-objective particle swarm optimization approach. We conducted empirical analysis of the system with multiple machine learning classifiers. Experimental results reveals that this technique with J48 classifier gives the highest gbest value 10.77 with minimum optimum value of false positive 0.02 and maximum true positive 0.995. Empirical evaluation shows an incredible improvement in Pareto set in the objective function space by attaining an optimum point.																	1064-1246	1875-8967					2020	38	5					6537	6547		10.3233/JIFS-179734													
J								Towards minimizing delay and energy consumption in vehicular fog computing (VFC)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intelligent transportation systems; modified differential evolution; vehicular fog computing	ARCHITECTURE; INTERNET	Vehicular Fog Computing (VFC) is a natural extension of Fog Computing (FC) in Intelligent Transportation Systems (ITS). It is an emerging computing model that leverages latency aware and energy aware application deployment in ITS. However, due to heterogeneity, scale and dynamicity of vehicular networks (VN), deployment of VFC is a challenging task. In this paper, we propose a multi-objective optimization model towards minimizing the response time and energy consumption of VFC applications. Using the concepts of probability and queuing theory, we propose an efficient offloading scheme for the fog computing nodes (FCN) used in VFC architecture. The optimization model is then solved using a modified differential evolution (MDE) algorithm. Extensive experimentations performed on real-world vehicular trace of Shenzhen, reveals the superiority of proposed VFC framework over generic cloud platforms.																	1064-1246	1875-8967					2020	38	5					6549	6560		10.3233/JIFS-179735													
J								An intelligent low power consumption routing protocol to extend the lifetime of wireless sensor networks based on fuzzy C-Means plus plus clustering algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wireless sensor networks; clustering; cluster head; k-means; k-means plus; fuzzy c-means algorithm; fuzzy c-means plus; algorithm; energy efficient network		The design of a wireless sensor network (WSN) faces many constraints. Mostly, WSN is energy constraint because the sensor nodes are battery operated. Available power expenditure in WSN largely depends on the efficient use of limited resources and appropriate routing of the data packets. The power consumption can be minimized by balancing the energy consumption between the sensor nodes and selecting the minimum power consumption route for the data packets. Clustering is one of the most effective technique that not only uniformly distributes the energy among all the sensor nodes but also play a vital role in the designing of routing protocols. So based on these advantages, a low power consumption routing protocol is proposed that makes use of fuzzy c-means++ algorithm. The proposed approach minimizes the power consumption of the sensor network by the excellent management of the WSN and also raises the lifespan. The simulation result illustrates the effectiveness of the proposed routing method when compared with the recently developed protocols based on k-means and fuzzy c-means algorithms.																	1064-1246	1875-8967					2020	38	5					6561	6570		10.3233/JIFS-179736													
J								Krill herd based TSP approach for mobile sink path optimization in large scale wireless sensor networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wireless sensor networks; data aggregation; clustering; travelling salesman problem; krill herd optimization		In wireless sensor networks (WSN), the establishment of large-scale sensor networks has always needed attention. One of the many challenges is to set up an architecture that is different from the rest and find mechanisms that can efficiently scale up with the growing number of nodes that may be essential to ensure sufficient coverage of large areas under study. Concurrently, these new architectures and mechanisms are supposed to maintain low power consumption per node to comply with energy guaranty acceptable network lifetime. The researchers utilized numerous Data collection techniques for the prompt data aggregation, yet still those outcomes the node with path failures. To solve this issue, the mobile sink is being extensively used for data aggregation in large scale wireless sensor networks (WSNs). This technique avoids imbalances in energy consumption due to multi-hop transmission but might lead to extended delay time. In this paper, our focus is on shortening the length of the mobile sink's travelling path to reduce the delay time during data gathering in large scale WSN. To achieve this, the mobile sink visits the cluster heads in an optimized path instead of sensors one by one. Here Hierarchical clusters are efficiently formed by modified K- means with outlier elimination and node proximity and residual energy based second level clustering algorithm. Next, we determine the optimal path for the mobile sink by formulating KH based Travelling Salesman Problem solving optimization algorithm. This technique proposed reduces not only the length of the path travelled by the mobile sink but also lessens the computational effort that is required for travelling-path planning and enhances the lifetime of nodes. And to ensure aggregation accuracy in cluster heads iterative filtering is implemented. Our experimental results show the proposed algorithm shortens the tour length by 40-60 percent compared to Bacterial foraging optimization-based TSP algorithm. Also delivers better results compared to other's in terms of the computational effort, time, energy use, and enhances the network lifetime.																	1064-1246	1875-8967					2020	38	5					6571	6581		10.3233/JIFS-179737													
J								A flattened architecture for distributed mobility management in IPv6 networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										IP networks; distributed access point; mobile agents; routing; integrity; authentication	PROTOCOL; SECURE; UPDATE; ROUTE	The rapid increase in internet usage for the past few decades has steered to higher demand and for adequate support for the network mobility in heterogeneous networks. However, in the existing mobile IPv6 (Internet Protocol version 6) protocols such as traditional, hierarchical, proxy and related methodologies have been stated to manage the recurrent mobility of the devices in a network with the centralized feature. However, the single point of failure, route optimization, handoff latency and security threats are highly exposed when the number of mobile device increases in the centralized approach. Also, it leads to the limitation of the size of binding information when the mobile host needs to update its place. Hence, this paper suggests a secure and optimized architecture by distributing mobility functions as distributed access points. Also, the paper addresses the prevention measures for security attacks such as false binding message, rerun and hijack. The proposed scheme is simulated and validated using network simulator and security model verifier - AVISPA. Finally, the numerical and experimental outcomes demonstrate that the proposed scheme offers a substantial diminution in the cost of the binding update, binding refresh, and packet delivery.																	1064-1246	1875-8967					2020	38	5					6583	6593		10.3233/JIFS-179738													
J								OUTFIT - An optimal data storage hosting model using Sugeno-type fuzzy inference system for multi-cloud environments	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CSP selection; QoS Attributes; Sugeno Inference system; Iaas storage selection; Cloud computing		The unprecedented growth in personal, business and research data motivates users to lease storage from multiple cloud storage providers like Amazon, Azure, etc. Selection of cost-effective cloud storage service by considering different pricing policies along with their performance characteristics is a challenging task. This research proposes a model named as OUTFIT (Optimal sUgeno Type Fuzzy Inference sysTem) an optimal data storage hosting model by suggesting an appropriate storage type based on user demands. In the first phase, we have surveyed Amazon, Google Cloud, Azure and Rackspace cloud storage providers and consolidated the different cloud storage types supported by them. In the second phase the cloud service providers are ranked by using Sugeno fuzzy inference system based on the user preference. The third phase designates the appropriate service that incurs minimal estimated storage usage cost. The proposed approach is able to categorize various cloud service providers with an optimal grading process by including multiple decision criteria for fine-grained storage type selection. The observed results prove it to be a more favourable selection tool in comparison with its counterpart tools like Cloudorado, RightCloudz in terms of cost.																	1064-1246	1875-8967					2020	38	5					6595	6605		10.3233/JIFS-179739													
J								A log-periodic spiral antenna array for L-band radio interferometric imaging	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Radio telescope; interferometry; synthesis array; log-periodic antenna array; antenna array configuration	INTEGRATION; VISIBILITY; DESIGN	The perfect Y antenna array configuration is among the most prevalent antenna array arrangement used in radio interferometry for synthesis imaging. It is crucial to determine an antenna array configuration that could offer further higher quality radio-images. In this paper, a novel and an efficient L-band log-periodic spiral antenna array design is presented. The radio-imaging performance of the log-periodic spiral antenna array is compared and shown to outperform an equivalent perfect Y antenna array. Radio imaging performance is evaluated using the computational simulation for the proposed L-band log-periodic spiral antenna array and the equivalent perfect Y antenna array. The metric used for evaluation is the Structural Similarity Index (SSIM) and Surface Brightness Sensitivity (SBS). The L-band log-periodic spiral antenna arraywas observed to have about five times higher bandwidth, 2.24 times greater sensitivity, angular resolution better by a factor of five, and 10% wider field of view than the perfect Y configuration antenna array of comparable extent. It has been analytically demonstrated that the log-periodic spiral antenna array is an optimum configuration based on Chow's optimization technique. The L-band log-periodic spiral antenna array has outperformed the perfect Y configuration in many different imaging aspects.																	1064-1246	1875-8967					2020	38	5					6607	6618		10.3233/JIFS-179740													
J								Intelligent autonomous navigation system for UAV in randomly changing environmental conditions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Localization; agent; robotic; control		The safe and reliable navigation of such autonomous systems as unmanned aerial vehicles (UAV) is a complex open problem in robotics, where a robotic system must simultaneously do many tasks of perception, control and localization. This task is especially complicated when working in an uncontrolled, unpredictable environment, for example, on city streets, in wooded areas, etc. In these cases, the autonomous agent must not only be guided to avoid collisions, but also interact safely with other agents in the environment. The developed system allows navigation of unmanned aerial vehicles in difficult environmental conditions. The results of training and the operation of the autonomous navigation system in the forest are presented. The system finds and follows the paths that are fairly difficult to distinguish. The results of field experiments are presented. Presentation of the model is presented on the youtube.com channel.																	1064-1246	1875-8967					2020	38	5					6619	6625		10.3233/JIFS-179741													
J								Application of machine learning technique for prediction of road accidents in Haryana-A novel approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Accident prediction model; linear regression; road safety; accidents		Over the last few years, road accidents in developing countries are increasing at an alarming rate. In India, almost 3% of GDP is getting wasted in road accidents, which not only cause social problems but, also, imposes a huge burden on the Indian economy. Various researches have been done to analyze this situation using different methods and techniques on different stretches and intersections. This paper makes one of the first attempts to develop an Accident Prediction Model (APM) in the Indian State of Haryana. This study describes the procedure for collection and analysis of accident data, as well as the detailed methodology used to develop APMs. The Models were developed using one of the most common algorithms of machine learning i.e. linear regression technique. Results obtained from APM of Haryana State were compared with the results given by some of the highly successful APMs like Smeed's Model, Valli's Model and their comparisons were discussed to find the most efficient model. It was observed that the proposed model shows highly accurate results in predicting road accidents in Haryana. The output of this work can be used for theoretical as well as practical applications like road safety management for improving existing conditions of the road network in Haryana and to regulate new traffic safety policies in the future.																	1064-1246	1875-8967					2020	38	5					6627	6636		10.3233/JIFS-179742													
J								Knowledge discovery in medical and biological datasets by integration of Relief-F and correlation feature selection techniques	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine learning; relief-F; correlation feature selection; classification; naive bayes	MICROARRAY DATA; GENE SELECTION; CLASSIFICATION; INFORMATION	Feature selection is a pre-processing method that identifies the significant features from high-dimensional data and able to diminish the computational cost of the learning algorithm because of removing the irrelevant and redundant features. It has traditionally been applied in a wide range of problems that include biological data processing, pattern recognition, and computer vision. The aim of this paper is to identify the best feature subsets from the benchmark datasets which improve the performance of the classifiers. Existing filter-based feature selection approaches fail to choose the relevant features from the original feature sets. To obtain the tiny subset of relevant features, we have introduced a novel filter-based feature selection method, called ReCFS. The proposed method is a combination of both feature-feature correlation and nearest neighbor weighted features to find an optimal subset of features to minimize correlation among features. The effectiveness of the selected feature subset by proposed method is evaluated by using two classifiers such as Naive Bayes and K-Nearest Neighbour on real-life datasets. For the diverse performance measurements, the experiments are conducted on eight real-life datasets of varied dimensionality and number of instances. The result demonstrates that the proposed method has found promising feature subsets which improved the classification accuracy over competing feature selection methods																	1064-1246	1875-8967					2020	38	5					6637	6648		10.3233/JIFS-179743													
J								An orthogonal moth flame optimization for global optimization and application to model order reduction problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Meta-heuristic; optimization; MFO; orthogonality; model order reduction	GENETIC ALGORITHM; SYSTEMS	Moth-Flame optimization is a meta-heuristic algorithm based on the navigation behaviour of moths. Generally, moth's poses a very effective mechanism called transverse orientation while moving a long distance in night and maintain of fixed angle with respect to the moon. MFO suffers with local optima and stagnation problem, in order to improve the performance and exploration rate of the existing algorithm and for solving the complex real world problems, a new version of MFO algorithms is proposed by adding the concept of orthogonality feature. The modified algorithm is termed as orthogonal Moth-Flame optimization (OMFO) algorithm. The main objective of this OMFO is going to solve the convergence problem to minimization of the search space and avoid the local optima. The proposed method can also be used to maintain the balance between exploration and exploitation. In this work, a set of 28 standard IEEE CEC 2017 benchmark test functions with 10 and 30 dimensions are used to evaluate and compare between the obtained results which prove that the proposed OMFO gives very promising and competitive performance as well as achieve better performance over original MFO algorithm with high stability over searching method. The efficiency of the proposed method is verified by applying in model order reduction problem. The performed analysis such as statistical measure, convergence analysis and complexity measure reveal that the proposed method is reliable and efficient in solving practical optimization problems.																	1064-1246	1875-8967					2020	38	5					6649	6661		10.3233/JIFS-179744													
J								Link prediction model based on geodesic distance measure using various machine learning classification models	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Link prediction; geodesic distance; classification model; complex network; data mining	COMPLEX NETWORKS	Link prediction tremendously gained interest in the field of machine learning and data mining due to its real world applicability on various fields such as in social network analysis, biomedicine, e-commerce, scientific community, etc. Several link prediction methods have been developed which mainly focuses on the topological features of the network structure, to figure out the link prediction problem. Here, the main aim of this paper is to perform feature extraction from the given real time complex network using subgraph extraction technique and labeling of the vertices in the subgraph according to the distance from the vertex associated with each target link. This proposed model helps to learn the topological pattern from the extracted subgraph without using the topological properties of each vertex. The Geodesic distance measure is used in labeling of the vertices in the subgraph. The feature extraction is carried out with different size of the subgraph as K = 10 and K = 15. Then the features are fit to different machine learning classification model. For the evaluation purpose, area under the ROC curve (AUC) metric is used. Further, comparative analysis of the existing link prediction methods is performed to have a clear picture of their variability in the performance of each network. Later, the experimental results obtained from different machine learning classifiers based on AUC metric have been presented. From the analysis, we can conclude that AdaBoost, Adaptive Logistic Regression, Bagging and Random forest maintain great performance comparatively on all the network. Finally, comparative analysis has been carried out between some best existing methods, and four best classification models, to make visible that link prediction based on classification models works well across several varieties of complex networks and solve the link prediction problem with superior performance and with robustness.																	1064-1246	1875-8967					2020	38	5					6663	6675		10.3233/JIFS-179745													
J								Oppositional spotted hyena optimizer with mutation operator for global optimization and application in training wavelet neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Swarm intelligence; spotted hyena optimizer; opposition-based learning; mutation operator; optimization; classification; wavelet neural network (WNN)	ALGORITHM	Success behind nature inspired evolutionary metaheuristic algorithms lies in its seemly combination of operator's castoff for smooth balance between exploration and exploitation. The deficit in such combination leads to untimely convergence of an algorithm, simultaneously failed to attain global optimum by stocking in local optimum. This work represents atypical algorithm termed as OBL-MO-SHO to improve the performance of existing SHO. To deal with more intricate realistic problems and to enhance the explorative and exploitative strength of SHO, we have integrated the oppositional learning concept with mutation operator. The proposed algorithm OBL-MO-SHO (oppositional spotted hyena optimizer with mutation operator) reveals promising performance in terms of achieving global optimum and superior convergence rate which confirms its improved exploration and exploitation capability within searching region. To establish competency of proposed OBL-MO-SHO algorithm the same is appraised by means of standard functions set belongs to IEEE CEC 2017. The efficacy of said method has been proven by means of various performance metrics and the outcomes also compared with state-of-the-art algorithms. To scrutinize its uniqueness statistically, Friedman and Holms test has been performed as one non-parametric test. Additionally as an application to unravel real world intricate difficulties the said OBL-MO-SHO algorithm has been castoff to train wavelet neural network by considering datasets selected from UCI depository. The reported results unveils that the evolved OBL-MO-SHO might be one potential algorithm for enlightening different optimization difficulties effectively.																	1064-1246	1875-8967					2020	38	5					6677	6690		10.3233/JIFS-179746													
J								A new parallel galactic swarm optimization algorithm for training artificial neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										nature inspired metaheuristic; parallel computation; galactic swarm optimization; artificial neural networks		Metaheuristic algorithms are a family of algorithms that help solve NP-hard problems by providing near-optimal solutions in a reasonable amount of time. Galactic Swarm Optimization (GSO) is the state-of-the-art metaheuristic algorithm that takes inspiration from the motion of stars and galaxies under the influence of gravity. In this paper, a new scalable algorithm is proposed to help overcome the inherent sequential nature of GSO and helps the modified version of the GSO algorithm to utilize the full computing capacity of the hardware efficiently. The modified algorithm includes new features to tackle the problem of training an Artificial Neural Network. The proposed algorithm is compared with Stochastic Gradient Descent based on performance and accuracy. The algorithm's performance was evaluated based on per-CPU utilization on multiple platforms. Experimental results have shown that PGSO outperforms GSO and other competitors like PSO in a variety of challenging settings.																	1064-1246	1875-8967					2020	38	5					6691	6701		10.3233/JIFS-179747													
J								Nonlinear fractional order PID controller for tracking maximum power in photo-voltaic system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fractional order nonlinear proportional-integral derivative (FONPID); maximum power point tracking (MPPT); elitist teaching learning based optimization (ETLBO)	O MPPT ALGORITHM; POINT TRACKING; DESIGN; SIMULATION; CELL	This paper presents a fractional order nonlinear Proportional Integral Derivative (FONPID) controller to efficiently achieve the Maximum Power Point Tracking (MPPT) in Photovoltaic (PV) systems working under rapidly varying solar intensity and the temperature. In this paper, comparisons have been made among different techniques in respect of the extent of energy extracted from the photovoltaic (PV) system using MATLAB platforms. Gains of the proposed FONPID controllers are optimally tuned using a meta-heuristic based Elitist Teaching Learning Based Optimization (ETLBO) algorithm. The performance assessment of the FONPID controller is made in terms of efficiency, settling time, rise time and ripple. The ETLBO tuned FONPID controller outperforms the other controller such as PID, Nonlinear PID (NPID), Fractional order PID (FOPID) and perturb and observe (P & O) technique. Therefore, in view of the meticulous investigation it is inferred that the proposed FONPID controller is an emerging MPPT technique with highest tracking efficiency and negligible ripple.																	1064-1246	1875-8967					2020	38	5					6703	6713		10.3233/JIFS-179748													
J								An improved MVO assisted global MPPT algorithm for partially shaded PV system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Solar PV system; MPPT; multi verse optimization; improved multi verse optimization	POWER-POINT TRACKING; PHOTOVOLTAIC SYSTEM; OPTIMIZATION; PERTURB	This article proposes an improved multiverse optimization (IMVO) assisted maximum power point tacking (MPPT) algorithm for attaining maximum global power from photovoltaic system under partial shading condition. The proposed control scheme overcomes the difficulties occurring in traditional MPPT algorithms such as difficulty in attaining global maximum power under partial shading condition and incapability of handling oscillations in power at maximum power point. The algorithm is an amalgamation of IMVO and direct duty cycle control approach. The wormhole existence probability and time distance ratio are considered to be adaptive in improved MVO so as to ensure precise exploration and exploitation. In this work multi crystal solar panel, KC130GT by M/S Kyocera, is analyzed for dynamic profiles of irradiance. Traditional P&O MPPT and improved particle swarm optimization MPPT (IPSO MPPT) are also designed for comparative analysis. The suggested IMVO MPPT proves to be superior in terms of power tracking performance, average efficiency and convergence capability as compared to other designed controllers.																	1064-1246	1875-8967					2020	38	5					6715	6726		10.3233/JIFS-179749													
J								Trends in chaos and instability for understanding system complexity	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lyapunov exponent (LE); single machine infinite bus (SMIB); dynamic stability assessment (DSA)	FRACTIONAL ORDER CONTROLLER; LYAPUNOV EXPONENTS; POWER; BIFURCATION; SPECTRUM; MODEL	Engineering systems are nowadays expanding beyond expected limits and their complexity is also increasing. One of the largest integrated system is power system. Some well-designed power system experience strange situation and suffer through chaos owing to weak dynamic performance. Stability issues also haunt power system in such cases. Loss of stability investigation in power system leads to evidence of chaos as intermediate state quite often. Black swan theory tells us to be ready for unseen unruly behavior at any time. Noah and Joseph effects are also surfacing in almost every large expanding power system from different parts of world. Complexity of system component behavior and complex stability boundaries pose a threat and ready to push power system where chaos is prevalent. It is debatable to see whether chaos leads to instability. This paper closely summarizes the chaos studies in light of reported research and advocates strongly the inclusion of advancement in chaos theory for detailed investigation post disturbance. This paper deals with a comprehensive review of strange behavior of nonlinear dynamic system and relevance of such studies for future anomalous behavior in the light of complexity science applied to engineering disasters such as blackouts. It is targeted to provide direction for futuristic complexity arising due to working on the brink of instability for economic reasons and to have certain preparation before inevitable blackouts, disasters and failures. A classified list of more than 50 relevant research publications is also given for quick reference.																	1064-1246	1875-8967					2020	38	5					6727	6737		10.3233/JIFS-179750													
J								Fault-tolerant technology for big data cluster in distributed flow processing system	WEB INTELLIGENCE										Distributed flow processing system; big data; cluster fault-tolerant technology; disk data fault-tolerant mechanism		In order to overcome the problem of poor recovery and stability of traditional big data fault-tolerant technology, a fault-tolerant technology of big data cluster in distributed flow processing system is proposed. Using the target protocol to build the cache data fault-tolerant mechanism of the distributed flow processing system and the disk data fault-tolerant mechanism, so as to build the internal data fault-tolerant mechanism of the system. By using spark application framework, a fault-tolerant model of big data cluster is built to realize the fault-tolerant of big data cluster in distributed flow processing system. The experimental results show that compared with the traditional methods, the recovery rate of big data cluster fault-tolerant technology proposed in this paper is higher, the highest recovery efficiency is 99.83%, the stability is stronger, and it is more suitable for big data fault-tolerant processing of distributed flow processing system.																	2405-6456	2405-6464					2020	18	2					101	110		10.3233/WEB-200432													
J								Research on the integration of library e-book borrowing history data based on big data technology	WEB INTELLIGENCE										Big data; e-book; borrowing; historical data; integration		In order to overcome the shortcomings of the traditional method of integrating historical data of library e-book borrowing, such as high delay and low integration throughput, this paper proposes a method of integrating historical data of library e-book borrowing based on big data technology. Based on big data technology, this method builds an integrated platform for library e-book borrowing, which consists of data layer, business layer, interface layer and application layer. Through the data layer, a series of borrowing historical data generated by the borrowing integration platform are collected, and data integration module is used to clean and filter the collected borrowing historical data of e-books. The data integration layer solves the local distortion and realizes the integrated processing of borrowed historical data. Experimental results show that the proposed method has the advantages of short time delay and low integration error rate, and the highest integration throughput can reach 97%.																	2405-6456	2405-6464					2020	18	2					111	120		10.3233/WEB-200433													
J								Research on interest reading recommendation method of intelligent library based on big data technology	WEB INTELLIGENCE										Big data technology; intelligent library; interest reading recommendation; data mining		In order to overcome the problems of inaccurate recommendation results and high response delay existing in traditional recommendation methods, this paper proposes an interest reading recommendation method based on big data technology for intelligent library. Build the recommendation platform of intelligent library, design the cataloging subsystem, book category setting subsystem, new book storage subsystem and new journal storage subsystem. Through big data processing technology, users' reading interest in each subsystem is clustered and fully mined. Based on the results of reading interest mining, the user interest model is established according to the semantic topic and the updating scheme of user interest model is designed. Combined with the user's score, we recommend the reading target which accords with the user's reading interest to the user. The experimental results show that compared with the traditional recommendation method, the proposed method can achieve high-precision reading recommendation with low response delay, and the minimum delay is only 0.019.																	2405-6456	2405-6464					2020	18	2					121	131		10.3233/WEB-200434													
J								Research on distributed data stream mining algorithms based on matrix weighted association rules	WEB INTELLIGENCE										Matrix weighted association rules; data flow; mining; data dimension reduction		In order to overcome the low efficiency of traditional data mining algorithms without considering weighted association rules, this paper proposes a distributed data flow mining algorithm based on matrix weighted association rules. According to the way of separating metadata and data flow, garbage data processing in data flow is realized. By using sliding window and data summary structure to optimize PCA algorithm, the main component decision matrix is formed in the window, and the dimension of data in sliding window is reduced by using the decision matrix. The matrix weighted association rules are used to mine the distributed data. After dimensionality reduction, the transactions in the database are clustered according to the time distribution. The weighted analysis is carried out for each aggregation to obtain the weighted frequent item set with time and output the mining results. The experimental results show that the proposed algorithm has high efficiency and the highest accuracy of 98.9%.																	2405-6456	2405-6464					2020	18	2					133	143		10.3233/WEB-200435													
J								Accelerated algorithm for intelligent mining of communication data in cloud computing environment	WEB INTELLIGENCE										Cloud computing; communication data; mining; particle swarm optimization	IDENTIFICATION; TRANSMISSION	Traditional methods have some problems such as large memory occupation and slow mining speed, so an intelligent mining acceleration algorithm based on particle swarm optimization is proposed. Based on the analysis of the communication data, the features of the communication data are selected by the acceleration strategy, and multiple feature subsets of the communication data are obtained repeatedly by using the remaining attributes. Particle swarm optimization (pso) algorithm is used to select the optimal feature subset, and average classification error is used as fitness function to complete intelligent mining of communication data. The experimental results show that the memory usage of this algorithm is between 62 and 71 GB in the experimental process, which is small and the average running time is better than the traditional algorithm. The results show that the algorithm has lower memory consumption and faster mining speed.																	2405-6456	2405-6464					2020	18	2					145	153		10.3233/WEB-200436													
J								Intelligent detection method for abnormal big data in heterogeneous networks based on Bayesian classification	WEB INTELLIGENCE										Bayesian classification; heterogeneous network; abnormal big data; intelligent detection		In order to overcome the existing abnormal big data intelligent detection method, the problem of low detection accuracy and poor convergence is not carried out without abnormal big data classification. A new Bayesian classification based heterogeneous network anomaly big data intelligent detection is proposed in this paper. method. Design an abnormal big data intelligent detection architecture, use TcpDump collection tool to collect and process heterogeneous network traffic data, and build the relationship between bottleneck traffic and abnormal big data based on the processed data, through Fourier transform The method obtains the data frequency information and uses the Bayesian network classification method to realize the intelligent detection of abnormal big data in heterogeneous networks. The experimental results show that compared with the traditional method, the proposed method greatly improves the detection accuracy, convergence and anti-interference, and fully demonstrates that the proposed method has better detection effect.																	2405-6456	2405-6464					2020	18	2					155	165		10.3233/WEB-200437													
J								Redundancy evaluation method of massive heterogeneous data in Internet of Things based on attributes and relations	WEB INTELLIGENCE										Internet of Things; heterogeneous data; redundancy; evaluation		In order to overcome the inaccuracy recognition results of the current network redundancy data, a new method based on attributes and relations for evaluating the redundancy of massive heterogeneous data in the Internet of Things is proposed. This method constructs a global database system and preliminarily obtains redundant data. Using angle variance, anomaly recognition and detection of Internet of Things data are carried out. A weighted network based on attribute distance is constructed, in which attributes and relations are used to calculate comprehensive redundancy, and heterogeneous data redundancy evaluation of the Internet of Things is realized. The experimental results show that the redundancy calculation accuracy and recall rate of the results of this method are both above 95%, which is obviously higher than other methods. It proves that the proposed method has high accuracy and strong anomaly recognition performance, and the integrated database of the Internet of Things is security.																	2405-6456	2405-6464					2020	18	2					167	177		10.3233/WEB-200438													
J								Using continuous sensor data to formalize a model of in -home activity patterns	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS											ACTIVITY RECOGNITION; SMART HOME; HUMAN DYNAMICS; TIME; BEHAVIOR; USAGE																		1876-1364	1876-1372					2020	12	3					183	201		10.3233/AIS-200562													
J								Remote detection of social interactions in indoor environments through bluetooth low energy beacons	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS											LOCALIZATION TECHNIQUE																		1876-1364	1876-1372					2020	12	3					203	217		10.3233/AIS-200560													
J								Smart and intelligent network selection approach to support location -dependent and context -aware service migration	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										LDCAMS; cooperative & cost effective network selection algorithm (CACENSA); VANET deployment; VANET services; network Integration; WAVE; UMTS		Vehicular networking has gained considerable interest within the research community and industry. The automotive industry is supporting the notion of pervasive connectivity by agreeing to equip vehicles with devices required for vehicular ad hoc networking. Equipped with these devices, mobile nodes in vehicular ad hoc networks (VANETs) are capable of hosting many types of applications as services for other nodes in the network. This research focuses on addressing the challenges of location-dependence, intermittent network connectivity and irregular network traffic flows in unplanned areas for VANETs to host and operate non-safety-critical VANETs services. We assume unplanned areas as the one that lack communication infrastructure and planning. Such areas observe irregular vehicular traffic on the roads as well as on the networks. This research investigates the shortcomings of location-dependence, intermittent network connectivity and irregular network traffic flows and addresses them by exploiting location-dependent service migration over an integrated network in an efficient and cost-effective manner.																	1876-1364	1876-1372					2020	12	3					219	237		10.3233/AIS-200559													
J								Smoking recognition with smartwatch sensors in different postures and impact of user?s height	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS											PHYSICAL-ACTIVITY; ALGORITHM																		1876-1364	1876-1372					2020	12	3					239	261		10.3233/AIS-200558													
J								Predicting retail business success using urban social data mining	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS																													1876-1364	1876-1372					2020	12	3					263	277		10.3233/AIS-200561													
J								Physical processes, their life and their history	APPLIED ONTOLOGY										Physical process; physical object; event; property; fact; proposition; occurrence-maker	EVENTS; ONTOLOGY; OBJECTS	Here, I lay the foundations of a high-level ontology of particulars whose structuring principles differ radically from the 'continuant' vs. 'occurrent' distinction traditionally adopted in applied ontology. These principles are derived from a new analysis of the ontology of "occurring" or "happening" entities. Firstly, my analysis integrates recent work on the ontology of processes, which brings them closer to objects in their mode of existence and persistence by assimilating them to continuant particulars. Secondly, my analysis distinguishes clearly between processes and events, in order to make the latter abstract objects of thought (alongside propositions). Lastly, I open my ontological inventory to properties and facts, the existence of which is commonly admitted. By giving specific roles to these primitives, the framework allows one to account for static and dynamic aspects of the physical world and for the way that subjects conceive its history: facts account for the life of substances (physical objects and processes), whereas events enable cognitive subjects to account for the life story of substances.																	1570-5838	1875-8533					2020	15	2					109	133		10.3233/AO-200222													
J								Location ontologies based on mereotopological pluralism	APPLIED ONTOLOGY										Mereotopology; location; parthood; ontology verification		Location ontologies axiomatize the relationship between physical bodies and the space that they occupy, and there is a rich literature on the philosophical underpinnings of these ontologies. Existing location ontologies have given primacy to the structure of what might be called abstract space. Consequently, the spatial properties and relations between physical bodies are extracted from the spatial properties and relations of the spatial regions that they occupy. In this paper, we take a different approach by beginning with the philosophical position of mereological pluralism, in which there are different mereologies for different kinds of entities. In particular, we make the fundamental ontological commitment that the mereology for spatial regions is different than the mereology for physical bodies. Different intuitions about location and different classes of physical bodies can be formally specified by imposing different conditions on this mapping, thus leading to different possible location ontologies. We propose an axiomatization of a location ontology and characterize the models of the axiomatization, up to isomorphism, by providing a representation theorem with respect to graph and poset homomorphisms.																	1570-5838	1875-8533					2020	15	2					135	184		10.3233/AO-200224													
J								MODDALS methodology for designing layered ontology structures />	APPLIED ONTOLOGY										Layered ontology networks; methodology; ontology reusability; ontology usability; SPL engineering	PATTERNS; ONTOCAPE	Global ontologies include common vocabularies to provide interoperability among different applications. These ontologies require a balance of reusability-usability to minimise the ontology reuse effort in different applications. To achieve such a balance, reusable and usable ontology design methodologies provide guidelines to design and develop layered ontology networks. Layered ontology networks classify into different abstraction layers the domain knowledge relevant to many applications (common domain knowledge) and the domain knowledge relevant only to certain application types (variant domain knowledge). This knowledge classification is performed from scratch by domain experts and ontology engineers. This process is a heavy workload, making it difficult to design the layered structures of reusable and usable global ontologies. Considering how common and variant software features are classified when designing Software Product Lines (SPLs), we argue that SPL engineering techniques can facilitate the domain knowledge classification taking as reference existing ontologies. This paper presents a methodology that provides guidelines to design the layered structure of reusable and usable ontology networks called MODDALS. In contrast to previous methods, MODDALS applies SPL engineering techniques to systematically (1) identify the ontology common and variant domain knowledge and (2) classify it into different abstraction layers taking as reference existing ontologies. This approach complements domain experts' and ontology engineers' expertise, preventing them from classifying the domain knowledge from scratch facilitating the design of the layered ontology structure. MODDALS methodology is evaluated in the design of the layered structure of a reusable and usable global ontology for the energy domain. The results show that MODDALS enables to classify the domain knowledge taking as reference existing ontologies.																	1570-5838	1875-8533					2020	15	2					185	217		10.3233/AO-200225													
J								Aging Neuro-Behavior Ontology	APPLIED ONTOLOGY										Elderly; neuro-psychology; ontology engineering; ambient intelligence	MILD COGNITIVE IMPAIRMENT; ADULT LIFE-SPAN; GENE ONTOLOGY; MEMORY; INFORMATION; LONG	It is known that the aging process entails a cognitive decline in certain processes such as attention, episodic memory, working memory, processing speed and executive functions. In recent years, efforts have been made to investigate the potential of Information and Communication Technologies to improve cognitive functioning and quality of life in older adults with and without cognitive impairments. In this paper, we propose the Aging Neuro-Behaviour Ontology (ANBO), a formal model of cognitive processes involved in day-to-day living and whose performance usually decline with age. ANBO has been created with the aim of being an aid in developing tools for cognitive rehabilitation by means of integrating aging-related behaviors with monitoring activities of daily living. As an example of these tools, we introduce an integration of ANBO with the Ontology SmartLab Elderly (OSLE), an ontology related with Telehealth Smart Homes wherein activities of daily living are recorded. This ANBO and OSLE integration enable the interpretation of these activities as the result of cognitive processes of interest in the domain of elderly decline.																	1570-5838	1875-8533					2020	15	2					219	239		10.3233/AO-200229													
J								Distributed frequent subgraph mining on evolving graph using SPARK	INTELLIGENT DATA ANALYSIS											ALGORITHM; PATTERNS																		1088-467X	1571-4128					2020	24	3					495	513		10.3233/IDA-194601													
J								Online analytical processsing on graph data	INTELLIGENT DATA ANALYSIS											OLAP																		1088-467X	1571-4128					2020	24	3					515	541		10.3233/IDA-194576													
J								Unsupervised learning of textual pattern based on Propagation in Bipartite Graph	INTELLIGENT DATA ANALYSIS											CLASSIFICATION																		1088-467X	1571-4128					2020	24	3					543	565		10.3233/IDA-194528													
J								Estimating a one -class naive Bayes text classifier	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					567	579		10.3233/IDA-194669													
J								An efficient and robust bat algorithm with fusion of opposition -based learning and whale optimization algorithm	INTELLIGENT DATA ANALYSIS											PARTICLE SWARM OPTIMIZATION; SUPPORTING SELECTIVE UNDO; ANT COLONY OPTIMIZATION; ARTIFICIAL BEE COLONY; GLOBAL OPTIMIZATION; CLASSIFICATION; DESIGN; OPERATIONS																		1088-467X	1571-4128					2020	24	3					581	606		10.3233/IDA-194641													
J								An active learning ensemble method for regression tasks	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					607	623		10.3233/IDA-194608													
J								An improvement of SAX representation for time series by using complexity invariance	INTELLIGENT DATA ANALYSIS											SYMBOLIC AGGREGATE APPROXIMATION																		1088-467X	1571-4128					2020	24	3					625	641		10.3233/IDA-194574													
J								Extraction of qualitative behavior rules for industrial processes from reduced concept lattice	INTELLIGENT DATA ANALYSIS											FORMAL CONCEPT ANALYSIS; COLD-ROLLING PROCESS; ATTRIBUTE REDUCTION THEORY; KNOWLEDGE REDUCTION; NEURAL-NETWORKS; REPRESENTATION; SELECTION; FCANN; ANN; CLASSIFICATION																		1088-467X	1571-4128					2020	24	3					643	663		10.3233/IDA-194569													
J								Analyzing concept drift: A case study in the financial sector	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					665	688		10.3233/IDA-194515													
J								Bayesian approach to linear state space model with unknown measurement matrix	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					689	704		10.3233/IDA-194624													
J								A semantic -aware collaborative filtering recommendation method for emergency plans in response to meteorological hazards	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					705	721		10.3233/IDA-194571													
J								User -item content awareness in matrix factorization based collaborative recommender systems	INTELLIGENT DATA ANALYSIS																													1088-467X	1571-4128					2020	24	3					723	739		10.3233/IDA-194599													
J								Large deviations for empirical measures of dense stochastic block graphs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Large deviations; Stochastic block model; Information systems	COMMUNITY STRUCTURE	Stochastic block model has been proved to be the most successful tool for detecting community structure in various networks as well as synthesizing networks with different communities. In this paper, we discuss dense stochastic block model, and suppose that every vertex's block membership is randomly taken from a finite set independently according to a fixed law. To capture the asymptotic behavior of the stochastic block graphs when the number of vertices tends to infinity, we first define the empirical pair measure for any dense stochastic block graph, exploring the number of edges connecting any given pair of blocks. Then we put forward the empirical block measure, which computes the number of vertices with given block membership. Our concern here is the large deviation principles for these crucial empirical measures in the corresponding weak topology, we first derive the large deviation principle for the empirical pair measure under the empirical block measure, then through a mixing approach, we obtain the joint large deviation principle for these two empirical measures.																	1868-5137	1868-5145															10.1007/s12652-019-01666-8		JAN 2020											
J								Recognition and classification of damaged fingerprint based on deep learning fuzzy theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Damaged fingerprint recognition; deep learning; OPTA algorithm; deep convolutional neural network; softmax classifier	CONVOLUTIONAL NEURAL-NETWORK	With the development of technology, fingerprint identification has become an effective means of personal identification, and has been widely used in the fields of public security, custom, banking, network security and other areas requiring identification. Nowadays, many effective methods have been proposed for fingerprint identification, but these methods are not effective in identifying damaged fingerprints, and the correct recognition rate is low. In order to effectively solve the problem of identification and classification of damaged fingerprints, this paper proposes a method for classification of broken fingerprints based on deep learning fuzzy theory. Firstly, after pre-processing the fingerprint, using the bifurcation point and the endpoint in the broken fingerprint image as the minutiae, the feature extraction ability of the deep convolutional neural network is utilized to extract the feature of the damaged fingerprint minutiae. Secondly, the fuzzy rough set is used to reduce the feature. Finally, using the reduced feature uses the Softmax classifier to classify the damaged fingerprint image. The simulation results show that, after preprocessing the damaged fingerprint image, using OPTA algorithm to refine the damaged fingerprint image, the features of the fingerprint image can be extracted effectively by deep convolutional neural network, and then the classification accuracy can be improved by using Softmax classifier to reduce the features.																	1064-1246	1875-8967					2020	38	4			SI		3529	3537		10.3233/JIFS-179575													
S								Music Genre Classification Using Stacked Auto-Encoders	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											In this paper, we propose an architecture based on a stacked auto-encoder (SAE) for the classification of music genre. Each level in the stacked architecture works by stacking some hidden representations resulting from the previous level and related to different frames of the input signal. In this way, the proposed architecture shows a more robust classification compared to a standard SAE. The input to the first level of the SAE is fed by a set of 57 peculiar features extracted from the music signals. Some experimental results show the effectiveness of the proposed approach with respect to other state-of-the-art methods. In particular, the proposed architecture is compared to the support vector machine (SVM), multi-layer perceptron (MLP) and logistic regression (LR).																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						11	19		10.1007/978-981-13-8950-4_2	10.1007/978-981-13-8950-4												
S								Linear Artificial Forces for Human Dynamics in Complex Contexts	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											In complex contexts, people need to adapt their behavior to interact with the surrounding environment to reach the intended destination or avert collisions. Motion dynamics should therefore include both social and kinematic rules. The proposed analysis aims at defining a linear dynamic model to predict future positions of different types of agents, namely pedestrians and cyclists, observing a limited number of frames. The dynamics are defined in terms of artificial potentials fields (APFs) obtained by static (e.g., walls, doors or benches) and dynamic (e.g, other agents) elements to produce attractive and repulsive forces that influence the motion. A linear combination of such forces affects the resulting behavior. We exploit the context using a semantic scene segmentation to derive static forces while the interactions between agents are defined in terms of their reciprocal physical distances. We conduct experiments both on synthetic and on subsets of publicly available datasets to corroborate the proposed model.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						21	33		10.1007/978-981-13-8950-4_3	10.1007/978-981-13-8950-4												
S								Convolutional Recurrent Neural Networks and Acoustic Data Augmentation for Snore Detection	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										SLEEP; CLASSIFICATION	In this paper, we propose an algorithm for snoring sounds detection based on convolutional recurrent neural networks (CRNN). The log Mel energy spectrum of the audio signal is extracted from overnight recordings and is used as input to the CRNN with the aim to detect the precise onset and offset time of the sound events. The dataset used in the experiments is highly imbalanced toward the non-snore class. A data augmentation technique is introduced, that consists in generating new snore examples by simulating the target acoustic scenario. The application of CRNN with the acoustic data augmentation constitutes the main contribution of the work in the snore detection scenario. The performance of the algorithm has been assessed on the A3-Snore corpus, a dataset which consists of more than seven hours of recordings of two snorers and consistent environmental noise. Experimental results, expressed in terms of Average Precision (AP), show that the combination of CRNN and data augmentation in the raw data domain is effective, obtaining an AP up to 94.92%, giving superior results within the related literature.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						35	46		10.1007/978-981-13-8950-4_4	10.1007/978-981-13-8950-4												
S								Italian Text Categorization with Lemmatization and Support Vector Machines	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											The paper describes an Italian language text categorizer by Lemmatization and support vector machines. The categorizer is composed of six modules. The first module performs the tokenization, removing the punctuation signs; the second and third ones carry out stopping and lemmatization, respectively; the fourth module implements the bag-of-words approach; the fifth one performs feature dimensionality reduction eliminating poor discriminant features; finally, the last module does the classification. The Italian text categorizer has been validated on a database composed of more than 1100 articles, extracted from online edition of three Italian language newspapers, belonging to eight different categories. Thework is highly novel, since to the best our knowledge, there are no works in literature on Italian text categorization.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						47	54		10.1007/978-981-13-8950-4_5	10.1007/978-981-13-8950-4												
S								SOM-Based Analysis of Volcanic Rocks: An Application to Somma-Vesuvius and Campi Flegrei Volcanoes (Italy)	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											Algorithms based on artificial intelligence (AI) have had a strong development in recent years in different research fields of earth science such as seismology and volcanology. In particular, they have been applied to the study of the volcanic eruptive products of the recent activity of Mount Etna volcano. This work presents an application of the self-organizing map (SOM) neural networks to perform a clustering analysis on petrographic patterns of rocks of Somma-Vesuvius and Campi Flegrei volcanoes, in the Neapolitan area. The goal is to highlight possible affinity between the magmatic reservoirs of these two volcanic complexes. The SOM is known for its ability to cluster data by using intrinsic similarity measures without any previous information about their distribution. Moreover, it allows an easy understandable data visualization by using a two-dimensional map. The SOM has been tested on a geochemical dataset of 271 samples, consisting of 134 samples of Campi Flegrei eruptions (named CF), 24 samples of Somma-Vesuvius effusive eruptions (VF), 73 samples of Somma-Vesuvius explosive eruptions (VX), and finally 40 samples of "foreign" eruptions (ET), included to verify the neural net classification capability. After a preprocessing phase, applied to have a more appropriate data representation as input for the SOM, each sample has been encoded through a vector of 23 features, containing information about major bulk components, trace elements, and Sr isotopic ratio. The resulting SOM identifies three main clusters, and in particular, the foreign patterns (ET) are well separated from the other ones being mainly grouped in a single node. In conclusion, the obtained results suggest the ability of SOM neural network to associate volcanic rock suites on the basis of their geochemical imprint and can be consistent with the hypothesis that there might be a common magma source beneath the whole Neapolitan area.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						55	60		10.1007/978-981-13-8950-4_6	10.1007/978-981-13-8950-4												
S								Toward an Automatic Classification of SEM Images of Nanomaterials via a Deep Learning Approach	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										NANOFIBERS	Nanofibrous materials produced by electrospinning process may exhibit characteristic localized defects and anomalies (i.e., beads, speck of dust) that make the nanostructure a network of nonhomogeneous nanofibers, unsuitable for industrial production at large scale of the nanoproducts. Therefore, monitoring and controlling the quality of nanomaterials production has become increasingly important and intelligent anomalies detection systems have been emerging. In this study, we propose an innovative framework based on machine (deep) learning for automatic anomaly detection. Specifically, a deep convolutional neural network (CNN) is proposed to automatically classify scanning electronmicroscope (SEM) images of homogeneous (HNF) and nonhomogeneous nanofibers (NHNF), interpreted as two different categories. The proposed approach has been validated on experimental SEM images acquired through SEM images analyzer on polyvinylacetate (PVAc) nanofibers produced by electrospinning process. Experimental results showed that the designed deepCNNachieved an accuracy rate up to 80% and average precision, recall, F_score of, 78.5, 79, and 78.5%, respectively. These promising results encourage the use of this effective technique in industrial production.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						61	72		10.1007/978-981-13-8950-4_7	10.1007/978-981-13-8950-4												
S								An Effective Fuzzy Recommender System for Fund-raising Management	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										MONEY; TIME; SIMILARITY	In the social economics field that deals with the nonprofit organizations (NPOs), the fund-raising is a crucial activity that requires the management of a great number of quantitative and qualitative information regarding Donors and Contacts (i.e., potential donors). This data is normally stored in a structured database (DB) by each NPO, and it is clear that their effective processing by data science methods significantly improves the performances of the fund-raising campaigns. For this reason, the use of rigorous mathematical methods and decision support systems (DSS) has been playing a very important role in this context. The process of fund-raising is very complex and in part different depending on the characteristics of each organization. However, a common important feature is the role of the Contacts, and therefore, the method for turning the Contacts into actual Donors contextualized in the so-called giving pyramid is crucial from a strategic point of view. Recently, a recommender system (RS) has been proposed to optimize the Contacts' management, by computing the similarity of each Contact with respect to the Donors. In this contribution, we enhance and complete this model by considering both a large DB and two significant extensions of the model, obtaining in this way an effective and whole fuzzy RS. With respect to the DB, the availability of information is effectively exploited. As for the algorithm, a proper similarity measure is defined, based on the specificity of the context. Moreover, a complete estimation of the Contacts' characteristics is taken into account, by considering not only the frequency but the averaged amount of the gift as well, in the context of a nonparametric approach. The experimental results show the effectiveness of the proposed system.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						73	82		10.1007/978-981-13-8950-4_8	10.1007/978-981-13-8950-4												
S								Reconstruction, Optimization and Quality Check of Microsoft HoloLens-Acquired 3D Point Clouds	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											In the context of three-dimensional acquisition and elaboration, it is essential tomaintain a balanced approach between model accuracy and required resources. As a possible solution to this problem, the present paper proposes a method to obtain accurate and lightweight meshes of a real environment using the Microsoft (R) HoloLensT as a device for point clouds acquisition. Firstly, we describe an empirical procedure to improve 3D models, with the use of optimal parameters found by means of a genetic algorithm. Then, a systematic review of the indexes for evaluating the quality of meshes is developed, in order to quantify and compare the goodness of the obtained outputs. Finally, in order to check the quality of the proposed approach, a reconstructed model is tested in a virtual scenario implemented in Unity (R) 3D Game Engine.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						83	93		10.1007/978-981-13-8950-4_9	10.1007/978-981-13-8950-4												
S								The "Probabilistic Rand Index": A Look from Some Different Perspectives	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										FUZZY	One crucial tool in machine learning is a measure of partition similarity. This study focuses on the "probabilistic Rand index", a variant of the Rand index. We look at this measure from different perspectives: probabilistic, information-theoretic, and diversity-theoretic. These give some insight, reveal relationships with other types of measures, and suggest some possible alternative interpretations.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						95	105		10.1007/978-981-13-8950-4_10	10.1007/978-981-13-8950-4												
S								Dimension Reduction Techniques in a Brain-Computer Interface Application	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										P300; ENSEMBLE; PEOPLE; ICA	Electroencephalography (EEG)-based Brain-computer interface (BCI) technology allows a user to control an external device without muscle intervention through recorded neural activity. Ongoing research on BCI systems includes applications in the medical field to assist subjects with impaired motor functionality (e.g., for the control of prosthetic devices). In this context, the accuracy and efficiency of a BCI system are of paramount importance. Comparing four different dimension reduction techniques in combination with linear and nonlinear classifiers, we show that integrating thesemethods in a BCI system results in a reduced model complexity without affecting overall accuracy.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						107	118		10.1007/978-981-13-8950-4_11	10.1007/978-981-13-8950-4												
S								Blind Source Separation Using Dictionary Learning in Wireless Sensor Network Scenario	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										ALGORITHMS	The aim of this paper is to introduce a block-wise approach with adaptive dictionary learning for solving a determined blind source separation problem. A potential real-case scenario is illustrated in the context of a stationary wireless sensor network in which a set of sensor nodes transmits data to a multi-receiver node (sink). The method has been designed as a multi-step approach: the estimation of the mixing matrix, the separation of the sources by sparse coding and the source reconstruction. A sparse mixture from the original signals is used for estimating the mixing matrix, and later on, a sparse coding approach is used for separating the block-wise sources which are finally reconstructed by means of a dictionary. The proposed model is based on a block-wise approach which has the advantage of considerably improving the computational efficiency of the signal recovery process without particularly degrading the separation performance. Some experimental results are provided for comparing the computational and separation performances of the proposed system by varying the type of dictionary used, whether it is fixed or learned from the data.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						119	131		10.1007/978-981-13-8950-4_12	10.1007/978-981-13-8950-4												
S								A Comparison of Apache Spark Supervised Machine Learning Algorithms for DNA Splicing Site Prediction	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										EXTRACTION	Thanks to next-generation sequencing techniques, a very big amount of genomic data are available. Therefore, in the last years, biomedical databases are growing more and more. Analyzing this big amount of data with bioinformatics and big data techniques could lead to the discovery of new knowledge for the treatment of serious diseases. In this work, we deal with the splicing site prediction problem in DNA sequences by using supervised machine learning algorithms included in the MLlib library of Apache Spark, a fast and general engine for big data processing. We show the implementation details and the performance of those algorithms on two public available datasets adopting both local and cloud environments, emphasizing the importance of this last environment for its scalability and elasticity of use. We compare the performance of the algorithms with U-BRAIN, a general-purpose learning algorithm originally designed for the prediction of DNA splicing sites. Results show that, among the Spark algorithms, all have good prediction accuracy (>0.9)-that is comparable with the one of U-BRAIN-and much lower execution time. Therefore, we can state that Apache Spark machine learning algorithms are promising candidates for dealing with the DNA splicing site prediction problem.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						133	143		10.1007/978-981-13-8950-4_13	10.1007/978-981-13-8950-4												
S								Recurrent ANNs for Failure Predictions on Large Datasets of Italian SMEs	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											The prediction of failure of a firm is a challenging topic in business research. In this paper, we consider a machine learning approach to detect the state of asset shortfall in the Italian small and medium-sized enterprises' context. More precisely, we use the recurrent neural networks to predict the insolvency of firms. The huge dataset we study allows us to overcome problems of distortions given by smaller sample sizes. The observed sample comes from AIDA database, and consider thirty variables replicated for five years. The main result is that recurrent neural networks outperform the multi-layer perceptron architecture used as benchmark. The obtained accuracy scores are in line with those found in the literature, and this suggests that the use of new techniques such as those tried out in this study could produce even better results.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						145	155		10.1007/978-981-13-8950-4_14	10.1007/978-981-13-8950-4												
S								Inverse Classification for Military Decision Support Systems	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											We propose in this paper a military application, which can be used in civil contexts as well, for solving inverse classification problems. Pattern recognition and decision support systems are typical tools through which inverse classification problems can be solved in order to achieve the desired goals. As standard classifiers do notwork properly for inverse classification, which is an inherent ill-posed problem and therefore difficult to be inverted, we propose a new approach that exploits all the information associated with the decisions observed in the past. The experimental results prove the feasibility of the proposed algorithm, with errors lower than 10% with respect to standard classification models.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						157	166		10.1007/978-981-13-8950-4_15	10.1007/978-981-13-8950-4												
S								Simultaneous Learning of Fuzzy Sets	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										INDUCTION	We extend a procedure based on support vector clustering and devoted to inferring the membership function of a fuzzy set to the case of a universe of discourse over which several fuzzy sets are defined. The extended approach learns simultaneously these sets without requiring as previous knowledge either their number or labels approximating membership values. This data-driven approach is completed via expert knowledge incorporation in the form of predefined shapes for the membership functions. The procedure is successfully tested on a benchmark.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						167	175		10.1007/978-981-13-8950-4_16	10.1007/978-981-13-8950-4												
S								Trees in the Real Field	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											This paper proposes an algebraic view of trees which opens the doors to an alternative computational scheme with respect to classic algorithms. In particular, it is shown that this view is very well-suited for machine learning and computational linguistics.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						177	187		10.1007/978-981-13-8950-4_17	10.1007/978-981-13-8950-4												
S								Graded Possibilistic Meta Clustering	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											Meta clustering starts from different clusterings of the same data and aims to group them, reducing the complexity of the choice of the best partitioning and the number of alternatives to compare. Starting from a collection of single feature clusterings, a graded possibilistic medoid meta clustering algorithm is proposed in this paper, exploiting the soft transition from probabilistic to possibilistic memberships in a way that produces more compact and separated clusters with respect to other medoid-based algorithms. The performance of the algorithm has been evaluated on six publicly available data sets over three medoid-based competitors, yielding promising results.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						189	199		10.1007/978-981-13-8950-4_18	10.1007/978-981-13-8950-4												
S								Probing a Deep Neural Network	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											We report a number of experiments on a deep convolutional network in order to gain a better understanding of the transformations that emerge from learning at the various layers. We analyze the backward flow and the reconstructed images, using an adaptive masking approach in which pooling and nonlinearities at the various layers are represented by data-dependent binary masks. We focus on the field of view of specific neurons, also using random parameters, in order to understand the nature of the information that flows through the activation's "holes" that emerge in the multi-layer structure when an image is presented at the input. We show how the peculiarity of the multi-layer structure is not so much in the learned parameters, but in the patterns of connectivity that are partly imposed and then learned. Furthermore, a deep network appears to focus more on statistics, such as gradient-like transformations, rather than on filters matched to image patterns. Our probes seem to explain why classical image processing algorithms, such as the famous SIFT, have provided robust, although limited, solutions to image recognition tasks.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						201	211		10.1007/978-981-13-8950-4_19	10.1007/978-981-13-8950-4												
S								Neural Epistemology in Dynamical System Learning	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies									Epistemology; Time series learning; Feed-forward neural networks; Machine learning; Forecasting; Poincare recurrence theorem; Bifurcation theory		In the last few years, neural networks are effectively applied in different fields. However, the application of empirical-like algorithms as feed-forward neural networks is not always justified from an epistemological point of view [1]. In this work, the assumptions for the appropriate application of machine learning empirical-like algorithms to dynamical system learning are investigated from a theoretical perspective. A very simple example shows how the suggested analyses are crucial in corroborating or discrediting machine learning outcomes.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						213	221		10.1007/978-981-13-8950-4_20	10.1007/978-981-13-8950-4												
S								Assessing Discriminating Capability of Geometrical Descriptors for 3D Face Recognition by Using the GH-EXIN Neural Network	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											In pattern recognition, neural networks can be used not only for the classification task, but also for feature selection and other intermediate steps. This paper addresses the 3D face recognition problem in order to select the most meaningful geometric descriptors. At this aim, the classification results are directly integrated in a biclustering process in order to select the best leaves of a neural hierarchical tree. This tree is created by a novel neural network GH-EXIN. This approach results in a new criterion for the feature selection. This technique is applied to a database of face expressions where both traditional and novel geometric descriptors are used. The results state the importance of the curvedness novel descriptors and only of a few Euclidean distances.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						223	233		10.1007/978-981-13-8950-4_21	10.1007/978-981-13-8950-4												
S								Growing Curvilinear Component Analysis (GCCA) for Stator Fault Detection in Induction Machines	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											Fault diagnostics for electrical machines is a very difficult task because of the non-stationarity of the input information. Also, it is mandatory to recognize the pre-fault condition in order not to damage the machine. Only techniques like the principal component analysis (PCA) and its neural variants are used at this purpose, because of their simplicity and speed. However, they are limited by the fact they are linear. The GCCA neural network addresses this problem; it is nonlinear, incremental, and performs simultaneously the data quantization and projection by using the curvilinear component analysis (CCA), a distance-preserving reduction technique. Using bridges and seeds, it is able to fast adapt and track changes in the data distribution. Analyzing bridge length and density, it is able to detect a pre-fault condition. This paper presents an application of GCCA to a real induction machine on which a time-evolving stator fault in one phase is simulated.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						235	244		10.1007/978-981-13-8950-4_22	10.1007/978-981-13-8950-4												
S								A Neural Based Comparative Analysis for Feature Extraction from ECG Signals	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										NETWORK	Automated ECG analysis and classification are nowadays a fundamental tool for monitoring patient heart activity properly. The most important features used in literature are the raw data of a time window, the temporal attributes and the frequency information from the eigenvector techniques. This paper compares these approaches from a topological point of view, by using linear and nonlinear projections and a neural network for assessing the corresponding classification quality. The nonlinearity of the feature data manifold carries most of the QRS-complex information. Indeed, it yields high rates of classification with the smallest number of features. This is most evident if temporal features are used: Nonlinear dimensionality reduction techniques allow a very large data compression at the expense of a slight loss of accuracy. It can be an advantage in applications where the computing time is a critical factor. If, instead, the classification is performed offline, the raw data technique is the best one.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						247	256		10.1007/978-981-13-8950-4_23	10.1007/978-981-13-8950-4												
S								A Multi-modal Tool Suite for Parkinson's Disease Evaluation and Grading	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										INDUCED DYSKINESIAS; BRADYKINESIA; TREMOR	The traditional diagnosis of Parkinson's disease (PD) aims to assess several clinical manifestations, and it is commonly based on medical observations. However, an overall evaluation is extremely difficult due to the large variety of symptoms that affect PD patients. Furthermore, the traditional PD assessment is based on visual subjective observation of different motor tasks. For this reasons, an automatic system could be able to automatically assess and rate the PD and objectively evaluate the performed motor tasks. Such system could then support medical specialists in the assessment and rating of PD patients in a real clinical scenario. In this work, we developed multi-modal tool suite able to extract and process meaningful features from different motor tasks by means of two main experimental set-ups. In detail, we acquired and evaluated the motor performance acquired during the finger tapping, the foot tapping and the hand writing exercises. Several sets of features have been extracted from the acquired signals and used to both successfully classify a subject as PD patient or healthy subject, and rate the disease among PD patients.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						257	268		10.1007/978-981-13-8950-4_24	10.1007/978-981-13-8950-4												
S								CNN-Based Prostate Zonal Segmentation on T2-Weighted MR Images: A Cross-Dataset Study	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										MAGNETIC-RESONANCE; NEURAL-NETWORKS; CANCER; ATLAS	Prostate cancer is the most common cancer among US men. However, prostate imaging is still challenging despite the advances in multi-parametric magnetic resonance imaging (MRI), which provides both morphologic and functional information pertaining to the pathological regions. Along with whole prostate gland segmentation, distinguishing between the central gland (CG) and peripheral zone (PZ) can guide toward differential diagnosis, since the frequency and severity of tumors differ in these regions; however, their boundary is often weak and fuzzy. This work presents a preliminary study on deep learning to automatically delineate the CG and PZ, aiming at evaluating the generalization ability of convolutional neural networks (CNNs) on two multi-centric MRI prostate datasets. Especially, we compared three CNN-based architectures: SegNet, U-Net, and pix2pix. In such a context, the segmentation performances achieved with/without pre-training were compared in 4-fold cross-validation. In general, U-Net outperforms the other methods, especially when training and testing are performed on multiple datasets.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						269	280		10.1007/978-981-13-8950-4_25	10.1007/978-981-13-8950-4												
S								Understanding Cancer Phenomenon at Gene Expression Level by using a Shallow Neural Network Chain	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										MODELS	Exploiting the availability of the largest collection of patient-derived xenografts from metastatic colorectal cancer annotated for a response to therapies, this manuscript aims to characterize the biological phenomenon from a mathematical point of view. In particular, we design an experiment in order to investigate how genes interact with each other. By using a shallow neural network model, we find reduced feature subspaces where the resistance phenomenon may be much easier to understand and analyze.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						281	290		10.1007/978-981-13-8950-4_26	10.1007/978-981-13-8950-4												
S								Infinite Brain MR Images: PGGAN-Based Data Augmentation for Tumor Detection	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										SEGMENTATION; CNN	Due to the lack of available annotated medical images, accurate computer-assisted diagnosis requires intensive data augmentation (DA) techniques, such as geometric/intensity transformations of original images; however, those transformed images intrinsically have a similar distribution to the original ones, leading to limited performance improvement. To fill the data lack in the real image distribution, we synthesize brain contrast-enhanced magnetic resonance (MR) images-realistic but completely different from the original ones-using generative adversarial networks (GANs). This study exploits progressive growing of GANs (PGGANs), a multistage generative training method, to generate original-sized 256 x 256 MR images for convolutional neural network-based brain tumor detection, which is challenging via conventional GANs; difficulties arise due to unstable GAN training with high resolution and a variety of tumors in size, location, shape, and contrast. Our preliminary results show that this novel PGGAN-based DA method can achieve a promising performance improvement, when combined with classical DA, in tumor detection and also in other medical imaging tasks.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						291	303		10.1007/978-981-13-8950-4_27	10.1007/978-981-13-8950-4												
S								DNA Microarray Classification: Evolutionary Optimization of Neural Network Hyper-parameters	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										MODELS	The analysis of complex systems, such as cancer resistance to drugs, requires flexible algorithms but also simple models, as they will be used by biologists in order to get insights on the underlying phenomenon. Exploiting the availability of the largest collection of patient-derived xenografts from metastatic colorectal cancer annotated for response to therapies, this manuscript aims to (i) forecast the response to treatments on human tissues using murine information; (ii) providing a trade-off between model accuracy and interpretability, evolving a shallow neural network using a genetic algorithm.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						305	311		10.1007/978-981-13-8950-4_28	10.1007/978-981-13-8950-4												
S								Evaluation of a Support Vector Machine Based Method for Crohn's Disease Classification	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										TISSUE CLASSIFICATION; MR ENTEROGRAPHY; PART 1; IMAGES; DIAGNOSIS	Crohn's disease (CD) is a chronic, disabling inflammatory bowel disease that affects millions of people worldwide. CD diagnosis is a challenging issue that involves a combination of radiological, endoscopic, histological, and laboratory investigations. Medical imaging plays an important role in the clinical evaluation of CD. Enterography magnetic resonance imaging (E-MRI) has been proven to be a useful diagnostic tool for disease activity assessment. However, the manual classification process by expert radiologists is time-consuming and expensive. This paper proposes the evaluation of an automatic Support Vector Machine (SVM) based supervised learning method for CD classification. A real E-MRI dataset composed of 800 patients from the University of Palermo Policlinico Hospital (400 patients with histologically proved CD and 400 healthy patients) has been used to evaluate the proposed classification technique. For each patient, a team of radiology experts has extracted a vector composed of 20 features, usually associated with CD, from the related E-MRI examination, while the histological specimen results have been used as the ground-truth for CD diagnosis. The dataset composed of 800 vectors has been used to train and validate the SVM classifier. Automatic techniques for feature space reduction have been applied and validated by the radiologists to optimize the proposed classification method, while K-fold cross-validation has been used to improve the SVM classifier reliability. The measured indexes (sensitivity: 97.07%, specificity: 96.04%, negative predictive value: 97.24%, precision: 95.80%, accuracy: 96.54%, error: 3.46%) are better than the operator-based reference values reported in the literature. Experimental results also show that the proposed method outperforms the main standard classification techniques.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						313	327		10.1007/978-981-13-8950-4_29	10.1007/978-981-13-8950-4												
S								Seniors' Appreciation of Humanoid Robots	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											This paper is positioned inside a research project investigating elders' preferences and acceptance toward robots, in order to collect insights for the design and implementation of socially assistive robots. To this aim, short video clips of five manufactured robots (Roomba, Nao, Pepper, Ishiguro, and Erica) were shown to 100 seniors (50 Female) aged 65+ years (average age: 71.34 years, DS: +/- 5.60). After watching each robot video clip, seniors were administered a short questionnaire assessing their willingness to interact with robots, feelings robots aroused, and duties they would entrust to robots. The questionnaire's scores were assessed through repeated measures ANOVA in order to ascertain statistically significant differences among seniors' preferences. A clear uncanny valley effect was identified. The robot Pepper received significantly higher scores than Roomba, Nao, Ishiguro, and Erica on communication skills, ability to remind friendly and pleasant memories, comprehension, and ability to provide emotional support. In addition, Pepper was considered the most suitable, among the five proposed robots, in performing welfare duties for elders, children and disabled, protection and security, and front desk occupations.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						331	345		10.1007/978-981-13-8950-4_30	10.1007/978-981-13-8950-4												
S								The Influence of Personality Traits on the Measure of Restorativeness in an Urban Park: A Multisensory Immersive Virtual Reality Study	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										ENVIRONMENTS; PERCEPTION; RESPONSES; BENEFITS; STRESS; SOUND; NOISE; MODEL	This study investigates the influence of personality traits and water masking installations on the perceived restorativeness of an urban park by means of Multisensory Immersive Virtual Reality (M-IVR) methodology. To this aim, 95 adults (67 females, 28 males) were administered the NEO-FFI to measure personality and were presented two kinds of M-IVR scenarios, representing an urban park without any installation (S0) and the same park with a water installation (S1); in both scenarios, the perceived restorativeness was measured by means of the Perceived Restorativeness Scale (PRS-11). Results of ANOVAs showed that the perceived restorativeness (fascination and being-away components) was increased by the water installations, but that the effect was attenuated by personality. Correlation analysis showed that the extroversion dimension was weakly and negatively related to the fascination and being-away change score. These results suggest that M-IVR is a valid paradigm to investigate in a controlled but ecological way the effect of installations on the perceived restorativeness of environment and showed that beneficial effect of water installations on the evaluation of urban green parks is also related to personality characteristics.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						347	357		10.1007/978-981-13-8950-4_31	10.1007/978-981-13-8950-4												
S								Linguistic Repetition in Three-Party Conversations	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											The conversational mechanism of repetition appears to be strongly connected to the development of common ground among conversation participants. We report on three-party game-based interactions where two players participate in a quiz supervised by a facilitator. We use a semi-automatic method to detect alignment between players by observing linguistic repetitions in the dialogue transcripts and investigate the relation of the alignment to the type of the facilitator's feedback. Results suggest that the repetitions detected with this method have a function in the interaction, as it is reflected in the verbal and non-verbal behaviours of an interaction facilitator: facilitators provided more encouragement than expected where alignment lacked evidence and less than expected where alignment was ample.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						359	370		10.1007/978-981-13-8950-4_32	10.1007/978-981-13-8950-4												
S								An Experiment on How Naive People Evaluate Interruptions as Effective, Unpleasant and Influential	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										GENDER; PERCEPTIONS; POWER	We conducted an experimental study on 144 participants to evaluate how naive people evaluate different kinds of interruptions. We manipulated the point in which interruption occurs (early, late and no interruption) and the type of interruption (change subject, disagreement, clarification and agreement) on pre-built, acted and audio-recorded dialogues. Then, participants evaluated how much each interruption was effective, unpleasant and influential. The main results show that (a) with some exceptions, early and late interruptions were evaluated as more influential and unpleasant than control, and early interruption more unpleasant than late interruption; (b) change subject was more unpleasant and less effective than disagreement, and disagreement than clarification but only in not interruptive turn-taking while there was no difference between clarification and disagreement for effectiveness and unpleasantness; (c) the point was more important than type in determining the evaluation of interruption; (d) the perception of a turn-taking as an interruption was correlated with unpleasantness and only partially with influence; (e) all over the study, the effectiveness went in the opposite direction with respect to our hypothesis. Results are interpreted at the light of the literature on the effects of interruption and of politeness theory applied to interruption.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						371	382		10.1007/978-981-13-8950-4_33	10.1007/978-981-13-8950-4												
S								Analyzing Likert Scale Inter-annotator Disagreement	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										AGREEMENT; RELIABILITY	Assessment of annotation reliability is typically undertaken as a quality assurance measure in order to provide a sound fulcrum for establishing the answers to research questions that require the annotated data. We argue that the assessment of inter-rater reliability can provide a source of information more directly related to the background research. The discussion is anchored in the analysis of conversational dominance in the MULTISIMO corpus. Other research has explored factors in dialogue (e.g. big-five personality traits and conversational style of participants) as predictors of independently perceived dominance. Rather than assessing the contributions of experimental factors to perceived dominance as a unitary aggregated response variable following verification of an acceptable level of inter-rater reliability, we use the variability in inter-annotator agreement as a response variable. We argue the general applicability of this in exploring research hypotheses that focus on qualities assessed with multiple annotations.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						383	393		10.1007/978-981-13-8950-4_34	10.1007/978-981-13-8950-4												
S								PySiology: A Python Package for Physiological Feature Extraction	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										RESPONSES	Physiological signals have been widely used to measure continuous data from the autonomic nervous system in the fields of computer science, psychology, and human-computer interaction. Signal processing and feature estimation of physiological measurements can be performed with several commercial tools. Unfortunately, those tools possess a steep learning curve and do not usually allow for complete customization of estimation parameters. For these reasons, we designed PySiology, an open-source package for the estimation of features from physiological signals, suitable for both novice and expert users. This package provides clear documentation of utilized methodology, guided functionalities for semi-automatic feature estimation, and options for extensive customization. In this article, a brief introduction to the features of the package, and to its design workflow, are presented. To demonstrate the usage of the package in a real-world context, an advanced example of image valence estimation from physiological measurements (ECG, EMG, and EDA) is described. Preliminary tests have shown high reliability of feature estimated using PySiology.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						395	402		10.1007/978-981-13-8950-4_35	10.1007/978-981-13-8950-4												
S								Effect of Sensor Density on eLORETA Source Localization Accuracy	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										RESOLUTION ELECTROMAGNETIC TOMOGRAPHY; EEG; ELECTRODE; SLORETA; DISEASE; SYSTEMS; FMRI	The EEG source localization is an interesting area of research because it provides a better understanding of brain physiology and pathologies. The source localization accuracy depends on the head model, the technique for solving the inverse problem, and the number of electrodes used for detecting the EEG. The purpose of this study is to examine the localization accuracy of the eLORETA method applied to high-density EEGs (HDEEGs). Starting from the 256-channel EEGs, three different configurations were extracted. They consist of 18, 64, and 173 electrodes. The comparison of the results obtained from the different configurations shows that an increasing number of electrodes improve eLORETA source localization accuracy. It is also proved that a few number of electrodes could be not sufficient to detect all active sources. Finally, some sources resulting significant when few electrodes are used could turn out to be less significant when EEG is detected by a greater number of sensors.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						403	414		10.1007/978-981-13-8950-4_36	10.1007/978-981-13-8950-4												
S								To the Roots of the Sense of Self: Proposals for a Study on the Emergence of Body Awareness in Early Infancy Through a Deep Learning Method	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										HUMAN ACTIVITY RECOGNITION; SHORT-TERM-MEMORY; NEURAL-NETWORKS; GESTURE; MOTHER; REPRESENTATION; CONSCIOUSNESS; SYNCHRONY; BEHAVIOR; SYSTEM	In the psychological field, the study of the interaction between the caregiver and the child is considered the context of primary development. Observing how infants interact with their mothers, in the first year of their life, has allowed researchers to identify behavioural indicators useful to highlight the crucial role of their synchronisation, within the behavioural and cognitive domain. The aim of this study is to employ a new observational method, mediated by an artificial neural network model, to systematically study the complex bodily gestural interaction between child and caregiver, which concurs in the development of the child's bodily awareness, and how the quality of early interactions affects the child's relationship with his body. The first part will deal with qualitative observational methods, their limits and the object of the study; the second part will describe the reference theory; the third part will describe the neural network model, the details of the proposed study and the expected results. We hypothesise that this model could be used for the construction of a tool that could compensate for some of the critical shortcomings that lie within observational methods in the field of psychology.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						415	429		10.1007/978-981-13-8950-4_37	10.1007/978-981-13-8950-4												
S								Performance of Articulation Kinetic Distributions Vs MFCCs in Parkinson's Detection from Vowel Utterances	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										SPEECH; DISEASE	Speech is a vehicular tool to detect neurological degeneration using certain accepted biomarkers derived from sustained vowels, diadochokinetic exercises, or running speech. Classically, mel-frequency cepstral coefficients (MFCCs) have been used in the organic and neurologic characterization of pathologic phonation using sustained vowels. In the present paper, a comparative study has been carried on comparing Parkinson's disease detection results using MFCCs and vowel articulation kinematic distributions derived from the first two formants. Binary classification results using support vector machines avail the superior performance of articulation kinematic distributions with respect to MFCCs regarding sensitivity, specificity, and accuracy. The fusion of both types of features could lead to improve general performance in PD detection and monitoring from speech.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						431	441		10.1007/978-981-13-8950-4_38	10.1007/978-981-13-8950-4												
S								From "Mind and Body" to "Mind in Body": A Research Approach for a Description of Personality as a Functional Unit of Thoughts, Behaviours and Affective States	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										SAMPLE; BRAIN	The study of personality has been developed according to different lines of research in which it is also possible to identify transversal elements that are useful for an unprecedented trans-theoretical research programme. The existence of a close relation between the contents of thought (narrative system) and the vegetative and motor functions (experiential system) of the human being is currently considered of great relevance. However, until now, the studies using classic psychological test in order to describe the contents of the narrative system customize the experiential system and produce partial descriptions and non-integrated description of the personality. Nevertheless, when the people reflect about the questions in a personality test, they present (under the action of this cognitive stimulus) a synchronous activity of the narrative and experiential system. The intention of the study proposed in this paper, with the use of biofeedback and neurofeedback methods, is to record the physiological responses of the subject while answering to the items of a personality test, in order to describe it as a functional unit of thoughts, behaviours and affective states. The personality assessment tool, Cloninger's temperament and character inventory, will be provided with a computerized system: the electrophysiological recording will be produced using the neurofeedback system "Neurobit Optima 4"; for the analysis of electrophysiological measurements, software ("BioExplorer" system) will be used to manage the brain-computer interface (BCI). It is expected that the aforementioned methodology will allow for an original description of the personality as a functional unit of thoughts, behaviours and affective states.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						443	452		10.1007/978-981-13-8950-4_39	10.1007/978-981-13-8950-4												
S								Online Handwriting and Signature Normalization and Fusion in a Biometric Security Application	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											In this paper, we analyze the combined application of signatures and capital handwriting in a biometric recognition application. We combine a signature recognition system based in a multi-section vector quantization with a handwriting text recognition system based in self-organizing maps and DTW. Due to the need to normalize the scores before the combination, we study the effect of different normalization methods and we propose the application of a logarithmic transformation for signature scores previous normalize them. Experimental results show that the identification rate raises from 86.11% using capital letter words and 96.95% using signatures up to 99.72% with a fusion of both traits. Minimum detection cost function (DCF) also improves, from 3.56 and 3.51%, respectively, up to 1.0% using the fusion of both traits.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						453	463		10.1007/978-981-13-8950-4_40	10.1007/978-981-13-8950-4												
S								Data Insights and Classification in Multi-sensor Database for Cervical Injury	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										FRAMEWORK; SELECTION	The aim of this work is to produce a classification system to assess cervical injury from a multi-sensor database. This is a database that has never been gathered before, with data coming from three different sensors: inertial, EEG, and thermography. How well these sensors help to build a good model and which right features influence the response variable are key to a better understanding of the link between sensor data and the presence of a cervical injury. There is an additional data set related to the user/patient (a survey) that will provide us with a different view and a baseline to compare with the sensor data. Both data sets are characterized by few observations and many variables, such that feature selection or feature engineering is crucial to success in building a good classification system. The approach used with both data sets is penalized logistic regression. This type of models helps in the selection of the best features and also prevents overfitting by adding a penalized term over the coefficients in the objective function. To verify the ability of each data set, a k-fold cross-validation was carried out with promising results for both the sensor and survey data sets. Results demonstrate an accuracy of up to 70%, which gives a good starting point to encourage increasing the size of the database with more patients. A final discussion highlights the importance of never underestimating the information provided by the patients.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						465	474		10.1007/978-981-13-8950-4_41	10.1007/978-981-13-8950-4												
S								Estimating the Asymmetry of Brain Network Organization in Stroke Patients from High-Density EEG Signals	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies											Following a stroke, the functional brain connections are impaired, and there is some evidence that the brain tries to reorganize them to compensate the disruption, establishing novel neural connections. Electroencephalography (EEG) can be used to study the effects of stroke on the brain network organization, indirectly, through the study of brain-electrical connectivity. The main objective of this work is to study the asymmetry in the brain network organization of the two hemispheres in case of a lesion due to stroke (ischemic or hemorrhagic), starting from high-density EEG (HD-EEG) signals. The secondary objective is to show how HD-EEG can detect such asymmetry better than standard low-density EEG. A group of seven stroke patients was recruited and underwent HD-EEG recording in an eye-closed resting state condition. The permutation disalignment index (PDI) was used to quantify the coupling strength between pairs of EEG channels, and a complex network model was constructed for both the right and left hemispheres. The complex network analysis allowed to compare the small-worldness (SW) of the two hemispheres. The impaired hemisphere exhibited a larger SW (p < 0.05). The analysis conducted using traditional EEG did not allow to observe such differences. In the future, SW could be used as a biomarker to quantify longitudinal patient improvement.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						475	483		10.1007/978-981-13-8950-4_42	10.1007/978-981-13-8950-4												
S								Preliminary Study on Biometric Recognition Based on Drawing Tasks	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										PRESSURE	In this paper, we analyze the possibility to identify people using online handwritten tasks different from the classic ones (signature and text). Our preliminary results reveal that some drawings offer a reasonably good identification rate, which is 78.5% in the case of multi-sectional vector quantization applied to classify circles and house drawing tasks. To the best of our knowledge, this is the first paper devoted to biometric recognition based on drawing tasks.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						485	494		10.1007/978-981-13-8950-4_43	10.1007/978-981-13-8950-4												
S								Exploring the Relationship Between Attention and Awareness. Neurophenomenology of the Centroencephalic Space of Functional Integration	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										MEMORY; BRAIN	Although there is a no established theory, there is no longer any doubt about the multiplicity of the structures involved in the attentional processes. Attention is involved, in fact, in several fundamental functions: consciousness, perception, motor action, memory and so on. For several decades, the hypothesis that attention is highly variable (for extension and clarity) in terms of consciousness has been quite influential, which would range within itself in relation to its changes of state: from sleep to wakefulness, from drowsiness to twilight state of consciousness, from confusion to hyperlucidity, from dreamlike to oneiric states. More recently, other fields of considerable theoretical importance have linked attention to emotion, to affectivity or primary autonomous psychic energy or to social determinants. In this paper, we shall demonstrate how paying attention to something does not mean becoming aware of it. A series of experiments has shown that these are two distinct mental states. This decoupling could represent a useful mechanism for the ability to survive that has developed during the course of evolution.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						495	501		10.1007/978-981-13-8950-4_44	10.1007/978-981-13-8950-4												
S								Decision-Making Styles in an Evolutionary Perspective	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										INDIVIDUAL-DIFFERENCES; STRATEGIES; COGNITION	Naturalistic decision-making (NDM) investigates the cognitive strategies used by experts in making decisions in real-world contexts. Unlike studies conducted in the laboratory, the NDM paradigm is applied to real human interactions, often characterized by uncertainty, risk, complexity, time pressures and so on. In this approach, the role of experience is crucial in making possible a quick classification of decision-making situations and therefore inmaking an effective, rapid and prudent choice. Models of behaviour resulting from these studies represent an extraordinary resource for research and for the application of decision-making strategies in high-risk environments. They particularly underline not only that most of the critical decisions that we take are based on our intuition, but that the ability to recognize patterns and other signals that allow us to act effectively is a natural extension of experience.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						503	512		10.1007/978-981-13-8950-4_45	10.1007/978-981-13-8950-4												
S								The Unaware Brain: The Role of the Interconnected Modal Matrices in the Centrencephalic Space of Functional Integration	NEURAL APPROACHES TO DYNAMICS OF SIGNAL EXCHANGES	Smart Innovation, Systems and Technologies										BASAL GANGLIA; CONSCIOUSNESS; SELECTION; WILL	For over a century, it has been accepted that there exists a remote psychic space that influences our way of thinking, perception, decision-making and so on. This space, defined by Freud as the 'unconscious', embodies the psychic element that we are unaware of. It is a space that is an extension and a wider representation of the complex and sophisticated metapsychological apparatus he conceived. With respect to the conscious sphere (whose related anatomical function concerns the encephalic trunk, diencephalon and associative cortical areas), this unconscious dimension relates to the limbic lobe and specific areas of the frontal, parietal and temporal lobes. This complex neurophysiological system connects and coordinates the sensory, emotional, cognitive and behavioural systems. Its sophisticated adaptive functions, implied and unaware, allow the prefrontal cortex to transform a huge amount of information into explicit behaviour, thus affecting our behaviour in terms of executive functions, decision-making, moral judgments and so on. In this paper, we advance the hypothesis that these subcortical components constitute interconnected modal matrices that intervene under certain circumstances to respond to environmental requirements. In this space of functional integration, they act as an intermediary between the frontal cortex, the limbic system and the basal ganglia and are a key player in the planning, selection and decision to carry out appropriate actions. Due to their generativity and intramodal and extramodal connections, it is plausible to assume that they also play a role in mediation between unconscious and conscious thought.																	2190-3018		978-981-13-8950-4; 978-981-13-8949-8				2020	151						513	521		10.1007/978-981-13-8950-4_46	10.1007/978-981-13-8950-4												
J								EvoAAA: An evolutionary methodology for automated neural autoencoder architecture search	INTEGRATED COMPUTER-AIDED ENGINEERING											FRAME ROOF STRUCTURES; DIFFERENTIAL EVOLUTION; LEARNING ALGORITHM; CONJUGATE-GRADIENT; NETWORK; OPTIMIZATION; COMPLEX; CLASSIFICATION; SYSTEMS; MODEL																		1069-2509	1875-8835					2020	27	3					211	231															
J								Multiobjective optimization of deep neural networks with combinations of Lp-norm cost functions for 3D medical image super -resolution	INTEGRATED COMPUTER-AIDED ENGINEERING											SUPPORT VECTOR REGRESSION; VARIANCE ANALYSIS; SUPERRESOLUTION; EVOLUTIONARY; SCALARIZATION; INTELLIGENCE; MINIMIZATION; CONVERGENCE; STATISTICS; ALGORITHMS																		1069-2509	1875-8835					2020	27	3					233	251															
J								Background subtraction by probabilistic modeling of patch features learned by deep autoencoders	INTEGRATED COMPUTER-AIDED ENGINEERING											STACKED DENOISING AUTOENCODERS; FOREGROUND DETECTION; NEURAL-NETWORKS; TRACKING																		1069-2509	1875-8835					2020	27	3					253	265															
J								Intelligent trajectory planner and generalised proportional integral control for two carts equipped with a red -green -blue depth sensor on a circular rail	INTEGRATED COMPUTER-AIDED ENGINEERING											ONLINE ALGEBRAIC IDENTIFICATION; DISTURBANCE REJECTION CONTROL; MAGNETIC-LEVITATION SYSTEMS; NONLINEAR CONTROL; VIBRATION CONTROL; TRACKING; REHABILITATION; RECOGNITION; ALGORITHM; MATRIX																		1069-2509	1875-8835					2020	27	3					267	285															
J								Design of reliable virtual human facial expressions and validation by healthy people	INTEGRATED COMPUTER-AIDED ENGINEERING											EMOTION RECOGNITION; SOCIAL COGNITION; SCHIZOPHRENIA; FACE																		1069-2509	1875-8835					2020	27	3					287	299															
J								Semantic visual recognition in a cognitive architecture for social robots	INTEGRATED COMPUTER-AIDED ENGINEERING											NEURAL-NETWORK																		1069-2509	1875-8835					2020	27	3					301	316															
J								Content based image retrieval by ensembles of deep learning object classifiers	INTEGRATED COMPUTER-AIDED ENGINEERING											CONVOLUTIONAL NEURAL-NETWORKS; SELECTION																		1069-2509	1875-8835					2020	27	3					317	331															
J								Neutrosophic Psychology for Emotional Intelligence Analysis in Students of the Autonomous University of Los Andes, Ecuador	NEUTROSOPHIC SETS AND SYSTEMS										Emotional intelligence; group emotional igenc neutrosophical psychology theory; higher education.		Emotional intelligence is a relatively recent and important concept in psychology, where the individual's ability to control his (her) emotions and to deal with the behavior of those around him (her) is taken into account. This implies a dynamic relationship between concepts such as the opposite which are rationality and emotion, where the emotionally intelligent individual would be located in the right middle of these two poles. A very recent way of representing these triadic relationships is the neutrosophical psychology theory, where if A is a psychological concept, the dynamical interaction of the concept is represented by the scheme (<A> <NeutA> <AntiA>). This paper studies the behavior of the emotional intelligence in a group of university students from the Autonomous University of Los Andes in Ecuador using classical statistical inference tools, according to the triad ( <EI> <NeutEl> <antiEl >). The main motivation of this paper is to study the state of El in the students of this university since a high El will guarantee better future professionals and higher quality learning.																	2331-6055	2331-608X					2020	34						1	8															
J								Priorization of educational strategies on nutrition and its correlation in anthropometry in children from 2 to 5 years with neutrosophic topsis	NEUTROSOPHIC SETS AND SYSTEMS										nutrition; anthropometry; eating habits; quality of life		The study carried out at the Tajamar health Center allows the evaluation of the educational strategies to be followed by the mothers of children aged from 2 to 5 years, leading to the definition of a correct nutrition pattern and its correlation with the value of anthropometry by applying the TOPS'S Neutrosophic method. A descriptive, qualitative, quantitative and field study was conducted. In order to determine the level of knowledge of the children's mothers, we used the survey as a technique; which allowed to identify the status of nutrition in which they are and whether the growth and development of each child is according to their age or not. While carrying out the investigation, it was continued that there is an eating disorder in children, since 50% of the mothers prepare their food based on carbohydrates. In addition, only 20% of the mothers indicate that they put fruit in their children's lunchboxes; 55% refers to the consumption of junk food. The lack of physical activity is often a risk factor for diseases due to eating disorder. The obtained results allow the implementation of strategies based on the assessment of nutritional status in order to improve the lifestyle of each children.																	2331-6055	2331-608X					2020	34						9	15															
J								Neutrosophic AHP in the analysis of Business Plan for the company Rioandes bus tours	NEUTROSOPHIC SETS AND SYSTEMS										Business Plan; AHP; Neutrosophic		Tourism development represents a key area for the economy. Its development depends largely on the ability to capture new sources of income. The city of Riobamba of Ecuador has the problem of not having a comfortable route that meets the current demands of tourists, which causes low levels of income. This research aims to develop a method for the analysis of the business plan through the Hierarchical Neutrosophics Analytical Process. The proposal is implemented in the Rioandes Company from which it is achieved as a result that the proposed plan represents a satisfactory alternative for its implementation.																	2331-6055	2331-608X					2020	34						16	23															
J								An integrative neutrosophic model focused on personality (inmfp) for the adequate management of the level of work stress	NEUTROSOPHIC SETS AND SYSTEMS										Workplace stress; eutress; distress; Integrative Model Focused on Personality; Neutrosophical Psychology Theory		Stress is an inevitable way of adapting each individual to the challenges of the surrounding environment. When the adaptation is done in a negative way and produces physiological or psychological imbalances in the person, then stress is called distress. However, stress is not always harmful to the person, there is stress in situations of joy or happiness, which is known by eustress. In the work environment, distress can cause physical and mental health problems for workers and therefore damage to the dynamics of the job. This paper aims to propose an index of occupational stress measurement based on the Integrative Model Focused on Personality and the Neutrosophical Psychology Theory. The Integrative Model Focused on Personality considers personality as the result of a combination of dynamically interacting elements, while Neutrosophical Psychology understands personality traits as a triad (<A>, <neutA>, <antiA>), where A is a personality trait, antiA is its opposite, and the trait manifests itself in each person in an intermediate way between <A> and <antiA>, where <neutA> is an intermediate state of indeterminacy, where it is neither A nor antiA. Particularly, we explicitly consider the case in which the individual is identified in an intermediate state of stress and therefore has a risk to be conducted towards either the distress or the eustress.																	2331-6055	2331-608X					2020	34						24	32															
J								A Neutrosophic Statistic Method to Predict Tax Time Series in Ecuador	NEUTROSOPHIC SETS AND SYSTEMS										time series; tax; prediction; neutrosophic statistic; autocorrelation; median average	PROGRESS; CURVES	Prediction of tax collection behavior is an essential tool for social planning by the State of any country. The tax is the State's mechanism for budget collection; which is necessary to accomplish public services that benefit the whole society. This paper firstly aims to propose a method of predicting tune series where values can be given in form of intervals rather than numbers. This form permits to obtain more truthful results; but with a greater indeterminacy. Because statistical prediction methods are used, where data in form of intervals are included, we can classify this approach as a kind of Neutrosophic Statistics technique. Basically, the method converts a set at predicted numerical values into intervals. The second objective is to apply the method to predict the monthly income front taxes in Ecuador for the year 2019.																	2331-6055	2331-608X					2020	34						33	39															
J								A neutrosophic model for the evaluation of the formative development of investigative competences	NEUTROSOPHIC SETS AND SYSTEMS										Law School graduates follow-up; competencies; multi-criteria selection; neutrosophic numbers		This paper presents a multicriteria method, solved by means of a method which is based on the ideal distance using neutrosophic numbers, which is convenient to evaluate the development of the investigative competencies of graduates in the UNIANDES Ibarra Law School, their competences and functions, for which we used theoretical samples that address the follow-up programs for higher education graduates, the analysis of their competencies and their impact on labor markets. The descriptive field research with a quantitative approach was applied. The sample covered a total of 122 Law professionals. The results obtained showed a similarity in the decisions obtained in practice.																	2331-6055	2331-608X					2020	34						40	47															
J								Neutrosophic Decision Map for critical success factors prioritization in a museum of religious Art	NEUTROSOPHIC SETS AND SYSTEMS										method; cognitive neutrophic map; museum; recommendations		Visits to museums represent a Way of keeping culture in memory of new generations. Getting to know the details that characterize each muse logical installation is a fundamental task for workers in the sector. That is why, depending on the knowledge that people have about the place, room recommendations are a problem of decision-making that are currently addressed. The present investigation proposes a method for the recommendation of museum rooms through the use of Neutrosophic Cognitive Map. A case study is applied where the proposal for the recommendation of rooms of the religious art museum of La Concepcion in Riobamba is implemented.																	2331-6055	2331-608X					2020	34						48	56															
J								Critical success factors modelling in operational management and the recovery of overdue portfolio of the Babahoyo GAD in The Municipal Market May 4	NEUTROSOPHIC SETS AND SYSTEMS										neutrosophic cognitive map; recovery of overdue portfolio; neutrosophic graph; neutrosophic number; Ishikawa diagram		This paper aims to study the situation of the recovery of the overdue portfolio of the Babahoyo Municipal Decentralized Autonomous Government (MDAG) in the municipal market "4 de Mayo". The problem consists in the difficulties arose in tax collection by the municipal government's employees to this market. In the present investigation neutrosophic cognitive maps are applied to assess the relationship between every pair of causes of this problem. For the evaluation, we count on live experts' criteria. Because there exist some pairs of causes whose relationship are unknown, neutrosophic cognitive maps are used instead of the fuzzy ones, where symbol 1 denotes indeterminate relationships among them. Additionally, the Ishikawa Diagram is applied; which is a simple graphical way to represent the causes and effects of problems.																	2331-6055	2331-608X					2020	34						57	62															
J								Proposal of a neutrosophic index to evaluate the management of internal control	NEUTROSOPHIC SETS AND SYSTEMS										Internal control; audit; neutrosophy; Single-Valued Neutrosophic Number		Corporate auditing is part of the necessary mechanisms to evaluate and monitor the practices being carried out in enterprises. It also allows for the timely correction of future trends that will damage the company. internal control is part of these audits, where the company is internally analyzed, and is a highly recommended practice for both state and private companies. Evaluation of internal control presents some challenges, such as the presence of incomplete or contradictory information, as well as the importance of properly understanding and communicating what it is wished to study. This is why in this paper we propose an index to evaluate the internal control of a company, based on Single-Valued Neutrosophic Numbers (SVNN) and natural language. The advantage of this tool is that linguistic terms can be more easily used for assessing and better understood by the evaluated; also the indeterminacy that exists in any evaluation can be incorporated. To illustrate the use of this index, a case study is carried out in the internal control of the municipal public water company of Tulcan, Ecuador.																	2331-6055	2331-608X					2020	34						63	69															
J								PESTEL analysis of environment state responsibility in Ecuador	NEUTROSOPHIC SETS AND SYSTEMS										Method; Environmental evaluation; utrosophics Cogi IVIapS.	FUZZY COGNITIVE MAPS; CLIMATE-CHANGE	The care and conservation of the environment is currently a task to ensure human survival. Various arc the negative enviromnental impacts to which the planet is subjected. Quantifying the adverse events that the planet faces constitutes an activity little addressed by science. The present investigation describes a solution to the problem posed from the development of a method for environmental evaluation. It uses a multicriteria approach and models its inference by means ofNeutrosophics Cognitive Maps.																	2331-6055	2331-608X					2020	34						70	78															
J								Evaluating the acceptance level of the papillomavirus vaccine using a neutrosophic linguistic model	NEUTROSOPHIC SETS AND SYSTEMS										Human Papilloma Virus; 2-tuple linguistic neutrosophic) -; Computing with Words; linguistic model.	REPRESENTATION MODEL; NUMBERS; WORDS	Human Papilloma Virus is a health problem for paying attention to. This is due to the consequences it can bring to infected peoples and the society. Thus, it is a privilege to have the vaccine that prevents children to get such disease. Particularly we propose in this paper to carry out a study of the approval level of the Human Papilloma Virus vaccine in the fiscal school "Bolivar" sited in Tuledn city, Ecuador. To achieve greater accuracy in this study it is applied the method of 2 -tuples linguistic neutrosophic model, which is part of the Computing with Words. This tool allows obtaining results from the evaluation of the satisfaction of the respondents by means of linguistic tenns, which is the most natural way of evaluation compared with the numerical one. There is also a numeric element that measures the accuracy of the result. The incorporation ofneutrosophy allows the explicit inclusion of indetenninacy within what is evaluated, that results in a greater accuracy in the model with respect to the 2-tuples linguistic fuzzy method.																	2331-6055	2331-608X					2020	34						79	85															
J								Neutrosophic ladov tecnique for assessing the proposal of standardization of the beef cutting for roasting in Patate canton, Ecuador	NEUTROSOPHIC SETS AND SYSTEMS										beef cutting; beef quality; neutrosophic ladov technique; single -valued neutrosophic number.		Meat is an important food for human consumption. Its consumption is associated with economic development, so that the more meat consumed, the higher the level of quality of life or index of wealth attributed to a population. This consideration has led, during the second half of the 20th centuiy, to a greater consumption of meat. To prepare different dishes requires an adequate knowledge of cutting meat, so ignoring this aspect would be contradictor with gastronomy. In Ecuador, and particularly in the Palate canton, there is a lack of knowledge about the types of cuts that can be made with meat, in order to obtain favorable, appealing results and, in turn, make culinary art an art of excellence and quality. The objective of this work is to analyze the real situation of the knowledge of cutting meat, for a gastronomy of excellence and quality. To validate the results of the analysis that is carried out, the.ladov technique is used, and in particular the Neutrosophical.ladov is used, a technique that when using Neutrosophy provides accurate results and contributes to a greater interpretability of qualitative information, and in particular of linguistic terms, useful for decision-making support. Conclusions are presented that indicate how knowledge of cutting meat behaves, in the Patate canton of Ecuador and where emphasis should be placed to obtain a gastronomy of excellence and quality.																	2331-6055	2331-608X					2020	34						86	92															
J								Neutrosophic causal modeling for analyzing the diffusion of the institutional culture: the case UN IAN DES	NEUTROSOPHIC SETS AND SYSTEMS										Diffusion of the culture; neutrosophic numbers; evaluation; multi-criteria		The dissemination of culture is an activity assumed by teaching institutions. Knowing the cultural elements that characterize each nation allows to preserve the cultural heritage. However; quantifying this result represents a complex task to perform. This research proposes a solution to the problem posed with the design of a multicriteria method for the evaluation of cultural diffusion. The method uses neutrosophical numbers to model uncertainty. The proposal introduced the results hi UNIANDES; Ibarra and it was found that it has a high rate of cultural diffusion.																	2331-6055	2331-608X					2020	34						93	99															
J								Prioritization internal factors in the emergency service of the "Luis Gabriel Davila" Hospital that cause the reentry of patients within 48 hours, based on neutrosophic DEMATEL	NEUTROSOPHIC SETS AND SYSTEMS										patient readmission; Neutrosophic DE A'i'EL; single -valued trapezoidal neutrosophic number; valued triangular neutrosophic number.	RETURN	A high incidence of patient readmission in a hospital is a public health problem to consider. This investigation aims to address this problem at the "Luis Gabriel Davila" hospital in the Province of Carchi, Ecuador, in its emergency area, where readmission incidence is considered in a time span of 48 hours. For this end, Neutrosophic DEMATEL method was used as the technique that studies the cause -effect relationships to allow decision -making in the problems solution of this type. Neutrosophic DEMATEL includes the modeling of indeterminacy due to the lack of information, or contradictory, paradoxical or inconsistent information. This paper uses a scale of linguistic terms, which is an advantage for experts to emit their criteria. A team of three experts is selected, who evaluated twelve factors related to this topic.																	2331-6055	2331-608X					2020	34						100	109															
J								Neutrosophic interrelationship of Key Performance Indicators in an accounting process	NEUTROSOPHIC SETS AND SYSTEMS										Key Performance Indicator; financial accounting; neutrosophic cognitive map; neutrosophic number	IMAGE SEGMENTATION; INDETERMINACY; ALGORITHM; SYSTEM; MODEL	This paper is an investigation on the interrelationship among some financial performance indicators of the company "Zambrano Loor Naira Narcisa" located in the city of Santo Domingo, Ecuador. The study is conducted because this company presents some difficulties in this regard. In order to guarantee the profundity of the analysis, we apply the technique of dynamic study of Neutrosophic Cognitive Maps, where the cause-effect relationship among the indicators is described and established; in addition it is provided the causal connecting weights between each pair of indicators. Four experts have made the evaluations. The advantage of neutrosophic cognitive maps over fuzzy cognitive maps is that the former maintains the modeling of the uncertainty of the latter, and also offers the possibility of incorporating indetermination, thus constituting a more realistic source to assess cause-effect relationships.																	2331-6055	2331-608X					2020	34						110	116															
J								Neutrosophic Iadov for measuring of user satisfaction in a virtual learning environment at UNIANDES Puyo	NEUTROSOPHIC SETS AND SYSTEMS										Virtual Learning Environment; higher education; neutrosophic Iadov technique; computer assisted instruction		In present, the university education promoted by the Regional Autonomous University of Los Andes, Ecuador, is supported by a Virtual Learning Environment. This is the fundamental reason that originated this research, for knowing if the use of the Virtual Learning Environment has either a positive or negative effect on the teaching-learning process in this university, especially in the Puyo campus. To carry out this work we consider the academic cycle April-August 2018. A survey was conducted in a unique environment and was applied to teachers and students, such that their opinion on the use of this tool is investigated. The processing of data of the survey was based on the neutrophic Iadov technique. Iadov technique consists of a set of three closed questions, and in addition an undetermined number of open questions that are processed by a logical tree that combines every triple of possible answers with a number that serves to logically evaluate such a combination. Neutrosophic Iadov generalizes the Iadov technique in order to take into account the indeterminacy that is typical of any evaluation. Therefore, this method is more accurate than its precedent.																	2331-6055	2331-608X					2020	34						117	125															
J								Evaluation of actions to implement quality management and institutional projects in UNIANDES-Quevedo University a neutrosophic approach	NEUTROSOPHIC SETS AND SYSTEMS										Method; neutrosophic; multicriteria; quality management		The processes of quality management and human vocational training constitute an important element to be measured in higher education organizations. Quantifying its impact makes it possible to project the organization to improve this indicator. However, consolidating the processes of quality management and institutional development for the achievement of professional human development of excellence at the UNIANDES-Quevedo University represents an unresolved task. The present investigation proposes a solution to the problem posed from the development of a method to measure the implementation of quality management in institutional projects. The method bases its operation by neutrosophic numbers and uses a multicriteria approach. The proposal is applied as a case study at the UNIANDES-Quevedo University, from which it was possible to quantify its implementation rate.																	2331-6055	2331-608X					2020	34						126	134															
J								Analysis of the Venezuelan migratory impact on the economic development of Santo Domingo city, a neutrosophic cognitive map approach	NEUTROSOPHIC SETS AND SYSTEMS										Migration. local economy; employment; cool aps; neutrosophic cognitive maps.	MODEL	In the most recent years, there has been a massive Venezuelan emigration to Santo Domingo city in Ecuador. This phenomenon has affected the local economy as well as the society of the city. This paper aims to study the economic impact of Venezuelan emigration to the city of Santo Domingo since a Nentrosophic Cognitive Map (NCM) approach. The advantage of using NCMs is that the causal relationships between the variables that influence the phenomenon under study are established and ranked in order of their importance. This allows determining which are the elements related to the Venezuelan migration to the Ecuadorian city of Santo Domingo that most impact the city economically, and if this impact is positive or negative for citizens and migrants. Finally, decision -makers will be provided with valuable information to take the necessary measures, where those favorable trends are reinforced and the unfavorable ones are reversed, always that humanitarian policies were dictated.																	2331-6055	2331-608X					2020	34						135	142															
J								The transformational leadership, sustainable key for the development of ecuadorian companies. A neutrosophic psychology approach	NEUTROSOPHIC SETS AND SYSTEMS										Transformational leadership; neutrosophy; neutrosophical psychology theory; neutrosophic crisp personality		The study of leadership is a fairly recurring topic in the scientific literature in recent years. Some approaches concern with the relationship between leadership and some personality trails of leaders. One field where leadership is of great importance is the business world, where leaders arc needed to direct the company's progress because they inspire the other members of the organization. This paper aims to propose a mathematical method for measuring the transformational leadership degree in the company. Transformational leadership is the most complete of leaderships, the transformational leader is versatile, charismatic, communicative, empathic, and produces positive changes in the company. The method is based on the opinion of colleagues and subordinates of the leader about its leadership capacity, rather than on the study of its own personality. For the method to be easily usable, it is based on a graphic. representation of both, the individual evaluations and the final results. The method is derived from the neutrosophical psychology theory, since it is considered not only the concepts of <leadership> or <anti-leadership>, but for the first time the <al-eadership> is defined to classify those people who exist in the organization that neither direct, nor restrain the development of the company, moreover, the a-leadership can be a component of any leader's personality.																	2331-6055	2331-608X					2020	34						143	152															
J								Prioritization of non-functional requirements in a mobile application for panic button system using neutrosophic decision maps	NEUTROSOPHIC SETS AND SYSTEMS										Non-functional req nts requirementneutrosophic logic neutrosophic co nitive maps.		Several countries around the world have implemented systems to preserve public safety. Taking advantage of technological advances that are available to all citizens, in Ecuador it has been implemented a system called panic button, which consists of a cell phone network with geo-location, which can assist every citizen who needs to ask for help and the authorities will respond efficiently. This research proposes to improve this system in order to increase its efficiency and effectiveness. This requires the use of prioritization of non-functional requirements. In this paper we use the static analysis of Neutrosophic Cognitive Maps to determine, evaluate, and compare non-functional requirements. Neutrosophic Cognitive Maps allow establishing causal relationships among different criteria, with the objective of determining an order of preference. In this investigation we have three experts who pmvided the evaluations.																	2331-6055	2331-608X					2020	34						153	158															
J								Neutrosophic AHP for the prioritization of requirements for a computerized facial recognition system	NEUTROSOPHIC SETS AND SYSTEMS										prioritization of software requirements facial recognition system; neutrosophic A IP; nulticriteria-decision making		The Cooperative of Taxis and Vans of Puyo in Ecuador is dedicated to the transportation of people and minor loads. Due to the considerable number of members of this cooperative, it is difficult to determine the presence of each of the participants in the meetings. That is why it N.vas decided to implement a facial recognition sy stem that allo WS identifying the presence of members in each moment. lb wever, in order to apply this system, certain requirements are needed for guaranteeing its success. This paper aims to apply the neutrosophic Analytic Hierarchy Process (NAHP) technique for analyzing the prioritization of requirements necessary to implement a facial recognition system in the cooperative. Neutrosophic AHP permits including the indetermination incorporated hi neutrosophic models and additionally experts can provide their evaluations based on linguistic terms, which results in greater ease and effectiveness to evaluate.																	2331-6055	2331-608X					2020	34						159	168															
J								A neutrosophic linguistic model for internal control evaluation to an Ecuadorian Company	NEUTROSOPHIC SETS AND SYSTEMS										internal control; computing with words; linguistic model; 2-tuple method	REPRESENTATION MODEL; CONSTRUCTION; WORDS	The internal control of a company is essential to evaluate the performance of the institution during a certain period of time, which allows taking the necessary measures to correct in advance the mistakes that are being made or to enhance the positive practices within the company. In this paper we propose to evaluate the Commercial Company Manolo's located in the city of Babahoyo, Ecuador, regarding the aspects measured in the internal control. For this, we use the neutrosophic 2-tuples method. This method is a part of the Computing with Word (CWW) that is distinguished by performing calculations on words rather than numbers and is an easier and more natural way to evaluate by experts. The inclusion of neutrosophy allows for greater accuracy of the results, since the calculations incorporate indeterminacy. Three experts evaluated the internal control for this investigation.																	2331-6055	2331-608X					2020	34						169	176															
J								Neutrosociology for the Analysis of the Pros and Cons of the LIFE Series in UNIANDES, Ecuador	NEUTROSOPHIC SETS AND SYSTEMS										English course; higher education; neutrosociolop neutrosoph c statistics		To teach English in Higher Education is of great importance to have professionals prepared for the challenges imposed by the globalization. Ecuador is a Spanish-speaking country for which the teaching of English is part of the programs of study of the courses taught in the Ecuadorian's universities. This paper aims to carry out a statistical study of the acceptance of the LIFE program in the teaching of English at the Regional Autonomous University of Los Andes (UNIANDES in Spanish), its pros and cons. This is measured according to a Liked scale, with five options, two positive, two negative and one neutral. We consider that none of these three levels should pass unnoticed, since each of them shows a tendency to consider. This includes the aspect of indeterminacy, which indicates the existence of contradictions, indifference, among other motivations of this type. Therefore, we perform a non-traditional processing of the Liked scale, more in line with the principles of Neutrosociology and Neutrosophic Statistics, where the indeterminate probability is taken into account for the social group which is student body.																	2331-6055	2331-608X					2020	34						177	182															
J								Analysis of an strategic plan to increase the sales level of the company "TIENS" of Babahoyo using neutrosophic methods	NEUTROSOPHIC SETS AND SYSTEMS										Analytic Hierarchy Process (AHP); SWOT analysis; naturopathy; sale strategies	SWOT ANALYSIS; AHP	In the world, natural medicine is increasingly appreciated as a complement to conventional medicine that treats diseases with industrially produced drugs. Natural medicine is cheaper, more accessible than the other one; it usually has fewer contraindications and also carries empirical and traditional knowledge. In Ecuador it is a very popular alternative for the population. "TIENS" Company produces natural medicines, which has been facing the difficulty that its sales are appreciably decreasing. That is why the company performs an analysis to increase the sale levels of produced medicines. To perform the evaluation of the alternatives, the neutrosophic AHP-SWOT method is used. This method allows studying the Strengths, Weaknesses, Opportunities, and Threats of the company, which in its original torn does not allow quantifications, that is Why the AHP technique is incorporated to evaluate pair-wise comparisons of criteria. In the neutrosophic framework, AHP-SWOT takes into account the indeterminacy that exists in decision making processes.																	2331-6055	2331-608X					2020	34						183	192															
J								Using a neutrosophic model to evaluate website usability of a web portal for the commercial management of an advertising company	NEUTROSOPHIC SETS AND SYSTEMS										Publicity; website; neutrosophic AHP; neutrosophic TOPSIS	NUMBERS; OPERATORS	In the field of the development of Information Technology solutions, web applications are the most used tools for the businesses commercial management in their different scales. However, in the company "JM" Advertising, located in the city of Quevedo, Ecuador, does not have a digital medium in this regard, which supports the commercial management. That is, it does not have a digital catalog of products and services, nor orders online or online collections. In this context, web usability plays a key role in the process of developing successful websites. This paper proposes a methodology that allows the evaluation of the usability of two prototypes of web portals in the 'JM' advertising company. This methodology is applicable to other companies of similar functions. The methodology contains tools such as neutrosophic TOPSIS and neutrosophic AHP. Evaluation in the framework of neutrosophy incorporates the indeterminacy that is typical of decision-making processes, while the combination of AHP with TOPSIS allows us to take the advantages of both techniques.																	2331-6055	2331-608X					2020	34						193	203															
J								Delphi method for evaluating scientific research proposals in a neutrosophic environment	NEUTROSOPHIC SETS AND SYSTEMS										Research proposal; Delphi method; fuzzy Delphi method; single valued neutrosophic set; single valued triangular neutrosophic number	FUZZY DELPHI; MODEL	The scientific research proposal is part of the task to be carried out in academic and research institutions around the world. This is a complex decision-making problem, because decision-makers must determine the projects that are appropriate to the subjects addressed by the institution, those projects must be achievable within a reasonable deadline, they must have the financial means and the budget necessary to he carried out; the staff must be sufficiently qualified and an optimum number of personnel must be available to succeed the tasks and not interfere with other research projects. This is a predictive problem, thus, the proposed model is based on Delphi method for evaluating research projects and is supported by neutrosophy. Delphi method is widely applied in the prediction of future events, in this model we introduce the uncertainty and indeterminacy modeled with neutrosophy. As the best of our knowledge, this model is the first one, which applies a neutrosophic Delphi method in the evaluation of scientific research proposals. Finally, a hypothetical case study illustrates the applicability of the method.																	2331-6055	2331-608X					2020	34						204	213															
J								Eight Kinds of Graphs of BCK-algebras Based on Ideal and Dual Ideal	NEUTROSOPHIC SETS AND SYSTEMS										BCK- algebra; Diameter; Chromatic number; Euler graph		In this paper, at first we introduce the concepts of ideal-annihilator, dual ideal- annihilator, right- ideal- annihilator, left- ideal- annihilator, right- dual ideal- annihilator, left- dual ideal-annihilator. Then by using of these concepts, we constructed six new types of graphs in a bounded BCK-algebra (X,*,0) based on ideal and dual ideal which are denoted by Phi(I)(X), Phi(I)nu (X), Delta(I)(X), Sigma(I)(X), Delta(I)nu(X), and Sigma(I)nu(X), respectively. Then basic properties of graph theory such as connectivity, regularity, and planarity on the structure of these graphs are investigated. Finally, by utilizing of binary operations boolean AND and nu, we construct graphs gamma(I)(X) and gamma(I)nu(X), respectively, some their interesting properties are presented.																	2331-6055	2331-608X					2020	33						1	22															
J								Neutrosophic Soft Structures	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic soft set (NSS); neutrosophic soft point; neutrosophic soft b-open set and neutrosophic b-separation axioms		In paper, neutrosophic soft points with the concept of one point greater than the other and their properties, generalized neutrosophic soft open set known as soft b-open set, neutrosophic soft separation axioms theoretically with support of suitable examples with respect to soft points, neutrosophic soft b(0)-space engagement with generalized neutrosophic soft closed set, neutrosophic soft b(2)-space engagement with generalized neutrosophic soft open set are addressed. In continuation, neutrosophic soft b(0)-space behave as neutrosophic soft b(2)-space with the plantation of some extra condition on soft b(0)-space, neutrosophic soft b(3)-space and related theorems, neutrosophic soft b(4)-space, monotonous behavior of neutrosophic soft function with connection of different neutrosophic soft separation axioms, monotonous behavior of neutrosophic soft function with connection of different neutrosophic soft close sets are reflected. Secondly, long touched has been given to neutrosophic soft countability connection with bases and sub-bases, neutrosophic soft product spaces and its engagement through different generalized neutrosophic soft open set and close sets, neutrosophic soft coordinate spaces and its engagement through different generalized neutrosophic soft open set and close sets, Finally, neutrosophic soft countability and its relationship with Bolzano Weirstrass Property through engagement of compactness, neutrosophic soft strongly spaces and its related theorems, neutrosophic soft sequences and its relation with neutrosophic soft compactness, neutrosophic soft Lindelof space and related theorems are supposed to address.																	2331-6055	2331-608X					2020	33						23	58															
J								Neutrosophic Boolean Rings	NEUTROSOPHIC SETS AND SYSTEMS										Boolean ring; Neutrosophic Boolean ring; Neutrosophic self-additive inverse elements; Neutrosophic compliments; Self and Mutual additive inverses	SUPPLIER SELECTION	In this paper, we are going to define Neutrosophic Boolean rings and study their algebraic structure. A finite Boolean ring R satisfies the identity a(2) = a for all a is an element of R , which implies the identity a(2) = a for each positive integer n >= 1. With this as motivation, we consider a Neutrosophic Boolean ring N(R, I) which fulfils the identity (a + bI)(2) = a + bI for all a + bI is an element of N(R,I) and describes several Neutrosophic rings which are Neutrosophic Boolean rings with various algebraic personalities. First, we show a necessary and sufficient condition for a Neutrosophic ring of a classical ring to be a Neutrosophic Boolean ring. Further, we achieve a couple of properties of Neutrosophic Boolean rings satisfied by utilizing the Neutrosophic self-additive inverse elements and Neutrosophic compliments.																	2331-6055	2331-608X					2020	33						59	66															
J								Neutrosophic Weakly Generalized open and Closed Sets	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Generalized closed sets; Neutrosophic Weakly Generalized Closed Sets; Neutrosophic Weakly Generalized open Sets; Neutrosophic topological spaces		Smarandache presented and built up the new idea of Neutrosophic concepts from the Neutrosophic sets. A.A. Salama presented Neutrosophic topological spaces by utilizing the Neutrosophic sets. Point of this paper is we present and concentrate the ideas Neutrosophic Weakly Generalized Closed Set in Neutrosophic topological spaces and its Properties are talked about subtleties																	2331-6055	2331-608X					2020	33						67	77															
J								Concentric Plithogenic Hypergraph based on Plithogenic Hypersoft sets - A Novel Outlook	NEUTROSOPHIC SETS AND SYSTEMS										Plithogenic sets; Hypergraph; Plithogenic Hypersoft sets; Concentric Plithogenic Hypergraphs		Plithogenic Hypersoft sets (PHS) introduced by Smarandache are the extensions of soft sets and hypersoft sets and it was further protracted to plithogenic fuzzy whole Hypersoft set to make it more applicable to multi attribute decision making environment. The fuzzy matrix representation of the plithogenic hypersoft sets lighted the spark of concentric plithogenic hypergraph. This research work lays a platform for presenting the concept of concentric plithogenic hypergraph, a graphical representation of plithogenic hypersoft sets. This paper comprises of the definition, classification of concentric plithogenic hypergraphs, extended hypersoft sets, extended concentric plithogenic hypergraphs and it throws light on its application. Concentric Plithogenic hypergraphs will certainly open the new frontiers of hypergraphs and this will undoubtedly bridge hypersoft sets and hypergraphs.																	2331-6055	2331-608X					2020	33						78	91															
J								Systems of Neutrosophic Linear Equations	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; Neutrosophic number; Neutrosophic linear equation; Neutrosophic linear system; Embedding method	FUZZY SYSTEM; SETS	In the present paper, for first time, a System of Neutrosophic Linear Equations (SNLE) is investigated based on the embedding approach. To this end, the (alpha,beta,gamma)-cut is used for transformation of SNLE into a crisp linear system. Furthermore, the existence of a neutrosophic solution to n x n linear system is proved in details and a computational procedure for solving the SNLE is designed. Finally, numerical experiments are presented to show the reliability and efficiency of the method.																	2331-6055	2331-608X					2020	33						92	104															
J								The Neutrosophic Time Series-Study Its Models (Linear-Logarithmic) and test the Coefficients Significance of Its linear model	NEUTROSOPHIC SETS AND SYSTEMS										Time Series; Neutrosophic logic; Neutrosophic Time Series; the linear model of the Neutrosophic time series; the significant of coefficients to the Neutrosophic linear model; The logarithmic model of the Neutrosophic time series		In this paper, we present the Neutrosophic time series by studying the classical time series within the framework of the Neutrosophic logic. (Logic established by the American philosopher and mathematician Florentin Smarandache presented it as a generalization of fuzzy logic, especially intuitionistic fuzzy logic). As an extension of this, A.A. Salama presented the theory of Neutrosophic crisp sets as a generalization of crisp sets theory. This study enables us to deal with all the time series values whether it is specified or not specified, we present the linear model for the Neutrosophic time series, and we test the significant of its coefficient based on Student's distribution. We present an example in which we pave the Neutrosophic time series according to the linear model, test the significant of its coefficient, and show how to deal with the unspecified values of the time series. Then we present the logarithmic model of the Neutrosophic time series. We conclude that the existence of indeterminacy in the matter we cannot ignored because it actually affects the estimated values of the time series and thus affects the prediction of the future of the series.																	2331-6055	2331-608X					2020	33						105	115															
J								Neutrosophic Triplet Partial g - Metric Spaces	NEUTROSOPHIC SETS AND SYSTEMS										g - metric space; neutrosophic triplet set; neutrosophic triplet metric space; neutrosophic triplet g - metric space; neutrosophic triplet partial g - metric space	SETS; OPERATOR	In this chapter, neutrosophic triplet partial g - metric spaces are obtained. Then, some definitions and examples are given for neutrosophic triplet partial g - metric space. Based on these definitions, new theorems are given and proved. In addition, it is shown that neutrosophic triplet partial g - metric spaces are different from the classical g - metric spaces, neutrosophic triplet metric spaces. Thus, we add a new structure in neutrosophic triplet theory. Also, thanks to neutrosophic triplet partial g - metric space, researchers can obtain new fixed point theorems for neutrosophic triplet theory.																	2331-6055	2331-608X					2020	33						116	+															
J								A Note on Neutrosophic Bitopological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Closed set; Neutrosophic Open set; (tau(i), tau(j))- N-Interior; (tau(i), tau(j)) - N-Closure; (tau(i), tau(j) )- N- Boundary; Neutrosophic Bitopological Space	FUZZY; TOPOLOGY	In this paper, we have introduced the idea on neutrosophic bitopological space and studied its properties with examples. We have defined several definitions of neutrosophic interior, closure and boundary also we have studied all of its properties.																	2331-6055	2331-608X					2020	33						134	144															
J								Neutrosophic Fuzzy Soft BCK-submodules	NEUTROSOPHIC SETS AND SYSTEMS										BCK-algebras; BCK-modules; soft sets; fuzzy soft sets; neutrosophic sets; neutrosophic soft sets; neutrosophic fuzzy soft BCK-submodules		The target of this study is to apply the notion of neutrosophic soft sets to the theory of BCK-modules by introducing the notion of neutrosophic fuzzy soft BCK-submodules and deriving their basic properties. Also, (alpha, beta, gamma)-soft top of neutrosophic fuzzy soft sets in BCK-modules is presented. The concept of Cartesian product of neutrosophic fuzzy soft BCK-submodules is defined and some results are investigated. Finally, an application of neutrosophic fuzzy soft sets in decision making is investigated and an example demonstrating the successfully application of this method is provided.																	2331-6055	2331-608X					2020	33						145	156															
J								Subtraction operational aggregation operators of simplified neutrosophic numbers and their multi-attribute decision making approach	NEUTROSOPHIC SETS AND SYSTEMS										Decision making; Simplified neutrosophic set; Subtraction operation; Subtraction operational aggregation operator	SIMILARITY MEASURES; CROSS-ENTROPY; SETS	The simplified form of a neutrosophic set (NS) was introduced as the simplified NS (S-NS) containing an interval-valued NS (IV-NS) and a single-valued NS (SV-NS) when its truth, indeterminacy and falsity membership degrees are constrained in the real standard interval [0, 1] for the convenience of actual applications. Then, Ye presented subtraction operations of simplified neutrosophic numbers (S-NNs), containing the subtraction operations of interval-valued neutrosophic numbers (IV-NNs) and single-valued neutrosophic numbers (SV-NNs) in S-NN setting. However, the subtraction operations of S-NSs lack actual applications in current research. Since simplified neutrosophic aggregation operators are one of critical mathematical tools in decision making (DM) applications, they have been not investigated so far. Regarding the subtraction operations of S-NNs (SV-NNs and IV-NNs), this work proposes an IV-NN subtraction operational weighted arithmetic averaging (IV-NNSOWAA) operator and a SV-NN subtraction operational weighted arithmetic averaging (SV-NNSOWAA) operator as a necessary complement to existing aggregation operators of S-NNs to aggregate S-NNs (SV-NNs and IV-NNs). Then, a DM approach is developed by means of the SV-NNSOWAA and IV-NNSOWAA operators. Finally, an illustrative example is presented to indicate the applicability and effectiveness of the developed approach.																	2331-6055	2331-608X					2020	33						157	168															
J								Neutrosophic Quadruple Algebraic Codes over Z(2) and their Properties	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Quadruples; NQ-vector spaces; NQ-groups; Neutrosophic Quadruple Algebraic codes (NQ-algebraic codes); Dual NQ-algebraic codes; orthogonal NQ- algebraic codes; NQ generator matrix; parity check matrix; self dual NQ algebraic codes		In this paper we for the first time develop, define and describe a new class of algebraic codes using Neutrosophic Quadruples which uses the notion of known value, and three unknown triplets (T, I, F) where T is the truth value, I is the indeterminate and F is the false value. Using this Neutrosophic Quadruples several researchers have built groups, NQ-semigroups, NQ-vector spaces and NQ-linear algebras. However, so far NQ algebraic codes have not been developed or defined. These NQ-codes have some peculiar properties like the number of message symbols are always fixed as 4-tuples, that is why we call them as Neutrosophic Quadruple codes. Here only the check symbols can vary according to the wishes of the researchers. Further we find conditions for two NQ-Algebraic codewords to be orthogonal. In this paper we study these NQ codes only over the field Z(2). However, it can be carried out as a matter of routine in case of any field Z(p) of characteristics p.																	2331-6055	2331-608X					2020	33						169	182															
J								Multi-Polar Neutrosophic Soft Sets with Application in Medical Diagnosis and Decision-Making	NEUTROSOPHIC SETS AND SYSTEMS										mNS Set; Operators on mNS; Properties; Distance and Similarity Measure; Medical Diagnosis; Decision-Making	SIMILARITY MEASURES; HYPERSOFT SET; TOPSIS	A Similarity measure for Neutrosophic function performs a fundamental role in tackling the problems that include blurred and hazed information but is not able to handle the fuzziness and vagueness of the problems which have numerous information. The objective of this research paper is to generalize neutrosophic soft set to the multi-polar neutrosophic soft set (mNS set), aggregation operators and their properties on mNS sets. It also discusses the distance-based similarity measures that rely on between two mNS sets. It explains with the help of examples that the intended similarity measures of mNS sets are applicable in the field of medical diagnosis and decision-making problem for selection of lecturer in universities. Eventually, this proposed method is concluded as an algorithm in the application.																	2331-6055	2331-608X					2020	33						183	207															
J								Introduction to Plithogenic Hypersoft Subgroup	NEUTROSOPHIC SETS AND SYSTEMS										Hypersoft set; Plithogenic set; Plithogenic hypersoft set; Plithogenic hypersoft subgroup	SOFT SET-THEORY; NEUTROSOPHIC SETS; FUZZY; INFORMATION	In this article, some essential aspects of plithogenic hypersoft algebraic structures have been analyzed. Here the notions of plithogenic hypersoft subgroups i.e. plithogenic fuzzy hypersoft subgroup, plithogenic intuitionistic fuzzy hypersoft subgroup, plithogenic neutrosophic hypersoft subgroup have been introduced and studied. For doi ng that we have redefined the notions of plithogenic crisp hypersoft set, plithogenic fuzzy hypersoft set, plithogenic intuitionistic fuzzy hypersoft set, and plithogenic neutrosophic hypersoft set and also given their graphical illustrations. Furthermore, by introducing function in different plithogenic hypersoft environments, some homomorphic properties of plithogenic hypersoft subgroups have been analyzed.																	2331-6055	2331-608X					2020	33						208	233															
J								Neutrosophic Cubic Hamacher Aggregation Operators and Their Applications in Decision Making	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; Neutrosophic cubic set; Score function; Accuracy function; Hamacher operations; Decision making	SIMILARITY	In this paper, firstly, novel approaches of score function and accuracy function are introduced to achieve more practical and convincing comparison results of two neutrosophic cubic values. Furthermore, the neutrosophic cubic Hamacher weighted averaging operator and the neutrosophic cubic Hamacher weighted geometric operator are developed to aggregate neutrosophic cubic values. Some desirable properties of these operators such as idempotency, monotonicity and boundedness are discussed. To deal with the multi-criteria decision making problems in which attribute values take the form of the neutrosophic cubic elements, the decision making algorithms based on some Hamacher aggregation operators, which are extensions of the algebraic aggregation operators and Einstein aggregation operators, are constructed. Finally, the illustrative examples and comparisons are given to verify the proposed algorithms and to demonstrate their practicality and effectiveness.																	2331-6055	2331-608X					2020	33						234	255															
J								Plithogenic Soft Set	NEUTROSOPHIC SETS AND SYSTEMS										Soft set; Neutrosophic set; Neutrosophic soft set; Plithogenic set; Plithogenic soft set	INTUITIONISTIC FUZZY	In 1995, Smarandache initiated the theory of neutrosophic set as new mathematical tool for handling problems involving imprecise, indeterminacy, and inconsistent data. Molodtsov initiated the theory of soft set as a new mathematical tool for dealing with uncertainties, which traditional mathematical tools cannot handle. He has showed several applications of this theory for solving many practical problems in economics, engineering, social science, medical science, etc. In 2017 Smarandache initiated the theory of Plithogenic Set and their properties. He also generalized the soft set to the hypersoft set by transforming the function F into a multi-attribute function and introduced the hybrids of Crisp, Fuzzy, Intuitionistic Fuzzy, Neutrosophic, and Plithogenic Hypersoft Set. In this research, for the first time we define the concept of Plithogenic soft set, and give it some generalizations and study some of its operations. Furthermore, We give examples for these concepts and operations. Finally, the similarities between two Plithogenic soft sets are also given.																	2331-6055	2331-608X					2020	33						256	274															
J								Structures on Doubt Neutrosophic Ideals of BCK/BCI-Algebras under (S, T)-Norms	NEUTROSOPHIC SETS AND SYSTEMS										BCK/BCI-algebra; doubt neutrosophic subalgebra (ideal); (S, T)-normed doubt neutrosophic subalgebra (ideal)	FUZZY H-IDEALS; SUBALGEBRAS	Smarandache implemented the idea of neutrosophic set theory as a method for dealing undetermined data. Neutrosophic set theory is commonly used in various algebric structures, such as groups, rings and BCK/BCI-algebras. At present, there exist no results on doubt neutrosophic ideals of BCK/BCI-algebras using t-conorm and t-norm. First, the notions of (S, T)- normed doubt neutrosophic subalgebras and ideals of BCK/BCI-algebras are introduced and the characteristic properties are described. Then, images and preimages of (S, T)- normed doubt neutrosophic ideals under homomorphism are considered. Moreover, the direct product and (S, T)- product of (S, T)- normed doubt neutrosophic ideals of BCK/BCI-algebras are also discussed.																	2331-6055	2331-608X					2020	33						275	289															
J								Extension of HyperGraph to n-SuperHyperGraph and to Plithogenic n-SuperHyperGraph, and Extension of HyperAlgebra to n-ary (Classical-/Neutro-/Anti-) HyperAlgebra	NEUTROSOPHIC SETS AND SYSTEMS										n-Power Set of a Set; n-SuperHyperGraph (n-SHG); n-SHG-vertex; n-SHG-edge; Plithogenic n-SuperHyperGraph; n-ary HyperOperation; n-ary HyperAxiom; n-ary HyperAlgebra; n-ary NeutroHyperOperation; n-ary NeutroHyperAxiom; n-ary NeutroHyperAlgebra; n-ary AntiHyperOperation; n-ary AntiHyperAxiom; n-ary AntiHyperAlgebra		We recall and improve our 2019 concepts of n-Power Set of a Set, n-SuperHyperGraph, Plithogenic n-SuperHyperGraph, and n-ary HyperAlgebra, n-ary NeutroHyperAlgebra, n-ary AntiHyperAlgebra respectively, and we present several properties and examples connected with the real world.																	2331-6055	2331-608X					2020	33						290	296															
J								Neutrosophic Triplet Partial Bipolar Metric Spaces	NEUTROSOPHIC SETS AND SYSTEMS										triplet set; neutrosophic triplet metric space; bipolar metric space; neutrosophic triplet bipolar metric space; neutrosophic triplet partial metric space; neutrosophic triplet partial bipolar metric	SETS; OPERATOR	In this article, neutrosophic triplet partial bipolar metric spaces are obtained. Then some definitions and examples are given for neutrosophic triplet partial bipolar metric space. Based on these definitions, new theorems are given and proved. In addition, neutrosophic triplet partial bipolar metric spaces have been shown to be different from classical partial metric space, neutrosophic triplet partial metric space and neutrosophic triplet metric space. Thus, we add a new structure in neutrosophic triplet theory.																	2331-6055	2331-608X					2020	33						297	313															
J								The Neutrosophic Triplet of BI-algebras	NEUTROSOPHIC SETS AND SYSTEMS										BI-algebra; Neutro- BI -algebra; sub-Neutro- BI -algebra; Anti- BI -algebra; sub-Anti-BI-algebra; Neutrosophic Triplet of BI-algebra		In this paper, the concepts of a Neutro-BI-algebra and Anti-BI-algebra are introduced, and some related properties are investigated. We show that the class of Neutro-BI-algebra is an alternative of the class of BI-algebras.																	2331-6055	2331-608X					2020	33						314	322															
J								A Novel Plithogenic MCDM Framework for Evaluating the Performance of IoT Based Supply Chain	NEUTROSOPHIC SETS AND SYSTEMS										supply chain management (SCM); Internet of Things (IoT); Multi-criteria decision-making (MCDM); VIKOR method; BWM; Plithogenic set	DECISION-MAKING APPROACH; INTERNET; MODEL	The Internet of Things (IoT) is used in the Supply chain management (SCM) systems to respond to the globalization of complex and dynamic markets and competitiveness in various supply chain scopes. Despite the current buzz about IoT and its role in the supply chain, there is not enough empirical data or extensive expertise to guide its implementation. Therefore, this paper addresses the ambiguity of assessing the performance of the IoT based supply chain by integrating plithogenic set with both Best-Worst (BWM) and Vlse Kriterijumska Optimizacija Kompromisno Resenje (VIKOR) methods in a decision-making framework tailored for this field. The framework is based on 23 criteria that measure different aspects of the performance. The performance of the framework is assessed according to the plithogenic set theory and to the neutrosophic set theory using a case study of comparing the performance of IoT implantation with the SC of five e-commerce companies using three experts. The case study shows that the proposed framework has more consideration of the contradiction degree of each criteria to improve the accuracy of the evaluation results.																	2331-6055	2331-608X					2020	33						323	341															
J								Abstract argumentation and (optimal) stable marriage problems	ARGUMENT & COMPUTATION										Abstract Argumentation; Stable Marriage; optimality criteria	RELAXATION; ALGORITHMS; STABILITY; DEFENSE	In his pioneering work on Abstract Argumentation, P.M. Dung set a wide scenario by connecting stable models in Logic and Game Theory to simple Abstract Argumentation Frameworks (AAF), which are essentially directed graphs in which arguments are represented as nodes, and the attack relation is represented by arrows. From such abstraction and simplicity, it is possible to capture important properties in many different fields. The Stable Marriage (SM) problem is exactly one of such representable problems. Given two sets of individuals partitioned into men and women, a matching is stable when there does not exist any man-woman match by which both man and woman would be individually better off than they are with the person to which they are currently matched. Stable matchings correspond to stable extensions if an AAF is correctly generated from the given SM problem. In this paper we elaborate on the original formulation by Dung, by also encoding SM problems with ties and incomplete preference lists into AAFs. Moreover, we use Weighted Abstract Argumentation to represent optimality criteria in the optimal extension of SM problems, where some matchings are better than others: criteria may consider only the preference of either men, or women, or a more global view obtained by combining the preferences of both.																	1946-2166	1946-2174					2020	11	1-2					15	40		10.3233/AAC-190474													
J								Logical theories and abstract argumentation: A survey of existing works	ARGUMENT & COMPUTATION										Abstract argumentation; logics	QBF-BASED FORMALIZATION; SEMANTICS; FRAMEWORKS; SUPPORT; GRAPHS; ATTACK	In 1995, in his seminal paper introducing the abstract argumentation framework, Dung has also established the first relationship between this framework and a logical framework (in this case: logic programming). Since that time, a lot of work have pursued this path, proposing different definitions, uses and exhibiting distinct relationships between argumentation and logic. In this paper, we present a survey of existing works about this topic and more especially those that address the following question: "How logic has been used for capturing various aspects or parts of Dung's argumentation". This survey covers many different approaches but is not intended to be totally exhaustive due to the huge quantity of papers in this scope. Moreover, due to the fact that each approach has its own specificities, sometimes antagonistic with the other approaches, and is also justified by its own context of definition or use, the aim of this survey is not to identify one approach as being better than another.																	1946-2166	1946-2174					2020	11	1-2					41	102		10.3233/AAC-190476													
J								Similarity notions in bipolar abstract argumentation	ARGUMENT & COMPUTATION										Bipolar Argumentation; similarity; cohesion and controversy values	ACCEPTABILITY; COMBINATION; RETRIEVAL	The notion of similarity has been studied in many areas of Computer Science; in a general sense, this concept is defined to provide a measure of the semantic equivalence between two pieces of knowledge, expressing how "close" their meaning can be regarded. In this work, we study similarity as a tool useful to improve the representation of arguments, the interpretation of the relations between arguments, and the semantic evaluation associated with the arguments in the argumentative process. In this direction, we present a novel mechanism to determine the similarity between two arguments based on descriptors representing particular aspects associated with these arguments. This mechanism involves a comparison process influenced by the context in which the process develops, where this context provides the relevant aspects that need to be analyzed in the application domain. Then, we use this similarity measure as a quantity to compute the result of attacks and supports in the argumentation process. These valuations, applied to a Bipolar Argumentation Frameworks, allowed us to refine the argument relations, providing the tools to establish a family of new argumentation semantics that considers the similarity between arguments as a crucial part for the argumentation process.																	1946-2166	1946-2174					2020	11	1-2					103	149		10.3233/AAC-190479													
J								Structural constraints for dynamic operators in abstract argumentation	ARGUMENT & COMPUTATION										Abstract argumentation; dynamics of argumentation; argumentation frameworks; abstract dialectical frameworks; structured argumentation; computational complexity; constraints; answer set programming	DIALECTICAL FRAMEWORKS; LOGIC; SEMANTICS; MODEL; BIPOLARITY; COMPLEXITY; REVISION; ACCOUNT; DIAMOND; SUPPORT	Many recent studies of dynamics in formal argumentation within AI focus on the well-known formalism of Dung's argumentation frameworks (AFs). Despite the usefulness of AFs in many areas of argumentation, their abstract notion of arguments creates a barrier for operators that modify a given AF, e.g., in the case that dependencies between arguments have been abstracted away that are important for subsequent modifications. In this paper we aim to support development of dynamic operators on formal models in abstract argumentation by providing constraints imposed on the modification of the structure that can be used to incorporate information that has been abstracted away. Towards a broad reach, we base our results on the general formalism of abstract dialectical frameworks (ADFs) in abstract argumentation. To show applicability, we present two cases studies that adapt an existing extension enforcement operator that modifies AFs: in the first case study, we show how to utilize constraints in order to obtain an enforcement operator on ADFs that is allowed to only add support relations between arguments, and in the second case study we show how an enforcement operator on AFs can be defined that respects dependencies between arguments. We show feasibility of our approach by studying the complexity of the proposed structural constraints and the operators arising from the case studies, and by an experimental evaluation of an answer set programming (ASP) implementation of the enforcement operator based on supports.																	1946-2166	1946-2174					2020	11	1-2					151	190		10.3233/AAC-190471													
J								Investigating subclasses of abstract dialectical frameworks	ARGUMENT & COMPUTATION										Abstract argumentation; abstract dialectical frameworks; acyclic frameworks; symmetric frameworks; expressiveness	COMPLEXITY; SEMANTICS	dialectical frameworks (ADFs) are generalizations of Dung argumentation frameworks where arbitrary relationships among arguments can be formalized. This additional expressibility comes with the price of higher computational complexity, thus an understanding of potentially easier subclasses is essential. Compared to Dung argumentation frameworks, where several subclasses such as acyclic and symmetric frameworks are well understood, there has been no in-depth analysis for ADFs in such direction yet (with the notable exception of bipolar ADFs). In this work, we introduce certain subclasses of ADFs and investigate their properties. In particular, we show that for acyclic ADFs, the different semantics coincide. On the other hand, we show that the concept of symmetry is less powerful for ADFs and further restrictions are required to achieve results that are similar to the known ones for Dung's frameworks. A particular such subclass (support-free symmetric ADFs) turns out to be closely related to argumentation frameworks with collective attacks (SETAFs); we investigate this relation in detail and obtain as a by-product that even for SETAFs symmetry is less powerful than for AFs. We also discuss the role of odd-length cycles in the subclasses we have introduced. Finally, we analyse the expressiveness of the ADF subclasses we introduce in terms of signatures.																	1946-2166	1946-2174					2020	11	1-2					191	219		10.3233/AAC-190481													
J								Before and after Dung: Argumentation in AI and Law	ARGUMENT & COMPUTATION										Abstract argumentation; legal argumentation; argumentation frameworks	ABSTRACT ARGUMENTATION; ASPIC(+) FRAMEWORK; CARNEADES; PERSUASION; MODEL; HAYASHI	Dung's abstract argumentation frameworks have had a very significant role in the rise in interest in argumentation throughout this century. In this paper we will explore the impact of this seminal idea on a specific application domain, AI and Law. Argumentation is central to legal reasoning and there had been a considerable amount of work on it in AI and Law before Dung's paper. It had, however, been rather fragmented. We argue that the abstract argumentation frameworks had a unifying effect by offering a means of relating previously diverse work. We also discuss how the particular demands of legal systems have led to developments building on the basic notions of abstract argumentation.																	1946-2166	1946-2174					2020	11	1-2					221	238		10.3233/AAC-190477													
J								Electrocardiogram classification of lead convolutional neural network based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Electrocardiogram; 12 lead; convolutional neural network; multi lead filter; residual learning	FEATURE-SELECTION; SVM	With the development of society, health has attracted more and more attention. Heart disease is a common and frequently occurring disease, and it is fatal. Rapid and timely diagnosis and treatment of heart disease is very important. Electrocardiogram (ECG) reflects human heart health and is widely used in heart disease examination. Existing methods depending on doctors' personal experience and diagnostic level are time-consuming and inefficient. Therefore, a classification method that can automatically analyze ECG is required. Aiming at the classification of 12-lead ECG, based on the good performance of convolution neural network, this paper proposes a method of ECG classification based on lead convolution neural network, which can effectively and accurately detect, recognize and classify ECG. First, the image features are extracted after the ECG is preprocessed, and then using the fuzzy set reduces the extracted ECG image features. Then, residual learning is used to optimize the convolutional neural network, and in order to ensure that the network is easy to train and fast convergence, a random parameter initialization method is introduced to achieve better classification results. The simulation results show that the proposed multi-lead filtering algorithm reduces the loss of useful information while eliminating noise; at the same time, the convolution neural network can effectively and accurately classify ECG images; and the introduction of residual network can improve the classification effect.																	1064-1246	1875-8967					2020	38	4			SI		3539	3548		10.3233/JIFS-179576													
J								Blood color detection of color ultrasound images based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy algorithm; color ultrasound image		With the establishment and development of technologies and theories such as computer technology, image processing, pattern recognition and artificial intelligence, image analysis systems have gradually become one of the methods of automatic quantitative analysis and testing in the medical field. However, the current technology is limited to the objectivity and comprehensiveness of blood edge detection, and no detection method with high accuracy can be found. In order to accurately and effectively detect the blood color of the color ultrasound image, this paper classifies the image feature extraction method, and simulates the classical differential algorithm, mathematical morphology algorithm and fuzzy Pal.King. In the following, the classical canny algorithm and its improved algorithm and the improved fuzzy Pal.King algorithm are introduced in detail. Finally, the simulation results are obtained. Among the indicators tested, DD has the highest accuracy. In the curve analysis, the FI value of FIB was > 0.05, the area under the curve of FDP was 67.9%, the sensitivity was 64%, and the specificity was 59%. In this paper, the quantitative analysis of the image feature extraction effect is given by the calculation results, and the subjective and objective unity is achieved. At the same time, the improved algorithm proposed in this paper is applied to the evaluation system for analysis and summary, and the results obtained are consistent with the theoretical analysis.																	1064-1246	1875-8967					2020	38	4			SI		3549	3556		10.3233/JIFS-179577													
J								Image edge detection algorithm based on fuzzy set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS																													1064-1246	1875-8967					2020	38	4			SI		3557	3566															
J								Remote sensing image classification based on RBF neural network based on fuzzy C-means clustering algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Remote sensing image classification; fuzzy C-means clustering algorithm; Kappa coefficient; data set; RBP neural network		With the development of modern remote sensing technology, remote sensing images have become one of the powerful tools for people to understand the Earth and its surroundings. However, there is currently no good classification algorithm that can accurately classify images. In order to accurately classify remote sensing images, this paper studies the content of the article by using fuzzy C-means clustering algorithm and radial basis neural network (RBF). The classification accuracy of SIRI-WHU dataset was analyzed by using the classification accuracy evaluation index such as overall accuracy and Kappa coefficient. The Kappa coefficient of vegetation classification in SIRI-WHU dataset was 0.9678, and the overall accuracy reached 97.18%. According to the classification problem of remote sensing image, according to the characteristics of remote sensing image, the improved model Alex Net-10-FCM is used to classify the remote sensing image dataset, and very high classification accuracy is obtained.																	1064-1246	1875-8967					2020	38	4			SI		3567	3574		10.3233/JIFS-179579													
J								FCM fuzzy clustering image segmentation algorithm based on fractional particle swarm optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fractional particle swarm; maximum entropy; c-means clustering algorithm; image segmentation	VARIANT	The purpose of image segmentation is to select the target region from the existing image, which is the core technology for image understanding, description and analysis. When faced with some complicated problems, the image segmentation effect of the traditional method is often unsatisfactory. As a branch of the swarm intelligence optimization algorithm, Particle Swarm Optimization (PSO) provides a new power and direction for the development of image segmentation. However, the algorithm has a large probability of loss of particle diversity in the late stage, which makes the algorithm converge prematurely. Therefore, the purpose of this paper is to improve the problem existing in the PSO algorithm and apply the improved algorithm in image segmentation. In this paper, the whole population of PSO algorithm is divided into multiple sub-populations and co-evolution. The mutation operation from the genetic algorithm is introduced at the same time. The worst sub-population is mutated according to the mutation probability. The larger inertia factor is selected to speed the particles. Update, and then carry out simulation experiments on some classical test functions. Finally, combined with the improved PSO algorithm and fuzzy C-means clustering algorithm (FCM), the fuzzy clustering validity index is introduced, and the blood cell image is segmented by the algorithm. The experimental results show that the algorithm can find a reasonable number of cluster center segmentation categories and efficiently perform adaptive segmentation of images.																	1064-1246	1875-8967					2020	38	4			SI		3575	3584		10.3233/JIFS-179580													
J								Kalman tracking algorithm of ping-pong robot based on fuzzy real-time image	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy ecognition; ping-pong robot; real-time tracking; image recognition; Kalman filtering	FILTER; SYSTEM	The moving target tracking is a very important in computer vision research topic and is widely used, it is of great research significance, has been widely used, and involved in image recognition, production automation, intelligent pattern recognition, artificial intelligence, weather information, and other fields, but in some high speed movement, such as target under complex background target trajectory when not much target tracking is still more difficult. This paper mainly studies the Kalman tracking algorithm of ping-pong robot based on fuzzy real-time image. For table tennis high-speed motion blurred images, air resistance and the camera imaging distortion caused by factors such as the error problem, puts forward an adaptive measurement covariance discrete Kalman trajectory estimation algorithm. The algorithm with dynamic adjustment of measuring the size of the covariance, has realized the accurate tracking of the target motion trajectory, and further laid the groundwork for table tennis balls prediction and hitting arm. The experimental results show that the algorithm can effectively overcome the interference of measurement noise and data loss and give excellent tracking results when the image acquisition rate is higher than 70 frames/s and the table tennis speed is higher than 5 m/s.																	1064-1246	1875-8967					2020	38	4			SI		3585	3594		10.3233/JIFS-179581													
J								SAR image change detection method based on intuitionistic fuzzy C -means clustering algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Algorithms; models; images; SAR; change	GROUP DECISION-MAKING; SET	OBJECTIVE: The purpose of this study is to realize the precise detection of Synthetic Aperture Radar (SAR) image changes. METHODS: In this study, an intuitionistic fuzzy C-means clustering algorithm is used to accurately detect the target changes in SAR images. The change of SAR image is detected by the constructed intuitionistic fuzzy C-means clustering algorithm. Then, the effect of intuitionistic fuzzy C-means clustering algorithm, block principal component analysis (PCA) and logarithmic ratio method is compared and analyzed in the aspects of stability, accuracy, image extraction, restoration, error and work efficiency of the algorithm. RESULTS: Compared with block PCA and logarithmic ratio methods, intuitionistic fuzzy C-means clustering algorithm has obvious advantages in stability, with standard deviation of 0.010 and other two algorithms of 0.014 and 0.017. In terms of detection accuracy and error, the algorithm in this study also has a good performance, and the detection accuracy can reach 92.4%. In addition, the intuitionistic fuzzy C-means clustering algorithm is clear and efficient for SAR image target extraction and restoration. Compared with the other two algorithms, the algorithm in this study improves by at least 20% in operation speed. There is no significant difference in the detection results of the proposed algorithm for SAR images with different targets, such as objects, people, geographical environment, etc. CONCLUSION: In this study, based on intuitionistic fuzzy C-means clustering algorithm, target changes in SAR images are detected, and the operation of the algorithm is studied. The algorithm used in this study shows a relatively comprehensive and good result, and also shows that the algorithm is a comprehensive result, which requires a good operation at many levels. This research greatly improves the recognition of intuitionistic fuzzy C-means clustering algorithm and SAR image.																	1064-1246	1875-8967					2020	38	4			SI		3595	3604		10.3233/JIFS-179582													
J								The image segmentation algorithm of colorimetric sensor array based on fuzzy C-means clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy C-means clustering; image segmentation; colorimetric sensor array		The conventional image segmentation algorithm of the colorimetric sensor array is inefficient and vulnerable to the interferences of the environment. Therefore, in order to improve the conventional algorithm, an image segmentation algorithm based on fuzzy C-means clustering (FCM) algorithm is proposed in this study. Through the information of the gray-scale distribution histogram, the proposed algorithm divides the different wave-peak regions, where the pixels are relatively concentrated, into different clusters to determine the number of clusters. In addition, the gray values of these clusters are calculated to determine the initial cluster center. Next, the calculation results are used as the input of the FCM algorithm to complete the clustering segmentation of FCM. The research results show that the algorithm proposed in this study avoids the human participations of the traditional FCM algorithm. Also, based on the original algorithm, the proposed algorithm can reduce the calculation iterations, thereby improving the computational efficiency and obtaining the number of clusters with reference significance. As the results indicate, the proposed algorithm can better describe the fuzzy information in the image, thereby avoiding the problem of classifying the pixels into one category. Besides, the exponential function is used to control the influence weight of the neighboring pixels, and the adaptive weighting of the pixel grayscale is realized to improve the calculation accuracy of pixel grayscale and realize the image segmentation.																	1064-1246	1875-8967					2020	38	4			SI		3605	3613		10.3233/JIFS-179583													
J								The role of antibiotics in the preparation of antitumor drugs under fuzzy system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Antibiotic; membership function; fuzzy system; genetic algorithm		In order to explore the application effect of antibiotics in the preparation of antitumor drugs, the basic theories of fuzzy system and genetic algorithm are introduced in this study. The characteristics of Gaussian functions, Cauchy functions and membership functions of Gamma-function are compared, and the optimal membership function is chosen to construct the fuzzy system. The fuzzy rule editor is used to write the fuzzy rules, and the corresponding mathematical model is constructed. The crossover rate and mutation rate in the model are set. Based on the optimal reserved genetic algorithm, the optimal fuzzy rules are searched and the operation flow of fuzzy rules is determined. According to the actual situation, the crossover rule and mutation rule are determined and the fuzzy genetic algorithm is implemented. The results show that the gaussian membership function has lower error than other functions. In addition, the introduction of genetic algorithm model constructed by fuzzy system can effectively improve the accuracy of model prediction.																	1064-1246	1875-8967					2020	38	4			SI		3615	3624		10.3233/JIFS-179584													
J								Examination on image segmentation method of ischemic optic neuropathy based on fuzzy clustering theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Medical image segmentation; fuzzy c-means; kernel method; fuzzy clustering algorithm; spatial information		The purpose is to use medical image processing technology to avoid the influence of subjective factors through the mutual penetration and development of clinical medicine and computer science. Can diagnose the degree of malignancy of ischemic optic neuropathy as quickly as possible, and can take an effective treatment plan for the patient early. Therefore, image segmentation of ischemic optic neuropathy based on fuzzy clustering theory is particularly important for the diagnosis of disease in patients. This paper analyzes the research status of medical image segmentation at home and abroad and the development trend in this aspect in China. Discussed the fuzzy C-means clustering (FCM) image segmentation algorithm in depth, studied the effects of iterative cutoff error, initial clustering center, number of clustering categories and fuzzy weighted index on the practical application of the algorithm. At the same time, the traditional algorithm is not sensitive to the spatial information of the image, making the algorithm sensitive to noise. Firstly, introduced the spatial information of the image, and introduced the algorithm based on spatial information constraint, Based on the above description and based on the neighborhood properties described by the two-dimensional histogram, studied and proposed a relatively easy to understand multidimensional distance measurement method. That is, the two-dimensional pixel value and the neighborhood pixel value viewpoint that can be updated in the two-dimensional direction, by setting a clustering objective function, a clustering measurement method includes neighborhood information. Through the above two-dimensional image segmentation algorithm based on neighborhood spatial information, proposed an image segmentation algorithm for ischemic optic neuropathy of fuzzy kernel clustering theory combined with spatial information. The experimental results show that the proposed algorithm can show excellent results in ischemic neuropathy image segmentation, and the algorithm has faster convergence speed and higher classification accuracy. Experimental results of artificial images and actual images show that the algorithm has strong noise immunity and practicability.																	1064-1246	1875-8967					2020	38	4			SI		3625	3633		10.3233/JIFS-179585													
J								Application of fuzzy C-means clustering method in the analysis of severe medical images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Medical image processing; fuzzy C-means algorithm; clustering algorithm; medical image analysis	ALGORITHM	Medical image processing is an interdisciplinary subject of integrated medical imaging, mathematics, computer science and other disciplines. With high spatial resolution, high signal-to-noise ratio and high resolution of soft tissue, the technology can accurately locate the target areas of interest in medical images, thus providing useful information for clinicians to formulate disease treatment plans. These techniques include digital subtraction angiography, magnetic resonance imaging, computed tomography, ultrasound imaging and positron emission tomography. The purpose of this paper is to study the application of fuzzy C-means clustering in image analysis of critical medicine. This paper discusses the classification effect, clustering process, iteration times and running time of different algorithms, and the segmentation effect of different algorithms. By designing parameters and carrying out simulation experiments, the traditional clustering algorithm and improved local adaptive method are compared, and the problem of long coding time of traditional image compression algorithm is solved. The simulation results under the same working environment show that the coding speed of the algorithm is about five times faster than that of the traditional image compression algorithm without affecting the signal-to-noise ratio and compression rate, which proves the superiority of the algorithm.																	1064-1246	1875-8967					2020	38	4			SI		3635	3645		10.3233/JIFS-179586													
J								Gray image segmentation based on fuzzy c-means and artificial bee colony optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy clustering; c-means clustering; artificial bee colony; gray image segmentation	ALGORITHM	The regions obtained by image segmentation need to satisfy both the requirements of uniformity and connectivity. Image segmentation is the process of dividing an image into several specific regions. The result of image segmentation is a set of combinations covering the main feature areas of the whole image. The pixels in an area are similar to some or calculated characteristics, but there are obvious differences between adjacent areas. In this paper, a gray image segmentation algorithm based on fuzzy C-means combined with bee colony algorithm is proposed, which has strong optimization ability for multi-objective problems. By using the fuzzy membership function of the fuzzy C-means algorithm, the optimal clustering centers in the artificial bee colony optimization algorithm can be quickly calculated. It makes image segmentation faster and more accurate. The bee colony search algorithm is optimized and an effective local search algorithm is designed, it makes the bee colony converge to the optimal solution efficiently. Finally, the improved fuzzy C-means and artificial bee colony optimization algorithm are used to improve and optimize the seed region growth method. The multi-criteria are taken as the multi-objective optimization problem, and the segmentation results are finally obtained. Benefiting from our local search program and feature extraction in multi-color space, it makes the stability; efficiency and accuracy of image segmentation are higher.																	1064-1246	1875-8967					2020	38	4			SI		3647	3655		10.3233/JIFS-179587													
J								Intelligent fault diagnosis method of mechanical equipment based on fuzzy pattern recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fault diagnosis of mechanical equipment; fuzzy pattern recognition; convolutional neural network; fuzzy c-means clustering		With the rapid development of modern industry and science and technology, mechanical equipment has become larger, faster and more intelligent. In real life, there is no absolutely safe and reliable equipment, so it is impossible to require mechanical equipment not to break down in the operation process, and the working environment of mechanical equipment is complex and harsh, aging is serious, and breakdowns occur frequently. Research on effective intelligent fault detection methods has become a theoretical hot spot of current discipline research. Intelligent fault diagnosis of mechanical equipment is based on the algorithm to analyze the problems of equipment fault. In this paper, a fault detection model of mechanical equipment is proposed based on the method of fuzzy pattern recognition, and the fault detection is classified by the method of Fuzzy C-Means clustering. In this paper, the method of mechanical equipment fault detection based on Convolutional Neural Network is compared with the method proposed in this paper. The experimental results show that the model has good performance in fault detection and has strong practicability.																	1064-1246	1875-8967					2020	38	4			SI		3657	3664		10.3233/JIFS-179588													
J								Analytical results towards fuzzy error bound of indoor neighbor matching based positioning algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Indoor localization; fuzzy membership; environmental parameters; error bound; neighbor matching	LOCATION	In this paper, we derive out an analytical result towards fuzzy error bound of indoor Neighbor Matching based Positioning Algorithm (NMPA). First of all, in a typical Line-of-sight (LOS) environment, we utilize the fuzzy comprehensive evaluation approach to obtain the fuzzy membership of the target in fingerprint matching. Second, we present an analysis of the theoretical relationship between the positioning error of NMPA and different environmental parameters. Third, we work out the fuzzy closed form solution to the positioning error of NMPA concerning the size of environment, the spacing of Reference Points (RPs), number of neighbors, and positions of Access Points (APs). Finally, relevant experiments verify that the fuzzy error bound proposed in this paper can reflect the influence of different factors of the environment on the performance of the positioning system, thereby, the positioning accuracy can be improved and the deployment costs can be reduced by optimizing the environmental parameters.																	1064-1246	1875-8967					2020	38	4			SI		3675	3686		10.3233/JIFS-179590													
J								Indoor intrusion detection based on fuzzy membership-aided Dempster-Shaper theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Passive intrusion detection; indoor WLAN; fuzzy membership; Dempster-Shafer theory; weighted maximum likelihood and centroid method	LOCALIZATION; UNCERTAINTY; SYSTEM; MODE	At present, indoor intrusion detection technologies based on WLAN are widely applied to protect the privacy of users and have a robust anti-interference ability under the condition of the Non-line-of-sight (NLOS), which become the mainstream topics of domestic and foreign studies. Most of the existing researches rely on the signal strength to train heuristic models, while the relationship between intrusion targets and signal fluctuations is not explored fully. In this circumstance, this paper proposes a novel indoor intrusion detection method based on fuzzy membership degree and Dempster-Shafer Theory (DST). First of all, the correlation between WLAN signal fluctuation features and locations of intrusion targets are converted into DST mass function by fuzzy membership. Second, the reliability values from each MP are combined to select reliable reference positions by using the reliability combination rules in DST. Finally, the positions of the intrusion target are calculated based on the weighted maximum likelihood and centroid method. Finally, the related experimental results show that the proposed approach can not only ensure the high accuracy of intrusion detection but also obtain ideally accurate locations of the intrusion target.																	1064-1246	1875-8967					2020	38	4			SI		3687	3696		10.3233/JIFS-179591													
J								AVG comprehensive practice reform of digital media art based on fuzzy theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy theory; digital media art; AVG comprehensive practice; education	TALENT DEVELOPMENT; EDUCATION; POLICY	The digital media technology major is a multi-disciplinary and multi-disciplinary discipline. The cultivated talents should be based on applied talents. According to the traditional practice teaching methods, the disciplines are no longer suitable for the discipline. Colleges and universities strengthen the practice teaching reform of the digital media technology profession. It is a very important and necessary means. The purpose of this paper is to realize the evaluation of the reform effect of AVG comprehensive practice in digital media art through fuzzy theory. In this paper, the fuzzy comprehensive evaluation method is used to comprehensively evaluate the AVG practice of digital media art. Because the fuzzy comprehensive evaluation method is very restrictive to the index weight, it cannot be well adapted to the system. In this paper, the processing scheme of the index weight value of the fuzzy comprehensive evaluation method is interval data, so that the algorithm can be better applied in the system. In the determination of weight, this paper uses the improved entropy weight method to determine the weight of the index. By comparing with other algorithms that obtain the weight of the index, it can be concluded that using the improved entropy weight method to obtain the weight value can not only effectively reduce the external interference. Moreover, the weight value obtained can well reflect the importance of the indicator. The experimental results show that the dynamic variability of fuzzy set theory can satisfactorily meet the comprehensiveness and reliability of AVG comprehensive practice reform evaluation results. Therefore, the application of fuzzy set theory in the field of AVG comprehensive practice reform effect evaluation is reasonable and accurate.																	1064-1246	1875-8967					2020	38	4			SI		3697	3706		10.3233/JIFS-179592													
J								Night vehicle target recognition based on fuzzy particle filter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Night vehicle recognition; particle filter algorithm; nonlinear statistics; fuzzy clustering	SYSTEM	All along, the identification of night-driving safety car features is a major research direction in the field of intelligent traffic management, with a wide range of applications and development space, and these identification technologies include theoretical knowledge and important theoretical research in many fields. Due to the interference of lights and other light sources, the gray value of the background area also changes frequently. A common method during the day is to detect these background areas as moving vehicles, which greatly reduces the detection accuracy. The most reliable information at night is the headlights. If the light can be accurately detected and other sources are excluded, accurate detection can be performed and tracking accuracy can be guaranteed. Vehicle safety assisted driving technology is one of the main research directions of intelligent transportation systems. It mainly uses computer technology, sensor technology and communication technology to collect and analyze the state information of roads, other vehicles and drivers. Provide advice and warnings to the driver before reaching the danger, determine current traffic conditions and avoid traffic accidents in advance. This paper studies some problems of night vehicle target recognition and detection, mainly the division of target and background, and the classification and recognition of target extraction. To solve these problems, a particle filter algorithm is introduced to introduce nonlinear statistics. The fuzzy theory is introduced to classify the video processed by the particle filter algorithm. The target recognition is realized by the method, and the purpose of identifying the night vehicle target is achieved. Comparative experimental analysis shows that this method is more accurate and powerful than the common target recognition algorithm and can be applied to real scenes.																	1064-1246	1875-8967					2020	38	4			SI		3707	3716		10.3233/JIFS-179593													
J								Data mining method based on rough set and fuzzy neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; rough set; fuzzy logic; fuzzy neural network	PREDICTION	With the rapid development of database and Internet technologies, data collection and storage is possible. It is often impossible to correctly analyze the valuable information contained in the data, and it becomes more difficult to obtain valuable information. Therefore, it faces the status of "rich data and scarce knowledge". Traditional information processing technology can no longer meet the needs of reality. There is an urgent need for more capable and effective information processing skills to help us analyze the information we need from big data and guide us to make the right decisions. Data mining technology is born in the background. Data mining technology is one of the effective methods to solve rich data and improve lack of knowledge. It is also one of the main research topics in the field of information science. Related research and applications have greatly improved people's decision-making ability. It has been recognized as one of the extremes of data research and has a very broad application prospect. Large databases often contain redundant and unnecessary attributes for many search rules, so the ability to remove duplicate attributes can greatly improve the clarity of potential system knowledge and reduce the time complexity of finding rules. At the same time, it enables efficient operation and improved adaptability. Because the structure of the neural network is variable, it has strong self-organization, self-learning, nonlinearity and high fault tolerance, but the ability to express and interpret knowledge is very poor. The network parameters lack physical meaning and learning. Therefore, it has become an inevitable trend to form a fuzzy neural network combining the characteristics of the two. Therefore, exploring the organic combination between rough sets and fuzzy neural networks is undoubtedly of great significance for data mining technology research. This paper proposes a data mining method based on the combination of rough set and fuzzy neural network technology. Using the approximate set to discover the rules of the database rules, the initial structure of the fuzzy neural network is determined, and the training data is used to train the network. Since the fuzzy neural network thus constructed has a good topology of data distribution features from the beginning, the network scale can be greatly reduced and the network training speed can be improved.																	1064-1246	1875-8967					2020	38	4			SI		3717	3725		10.3233/JIFS-179594													
J								Web text data mining method based on Bayesian network with fuzzy algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy algorithm; Bayesian network; data mining; web text data mining	CLUSTERING-ALGORITHM	With the advent of Web 3.0 era, the number and complexity of Web pages in Bayesian networks have shown an explosive growth trend. Accompanying this is the geometric growth of information contained in Web pages. Web text data in Bayesian networks usually hide rich knowledge and rules of user value. However, due to the semi-structured, real-time and discrete characteristics of Web text data, it is difficult for users to obtain the knowledge they need directly from such complex data sets. The emergence of fuzzy mathematics provides a good research idea and method for solving such problems. It can use the idea of fuzzy mathematics to analyze the practical problems in text data. Therefore, how to effectively mine the Web text data information and knowledge that users really care about from Bayesian network, and present it in a way that users can understand, it is a very popular research topic at present. In this paper, we select the text of Bayesian network: microblog data for experiments. User data model of microblog is established by using relevant knowledge of fuzzy theory. The concept of fuzzy measure is introduced to calculate the non-additive measure value under the interaction relationship between the detection indicators. Determine the membership function relationship between the detection user and the text data, calculate the integral values of Choquet integral, Sugeno integral and Wang integral of the membership function under the non-additive measure, the final valuable web text data is judged by integral value. On the basis of the above research contents, the research results of Web text mining technology and fuzzy arithmetic mathematics are combined, design and implement information acquisition and analysis for Bayesian network community. The recall rate obtained by the experimental method in this paper is as low as 4%, and tends to be more stable.																	1064-1246	1875-8967					2020	38	4			SI		3727	3735		10.3233/JIFS-179595													
J								Fuzzy sets for data mining and recommendation algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; recommendation algorithm; fuzzy system; fuzzy set		A fuzzy system is a system that defines input, output, and state variables on a fuzzy set and is a generalization of a deterministic system. The fuzzy system begins at the macro level and covers the fuzzy features of human brain thinking. It has advantages in describing advanced knowledge. Fuzzy sets can mimic the comprehensive conclusions of people solving fuzzy information problems, which are difficult to solve by conventional mathematical methods, so computer applications can be extended to humanities, social sciences and complex systems. In this way, it can better solve nonlinear problems and is widely used in automatic control, decision analysis, time series signal processing, economic information systems, medical diagnostic systems, and earthquake prediction systems. This paper aims to study the data mining algorithm of fuzzy systems based on fuzzy sets. By using the powerful fuzzy data modeling function of fuzzy theory, it combines with other intelligent processing methods, and makes practical use in industrial life. By comparing the application of fuzzy set data mining and algorithm, it is found that in the application state, the economic benefits of the factory are improved by 36% and the production efficiency is improved by 25% under the application of data mining and algorithm. The research data shows that the data mining and recommendation algorithms of fuzzy sets are beneficial to the development and operation of the factory. The research results show that compared with the conventional production and processing plan, the technology uses fuzzy set theory to transform the fuzzy attributes, which is more advantageous in scientific and technical systems and algorithms with its scientificity, accuracy, innovation and extensiveness.																	1064-1246	1875-8967					2020	38	4			SI		3737	3745		10.3233/JIFS-179596													
J								Reliability analysis of high voltage electric system of pure electric passenger car based on polymorphic fuzzy fault tree	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Polymorphic fuzzy fault tree; eliability analysis		In recent years, with the rapid increase in the number of electric vehicles in China, the accidents caused by electrical system failures are also increasing year by year. Therefore, it is necessary to carry out reliability analysis on the electrical systems of electric vehicles. Traditional reliability analysis cannot be quantitatively evaluated and it is not possible to accurately diagnose multiple fault conditions of the system. Aiming at this problem, this paper combines fuzzy mathematics theory with fault tree analysis, and establishes a multi-state fuzzy fault tree to analyze the reliability of pure electric bus high-voltage electrical system, including qualitative analysis and quantitative analysis. The results show that the multi-state fuzzy fault tree reliability analysis method can accurately describe the various fault states of the high-voltage electric system of pure electric passenger car, and can quantitatively evaluate the reliability, which has great practical significance.																	1064-1246	1875-8967					2020	38	4			SI		3747	3754		10.3233/JIFS-179597													
J								The use of improved algorithm of adaptive neuro-fuzzy inference system in optimization of machining parameters	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Adaptive neuro-fuzzy reasoning system; machining; parameter optimization; machining error	MODELS	In order to effectively avoid the violent vibration in the process of mechanical processing and to achieve high efficiency and high precision machining of mechanical parts, the improved algorithm of adaptive neuro-fuzzy inference system is used to study the optimization of parameters in the process of side milling of mechanical parts, and an adaptive network structure is formed. It has the learning ability of artificial neural network and the expression ability of "if-then" of fuzzy reasoning system, which is a new prediction and control method. The results validate the applicability of the stability. The machined surface topography is measured and the effect of flutter on the surface topography is analyzed. The three-dimensional stability of milling provides a theoretical basis for the rational selection of milling parameters of mechanical parts, the realization of stable milling and the improvement of processing efficiency. Thus, the relationship between the radial depth of cut, the axial depth of cut and the spindle speed is established, and the contour of material removal rate is obtained. The corresponding spindle speed and radial shear depth are obtained when the material removal rate is maximum. The reasonable selection of machining parameters is carried out in the region near the maximum spindle speed with stability.																	1064-1246	1875-8967					2020	38	4			SI		3755	3764		10.3233/JIFS-179598													
J								Semantic ordering of English machine translation based on fuzzy theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Semantic ordering; machine translation; fuzzy theory; semantic parser; system performance		With the development and popularization of electronic computers and the Internet, the problem of language barriers has once again become prominent in the new era, and people are more in need of machine translation. However, there is currently no suitable method for effective semantic ordering of English machine translation. In order to better perform semantic ordering on English machine translation, the article combines fuzzy theory to construct an algorithm model, and analyzes the experimental results through evaluation indicators. The results show that with the increase of training concentration training examples, the semantic parser can learn more natural language sentence analysis methods from the training examples, and the natural language sentences that can be correctly parsed gradually increase, so with the training examples increased recall rate and F value gradually increased. The experimental results also show that the use of higher precision syntax analyzers can effectively improve the performance of statistical machine translation systems, whether in phrase-based or machine-based translation methods.																	1064-1246	1875-8967					2020	38	4			SI		3765	3772		10.3233/JIFS-179599													
J								Motion path planning of 6-DOF industrial robot based on fuzzy control algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy control; motion path; robot	DYNAMICS	The fuzzy control algorithm is used to establish the relation between the functions of motion of 6-DOF (degree of freedom) industrial robots on the rotation angle of each joint. The algorithm optimizes the motion path of the robot, thereby accelerating the increase in industrial productivity and promoting the development of industrial production. During the motions, the 6-DOF industrial robots have weak avoidance ability toward the encountered obstacles, which is not conducive to the safe production and will reduce industrial efficiency. Therefore, by analyzing and summarizing the previous researches, the fuzzy control algorithm is used to construct and optimize the kinematics model, thereby proposing a method of robot motion path planning. Also, based on the unstructured operating environment, a multi-functional motion navigation system for 6-DOF industrial robots is proposed. The experimental results show that the fuzzy control algorithm can optimize the robot motion path, shorten the time of motion, and make the robots reach the destinations smoothly. The algorithm can avoid safety accidents in industrial production effectively, reduce casualties, improve industrial productivity, and promote the optimized allocation of human resources. The motion system of 6-DOF industrial robot based on fuzzy control algorithm has excellent practicability, which can promote the development of industrial production effectively and be widely applied to industrial production.																	1064-1246	1875-8967					2020	38	4			SI		3773	3782		10.3233/JIFS-179600													
J								The unordered time series fuzzy clustering algorithm based on the adaptive incremental learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Time series; incremental learning; fuzzy clustering	CONTEXT; SYSTEMS	The data of time series are massive in quantity and not conducive to subsequent processing. Therefore, the unordered time series fuzzy clustering algorithm of adaptive incremental learning has been utilized to explore the segmentation of time series in further. The research results show that the emergence of incremental learning technology can solve such problems. Also, it can continuously accumulate and increase the data, as well as improving the learning accuracy. Incremental learning technology correctly processes, retains, and utilizes the historical results, thereby reducing the training time of new samples by using historical results. Therefore, the clustering algorithm mostly clusters the cluster-liked shape of discrete datasets and uses the hierarchical clustering algorithm, which is more suitable for measuring the similarity of time series, to replace the Euclidean distance for distance metric and hierarchical clustering. The distance matrix update method is improved to reduce the computational complexity, which proves that the algorithm has higher clustering validity and reduces the operating time of the algorithm.																	1064-1246	1875-8967					2020	38	4			SI		3783	3791		10.3233/JIFS-179601													
J								Application of network virtual cloud computing data center based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy Algorithm; Network Virtualization; Cloud Computing; Network Load		Network virtualization technology releases human resources to some extent through network and cloud computing technology, reducing the workload of staff. The application of network virtualization technology in cloud computing data centers is based on this condition to improve work quality and efficiency. The purpose of this paper is to use the fuzzy algorithm to realize network virtualization of cloud computing data center. In this paper, we study the adaptive fuzzy control in depth, and conduct the practical application based on the basic knowledge of adaptive fuzzy control we learned, achieved "learn to use". Apply the design of adaptive fuzzy control to the load balancing algorithm of the network virtual cloud computing data center, realized the load balancing algorithm of the network virtual cloud computing data center based on adaptive fuzzy control. According to the load balancing algorithm based on adaptive fuzzy control to achieve this algorithm by using Internet knowledge, and designed the load balancing system of the whole network virtual cloud computing data center. Test the whole load balancing system which has been achieved, and obtained the performance variance curve of the system under different algorithms. Then obtained advantages and disadvantages of the algorithm by analyzing the experimental data. The experimental results show that the proposed method can effectively improve the execution performance of communication-intensive applications and ensure the stable execution of the application. At the same time, the algorithm inherits the advantages of the general fuzzy control load balancing algorithm. The stability is strong and the variance curve does not appear pulsed fluctuation. There is also no divergence phenomenon with time increased.																	1064-1246	1875-8967					2020	38	4			SI		3793	3801		10.3233/JIFS-179602													
J								Regional classification of urban land use based on fuzzy rough set in remote sensing images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Urban land use area; remote sensing image; fuzzy rough set; support vector machine		With the rapid development of the economy, the demand for urban land resources is also growing. How to make more rational use of land resources and make more rational planning of cities become a major problem in current economic development. At present, the use of remote sensing images to classify urban land use areas has become a research hot spot. However, the traditional classification accuracy rate using the maximum likelihood classification method needs to be improved. How to improve the classification accuracy rate of urban land use area of remote sensing image has become the focus and key of the research. Both rough sets and fuzzy sets are mathematical methods for dealing with uncertain problems. The rough fuzzy sets generated by the combination of the two can solve the problem of information loss due to the rough set discretization process. Based on the advantages of fuzzy rough sets, this paper applies fuzzy rough sets to the study of urban land use area classification of remote sensing images, so as to improve the accuracy of urban land use area classification of remote sensing images. Firstly, the spectral features and texture features of the remote sensing image are extracted after preprocessing the remote sensing image. Secondly, using the domain relationship fuzzy rough set reduces the extracted features. Finally, the support vector machine is used to classify the reduced feature set, and the classification of urban land use area is realized. In the simulation experiment, the classification accuracy is evaluated by the overall classification accuracy, Kappa coefficient, and single class classification success index. The evaluation data shows that the fuzzy rough set is applied to the remote sensing image urban land use area classification, which has a good application effect.																	1064-1246	1875-8967					2020	38	4			SI		3803	3812		10.3233/JIFS-179603													
J								Optimization of e-commerce logistics of marine economy by fuzzy algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy algorithm; marine economy; e-commerce logistics optimization	RECOMMENDATION	In order to achieve the improvement and optimization of e-commerce logistics, a fuzzy algorithm was built to optimize the e-commerce logistics in the marine economy. Through the construction of the fuzzy algorithm, a comparative analysis was made on the effects of logistics before the optimization of marine e-commerce, such as customer satisfaction, product arrival rate on time, logistics cost, number of routes, safety rate, work efficiency, and other aspects. The research shows that the fuzzy algorithm has a better performance in logistics cost than before. After optimization, the monthly mileage of logistics has also been significantly reduced, and the economic benefit is very significant. The algorithm in this paper has an excellent effect in customer satisfaction and product arrival rate on time, which has obvious improvement effect on the logistics network. The improvement of the customer satisfaction is of great value and significance to the follow-up development of marine e-commerce. The improvement and optimization of logistics path based on fuzzy algorithm has reference value for the development of logistics in other fields. Based on the fuzzy algorithm, the function of the algorithm is studied by analyzing the logistics optimization of marine e-commerce. The fuzzy algorithm used shows a comprehensive and excellent result in improving the marine logistics situation, and shows that the optimization and improvement of the algorithm is a process that needs to be improved in many ways. The research greatly improves the understanding of the fuzzy algorithm and the e-commerce logistics in the marine economy environment.																	1064-1246	1875-8967					2020	38	4			SI		3813	3821		10.3233/JIFS-179604													
J								Fuzzy decision-making in patients with Alzheimer's disease - a manifestation of pathological aging	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AD; pathological aging; fuzzy decision-making; risk decision-making; cognitive impairment	CARE	The purpose of the study was, based on the pathological aging of Alzheimer's disease (AD) patients, to analyze the negative impact of the pathological causes of AD on decision-making behavior, to explore the causes of fuzzy decision-making in AD patients, and to put forward the problems to be solved. Firstly, the pathological basis and pathogenesis of AD are analyzed to clarify the causes of cognitive impairment. In addition, the significance and influencing factors of fuzzy decision-making are analyzed, and the decision-making ability of AD patients is evaluated to explore the reasons for the decline of decision-making ability and the impacts on the decision-making results. The results show that, with the increase of age, people's basic cognitive ability declines, which will affect individual decision-making in different directions, such as single decision-making thinking, one-sided information inclusion, and inefficient decision-making. For young people, there are many relevant factors to be considered in decision-making, and they will make rational analysis one by one, so as to grasp the scientificity and rationality of decision-making. In summary, AD patients have obvious pathological aging symptoms because of physiological aging, confused thinking mode, unable to make rational analysis in decision-making, strong subjective consciousness, and obvious fuzzy decision-making behavior, which make the scientific rationality of decision-making show loopholes, and ultimately affect the development and trend of events.																	1064-1246	1875-8967					2020	38	4			SI		3823	3829		10.3233/JIFS-179605													
J								Application of fuzzy mathematics in mapping	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy mathematics; mapping; equal ratio series; comprehensive evaluation		With the wide application of fuzzy mathematics in various fields of life, fuzzy mathematics has shown its strong vitality in many disciplines, such as civil architecture, environmental protection, medical science, operations management, chemical industry and so on. With the increasing depth of research issues, various disciplines are also constantly intersecting. Fuzzy mathematics is gradually being combined with other analytical methods. The purpose of this paper is to discuss the application of fuzzy mathematics in cartography. Through the quota selection model and the structure selection model, the membership function, the equal ratio sequence method, and other fuzzy mathematics methods are applied to the river map making. Through the above methods, it is concluded that they can solve many kinds of problems, including the study of the geographical distribution of cartographic objects, cartographic selection, mutual relations, and evaluation and prediction models. Therefore, it can be concluded that the concept of fuzzy mathematics is applied to cartographic generalization. Fuzzy mathematics can deal with this kind of fuzziness better, which makes cartographic generalization possible to use more map information. It also provides a new means for the study of cartographic generalization. At the same time, it also provides a new research way for map database and automatic mapping. The accuracy of the experimental method in this paper is 5% higher than that of traditional mathematical cartography; it tends to restore the truth.																	1064-1246	1875-8967					2020	38	4			SI		3831	3840		10.3233/JIFS-179606													
J								Edge detection algorithm of ultrasound image in obstetrics and gynecology based on multiplicative gradient	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Vaginal ultrasound; hydatidiform mole; missed abortion; edge detection algorithm; multiplicative gradient		It is a common disease of abnormal early pregnancy in obstetrics and gynecology. Inside them, abortion and hydatidiform mole are two common types, which seriously threaten the life safety of pregnant women. To evaluate the diagnostic value of transvaginal ultrasound and serum-HCG in hydatidiform mole and missed abortion. This article introduces the application of transvaginal ultrasound in disease diagnosis. An edge detection algorithm based on multiplicative gradient was proposed: normal pregnant women and abnormal pregnant women were selected as the research objects. According to the pathological examination results of abnormal pregnancy, all pregnant women were divided into two groups: hydatidiform mole group and abortion group. The level of serum beta-HCG was measured and its diagnostic value was analyzed. Transvaginal ultrasound combined with serum beta-hcg confirmed the diagnosis of beta-HCG.																	1064-1246	1875-8967					2020	38	4			SI		3841	3847		10.3233/JIFS-179607													
J								The study of automatic matching and optimal location of epidural anesthesia based on ultrasonic image processing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ultrasound image; epidural anesthesia; ultrasound location guidance		The traditional epidural anesthesia often relies on blind touch method, which has high technical requirements and low efficiency for doctors. But the ultrasonic image localization method also affects the doctor's use because of the insufficient definition, the big spot noise and so on. Based on this, a new lumbar ultrasound processing system is proposed. Anisotropic diffusion filter can enhance contrast and make image smoother. The new adaptive threshold binarization algorithm can not only remove speckle noise, but also get clear ultrasound image. The template matching algorithm is used to locate the epidural anesthesia automatically. Through several ultrasound images were examined, The success rate was 98.12%. This technology was used in the control experiment of obese patients, which showed that this technology can better solve the problem of low epidural anesthesia operation.																	1064-1246	1875-8967					2020	38	4			SI		3849	3857		10.3233/JIFS-179608													
J								Digital image processing and recognition technology for classification and recognition of hydrothorax cancer cells	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hydrothorax cancer cell; image segmentation; image recognition; feature extraction		Pathological diagnosis is the most common and reliable method of cancer diagnosis, but the technology of pathological diagnosis is relatively backward. It is an urgent problem to identify and classify the pathological pictures of cancer cells. Based on this, the digital image processing and recognition technology are analyzed for the classification and recognition of hydrothorax cancer cells. There is a big difference in the morphology of pleural effusion cancer cells, and uncertainty, so the edge detection algorithm is improved, with the simulated edge detection method used to extract information. After image segmentation, feature extraction is of vital importance for cell image classification. A method of block statistics based on Gabor coefficient is proposed. Firstly, the cell image is filtered by multi-scale and multi-directional filtering, then the average and variance are calculated, and the image is divided into several blocks to solve the problem of large amount of data. Finally, BP neural network is established to input the morphological characteristics of hydrothorax cells, and the results are classified directly. After the experiment, the proposed classification method can improve the classification effectiveness; the design model can accurately identify the breast water cancer cells, and can be effectively applied to the early diagnosis of breast water cancer cells.																	1064-1246	1875-8967					2020	38	4			SI		3859	3866		10.3233/JIFS-179609													
J								Blind image restoration algorithm based on improved sparse Bayesian low dose CT	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CT angiography; moyamoya disease; artery bypass	MOYAMOYA-DISEASE	In China, moyamoya disease is one of the main causes of stroke events in children and adults. Early diagnosis and early treatment can prevent irreversible damage to nerve function and greatly improve the quality of life of patients. Based on the improved sparse Bayesian low-dose CT image blind restoration algorithm, moyamoya disease diagnosis and treatment has important application value guidance and evaluation Cheng, through the retrospective analysis of the clinical characteristics, imaging characteristics and treatment of 105 patients with moyamoya disease diagnosed by Blind image restoration algorithm based on improved sparse Bayesian low dose CT from April 2012 to November 2015 in a hospital in Hefei City, we think that CT angiography has important application value in the diagnosis and treatment of moyamoya disease, and we can draw a conclusion. Among these 105 patients, women are the majority. The ratio of male to female was 1:1.188, and the peak period was 30-40 years old. There were 41 cases of ischemic stroke and 64 cases of hemorrhagic stroke. 29 patients underwent STA-MCA bypass, temporal muscle compression and dural reversal. Blind image restoration algorithm based on improved sparse Bayesian low dose CT was reexamined. The results of Blind image restoration algorithm based on improved sparse Bayesian low dose CT and DSA were the same. It can be seen that Blind image restoration algorithm based on improved sparse Bayesian low dose CT is a reliable method to diagnose moyamoya disease, which can be used as a preoperative guidance and postoperative evaluation of bypass vessel patency and collateral circulation formation.																	1064-1246	1875-8967					2020	38	4			SI		3867	3875		10.3233/JIFS-179610													
J								The finite element analysis of the influence of the load change of the internal oblique muscle on the near end of the patellar tendon based on the three-dimensional image model of MRI	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Three-dimensional reconstruction technology; medical image; MRI		Medical image segmentation is an important step of medical image processing, which divides medical image into thousands of regions and extracts the regions of tissues and organs of interest. The accuracy of segmentation is very important for the follow-up processing of medical image and doctors' judgment of the real situation of diseases. Medical image segmentation is a classic problem in the field of image segmentation. 3D image reconstruction technology is to obtain 3D structure information from 2D images of objects, to provide users with realistic 3D graphics, and to restore the prototype of objects, so that users can observe and analyze from multiple perspectives, which greatly improves the accuracy of measurement and the scientific accuracy of medical diagnosis, and plays a very important role in assisting doctors in clinical diagnosis. Based on the three-dimensional image model of MRI, the load variation of the internal oblique muscle can be applied to the finite element analysis of the near end of the patellar tendon.																	1064-1246	1875-8967					2020	38	4			SI		3877	3883		10.3233/JIFS-179611													
J								Feature extraction and measurement algorithm based on color in image database	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Feature extraction; image recognition; color characteristics; medical image database		Image recognitionan is an important part of pattern recognition, It has been widely used in modern production and life. Using computer technology and modern information processing technology to complete human visual cognition and understanding. Thus, It have become a hot topic about how to study image recognition based on medical color feature extraction. This study firstly studied the existing literature, explored and analyzed relevant theories and technologies, and studied several algorithms related to current image recognition. Then the image recognition algorithm based on wavelet moment and support vector machine is combined with the artificial intelligence technology based on image feature extraction theory to establish the color medical image recognition algorithm based on wavelet moment and support vector machine. In order to verify the feasibility and advancedness of the new algorithm, practical experiments are carried out, and the experimental results are compared and analyzed by statistical method. The concrete chart proves the correctness of the conclusion. The final of the new algorithm is proved to be successful.																	1064-1246	1875-8967					2020	38	4			SI		3885	3891		10.3233/JIFS-179612													
J								Simulation analysis of 3D medical image reconstruction based on ant colony optimization algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Three dimensional reconstruction; image processing; segmentation	3-DIMENSIONAL RECONSTRUCTION; ITERATIVE RECONSTRUCTION; PROCESSING TECHNOLOGY; REAL-TIME	Three-dimensional reconstruction technology can display the three-dimensional graphical data of medical images to diagnostic personnel, so as to facilitate multi-dimensional and multi-level observation of patient data, and assist doctors in qualitative and quantitative analysis of pathological tissue. A surface reconstruction algorithm for three-dimensional medical images based on segmentation is proposed. It combines image segmentation with MC (marching cubes) algorithm organically, which can be based on the characteristics of different medical images. An appropriate segmentation method is used to segment different tissues accurately, and the result of segmentation is used to extract the isosurface accurately, which avoids the limitation that MC is only suitable for threshold segmentation. After the medical image is segmented by combining threshold and region growth, the segmentation results are input as the reconstructed data, and the improved algorithm of three-dimensional reconstruction is realized. The medical image is rendered on three-dimensional surface, and the debugging results of the software are displayed. At the same time, a cube detection method based on region growing is adopted to improve the efficiency of surface tracking. Experiments show that this algorithm can improve the reconstruction speed and display effect.																	1064-1246	1875-8967					2020	38	4			SI		3893	3902		10.3233/JIFS-179613													
J								Image segmentation algorithm based on partial differential equation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Partial differential equation; image segmentation; algorithm analysis		Image segmentation is very important for various fields. With the development of computer technology, computer technology has become more and more effective for image segmentation, and it is studied on the basis of partial differential equations. The curve representation method in plane differential geometry is expounded, with the SegNet-v2 segmentation model analyzed and tested in medical image segmentation. The test results show that the partial differential equation image segmentation algorithm can achieve more accurate segmentation, especially in medical image segmentation, which can achieve good results, and it is worth in practice to further promote.																	1064-1246	1875-8967					2020	38	4			SI		3903	3909		10.3233/JIFS-179614													
J								Research on signal processing technology optimization of contact image sensor based on BP neural network algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										BP neural network algorithms; contact image; sensors; signal processing technology		In order to improve the signal processing technology of contact image sensor, the optimization of image processing technology is carried out from the aspects of feature extraction, fine-grained semantic feature generation and semantic analysis matching optimization. To solve the problem of inaccurate feature extraction, a multi-scale feature representation algorithm for food images is proposed. By extracting the features of multi-scale convolution layer, and according to the food image, the feature extraction process is dispersed into each convolution process. By comparing the features of the layers with the image library, the most accurate features are selected for transmission. To solve the problem of conservative vocabulary and poor generalization performance of generated sentences, a fine-grained image semantic analysis algorithm based on subword segmentation is proposed. The results show that compared with the mainstream methods, the proposed method has improved in varying degrees on the four evaluation indicators. The research provides a reference for the optimization of image sensor signal processing technology and the wide application of BP neural network algorithm.																	1064-1246	1875-8967					2020	38	4			SI		3911	3919		10.3233/JIFS-179616													
J								Network security risk assessment model based on fuzzy theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cyber security risk assessment; fuzzy theory		With the rapid development of information science and technology, network security has occupied a very important position in people's lives. Since the network security situation problem does not form a unified optimal solution in the model and algorithm, it is still necessary for researchers to continue to explore. In order to better evaluate the network security risk, based on fuzzy theory, particle swarm optimization and RBF neural network, this paper proposes a network security risk assessment model based on fuzzy theory. By mining the rules in the historical data of the network security situation and combining with the current network status, the assessment of the current network security situation is realized, and the objectivity and comprehensibility of the evaluation results are improved. The experimental comparison shows that the fuzzy theory prediction model with PSO-RBF neural network has more rapid and effective evaluation and prediction results than the fuzzy theory prediction model with RBF neural network only.																	1064-1246	1875-8967					2020	38	4			SI		3921	3928		10.3233/JIFS-179617													
J								Examination on face recognition method based on type 2 blurry	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Type 2 fuzzy rules; linear identification; face recognition; feature extraction	SYSTEM	In recent years, technology of face recognition has developed rapidly, more and more face recognition technologies have been integrated into our work and life. In practical applications, due to influence of various factors, the resolution of the face image is low, the noise interference is large, and the illumination changes sharply during the imaging process, which brings difficulties to the face recognition, which seriously affects the accuracy of the face recognition method. This paper aims to introduce two-type fuzzy theory into face recognition and study its extraction and recognition methods of face feature. Firstly, it introduce the face recognition technology simply. Face recognition is a technique that uses a computer to analyze a face image and extract valid identification information to identify the identity. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two methods for extracting features from face recognition. Principal Component Analysis (PCA) is a data analysis method that uses a small number of characterizations to reduce the number of dimensions, which reduces computational complexity greatly. The purpose of linear discriminant analysis is to extract data from high-dimensional feature spaces. Extracted the low-dimensional features with recognition ability, and studied the two-type fuzzy system based on fuzzy sets deeply. Obtained the output function of the two-type fuzzy system by studying the structure of each layer of the two-type fuzzy system. Introduce two types of fuzzy ideas into linear discriminant analysis. Discussed the construction of fuzzy membership functions, the selection of kernel functions and the determination of clustering rules. Finally, the ORL face database of the trained fuzzy face recognition model. As a result, the face recognition method based on the type 2 fuzzy has certain feasibility. The experimental results show that face recognition based on interval two-type fuzzy neural network has good recognition rate and anti-noise ability.																	1064-1246	1875-8967					2020	38	4			SI		3929	3938		10.3233/JIFS-179618													
J								Examination on avionics system fault prediction technology based on ashy neural network and fuzzy recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ashy neural network; avionics system; fuzzy recognition; fault prediction; combined forecast		The purpose of this paper is to accurately locate the fault prediction and diagnosis technology, to have a high degree of automation, and to handle it quickly, for the large aircraft avionics system failure presents the feature of multiple coupling, complex impact and rapid spread. At the same time, the fault prediction diagnosis technology is one of the most important contents of the avionics system equipment prediction, so how to quickly and effectively predict the failure of key system parts of avionics is the core essential to ensure the complete operation of the whole system. This paper through establishing the gray neural network model, combining the advantages of gray model to deal with poor information and the characteristics of artificial neural network processing nonlinear data, to realize the fault prediction of avionics system, At the same time, At the same time, through the fuzzy recognition method based on the deterioration degree, established the bridge between the two, in turn, to achieve the health prediction management of system. The method mainly includes: Firstly, by combining gray theory and artificial neural network algorithm with fuzzy recognition to establish a network model that contains gray neural network models and can reflect the excellent characteristics of fuzzy recognition and conduct experimental analysis; Second, on this basis, improve the weight update strategy of the gray neural network by using additional learning rate method which based on momentum and improve the accuracy of the algorithm. Therefore, it can be concluded that the predictions presented in this paper should not be directly imitated when the system disturbance factor is too large or the system is abnormally caused by a serious disturbance suddenly appearing at a certain point in time, but should properly processed the data firstly according to the actual situation. According to the time series of the actual situation, several models are established, and the data correction is explained from the model prediction effect, and the gray model and description are improved. The improved combination of gray neural network and gray neural network can not only improve the prediction accuracy, but also provide a feasible method for such time series prediction, which provides a practical and effective technical method for avionics system fault prediction.																	1064-1246	1875-8967					2020	38	4			SI		3939	3947		10.3233/JIFS-179619													
