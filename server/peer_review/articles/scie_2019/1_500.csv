PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								A knowledge-based approach to hierarchical classification: A voting metaphor	EXPERT SYSTEMS WITH APPLICATIONS										Hierarchical classification; Knowledge-based classification; Rule-based systems; Performance measures		The paper proposes a new approach to hierarchical classification based on condition-action rules that represent expert knowledge in a given domain. The approach adopts a voting metaphor: each rule is regarded as a voter that expresses a preference for a given category to be assigned to an item to be classified; the category that receives more votes wins. Novel performance measures of hierarchical classifiers are also introduced that aim at overcoming the limitations of the current concepts of precision and recall. The proposed approach can be applied to any hierarchical classification task, for which expert knowledge is available. The viability of the approach and its performance are shown through a real-size application concerning the e-mail dispatching task inside a large public administration. The results obtained demonstrate that the proposed knowledge-based approach to hierarchical classification can reach a performance level comparable to that of human experts, if not even better. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113737	10.1016/j.eswa.2020.113737													
J								Research on an advanced intelligence implementation system for engineering process in industrial field under big data	EXPERT SYSTEMS WITH APPLICATIONS										Advanced CBR; Big data; IE algorithm; DECMBD algorithm	BAYESIAN NETWORKS; CBR; PREDICTION; PERFORMANCE; ALGORITHM; SELECTION; ONTOLOGY; MACHINE; CLOUD; MODEL	To develop an advanced CBR system to well adapt to the intelligence implementation of new engineering process in the big data environment, Bayesian network (BN) model is introduced to CBR system for knowledge reasoning. However, as engineering application is becoming more and more complicated, the number of parameters used to define engineering application grows larger and larger, leading to the seriously reduced efficiency as well as the accuracy of the integrated model. For the problem of reduced efficiency, this paper proposes In-External (IE) algorithm to perform the assignment of big data distribution for parallel data processing, which can fully utilize the capacity of Hadoop system and attain the best efficiency of knowledge reasoning. For the problem of reduced accuracy, in view of the fact that traditional probability learning methods are unfit for the proposed CBR system, this paper proposes Discount Exponential Coefficients of Multivariate Beta Distribution (DECMBD) algorithm to conduct the probability learning of proposed system. In DECMBD algorithm, a discount ratio is given to each exponential coefficient of multivariate Beta distribution to improve the occurrence times counting of all problem features and then gain better effect of probability learning. Finally, lots of experiments are performed to validate the effectiveness of the proposed advanced CBR system. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113751	10.1016/j.eswa.2020.113751													
J								Dominant point detection based on suboptimal feature selection methods	EXPERT SYSTEMS WITH APPLICATIONS										Dominant point detection; Image compression; Suboptimal feature selection; Turning angle curvature; Computer vision	DIGITAL PLANAR CURVES; POLYGONAL-APPROXIMATION; ALGORITHM; POLYGONIZATION	This paper presents a viable alternative solution for dominant point detection predicated on the comparison of suboptimal feature selection methods. Suboptimal feature selection methods are utilized as standard criteria to identify dominant points. Considering that all of the combinations of points comprise many sets, an algorithm that eliminates some of them is affirmed and illustrated. The sequential backward selection, sequential forward selection, generalized sequential forward selection, generalized sequential backward selection and plus l-take away r selection methods are performed on the remaining points to extract the dominant points. The simulation results exhibit that this method is significantly more effective and efficient in comparison to other proposed methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113741	10.1016/j.eswa.2020.113741													
J								Skin lesion segmentation using fully convolutional networks: A comparative experimental study	EXPERT SYSTEMS WITH APPLICATIONS										Deep Learning; Convolutional Neural Network; Fully Convolutional Network; Medical Image Segmentation	BRAIN-TUMOR SEGMENTATION; COMPUTATIONAL APPROACH; IMAGE SEGMENTATION; DEEP; ACCURACY	Because the most dangerous type of skin cancer, melanoma, is very difficult for dermatologists to detect because of the low contrast between the lesion and the adjacent skin, the automatic application of skin lesion segmentation is regarded as very challenging. This paper proposes the implementation of a medical image segmentation that will accelerate a melanoma diagnosis by dermatologists. In the implementation, Fully Convolutional Network (FCN) architectures generated by modifying Convolutional Neural Network (CNN) architectures are used. The proposed algorithm for an automatic semantic segmentation of skin lesions utilizes four different FCN architectures, FCN-AlexNet, FCN-8s, FCN-16s, and FCN-32s. The experimental studies in this paper are constructed on the ISIC 2017 dataset, and the evaluations of these architectures on the dataset are carried out for the first time with this study. In the experimental studies, once the images in the dataset are preprocessed, the FCNs are first trained separately. Secondly, the accuracies and Dice coefficients on the validation dataset are calculated by using these trained FCN architectures. Thirdly, the obtained results are compared. Finally, the inferences of lesion segmentation are visualized in order to exhibit how exactly the FCN architectures can segment the lesions. The experimental results show that the FCNs in the proposed algorithm are suitable for skin lesion segmentation. In addition, it is thought that the experimental results will contribute to the scientific literature and assist the researchers who are working on medical image segmentation. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113742	10.1016/j.eswa.2020.113742													
J								Inverse data envelopment analysis for operational planning: The impact of oil price shocks on the production frontier	EXPERT SYSTEMS WITH APPLICATIONS										Operational planning; Frontier shift; Data envelopment analysis; Anchoring point; Natural gas; Cross-price elasticity	NETWORK DEA APPROACH; MEASURING EFFICIENCY; FUZZY-DEA; TECHNOLOGY; MODEL; PERFORMANCE; EVOLUTION; BANKS; STATE; ART	Inverse data envelopment analysis (DEA) is a useful planning tool, especially when it is combined with frontier changes that accurately reflect reality. This paper proposes an inverse optimization model for operational planning by taking into account frontier changes in conjunction with environmental factors. The aim is not only to present a computational procedure of a new measure to properly capture the effective frontier changes, but more importantly to demonstrate how frontier changes observed in the past can be utilized to provide insights into estimation of the future production frontier. In essence, the proposed model is intended to help establish realistic goals for operational planning practices. The model is applied to the Korean natural gas industry as an empirical demonstration. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113726	10.1016/j.eswa.2020.113726													
J								An effective binary artificial bee colony algorithm for maximum set k-covering problem	EXPERT SYSTEMS WITH APPLICATIONS										Maximum set k-covering problem; Artificial bee colony algorithm; Local search; Heuristic; Binary programming	TABU SEARCH	Given a row set M and a column set C, where each column in C covers several rows of M, the maximum set k-covering problem (MKCP) is to select k columns from C, such hat the number of rows covered by the selected columns is maximized. It can be formulated as linear integer programming, and has several real world applications. Several heuristic approaches have been previously presented for solving the MKCP. However, the obtained solution quality is not stable, and the solution time increases very quickly as the size of the instance increases. This work proposes a hybrid binary artificial bee colony algorithm (HBABC) to solve the MKCP. First, based on the characteristic of MKCP, HBABC redesigns a food source updating method. The new updating method uses the previously found solutions to guide the search. Second, to improve the exploitation ability of the HBABC, a tabu based simulated annealing (TBSA) is proposed. Moveover, we employ a bucket sorting technique to speed up the TBSA. Finally, the computational results on 75 benchmark instances demonstrate that the HBABC competes favorably with other algorithms. Specifically, our algorithm improves the best known solutions on about 20 percent of the tested instances. In addition, the HBABC performs better than two binary artificial bee colony algorithms on the tested instances. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113717	10.1016/j.eswa.2020.113717													
J								Detecting and visualizing hate speech in social media: A cyber Watchdog for surveillance	EXPERT SYSTEMS WITH APPLICATIONS										Natural language processing; Deep learning; Hate speech; Aggression detection; Text classification; Social media visualization		The multi-fold growth of the social media user-base fuelled a substantial increase in the amount of hate speech posts on social media platforms. The enormous data volume makes it hard to capture such cases and either moderate or delete them. This paper presents an approach to detect and visualize online aggression, a special case of hate speech, over social media. Aggression is categorized into overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive labels (NAG). We have designed a user interface based on a web browser plugin over Facebook and Twitter to visualize the aggressive comments posted on the Social media user's timelines. This plugin interface might help to the security agency to keep a tab on the social media stream. It also provides citizens with a tool that is typically only available for large enterprises. The availability of such a tool alleviates the technological imbalance between industry and citizens. Besides, the system might be helpful to the research community to create further tools and prepare weakly labeled training data in a few minutes using comments posted by users on celebrity's Facebook, Twitter timeline. We have reported the results on a newly created dataset of user comments posted on Facebook and Twitter using our proposed plugins and the standard Trolling Aggression Cyberbullying 2018 (TRAC) dataset in English and code-mixed Hindi. Various classifiers like Support Vector Machine (SVM), Logistic regression, deep learning model based on Convolution Neural Network (CNN), Attention-based model, and the recently proposed BERT pre-trained language model by Google AI, have been used for aggression classification. The weighted F1-score of around 0.64 and 0.62 is achieved on TRAC Facebook English and Hindi datasets while on Twitter English and Hindi datasets, the weighted F1-score is 0.58 and 0.50, respectively. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113725	10.1016/j.eswa.2020.113725													
J								Gaining insight to B2B relationships through new segmentation approaches: Not all relationships are equal	EXPERT SYSTEMS WITH APPLICATIONS										Market segmentation; B2B market; Multi-objective market segmentation; Pareto optimal solution	CUSTOMER RELATIONSHIP MANAGEMENT; SUPPLY CHAIN RELATIONSHIPS; MARKET-SEGMENTATION; SOCIAL-EXCHANGE; TRUST; SATISFACTION	B2B market segmentation has both structure complexity and computation complexity. The existing market segmentation methods can not directly address these two challenges simultaneously and provide a comprehensive view of the whole problem. Nor are they able to provide guidance on the selection of the most suitable solution among candidates. This study formulates the B2B segmentation as a multidimensional optimization problem that integrates both customer behavior and marketing effectiveness. It applies an integrated segmentation method that unifies two market segmentation approaches: the Embedded Exchange Approach (a descriptive model) and the Predictive Satisfaction Approach (a predictive model). It proposes the use of an evolutionary based, multi-objective segmentation method to solve the structural and computational challenges. The method generates a set of Pareto optimal solutions which not only gives a holistic view of possible solutions in the Pareto optimal space but also allows marketers to use solution selection algorithm based on the properties of Pareto optimal sets. The study develops a solution selection algorithm that represents a good tradeoff of two objectives based on the geometric shape of the Pareto optimal solution front. Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 15	2020	161								113767	10.1016/j.eswa.2020.113767													
J								Robust matching cost function based on evolutionary approach	EXPERT SYSTEMS WITH APPLICATIONS										Meta-heuristics; Matching cost; Stereo matching	OPTIMIZATION	This paper proposes a novel stereo matching method with a matching cost function learned from training data. Because the cost function includes a considerably large number of parameters required to select their values, it is nearly impossible to manually select the values. We employ an evolutionary algorithm to automatically optimize the parameter values for each dataset. Using Middlebury, KITTI 2012, and KITTI 2015 dataset, we compare the proposed stereo matching method with state-of-the-art stereo matching methods that can achieve real-time computation. Experimental results show that the proposed method outperforms the real-time state-of-the-art stereo matching methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113712	10.1016/j.eswa.2020.113712													
J								Latent state recognition by an enhanced hidden Markov model	EXPERT SYSTEMS WITH APPLICATIONS										LASSO; Vector-autoregressive model; Hidden Markov model	TIME-SERIES; CONDITIONAL HETEROSCEDASTICITY; MAXIMUM-LIKELIHOOD; STYLIZED FACTS; STOCK RETURNS; SHRINKAGE; REGULARIZATION; INFORMATION; ESTIMATOR; ALGORITHM	In this paper, we start from relaxing assumptions of traditional hidden Markov model then develop a novel framework for decoding the latent states, from which the dynamics of multi-variable financial data is generated. To construct the framework, we model the observed variables as a p-order vector autoregressive process, allow the latent state to evolve through a semi-Markov chain, and shrink the auto-regression and covariance matrices via a penalized maximization likelihood method. Using the 50-dimensional simulated data, the 12-dimensional 5-min order book data of the Chinese CSI 300 index component stocks, the 49-dimensional daily data of U.S. industry portfolio, and 1-dimensional hourly data of four primary foreign exchange rates, our empirical analyses show that the proposed model outperforms the alternative model in accurately recognizing anomalous events and achieves better sharp ratio in a pseudo trading strategy via the latent states. The superior performance is across the data frequency of minute, hour and daily, the dimension of one, twelve, and fifty, the data type of stock, foreign exchange rate, and industry portfolio. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113722	10.1016/j.eswa.2020.113722													
J								Joint production planning, pricing and retailer selection with emission control based on Stackelberg game and nested genetic algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Stackelberg game; Nonlinear bilevel programming; Nested genetic algorithm; Emission control; Joint decision making	SUPPLY CHAIN; PRODUCTION DECISIONS; MODEL; DESIGN; COORDINATION; TECHNOLOGY; POLICIES	In practice, it is of paramount importance that firms make joint decisions in production planning, pricing and retailer selection while considering emission regulation. This is because the joint decisions can ensure firms to obtain higher profits while contributing to sustainable environments. However, due to the problem complexity, no models facilitating such decision making are available. This study aims to develop a model to help firms make optimal joint decisions. To model the situations where a manufacturer is the leader and the retailers are followers, we adopt the Stackelberg game theory and develop a 0- 1 mixed nonlinear bilevel program to maximize the profits of both the manufacturer and his retailers. We further develop a nested genetic algorithm to solve the game model. Numerical examples demonstrate (i) the applicability of the game model and the algorithm and (ii) the robustness of the algorithm. Managerial insights are obtained, suggesting that (i) manufacturers need to identify the capacity ranges (called capacity traps) where capacity increases result in reduced profits when making decisions to optimize profits; (ii) retailers should make suitable, e.g., pricing decisions so that the manufacturers can include them in the supply chains; (iii) both manufacturers and retailers may not need to consider the carbon emission buying (or selling) price when making decisions. (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 15	2020	161								113733	10.1016/j.eswa.2020.113733													
J								A density-based approach for querying informative constraints for clustering	EXPERT SYSTEMS WITH APPLICATIONS										Constrained clustering; Density tracking; Must-link; Cannot-link	ALGORITHM	During the last years, constrained clustering has emerged as an interesting direction in machine learning research. With constrained clustering, the quality of results can be improved by using constraints if a high-quality set of constraints is selected. Querying beneficial constraints is a challenging task because there is no metric for measuring the quality of constraints before clustering. A new method is proposed in this study that estimates density and impurity of data points on different adjacency distances and calculates centrality for each data point by applying a density tracking approach on the obtained densities. The obtained information is then used to select a set of high-quality constraints. Multi-resolution density analysis to more accurately estimate the point-point relationship of data, data density tracking in order to estimate the impurity and centrality of data, and selection of constraints from skeleton of clusters in order to discover the intrinsic structure of data can be mentioned as the most important contributions of this study. To verify the effectiveness of the proposed method, we conducted a series of experiments on real data sets. The obtained results show that the proposed algorithm can improve the clustering process compare with some recent reference algorithms. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113690	10.1016/j.eswa.2020.113690													
J								Enhancing web service clustering using Length Feature Weight Method for service description document vector space representation	EXPERT SYSTEMS WITH APPLICATIONS										Web Service Clustering; Web Service Description Language (WSDL); Term Frequency - Inverse Document Frequency (TF-IDF); Length Feature Weight (LFW); K-Means clustering	IDF	Due to the rapid growth of web services in repositories, discovering the requisite web service is becoming increasingly cumbersome task. It has raised the demand for efficient web service clustering algorithms. In service repositories, when related web services are stored in a clustered way, it enhances the web service discovery process by reducing search space and time. Many eminent researchers have worked in this field and used the Term Frequency - Inverse Document Frequency (TF-IDF) method for representing web services in vector space. In general, there are various limitations of the TF-IDF approach i.e. (1) Not efficient for large documents (2) Position of term and its co-occurrences does not matter (3) Unable to analyze how terms are dispersed in different documents. In the web service scenario, services are represented in short text form. TF-IDF does not work well in web service representation because of the reason that it is unable to effectively find the importance of a term concerning its occurrence in other documents. If we compare two service documents i.e. 's1' and 's2' first having a large and second having small number of terms respectively then TF-IDF does not demonstrate the importance of terms in 's1' as smaller to 's2'. Therefore, it is not possible to assign effective weights to the terms. In the lack of effective vector space representation, the performance of the clustering algorithm also degrades. In this paper, we propose a new approach i.e. LFW+K which is based on Length Feature Weight (LFW) for the vectorized representation of service followed by K-Means clustering. The proposed approach helps to find the informative term from web service and assigns the term weight accordingly by considering parameters like the dimension of the web service document, maximum frequency of a term in the document and occurrences of a term in other documents. LFW+K is applied on the datasets of real-world web services and the performance is measured using standard measurement criteria (i.e. precision, recall, F1-score, and accuracy). Results of the proposed approach are compared with K-Means clustering on TF-IDF representation method i.e. TF-IDF+K. Results show that the proposed method outperforms the clustering done by using TF-IDF method for vector space representation of web services. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113682	10.1016/j.eswa.2020.113682													
J								Adaptive boost LS-SVM classification approach for time-series signal classification in epileptic seizure diagnosis applications	EXPERT SYSTEMS WITH APPLICATIONS										Epileptic seizure; Health informatics; Electroencephalogram; Covariance; Eigen values; Adaptive Boosting Least Square-Support; Vector Machine; AB-LS-SVM	WAVELET TRANSFORM; AUTOMATED IDENTIFICATION; EEG; ENTROPY; STATISTICS	Epileptic seizures are characterised by abnormal neuronal discharge, causing notable disturbances in electrical activities of the human brain. Traditional methods based on manual approaches applied in seizure detection in electroencephalograms (EEG) have drawbacks (e.g., time constraint, lack of effective feature identification relative to disease symptoms and susceptibility to human errors) that can lead to inadequate treatment options. Designing an automated expert system to detect epileptic seizures can proactively support a neurologist's effort to improve authenticity, speed and accuracy of detecting signs of a seizure. We propose a novel two-phase EEG classification technique to detect seizures from EEG by employing covariance matrix coupled with Adaptive Boosting Least Square-Support Vector Machine (i.e., AdaBoost LS-SVM) framework. In first phase, the covariance matrix is employed as a dimensionality reduction tool with feature extraction applied to analyse epileptic patients' EEG records. Initially, each single EEG channel is partitioned into respective k segment with m clusters. Subsequently, covariance matrix is adopted with eigenvalues of each cluster extracted and tested through statistical metrics to identify the most representative, optimally classified features. In the second phase, a robust classifier (i.e., AB-LS-SVM) is proposed to resolve issues of unbalanced data, to detect epileptic events, yielding a high classification accuracy compared to its competing counterparts. The results demonstrates that AB-LS-SVM (optimised by a covariance matrix) is able to achieve satisfactory results (>99% accuracy) for eleven prominent features in EEG signals. The results are compared with state-of-art algorithms (i.e., k-means, SVM, k-nearest neighbour, Random Forest) on identical databases, demonstrating the capability of AB-LS-SVM method as a promising diagnostic tool and its practicality for implementation in seizure detection. The study avers that the proposed approach can aid clinicians in diagnosis or interventions to treat epileptic disease, including a potential use in expert systems where EEG needs to be classified through pattern recognition. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113676	10.1016/j.eswa.2020.113676													
J								Heap-based optimizer inspired by corporate rank hierarchy for global optimization	EXPERT SYSTEMS WITH APPLICATIONS										Social optimization algorithm; Corporate hierarchy based optimization; Nature-inspired meta-heuristic; Global optimization algorithm	METAHEURISTIC ALGORITHM; DIFFERENTIAL EVOLUTION; DESIGN OPTIMIZATION; COLONY	In an organization, a group of people working for a common goal may not achieve their goal unless they organize themselves in a hierarchy called Corporate Rank Hierarchy (CRH). This principle motivates us to map the concept of CRH to propose a new algorithm for optimization that logically arranges the search agents in a hierarchy based on their fitness. The proposed algorithm is named as heap-based optimizer (HBO) because it utilizes the heap data structure to map the concept of CRH. The mathematical model of HBO is built on three pillars: the interaction between the subordinates and their immediate boss, the interaction between the colleagues, and self-contribution of the employees. The proposed algorithm is benchmarked with 97 diverse test functions including 29 CEC-BC-2017 functions with very challenging landscapes against 7 highly-cited optimization algorithms including the winner of CEC-BC-2017 (EBO-CMAR). In the first two experiments, the exploitative and explorative behavior of HBO is evaluated by using 24 unimodal and 44 multimodal functions, respectively. It is shown through experiments and Friedman mean rank test that HBO outperforms and secures 1st rank. In the third experiment, we use 29 CEC-BC-2017 benchmark functions. According to Friedman mean rank test HBO attains 2nd position after EBO-CMAR; however, the difference in ranks of HBO and EBO-CMAR is shown to be statistically insignificant by using Bonferroni method based multiple comparison test. Moreover, it is shown through the Friedman test that the overall rank of HBO is 1st for all 97 benchmarks. In the fourth and the last experiment, the applicability on real-world problems is demonstrated by solving 3 constrained mechanical engineering optimization problems. The performance is shown to be superior or equivalent to the other algorithms, which have been used in the literature. The source code of HBO is publicly available at https://github.com/qamar-askari/HBO. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113702	10.1016/j.eswa.2020.113702													
J								Ensemble topic modeling using weighted term co-associations	EXPERT SYSTEMS WITH APPLICATIONS										Topic modeling; Ensemble learning; Evaluation; Word embeddings; Interpretation		Topic modeling is a popular unsupervised technique that is used to discover the latent thematic structure in text corpora. The evaluation of topic models typically involves measuring the semantic coherence of the terms describing each topic, where a single value is used to summarize the quality of an overall model. However, this can create difficulties when one seeks to interpret the strengths and weaknesses of a given topic model. With this in mind, we propose a new ensemble topic modeling approach that incorporates both stability information, in the form of term co-associations, and semantic similarity information, as derived from a word embedding constructed on a background corpus. Our evaluations show that this approach can simultaneously yield higher quality models when considering the produced topic descriptors and document-topic assignments, while also facilitating the comparison and evaluation of solutions through the visualization of the discovered topical structure, the ordering of the topic descriptors, and the ranking of term pairs which appear in topic descriptors. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113709	10.1016/j.eswa.2020.113709													
J								A two-step hybrid unsupervised model with attention mechanism for aspect extraction	EXPERT SYSTEMS WITH APPLICATIONS										Aspect extraction; Aspect-level sentiment analysis; Attention model; Deep learning; LSTM; Unsupervised learning	SENTIMENT ANALYSIS; IMPACT	Social networking sites have a wealth of user-generated unstructured text for fine-grained sentiment analysis regarding the changing dynamics in the marketplace. In aspect-level sentiment analysis, aspect term extraction (ATE) task identifies the targets of user opinions in the sentence. In the last few years, deep learning approaches significantly improved the performance of aspect extraction. However, the performance of recent models relies on the accuracy of dependency parser and part-of-speech (POS) tagger, which degrades the performance of the system if the sentence doesn't follow the language constraints and the text contains a variety of multi-word aspect-terms. Furthermore, lack of domain and contextual information is again an issue to extract domain-specific, most relevant aspect terms. The existing approaches are not capable of capturing long term dependencies for noun phrases, which in turn fails to extract some valid aspect terms. Therefore, this paper proposes a two-step mixed unsupervised model by combining linguistic patterns with deep learning techniques to improve the ATE task. The first step uses rules-based methods to extract the single word and multi-word aspects, which further prune domain-specific relevant aspects using fine-tuned word embedding. In the second step, the extracted aspects in the first step are used as label data to train the attention-based deep learning model for aspect-term extraction. The experimental evaluation on the SemEval-16 dataset validates our approach as compared to the most recent and baseline techniques. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113673	10.1016/j.eswa.2020.113673													
J								Cepstral-based clustering of financial time series	EXPERT SYSTEMS WITH APPLICATIONS										Cepstral; Fuzzy c-medoids; Weighting system; Financial time series; NASDAQ index; MIBTEL index	CLASSIFICATION; INDEX; ALGORITHMS; NETWORKS; RETURNS; VECTOR; MODEL	In this paper, following the Partitioning Around Medoids (PAM) approach and the fuzzy theory, we propose a clustering model for financial time series based on the estimated cepstrum which represents the spectrum of the logarithm of the spectral density function. Selecting the optimal set of financial securities to build a portfolio that aims to maximize the risk-return tradeoff is a largely investigated topic in finance. The proposed model inherits all the advantages connected to PAM approach and fuzzy theory and it is able to compute objectively the cepstral weight associated to each cepstral coefficient by means of a suitable weighting system incorporated in the clustering model. In this way, the clustering model is able to tune objectively the different influence of each cepstral coefficient in the clustering process. The proposed clustering model performs better with respect to other clustering models. The proposed clustering model applied to each security sharpe ratio provides an efficient tool of clustering of stocks. (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 15	2020	161								113705	10.1016/j.eswa.2020.113705													
J								Knowledge-based framework for estimating the relevance of scientific articles	EXPERT SYSTEMS WITH APPLICATIONS										Relevance metric; Relevance lexicon; Article reputation; Scientific relevance; Dictionary generation	SENTIMENT ANALYSIS; LEXICON; DICTIONARY; GENERATION	The volume of published papers provided by the scientific community has increased over the last years in a drastic way. This fact has led to having a considerable growth of the topics covered by different publications. Despite topics under discussion on these publications were usually regarded as cutting edge subjects when released in conferences and journals, the restless evolution of science may have faded their relative importance away over the years. This issue undoubtedly poses big challenges to those researchers interested in gathering information to enrich their own background. Consequently, the development of a system able to automatically organize and provide relevance to scientific papers should play a crucial role to address the aforementioned problem. In this paper, the Webelance framework is presented. It makes use of a lexicon and Machine Learning techniques to accomplish these tasks. It has been built by using specific metrics for the scientific domain to measure the relative importance of papers. Several experiments using more than 50; 000 articles focused on the medicine domain have been addressed to illustrate the viability of the proposal. The obtained results both confirm the usability of the system and its good performance. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113692	10.1016/j.eswa.2020.113692													
J								Genetic state-grouping algorithm for deep reinforcement learning	EXPERT SYSTEMS WITH APPLICATIONS										Reinforcement learning; Genetic algorithm; Hybrid method; Monte Carlo Tree Search; Game AI		Although Reinforcement learning has already been considered one of the most important and well-known techniques of machine learning, its applicability remains limited in the real-world problems due to its long initial learning time and unstable learning. Especially, the problem of an overwhelming number of the branching factors under real-time constraint still stays unconquered, demanding a new method for the next generation of reinforcement learning. In this paper, we propose Genetic State-Grouping Algorithm based on deep reinforcement learning. The core idea is to divide the entire set of states into a few state groups. Each group consists of states that are mutually similar, thus representing their common features. The state groups are then processed with the Genetic Optimizer, which finds outstanding actions. These steps help the Deep Q Network avoid excessive exploration, thereby contributing to the significant reduction of initial learning time. The experiment on the real-time fighting video game (FightingICE) shows the effectiveness of our proposed approach. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113695	10.1016/j.eswa.2020.113695													
J								Inference on historical factions based on multi-layered network of historical figures	EXPERT SYSTEMS WITH APPLICATIONS										Historical big data analysis; Historical faction identification; Multi-layered network of historical figures; Semi-supervised learning	ARCHIVE	With immense influx of historical data, quantitative inferences on history based on machine learning is becoming more prevalent, attracting many researchers. In particular, understanding the dynamics of historical factions is important as they shared academic beliefs, political views and interests, in which the interactions between the factions portray general political, social, and economic structure of a certain era. In recent years, studying such dynamics through network-based methods on human networks, constructed from genealogy data, have shown promising results. In this paper, we enhance the identification of historical factions by exploiting multi-layered network of historical figures. To understand the mechanisms of historical factions, it is pivotal to comprehend the change in relation between important historical events. The proposed method consists of constructing a multi-layered network of historical figures and applying semi-supervised learning framework to identify historical factions. The proposed method was applied to the classification of factions in the political turmoil occurred during the 15th to 16th century Korea. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113703	10.1016/j.eswa.2020.113703													
J								Stock market forecasting with super-high dimensional time-series data using ConvLSTM, trend sampling, and specialized data augmentation	EXPERT SYSTEMS WITH APPLICATIONS										Stock market index; Deep learning; Overfitting; Mini-batch sampling; Data augmentation; ConvLSTM	NEURAL-NETWORK; FINANCIAL MARKET; HYBRID MODEL; P 500; PREDICTION; INDEX; VOLATILITY; ALGORITHM; MACHINE; FUSION	Forecasting stock market indexes is an important issue for market participants, because even a small improvement in forecast accuracy may lead to better trading decisions than those of other participants. Rising interest in deep learning has led to its application in stock market forecasting. However, it is still challenging to use market-size time-series data to predict composite index prices. In this study, we propose a new stock market forecasting framework, NuNet, which can successfully learn high-level features from super-high dimensional time-series data. NuNet is an end-to-end integrated neural network framework consisting of two feature extractor modules, a super-high dimensional market information feature extractor and a target index feature extractor. In addition, we propose a mini-batch sampling technique, trend sampling, which probabilistically samples more recent data when training. Furthermore, we propose a novel regularization method, called column-wise random shuffling, which is a data augmentation technique that can be applied to convolutional neural networks. The experiments are comprehensively carried out in three aspects for three indexes, namely S&P500, KOSPI200, and FTSE100. The results demonstrate that the proposed model outperforms all baseline models. Specifically, for the S&P500, KOSPI200, and FTSE100, the overall mean squared error of our proposed model NuNet(DA, T) is 60.79%, 51.29%, and 43.36% lower than that of the baseline model SingleNet(R), respectively. Moreover, we employ trading simulations with realistic transaction costs. Our proposed model outperforms the buy-and-hold strategy being an average of 2.57 times more profitable in three indexes. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113704	10.1016/j.eswa.2020.113704													
J								Heart arrhythmia diagnosis based on the combination of morphological, frequency and nonlinear features of ECG signals and metaheuristic feature selection algorithm	EXPERT SYSTEMS WITH APPLICATIONS										ECG signal; Cardiac arrhythmia recognition; Nonlinear indices; Meta-heuristic optimization algorithm; FF net classifier	CONVOLUTIONAL NEURAL-NETWORK; MYOCARDIAL-INFARCTION; CLASSIFICATION; RECOGNITION; OPTIMIZATION; PREDICTION; MODEL	Cardiac arrhythmia disorder is known as one of the most common diseases in the world. Today, this disease is considered as the leading cause of death in industrial and semi-industrial societies. Various tools and methods have been developed to study the detection of heart diseases, based on analyzing the electrocardiogram (ECG) signal. Due to the simplicity and noninvasive nature, ECG signals are vastly used by physicians to determine the heart problems and abnormalities. In this paper, a computer-aided diagnosis (CAD) system is provided for the automated classification and accurate diagnosis of seven types of cardiac arrhythmias using the ECG signal. The basis of this method is using machine learning algorithms to classify normal rhythm and six abnormal cardiac functions. In the proposed method, after the pre-processing stage, the ECG signal is segmented, and various morphological characteristics, frequency domain features, and nonlinear indices are extracted for the ECG signal. Several metaheuristic optimization algorithms are used to remove redundant or irrelevant features and reduce the feature space dimension. These are used on the combination of the extracted features in which, non-dominated sorting genetic algorithm (NSGA II) as a multi-objective optimization algorithm has the best performance. Furthermore, various machine learning algorithms include k-nearest neighbor (KNN), feed-forward neural network (FF net), fitting neural network (Fit net), radial basis function neural network (RBFNN) and pattern recognition network (Pat net) are employed for the classification. The highest accuracy obtained based on ten-fold cross-validation from the FF net is 98.75%, demonstrates the efficiency of the proposed method and the achieved improvement compared to the other similar works with the same dataset. The combination of a vast and various features from morphology, frequency, and nonlinear characteristics to demonstrate the diverse aspects of ECG signals as well as employing a multi-objective meta-heuristic optimization algorithm for selecting the more correlated features are the main contributions of this study. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113697	10.1016/j.eswa.2020.113697													
J								Improving classification accuracy using data augmentation on small data sets	EXPERT SYSTEMS WITH APPLICATIONS										Deep Learning; Data augmentation; GAN; VAE; Unbalanced sets	NEURAL-NETWORKS	Data augmentation (DA) is a key element in the success of Deep Learning (DL) models, as its use can lead to better prediction accuracy values when large size data sets are used. DA was not very much used with earlier neural network models before 2012, and the reason might be related to the type of models and the size of the data sets used. We investigate in this work, applying several state-of-the-art models based on Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), the effect of DA when using small size data sets, analyzing the results in terms of the prediction accuracy obtained according to the different characteristics of the training samples (number of instances and features, and class unbalance degree). We further introduce modifications to the standard methods used to generate the synthetic samples to alter the class balance representation, and the overall results indicate that with some computational effort a significant increase in prediction accuracy can be obtained when small data sets are considered. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113696	10.1016/j.eswa.2020.113696													
J								Stacked auto-encoder based tagging with deep features for content-based medical image retrieval	EXPERT SYSTEMS WITH APPLICATIONS										CBMIR; CNN; Retrieval; SMOTE; IRMA; Auto-encoder	CLASSIFICATION	Content-based medical image retrieval (CBMIR) is one of the most challenging and ambiguous tasks used to minimize the semantic gap between images and human queries in datasets with rich information content. Similar to the human visual saliency mechanism, CBMIR systems also use the visual features in the images for searching purposes. As a result of this search process, automatically accessing the images is very convenient in large and balanced datasets. Still, it is generally not possible to find such datasets in the medical domain. In this study, a four-step and effective hash code generation technique is presented to reduce the semantic gap between low-level features and high-level semantics for unbalanced medical image datasets. In the first stage, the convolutional neural network (CNN) architecture, the most effective feature representation method available today, is employed to extract discriminative features from images automatically. The features obtained in the last fully connected layer (FCL) at the output of the CNN architecture are used for hash code generation. In the second stage, using the Synthetic Minority Over-sampling Technique (SMOTE), the imbalance between the classes in the dataset is reduced. The solution to the unbalanced problem increases performance by almost 3%. In the third stage, balanced features are converted to a code of 13 symbols by using deep stacked auto-encoder. Finally, this code is translated to the standard 13-character labeling and retrieval code used by the 'Image retrieval in the medical application' (IRMA) dataset, since this is the database with which experiments have been done. IRMA error parameter, classification performance, and retrieval performance of the proposed method are more successful than other state-of-the-art methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113693	10.1016/j.eswa.2020.113693													
J								Search and rescue optimization algorithm: A new optimization method for solving constrained engineering optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Constrained optimization; Engineering optimization problems; Search and rescue optimization algorithm; Metaheuristic algorithms	PARTICLE SWARM OPTIMIZATION; META-HEURISTIC OPTIMIZATION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; DESIGN; MACHINE	A new optimization method namely the Search and Rescue optimization algorithm (SAR) is presented here to solve constrained engineering optimization problems. This metaheuristic algorithm imitates the explorations behavior of humans during search and rescue operations. The e-constrained method is utilized as a constraint-handling technique. Besides, a restart strategy is proposed to avoid local infeasible minima in some complex constrained optimization problems. SAR is applied to solve 18 benchmark constraint functions presented in CEC 2010, 13 benchmark constraint functions, and 7 constrained engineering design problems reported in the specialized literature. The performance of SAR is compared with some state-of-the-art optimization algorithms. According to the statistical comparison results, the performance of SAR is better or highly competitive against the compared algorithms on most of the studied problems. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113698	10.1016/j.eswa.2020.113698													
J								Deep multi-hybrid forecasting system with random EWT extraction and variational learning rate algorithm for crude oil futures	EXPERT SYSTEMS WITH APPLICATIONS										Machine learning; Long short-term memory; Empirical wavelet transform; Deep bidirectional training structure; Random inheritance formula; Variational learning rate	SHORT-TERM-MEMORY; RECURRENT NEURAL-NETWORK; SUPPORT VECTOR MACHINE; TIME-SERIES; STOCK-PRICE; MODE DECOMPOSITION; ENERGY-CONSUMPTION; COMPONENT ANALYSIS; DISCRETE WAVELET; OPTIMIZATION	Machine learning algorithms provide feasibility for crude oil price prediction. In this paper, a novel multi-hybrid predictive neural network model is proposed based on complex deep learning algorithm, which integrates empirical wavelet transform, random inheritance formula error correction algorithm, deep bidirectional LSTM neural network and Elman recurrent neural network with variational learning rate. The prediction model is selected according to the sequence frequency after EWT feature extraction, and the prediction results are obtained by separately predicting and reintegrating. On the basis of individual model, the structure of deep bidirectional training, random inheritance formula and variational learning rate are proposed, which further ameliorate the performance of the model and achieve more effective data information capture. Simultaneously, the examination of variational learning rate provides us with a feasible parameter selection. The proposed model achieves high-precision prediction of crude oil futures price, and stands out in the multi-model comparison analysis and q-DSCID synchronous evaluation, with superior prediction accuracy. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113686	10.1016/j.eswa.2020.113686													
J								Facial expression distribution prediction based on surface electromyography	EXPERT SYSTEMS WITH APPLICATIONS										Facial expression recognition; Emotion distribution learning; Surface electromyography; Principal component analysis	RECOGNITION; CLASSIFICATION	Facial expression recognition plays an important role in research on human-computer interaction. The common facial expressions are mixtures of six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. The current study, however, focused on a single basic emotion on the basis of physiological signals. We proposed emotion distribution learning (EDL) based on surface electromyography (sEMG) for predicting the intensities of basic emotions. We recorded the sEMG signals from the depressor supercilii, zygomaticus major, frontalis medial, and depressor anguli oris muscles. Six features were extracted in the frequency, time, time-frequency, and entropy domains. Principal component analysis (PCA) was used to select the most representative features for prediction. The key idea of EDL is to learn a function that maps the PCA-selected features to the facial expression distributions such that the special description degrees of all basic emotions for an emotion can be learned by EDL. Simultaneously, Jeffrey's divergence considered the relationship between different basic emotions. The performance of EDL was compared with that of multilabel learning based on PCA-selected features. Predicted results were measured by six indices, which could reflect the distance or similarity degree between distributions. We conducted an experiment on six different emotion distributions. Experimental results show that the EDL can predict the facial expression distribution more accurately than the other methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113683	10.1016/j.eswa.2020.113683													
J								ReEx: An integrated architecture for preference model representation and explanation	EXPERT SYSTEMS WITH APPLICATIONS										Explaining recommendations; Recommender systems; Latent factor models; Aspects of preferences; User and item bias; Feature preferences; Feature value preferences; Preference drift	RECOMMENDATION AGENTS	Recommender systems based on collaborative filtering suggest items to users according to the similarity of items or similarity of preferences of other users. Latent factor models produce rather accurate predictions of user preferences, but the latency of the features extracted make it difficult to substantiate a recommendation to a user. The realisation that many aspects tend to exist in rating and social connections data, such as social influence or bias, led to the development of a component-based matrix factorisation approach in earlier work. The ability to quantify the contributions of a component to a recommendation opens up possibilities to identify reasons for recommendations which can be presented to the consumer receiving them. Reviews that accompany rating data can be analysed and correlated with latent factors to provide more detailed reasons for recommendations. This paper introduces a general comprehensive framework which supplements earlier work that models a users' preferences and makes recommendations by extracting reasons from the recommendation framework as well as the accompanying reviews. Both rating-based and review-based reasons for recommending an item are presented in a visual manner. These visualisations help the user understand the origins of the recommendations but also assist businesses in identifying properties in users and communities that open up opportunities for targeted marketing and customer management. The usefulness of the explanation tool is demonstrated with an example recommendation of four items for a user in the Yelp restaurants dataset. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113706	10.1016/j.eswa.2020.113706													
J								An effective discrete artificial bee colony algorithm for multi-AGVs dispatching problem in a matrix manufacturing workshop	EXPERT SYSTEMS WITH APPLICATIONS										Automated guided vehicle; Dispatching; Heuristics; Matrix manufacturing workshop; Discrete artificial bee colony algorithm	VEHICLE-ROUTING PROBLEM; PARTICLE SWARM OPTIMIZATION; SEARCH ALGORITHM; HYBRID; PATH; LOGISTICS; LOCATION; MODEL	This paper addresses a new multiple automatic guided vehicle dispatching problem (AGVDP) from material handling process in a matrix manufacturing workshop. The problem aims to determine a solution with the objective of minimizing the transportation cost including travel cost, penalty cost for violating time and AGV cost. For this purpose, a mixed integer linear programming model is first formulated based on a comprehensive investigation. Then, a discrete artificial bee colony algorithm (DABC) is presented together with some novel and advanced techniques for solving the problem. In the proposed DABC algorithm, a nearest-neighbor-based heuristic based on the problem-specific characteristics is presented to generate an initial solution with a high level of quality. Five effective neighborhood operators are presented to generated neighboring solutions with a high level of diversity. Four theorems are proposed to avoid the unfeasible solutions generated by the neighborhood operators. Two new control parameters are introduced. One is to balance the global exploration and local exploitation in employed bee and onlooker bee phases. The other is to enhance the local exploitation capability of the neighborhood operators. Besides, an insertion-based local search method is provided for the scout bee phase to lead the algorithm to a promising region of the solution space. A comprehensive and thorough evaluation with 110 instances collected from a real-world factory shows that the presented algorithm produces superior results which are also demonstrated to be statistically significant than the existing algorithms in the close related literature. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113675	10.1016/j.eswa.2020.113675													
J								A binary social spider algorithm for uncapacitated facility location problem	EXPERT SYSTEMS WITH APPLICATIONS										Binary optimization; Social Spider Algorithm; Location analysis	DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; BEE COLONY ALGORITHM; SEARCH APPROACH; SELECTION; SIMILARITY; BEHAVIOR	In order to find efficient solutions to real complex world problems, computer sciences and especially heuristic algorithms are often used. Heuristic algorithms can give optimal solutions for large scale optimization problems in an acceptable period. Social Spider Algorithm (SSA), which is a heuristic algorithm created on spider behaviors are studied. The original study of this algorithm was proposed to solve continuous problems. In this paper, the binary version of the Social Spider Algorithm called Binary Social Spider Algorithm (BinSSA) is proposed for binary optimization problems. BinSSA is obtained from SSA, by transforming constant search space to binary search space with four transfer functions. Thus, BinSSA variations are created as BinSSA1, BinSSA2, BinSSA3, and BinSSA4. The study steps of the original SSA are re-updated for BinSSA. A random walking schema in SSA is replaced by a candidate solution schema in BinSSA. Two new methods (similarity measure and logic gate) are used in candidate solution production schema for increasing the exploration and exploitation capacity of BinSSA. The performance of both techniques on BinSSA is examined. BinSSA is named as BinSSA(Sim&Logic). Local search and global search performance of BinSSA is increased by these two methods. Three different studies are performed with BinSSA. In the first study, the performance of BinSSA is tested on the classic eighteen unimodal and multimodal benchmark functions. Thus, the best variation of BinSSA and BinSSA (Sim&Logic) is determined as BinSSA4(Sim&Logic). BinSSA4(Sim&Logic) has been compared with other heuristic algorithms on CEC2005 and CEC2015 functions. In the second study, the uncapacitated facility location problems (UFLPs) are solved with BinSSA(Sim&Logic). UFL problems are one of the pure binary optimization problems. BinSSA is tested on low-scaled, middle-scaled, and large-scaled fifteen UFLP samples and obtained results are compared with eighteen state-of-art algorithms. In the third study, we solved UFL problems on a different dataset named M* with BinSSA(Sim&Logic). The results of BinSSA (Sim&Logic) are compared with the Local Search (LS), Tabu Search (TS), and Improved Scatter Search (ISS) algorithms. Obtained results have shown that BinSSA offers quality and stable solutions. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113618	10.1016/j.eswa.2020.113618													
J								Robust link prediction in criminal networks: A case study of the Sicilian Mafia	EXPERT SYSTEMS WITH APPLICATIONS										Criminal networks; Social network analysis; Network science; Link prediction in uncertain graphs	RESILIENCE; CRIME	Link prediction exercises may prove particularly challenging with noisy and incomplete networks, such as criminal networks. Also, the link prediction effectiveness may vary across different relations within a social group. We address these issues by assessing the performance of different link prediction algorithms on a mafia organization. The analysis relies on an original dataset manually extracted from the judicial documents of operation "Montagna", conducted by the Italian law enforcement agencies against individuals affiliated with the Sicilian Mafia. To run our analysis, we extracted two networks: one including meetings and one recording telephone calls among suspects, respectively. We conducted two experiments on these networks. First, we applied several link prediction algorithms and observed that link prediction algorithms leveraging the full graph topology (such as the Katz score) provide very accurate results even on very sparse networks. Second, we carried out extensive simulations to investigate how the noisy and incomplete nature of criminal networks may affect the accuracy of link prediction algorithms. The experimental findings suggest the soundness of link predictions is relatively high provided that only a limited amount of knowledge about connections is hidden or missing, and the unobserved edges follow some kind of generative law. The different results on the meeting and telephone call networks indicate that the specific features of a network should be taken into careful consideration. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113666	10.1016/j.eswa.2020.113666													
J								Dynamic prioritization of surveillance video data in real-time automated detection systems	EXPERT SYSTEMS WITH APPLICATIONS										Video surveillance; Computer vision; Real-time systems; Object detection	SENSOR NETWORKS	Automated object detection systems are a key component of modern surveillance applications. These systems rely on computationally expensive computer vision algorithms that perform object detection on visual data recorded by surveillance cameras. Due to the security and safety implications of these systems, this visual data st be processed accurately and in real-time. However, many of the frames that are created by the surveillance cameras may be of low importance, providing little or no useful information to the object detection system. Sub-sampling surveillance data by prioritizing important camera frames can greatly reduce unnecessary computation. Consequently, several works have explored dynamic visual data sub-sampling using various modalities of information (ie. spatial or temporal information) for prioritization. Few works, however, have combined and evaluated different modalities of information together for real-time prioritization of visual surveillance data. This work evaluates several individual and combined prioritization metrics derived from different modalities of information for use with a modern deep learning-based object detection algorithm. Both processing time and object detection rate are measured and used to rank the prioritization metrics. A novel approach that uses the historical detection confidences created by the object detection algorithm was demonstrated to be the best standalone prioritization metric. Additionally, a novel ensemble method that uses a KNN regressor to combine the best of the previously evaluated metrics to create a dynamic prioritization method is presented. This ensemble approach is shown to increase the object detection rate by up to 60% as compared to a static sub-sampling baseline as demonstrated using three publicly available datasets. The increased object detection rate was achieved while meeting the real-time constraints of the automated object detection system. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113672	10.1016/j.eswa.2020.113672													
J								Combination of fuzzy and cognitive theories for adaptive e-assessment	EXPERT SYSTEMS WITH APPLICATIONS										Adaptivity; E-assessment; E-learning; Cognitive theories; Fuzzy rules	SYSTEM; PERFORMANCE; CHALLENGES; KNOWLEDGE; TESTS	A crucial factor for successful educational results in computer-based educational systems and e-learning systems is the learner's assessment. Assessment is more effective when it is tailored to each individual student's learning needs and abilities. Therefore, a significant research challenge is to create tests that include exercises/questions/activities etc., which conform to each learner's knowledge level and learning needs and abilities. This goal constitutes the need for creating adaptive tests. However, the area of adaptive e-assessment has not yet been explored sufficiently and thus there is scope for a lot of improvement. To this end, in this paper we present a novel solution for adaptive e-assessment. The novelty and significance lie in the blending of fuzzy logic and cognitive theories for further enhancing the personalization and adaptivity in e-assessment. Particularly, fuzzy sets are used to describe the knowledge level of students in a more realistic way. Furthermore, the cognitive theory of Revised Bloom Taxonomy is used to express the learning objectives that are required to be assessed through the created test. In addition, a fuzzy rule-based reasoner, which decides about the number and the difficulty level of the test items that have to be included into the created personalized test for each level of the Revised Bloom Taxonomy, is used. The fuzzy rules are applied to the fuzzy sets that describe the learners' knowledge level. For the formation of the fuzzy sets and rules, the opinion of several tutors, holding experience in the educational process and instruction, was taken into consideration. The created adaptive test comprises distinct test items based on the individual learning needs of each student. The presented method has been used in two tutoring systems and has been fully evaluated. The evaluation results show great accuracy in the selection of test items for each individual student. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113614	10.1016/j.eswa.2020.113614													
J								Using metaheuristics for the location of bicycle stations	EXPERT SYSTEMS WITH APPLICATIONS										Bike station location; p-Median problem; Metaheuristics	TABU SEARCH; OPTIMIZATION; ALGORITHMS; MODEL	In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113684	10.1016/j.eswa.2020.113684													
J								Cost-sensitive learning classification strategy for predicting product failures	EXPERT SYSTEMS WITH APPLICATIONS										Cost-sensitive learning; Predictive manufacturing; Failure prediction; Imbalance classification; Genetic algorithm; Voronoi diagram	IMBALANCED DATA; PERFORMANCE; RELIABILITY; ALGORITHMS; TOOL	In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113653	10.1016/j.eswa.2020.113653													
J								Genetic programming-based learning of texture classification descriptors from Local Edge Signature	EXPERT SYSTEMS WITH APPLICATIONS										Texture classification; Genetic programming; Texture descriptor; Feature extraction; Local Edge Signature	BINARY PATTERNS; GRAY-SCALE; INFORMATION; FRAMEWORK; ROBUST	Describing texture is a very challenging problem for many image-based expert and intelligent systems (e.g. defective product detection, people re-identification, abnormality investigation in medical imaging and remote sensing applications ... ) since the process of texture classification relies on the quality of the extracted features. Indeed, detecting and extracting features is a hard and time-consuming task that requires the intervention of an expert, notably when dealing with challenging textures. Thus, machine learning-based descriptors have emerged as another alternative to deal with the difficulty of feature extracting. In this work, we propose a new operator, which we named Local Edge Signature (LES) descriptor, to locally represent texture. The proposed texture descriptor is based on statistical information on edge pixels' arrangement and orientation in a specific local region, and it is insensitive to rotation and scale changes. A genetic programming-based approach is then fitted to automatically learn a global tex ture descriptor that we called Genetic Texture Signature (GTS). In fact, a tree representation of individuals is used to generate global texture features by applying elementary operations on LES elements at a set of keypoints, and a fitness function evaluates the descriptors considering intra-class homogeneity and interclass discrimination properties of their generated features. The obtained results, on six challenging tex ture datasets (Brodatz, Outex_TC_00000, Outex_TC_00013, KTH-TIPS, KTH-TIPS2b and UIUCTex), show that the proposed classification method, which is fully automated, achieves state-of-the-art performance, especially when the number of available training samples is limited. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113667	10.1016/j.eswa.2020.113667													
J								Two-stage DEA in banks: Terminological controversies and future directions	EXPERT SYSTEMS WITH APPLICATIONS										Two-stage DEA; Banks; Literature review	DATA ENVELOPMENT ANALYSIS; NONPARAMETRIC FRONTIER MODELS; SLACKS-BASED INEFFICIENCY; NETWORK DEA; BRANCH EFFICIENCY; MANAGERIAL EFFICIENCY; COMMERCIAL-BANKS; DOUBLE-BOOTSTRAP; ISLAMIC BANKS; ENVIRONMENTAL VARIABLES	Given the importance that two-stage Data Envelopment Analysis (DEA) models have attained in recent years, this paper presents a systematic review of the literature on the topic focusing on the banking industry. We discuss the two-stage terminology itself, which is not yet not consolidated. We also discuss the current state-of-the-art and present opportunities, as well as challenges, for future studies. We analyse 59 papers, divided them into ten classes that cover various perspectives of two stage DEA studies, such as the economic context, geographic region of the banking units, methodological characteristics, and type of the models, either internal or external. Additionally, we investigate several controversial points regarding two-stage DEA models, such as the variable selection approach, the technique used in the second stage, and the possible impact of non-discretionary variables on efficiency. Results of the literature review indicate the lack of a uniform or universal terminology for two-stage DEA models in the baking industry. Moreover, the main objective of most papers involves extending or improving DEA models. Radial models, with variable returns of scale, and the intermediation approach are the most frequent configurations. Finally, we identify seven gaps in the literature for both internal and external two-stage DEA models and two specific gaps to external ones. Each gap is discussed in depth in the text and can be considered opportunities for future studies. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113632	10.1016/j.eswa.2020.113632													
J								Self-supervised multimodal reconstruction of retinal images over paired datasets	EXPERT SYSTEMS WITH APPLICATIONS										Self-supervised learning; Eye fundus; Deep learning; Multimodal; Retinography; Angiography	CONVOLUTIONAL NEURAL-NETWORKS	Data scarcity represents an important constraint for the training of deep neural networks in medical imaging. Medical image labeling, especially if pixel-level annotations are required, is an expensive task that needs expert intervention and usually results in a reduced number of annotated samples. In contrast, extensive amounts of unlabeled data are produced in the daily clinical practice, including paired multi-modal images from patients that were subjected to multiple imaging tests. This work proposes a novel self-supervised multimodal reconstruction task that takes advantage of this unlabeled multimodal data for learning about the domain without human supervision. Paired multimodal data is a rich source of clinical information that can be naturally exploited by trying to estimate one image modality from others. This multimodal reconstruction requires the recognition of domain-specific patterns that can be used to complement the training of image analysis tasks in the same domain for which annotated data is scarce. In this work, a set of experiments is performed using a multimodal setting of retinography and fluorescein angiography pairs that offer complementary information about the eye fundus. The evaluations performed on different public datasets, which include pathological and healthy data samples, demonstrate that a network trained for self-supervised multimodal reconstruction of angiography from retinography achieves unsupervised recognition of important retinal structures. These results indicate that the proposed self-supervised task provides relevant cues for image analysis tasks in the same domain. (c) 2020 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	0957-4174	1873-6793				DEC 15	2020	161								113674	10.1016/j.eswa.2020.113674													
J								Comparison of state-of-the-art deep learning APIs for image multi-label classification using semantic metrics	EXPERT SYSTEMS WITH APPLICATIONS										Image multi-label classification comparison; Semantic evaluation; Deep learning; Image understanding		Image understanding heavily relies on accurate multi-label classification. In recent years, deep learning algorithms have become very successful for such tasks, and various commercial and open-source APIs have been released for public use. However, these APIs are often trained on different datasets, which, besides affecting their performance, might pose a challenge to their performance evaluation. This challenge concerns the different object-class dictionaries of the APIs' training dataset and the benchmark dataset, in which the predicted labels are semantically similar to the benchmark labels but considered different simply because they have different wording in the dictionaries. To face this challenge, we propose semantic similarity metrics to obtain richer understating of the APIs predicted labels and thus their performance. In this study, we evaluate and compare the performance of 13 of the most prominent commercial and open-source APIs in a best-of-breed challenge on the Visual Genome and Open Images benchmark datasets. Our findings demonstrate that, while using traditional metrics, the Microsoft Computer Vision, Imagga, and IBM APIs performed better than others. However, applying semantic metrics also unveil the InceptionResNet-v2, Inception-v3, and ResNet50 APIs, which are trained only with the simple ImageNet dataset, as challengers for top semantic performers. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113656	10.1016/j.eswa.2020.113656													
J								The Time-dependent Electric Vehicle Routing Problem: Model and solution	EXPERT SYSTEMS WITH APPLICATIONS										Green logistics; Time-dependent vehicle routing; Efficient constraint handling; Congestion	VARIABLE NEIGHBORHOOD SEARCH; TRAVEL-TIMES; FLEET SIZE; ALGORITHM	We study a new problem named the Time-dependent Electric Vehicle Routing Problem (TDEVRP) which involves routing a fleet of electric vehicles to serve a set of customers and determining the vehicle's speed and departure time at each arc of the routes with the purpose of minimizing a cost function. We propose an integer linear programming (ILP) model to formulate the TDEVRP and show that the state-of-the-art commercial optimizer (CPLEX) can only solve instances of very limited sizes (with no more than 15 customers). We thus propose an iterated variable neighbourhood search (IVNS) algorithm to find near-optimal solutions for larger instances. The key ingredients of IVNS include a fast evaluation method that allows local search moves to be evaluated in constant time O(1), a variable neighbourhood descent (VND) procedure to optimize the node sequences, and a departure time and speed optimization procedure (DSOP) to optimize the speed and departure time on each arc of the routes. The proposed algorithm demonstrates excellent performances on a set of newly created instances. In particular, it can achieve optimal or near-optimal solutions for all small-size instances (with no more than 15 customers) and is robust for large-size instances where the gap between the average and the best solution value is consistently lower than 2.38%. Additional experimental results on 40 benchmark instances of the closely related Time-Dependent Pollution Routing Problem indicate that the proposed IVNS algorithm also performs very well and even discovers 39 new best-known solutions (improved upper bounds). (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 15	2020	161								113593	10.1016/j.eswa.2020.113593													
J								Graph classification algorithm based on graph structure embedding	EXPERT SYSTEMS WITH APPLICATIONS										Graph classification; Graph embedding; Neural network	EXTREME LEARNING-MACHINE; PREDICTION	With the application of data mining in many fields such as information science, bioinformatics, and network intrusion detection, more and more data are showing new features such as strong structuration and complex relationships between data. As a complex data structure, a graph can be used to describe the relationship between things. Traditional graph classification methods based on graph feature vector construction need to select a feature vector construction criterion in advance, such as graph-based theoretical indicators or graph-based topology occurrences, and then extract features from each graph in the graph set according to the designated criterion. However, the construction method of the graph feature vector is easy to lose the graph structural information and requires strong professional knowledge. Inspired by the Word2Vec and Doc2Vec models in the Natural Language Processing (NLP), this paper first constructs a "word list" of graph data consisting of subgraphs. Then a neural network for training graph embedding is designed with the graph itself as its input, and the "word" in the graph and the attribute features of the graph are used as its output, so that the neural network automatically learns the graph embedding corresponding to each graph. The graph embedding not only reflects the features of the graph itself but also includes the relative relationship among graphs. Finally, on the basis of the well-trained graph embedding, the common classifier can be used to classify graphs. Based on real-world bioinformatics and social data sets, the experiments demonstrate that the proposed graph classification algorithm has advantages over the existing graph classification algorithms based on feature vector construction. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113715	10.1016/j.eswa.2020.113715													
J								Foreground detection by ensembles of random polygonal tilings	EXPERT SYSTEMS WITH APPLICATIONS										Foreground detection; Background modelling; Computer vision; Plane tilings	TRACKING	In this work a novel region-based approach for the detection of foreground in video sequences is presented. The model consists of an ensemble of layers or tilings, where each tiling represents, by means of randomly chosen parallelogram regions, the background of the scene. Currently, the image size of video surveillance cameras far exceeds one megapixel (more than 1024 x 768), and pixel-based propos als are poorly suited for near real-time ratios. Therefore, the analysis by pixel is replaced by an analysis by region, improving the final resolution by overlapping regions or parallelograms with different shapes and sizes. Thus, for each frame, each region estimates the probability of belonging to the foreground or background, to finally compute the consensus foreground mask among all the tilings. With this proposal, it is possible to detect the foreground in high resolution sequences, a process that is not feasible using pixel-level techniques. Several experiments have been carried out by employing a wide range of videos. A qualitative and quantitative comparison with the state-of-the-art algorithms is performed by using a well-known video dataset benchmark. The results show the feasibility of our proposal compared with higher resolution methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113518	10.1016/j.eswa.2020.113518													
J								Corporate default forecasting with machine learning	EXPERT SYSTEMS WITH APPLICATIONS										Credit scoring; Machine learning; Random forest; Gradient boosting machine	CLASSIFICATION ALGORITHMS; CREDIT; MODELS	We analyze the performance of a set of machine learning models in predicting default risk, using standard statistical models, such as the logistic regression, as a benchmark. When only a limited information set is available, for example in the case of an external assessment of credit risk, we find that machine learning models provide substantial gains in discriminatory power and precision, relative to statistical models. This advantage diminishes when confidential information, such as credit behavioral indicators, is also available, and it becomes negligible when the dataset is small. Moreover, we evaluate the consequences of using a credit allocation rule based on machine learning ratings on the overall supply of credit and the number of borrowers gaining access to credit. Machine learning models concentrate a greater extent of credit towards safer and larger borrowers, which would result in lower credit losses for their lenders. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113567	10.1016/j.eswa.2020.113567													
J								Saliency detection via multiple-morphological and superpixel based fast fuzzy C-mean clustering network	EXPERT SYSTEMS WITH APPLICATIONS										Gradient image; Superpixel image; Background subtraction; Clustering; Saliency map	OBJECT DETECTION; REGION DETECTION; IMAGE; SEGMENTATION	To model a human perception-based saliency detection algorithm in cluttered and noisy background images is a challenging problem in computer vision. Recently, many saliency detection algorithms have been proposed, which exploit the background information, or boundary priors of an image, to detect the salient object similar to the human attention system. These algorithms may not provide satisfying detection results for color images due to the assimilation of local spatial information. In this paper, we propose an unsupervised saliency detection technique, which uses multi-color space-based morphological gradient images. These gradient images contain different edge features, which are useful to obtain an accurate counter-based superpixel image containing both foreground and background clusters. A robust background measuring technique is implemented to remove background clusters, which describes the spatial information of an image cluster to image boundaries. This geometric clarification method effectively removes multiple low-level clues to produce a precise and uniform saliency map. These initially obtained saliency maps are fused using a multi-map fusion technique, and a compact saliency map prevails. The proposed algorithm is evaluated by executing different experiments on nine data sets. The results show that the proposed algorithm performs well for the detection of both single and multiple objects. The proposed algorithm is computationally efficient and provides better saliency detection results than state-of-the-art algorithms. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113654	10.1016/j.eswa.2020.113654													
J								Succinct contrast sets via false positive controlling with an application in clinical process redesign	EXPERT SYSTEMS WITH APPLICATIONS										Data mining; Contrast set mining; Classification; False discovery rate; Emergency department; Length of stay (LOS)	MINING APPROACH; CARE	Many applications of intelligent systems involve understanding a group of contrastively different outcome (e.g., all survivors of a deadly cancer, a top performing team in a large corporation). The intelligent system needs to identify attributes (features) which best describe or explain the group versus its alternatives. In data mining, this problem is studied under the framework of contrast set mining (CSM). Although CSM is not new, the era of big data has produced new computational and statistical challenges. In particular, existing algorithms fail (1) to perform efficiently in terms of runtime on large-scale datasets and (2) to accommodate simultaneous inference on an overwhelming array of features which are often repetitive and collinear. In this paper, we develop a CSM algorithm which addresses both challenges. The computational challenge is addressed with a tree structure and two theorems while the statistical challenge is addressed with the application of false discovery rate for multiple testing. The computational and statistical advantages of the proposed algorithm over three state-of-the-art algorithms are demonstrated with comprehensive experiments. In addition, we also show the effectiveness of our proposed method in an intelligence-system application involving hospital process redesign. The proposed method not only improves the performance of machine learning systems, but also generates succinct and insightful patterns directly relevant to clinical decision-making. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113670	10.1016/j.eswa.2020.113670													
J								Remanufacturing in a competitive market: A closed-loop supply chain in a Stackelberg game framework	EXPERT SYSTEMS WITH APPLICATIONS										Remanufacturing; Stackelberg game; Closed-loop supply chain; Competitive markets; Progressive transfer prices	CORPORATE SOCIAL-RESPONSIBILITY; DECISION-MAKING; FUZZY; DESIGN; COORDINATION; PERFORMANCE; CONTRACTS; CONSUMERS; PRODUCTS; MODELS	In this work, we study a closed-loop supply chain with remanufacturing in a competitive market, where the supply chain is a price taker. A Stackelberg game framework is considered, where a manufacturer (leader) has sufficient channel power over a retailer (follower). We develop analytical models to show that a closed-loop supply chain with remanufacturing in a competitive market can achieve the same return rate as that in the centrally coordinated channel by employing a contract between the manufacturer and the retailer. The contract consists of a wholesale price, and a progressive transfer price scheme with additional allowances (or charges) for the returned products. Our models also take into account the effect of green initiatives on consumers' purchase intention by relating the green achievement from remanufacturing to consumer demand. In practice, our models can be directly applied to a closed-loop supply chain with remanufacturing in a competitive market. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113655	10.1016/j.eswa.2020.113655													
J								Performance improvement strategies on Cuckoo Search algorithms for solving the university course timetabling problem	EXPERT SYSTEMS WITH APPLICATIONS										Course timetabling; Cuckoo Search; Levy flights; Experimental design; Self-adaptive; Metaheuristics	GENETIC ALGORITHM; OPTIMIZATION; DESIGN; HEURISTICS; MECHANISM; TOOL	The university course timetabling problem (UCTP) arises every academic year and must be solved by academic staff with/without a course timetabling tool. A Hybrid Self-adaptive Cuckoo Search-based Timetabling (HSCST) tool has been developed for minimising the total university operating costs. The HSCST tool was applied to solve eleven problem instances obtained from the Faculty of Engineering, Naresuan University. The performance improvements of the Cuckoo Search (CS) algorithm embedded within the proposed tool were demonstrated using three strategies: parameter setting approaches (static and adaptive), movement strategies (Levy flights and Gaussian random walks), and local search hybridisation techniques. Sequential computational experiments were designed and conducted to investigate the efficiency of the three proposed strategies. The statistical analysis on the computational results suggested that the proposed algorithms significantly outperformed the conventional CS, Particle Swarm Optimisation (PSO), and hybrid PSO for all problem instances. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113732	10.1016/j.eswa.2020.113732													
J								A novel approach to predictive analysis using attribute-oriented rough fuzzy sets	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy information system (FIS); delta-Cluster; (gamma, delta)-Rough fuzzy set ((gamma, delta)-RFS); TPELDTP; Decision making	FEATURE-SELECTION; RULES; MODEL	In this study, a forecasting decision-making method is put forward to deal with multi-attribute decision making problems. On the basis of rough set theory, (gamma,delta)-rough fuzzy sets are presented using delta-clusters in data mining. Furthermore, several characteristics of the upper and lower (gamma,delta)-approximations are obtained. Lastly, the difference between the fuzzy set A of the object and the upper and lower rough (gamma,delta)-approximation operators on A is analyzed. We also design a novel algorithm to forecast decision making and provide a related example illustrating the new method. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 15	2020	161								113644	10.1016/j.eswa.2020.113644													
J								Encoded summarization: summarizing documents into continuous vector space for legal case retrieval	ARTIFICIAL INTELLIGENCE AND LAW										Legal case; Document retrieval; Document summarization; Deep learning; Document representation	ONTOLOGY	We present our method for tackling a legal case retrieval task by introducing our method of encoding documents by summarizing them into continuous vector space via our phrase scoring framework utilizing deep neural networks. On the other hand, we explore the benefits from combining lexical features and latent features generated with neural networks. Our experiments show that lexical features and latent features generated with neural networks complement each other to improve the retrieval system performance. Furthermore, our experimental results suggest the importance of case summarization in different aspects: using provided summaries and performing encoded summarization. Our approach achieved F1 of 65.6% and 57.6% on the experimental datasets of legal case retrieval tasks.																	0924-8463	1572-8382				DEC	2020	28	4					441	467		10.1007/s10506-020-09262-4													
J								Analyzing cognitive processes from complex neuro-physiologically based data: some lessons	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Analysis of cognitive processes; Brain correlates; Classification; Machine learning; Feature selection	BRAIN ACTIVITY; RECOGNITION; PHASE	In the past few years, due to their ability to extract multivariate correlations, machine learning tools have become more and more important for discovery of information in very complex data sets. This has had specific application to various data sets related to human brain tasks. However, this is far from a simple and direct methodology. Some of the issues involve dealing with the extreme signal to noise ratios, as well as variation between different individuals. Moreover, the huge amount of features relative to the number of data points is a challenge. As a result, in attacking these problems, we found it necessary to adapt a large variety of methodologies; chosen to overcome specific obstructions for specific problems. In this paper, we describe our experience working on several examples at the edge of capabilities of these systems and describe the various and variant methodologies we needed to overcome these sort of challenges. Hopefully these cases will serve as a guideline for other applications.																	1012-2443	1573-7470				DEC	2020	88	11-12					1125	1153		10.1007/s10472-019-09669-z													
J								Deep learning models for brain machine interfaces	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Deep learning; Convolutional neural networks; Autoencoders; Brain machine interface; Affective computing	NETWORKS; EMOTION	Deep Learning methods have been rising in popularity in the past few years, and are now used as a fundamental component in various application domains such as computer vision, natural language processing, bioinformatics. Supervised learning with Convolutional Neural Networks has become the state of the art approach in many image related works. However, despite the great success of deep learning methods in other areas they remain relatively unexplored in the brain imaging field. In this paper we make an overview of recent achievements of Deep Learning to automatically extract features from brain signals that enable building Brain-Machine Interfaces (BMI). Major challenge in the BMI research is to find common subject-independent neural signatures due to the high brain data variability across multiple subjects. To address this problem we propose a Deep Neural Autoencoder with sparsity constraint as a promising approach to extract hidden features from Electroencephalogram data (in-dept feature learning) and build a subject-independent noninvasive BMI in the affective neuro computing framework. Future direction for research are also outlined.																	1012-2443	1573-7470				DEC	2020	88	11-12					1175	1190		10.1007/s10472-019-09668-0													
J								Lattice map spiking neural networks (LM-SNNs) for clustering and classifying image data	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Spiking neural networks (SNN); Self-Organized Maps (SOMs); Self clustering; Online learning; Robostness; Unsupervised learning; Winner-take-all classification	NEURONS; ORIENTATION	Spiking neural networks(SNNs) with a lattice architecture are introduced in this work, combining several desirable properties of SNNs andself-organized maps(SOMs). Networks are trained with biologically motivated, unsupervised learning rules to obtain a self-organized grid of filters via cooperative and competitive excitatory-inhibitory interactions. Several inhibition strategies are developed and tested, such as (i)incrementally increasinginhibition level over the course of network training, and (ii) switching the inhibition level from low to high (two-level) after an initial training segment. During thelabelingphase, the spiking activity generated by data with known labels is used to assign neurons to categories of data, which are then used to evaluate the network's classification ability on a held-out set of test data. Several biologically plausible evaluation rules are proposed and compared, including a population-level confidence rating, and ann-gram inspired method. The effectiveness of the proposed self-organized learning mechanism is tested using the MNIST benchmark dataset, as well as using images produced by playing the Atari Breakout game.																	1012-2443	1573-7470				DEC	2020	88	11-12					1237	1260		10.1007/s10472-019-09665-3													
J								PadChest: A large chest x-ray image dataset with multi-label annotated reports	MEDICAL IMAGE ANALYSIS										X-Ray image dataset; Deep neural networks; Radiographic findings; Differential diagnoses; Anatomical locations	RADIOGRAPHS; SYSTEM; LUNG	We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,0 00 images obtained from 67,0 00 patients that were interpreted and reported by radiologists at San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray databases suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101797	10.1016/j.media.2020.101797													
J								Deep hiearchical multi-label classification applied to chest X-ray abnormality taxonomies	MEDICAL IMAGE ANALYSIS										Hierarchical multi-label classification; Chest X-ray; Computer aided diagnosis	PROJECT	Chest X-rays (CXRs) are a crucial and extraordinarily common diagnostic tool, leading to heavy research for computer-aided diagnosis (CAD) solutions. However, both high classification accuracy and meaningful model predictions that respect and incorporate clinical taxonomies are crucial for CAD usability. To this end, we present a deep hierarchical multi-label classification (HMLC) approach for CXR CAD. Different than other hierarchical systems, we show that first training the network to model conditional probability directly and then refining it with unconditional probabilities is key in boosting performance. In addition, we also formulate a numerically stable cross-entropy loss function for unconditional probabilities that provides concrete performance improvements. Finally, we demonstrate that HMLC can be an effective means to manage missing or incomplete labels. To the best of our knowledge, we are the first to apply HMLC to medical imaging CAD. We extensively evaluate our approach on detecting abnormality labels from the CXR arm of the Prostate, Lung, Colorectal and Ovarian (PLCO) dataset, which comprises over 198,000 manually annotated CXRs. When using complete labels, we report a mean area under the curve (AUC) of 0.887, the highest yet reported for this dataset. These results are supported by ancillary experiments on the PadChest dataset, where we also report significant improvements, 1.2% and 4.1% in AUC and average precision, respectively over strong "flat" classifiers. Finally, we demonstrate that our HMLC approach can much better handle incompletely labelled data. These performance improvements, combined with the inherent usefulness of taxonomic predictions, indicate that our approach represents a useful step forward for CXR CAD. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101811	10.1016/j.media.2020.101811													
J								A unified framework for multimodal structure-function mapping based on eigenmodes	MEDICAL IMAGE ANALYSIS											HUMAN CONNECTOME; DIFFUSION MRI; CONNECTIVITY; NETWORKS; MODEL	Characterizing the connection between brain structure and brain function is essential for understanding how behaviour emerges from the underlying anatomy. A number of studies have shown that the network structure of the white matter shapes functional connectivity. Therefore, it should be possible to predict, at least partially, functional connectivity given the structural network. Many structure-function mappings have been proposed in the literature, including several direct mappings between the structural and functional connectivity matrices. However, the current literature is fragmented and does not provide a uniform treatment of current methods based on eigendecompositions. In particular, existing methods have never been compared to each other and their relationship explicitly derived in the context of brain structure-function mapping. In this work, we propose a unified computational framework that generalizes recently proposed structure-function mappings based on eigenmodes. Using this unified framework, we highlight the link between existing models and show how they can be obtained by specific choices of the parameters of our framework. By applying our framework to 50 subjects of the Human Connectome Project, we reproduce 6 recently published results, devise two new models and provide a direct comparison between all mappings. Finally, we show that a glass ceiling on the performance of mappings based on eigenmodes seems to be reached and conclude with possible approaches to break this performance limit. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101799	10.1016/j.media.2020.101799													
J								AGE challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography	MEDICAL IMAGE ANALYSIS										AS-OCT; Anterior chamber angle; Angle closure classification; Scleral spur localization	DIABETIC-RETINOPATHY; CHAMBER; CLASSIFICATION; VALIDATION; DISEASES; IMAGES; SYSTEM	Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually lead to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large dataset of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results for the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixels (10 mu m) in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved satisfactory performances, with two best obtaining an accuracy rate of 100%. These artificial intelligence techniques have the potential to promote new developments in AS-OCT image analysis and image-based angle closure glaucoma assessment in particular. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101798	10.1016/j.media.2020.101798													
J								Position paper on COVID-19 imaging and AI: From the clinical needs and technological challenges to initial AI solutions at the lab and national level towards a new era for AI in healthcare	MEDICAL IMAGE ANALYSIS										COVID-19; Imaging; AI		In this position paper, we provide a collection of views on the role of AI in the COVID-19 pandemic, from clinical requirements to the design of AI-based systems, to the translation of the developed tools to the clinic. We highlight key factors in designing system solutions - per specific task; as well as design issues in managing the disease at the national level. We focus on three specific use-cases for which AI systems can be built: early disease detection, management in a hospital setting, and building patient-specific predictive models that require the combination of imaging with additional clinical data. Infrastructure considerations and population modeling in two European countries will be described. This pandemic has made the practical and scientific challenges of making AI solutions very explicit. A discussion concludes this paper, with a list of challenges facing the community in the AI road ahead. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101800	10.1016/j.media.2020.101800													
J								Group-level cortical surface parcellation with sulcal pits labeling	MEDICAL IMAGE ANALYSIS										Morphometry; Cortical folding; Sulcal pits	SPATIAL-DISTRIBUTION; GENERIC MODEL; LANDMARKS; SEGMENTATION; VARIABILITY; FRAMEWORK; CORTEX; BRAIN	Sulcal pits are the points of maximal depth within the folds of the cortical surface. These shape descriptors give a unique opportunity to access to a rich, fine-scale representation of the geometry and the developmental milestones of the cortical surface. However, using sulcal pits analysis at group level requires new numerical tools to establish inter-subject correspondences. Here, we address this issue by taking advantage of the geometrical information carried by sulcal basins that are the local patches of surfaces surrounding each sulcal pit. Our framework consists in two phases. First, we present a new method to generate a population-specific atlas of this sulcal basins organi- zation as a fold-level parcellation of the cortical surface. Then, we address the labeling of individual sulcal pits and corresponding basins with respect to this atlas. To assess their validity, we applied these methodological advances on two different populations of healthy subjects. The first database of 137 adults allowed us to compare our method to the state-of-the-art and the second database of 209 children, aged between 0 and 18 years, illustrates the adaptability and relevance of our method in the context of pediatric data showing strong variations in cortical volume and folding. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				DEC	2020	66								101749	10.1016/j.media.2020.101749													
J								BIAS: Transparent reporting of biomedical image analysis challenges	MEDICAL IMAGE ANALYSIS										Biomedical challenges; Good scientific practice; Biomedical image analysis; Guideline	LOCALIZATION; VALIDATION; MEDICINE	The number of biomedical image analysis challenges organized per year is steadily increasing. These international competitions have the purpose of benchmarking algorithms on common data sets, typically to identify the best method for a given problem. Recent research, however, revealed that common practice related to challenge reporting does not allow for adequate interpretation and reproducibility of results. To address the discrepancy between the impact of challenges and the quality (control), the Biomedical Image Analysis ChallengeS (BIAS) initiative developed a set of recommendations for the reporting of challenges. The BIAS statement aims to improve the transparency of the reporting of a biomedical image analysis challenge regardless of field of application, image modality or task category assessed. This article describes how the BIAS statement was developed and presents a checklist which authors of biomedical image analysis challenges are encouraged to include in their submission when giving a paper on a challenge into review. The purpose of the checklist is to standardize and facilitate the review process and raise interpretability and reproducibility of challenge results by making relevant information explicit. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				DEC	2020	66								101796	10.1016/j.media.2020.101796													
J								The reliability of a deep learning model in clinical out-of-distribution MRI data: A multicohort study	MEDICAL IMAGE ANALYSIS										Neuroimaging; Deep learning; Domain shift; Clinical application	ALZHEIMERS-DISEASE; ATROPHY; CLASSIFICATION; MULTICENTER	Deep learning (DL) methods have in recent years yielded impressive results in medical imaging, with the potential to function as clinical aid to radiologists. However, DL models in medical imaging are often trained on public research cohorts with images acquired with a single scanner or with strict protocol harmonization, which is not representative of a clinical setting. The aim of this study was to investigate how well a DL model performs in unseen clinical datasets-collected with different scanners, protocols and disease populations-and whether more heterogeneous training data improves generalization. In total, 3117 MRI scans of brains from multiple dementia research cohorts and memory clinics, that had been visually rated by a neuroradiologist according to Scheltens' scale of medial temporal atrophy (MTA), were included in this study. By training multiple versions of a convolutional neural network on different subsets of this data to predict MTA ratings, we assessed the impact of including images from a wider distribution during training had on performance in external memory clinic data. Our results showed that our model generalized well to datasets acquired with similar protocols as the training data, but substantially worse in clinical cohorts with visibly different tissue contrasts in the images. This implies that future DL studies investigating performance in out-of-distribution (OOD) MRI data need to assess multiple external cohorts for reliable results. Further, by including data from a wider range of scanners and protocols the performance improved in OOD data, which suggests that more heterogeneous training data makes the model generalize better. To conclude, this is the most comprehensive study to date investigating the domain shift in deep learning on MRI data, and we advocate rigorous evaluation of DL models on clinical data prior to being certified for deployment. (C) 2020 The Author(s). Published by Elsevier B.V.																	1361-8415	1361-8423				DEC	2020	66								101714	10.1016/j.media.2020.101714													
J								Image-level detection of arterial occlusions in 4D-CTA of acute stroke patients using deep learning	MEDICAL IMAGE ANALYSIS										4D-CTA; Stroke; Deep Learning; Convolutional Neural Networks	ISCHEMIC-STROKE; CT PERFUSION; CLASSIFICATION; THROMBECTOMY; ONSET	The triage of acute stroke patients is increasingly dependent on four-dimensional CTA (4D-CTA) imaging. In this work, we present a convolutional neural network (CNN) for image-level detection of intracranial anterior circulation artery occlusions in 4D-CTA. The method uses a normalized 3D time-to-signal (TTS) representation of the input image, which is sensitive to differences in the global arrival times caused by the potential presence of vascular pathologies. The TTS map presents the time within the cranial cavity at which the signal reaches a percentage of the maximum signal intensity, corrected for the baseline intensity. The method was trained and validated on (n = 214) patient images and tested on an independent set of (n = 279) patient images. This test set included all consecutive suspected-stroke patients admitted to our hospital in 2018. The accuracy, sensitivity, and specificity were 92%, 95%, and 92%. The area under the receiver operating characteristics curve was 0.98 (95% CI: 0.95-0.99). These results show the feasibility of automated stroke triage in 4D-CTA. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				DEC	2020	66								101810	10.1016/j.media.2020.101810													
J								Integrating a cognitive assistant within a critique-based recommender system	COGNITIVE SYSTEMS RESEARCH										Recommender system; Cognitive assxoistant; Cognitive systems		Recommender systems are cognitive computing systems designed to support humans in their decision-making processes through convincing, timely product suggestions. In the field of recommender systems, critique-based recommenders have been widely applied as an effective approach for guiding users through a product space in pursuit of suitable products. To date, no critique-based approach has included an assistant that support users in their search in a pleasant way. In this paper, we describe how we integrate an assistant within a critique-based recommender. We consider the proposed assistant to be cognitive because its reasoning process when recommending products is based on a cognitively-inspired clustering algorithm. The proposal is evaluated by users and compared with a non-assistant approach. The results of this research demonstrate that the integration of a cognitive assistant within the recommender improves the user experience and increases the performance of the recommendation process, i.e., users need fewer cycles to achieve the desired product or service. (C) 2020 Published by Elsevier B.V.																	2214-4366	1389-0417				DEC	2020	64						1	14		10.1016/j.cogsys.2020.07.003													
J								Role of the secondary visual cortex in HMAX model for object recognition	COGNITIVE SYSTEMS RESEARCH										Biologically inspired model; Object Recognition; Human visual system; HMAX; Secondary visual cortex	HIERARCHICAL-MODELS; K-MEANS; FEATURES	The models inspired by visual systems of life creatures (e.g., human, mammals, etc.) have been very successful in addressing object recognition tasks. For example, Hierarchical Model And X (HMAX) effectively recognizes different objects by modeling the V1, V4, and IT regions of the human visual system. Although HMAX is one of the superior models in the field of object recognition, its implementation has been limited due to some disadvantages such as the unrepeatability of the process under constant conditions, extreme redundancy, high computational load, and time-consuming. In this paper, we aim at revising the HMAX approach by adding the model of the secondary region (V2) in the human visual system which leads to removing the mentioned drawbacks of standard HMAX. The added layer selects repeatable and more informative features that increase the accuracy of the proposed method by avoiding the redundancy existing in the conventional approaches. Furthermore, this feature selection strategy considerably reduces the huge computational load. Another contribution of our model is highlighted when a small number of training images is available where our model can efficiently cope with this issue. We evaluate our proposed approach using Caltech5 and GRAZ-02 database as two famous benchmarks for object recognition tasks. Additionally, the results are compared with standard HMAX that validate and highlight the efficiency of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						15	28		10.1016/j.cogsys.2020.07.001													
J								Fuzzy rough sets: Survey and proposal of an enhanced knowledge representation model based on automatic noisy sample detection	COGNITIVE SYSTEMS RESEARCH										Automatic noisy sample detection; Classification; Fuzzy rough sets; Model; Robustness; Survey	FEATURE-SELECTION; ATTRIBUTE REDUCTION; 3-WAY DECISIONS; INSTANCE SELECTION; CLASSIFICATION; ALGORITHM; APPROXIMATIONS; CLASSIFIERS; DEPENDENCY; SIMILARITY	Fuzzy Rough Set (FRS) theory, which has been emerged thanks to unifying Rough Set and Fuzzy Set ones, is a powerful mathematical tool for handling and processing real data of imprecise, incomplete, inconsistent and uncertain nature. It has drawn attention of many researchers, scientists and industrials in various domains over the last three decades. However, different studies have showed that its classical knowledge representation model has a main weakness linked to its sensitivity to data noise which decreases both its effectiveness and application scope. In this paper, we survey the current FRS paradigms developed to deal with this issue and propose a new FRS model based on the Automatic Noisy Sample Detection (ANSD-FRS) able to cope with noise influence in classification tasks. Besides, we study the principal properties of this new model and reformulate the most applied FRS concepts relying on its operators. Numerous experiments have been conducted to analyze the ANSD-FRS behavior compared to the commonly used FRS models reputed as the most noise-resistant paradigms. These experiment results have proved the performance and robustness of the ANSD-FRS in comparison with those renowned models. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						37	56		10.1016/j.cogsys.2020.05.001													
J								A multi-modal approach to cognitive training and assistance in minimally invasive surgery	COGNITIVE SYSTEMS RESEARCH										Spatial cognition; Cognitive training; Computer-adaptive testing and training; Laparoscopy; Auditory display; Sonification	VISUAL-SPATIAL ABILITY; LAPAROSCOPIC SURGERY; AUDITORY-FEEDBACK; PERFORMANCE; PERCEPTION; SIMULATION; NAVIGATION; SKILLS; SONIFICATION; ACQUISITION	Minimally-invasive surgery (MIS) offers many benefits to patients, but is considerably more difficult to learn and perform than is open surgery. One main reason for the observed difficulty is attributable to the visuo-spatial challenges that arise in MIS, taxing the surgeons' cognitive skills. In this contribution, we present a new approach that combines training and assistance as well as the visual and the auditory modality to help surgeons to overcome these challenges. To achieve this, our approach assumes two main components: An adaptive, individualized training component as well as a component that conveys spatial information through sound. The training component (a) specifically targets the visuo-spatial processes crucial for successful MIS performance and (b) trains surgeons in the use of the sound component. The second component is an auditory display based on a psychoacoustic sonification, which reduces and avoids some of the commonly experienced MIS challenges. Implementations of both components are described and their integration is discussed. Our approach and both of its components go beyond the current state of the art in important ways. The training component has been explicitly designed to target MIS-specific visuo-spatial skills and to allow for adaptive testing, promoting individualized learning. The auditory display is conveying spatial information in 3-D space. Our approach is the first that encompasses both training for improved mastery and reduction of cognitive challenges in MIS. This promises better tailoring of surgical skills and assistance to the needs and the capabilities of the surgeons and, thus, ultimately, increased patient safety and health. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						57	72		10.1016/j.cogsys.2020.07.005													
J								Planet Braitenberg: Experiments in virtual psychology	COGNITIVE SYSTEMS RESEARCH										Virtual environment; Virtual robotics; Artificial intelligence; Game engine; Computational simulation; Artificial life; Situated cognition; Embodied cognition; Unity		Braitenberg vehicles are simple robotic platforms, equipped with rudimentary sensor and motor components. Such vehicles have typically featured as part of thought experiments that are intended to show how complex behaviours are apt to emerge from the interaction of inner control mechanisms with aspects of bodily structure and features of the wider (extra-agential) environment. The present paper describes a framework for creating Braitenberg-like vehicles, which is built on top of a widely used and freely available game engine, namely, the Unity game engine. The framework can be used to study the behaviour of virtual vehicles within a multiplicity of virtual environments. All aspects of the vehicle's design, as well as the wider virtual environment in which the vehicle is situated, can be modified during the design phase, as well as at runtime. The result is a general-purpose simulation capability that is intended to provide the foun- dation for studies in so-called computational situated cognition a field of study whose primary objective is to support the computational modelling of cognitive processes associated with the physically-embodied, environmentally-embedded, and materially-extended mind. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						73	95		10.1016/j.cogsys.2020.06.001													
J								A Classification Framework using a Diverse Intensified Strawberry Optimized Neural Network (DISON) for Clinical Decision-making	COGNITIVE SYSTEMS RESEARCH										Clinical data; Strawberry plant optimization; ANN; Bio-inspired computing; Extremely randomized trees	DIAGNOSIS; PREDICTION; SELECTION	A novel classification framework for clinical decision making that uses an Extremely Randomized Tree (ERT) based feature selection and a Diverse Intensified Strawberry Optimized Neural network (DISON) is proposed. DISON is a Feed Forward Artificial Neural Network where the optimization of weights and bias is done using a two phase training strategy. Two algorithms namely Strawberry Plant Optimization (SPO) algorithm and Gradient-descent Back-propagation algorithm are used sequentially to identify the optimum weights and bias. The novel two phase training method and the stochastic duplicate-elimination strategy of SPO helps in addressing the issue of local optima associated with conventional neural networks. The relevant attributes are selected based on the feature importance values computed using an ERT classifier. Vertebral Column, Pima Indian diabetes (PID), Cleveland Heart disease (CHD) and Statlog Heart disease (SHD) datasets from the University of California Irvine machine learning repository are used for experimentation. The framework has achieved an accuracy of 87.17% for Vertebral Column, 90.92% for PID, 93.67% for CHD and 94.5% for SHD. The classifier performance has been compared with existing works and is found to be competitive in terms of accuracy, sensitivity and specificity. Wilcoxon test confirms the statistical superiority of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						98	116		10.1016/j.cogsys.2020.08.003													
J								Toward ethical cognitive architectures for the development of artificial moral agents	COGNITIVE SYSTEMS RESEARCH										Ethical cognitive architectures; Cognitive functions; Artificial agents; Machine ethics; Artificial moral agents	MACHINE MORALITY; DECISION-MAKING; SYSTEMS; MOTIVATION; ROBOTICS; VEHICLES; CARE	New technologies based on artificial agents promise to change the next generation of autonomous systems and therefore our interaction with them. Systems based on artificial agents such as self-driving cars and social robots are examples of this technology that is seeking to improve the quality of people's life. Cognitive architectures aim to create some of the most challenging artificial agents commonly known as bio-inspired cognitive agents. This type of artificial agent seeks to embody human-like intelligence in order to operate and solve problems in the real world as humans do. Moreover, some cognitive architectures such as Soar, LIDA, ACT-R, and iCub try to be fundamental architectures for the Artificial General Intelligence model of human cognition. Therefore, researchers in the machine ethics field face ethical questions related to what mechanisms an artificial agent must have for making moral decisions in order to ensure that their actions are always ethically right. This paper aims to identify some challenges that researchers need to solve in order to create ethical cognitive architectures. These cognitive architectures are characterized by the capacity to endow artificial agents with appropriate mechanisms to exhibit explicit ethical behavior. Additionally, we offer some reasons to develop ethical cognitive architectures. We hope that this study can be useful to guide future research on ethical cognitive architectures. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						117	125		10.1016/j.cogsys.2020.08.010													
J								The theory of learning styles applied to distance learning	COGNITIVE SYSTEMS RESEARCH										Distance learning; Learning styles; Behavior patterns; Distance education		Distance Education (DE) associated with the use of Virtual Learning Environments (VLE) as interaction tools between the student and the educator has become a large research niche spread around the world. Techniques to improve learning effectiveness in VLEs seek to find relation-ships connections between pedagogical advances and educational technological resources available in VLEs. In this context, this work sought to associate the theory of Learning Styles with the behavior of the student of Distance Education, observing their interaction with the Virtual Learning Environment and trying to associate them with their learning style identified by the CHAEA questionnaire. For this purpose, the CHAEA questionnaire was applied to a group of distance learning students and the correlation between their interactions with VLE and their learning styles was verified. The results show that there is no correlation between these elements, which may show, in fact, the lack of coherence between these theories. Seeking a model for data, a linear regression was applied and the results were, then, confirmed. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						134	145		10.1016/j.cogsys.2020.08.004													
J								Segment routing based energy aware routing for software defined data center	COGNITIVE SYSTEMS RESEARCH										Software Defined Data Center (SDDC); Multipath TCP (MPTCP); Segment Routing (SR); Data Center Network (DCN); Software Defined Network (SDN); Segment Label Stack (SLS); Open Daylight (ODL)		Despite the fact that most of the data centers are software-defined, the multifaceted network architecture and increase in network traffic make data centers to suffer from overhead. Multipath TCP supports multiple paths for a single routing session and ensures proper utilization of bandwidth over all available links. As rise in number of nodes in data center is frequent and drastic, scalability issue limits the performance of many existing techniques. Segment Routing is vibrant in reducing scalability disputes and routing overhead. Segment routing approach combined with MPTCP traffic result in efficient routing approach. The downfall of the link capacity due to drastic incoming traffic remains as a major concern in data center network which enforces preventing link energy depletion due to high network traffic. Our proposed work, segment routing based energy aware routing approach for software defined data center aims to achieve throughput maximization through preserving link residual capacity and proper utilization of links. As well, our approach shows a decrease in length of segment label stack with respect to maximum segment label depth. Analysis is done by comparing the executions of other existing approaches in a single-controller environment with our energy-aware routing approach in a distributed environment. Distributed controller setup prevents network from single point of failure. It helps to prevent controller overhead and provides improved network performance through throughput. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						146	163		10.1016/j.cogsys.2020.08.009													
J								Evaluating agents' trustworthiness within virtual societies in case of no direct experience	COGNITIVE SYSTEMS RESEARCH										Social recommendation; Trust; Multi-agent system	SOCIAL NETWORKS; TRUST; REPUTATION	A great deal of effort has been made to introduce trust models to assess trustworthiness within virtual societies. The great majority of them makes extensive use of direct experience as the main source of information, considering recommendation/reputation and inferential processes just later, as a secondary mechanism to refine trust assessment. In this kind of networks, unfortunately, direct experience might not always represent the best solution to assess trustworthiness. In fact, their highly dynamic structure promotes an increase of the average number of interconnections among agents. This in turn negatively affects the degree of knowledge the agents possess about each specific individual, i.e. direct experience. To date, however, it has not been said much about how to face these situations. It is fundamental to find an effective approach for trust assessment even in lack of direct experience, which is the central focus of this research. By the means of a multi-agent social simulation, we consider the situation in which an agent can just access indirect knowledge for trust assessment, namely recommendations of specific individuals or whole categories of individuals. Then, we compare the efficiency of these two approaches in order to identify when it is more convenient to rely on the first or on the second one. As expected, our results confirm that the dynamic nature of these networks strongly affects the role of categories. We modeled this feature introducing the "turnover" in the simulations, whereby the higher is the turnover the more convenient it is relying on categories. Besides this confirmatory result, our simulations highlight the higher degree of robustness of categories in the presence of unreliable recommenders. Such a result is even more significant if there is no available information about how reliable the recommenders are. The results we obtained are in accordance with the current literature and can be of important interest for the development of this sector. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						164	173		10.1016/j.cogsys.2020.08.005													
J								A fuzzy-based driver assistance system using human cognitive parameters and driving style information	COGNITIVE SYSTEMS RESEARCH										Human robot interaction; Human cognition; Driver assistance system; Fuzzy logic		Reducing the number of traffic accidents due to human errors is an urgent need in several countries around the world. In this scenario, the use of human-robot interaction (HRI) strategies has recently shown to be a feasible solution to compensate human limitations while driving. In this work we propose a HRI system which uses the driver's cognitive factors and driving style information to improve safety. To achieve this, deep neural networks based approaches are used to detect human cognitive parameters such as sleepiness, driver's age and head posture. Additionally, driving style information is also obtained through speed analysis and external traffic information. Finally, a fuzzy-based decision-making stage is proposed to manage both human cognitive information and driving style, and then limit the maximum allowed speed of a vehicle. The results showed that we were able to detect human cognitive parameters such as sleepiness - 63% to 88% accuracy-, driver's age -80% accuracy- and head posture -90.42% to 97.86% accuracy- as well as driving style -87.8% average accuracy. Based on such results, the fuzzy-based architecture was able to limit the maximum allowed speed for different scenarios, reducing it from 50 km/h to 17 km/h. Moreover, the fuzzy-based method showed to be more sensitive with respect to inputs changes than a previous published weighted-based inference method. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						174	190		10.1016/j.cogsys.2020.08.007													
J								PSO-GA based hybrid with Adam Optimization for ANN training with application in Medical Diagnosis	COGNITIVE SYSTEMS RESEARCH										ANNs (Artificial Neural Networks); PSO (Particle Swarm Optimization); GA (Genetic Algorithm); Gradient Descent; BP (Backward Propagation); Adam Optimization; Medical Diagnosis		This paper introduces a novel PSO-GA based hybrid training algorithm with Adam Optimization and contrasts performance with the generic Gradient Descent based Backpropagation algorithm with Adam Optimization for training Artificial Neural Networks. We aim to overcome the shortcomings of the traditional algorithm, such as slower convergence rate and frequent convergence to local minima, by employing the characteristics of evolutionary algorithms. PSO has a property of faster convergence rate, which can be exploited to account for the slower pace of convergence of the traditional BP (which is due to low values of gradients). In contrast, the integration with GA complements the drawback of convergence to local minima as GA, possesses the capability of efficient global search. So by this integration of these algorithms, we propose our new hybrid algorithm for training ANNs. We compare both the algorithms for the application of medical diagnosis. Results display that the proposed hybrid training algorithm, significantly outperforms the traditional training algorithm, by enhancing the accuracies of the ANNs with an increase of 20% in the average testing accuracy and 0.7% increase in the best testing accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				DEC	2020	64						191	199		10.1016/j.cogsys.2020.08.011													
J								New fundamental modulation technique with SHE using shuffled frog leaping algorithm for multilevel inverters	EVOLVING SYSTEMS										Multilevel converter; Modulation index; Shuffled frog leaping algorithm; Selective harmonic elimination	HARMONIC ELIMINATION; ECONOMIC-DISPATCH; OPTIMIZATION	This paper presents the selective harmonic elimination of cascade H-bridge multilevel inverters using shuffled frog leaping algorithm. This algorithm takes the advantages of the genetic-based memetic algorithm and the social behavior-based PSO algorithm. In addition, this study provides a new fundamental modulation technique with SHE for multilevel inverters which can generate output waveforms with a full range of modulation indices. There are two control objectives formulated as a multi-objective optimization problem. The mentioned algorithm finds the optimal solution set of switching angles. Simulation is performed in MATLAB to confirm the validity of the proposed method.																	1868-6478	1868-6486				DEC	2020	11	4					541	557		10.1007/s12530-019-09273-w													
J								Application of hybrid forecast engine based intelligent algorithm and feature selection for wind signal prediction	EVOLVING SYSTEMS										Neural network; Wind power forecast; Hybrid forecast engine; Feature selection; EEMD	ARTIFICIAL NEURAL-NETWORK; POWER-SYSTEM; MODEL; PRICE	This paper presents a new prediction model based on empirical mode decomposition, feature selection and hybrid forecast engine. The whole structure of proposed model is based on nonstationarity and non-convex nature of wind power signal. The hybrid forecast engine consists of three main stages as; empirical mode decomposition, an intelligent algorithm and back propagation neural network. All parameters of proposed neural network will be optimized by intelligent algorithm. Effectiveness of the proposed model is tested with real-world hourly data of wind farms in Spain and Texas. In order to demonstrate the validity of the proposed model, it is compared with several other wind speed and power forecast techniques. Obtained results confirm the validity of the developed approach.																	1868-6478	1868-6486				DEC	2020	11	4					559	573		10.1007/s12530-019-09271-y													
J								Energy-efficient clustering method for wireless sensor networks using modified gravitational search algorithm	EVOLVING SYSTEMS										Wireless sensor network; Clustering methods; Energy consumption; Gravitational search algorithm; Fuzzy logic controller	OPTIMIZATION; WSN; PSO	Past decades have witnessed the advancement of wireless sensor networks (WSNs) in both academic and industrial communities. Clustering is one of the most popular methods to increase the lifespan of WSNs. The optimal number of cluster heads and how to organize the clusters are the most important issues to be addressed in the clustering methods. In this paper, we proposed a novel user-independent and dynamical method to calculate the optimal number of clusters, organize the clusters, and determine the best cluster heads in each round. In this method, efficient energy consumption and link quality were considered to compute the optimal number of clusters. Then, the algorithm began to organize the compact clusters with high energy level cluster heads. We investigated a new fitness function in order to achieve these objectives. A new version of gravitational search algorithm (GSA) was used to solve this optimization problem. In this algorithm, the power distance sums scaling method was applied to calculate the mass values. Then, a fuzzy logic controller is employed to identify the parameter of this algorithm to control the exploitation and exploration abilities of the method during the computational process of the algorithm. Then, the novel version of GSA was applied to reach an appropriate solution for the fitness function, find the optimal number of clusters, and properly organize these clusters. To evaluate the effectiveness of the proposed method, several experiments were performed and the obtained results were compared with the results of other popular clustering methods. The simulation results revealed that the performance of the modified GSA was better than other state-of-the-art meta-heuristic optimization algorithms. Moreover, the proposed method for the clustering problem in WSNs outperformed other popular clustering methods and increased the lifetime of WSNs.																	1868-6478	1868-6486				DEC	2020	11	4					575	587		10.1007/s12530-019-09264-x													
J								Automated skin lesion division utilizing Gabor filters based on shark smell optimizing method	EVOLVING SYSTEMS										Lesion division; Median filter; Classification; Gabor filters; Shark smell optimization; K-means; Characteristic space	HYBRID NEURAL-NETWORK; OPTIMIZATION ALGORITHM; FORECAST ENGINE; FUZZY; SEGMENTATION; PRICE	In this work, we have proposed an unmonitored method in order to divide the photograph of lesions on the skin using the fabric characteristics. The fabric characteristic in the photograph is described using frequency of energy which is utilized by statistic based approaches called Gabor filter. Optimization of Gabor filters is done by a meta-heuristic algorithm which is named shark smell optimization. Every Gabor filter in the bank is modified to identify the trend of a given frequency and direction in case it is convoluted with photograph of the lesion. The convolving is conducted in the Fourier space. Also the yielded solution photograph is a characteristic which has joined the characteristic vector. Ultimately, the K-means division is utilized in order to distinguish the lesion from the regular part of skin in the photograph. The empirical outcomes indicate that the suggested analytic technique is completely productive in detecting the lesion on the skin for medical purposes. Obtained results demonstrate the validity of proposed optimization approach.																	1868-6478	1868-6486				DEC	2020	11	4					589	598		10.1007/s12530-018-9258-4													
J								Modelling gene interaction networks from time-series gene expression data using evolving spiking neural networks	EVOLVING SYSTEMS										Artificial intelligence; Evolving spiking neural networks; Transcriptome; Gene expression; Microarray; Data analysis; Gene interaction networks; Nickel allergy; Allergic contact dermatitis	SPATIOTEMPORAL DATA; CLASSIFICATION; BRAIN; NEUCUBE; ARRAY; SET	The genetic mechanisms responsible for the differentiation, metabolism, morphology and function of a cell in both normal and abnormal conditions can be uncovered by the analysis of transcriptomes. Mining big data such as the information encoded in nucleic acids, proteins, and metabolites has challenged researchers for several years now. Even though bioinformatics and system biology techniques have improved greatly and many improvements have been done in these fields of research, most of the processes that influence gene interaction over time are still unknown. In this study, we apply state-of-the art spiking neural network techniques to model, analyse and extract information about the regulatory processes of gene expression over time. A case study of microarray profiling in human skin during elicitation of eczema is used to examine the temporal association of genes involved in the inflammatory response, by means of a gene interaction network. Spiking neural network techniques are able to learn the interaction between genes using information encoded from the time-series gene expression data as spikes. The temporal interaction is learned, and the patterns of activity extracted and analysed with a gene interaction network. Results demonstrated that useful knowledge can be extracted from the data by using spiking neural network, unlocking some of the possible mechanisms involved in the regulatory process of gene expression.																	1868-6478	1868-6486				DEC	2020	11	4					599	613		10.1007/s12530-019-09269-6													
J								An improved particle swarm optimization (PSO): method to enhance modeling of airborne particulate matter (PM10)	EVOLVING SYSTEMS										Environmental pollution; Air quality; Particle swarm optimization (PSO); ANFIS	HYBRID ANFIS MODEL; AIR-POLLUTION; PREDICTION; DIAGNOSIS; MACHINE; SVM	Nowadays, it is of paramount importance for human health the monitoring and modelling of air quality. Among the different pollutants, there are some that are considerable more difficult to model due to their chemical composition. Some of these are particulate matter (particles <= 10 microns, PM10, and particles <= 2.5 microns, PM2.5), which can cause respiratory diseases or even cause premature deaths. Furthermore, There are several models that can be used to evaluate air quality. In this contribution, the combination of a neuro-fuzzy based method with particle swarm optimization is proposed to crucially increase accuracy when dealing with the non-linear behavior of airborne particulate matter (PM10). Several experiments were carried out to show the feasibility of the proposed method and to show that even when the nature of the data, that has dynamic behavior, variance in the spread of data, the present of outliers, climatic conditions, among other factors, the modeling of this particular set of data may be made accurately, showing the robustness and feasibility of the proposed method.																	1868-6478	1868-6486				DEC	2020	11	4					615	624		10.1007/s12530-019-09263-y													
J								Finger kunckcle patterns based person recognition via bank of multi-scale binarized statistical texture features	EVOLVING SYSTEMS										Biometric; Image local descriptor; BSIF; Dimensionality reduction; Classification	PERFORMANCE; EIGENFACES	This paper proposes a novel finger knuckle patterns (FKP) based biometric recognition system that utilizes multi-scale bank of binarized statistical image features (B-BSIF) due to their improved expressive power. The proposed system learns a set of convolution filters to form different BSIF feature representations. Later, the learnt filters are applied on each FKP traits to determine the top performing BSIF features and respective filters are used to create a bank of features named B-BSIF. In particular, the presented framework, in the first step, extracts the region of interest (ROI) from FKP images. In the second step, the B-BSIF coding method is applied on ROIs to obtain enhanced multi-scale BSIF features characterized by top performing convolution filters. The extracted feature histograms are concatenated in the third step to produce a large feature vector. Then, a dimensionality reduction procedure, based on principal component analysis and linear discriminant analysis techniques (PCA + LDA), is carried out to attain compact feature representation. Finally, nearest neighbor classifier based on the cosine Mahalanobis distance is used to ascertain the identity of the person. Experiments with the publicly available PolyU FKP dataset show that the presented framework outperforms previously-proposed methods and is also able to attain very high accuracy both in identification and verification modes.																	1868-6478	1868-6486				DEC	2020	11	4					625	635		10.1007/s12530-018-9260-x													
J								Spatial shape feature descriptors in classification of engineered objects using high spatial resolution remote sensing data	EVOLVING SYSTEMS										Spatial shape features; Support vector machine; Spectral angle mapper; Spectral information divergence; Connected component analysis; Spatial and spectral resolution	SPECTRAL INFORMATION; FEATURE-EXTRACTION; URBAN; IMAGES; SEGMENTATION; COOCCURRENCE	Spatial and spectral features are two important attributes that form the knowledge based database, useful in classification of engineered objects, using remote sensing data. Spectral features alone may be insufficient to identify buildings and roads in urban areas due to spectral homogeneity and similarity exhibited by them. This has led researchers to explore the spatial features described in terms of shape descriptors to improve accuracy of classification of engineered objects. This paper discusses the parameters of spatial shape features and the method for implementing these features for improving the extraction of engineered objects, using the support vector machine (SVM). SVM classified results obtained using spatial shape features is compared with gray level co-occurrence statistical features in which the former has shown better classification accuracy for buildings and roads. The classification accuracy is also calculated using spectral features of buildings and roads by classifiers such as spectral angle mapper and spectral information divergence. The analysis shows that spatial shape features improve the classification results of buildings and roads in urban areas.																	1868-6478	1868-6486				DEC	2020	11	4					647	660		10.1007/s12530-019-09275-8													
J								New prior distribution for Bayesian neural network and learning via Hamiltonian Monte Carlo	EVOLVING SYSTEMS										Bayesian multilayer feedforward neural network; Prior; Hamiltonian Monte Carlo; Evidence framework; Hyper-parameter; Regularization	SMOOTHING L-1/2 REGULARIZATION; GRADIENT-METHOD	A prior distribution of weights for Multilayer feedforward neural network in Bayesian point of view plays a central role toward generalization. In this context, we propose a new prior law for weights parameters which motivate the network regularization more thanl1 early proposed. To train the network, we have based on Hamiltonian Monte Carlo, it is used to simulate the prior and the posterior distribution. The generated samples are used to approximate the gradient of the evidence which allows to re-estimate the hyperparameters that balance a trade off between the likelihood term and regularized term, on the other hand we use the obtained posterior samples to estimate the network output. The case problem studied in this paper includes a regression and classification tasks. The obtained results illustrate the advantages of our approach in term of error rate compared to old approach, unfortunately our method consomme time before convergence.																	1868-6478	1868-6486				DEC	2020	11	4					661	671		10.1007/s12530-019-09288-3													
J								An improved decision support system for ABC inventory classification	EVOLVING SYSTEMS										Inventory management; ABC inventory classification; Decision support systems; Multi-criteria decision making; Fuzzy classification	OPTIMIZATION; SELECTION	In this study, an Improved Decision Support System (IDSS) is developed to help the decision makers in their inventory classification decisions. For the first time in this paper, the novel IDSS for ABC classification is developed. Certain new algorithms regarding the manufacturing company's features are applied in a framework of IDSS. The IDSS is developed as modular structure and provided the integrated modules of "Data-Base" and "ABC Analysis". In the developed IDSS, the appropriate ABC classification models are considered among Annual Dollar Usage (ADU), Analytic Hierarchy Process (AHP), Scoring (SCR), Fuzzy C-means Algorithm (FCM), and Analytic Network Process (ANP). Some issues and applicability of the IDSS are illustrated with real case problems in the paper. The proposed IDSS software is considerably decreased the time for the inventory classification. In the meantime it could be easily used in various sectors. Therefore, the proposed IDSS significantly contributed to obtaining more accurate and quickly modifiable ABC classification in real cases. Furthermore, the user friendly software can be updated readily according to recent developments in the market.																	1868-6478	1868-6486				DEC	2020	11	4					683	696		10.1007/s12530-019-09276-7													
J								A novel adaptive output feedback control for DC-DC boost converter using immersion and invariance observer	EVOLVING SYSTEMS										DC-DC boost converter; Immersion and invariance; Estimator design; Exponential stability; Adaptive control	VOLTAGE TRACKING CONTROL; CONTROL SCHEME; DESIGN; NETWORK	This paper presents a class of novel adaptive output feedback controller for DC-DC boost converter with global exponential stability. In addition, the control input constraint is considered in stability analysis. The proposed adaptive control scheme is constructed to estimate input voltage and inductor current using output voltage and control signal information. In order to estimate unavailable state and parameter, immersion and invariance technique is employed. The effectiveness of the proposed method is investigated via experimental test and the practical results endorse the efficiency of this adaptive controller.																	1868-6478	1868-6486				DEC	2020	11	4					707	715		10.1007/s12530-019-09268-7													
J								Robust aerial image mosaicing algorithm based on fuzzy outliers rejection	EVOLVING SYSTEMS										UAV images; Bidirectional condition; RANSAC; Fuzzy outliers rejection	MOSAICKING	The use of unmanned aerial vehicles (UAVs) imagery for acquiring data is constantly evolving, due to the ease of use and low data acquisition costs. All of these made UAVs very popular with end customers and data acquisition companies. For most cases, the acquired UAV images need further more processing before being analyzed, and that because of changing of illumination condition in aerial environment and fog generated because of UAV flying speed. Image mosaicing is a practical solution for these problems; in which overlapped views of the same scene are combined to form a large image with high quality. The common problem associated with image mosaicing algorithms is the false associations (outliers) produced when defining the overlapping region between every two successive views. This article presents an image mosaicing algorithm based on efficient fuzzy technique for outliers rejection. Our proposed technique is based on using RANdom SAmpling Consensus (RANSAC) and bidirectional approaches with a fuzzy inference system in order to separate between inliers and outliers. The experimental results prove that the proposed method has good performance for aerial images and gives better results when compared with other techniques. Thus our approach is insensitive to the ordering, orientation, scale and illumination of the images.																	1868-6478	1868-6486				DEC	2020	11	4					717	729		10.1007/s12530-019-09279-4													
J								A machine learning model to identify early stage symptoms of SARS-Cov-2 infected patients	EXPERT SYSTEMS WITH APPLICATIONS										SARS-Cov-2; COVID-19; Coronavirus; Machine learning; Early stage symptom	COVID-19	The recent outbreak of the respiratory ailment COVID-19 caused by novel coronavirus SARS-Cov2 is a severe and urgent global concern. In the absence of effective treatments, the main containment strategy is to reduce the contagion by the isolation of infected individuals; however, isolation of unaffected individuals is highly undesirable. To help make rapid decisions on treatment and isolation needs, it would be useful to determine which features presented by suspected infection cases are the best predictors of a positive diagnosis. This can be done by analyzing patient characteristics, case trajectory, comorbidities, symptoms, diagnosis, and outcomes. We developed a model that employed supervised machine learning algorithms to identify the presentation features predicting COVID-19 disease diagnoses with high accuracy. Features examined included details of the individuals concerned, e.g., age, gender, observation of fever, history of travel, and clinical details such as the severity of cough and incidence of lung infection. We implemented and applied several machine learning algorithms to our collected data and found that the XGBoost algorithm performed with the highest accuracy (>85%) to predict and select features that correctly indicate COVID-19 status for all age groups. Statistical analyses revealed that the most frequent and significant predictive symptoms are fever (41.1%), cough (30.3%), lung infection (13.1%) and runny nose (8.43%). While 54.4% of people examined did not develop any symptoms that could be used for diagnosis, our work indicates that for the remainder, our predictive model could significantly improve the prediction of COVID-19 status, including at early stages of infection. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113661	10.1016/j.eswa.2020.113661													
J								Automatic robust estimation for exponential smoothing: Perspectives from statistics and machine learning	EXPERT SYSTEMS WITH APPLICATIONS										Forecasting; Exponential smoothing; M-estimators; Boosting; Bagging	TIME-SERIES; NEURAL-NETWORKS; TEMPORAL AGGREGATION; REGRESSION; CLASSIFICATION; ALGORITHMS; MODEL; TREES; STATE; REGULARIZATION	A major challenge in automating the production of a large number of forecasts, as often required in many business applications, is the need for robust and reliable predictions. Increased noise, outliers and structural changes in the series, all too common in practice, can severely affect the quality of forecasting. We investigate ways to increase the reliability of exponential smoothing forecasts, the most widely used family of forecasting models in business forecasting. We consider two alternative sets of approaches, one stemming from statistics and one from machine learning. To this end, we adapt M-estimators, boosting and inverse boosting to parameter estimation for exponential smoothing. We propose appropriate modifications that are necessary for time series forecasting while aiming to obtain scalable algorithms. We evaluate the various estimation methods using multiple real datasets and find that several approaches outperform the widely used maximum likelihood estimation. The novelty of this work lies in (1) demonstrating the usefulness of M-estimators, (2) and of inverse boosting, which outperforms standard boosting approaches, and (3) a comparative look at statistics versus machine learning inspired approaches. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113637	10.1016/j.eswa.2020.113637													
J								An efficient approach for outlier detection from uncertain data streams based on maximal frequent patterns	EXPERT SYSTEMS WITH APPLICATIONS										Outlier detection; Maximal frequent pattern mining; Uncertain data streams; Deviation factors; Data streaming mining	ITEMSETS	Outlier identification is an important technology to improve the credibility of data and aims at detecting patterns that rarely appear and exhibit a significant difference from other data. However, the detection accuracy achieved by the simple deviation factors of existing pattern-based outlier detection methods is not competitive. In addition, given the large scale of uncertain data streams, the efficiency of many pattern-based outlier detection methods is not high because they use a vast number of frequent patterns to conduct the outlier detection. In this paper, to contend with the uncertain data streams, we propose a maximal-frequent-pattern-based outlier detection method, namely, MFP-OD, for identifying the outliers with a lower time cost. For further improving the detection accuracy of existing outlier detection methods, we design three deviation factors to measure the deviation degree of each transaction. The experimental results indicate that the proposed MFP-OD method can quickly and accurately identify the outliers from uncertain data streams. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113646	10.1016/j.eswa.2020.113646													
J								Evaluation of customer behavior with temporal centrality metrics for churn prediction of prepaid contracts	EXPERT SYSTEMS WITH APPLICATIONS										Churn prediction; Social network analysis; Similarity forests; Time series; Centrality metrics	TELECOMMUNICATION INDUSTRY; SECTOR	The telecommunication industry is a saturated market where a proper implementation of a retention campaign is critical to be competitive, since retaining a customer is cheaper than attracting a new one. Hence, it is crucial to detect customer behavioral patterns and define accurate approaches to predict potential churners. Multiple researchers have used binary classification methods to predict churn of customers. Some of them verify that customers' social relationships influence the decision of changing the operator. We propose a novel method to extract the dynamic relevance of each customer using social network analysis techniques with a binary classification method called similarity forests. The dynamic importance of each customer is determined by applying various centrality metrics over temporal graphs, to represent the relationships between customers and to extract behavioral patterns of churners and non-churners. These relationships are established in a temporal graph using the call detail records (CDR) of telco's customers. In this paper, we compare the performance of different centrality metrics applied over two types of temporal graphs: Time-Order Graph and Aggregated Static Graph. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113553	10.1016/j.eswa.2020.113553													
J								Detection of unexpected findings in radiology reports: A comparative study of machine learning approaches	EXPERT SYSTEMS WITH APPLICATIONS										Unexpected findings; Natural language processing; Machine learning; Deep learning; Text report classification; Spanish radiology reports	LOGISTIC-REGRESSION; CLASSIFICATION; CANCER; GRADIENT; RISK	This study explores machine learning methods for the detection of unexpected findings in Spanish radiology reports. Regarding radiological reports, unexpected findings are the set of radiological signs identified at a certain imaging modality exam which meet two characteristics: they are not apparently related with the a priori expected results of the radiological exam and involve a clinical emergency or urgency situation that must be reported shortly to the prescribing physician or another medical specialist as well as to the patient in order to preserve life and/or prevent dangerous occurrences. Several traditional machine learning and deep learning classification algorithms are evaluated and compared. To carry out the task we use 5947 anonymous radiology reports from HT medica. Experimental results suggest that the performance of the Convolutional Neural Networks models are better than traditional machine learning. The best F1 score for the identification of an unexpected finding was 90%. Finally, we also perform an error analysis which will guide us to achieve better results in the future. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113647	10.1016/j.eswa.2020.113647													
J								A novel method based on symbolic regression for interpretable semantic similarity measurement	EXPERT SYSTEMS WITH APPLICATIONS										Knowledge engineering; Symbolic regression; Similarity learning; Semantic similarity measurement	INFORMATION-CONTENT; RELATEDNESS; WORDNET; MODEL	The problem of automatically measuring the degree of semantic similarity between textual expressions is a challenge that consists of calculating the degree of likeness between two text fragments that have none or few features in common according to human judgment. In recent times, several machine learning methods have been able to establish a new state-of-the-art regarding the accuracy, but none or little attention has been paid to their interpretability, i.e. the extent to which an end-user could be able to understand the cause of the output from these approaches. Although such solutions based on symbolic regression already exist in the field of clustering, we propose here a new approach which is being able to reach high levels of interpretability without sacrificing accuracy in the context of semantic textual similarity. After a complete empirical evaluation using several benchmark datasets, it is shown that our approach yields promising results in a wide range of scenarios. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113663	10.1016/j.eswa.2020.113663													
J								Supporting better practice benchmarking: A DEA-ANN approach to bank branch performance assessment	EXPERT SYSTEMS WITH APPLICATIONS										Artificial Neural Network; Data Envelopment Analysis; Banking; Performance; Best Practice; Benchmarking; Artificial Intelligence	DATA ENVELOPMENT ANALYSIS; ARTIFICIAL NEURAL-NETWORKS; TECHNICAL EFFICIENCY; DESIGN SCIENCE; METHODOLOGY; PROFITABILITY; INTEGRATION; PREDICTION	The quest for best practices may lead to an increased risk of poor decision-making, especially when aiming to attain best practice levels reveals that efforts are beyond the organization's present capabilities. This situation is commonly known as the "best practice trap". Motivated by such observation, the purpose of the present paper is to develop a practical methodology to support better practice benchmarking, with an application to the banking sector. In this sense, we develop a two-stage hybrid model that employs Artificial Neural Network (ANN) via integration with Data Envelopment Analysis (DEA), which is used as a preprocessor, to investigate the ability of the DEA-ANN approach to classify the sampled branches of a Greek bank into predefined efficiency classes. ANN is integrated with a family of radial and non-radial DEA models. This combined approach effectively captures the information contained in the characteristics of the sampled branches, and subsequently demonstrates a satisfactory classification ability especially for the efficient branches. Our prediction results are presented using four performance measures (hit rates): percent success rate of classifying a bank branch's performance exactly or within one class of its actual performance, as well as just one class above the actual class and just one class below the actual class. The proposed modeling approach integrates the DEA context with ANN and advances benchmarking practices to enhance the decision-making process for efficiency improvement. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113599	10.1016/j.eswa.2020.113599													
J								Entity disambiguation with context awareness in user-generated short texts	EXPERT SYSTEMS WITH APPLICATIONS										Entity disambiguation; Conceptualization; User-generated short text; Knowledge base	DISPLAY NAMES; ACCOUNTS	Conceptualization is to obtain the most appropriate concepts for noun terms (entities) under different contexts, which plays an important role in human knowledge understanding. However, in natural language, entities are often ambiguous, which creates difficulties in conceptualization. To accurately conceptualize, we must eliminate the ambiguity of entities. Existing methods mainly rely on similar or related entities in context for disambiguation. However, due to the sparsity of user-generated short texts, the number of entities that can be extracted from them is limited. In this paper, we propose an entity disambiguation method, which consists of three steps. (1) Measuring the correlation between terms, which uses both corpus and knowledge information to capture the specific semantic relationship. (2) Selecting informative terms, which considers various types of contextual terms, not just entities, thereby mitigating the effects of text sparsity. (3) Prioritizing informative terms to highlight their discriminative power, which reduces noise interference. Finally, the target entity is disambiguated based on informative terms. Experimental results on ground-truth datasets demonstrate that the proposed method outperforms baseline methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113652	10.1016/j.eswa.2020.113652													
J								Improving interpretability of word embeddings by generating definition and usage	EXPERT SYSTEMS WITH APPLICATIONS										Word embeddings interpretability; Definition modeling; Definition generation; Usage modeling; Usage generation	REPRESENTATIONS	Word embeddings are substantially successful in capturing semantic relations among words. However, these lexical semantics are difficult to be interpreted. Definition modeling provides a more intuitive way to evaluate embeddings by utilizing them to generate natural language definitions of corresponding words. This task is of great significance for practical application and in-depth understanding of word representations. We propose a novel framework for definition modeling, which can generate reasonable and understandable context-dependent definitions. Moreover, we introduce usage modeling and study whether it is possible to utilize embeddings to generate example sentences of words. These ways are a more direct and explicit expression of embedding's semantics for better interpretability. We extend the single task model to multi-task setting and investigate several joint multi-task models to combine usage modeling and definition modeling together. Experimental results on existing Oxford dataset and a new collected Oxford-2019 dataset show that our single-task model achieves the state-of-the-art result in definition modeling and the multi-task learning methods are helpful for two tasks to improve the performance. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113633	10.1016/j.eswa.2020.113633													
J								Incorporating part-whole hierarchies into fully convolutional network for scene parsing	EXPERT SYSTEMS WITH APPLICATIONS										Scene parsing; Caps-score layer; Fully convolutional network; The spatial hierarchy between features		In this paper, a new approach to scene parsing is proposed which integrates part-whole hierarchies relationship in the last feature map to assign a semantic class label to each pixel. Recently, deep learning based approaches have had a great impact on scene parsing. However, these methods could not preserve the spatial information about the high-level (or mid-level) features. Hence, Hinton, one of the fathers of deep learning, introduced the capsule concept to encode pose information such as orientation. All of the capsules which have a similar pose matrix value are grouped to form a parent capsule. However, their work has two challenges: 1) the extensive time required to perform dynamic routing agreement to obtain the routing coefficient and 2) the variation of the appearance and the spatial hierarchies between part capsules and their corresponding parent are not encoded. In this study, to consider these challenges, the general Hough transform (GHT) and tensor normal distribution are utilized to propose a novel capsule concept. In this case, each capsule has k offset vectors for each semantic class. The offset vectors are oriented from the capsule to the k other capsules which have an effective role in assigning that capsule to a specific semantic class. The problem formulation is proposed such that evaluating the approach on large datasets is feasable. Also, a new score function is designed to accumulate the vote's strengths for capsule class estimation. To do so, we use tensor normal distribution in which the covariance matrix is defined as the Kronecker product of the capsule feature covariance and the between-capsule covariance. The proposed approach, for the first time, encodes the relations between part capsules to vote to a whole capsule through the between-capsule covariance matrix. To evaluate our proposed approach, it is applied to SiftFlow, NYUD-v2 and PASCAL VOC 2012 datasets. The results show that our approach achieves superior performance. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113662	10.1016/j.eswa.2020.113662													
J								Stock index futures trading impact on spot price volatility. The CSI 300 studied with a TGARCH model	EXPERT SYSTEMS WITH APPLICATIONS										TGARCH; CSI 300 index; CSI 300 stock index futures; Index futures trading; Spot price variability; Co-integration causality tests	MARKET VOLATILITY; TIME-SERIES; ERROR-CORRECTION; CASH MARKET; LINEAR-DEPENDENCE; OPTION VALUATION; COINTEGRATION; CHINA; TRANSMISSION; DISCOVERY	A TGARCH modeling is argued to be the optimal basis for investigating the impact of index futures trading on spot price variability. We discuss the CSI-300 index (China-Shanghai-Shenzhen-300-Stock Index) as a test case. The results prove that the introduction of CSI-300 index futures (CSI-300-IF) trading significantly reduces the volatility in the corresponding spot market. It is also found that there is a stationary equilibrium relationship between the CSI-300 spot and CSI-300-IF markets. A bidirectional Granger causality is also detected. "Finally", it is deduced that spot prices are predicted with greater accuracy over a 3 or 4 lag day time span. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113688	10.1016/j.eswa.2020.113688													
J								Accuracy weighted diversity-based online boosting	EXPERT SYSTEMS WITH APPLICATIONS										Data stream; Concept drift; Online boosting; Diversity	DRIFT DETECTION; ENSEMBLE; CLASSIFIERS; MAJORITY	Target distributional change occurring in a data stream known as concept drift, causes a challenging task for an online learning method, as the accuracy of an online learning method may decrease due to these changes. In this paper, the Accuracy Weighted Diversity-based Online Boosting (AWDOB) method has been proposed, which is based on Adaptable Diversity-based Online Boosting (ADOB) and, other modifications. More precisely, AWDOB uses the proposed accuracy weighting scheme which is based on previous expert's results of the sums of correctly classified and incorrectly classified instances to calculate the weight of current expert, which improved the overall accuracy of the AWDOB. Experiments were conducted to compare the accuracy results of AWDOB against other methods using ten real-world datasets and thirty-two artificial datasets. Artificial datasets were generated by the four artificial data generators which included gradual and abrupt concept drifts within them. Experimental results suggest that AWDOB beats the accuracy results of other tested methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113723	10.1016/j.eswa.2020.113723													
J								Emerging technologies and industrial leadership. A Wikipedia-based strategic analysis of Industry 4.0	EXPERT SYSTEMS WITH APPLICATIONS										Emerging technologies; Industry 4.0; Innovation; Technology foresight; Industrial leadership; Germany	INSTITUTIONAL COMPLEMENTARITIES; COMPARATIVE ADVANTAGES; INNOVATION; GERMAN; CAPITALISM; VARIETIES; TRENDS; IDENTIFICATION; KNOWLEDGE; CLUSTERS	Among emerging technologies large attention has been devoted to the so called Fourth Industrial Revolution, or Industry 4.0, which is also a case study of a major industrial policy initiative, led by Germany. In the field of methodologies to profile and monitor emerging technologies the role of Wikipedia has been recently explored. In this paper we extend the use of Wikipedia by comparing the German edition with the world largest edition (in English) in order to examine whether there are significant structural differences. We first validate the use of Wikipedia for emerging technologies and for cross-language comparisons as a tool for (almost) real time strategic analysis. We then extract all Wikipedia pages related to Industry 4.0, build up a knowledge network and study its topological properties in the two editions. We find striking differences, which can be explained with respect to the persistence of the industrial pattern of specialization of Germany with respect to all other countries. Emerging technologies introduce novelty but also preserve path-dependency in the pattern of specialization. The implications for companies and policy makers are discussed. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113645	10.1016/j.eswa.2020.113645													
J								Extracting highlights of scientific articles: A supervised summarization approach	EXPERT SYSTEMS WITH APPLICATIONS										Highlight extraction; Extractive summarization; Regression models; Text mining and analytics		Scientific articles can be annotated with short sentences, called highlights, providing readers with an at-a-glance overview of the main findings. Highlights are usually manually specified by the authors. This paper presents a supervised approach, based on regression techniques, with the twofold aim at automatically extracting highlights of past articles with missing annotations and simplifying the process of manually annotating new articles. To this end, regression models are trained on a variety of features extracted from previously annotated articles. The proposed approach extends existing extractive approaches by predicting a similarity score, based on n-gram co-occurrences, between article sentences and highlights. The experimental results, achieved on a benchmark collection of articles ranging over heterogeneous topics, show that the proposed regression models perform better than existing methods, both supervised and not. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113659	10.1016/j.eswa.2020.113659													
J								A novel filter feature selection method using rough set for short text data	EXPERT SYSTEMS WITH APPLICATIONS										Short text classification; Rough set; Feature selection	CLASSIFICATION	High dimensionality problem is an important concern for short text classification due to its effect on computational cost and accuracy of classifiers. Also, short text data, besides being high dimensional, has an incomplete, inconsistent and sparse structure. Selection of important features that provide a better representation is a solution for high dimensionality problem. In this study, we developed a novel filter feature selection method, Proportional Rough Feature Selector (PRFS), which uses the rough set for a regional distinction according to the value set of term to identify documents that exactly belong to a class or that is possibly belong to a class. Documents possible to belong to a class are penalized by multiplying with a coefficient named a. Additionally, the effect of sparsity in the term vector space is calculated with the help of rough set. The PRFS is compared with state-of-the-art filter feature selection methods such as Gini index, information gain, distinguishing feature selector, recently proposed max-min ratio, and normalized difference measure methods. The comparison is carried out using various feature sizes on four different short text datasets with a Macro-F1 success measure. Experimental results demonstrated that the PRFS offers either better or competitive performance with respect to other feature selection methods in terms of Macro-F1. This study may be a pioneering study in this research field as it proposes a novel feature selection method for short text classification using a rough set theory. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113691	10.1016/j.eswa.2020.113691													
J								An ensemble imbalanced classification method based on model dynamic selection driven by data partition hybrid sampling	EXPERT SYSTEMS WITH APPLICATIONS										Imbalanced classification; Ensemble learning; Data partition hybrid sampling; Model dynamic selection	SMOTE; MULTICLASS; ALGORITHM; PERFORMANCE	In many real-world applications classification problems suffer from class-imbalance. The classification methods for imbalanced data with only data processing or algorithm improvement cannot get satisfied classification performance of the minority class. This paper proposes an ensemble classification method based on model dynamic selection driven by data partition hybrid sampling for imbalanced data. The method includes two core components: the generation of balanced datasets and the dynamic selection of classification models. At the data level a data partition hybrid sampling (DPHS) method is proposed to balance datasets. In particular the data space is divided into four regions according to the majority class proportion in minority class neighborhoods. Then we present a boundary minority class weighted over-sampling (BMW-SMOTE) method where the weight of each minority class instance is calculated by the ratio between the majority class proportion in the neighborhood of the current instance and the sum of all these proportions. The number of synthetic instances is determined by the weight. At the algorithm level we present a model dynamic selection (MDS) strategy. Three ensemble learning models are built. Among them the local regions reinforce and weaken model adopts the balanced dataset obtained by proposed DPHS method for training to strengthen the identification of test instances on the boundary and appropriately weakens the dense distribution of majority class. The model for each test instance is selected adaptively according to the imbalance degree of its neighbors. The experimental results show that the proposed method outperforms typical imbalanced classification methods for F-measure and G-mean. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113660	10.1016/j.eswa.2020.113660													
J								Stock returns prediction using kernel adaptive filtering within a stock market interdependence approach	EXPERT SYSTEMS WITH APPLICATIONS										Stock returns prediction; Sequential learning; Interdependence between markets; Kernel adaptive filtering	TIME-SERIES; NEURAL-NETWORK; LEARNING ALGORITHMS; GRANGER CAUSALITY; CO-MOVEMENT; PRICE; PARAMETERS; SYSTEMS; MODELS	Stock returns are continuously generated by different data sources and depend on various factors such as financial policies and national economic growths. Stock returns prediction, unlike traditional regression, requires consideration of both the sequential and interdependent nature of financial time-series. This work uses a two-stage approach, using kernel adaptive filtering (KAF) within a stock market interdependence approach to sequentially predict stock returns. Thus, unlike traditional KAF formulations, prediction uses not only their local models but also the individual local models learned from other stocks, enhancing prediction accuracy. The enhanced KAF plus market interdependence framework has been tested on 24 different stocks from major economies. The enhanced approach obtains higher sharpe ratio when compared with KAF-based methods, long short-term memory, and autoregressive-based models. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113668	10.1016/j.eswa.2020.113668													
J								Deep reinforcement learning based preventive maintenance policy for serial production lines	EXPERT SYSTEMS WITH APPLICATIONS										Preventive maintenance; Production loss; Deep reinforcement learning; Serial production line; Group maintenance; Opportunistic maintenance	OPPORTUNISTIC MAINTENANCE; MACHINES; SYSTEMS	In the manufacturing industry, the preventive maintenance (PM) is a common practice to reduce random machine failures by replacing/repairing the aged machines or parts. The decision on when and where the preventive maintenance needs to be carried out is nontrivial due to the complex and stochastic nature of a serial production line with intermediate buffers. In order to improve the cost efficiency of the serial production lines, a deep reinforcement learning based approach is proposed to obtain PM policy. A novel modeling method for the serial production line is adopted during the learning process. A reward function is proposed based on the system production loss evaluation. The algorithm based on the Double Deep Q-Network is applied to learn the PM policy. Using the simulation study, the learning algorithm is proved effective in delivering PM policy that leads to an increased throughput and reduced cost. Interestingly, the learned policy is found to frequently conduct "group maintenance" and "opportunistic maintenance", although their concepts and rules are not provided during the learning process. This finding further demonstrates that the problem formulation, the proposed algorithm and the reward function setting in this paper are effective. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113701	10.1016/j.eswa.2020.113701													
J								Sound quality prediction and improving of vehicle interior noise based on deep convolutional neural networks	EXPERT SYSTEMS WITH APPLICATIONS										Vehicle interior noise; Sound quality; Subjective evaluation; Convolutional neural networks; Feature visualization	SYSTEM; MODEL	Interior sound quality plays a vital role in vehicle quality assessment because it forms users' general impressions of vehicles and influences consumers' purchase intentions. Thus, evaluating vehicle interior sound quality is important. Many researchers have developed intelligent prediction models to precisely evaluate vehicle interior sound quality. Deep convolutional neural networks (CNNs) can automatically learn features and many studies have applied deep CNNs to address noise and vibration issues. However, those studies suffer from two problems: i) the time and frequency characteristics of noise that influence interior sound quality have not been considered simultaneously; ii) the noise features that deep CNNs have learned need to be explored. Therefore, in this paper, to overcome the first problem, we develop a regularized deep CNN model that takes a noise time-frequency image as input. In addition, we introduce a neuron visualization algorithm for deep CNNs to solve the second problem. To verify the proposed methods, we establish an interior noise dataset through vehicular road tests and subjective evaluations. The sound quality of this recorded interior noise is evaluated through the developed deep CNN model, which reveals that deep CNNs that use a noise time-frequency image as input perform better than do those using time vector and frequency vector data as input. By analyzing feature maps extracted from the convolutional layers and the fully connected layer of the CNNs, we found that the deep CNN feature learning process can be regarded as color filter and Gabor filter processes applied to the noise time- frequency image. These results provide a new approach for evaluating vehicle interior sound quality and help in understanding which noise features deep CNNs learn. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113657	10.1016/j.eswa.2020.113657													
J								Simultaneous incremental matrix factorization for streaming recommender systems	EXPERT SYSTEMS WITH APPLICATIONS										Recommender systems; Data fusion; Matrix factorization; Data streams; Incremental learning		Recommender systems are large-scale machine learning and knowledge discovery tools aimed at providing personalized recommendations to customers based on their preferences and needs. They need to handle large quantities of diverse and very sparse data in a matter of seconds. Matrix factorization techniques have proven to be useful and reliable for implementing recommender systems, while data sparsity problem can be indirectly alleviated by considering multiple heterogeneous data sources. Furthermore, utilization of data fusion can resolve in a higher predictive accuracy. For real-world applications, e.g., such with continuous user feedback, incrementally handling recommender systems upon multiple data streams remains a crucial and only partially solved problem. This paper presents one way of fusing multiple data streams through matrix factorization. Our proposed method (SIMF) models heterogeneous and asynchronous data streams and provides predictions in real time. As a result of incremental updating, the proposed method successfully adapts to changes in data concepts, while application of data fusion improves prediction accuracy and reduces effects of the cold-start problem. Using the proposed methodology, we have develop a streaming algorithm and show how prediction accuracy can be substantially increased by considering multiple data sources, while at the same time the negative effects of the cold-start can be greatly diminished. Evaluations on a large-scale real-life problem (Yelp recommendations) confirm these claims as we present a highly scalable streaming recommender system that adapts to new concepts in data and provides accurate predictions (compared to the other matrix factorization techniques) in a very sparse problem domain. Apart from a recommender system proposed in this work, the versatility of matrix factorization could further allow the presented methodology for adaptation to solve several other machine learning problems, such as dimensionality reduction, clustering and classification. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113685	10.1016/j.eswa.2020.113685													
J								Electric vehicle routing problem with non-linear charging and load-dependent discharging	EXPERT SYSTEMS WITH APPLICATIONS										Adaptive large neighborhood search; Electric vehicle routing problem; Non-linear charging; Partial charging; Load-dependent discharging	LARGE NEIGHBORHOOD SEARCH; LIMITATION	We propose a three-index formulation for E-VRP with Non-Linear charging and Load-Dependent discharging (E-VRP-NL-LD), and an Adaptive Large Neighborhood Search (ALNS) algorithm to solve the E-VRP-NLLD and E-VRP-NL-LD with Capacitated Charging Stations (E-VRP-NL-LD-CCS). Existing implementations of EVRP duplicate charging station nodes which enables the modelling of EVRP using extended VRP formulations. Two limitations of such an approach are: (i) the number of such duplications is not known a priori, and (ii) the size of the problem increases. In our formulation, we allow multiple visits to a charging station without duplicating nodes. We propose five new operators for ALNS which are tested on 120 instances each of E-VRP-NL and E-VRP-NL-LD, and 80 instances of E-VRP-NL-LD-CCS. Results show that our ALNS outperforms the existing algorithms improving the solution in 63% of the instances and matching the best known solution in 31% of the instances. Results also show that considering load-dependent discharge is critical to optimally solve E-VRP. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113714	10.1016/j.eswa.2020.113714													
J								Integrating systems thinking skills with multi-criteria decision-making technology to recruit employee candidates	EXPERT SYSTEMS WITH APPLICATIONS										Recruitment strategy; Systems thinking skills; Job-fit recruiting; Flexible recruiting; Lp Metric; ELECTRE III	PERSON-ORGANIZATION FIT; MANAGEMENT; STRATEGIES; FRAMEWORK	The emergence of modern complex systems is often exacerbated by a proliferation of information and complication of technologies. Because current complex systems challenges can limit an organization's ability to efficiently handle socio-technical systems, it is essential to provide methods and techniques that count on individuals' systems skills. When selecting future employees, companies must constantly refresh their recruitment methods in order to find capable candidates with the required level of systemic skills who are better fit for their organization's requirements and objectives. The purpose of this study is to use systems thinking skills as a supplemental selection tool when recruiting prospective employees. To the best of our knowledge, there is no prior research that studied the use of systems thinking skills for recruiting purposes. The proposed framework offers an established tool to HRM professionals for assessing and screening of prospective employees of an organization based on their level of systems thinking skills while controlling uncertainties of complex decision-making environment with the fuzzy linguistic approach. This framework works as an expert system to find the most appropriate candidate for the organization to enhance the human capital for the organization. Several large industries, among others, Boeing, the government such as the Army, Military Academy, and National Science Foundation, highlighted the significance of having qualified (systemic) individuals who can successfully deal with complex systems problems. The correct recruiting decision will reduce the rate of job turnover and also help organizations to eliminate unnecessary budget allocated for costly recruitment processes. The proposed framework is intended to first evaluate the pool of applicants according to their level of systems thinking skills and then rank them based on the recruitment strategy and workforce needs of the organization. To achieve the purpose of the study, two recruiting strategies are adopted from the human resource management literature 1) Job-Fit Recruiting strategy-finding candidates who are most aligned with a specific position requirement and 2) Flexible Recruiting strategy-finding candidates with the highest potentials. The proposed framework is validated using a real case study in a US large-scale organization.																	0957-4174	1873-6793				DEC 1	2020	160								113585	10.1016/j.eswa.2020.113585													
J								An intelligent stock trading decision support system based on rough cognitive reasoning	EXPERT SYSTEMS WITH APPLICATIONS										Rough set theory; Rough cognitive networks; Harmony search; Trading system	ALGORITHM	From the perspective of Momentum Investing (MI), more profitable trading opportunities for bullish investors would exist in the stocks occurring with limit-up. Motivated by this, we propose an intelligent stock trading decision support system by using rough cognitive reasoning, based on which stocks with the higher probabilities of rising in the short term after the occurrences of limit-up can be distinguished. Considering financial markets are full of uncertainty and high noise, an extended rough cognitive network (RCN) is established, which is a granular reasoning model based on rough set theory and fuzzy cognitive maps. As a kind of reasoning mechanism, the extended RCN can effectively analyze both the continuous and discrete features of financial data to deal with the uncertainty and inconsistency. Moreover, entropy-based method is involved into the extended RCN model such that the knowledge representation of model can be further improved, and harmony search algorithm is applied for optimization. The proposed model is further applied in Chinese stock market to carry out empirical studies, where the discussion on parameters are implemented and experiment results show the effectiveness and validity of the proposed model. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113763	10.1016/j.eswa.2020.113763													
J								Forecasting with time series imaging	EXPERT SYSTEMS WITH APPLICATIONS										Forecasting; Time series imaging; Time series feature extraction; Recurrence plots; Forecast combination	PERFORMANCE; FRAMEWORK; SELECTION; MODEL	Feature-based time series representations have attracted substantial attention in a wide range of time series analysis methods. Recently, the use of time series features for forecast model averaging has been an emerging research focus in the forecasting community. Nonetheless, most of the existing approaches depend on the manual choice of an appropriate set of features. Exploiting machine learning methods to extract features from time series automatically becomes crucial in state-of-the-art time series analysis. In this paper, we introduce an automated approach to extract time series features based on time series imaging. We first transform time series into recurrence plots, from which local features can be extracted using computer vision algorithms. The extracted features are used for forecast model averaging. Our experiments show that forecasting based on automatically extracted features, with less human intervention and a more comprehensive view of the raw time series data, yields highly comparable performances with the best methods in the largest forecasting competition dataset (M4) and outperforms the top methods in the Tourism forecasting competition dataset. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113680	10.1016/j.eswa.2020.113680													
J								A cooperative coevolutionary optimization design of urban transit network and operating frequencies	EXPERT SYSTEMS WITH APPLICATIONS										Transit network design; Frequency setting; Public transportation; Multiobjective optimization; Cooperative coevolutionary algorithm	EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHM	The transit network design and frequency setting problem (TNDFSP) is a complex combinatorial optimization problem. Generally, the nature of multiobjective in TNDFSP has not attracted enough attention, and the frequency setting is directly embedded as a subproblem to generate a unique set of frequencies for a given transit network, ignoring trade-off solutions among multiple objectives with different sets of frequencies. In this study, the problem is formulated as a multiobjective model with two conflicting objectives of minimizing passengers' and operators' costs. Moreover, we establish two populations to simultaneously optimize networks and frequencies. Also a cooperative coevolutionary multiobjective evolutionary algorithm (CCMOEA) is developed to collaboratively coevolve these two populations along multiple objectives. Unsatisfied demand is embedded into the individual prioritization process, and infeasible individuals can be retained instead of being replaced arbitrarily, driving the evolution to gradually generate more feasible solutions. The proposed CCMOEA is tested on the well-known Mandl's benchmark. The results show that our algorithm can efficiently produce a comprehensive set of high quality trade-off solutions. These solutions perform well with lower waiting time, competitive in vehicle travel time and number of transfers, resulting in lower user costs than previously published results in the same fleet size. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113736	10.1016/j.eswa.2020.113736													
J								Learning competitive channel-wise attention in residual network with masked regularization and signal boosting	EXPERT SYSTEMS WITH APPLICATIONS										Residual networks; Competitive channel-wise attention; Masked regularization; Signal boosting	CONVOLUTIONAL NEURAL-NETWORKS; CLASSIFICATION; REPRESENTATION; FEATURES; IMAGES; CNNS	Image classification is an essential component of expert and intelligent systems. The accuracy and efficiency of image classification algorithms significantly affect the performance of related expert systems. Residual network (ResNet) shows strong superiority in image modeling. However, it has also been proved to be low-efficient. In this study, we proposed a novel channel-wise attention mechanism to alleviate the redundancy of ResNet. We introduce the identity mappings into the scope of channel relationship modeling. In this way, the identity mapping can join the optimized process of self-supplementary modeling. Besides, we present the masked regularization for squeezed signals and enhance the robustness of channel-relation encoding. Finally, we verify the performance of the proposed method. The experiments are carried out on the datasets CIFAR-10, CIFAR-100, SVHN, and ImageNet. The proposed method effectively improves the performance of image classification-related expert systems. Moreover, our approach is hot-swappable, has broad applicability, so it has great practical significance for experts and intelligent systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113591	10.1016/j.eswa.2020.113591													
J								An efficient memetic algorithm for distributed flexible job shop scheduling problem with transfers	EXPERT SYSTEMS WITH APPLICATIONS										Distributed flexible job shop scheduling; Operation transfer; Memetic algorithm; Multi-objective optimization; Taguchi method	OPTIMIZATION ALGORITHM; INTEGRATED APPROACH; GENETIC ALGORITHM; IMMUNE ALGORITHM; TABU SEARCH	The traditional distributed flexible job shop scheduling problem (DFJSP) assumes that operations of a job cannot be transferred between different factories. However, in real-world production settings, the operations of a job may need to be processed in different factories owing to requirements of economic globalization or complexity of the job. Hence, in this paper, we propose a distributed flexible job shop scheduling problem with transfers (DFJSPT), in which operations of a job can be processed in different factories. An efficient memetic algorithm (EMA) is proposed to solve the DFJSPT with the objectives of minimizing the makespan, maximum workload, and total energy consumption of factories. In the proposed EMA, a well-designed chromosome presentation and initialization methods are presented to obtain a high-quality initial population. Several crossover and mutation operators and three effective neighborhood structures are designed to expand the search space and accelerate the convergence speed of the solution. Forty benchmark instances of the DFJSPT are constructed to evaluate the EMA and facilitate further studies. The Taguchi method of design of experiments is used to obtain the best combination of key EMA parameters. Extensive computational experiments are carried out to compare the EMA with three well-known algorithms from the literature. The computational results show that the EMA can obtain better solutions for approximately 90% of the tested benchmark instances compared to the three wellknown algorithms, thereby demonstrating the DFJSPT's competitive performance and efficiency. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113721	10.1016/j.eswa.2020.113721													
J								SVR-FFS: A novel forward feature selection approach for high-frequency time series forecasting using support vector regression	EXPERT SYSTEMS WITH APPLICATIONS										Support vector regression; Feature selection; Forecasting; Energy load forecasting; Automatic model specification	ELECTRICITY DEMAND; PARAMETER OPTIMIZATION; GENETIC ALGORITHM; NEURAL-NETWORKS; PREDICTION; MACHINES; CLASSIFICATION	In this paper, we propose a novel support vector regression (SVR) approach for time series analysis. An efficient forward feature selection strategy has been designed for dealing with high-frequency time series with multiple seasonal periods. Inspired by the literature on feature selection for support vector classification, we designed a technique for assessing the contribution of additional covariates to the SVR solution, including them in a forward fashion. Our strategy extends the reasoning behind Auto-ARIMA, a well-known approach for automatic model specification for traditional time series analysis, to kernel machines. Experiments on well-known high-frequency datasets demonstrate the virtues of the proposed method in terms of predictive performance, confirming the virtues of an automatic model specification strategy and the use of nonlinear predictors in time series forecasting. Our empirical analysis focus on the energy load forecasting task, which is arguably the most popular application for high-frequency, multi-seasonal time series forecasting. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113729	10.1016/j.eswa.2020.113729													
J								Classifying Papanicolaou cervical smears through a cell merger approach by deep learning technique	EXPERT SYSTEMS WITH APPLICATIONS										Papanicolaou Cervical Smears; Deep Learning; Convolutional Neural Network; Image classification	LIQUID-BASED CYTOLOGY; QUALITY-CONTROL; SYSTEM; CLASSIFICATION; SEGMENTATION; CARCINOMA; DIAGNOSIS; ACCURACY; LSIL	Early detection of cancer is important to improve survival and reduce associated morbility. Nowadays, there is no automatic classification process with enough accuracy to be recommended to its use in population cervical cancer screening. In most automatic medical image classifications, these images are clean in background and without overlap between elements, which means that these images do not reflect reality and the model cannot be applied to directly obtained images from medical samples. The objectives of this study are to design and implement a Cell Merger Approach to improve the efficiency and realism of the PAP-smears classification model, by allowing overlapping and folding of different cells, to design and implement a Convolutional Neural Network for PAP-smears image classification, and to optimize and integrate the cell fusion approach with the neural network building a feasible, reliable and highly accurate system for cervical smears classification. The carried out experiments have validated both the CNN and the proposed Cell Merger Approach with very interesting results. The most outstanding results show that the Convolutional Neural Network models together with the Cell Merger Approach have a classification accuracy of 88.8% with a standard deviation of 1%, obtaining a sensitivity and specificity of 0.92 and 0.83 respectively. This classification level depicts a robust and accurate model that is comparable to an expert pathologist competencies. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113707	10.1016/j.eswa.2020.113707													
J								Optimization-based automated unsupervised classification method: A novel approach	EXPERT SYSTEMS WITH APPLICATIONS										Unsupervised classification; Algorithms; Big data processing; Optimization; Remote sensing	SPECTRAL INDEXES; CLUSTERING APPROACH; LAND-USE; EXTRACTION; ALGORITHM; WETLAND	Unsupervised classification algorithms are methods for the analysis of remotely sensed images. Since these methods do not include a training phase, they require less time to apply and are more practical to use. Traditional unsupervised classification methods work with parameters given by the user, such as the number of classes, the stop criterion or the number of iterations of the algorithm. Determining the optimum values of these parameters to obtain successful classification result is a major problem. In this study, we propose two new methods, the weighted density based optimized classification method (DBOC-Weighted) and the automatic density based optimized classification method (DBOCAutomatic). Both work automatically without the need for parameters from the user, but the DBOCWeighted only requires layer weights. These methods consist of data range expansion, useful data selection, segmentation and optimization stages, and perform the classification automatically. Both create new layers of data using remotely sensed images. After creating the initial classes based on density from all the data layers, the results are created by optimizing all classes in terms of quality indices. Four Sentinel 2 images are used to test the performance of the proposed methods. These images are selected from regions that have different geographical, climatic and vegetation properties. The results obtained are compared with the unsupervised classification methods frequently used in the literature. The accuracy analysis results show that the proposed classification algorithms produce satisfactory accuracy compared to the results of other algorithms. The results show that the proposed methods can be used successfully in the creation of expert and intelligent analysis systems, by eliminating user induced error in the analysis of remotely sensed images. Thus, smart analysis tools can be created so that users from various professional disciplines can easily use them without being image processing specialists. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113735	10.1016/j.eswa.2020.113735													
J								Identifying influential nodes in heterogeneous networks	EXPERT SYSTEMS WITH APPLICATIONS										Social media; Influence; Influential nodes; Scholar; Heterogeneous networks	COMPLEX NETWORKS; SOCIAL-INFLUENCE	Identifying influential users and measure the influence of nodes in social networks have become an interesting and important topic of research. It is crucial to find out to what extent individuals influence each other because it can be used to control rumors, diseases, and diffusion. There are numerous relevant models most of which are based on a homogeneous network. However, in the real world, we face heterogeneous networks where the nodes and edges are different types. A network is homogeneous if and only if the edges and nodes are of the same type, and it is considered heterogeneous if the nodes and edges are different. In heterogeneous networks, there is a concept known as meta-path, which indicates the type of communication between two nodes. In this paper, we aim to locate influential nodes by calculating the entropy of different meta-paths. To evaluate information diffusion in a heterogeneous network, we used the known susceptible-infectious model. The results of our experiments on three real-world networks' dataset show that the proposed method outperforms state-of-the-art influence maximization algorithms. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113580	10.1016/j.eswa.2020.113580													
J								Probabilistic optimization algorithms for real-coded problems and its application in Latin hypercube problem	EXPERT SYSTEMS WITH APPLICATIONS										Optimization; Quantum Evolutionary Algorithms; Probabilistic Optimization Algorithms; Structured Population	INSPIRED EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; MEMETIC ALGORITHMS; SEARCH ALGORITHM; PARTICLE SWARM; DESIGNS; SIMULATION; LANDSCAPE; SELECTION	This paper proposes a novel optimization algorithm for read-coded problems called the Probabilistic Optimization Algorithm (POA). In the proposed algorithm, rather than a binary or integer, a probabilistic representation is used for the individuals. Each individual in the proposed algorithm is a probability density function and is capable of representing the entire search space simultaneously. In the search process, each solution performs as a local search and climbs the local optima, and at the same time, the interaction among the probabilistic individuals in the population offers a global search. The parameters of the proposed algorithm are studied in this paper and their effect on the search process is presented. A structured population is proposed for the algorithm and the effect of different structures is analyzed. The algorithm is used to solve Latin Hyper-cube problem and experimental studies suggest promising results. Different benchmark functions are also used to test the algorithm and results are presented. The analyses suggest that the improvement is more significant for large scale problems. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113589	10.1016/j.eswa.2020.113589													
J								Neural-based time series forecasting of loss of coolant accidents in nuclear power plants	EXPERT SYSTEMS WITH APPLICATIONS										Time series; Expert systems; LOCA; DNN/LSTM; Nuclear accidents	BOILING HEAT-TRANSFER; LARGE-BREAK LOCA; NETWORKS; ALGORITHM; REACTOR	In the last few years, deep learning in neural networks demonstrated impressive successes in the areas of computer vision, speech and image recognition, text generation, and many others. However, sensitive engineering areas such as nuclear engineering benefited less from these efficient techniques. In this work, deep learning expert systems are utilized to model and predict time series progression of a design-basis nuclear accident, featuring a loss of coolant accident. Two major findings are accomplished in this work. First, the ability to train expert systems with high accuracy, which could help nuclear power plant operators to figure out plant responses during the accident. Second, building fast, efficient, and accurate deep models to simulate nuclear phenomena, which could be valuable to nuclear computational science. In this work, large amount of time series data is obtained from simulation tools by simulating different conditions of the base-case/nominal accident scenario. Four critical outputs/responses are monitored during the accident (e.g. temperature, pressure, break flow rate, water level). Two approaches are adopted in this work. The first approach is to use feedforward deep neural networks (DNN) to fit all time steps and outputs in a single model. The second approach is to use long short-term memory (LSTM) to fit all time steps together for each reactor response separately. Both DNN and LSTM demonstrate very good performance in predicting the test and base-case scenarios, with accuracy as low as 92% and as high as 99%, where these test scenarios are unknown to the expert systems and are not included in the model training. In addition, both approaches demonstrate a significant reduction in computational costs, as the deep expert system is able to accurately predict the accident 100,000 times faster than the original simulation tool. Given sufficient data, the methodology adopted in this study demonstrates that DNN/LSTM expert systems can be used as a decision support system to model advanced time series phenomena within nuclear power plants with high accuracy and negligible computational costs. Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 1	2020	160								113699	10.1016/j.eswa.2020.113699													
J								A revision on multi-criteria decision making methods for multi-UAV mission planning support	EXPERT SYSTEMS WITH APPLICATIONS										Unmanned aerial vehicles; Mission planning; Multi-criteria decision making; Fuzzy methods	NETWORK; SELECTION; PROJECT; SWARMS; SYSTEM; VIKOR	Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the mission planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system. Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced. For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113708	10.1016/j.eswa.2020.113708													
J								An improved general variable neighborhood search for a static bike-sharing rebalancing problem considering the depot inventory	EXPERT SYSTEMS WITH APPLICATIONS										Bike-sharing systems; Rebalancing; Depot inventory; Variable neighborhood search	ALGORITHM; SYSTEMS; STRATEGIES; DELIVERY; PICKUP	Smart shared mobility is an emerging transportation strategy that promotes sustainable and intelligent transportation. As one mode of smart shared mobility bike sharing is gaining popularity in recent years. A daily rebalancing operation is commonly carried out to keep high level service of bike-sharing systems (BSSs). The static bike-sharing rebalancing problems (SBRPs) studied in existing papers focus on determining the vehicle routes with minimal traveling cost. However, the depot inventory is rarely considered during the relocation. Thus, this paper researches the integration of the depot inventory and vehicle routing problems, with the aim of minimizing the daily operational cost including the depot inventory cost (DIC) and the traveling cost. First, two mixed integer programming (MIP) formulations are proposed to find the daily optimal decision on the vehicle routes and the numbers of bikes and vehicles employed from the depot. Based on the models, an improved general variable neighborhood search (IGVNS) algorithm is developed with a variety of neighborhood structures and a hybrid strategy. Finally, we apply a set of benchmark instances to test our proposed model and approach, and the computational results demonstrate that IGVNS can efficiently compute the SBRP and achieve lower operational cost than the existing solutions. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113752	10.1016/j.eswa.2020.113752													
J								Cross-language text alignment: A proposed two-level matching scheme for plagiarism detection	EXPERT SYSTEMS WITH APPLICATIONS										Plagiarism detection; Cross-language plagiarism; Text alignment; Graph-of-words representation; Multilingual word embeddings	CONTINUOUS-SPACE; RETRIEVAL; CLIQUES	The exponential growth of documents in various languages throughout the web, along with the availability of several editing and translation tools have made the cross-language plagiarism detection a challenging issue. Regarding its high importance, the present study focuses on the task of cross-language text alignment also known as detailed analysis which works on the outputs of the source retrieval step of cross language plagiarism detection systems. The paper proposes a two-level matching approach with the aim of considering both syntactic and semantic information to align plagiarism fragments from the source and suspicious documents, accurately. At the first level, a vector space model which employs a multilingual word embeddings based dictionary and a local weighting technique is used in order to extract a minimal set of highly potential candidate fragment pairs rather than considering all possible pairs of fragments. This step also contains a dynamic expansion technique to cover more candidate pairs aiming at improving the system's recall. It is followed by a more precise algorithm that examines the candidate pairs at the sentence level using a graph-of-words representation of text. As a result, by modelling both the words and their relationships, an acceptable increase in the system's precision which is the goal of the second level is also observed. To identify evidence of plagiarism, i.e. potential cases of unauthorized text reuse, the algorithm tries to find maximum cliques from the match graph of source and suspicious texts. With this two-level investigation, the approach is capable to discriminate true plagiarism cases from the original text. The experimental results on different datasets such as PAN-PC-11, PAN-PC-12, and SemEval-2017 show that the proposed cross-language text alignment approach significantly outperforms the state-of-the-art models and can be fed into an expert system for further improvement of cross-language plagiarism detection. The source codes are publicly available on GitHub1, for the purposes of reproducible research. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113718	10.1016/j.eswa.2020.113718													
J								A time-series clustering methodology for knowledge extraction in energy consumption data	EXPERT SYSTEMS WITH APPLICATIONS										Time-series clustering; Energy efficiency; Knowledge extraction; Data mining	DECISION-MAKING; NEURAL-NETWORKS; ALGORITHM; IDENTIFICATION; SEGMENTATION; MANAGEMENT; BUILDINGS; DISTANCE; SYSTEM; MODEL	In the Energy Efficiency field, the incorporation of intelligent systems in cities and buildings is motivated by the energy savings and pollution reduction that can be attained. To achieve this goal, energy modelling and a better understanding of how energy is consumed are fundamental factors. As a result, this study proposes a methodology for knowledge acquisition in energy-related data through Time-Series Clustering (TSC) techniques. In our experimentation, we utilize data from the buildings at the University of Granada (Spain) and compare several clustering methods to get the optimum model, in particular, we tested k-Means, k-Medoids, Hierarchical clustering and Gaussian Mixtures; as well as several algorithms to obtain the best grouping, such as PAM, CLARA, and two variants of Lloyd's method, Small and Large. Thus, our methodology can provide non-trivial knowledge from raw energy data. In contrast to previous studies in this field, not only do we propose a clustering methodology to group time series straightforwardly, but we also present an automatic strategy to search and analyse energy periodicity in these series recursively so that we can deepen granularity and extract information at different levels of detail. The results show that k-Medoids with PAM is the best approach in virtually all cases, and the Squared Euclidean distance outperforms the rest of the metrics. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113731	10.1016/j.eswa.2020.113731													
J								Investigating the potential for using gamification to empower knowledge workers	EXPERT SYSTEMS WITH APPLICATIONS										Knowledge management; Gamification; Knowledge workers; Knowledge sharing	SERIOUS GAMES; QUALITATIVE RESEARCH; MANAGEMENT; COLLABORATION; CREATIVITY; EXPERTISE; TENSIONS; REWARDS; SYSTEMS; DESIGN	The increasingly popular trend of gamification has proved powerful in many areas, such as education and marketing, and has started making its way to the corporate environment. This exploratory study is focused on a particular part of corporate applications - using gamification to empower knowledge workers and to help them to interact with each other. Based on a review of the extant literature and an exploratory case study, we conceptualise different ways in which gamification supports knowledge workers and influences the dynamics of their interactions. The case study we present is that of online retailer Zappos who have been pioneers in this field. This paper is intended as the beginning of a journey towards utilising gamification in various aspects of knowledge work. Through studying the Zappos case, we draw out key learning points that can be used by other organisations in their journey to use gamification to empower knowledge workers. The paper also identifies areas for further research relevant to expert and intelligent systems, including the potential for synergies between gamification and intelligent systems, and the use of gamification in intelligent systems implementation. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113694	10.1016/j.eswa.2020.113694													
J								A progressive hybrid set covering based algorithm for the traffic counting location problem	EXPERT SYSTEMS WITH APPLICATIONS										Location; Traffic counters; Mathematical programming; Set covering; Exact and heuristic methods	MATRIX ESTIMATION; ALLOCATION; NETWORK; MODELS	This work tackles the Traffic Counting Location Problem (TCLP), where we aim finding the best number and location of counting stations to cover a road network in order to obtain its traffic flows. It is important to reduce deployment, maintenance and operation costs of traffic stations. We propose a progressive hybrid algorithm based on exact, heuristic and hybrid approaches embedded on a set covering framework to solve the TCLP. This algorithm employs a simple and innovative concept which has not yet been explored in the literature. Twenty-six real-world instances obtained from the Brazilian states are used in the computational experiments and the results show that the TCLP can be solved more efficiently than previous approaches, with 84% of the instances solved optimally, and three new best known solutions found. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113641	10.1016/j.eswa.2020.113641													
J								Improving neighbor-based collaborative filtering by using a hybrid similarity measurement	EXPERT SYSTEMS WITH APPLICATIONS										Collaborative filtering; K-nearest-neighbor; Recommendation system; Item-based; Memory-based; Similarity measurement	RECOMMENDER SYSTEMS; MODEL; HUBS	Memory-based collaborative filtering is one of the recommendation system methods used to predict a user's rating or preference by exploring historic ratings, but without incorporating any content information about users or items. It can be either item-based or user-based. Taking item-based Collaborative Filtering (CF) as an example, the way it makes predictions is accomplished in 2 steps: first, it selects based on pair-wise similarities a number of most similar items to the predicting item from those that the user has already rated on. Second, it aggregates the user's opinions on those most similar items to predict a rating on the predicting item. Thus, similarity measurement determines which items are similar, and plays an important role on how accurate the predictions are. Many studies have been conducted on memory-based CFs to improve prediction accuracy, but none of them have achieved better prediction accuracy than state-of-the-art model-based CFs. In this paper, we proposed a new approach that combines both structural and rating-based similarity measurement. We found that memory-based CF using combined similarity measurement can achieve better prediction accuracy than model-based CFs in terms of lower MAE and reduce memory and time by using less neighbors than traditional memory-based CFs on MovieLens and Netflix datasets. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113651	10.1016/j.eswa.2020.113651													
J								Ranking multiple-input and multiple-output units: A comparative study of data envelopment analysis and rank aggregation	EXPERT SYSTEMS WITH APPLICATIONS										Rank aggregation; Borda; DEA; Simulation; Production function	SLACKS-BASED MEASURE; SUPER-EFFICIENCY; DEA; MISSPECIFICATION; MODELS; SCALE	Ranking multiple-input and multiple-output units is a critical problem that arises in a broad range of disciplines. While various methods have been proposed and applied, their comparative strengths and weaknesses are not well understood. In this paper, we assess and compare two popular methods, data envelopment analysis (DEA) and heuristic rank aggregation approach (i.e., the Borda method), in the context of ranking multiple-input and multiple-output units. Both methods exploit the output-input ratios, but in different ways. The Borda method sorts the units by taking the arithmetic average of the ranks in terms of individual output-input ratios, whereas DEA ranks the units based on composite output-input ratios. We use simulations to compare Borda rank aggregation and six DEA models, including CCR (Charnes, Cooper, Rhodes), super-efficiency CCR, BCC (Banker, Charnes, Cooper), super-efficiency BCC, SBM (slacks-based measure), and super-efficiency SBM. The simulations are based on Cobb-Douglas and translog production functions with both single output and multiple outputs. We show that the heuristic Borda rank aggregation, though simple to implement, performs better than DEA models for Cobb-Douglas production function under three situations: small sample size, relatively balanced weights for production factors, and presence of multiple outputs. For translog production function, the Borda method generally performs better than the CCR model, but cannot match up to other DEA models. We also demonstrate the performance of different methods via application to the well-known problem of ranking countries by human development. Our research sheds light on the potential of rank aggregation to complement or even supplant DEA under certain conditions. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113687	10.1016/j.eswa.2020.113687													
J								Attention-based deep neural network for Internet platform group users' dynamic identification and recommendation	EXPERT SYSTEMS WITH APPLICATIONS										Recommendation system; Deep neural network; Attention mechanism; Group users	DECISION-SUPPORT MODEL; RESTAURANT RECOMMENDATION; SYSTEM	Under the Internet background, group recommendation has become a major interest in the study of recommendation systems. In the method of group recommendation, the existing researches are mostly conducted using cluster analysis and similarity analysis. The group characteristics studied are relatively generalized, and the group objects studied are mostly fixed, so the group cannot change in real time according to the different attributes and characteristics of the different products. At the same time, for the research object of group recommendation, the existing research mainly consider the recommended project group or user group, but seldom consider recommending the appropriate project group to the appropriate user group to improve the recommendation efficiency and user satisfaction. In view of these problems, this paper proposes a deep neural network that integrates the attention mechanism for group users' dynamic identification and recommendation on the Internet platform. This paper uses an attention mechanism and deep neural networks to generate the attention preference weights for the group users according to the product attributes. Doing so achieves the purpose of recommending many types of projects to different groups to adapt to their preferences. We compare this method with other baseline methods on two public datasets to validate the effectiveness of the proposed method, which achieves better performance than the most advanced methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113728	10.1016/j.eswa.2020.113728													
J								A study on adaptation lightweight architecture based deep learning models for bearing fault diagnosis under varying working conditions	EXPERT SYSTEMS WITH APPLICATIONS										Diagnosis; CNN; Normalization; Lightweight; Transfer learning	CONVOLUTIONAL NEURAL-NETWORK; MACHINERY	Deep learning models have been widely studied in fault diagnosis recently. A mainstream application is to recognize patterns in spectrograms of faults. However, some common drawbacks still remain as following: a) Preprocess to improve the quality of spectrograms is rarely explored; b) Computing cost of a conventional CNN far exceeds the requirements of fast analysis in industry; c) Adequate labeled data cannot be acquired to train a comprehensive diagnosis model for varying working conditions. In this paper, an Adaptive Logarithm Normalization (ALN) is proposed to realize preprocess considering data distribution, it attempts to improve the quality of spectrograms via eliminating truncation phenomenon and enriching details simultaneously; Meanwhile, simplified lightweight models are built on the basis of present lightweight building blocks to reduce parameters, while maintaining high performances; Furthermore, an adaptation architecture is proposed by integrating Deep Adaptation Network (DAN) idea with simplified lightweight models, aiming at enhancing the generalization capability of models. Experiments have been carried out to implement the proposed methods with two different datasets. The overall success not only proves the methods feasible, but also indicates a possible diagnosis prospect for real industrial scenarios. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113710	10.1016/j.eswa.2020.113710													
J								Non-intrusive load disaggregation based on composite deep long short-term memory network	EXPERT SYSTEMS WITH APPLICATIONS										Non-intrusive load disaggregation; Long short-term memory network; Cross-layer connection; Time series	CLASSIFICATION; ALGORITHM	Non-invasive load monitoring (NILM) is a vital step to realize the smart grid. Although the existing various NILM algorithms have made significant progress in energy consumption feedback, there are still some problems need to further addressed, such as the exponential growth of state space with the increase of the number of multi-state devices, which leads to the dimension disaster; and it is difficult to capture the power fluctuation information effectively because of the neglect of time-dependency problem load disaggregation; traditional disaggregation involves a process of one sequence to one sequence optimization, which is inefficient. In our study, a composite deep LSTM is proposed for load disaggregation. The proposed algorithm considers the process of load disaggregation as a signal separation process and establishes regression learning from a single sequence to multiple sequences to avoid dimension disaster. In addition, an encoder-separation-decoder structure is introduced for load disaggregation. Encoder completes the effective encoding of the mains power and differential power information, the time dependency of the encoding process implemented by a deep LSTM, separation realizes the disaggregation process by separating the encoded information, and decoder decode the separated signal into the sequences of corresponding electrical appliances. Compared with the one sequence to one sequence disaggregation method, the proposed method simplified disaggregation complexity and improves the efficiency of disaggregation. The experimental results on WikiEnergy and REDD datasets show that the proposed method can reduce the disaggregation error and improve the comprehensive performance of event detection. Besides, our study can provide conditions for the realization of the bidirectional interaction of the smart grid and the improvement of the smart grid scheduling. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113669	10.1016/j.eswa.2020.113669													
J								Representation learning via serial robust autoencoder for domain adaptation	EXPERT SYSTEMS WITH APPLICATIONS										Domain adaptation; Serial autoencoder; Representation learning	CLASSIFICATION	Domain adaptation aims to apply knowledge obtained from a labeled source domain to an unseen target domain from a different distribution. Recently, domain adaptation approaches based on autoencoder have achieved promising performances. However, almost of these approaches ignore the potential relationships of intra-domain features, which can be used to further reduce the distribution discrepancy between source and target domains. Furthermore, almost of them depend on the single autoencoder model, which brings the challenge of extracting multiple characteristics of data. To address these issues, in this paper, we propose a new representation learning method based on serial robust autoencoder for domain adaptation, named SERA. SERA first enriches intra-domain knowledge by mining the potential relationships of features in the source domain and target domain, respectively. Then, SERA learns domain invariant representations by serially connecting two new proposed autoencoder models, including marginalized denoising autoencoder via adaptation regularization (AMDA) and robust autoencoder via graph regularization (GRA). Extensive experiments on four public datasets demonstrate the effectiveness of the proposed method. (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				DEC 1	2020	160								113635	10.1016/j.eswa.2020.113635													
J								Backtracking search algorithm with competitive learning for identification of unknown parameters of photovoltaic systems	EXPERT SYSTEMS WITH APPLICATIONS										Backtracking search algorithm; Competitive learning; Engineering optimization; Photovoltaic systems; Parameter identification	PARTICLE SWARM OPTIMIZATION; ARTIFICIAL BEE COLONY; BIOGEOGRAPHY-BASED OPTIMIZATION; DIFFERENTIAL EVOLUTION; SOLAR-CELLS; CUCKOO SEARCH; EXTRACTION; MODEL; STRATEGIES; SINGLE	Metaheuristic algorithms have been successfully used to parameter identification of photovoltaic systems. However, this still faces the following two challenges. Firstly, most of the applied algorithms are complex and need some extra control parameters except the essential population size and stopping criterion, which is against their applications in photovoltaic systems with different characteristics. Secondly, how to obtain model parameters with higher accuracy and reliability has been a very valuable topic. To address the two challenges, this paper presents a new optimization method called backtracking search algorithm with competitive learning (CBSA) for parameter identification of photovoltaic systems. The remarkable features of CBSA are that it has a very simple structure and only needs the essential parameters. The core idea of CBSA is to increase the chance of backtracking search algorithm (BSA) to jump out of the local optimum by the designed competitive learning mechanism. In CBSA, the population is first divided into two subpopulations by built competitive mechanism. Then each subpopulation is optimized by the different search operators with multiple learning strategies. In order to test the performance of CBSA, CBSA is first employed to solve five challenging engineering design optimization problems and then is used to estimate the unknown parameters of three photovoltaic models. Experimental results show the solutions offered by CBSA outperform those of the compared algorithms including BSA, two recently proposed variants of BSA and other some state-of-the-art algorithms on nearly all test problems, which proves the effectiveness of the improved strategies. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113750	10.1016/j.eswa.2020.113750													
J								An ensemble discrete differential evolution for the distributed blocking flowshop scheduling with minimizing makespan criterion	EXPERT SYSTEMS WITH APPLICATIONS										Distributed blocking flowshop; Discrete differential evolution; Heuristics method; Front delay; Elitist retain strategy	ITERATED GREEDY ALGORITHM; EFFECTIVE HEURISTICS; SEARCH ALGORITHM; TOTAL FLOWTIME; OPTIMIZATION; MACHINE; METAHEURISTICS; MECHANISM; SYSTEM	The distributed blocking flowshop scheduling problem (DBFSP) plays an essential role in the manufacturing industry and has been proven to be as a strongly NP-hard problem. In this paper, an ensemble discrete differential evolution (EDE) algorithm is proposed to solve the blocking flowshop scheduling problem with the minimization of the makespan in the distributed manufacturing environment. In the EDE algorithm, the candidates are represented as discrete job permutations. Two heuristics method and one random strategy are integrated to provide a set of desirable initial solution for the distributed environment. The front delay, blocking time and idle time are considered in these heuristics methods. The mutation, crossover and selection operators are redesigned to assist the EDE algorithm to execute in the discrete domain. Meanwhile, an elitist retain strategy is introduced into the framework of EDE algorithm to balance the exploitation and exploration ability of the EDE algorithm. The parameters of the EDE algorithm are calibrated by the design of experiments (DOE) method. The computational results and comparisons demonstrated the efficiency and effectiveness of the EDE algorithm for the distributed blocking flowshop scheduling problem. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113678	10.1016/j.eswa.2020.113678													
J								Repair equipment allocation problem for a support-and-repair ship on a deep sea: A hybrid multi-criteria decision making and optimization approach	EXPERT SYSTEMS WITH APPLICATIONS										Support-and-repair ship; Repair equipment allocation; Evidence theory; Removal strategy; Genetic algorithm	KNAPSACK-PROBLEM; GENETIC ALGORITHM; SELECTION; CHOICE; MODEL; SYSTEMS; SOLVE; RULE	To address the repair equipment allocation problem for a support-and-repair ship on a deep sea, a hybrid multi-criteria decision making and optimization approach is designed. Evidence reasoning approach is used to aggregate the evaluation information of quantitative criteria (i.e., weight and economics) and qualitative criteria (i.e., repair ability, reliability and convenience). Then, a mathematical repair equipment allocation model of a support-and-repair ship is formulated, which is a mixed-integer nonlinear model. A removal strategy based on a greedy algorithm that modifies infeasible solutions is designed to facilitate the use of a genetic algorithm with an elite strategy to address the model above. The proposed solution method using the removal strategy based on the greedy algorithm obtains better solution accuracy and global search performance than three widely used penalty-based methods by several test instances generated randomly. The results of a case study prove that the mathematical model and solution method can effectively obtain the optimal repair equipment allocation solution. The hybrid multi criteria decision making and optimization approach has certain guiding significance for decision makers to determine their repair equipment allocation strategies for support-and-repair ships on a deep sea in practice. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				DEC 1	2020	160								113658	10.1016/j.eswa.2020.113658													
J								Deep learning based emotion analysis of microblog texts	INFORMATION FUSION										Microblog short text; Emotional analysis; Convolutional neural network; Word2vec	SENTIMENT CLASSIFICATION	Traditional text emotion analysis methods are primarily devoted to studying extended texts, such as news reports and full-length documents. Microblogs are considered short texts that are often characterized by large noises, new words, and abbreviations. Previous emotion classification methods usually fail to extract significant features and achieve poor classification effect when applied to processing of short texts or micro-texts. This study proposes a microblog emotion classification model, namely, CNN_Text_Word2vec, on the basis of convolutional neural network (CNN) to solve the above-mentioned problems. CNN_Text_Word2vec introduces a word2vec neural network model to train distributed word embeddings on every single word. The trained word vectors are used as input features for the model to learn microblog text features through parallel convolution layers with multiple convolution kernels of different sizes. Experiment results show that the overall accuracy rate of CNN_Text_Word2vec is 7.0% higher than that achieved by current mainstream methods, such as SVM, LSTM and RNN. Moreover, this study explores the impact of different semantic units on the accuracy of CNN_Text_Word2vec, specifically in processing of Chinese texts. The experimental results show that comparing to using feature vectors obtained from training words, feature vector obtained from training Chinese characters yields a better performance.																	1566-2535	1872-6305				DEC	2020	64						1	11		10.1016/j.inffus.2020.06.002													
J								Aspect terms grouping via fusing concepts and context information	INFORMATION FUSION										Aspect grouping; Sentiment analysis; Text feature fusion; Deep learning		We introduce a neural method that is able to fuse concepts from a knowledge base with the context information for the task of grouping of aspect terms. Rather than only using context information, we use the corresponding concepts of aspect terms as additional information for aspect terms representation. We also introduce a location-based attention mechanism for accurately representing context features. As both the concept and the aspect term are same level features, i.e. aspect level features, we develop a model with gating mechanism to fuse them together. All of the above features are fed into a parallel metric learning network which has the ability to learn an easier grouping representation of samples. Experimental results demonstrate that our approach outperforms different baselines and model variants on five datasets.																	1566-2535	1872-6305				DEC	2020	64						12	19		10.1016/j.inffus.2020.06.007													
J								Distributed multiple model joint probabilistic data association with Gibbs sampling-aided implementation	INFORMATION FUSION										Multiple target tracking; Distributed information fusion; Joint probabilistic data association; Gibbs sampling; Average consensus	MULTITARGET TRACKING; EFFICIENT IMPLEMENTATION; HYPOTHESIS TRACKING; CONSENSUS FILTERS; ALGORITHM; SYSTEMS; INFORMATION; RELAXATION	This paper proposes a new distributed multiple model multiple manoeuvring target tracking algorithm. The proposed tracker is derived by combining joint probabilistic data association (JPDA) with consensus-based distributed filtering. Exact implementation of the JPDA involves enumerating all possible joint association events and thus often becomes computationally intractable in practice. We propose a computationally tractable approximation of calculating the marginal association probabilities for measurement-target mappings based on stochastic Gibbs sampling. In order to achieve scalability for a large number of sensors and high tolerance to sensor failure, a simple average consensus algorithm-based information JPDA filter is proposed for distributed tracking of multiple manoeuvring targets. In the proposed framework, the state of each target is updated using consensus-based information fusion while the manoeuvre mode probability of each target is corrected with measurement probability fusion. Simulations clearly demonstrate the effectiveness and characteristics of the proposed algorithm. The results reveal that the proposed formulation is scalable and much more efficient than classical JPDA without sacrificing tracking accuracy.																	1566-2535	1872-6305				DEC	2020	64						20	31		10.1016/j.inffus.2020.04.007													
J								Cooperative and distributed decision-making in a multi-agent perception system for improvised land mines detection	INFORMATION FUSION										Land mine detection; Improvised explosive device; Neuroevolution; Genetic fuzzy systems; Feature extraction; Sensor fusion	NEURAL-NETWORKS; FUSION	This work presents a novel intelligent system designed using a multi-agent hardware platform to detect improvised explosive devices concealed in the ground. Each agent is equipped with a different sensor, (i.e. a ground-penetrating radar, a thermal sensor and three cameras each covering a different spectrum) and processes dedicated AI decision-making capabilities. The proposed system has a unique hardware structure, with a distributed design and effective selection of sensors, and a novel multi-phase and cooperative decision-making framework. Agents operate independently via a customised logic adjusting their sensor positions - to achieve optimal acquisition; performing a preliminary "local decision-making" - to classify buried objects; sharing information with the other agents. Once sufficient information is shared by the agents, a collaborative behaviour emerges in the so-called "cooperative decision-making" process, which performs the final detection. In this paper, 120 variations of the proposed system, obtained by combining both classic aggregation operators as well as advanced neural and fuzzy systems, are presented, tested and evaluated. Results show a good detection accuracy and robustness to environmental and data sets changes, in particular when the cooperative decision-making is implemented with the neuroevolution paradigm.																	1566-2535	1872-6305				DEC	2020	64						32	49		10.1016/j.inffus.2020.06.009													
J								A survey on empathetic dialogue systems	INFORMATION FUSION										Artificial intelligence; Affective computing; Dialogue systems	PERSONALITY; GENERATION; EMOTION; AGENTS; VOICE	Dialogue systems have achieved growing success in many areas thanks to the rapid advances of machine learning techniques. In the quest for generating more human-like conversations, one of the major challenges is to learn to generate responses in a more empathetic manner. In this review article, we focus on the literature of empathetic dialogue systems, whose goal is to enhance the perception and expression of emotional states, personal preference, and knowledge. Accordingly, we identify three key features that underpin such systems: emotion-awareness, personality-awareness, and knowledge-accessibility. The main goal of this review is to serve as a comprehensive guide to research and development on empathetic dialogue systems and to suggest future directions in this domain.																	1566-2535	1872-6305				DEC	2020	64						50	70		10.1016/j.inffus.2020.06.011													
J								Multi-focus image fusion: A Survey of the state of the art	INFORMATION FUSION										Multi-focus image fusion; Image transform; Activity level measurement; Fusion rule; Deep learning	MULTIRESOLUTION SIGNAL DECOMPOSITION; NONSUBSAMPLED CONTOURLET TRANSFORM; EMPIRICAL MODE DECOMPOSITION; CONVOLUTIONAL NEURAL-NETWORK; SPIKING CORTICAL MODEL; SPARSE REPRESENTATION; SPATIAL-FREQUENCY; GUIDED-FILTER; INFORMATION MEASURE; QUALITY ASSESSMENT	Mull-focus image fusion is an effective technique to extend the depth-of-field of optical lenses by creating an all-in-focus image from a set of partially focused images of the same scene. In the last few years, great progress has been achieved in this field along with the rapid development of image representation theories and approaches such as mull-scale geometric analysis, sparse representation, deep learning, etc. This survey paper first presents a comprehensive overview of existing mull-focus image fusion methods. To keep up with the latest development in this field, a new taxonomy is introduced to classify existing methods into four main categories: transform domain methods, spatial domain methods, methods combining transform domain and spatial domain, and deep learning methods. For each category, representative fusion methods are introduced and summarized. Then, a comparative study for 18 representative fusion methods is conducted based on 30 pairs of commonly-used mull-focus images and 8 popular objective fusion metrics. All the relevant resources including source images, objective metrics and fusion results are released online, aiming to provide a benchmark for the future study of multi-focus image fusion. Finally, several major challenges remained in the current research of this field are discussed and some future prospects are put forward.																	1566-2535	1872-6305				DEC	2020	64						71	91		10.1016/j.inffus.2020.06.013													
J								Ordinal scale based uncertainty models for AI	INFORMATION FUSION										Linguistically expressed; Fuzzy measure; Ordinal information; Multi-source fusion	SPECIFICITY	In human processed AI, HP-AI, we build our AI systems based on knowledge learned by human experts rather then that learned by artificial neural networks such as in the case of deep learning. The information provided by these human experts is typically linguistically expressed. In support of HP-AI we look at the properties of an ordinal scale, S, needed to model linguistically expressed quantitative information. Since fuzzy measures provide a very general structure for modeling uncertainty we look at ordinal fuzzy measures. We look at the Sugeno integral based on this ordinal S scale. We discuss the modeling of information about an uncertain variable using an ordinal scale. We look at the problem of multi-source in this ordinal environment.																	1566-2535	1872-6305				DEC	2020	64						92	98		10.1016/j.inffus.2020.06.010													
J								Data fusion strategies for energy efficiency in buildings: Overview, challenges and novel orientations	INFORMATION FUSION										Data fusion; Energy efficiency; Sensors; Appliance identification; Fusion of 2D descriptors; Machine learning	BLUETOOTH LOW-ENERGY; REAL-TIME; MANAGEMENT-SYSTEM; DATA VISUALIZATION; PREDICTIVE CONTROL; OFFICE BUILDINGS; SMART BUILDINGS; BEHAVIOR-CHANGE; MICRO-MOMENTS; SERIOUS GAMES	Recently, tremendous interest has been devoted to develop data fusion strategies for energy efficiency in buildings, where various kinds of information can be processed. However, applying the appropriate data fusion strategy to design an efficient energy efficiency system is not straightforward; it requires a priori knowledge of existing fusion strategies, their applications and their properties. To this regard, seeking to provide the energy research community with a better understanding of data fusion strategies in building energy saving systems, their principles, advantages, and potential applications, this paper proposes an extensive survey of existing data fusion mechanisms deployed to reduce excessive consumption and promote sustainability. We investigate their conceptualizations, advantages, challenges and drawbacks, as well as performing a taxonomy of existing data fusion strategies and other contributing factors. Following, a comprehensive comparison of the state-of-the-art data fusion based energy efficiency frameworks is conducted using various parameters, including data fusion level, data fusion techniques, behavioral change influencer, behavioral change incentive, recorded data, platform architecture, IoT technology and application scenario. Moreover, a novel method for electrical appliance identification is proposed based on the fusion of 2D local texture descriptors, where 1D power signals are transformed into 2D space and treated as images. The empirical evaluation, conducted on three real datasets, shows promising performance, in which up to 99.68% accuracy and 99.52% F1 score have been attained. In addition, various open research challenges and future orientations to improve data fusion based energy efficiency ecosystems are explored.																	1566-2535	1872-6305				DEC	2020	64						99	120		10.1016/j.inffus.2020.07.003													
J								Remote sensing image classification using subspace sensor fusion	INFORMATION FUSION										Multisensor data fusion; Classification; Dimensionality reduction; Feature extraction; Subspace fusion; Remote sensing	HIGH-RESOLUTION LIDAR; HYPERSPECTRAL IMAGE; CONTEST-PART; SEGMENTATION; MULTISOURCE; PROFILES	The amount of remote sensing and ancillary datasets captured by diverse airborne and spaceborne sensors has been tremendously increased, which opens up the possibility of utilizing multimodal datasets to improve the performance of processing approaches with respect to the application at hand. However, developing a generic framework with high generalization capability that can effectively fuse diverse datasets is a challenging task since the current approaches are usually only applicable to two specific sensors for data fusion. In this paper, we propose an accurate fusion-based technique called SubFus with capability to integrate diverse remote sensing data for land cover classification. Here, we assume that a high dimensional multisensor dataset can be represented fused features that live in a lower-dimensional space. The proposed classification methodology includes three main stages. First, spatial information is extracted by using spatial filters (i.e., morphology fillers). Then, a novel low-rank minimization problem is proposed to represent the multisensor datasets in subspaces using fused features. The fused features in the lower-dimensional subspace are estimated using a novel iterative algorithm based on the alternative direction method of multipliers. Third, the final classification map is produced by applying a supervised spectral classifier (i.e., random forest) on the fused features. In the experiments, the proposed method is applied to a three-sensor (RGB, multispectral LiDAR, and hyperspectral images) dataset captured over the area of the University of Houston, the USA, and a two-sensor (hyperspectral and LiDAR) dataset captured over the city of Trento, Italy. The land-cover maps generated using SubFus are evaluated based on classification accuracies. Experimental results obtained by SubFus confirm considerable improvements in terms of classification accuracies compared with the other methods used in the experiments. The proposed fusion approach obtains 85.32% and 99.25% in terms of overall classification accuracy on the Houston (the training portion of the dataset distributed for the data fusion contest of 2018) and trento datasets, respectively.																	1566-2535	1872-6305				DEC	2020	64						121	130		10.1016/j.inffus.2020.07.002													
J								Deepfakes and beyond: A Survey of face manipulation and fake detection	INFORMATION FUSION										Fake news; Deepfakes; Media forensics; Face manipulation; Face recognition; Benchmark; Databases	IMAGE FORGERY DETECTION; MULTIPLE CLASSIFIERS; ADVERSARIAL NETWORK; SYSTEMS	The free access to large-scale public databases, together with the fast progress of deep learning techniques, in particular Generative Adversarial Networks, have led to the generation of very realistic fake content with its corresponding implications towards society in this era of fake news. This survey provides a thorough review of techniques for manipulating face images including DeepFake methods, and methods to detect such manipulations. In particular, four types of facial manipulation are reviewed: i) entire face synthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv) expression swap. For each manipulation group, we provide details regarding manipulation techniques, existing public databases, and key benchmarks for technology evaluation of fake detection methods, including a summary of results from those evaluations. Among all the aspects discussed in the survey, we pay special attention to the latest generation of DeepFakes, highlighting its improvements and challenges for fake detection. In addition to the survey information, we also discuss open issues and future trends that should be considered to advance in the field.																	1566-2535	1872-6305				DEC	2020	64						131	148		10.1016/j.inffus.2020.06.014													
J								Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation	INFORMATION FUSION										Multimodal data fusion; Neuroimaging; Magnetic resonance imaging; PET; SPECT; Fusion rules; Assessment; Applications; Partial volume effect	MILD COGNITIVE IMPAIRMENT; MEDICAL IMAGE FUSION; OBSESSIVE-COMPULSIVE DISORDER; MAGNETIC-RESONANCE-SPECTROSCOPY; POSITRON-EMISSION-TOMOGRAPHY; PARTIAL VOLUME CORRECTION; ATLAS-BASED SEGMENTATION; SUBTRACTION SCATTER CORRECTION; STANDARDIZED UPTAKE VALUES; COMPLEX WAVELET TRANSFORM	Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamental limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and various sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread education and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging.																	1566-2535	1872-6305				DEC	2020	64						149	187		10.1016/j.inffus.2020.07.006													
J								Foundations of Multimodal Co-learning	INFORMATION FUSION										Multimodal machine learning; Multimodal learning; Co-learning		In the current state of the field of machine learning, often, real-world phenomena are learned through studies of isolated modalities; such as modeling language exclusively from verbal modality, which is a common theme in natural language processing. This is widely adopted since downstream tasksin different disciplines of machine learning are also often similarly isolated and unimodal. In sharp contrast to this, human learning from real-world experiences is rarely unimodal, and often exhibits a multisensory nature, regardless of any assumptions about downstream tasks. The cognitive constructs in human brain are consistently developed through multisensory reinforcement, and the same constructs generalize to unimodal scenarios. The difference between the trend of unimodal learning and human cognitive development raises the following question: "Even if downstream tasks are unimodal during test time, is it better to learn from the isolated modality or from multimodal information?". In this paper we focus on an in-depth study of this research question. We study the differences between unimodal learning and Multimodal Co-learning (MCl), both from empirical and theoretical standpoints. Through the lens of information entropy and characteristics of deep neural networks, we demonstrate strong theoretical justifications in favor of MCl.																	1566-2535	1872-6305				DEC	2020	64						188	193		10.1016/j.inffus.2020.06.001													
J								Unscented kalman filter with process noise covariance estimation for vehicular ins/gps integration system	INFORMATION FUSION										Vehicular navigation; Ins/gps integration; Unscented kalman filter; Process noise covariance; maximum likelihood estimation	UKF; NAVIGATION; FUSION	The unscented Kalman filter (UKF) has proved to be a promising methodology to integrate INS and GPS for vehicular navigation. Nevertheless, the disturbance suppression of system noise uncertainty on the UKF performance is still an open issue. In this paper, based on the maximum likelihood (ML) principle, a new adaptive UKF with process noise covariance estimation is proposed to enhance the UKF robustness against process noise uncertainty for vehicular INS/GPS integration. The proposed method extends the concept of ML estimation from the linear Kalman filter to the nonlinear UKF to estimate the process noise covariance. Meanwhile, an estimation window for fixed-length memory is introduced to emphasize the use of the new observations and gradually discard the old ones. Since it has the capability to estimate and update the process noise covariance online, the proposed method improves the standard UKF by restraining the disturbance of process noise uncertainty on the filtering solution. The effectiveness and superiority of the proposed method have been verified through Monte Carlo simulations and practical experiment in vehicular INS/GPS integration system.																	1566-2535	1872-6305				DEC	2020	64						194	204		10.1016/j.inffus.2020.08.005													
J								A practical tutorial on bagging and boosting based ensembles for machine learning: Algorithms, software tools, performance study, practical perspectives and opportunities	INFORMATION FUSION										Decision trees; Ensemble learning; Classification; Machine learning; Software	DECISION TREES; ROTATION FOREST; MISSING VALUE; CLASSIFICATION; CLASSIFIERS; REGRESSION; SELECTION; PREDICTION; STACKING; DESIGN	Ensembles, especially ensembles of decision trees, are one of the most popular and successful techniques in machine learning. Recently, the number of ensemble-based proposals has grown steadily. Therefore, it is necessary to identify which are the appropriate algorithms for a certain problem. In this paper, we aim to help practitioners to choose the best ensemble technique according to their problem characteristics and their workflow. To do so, we revise the most renowned bagging and boosting algorithms and their software tools. These ensembles are described in detail within their variants and improvements available in the literature. Their online-available software tools are reviewed attending to the implemented versions and features. They are categorized according to their supported programming languages and computing paradigms. The performance of 14 different bagging and boosting based ensembles, including XGBoost, LightGBM and Random Forest, is empirically analyzed in terms of predictive capability and efficiency. This comparison is done under the same software environment with 76 different classification tasks. Their predictive capabilities are evaluated with a wide variety of scenarios, such as standard multi-class problems, scenarios with categorical features and big size data. The efficiency of these methods is analyzed with considerably large data-sets. Several practical perspectives and opportunities are also exposed for ensemble learning.																	1566-2535	1872-6305				DEC	2020	64						205	237		10.1016/j.inffus.2020.07.007													
J								Multi-scale spatial context-based semantic edge detection	INFORMATION FUSION										Semantic edge detection; Convolutional neural network; Multi-scale feature fusion; Location-aware information fusion; Gradual fusion	IMAGE; BOUNDARIES	The good fusion of multi-scale features obtained by Convolutional neural networks (CNNs) is key to semantic edge detection; however, obtaining fusion is challenging. This paper presents a Multi-scale Spatial Context-based deep network for Semantic Edge Detection (MSC-SED). Different from state-of-the-art methods, MSC-SED gradually fuses multi-scale low-to-high level CNN features in an end-to-end architecture. This fusion structure obtains rich multi-scale features while enhancing the details of higher-level features. Beside the overall structure, we present the following two functional modules: the Context Aggregation Module (CAM) and Location-Aware fusion Module (LAM). The CAM helps to enrich context in features at each stage, before and after fusion. The LAM helps to selectively integrate lower-level features. The proposed method outperforms state-of-the-art approaches in terms of both the edge quality and the accuracy of edge categorization on both the SBD and Cityscapes datasets.																	1566-2535	1872-6305				DEC	2020	64						238	251		10.1016/j.inffus.2020.08.014													
J								The introduction of population migration to SEIAR for COVID-19 epidemic modeling with an efficient intervention strategy	INFORMATION FUSION										COVID-19; SEIAR; Basic reproduction number; Migration-in rate; Contact rate		In this paper, we present a mathematical model of an infectious disease according to the characteristics of the COVID-19 pandemic. The proposed enhanced model, which will be referred to as the SEIR (Susceptible-Exposed-Infectious-Recovered) model with population migration, is inspired by the role that asymptomatic infected individuals, as well as population movements can play a crucial role in spreading the virus. In the model, the infected and the basic reproduction numbers are compared under the influence of intervention policies. The experimental simulation results show the impact of social distancing and migration-in rates on reducing the total number of infections and the basic reproductions. And then, the importance of controlling the number of migration-in people and the policy of restricting residents' movements in preventing the spread of COVID-19 pandemic are verified.																	1566-2535	1872-6305				DEC	2020	64						252	258		10.1016/j.inffus.2020.08.002													
J								Sample greedy gossip distributed Kalman filter	INFORMATION FUSION										Distributed estimation; Kalman filter; Gossip process; Sample greedy; Information weighted fusion	CONSENSUS; TRACKING	This paper investigates the problem of distributed state estimation over a low-cost sensor network and proposes a new sample greedy gossip distributed Kalman filter. The proposed algorithm leverages the information weighted fusion concept and the sample greedy gossip averaging protocol. By introducing a stochastic sampling strategy in the greedy sensor node selection process, the proposed algorithm finds a suboptimal communication path for each local sensor node during the process of information exchange. Theoretical analysis on global convergence and uniform boundedness is also performed to investigate the characteristics of the proposed distributed Kalman filter. The main advantage of the proposed algorithm is that it provides well trade-off between communication burden and estimation performance. Extensive empirical numerical simulations are carried out to demonstrate the effectiveness of the proposed algorithm.																	1566-2535	1872-6305				DEC	2020	64						259	269		10.1016/j.inffus.2020.08.001													
J								Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy	INFORMATION FUSION										Federated learning; Differential privacy; Software framework; Sherpa. ai Federated Learning framework		The high demand of artificial intelligence services at the edges that also preserve data privacy has pushed the research on novel machine learning paradigms that fit these requirements. Federated learning has the ambition to protect data privacy through distributed learning methods that keep the data in its storage silos. Likewise, differential privacy attains to improve the protection of data privacy by measuring the privacy loss in the communication among the elements of federated learning. The prospective matching of federated learning and differential privacy to the challenges of data privacy protection has caused the release of several software tools that support their functionalities, but they lack a unified vision of these techniques, and a methodological workflow that supports their usage. Hence, we present the Sherpa. ai Federated Learning framework that is built upon a holistic view of federated learning and differential privacy. It results from both the study of how to adapt the machine learning paradigm to federated learning, and the definition of methodological guidelines for developing artificial intelligence services based on federated learning and differential privacy. We show how to follow the methodological guidelines with the Sherpa. ai Federated Learning framework by means of a classification and a regression use cases.																	1566-2535	1872-6305				DEC	2020	64						270	292		10.1016/j.inffus.2020.07.005													
J								Robust fusion algorithms for unsupervised change detection between multi-band optical images - A comprehensive case study	INFORMATION FUSION										Image fusion; Change detection; Different resolutions; Hyperspectral imagery; Multispectral imagery	CHANGE VECTOR ANALYSIS; HYPERSPECTRAL IMAGE; SPECTRAL RESOLUTIONS; SATELLITE IMAGES; LAND-COVER; SUPERRESOLUTION; ROC; AREA; MAD	Unsupervised change detection techniques are generally constrained to two multi-band optical images acquired at different times through sensors sharing the same spatial and spectral resolution. In the case of the optical modality, largely studied in the remote sensing community, a straight comparison of homologous pixels such as pixel-wise differencing is suitable. However, in some specific cases such as emergency situations, punctual missions, defense and security, the only available images may be those acquired through different kinds of sensors with different resolutions. Recently some change detection techniques, dealing with images with different spatial and spectral resolutions, have been proposed. Nevertheless, they are focused on a specific scenario where one image has a high spatial and low spectral resolution while the other has a low spatial and high spectral resolution. This paper addresses the problem of detecting changes between any two multi-band optical images disregarding their spatial and spectral resolution disparities. To overcome resolution disparity, state-of-the art methods apply conventional change detection methods after preprocessing steps applied independently on the two images, e.g. resampling operations intended to reach the same spatial and spectral resolutions. Nevertheless, these preprocessing steps may waste relevant information since they do not take into account the strong interplay existing between the two images. Conversely, in this paper, we propose a method that more effectively uses the available information by modeling the two observed images as spatially and spectrally degraded versions of two (unobserved) latent images characterized by the same high spatial and high spectral resolutions. Covering the same scene, the latent images are expected to be globally similar except for possible changes in spatially sparse locations. Thus, the change detection task is envisioned through a robust fusion task which enforces the differences between the estimated latent images to be spatially sparse. We show that this robust fusion can be formulated as an inverse problem which is iteratively solved using an alternating minimization strategy. The proposed framework is implemented for an exhaustive list of applicative scenarios and applied to real multi-band optical images. A comparison with state-of-the-art change detection methods evidences the accuracy and the versatility of the proposed robust fusion-based strategy.																	1566-2535	1872-6305				DEC	2020	64						293	317		10.1016/j.inffus.2020.08.008													
J								Revisiting crowd behaviour analysis through deep learning: Taxonomy, anomaly detection, crowd emotions, datasets, opportunities and prospects	INFORMATION FUSION										Crowd behaviour analysis; Crowd anomaly detection; Crowd emotions; Review; Deep learning; Models fusion	ABNORMAL-BEHAVIOR; COMPUTER VISION; NEURAL-NETWORKS; BASE-LINE; LOCALIZATION	Crowd behaviour analysis is an emerging research area. Due to its novelty, a proper taxonomy to organise its different sub-tasks is still missing. This paper proposes a taxonomic organisation of existing works following a pipeline, where sub-problems in last stages benefit from the results in previous ones. Models that employ Deep Learning to solve crowd anomaly detection, one of the proposed stages, are reviewed in depth, and the few works that address emotional aspects of crowds are outlined. The importance of bringing emotional aspects into the study of crowd behaviour is remarked, together with the necessity of producing real-world, challenging datasets in order to improve the current solutions. Opportunities for fusing these models into already functioning video analytics systems are proposed.																	1566-2535	1872-6305				DEC	2020	64						318	335		10.1016/j.inffus.2020.07.008													
J								Spectral clustering via ensemble deep autoencoder learning (SC-EDAE)	PATTERN RECOGNITION										Spectral clustering; Unsupervised ensemble learning; Autoencoder		Several works have studied clustering strategies that combine classical clustering algorithms and deep learning methods. These strategies generally improve clustering performance, however deep autoencoder setting issues impede the robustness of these approaches. To alleviate the impact of hyperparameters setting, we propose a model which combines spectral clustering and deep autoencoder strengths in an ensemble framework. Our proposal does not require any pretraining and includes the three following steps: generating various deep embeddings from the original data, constructing a sparse and low-dimensional ensemble affinity matrix based on anchors strategy and applying spectral clustering to obtain the common space shared by multiple deep representations. While the anchors strategy ensures an efficient merging of the encodings, the fusion of various deep representations enables to mitigate the deep networks setting issues. Experiments on various benchmark datasets demonstrate the potential and robustness of our approach compared to state-of-the-art deep clustering methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107522	10.1016/j.patcog.2020.107522													
J								Generalized low-rank approximation of matrices based on multiple transformation pairs	PATTERN RECOGNITION										Machine learning; Matrix data classification; Kronecker product; Dimensionality reduction; SVD; GLRAM	PRINCIPAL COMPONENT ANALYSIS; BILINEAR LANCZOS COMPONENTS; DIMENSIONALITY REDUCTION; DISCRIMINANT-ANALYSIS; FACE RECOGNITION; EFFICIENT; SVM	Dimensionality reduction is a critical step in the learning process that plays an essential role in various applications. The most popular methods for dimensionality reduction, SVD and PCA, for instance, only work on one-dimensional data. This means that for higher-order data like matrices or more generally tensors, data should be fold to the vector format. Thus, this approach ignores the spatial relationships of features and increases the probability of overfitting as well. Due to the mentioned issues, several methods like Generalized Low-Rank Approximation of Matrices (GLRAM) and Multilinear PCA (MPCA) proposed to deal with multi-dimensional data in their original format. Consequently, the spatial relationships of features preserved and the probability of overfitting diminished. Besides, the time and space complexity in such methods are less than vector-based ones. However, since the multilinear approach needs fewer parameters, its search space is much smaller than that of the vector-based one. To solve the previous problems of multilinear methods like GLRAM, we proposed a novel extension of GLRAM in which instead one transformation pair use multiple left and right transformation pairs on the projected data. Consequently, this provides the problem with a larger feasible region and smaller reconstruction error. This article provides several analytical discussions and experimental results that confirm the quality of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107545	10.1016/j.patcog.2020.107545													
J								Transductive semi-supervised metric learning for person re-identification	PATTERN RECOGNITION										Person re-identification; Transductive learning; Semi-supervised learning; Graph; Confidence score		Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID. Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107569	10.1016/j.patcog.2020.107569													
J								SEMEDA: Enhancing segmentation precision with semantic edge aware loss	PATTERN RECOGNITION										Semantic segmentation; Loss function; Computer vision		Per-Pixel Cross entropy (PPCE) is a commonly used loss on semantic segmentation tasks. However, it suffers from a number of drawbacks. Firstly, PPCE only depends on the probability of the ground truth class since the latter is usually one-hot encoded. Secondly, PPCE treats all pixels independently and does not take the local structure into account. While perceptual losses (e.g. matching prediction and ground truth in the embedding space of a pre-trained VGG network) would theoretically address these concerns, does not constitute a practical solution as segmentation masks follow a distribution that differs largely from natural images. In this paper, we introduce a SEMantic EDge-Aware strategy (SEMEDA) to solve these issues. Inspired by perceptual losses, we propose to match the 'probability texture' of predicted segmentation mask and ground truth through a proxy network trained for semantic edge detection on the ground truth masks. Through thorough experimental validation on several datasets, we show that SEMEDA steadily improves the segmentation accuracy with negligible computational overhead and can be added with any popular segmentation networks in an end-to-end training framework. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107557	10.1016/j.patcog.2020.107557													
J								MuLTReNets: Multilingual text recognition networks for simultaneous script identification and handwriting recognition	PATTERN RECOGNITION										Multrenets; Auto-weighter; Separable MDLSTM; Multilingual handwritten text recognition; Multi-task learning	LANGUAGE IDENTIFICATION; DATASET	Multilingual handwritten text recognition is often accomplished in two cascaded steps: script identification and handwriting recognition. However, this scheme is not optimal due to error accumulation. To perform simultaneous script identification and handwriting recognition, in this paper, we propose a new framework named multilingual text recognition networks (MuLTReNets). Specifically, the system has four major modules: feature extractor, script identifier, handwriting recognizer and auto-weighter. The feature extractor integrates both spatial and temporal knowledge to encode text images into features shared by the script identifier and recognizer. The script identifier predicts script category from a variable-length sequence incorporating an auto-weighter for balancing different scripts, while the handwriting recognizer adopts long-short term memory (LSTM) and Connectionist Temporal Classification (CTC) to accomplish sequence decoding. Via multi-task learning, the proposed framework can benefit both two multilingual recognition schemes: unified recognition with merged alphabet (MuLTReNetV1) and cascaded script identification-single script recognition with joint training (MuLTReNetV2). We evaluated the performance of the proposed method on handwritten text databases of five languages, which are English, French, Kannada, Urdu, and Bangla. Experimental results demonstrate that our method performs superiorly for both script identification and handwriting recognition. The accuracy of script identification reaches 99.9%. While in handwriting recognition, the proposed system not only outperforms cascade systems but also surpasses systems particularly designed for specific scripts. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107555	10.1016/j.patcog.2020.107555													
J								Sensor-based and vision-based human activity recognition: A comprehensive survey	PATTERN RECOGNITION										Human activity recognition; Action recognition; Sensors; Vision; Human-centric sensing; Deep learning; Context-awareness	RGB-D; BACKGROUND SUBTRACTION; NEURAL-NETWORK; K-MEANS; TRACKING; ACCELEROMETER; CLASSIFICATION; PREDICTION; SEGMENTATION; AUTOENCODER	Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type. After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107561	10.1016/j.patcog.2020.107561													
J								Diverse training dataset generation based on a multi-objective optimization for semi-Supervised classification	PATTERN RECOGNITION										Self-labeled; Semi-supervised learning; Evolutionary multi-objective optimization; Data density function; NSGA-II	SMOTE	The self-labeled technique is a type of semi-supervised classification that can be used when labeled data are lacking. Although existing self-labeled techniques show promise in many areas of classification and pattern recognition, they commonly incorrectly label data. The reasons for this problem are the shortage of labeled data and the inappropriate distribution of data in problem space. To deal with this problem, we propose in this paper a synthetic, labeled data generation method based on accuracy and density. Positions of generated data are improved through a multi-objective evolutionary algorithm with two objectives - accuracy and density. The density function generates data with an appropriate distribution and diversity in feature space, whereas the accuracy function eliminates outlier data. The advantage of the proposed method over existing ones is that it simultaneously considers accuracy and distribution of generated data in feature space. We have applied the new proposed method on four self-labeled techniques with different features, i.e., Democratic-co, Tri-training, Co-forest, and Co-bagging. The results show that the proposed method is superior to existing methods in terms of classification accuracy. Also, the superiority of the proposed method is confirmed over other data generation methods such as SMOTE, Borderline SMOTE, Safe-level SMOTE and SMOTE-RSB. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107543	10.1016/j.patcog.2020.107543													
J								Multi-view subspace learning via bidirectional sparsity	PATTERN RECOGNITION										Multi-view clustering; Subspace learning; Bidirectional sparsity; Non-convex optimization	SELECTION; FEATURES; SCALE; SCENE	With the improvement of multi-view data collection technology, multi-view learning has become a hot research area. How to deal with diverse and complex data is one of the challenging problems in multi-view learning. However, it is hard for traditional multi-view subspace learning methods to find an effective subspace dimension and deal with outliers simultaneously. In this paper, we propose a novel method, named as Multi-view Subspace Learning via Bidirectional Sparsity(SLBS), which is effective to overcome the above difficulties and learn a better representation. Specifically, we divide the shared subspace into two parts. One is a row sparse matrix to do a secondary extraction of features and the other is a column sparse matrix to reduce the influence of outliers. The proposed model is a non-convex problem which is difficult to be solved. To address this problem, we develop an efficient algorithm and analyze its convergence and computational complexity. Finally, compared with other multi-view subspace learning methods, the extensive experimental results on real-world datasets present the effectiveness of our SLBS. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107524	10.1016/j.patcog.2020.107524													
J								Image segmentation using dense and sparse hierarchies of superpixels	PATTERN RECOGNITION										Superpixel segmentation; Hierarchical image segmentation; Image foresting transform; Iterative spanning forest; Graph-based image segmentation; Irregular image pyramid	ALGORITHMS	We investigate the intersection between hierarchical and superpixel image segmentation. Two strategies are considered: (i) the classical region merging, that creates a dense hierarchy with a higher number of levels, and (ii) the recursive execution of some superpixel algorithm, which generates a sparse hierarchy with fewer levels. We show that, while dense methods can capture more intermediate or higher-level object information, sparse methods are considerably faster and usually with higher boundary adherence at finer levels. We first formalize the two strategies and present a sparse method, which is faster than its superpixel algorithm and with similar boundary adherence. We then propose a new dense method to be used as post-processing from the intermediate level, as obtained by our sparse method, upwards. This combination results in a unique strategy and the most effective hierarchical segmentation method among the compared state-of-the-art approaches, with efficiency comparable to the fastest superpixel algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107532	10.1016/j.patcog.2020.107532													
J								Density peak clustering based on relative density relationship	PATTERN RECOGNITION										Density based clustering; Density peak; Cluster center; Relative density relationship	FAST SEARCH; FIND	The density peak clustering algorithm treats local density peaks as cluster centers, and groups non-center data points by assuming that one data point and its nearest higher-density neighbor are in the same cluster. While this algorithm is shown to be promising in some applications, its clustering results are found to be sensitive to density kernels, and large density differences across clusters tend to result in wrong cluster centers. In this paper we attribute these problems to the inconsistency between the assumption and implementation adopted in this algorithm. While the assumption is based totally on relative density relationship, this algorithm adopts absolute density as one criterion to identify cluster centers. This observation prompts us to present a cluster center identification criterion based only on relative density relationship. Specifically, we define the concept of subordinate to describe the relative density relationship, and use the number of subordinates as a criterion to identify cluster centers. Our approach makes use of only relative density relationship and is less influenced by density kernels and density differences across clusters. In addition, we discuss the problems of two existing density kernels, and present an average distance based kernel. In data clustering experiments we validate the new criterion and density kernel respectively, and then test the whole algorithm and compare with some other clustering algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107554	10.1016/j.patcog.2020.107554													
J								Mutual information based feature subset selection in multivariate time series classification	PATTERN RECOGNITION										Multivariate time series; Supervised classification; Feature susbset selection; Mutual information	FRAMEWORK; RANKING	This paper deals with supervised classification of multivariate time series. In particular, the goal is to propose a filter method to select a subset of time series. Consequently, we adopt the framework proposed by Brown al. [1]. The key point in this framework is the computation of the mutual information between the features, which allows us to measure the relevance of each feature subset. In our case, where the features are a time series, we use an adaptation of existing nonparametric mutual information estimators based on the k-nearest neighbor. Specifically, for the purpose of bringing these methods to the time series scenario, we rely on the use of dynamic time warping dissimilarity. Our experimental results show that our method is able to strongly reduce the number of time series while keeping or increasing the classification accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107525	10.1016/j.patcog.2020.107525													
J								View-specific subspace learning and re-ranking for semi-supervised person re-identification	PATTERN RECOGNITION										Person re-identification; Metric learning; Semi-supervised learning; Re-ranking		Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other's ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107568	10.1016/j.patcog.2020.107568													
J								Relational graph neural network for situation recognition	PATTERN RECOGNITION										Situation recognition; Relationship modeling; Graph neural network; Reinforcement learning		Recently, situation recognition as a new challenging task for image understanding has gained great attention, which needs to simultaneously predict the main activity (verb) and its associated objects (noun entities) in a structured and detailed way. Several methods have been proposed to handle this task, but usually they cannot effectively model the relationships between the activity and the objects. In this paper, we propose a Relational Graph Neural Network (RGNN) for situation recognition, which builds a neural graph on the activity and the objects, and models the triplet relationships between the activity and pairs of objects through message passing between graph nodes. Moreover, we propose a two-stage training strategy to optimize the model. A progressive supervised learning is first adopted to obtain an initial prediction for the activity and the objects. Then, the initial predictions are refined by using a policy-gradient method to directly optimize the non-differentiable value-all metric. To verify the effectiveness of our method, we perform extensive experiments on the Imsitu dataset which is currently the only available dataset for situation recognition. Experimental results show that our approach outperforms the state-ofthe-art methods on verb and value metrics, and demonstrates better relationships between the activity and the objects . (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107544	10.1016/j.patcog.2020.107544													
J								Freely typed keystroke dynamics-based user authentication for mobile devices based on heterogeneous features	PATTERN RECOGNITION										Mobile user authentication; Keystroke dynamics; Freely typed text; Anomaly detection; Heterogeneous data fusion	BIOMETRICS	Keystroke dynamics-based authentication (KDA) is one of the human behavioral biometric-based user authentication methods based on the unique typing pattern of a person. Previous KDA studies on mobile devices primarily focused on fixed-length text-based KDA, such as passwords and personal identification numbers. This can strengthen the login system and prevent abnormal usage of impostors based on certain attack methods, such as shoulder surfing and smudge attacks. However, this method possesses a limitation that continuous monitoring is not possible after login. To solve this problem, KDA based on freely typed text was studied; however, there are only a few studies on this technique. Further, the performance authentication based on these studies is insufficient for a real-world implementation. In this paper, we propose a novel freely typed text-based KDA method for mobile devices named FACT, i.e., user authentication on mobile devices based on free text, accelerator, coordinate, and time. We collected data from three different smartphone sensors while typing in two languages (English and Korean), and 17 variables were extracted for a set of keystroke data. A total of six authentication methods were employed and the proposed FACT yielded an equal error rate lower than 1% with only one reference keystroke set; moreover, it demonstrated a perfect protection capability while using Korean when more than four reference keystroke sets were used. To contribute to the research and industrial community, we have publicized our collected keystroke dataset so that anyone who conducts a KDA study or develops a KDA-related mobile service can use the dataset without any restrictions. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107556	10.1016/j.patcog.2020.107556													
J								Bringing semantics into word image representation	PATTERN RECOGNITION										Word image embedding; Word spotting; Semantic spotting		The shift from one-hot to distributed representation, popularly referred to as word embedding has changed the landscape of natural language processing (NLP) and information retrieval (IR) communities. In the domain of document images, we have always appreciated the need for learning a holistic word image representation which is popularly used for the task of word spotting. The representations proposed for word spotting is different from word embedding in text since the later captures the semantic aspects of the word which is a crucial ingredient to numerous NLP and IR tasks. In this work, we attempt to encode the notion of semantics into word image representation by bringing the advancements from the textual domain. We propose two novel forms of representations where the first form is designed to be inflection invariant by focusing on the approximate linguistic root of the word, while the second form is built along the lines of recent textual word embedding techniques such as Word2Vec. We observe that such representations are useful for both traditional word spotting and also enrich the search results by accounting the semantic nature of the task. We conduct our experiments on the challenging document images taken from historical-modern collections, handwritten-printed domains, and Latin-Indic scripts. For the purpose of semantic evaluation, we have prepared a large synthetic word image dataset and report interesting results for the standard semantic evaluation metrics such as word analogy and word similarity. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107542	10.1016/j.patcog.2020.107542													
J								Abnormal event detection in surveillance videos based on low-rank and compact coefficient dictionary learning	PATTERN RECOGNITION										LRCCDL; Reconstruction cost; Abnormal event detection; Crowded scenes; Surveillance videos	CROWD BEHAVIOR DETECTION; ANOMALY DETECTION; SPARSE REPRESENTATION; ALGORITHM; LOCALIZATION; MODEL	In this paper, a novel approach to abnormal event detection in crowded scenes is presented based on a new low-rank and compact coefficient dictionary learning (LRCCDL) algorithm. First, based on the background subtraction and binarization of surveillance videos, we construct a feature space by extracting the histogram of maximal optical flow projection (HMOFP) feature of the foreground from a normal training frame set. Second, in the training stage, a new joint optimization of the nuclear-norm and l(2, 1)-norm is applied to obtain a compact coefficient low-rank dictionary. Third, in the detection stage, l(2, 1)-norm optimization is utilized to obtain the reconstruction coefficient vectors of the testing samples. Note that the l(2, 1)-norm forces the reconstruction vectors of all the testing samples to compactly surround the same center in the training stage, such that the reconstruction errors of abnormal testing samples are different from those of normal ones. Finally, a reconstruction cost (RC) is introduced to detect abnormal frames. Experimental results on both global and local abnormal event detection show the effectiveness of our algorithm. Based on comparisons with state-of-the-art methods employing various criteria, the proposed algorithm achieves comparable detection results. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107355	10.1016/j.patcog.2020.107355													
J								Discovering and incorporating latent target-domains for domain adaptation	PATTERN RECOGNITION										Unsupervised domain adaptation; Latent domain; Multiple kernel learning		In this paper, we aim to address the unsupervised domain adaptation problem where the data in the target domain are much more diverse compared with the data in the source domain. In particular, this problem is formulated as discovering and incorporating latent domains underlying target data of interest for unsupervised domain adaptation. More specifically, the discovery of the latent target domains is based on three criteria, including the maximization of compactness and distinctiveness of the data in the individual latent target-domain, as well as the minimization of total divergence from the latent target-domains to the source domain. For each pair formed by a latent target domain and the source domain, we learn a feature space where the discrepancy between the source domain and the specific latent target domain is shrunk. Finally, we consider the projected source domain data on the learned latent feature spaces as different views of the source domain, and propose an extended multiple kernel learning algorithm to train a more robust and precise classifier for predicting the unlabeled target data. The effectiveness of our proposed method is demonstrated on various benchmark datasets for object recognition and human activity recognition. Moreover, we also show that our proposed method can be treated as an effective complement to the deep learning based unsupervised domain adaptation. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107536	10.1016/j.patcog.2020.107536													
J								Geometric rectification of document images using adversarial gated unwarping network	PATTERN RECOGNITION										Distorted document image; Geometric rectification; Gated module; Deep learning	SIMILARITY; SHAPE; TEXT	Document images captured in natural scenes with a hand-held camera often suffer from geometric distortions and cluttered backgrounds. In this paper, we propose a simple yet efficient deep model named Adversarial Gated Unwarping Network (AGUN) to rectify these images. In this model, the rectification task is recast as a dense grid prediction problem. We thereby develop a pyramid encoder-decoder architecture to predict the unwarping grid at multiple resolutions in a coarse-to-fine fashion. Based on the observation that the structural visual cues, e.g., text-lines, text blocks, lines in tables, which are critical for the estimation of unwarping mapping, are non-uniformly distributed in the images, three gated modules are introduced to guide the network focusing on these informative cues rather than other interferences such as blank areas and complex backgrounds. To generate more visually pleasing rectification results, we further adopt adversarial training mechanism to implicitly constrain the unwarping grid estimation. Our model can rectify arbitrarily distorted document images with complicated page layouts and cluttered backgrounds. Experiments on the public benchmark dataset and the synthetic dataset demonstrate that our approach outperforms the state-of-the-art methods in terms of OCR accuracy and several widely used quantitative evaluation metrics. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107576	10.1016/j.patcog.2020.107576													
J								Fast and incremental algorithms for exponential semi-supervised discriminant embedding	PATTERN RECOGNITION										Semi-supervised discriminant embedding (SDE); Local discriminant embedding (LDE); Exponential semi-supervised discriminant embedding (ESDE); Small-sample-size problem (SSS); Incremental algorithm; Dimensionality reduction	PERTURBATION BOUNDS; MATRIX	In various pattern classification problems, semi-supervised learning methods have shown its effectiveness in utilizing unlabeled data to yield better performance than some supervised and unsupervised learning methods. Semi-supervised discriminant embedding (SDE) is a semi-supervised extension of local discriminant embedding (LDE). However, when dealing with high dimensional data, SDE often suffers from the small-sample-size (SSS) problem. In order to settle this problem, an exponential semi-supervised discriminant embedding (ESDE) method was proposed in [F. Dornaika, Y. EI Traboulsi. Matrix exponential based semi-supervised discriminant embedding for image classification, Pattern Recognition, 61 (2017): 92-103], which makes use of the tool of matrix exponential. Despite its high discriminative ability, the computational overhead of ESDE is very large for high dimensional data. In order to cure this drawback, the first contribution of this paper is to propose a fast implementation on the ESDE method. The key is to equivalently transform the large matrix problem of size d into a much smaller one of size n, where d is the data dimension and n is the number of training samples, with d >> n in practice. On the other hand, in many real world applications, it is likely that whole labeled training set is unavailable beforehand, and the training data is obtained incrementally. Many incremental semi-supervised learning methods have been proposed to deal with this problem, to the best of our knowledge, however, there are no incremental algorithms for matrix exponential discriminant methods till now. To fill in this gap, the second contribution of this paper is to propose incremental ESDE algorithms for incremental learning problems. Numerical experiments on some real-world data sets show the numerical behavior of the proposed algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107530	10.1016/j.patcog.2020.107530													
J								On unsupervised simultaneous kernel learning and data clustering	PATTERN RECOGNITION										Clustering; Matrix factorization; Correlation analysis; Kernel learning	NONNEGATIVE MATRIX FACTORIZATION; ALGORITHMS	A novel optimization framework for joint unsupervised clustering and kernel learning is derived. Sparse nonnegative matrix factorization of kernel covariance matrices is utilized to categorize data according to their information content. It is demonstrated that a pertinent kernel covariance matrix for clustering can be constructed such that it is block diagonal within arbitrary row and column permutations, while each diagonal block has rank one. To achieve this, a linear combination of a dictionary of kernels is sought such that it has rank equal to the number of clusters while a certain kernel eigenvalue is maximized by a novel difference of convex functions formulation. We establish that the proposed algorithm converges to a stationary solution. Numerical tests with different datasets demonstrate the effectiveness of the proposed scheme whose performance is very close to supervised methods, and performs better than unsupervised alternatives without the need of painstaking parameter tuning. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107518	10.1016/j.patcog.2020.107518													
J								One-vs-One classification for deep neural networks	PATTERN RECOGNITION										Deep learning; Computer vision; Multi-class classification; One-vs-One classification; Plant recognition	MULTICLASS; CLASSIFIERS; STRATEGY	For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture, we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme. (C) 2020 The Authors. Published by Elsevier Ltd.																	0031-3203	1873-5142				DEC	2020	108								107528	10.1016/j.patcog.2020.107528													
J								HCNN-PSI: A hybrid CNN with partial semantic information for space target recognition	PATTERN RECOGNITION										Space target recognition; Hybrid convolutional neural network with partial semantic information; Data augmentation; Components segment	NETWORKS	Space target recognition is the basic task of space situational awareness and has developed significantly in the last decade. This paper proposes a hybrid convolutional neural network with partial semantic information for space target recognition, which joints the global features and partial semantic information. Firstly, we propose a two-stage target detection network based on the characteristics of deep space targets. Secondly, we use the Mask R-CNN to segment the main components of the detected satellite. Thirdly, the recognized target and the segmented components are sent to the hybrid extractor to train the hybrid network. What we have done is to find the proper weights of the partial semantic information that plays different importance. The loss function of the hybrid network integrates the global-based and component-based loss with different weights. In comparison with several sets of comparative experiments, the proposed method has achieved a satisfactory result. Besides, we have simulated some real space target images by data processing and achieved a competitive performance in both the simulated dataset and the public dataset. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107531	10.1016/j.patcog.2020.107531													
J								Accurate, data-efficient, unconstrained text recognition with convolutional neural networks	PATTERN RECOGNITION										Text recognition; Optical character recognition; Handwriting recognition; CAPTCHA Solving; License plate recognition; Convolutional neural network; Deep learning	SCENE TEXT; LSTM	Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state-of-the-art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR, License Plate Recognition, and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset. (C) 2020 Published by Elsevier Ltd.																	0031-3203	1873-5142				DEC	2020	108								107482	10.1016/j.patcog.2020.107482													
J								Rich heterogeneous information preserving network representation learning	PATTERN RECOGNITION										Network representation learning; Heterogeneous information; Autoencoder		Network representation learning has attracted increasing attention recently due to its applicability in network analysis. However, most existing network representation learning models only focus on preserving fragmentary aspects of network information, either node proximities or fixed semantic information. In this paper, we propose a novel algorithm named Rich Heterogeneous Information Preserving Network Representation Learning (HIRL), which integrates the high-order proximity among nodes and semantic information into a generic framework by exploiting a flexible autoencoder network. Based on the proposed algorithm, we can explore the hidden information in heterogeneous information networks through any custom form of path schema, and represents different types of nodes in a continuous and common vector space. Moreover, the proposed HIRL is applicable to homogeneous information networks. Extensive experimental results demonstrate that our approach can effectively preserve the information in networks under various path schemas, and performs better on real-world applications such as network reconstruction, link prediction, and node classification compared with the state-of-the-art methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107564	10.1016/j.patcog.2020.107564													
J								Cross-modal knowledge reasoning for knowledge-based visual question answering	PATTERN RECOGNITION										Cross-modal knowledge reasoning; Multimodal knowledge graphs; Compositional reasoning module; Knowledge-based visual question answering; Explainable reasoning		Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information. To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a Graph-based Read, Update, and Control (GRUC) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments. The source code is available at: https://github.com/astro-zihao/gruc (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107563	10.1016/j.patcog.2020.107563													
J								SCUT-HCCDoc: A new benchmark dataset of handwritten Chinese text in unconstrained camera-captured documents	PATTERN RECOGNITION										Document analysis and recognition; Handwritten Chinese text recognition; Handwritten Chinese text detection; Benchmark dataset	RECOGNITION; SEQUENCE; NETWORK; ONLINE; DATABASE	In this paper, we introduce a large-scale dataset, called SCUT-HCCDoc, to address challenging detection and recognition problems of handwritten Chinese text (HCT) in the camera-captured documents. Despite extensive studies of optical character recognition (OCR) and offline handwriting recognition for document images, text detection and recognition in the camera-captured documents remains an unsolved problem that is worth for extensive study and investigation. With recent advances in deep learning, researchers have proposed useful architectures for feature learning, detection, and recognition for the scene text. However, the performance of deep learning methods highly depends on the amount and diversity of training data. Previous OCR and offline HCT datasets were built under specific constraints, and most of the recent scene text datasets are for non-handwritten text. Hence, there is a lack of a comprehensive scene handwritten text benchmark. This study focuses on scenes with handwritten Chinese text. We introduce the SCUT-HCCDoc database for HCT detection, recognition and spotting. SCUT-HCCDoc contains 12,253 camera-captured document images with 116,629 text lines and 1,155,801 characters. The diversity of SCUT-HCCDoc can be described at three levels: (1) image-level diversity : image appearance and geometric variances caused by camera-captured settings (such as perspective, background, and resolution) and different applications (such as note-taking, test papers, and homework); (2) text-level diversity : variances of text line length, rotation, etc.; (3) character-level diversity : variances of character categories (up to 6109 classes with additional English letters, and digits), character size, individual writing style, etc. Three kinds of baseline experiments were conducted, where we used several popular text detection methods for text line detection, CTC-based/attention-based methods for text line recognition, and combine text detectors with CTC-based recognizer to achieve end-to-end text spotting. The results indicate the diversity of SCUT-HCCDoc and the challenges of HCT understanding in document images. The dataset is available at https://github.com/HCIILAB/SCUT-HCCDoc_Dataset_Release . (c) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107559	10.1016/j.patcog.2020.107559													
J								Online kernel classification with adjustable bandwidth using control-based learning approach	PATTERN RECOGNITION										Online classification; Kernel learning; Adaptive learning; Adjustable bandwidth; Control-based approach		In this paper, a novel control-based kernel learning approach is proposed for inferring online binary classification tasks. Following a carefully designed alternating optimization scheme, the learning problems are transformed into two optimal feedback control problems for a series of linear, controllable systems. Model parameters including weights and kernel bandwidth can be efficiently updated by solving the control problems. These consequently lead to our control-based adaptive online kernel classification algorithm (CAOKC). The bandwidth, although nonlinear in our model, can still be updated accurately after linearization. Thus, compared with the existing benchmark algorithms with fixed kernels, the CAOKC algorithm is able to achieve a more adaptive, robust classification performance with better prediction accuracy by regarding the bandwidth as an adjustable parameter. The results presented in this paper also demonstrate how optimal control can provide novel insights and be an effective approach for addressing various learning tasks. Numerical results on benchmark synthetic and realistic datasets are provided to illustrate our method. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107566	10.1016/j.patcog.2020.107566													
J								Few-shot activity recognition with cross-modal memory network	PATTERN RECOGNITION										Few-shot learning; Activity recognition; Cross-modal memory		Deep learning based action recognition methods require large amount of labelled training data. However, labelling large-scale video data is time consuming and tedious. In this paper, we consider a more challenging few-shot action recognition problem where the training samples are few and rare. To solve this problem, memory network has been designed to use an external memory to remember the experience learned in training and then apply it to few-shot prediction during testing. However, existing memory-based methods just update the visual information with fixed label embeddings in the memory, which cannot adapt well to novel activities during testing. To alleviate the issue, we propose a novel end-to-end cross-modal memory network for few-shot activity recognition. Specifically, the proposed memory architecture stores the dynamic visual and textual semantics for some high-level attributes related to human activities. And the learned memory can provide effective multi-modal information for new activity recognition in the testing stage. Extensive experimental results on two video datasets, including HMDB51 and UCF101, indicate that our method could achieve significant improvements over other previous methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				DEC	2020	108								107348	10.1016/j.patcog.2020.107348													
J								Targeted evidence collection for uncertain supplier selection	EXPERT SYSTEMS WITH APPLICATIONS										Supply chain management; Supplier selection; Uncertainty; Multi-criteria decision analysis; Multi-objective optimization	TOTAL QUANTITY DISCOUNT; ACTIVATION COSTS; RISK; DEMAND; MODEL; AHP	The problem of selecting which suppliers, and how much of different items to order from each, involves multiple, often conflicting, criteria such as costs and delivery times. Within real world multi-criteria supplier selection problems there is inherent uncertainty involved, and consideration of its impacts and mitigation is a current and important research direction going forward within the field of supplier selection. Uncertainty within multi-criteria supplier selection may be in relation to (i) a decision maker's ambiguous preferences, such as the importance between criteria, (ii) the suppliers' supply capacities of, and demand for, different items, and (iii) known information about suppliers with respect to the set of criteria, such as each supplier's delivery times or their average defect ratios. Whilst previous work has explored the first two of these, less work has explored uncertainty pertaining to information about suppliers in terms of the criteria and, specifically, how it could be efficiently reduced. Such uncertainty is an important problem to address, as it may have a large impact upon an order regarding its perceived quality compared to its realised quality, so reducing such uncertainty can have a significant impact. This paper presents a Targeted Evidence Collection (TEC) approach for efficient reduction of uncertainty, pertaining to suppliers, by looking to efficiently collect additional evidence. The approach looks to utilise and gather evidence intelligently and dynamically - by considering both the likelihood that each supplier will be part of a solution, along with a decision maker's preferences between criteria - to reduce the uncertainty and efficaciously move towards the most appropriate solution given no uncertainty. The approach is able to handle scenarios for which there are both certain and uncertain criteria present, and can take into account any number of criteria. The TEC strategy is evaluated against alternative approaches, including an active learning based approach, for varying numbers of uncertain criteria, numbers of suppliers, and variations in a decision maker's preferences. The experimentation highlights how TEC efficiently reduces uncertainty, relating to information about suppliers with respect to the set of criteria, requiring up to three times less evidence than its competitors. In this way, TEC helps to effectively mitigate the uncertainty's adverse effects, and reduce the risks inherent within a supplier selection problem. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113583	10.1016/j.eswa.2020.113583													
J								A hybrid artificial neural network, genetic algorithm and column generation heuristic for minimizing makespan in manual order picking operations	EXPERT SYSTEMS WITH APPLICATIONS										Order picking; Order batching; Column generation; Artificial neural networks; Genetic algorithm	VARIABLE NEIGHBORHOOD SEARCH; TABU SEARCH; DISTRIBUTION CENTERS; MULTIPLE PICKERS; ROUTING PROBLEM; WAVE PICKING; WAREHOUSE; OPTIMIZATION; SYSTEM; SOLVE	At an operational level, order picking is the main activity in fulfillment centers. Motivated by and through collaboration with a third party logistic company, this study presents a novel hybrid column generation (CG), genetic algorithm (GA), and artificial neural network (ANN) heuristic for minimizing makespan in manual order picking operations. The results of column generation heuristic is compared against a mixed integer programming model solved by Gurobi, and a parallel simulated annealing and ant colony optimization (PSA-ACO) previously proposed in the literature. Through numerical experiments, the superiority of CG heuristic compared to other methods is shown, and some managerial insights regarding the relationship between makespan optimization, workload balance, picking capacity, and number of pickers in order picking operations is presented. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113566	10.1016/j.eswa.2020.113566													
J								Pixel sampling by clustering	EXPERT SYSTEMS WITH APPLICATIONS										Data sampling; Image segmentation; Clustering; Texture classification	IMAGE SEGMENTATION; LARGE DATASETS; SCALE; SHIFT	In this paper, we describe Pixel Sampling Clustering Technique (PSCT), a data-driven sampling procedure used to reduce pixel sets. We view the pixels in an image as a high redundancy 3D space. We also refer to this space as our color model. Our method aims to retain a relevant sample of the data so it can act as a new smaller, hence more efficient, color model. PSCT uses a pair of fast density-based clustering algorithms in tandem. First, it applies Birch and then DBSCAN to keep the most densely represented colors. We cluster the resulting color model and use the labels to segment images. We also complement the sampling method with a refinement algorithm intended to improve color representation. In our paper, we show how to reconstruct images using our reduced color model. We also show that reconstructed images have enough information to perform image related learning tasks with almost the same accuracy than the original images but with only a small fraction of the data. We test our sampling method in three image related supervised and unsupervised tasks and compare them with state-of-the-art methods. For our experiments, we use two image datasets: MIT's Vision Texture Dataset and Berkeley's BSD500. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113576	10.1016/j.eswa.2020.113576													
J								Grape detection with convolutional neural networks	EXPERT SYSTEMS WITH APPLICATIONS										Machine learning; Deep learning; Agriculture; Viticulture	RECOGNITION; BRANCHES	Convolutional neural networks, as a type of deep learning approach, have revolutionized the field of computer vision and pattern recognition through state of the art performance in a large number of classification tasks. Machine learning has been recently incorporated into intelligent systems related to agricultural and food production to decrease manual processing when dealing with large number of operations. Feedforward artificial neural networks such as convolutional neural networks can be used in agriculture for the segmentation and classification of images containing objects of interests such as fruits, or leaves. It is however unknown what is the best architecture to use, if it is necessary to propose new architectures, and what is the impact of the input feature space on the classification performance. In this paper, we propose to detect two types of grapes (Albarino white grapes and Barbera red grapes) in images. We investigate 1) the impact of the input feature space: color images, grayscale images, and color histograms using convolutional neural networks; 2) the impact of the parameters such as the size of the blocks, and the impact of data augmentation; 3) the performance of 11 pre-trained deep learning architectures, i.e. using a transfer learning approach for the classification. The results support the conclusion that images of grapes can be efficiently segmented using different feature spaces where color images provide the best performance. With convolutional neural networks using transfer learning, the best performance is achieved with Resnet networks reaching an accuracy of 99% for both red and white grapes. Finally, data augmentation, image normalization, and the input feature space have a key impact on the overall performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113588	10.1016/j.eswa.2020.113588													
J								Fake news detection using an ensemble learning model based on Self-Adaptive Harmony Search algorithms	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Fake news; Natural language processing; Harmony search algorithm		In general, the features of fake news are almost the same as those of real news, so it is not easy to identify them. In this paper, we propose a fake news detection system using a deep learning model. First, news articles are preprocessed and analyzed based on different training models. Then, an ensemble learning model combining four different models called embedding LSTM, depth LSTM, LIWC CNN, and N-gram CNN is proposed for fake news detection. Besides, to achieve higher accuracy in fake news detection, the optimized weights of the ensemble learning model are determined using the Self-Adaptive Harmony Search (SAHS) algorithm. In the experiments, we verify that the proposed model is superior to the state-of-the-art methods, with the highest accuracy of 99.4%. Furthermore, we also investigate the cross-domain intractability issue and achieve the highest accuracy of 72.3%. Finally, we believe there is still room for improving the ensemble learning model in addressing the cross-domain intractability issue. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113584	10.1016/j.eswa.2020.113584													
J								Classification of non-small cell lung cancer using one-dimensional convolutional neural network	EXPERT SYSTEMS WITH APPLICATIONS										Lung cancer; Deep learning; Staging; Grading	INFORMATION; ALGORITHMS; NODULES; LEVEL	Non-Small Cell Lung Cancer (NSCLC) is a major lung cancer type. Proper diagnosis depends mainly on tumor staging and grading. Pathological prognosis often faces problems because of the limited availability of tissue samples. Machine learning methods may play a vital role in such cases. 2D or 3D Deep Neural Networks (DNNs) has been the predominant technology in this domain. Contemporary studies tried to classify NSCLC tumors as benign or malignant. The application of 1D CNN in automated staging and grading of NSCLC is not very frequent. The aim of the present study is to develop a 1 D CNN model for automated staging and grading of NSCLC. The updated NSCLC Radiogenomics Collection from The Cancer Imaging Archive (TCIA) was used in the study. The segmented tumor images were fed into a hybrid feature detection and extraction model (MSER-SURF). The extracted features were clubbed with the clinical TNM stage and histopathological grade information and fed into the 1 D CNN model. The performance of the proposed CNN model was satisfactory. The accuracy and ROC-AUC score were higher than the other leading machine learning methods. The study also did well compared to state-of-the-art studies. The proposed model shows that 1D CNN is equally useful in NSCLC prediction like a conventional 2D/3D CNN model. The model may further be refined by carrying out experiments with varied hyper-parameters. Further studies may be conducted by considering semi-supervised or unsupervised learning techniques. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113564	10.1016/j.eswa.2020.113564													
J								A multi-kernel method of measuring adaptive similarity for spectral clustering	EXPERT SYSTEMS WITH APPLICATIONS										Adaptive neighbors; Multi-kernel; Reproducing kernel Hilbert space; Similarity measure; Spectral clustering	KERNEL; CUTS	Accurate representation of similarities between data points is an important determinant in the success of many clustering approaches. Previous studies have shown that kernel methods are effective in this representation. However, using multi-kernels to merge probabilistic neighborhoods in data still needs further research. In this paper, a multi-kernel method of measuring adaptive similarity for spectral clustering is proposed. Kernels with more accurate adaptive similarity measure on the data are assigned bigger weights and an optimum combined kernel that truly reflects the internal structure of the data points is obtained. The proposed method ascribes adaptive and optimal neighbors to each data point based on the local structure using the combined kernels. The combined similarity measure and data clustering are learnt simultaneously to obtain optimal clustering results. We rank constraint the Laplacian matrix of the data similarity matrix to ensure that the connected components in the similarity matrix are exactly equal to the cluster number. The presented technique is significant in the sense that it is able to search the underlying similarity relationships amongst data points and is robust to complex data. Compared with other state-of-the-art spectral clustering methods, our proposed method achieves better performance in terms of NMI and accuracy in experiments performed on both synthetic and real datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113570	10.1016/j.eswa.2020.113570													
J								Remote tracking of Parkinson's Disease progression using ensembles of Deep Belief Network and Self-Organizing Map	EXPERT SYSTEMS WITH APPLICATIONS										Deep Learning; Parkinson's Disease; Clustering; Unified Parkinson's Disease Rating Scale; Predictive Accuracy	HYBRID INTELLIGENT SYSTEM; HEALTH-CARE; PREDICTION; DIAGNOSIS; CLASSIFICATION; FEATURES	Parkinson's Disease (PD) is one of the most prevalent neurological disorders characterized by impairment of motor function. Early diagnosis of PD is important for initial treatment. This paper presents a newly developed method for application in remote tracking of PD progression. The method is based on deep learning and clustering approaches. Specifically, we use the Deep Belief Network (DBN) and Support Vector Regression (SVR) to predict Unified Parkinson's Disease Rating Scale (UPDRS). The DBN prediction models were developed by different epoch numbers. We use a clustering approach, namely, Self Organizing Map (SOM), to improve the accuracy and scalability of prediction. We evaluate our method on a real-world PD dataset. In all, nine clusters were detected from the data with the best SOM map quality for clustering, and for each cluster, a DBN was developed with a specific number of epochs. The results of the DBN prediction models were integrated by the SVR technique. Further, we compare our work with other supervised learning techniques, SVR and Neuro-Fuzzy techniques. The results revealed that the hybrid of clustering and DBN with the aid of SVR for an ensemble of the DBN outputs can make relatively better predictions of Total-UPDRS and Motor-UPDRS than other learning techniques. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113562	10.1016/j.eswa.2020.113562													
J								Enhanced Crow Search Algorithm for Feature Selection	EXPERT SYSTEMS WITH APPLICATIONS										Feature selection; Crow search algorithm (CSA); Metaheuristic; Dynamic local neighborhood; Adaptive awareness probability	ARTIFICIAL BEE COLONY; PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZATION; SINE-COSINE ALGORITHM; DIFFERENTIAL EVOLUTION; SCHEME	The crow search algorithm (CSA) is a recent metaheuristic inspired by the intelligent group behavior of crows. It has attracted the attention of many researchers because of its simplicity and easy implementation. However, it suffers from premature convergence because of its ability to balance between exploration and exploitation is weak. Therefore, we investigate in this paper, an enhanced version of CSA called by us ECSA as a wrapper feature selection method to extract the best feature subsets. This enhancement achieved by introducing three modifications to the original CSA to improve its performance. Firstly, we propose an adaptive awareness probability to enhance the balance between exploration and exploitation. Secondly, we replace the random choice of the crow to follow by the dynamic local neighborhood to guide the local search. Thirdly, we introduce a novel global search strategy to increase the global exploration capability of the crow. The performance of ECSA is measured using three performance metrics and statistical significance over 16 datasets from the UCI repository. The obtained results are compared with those of the original CSA and some state-of-the-art techniques in the literature. Experimental results showed that ECSA presents a better convergence speed and a better-quality solution. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113572	10.1016/j.eswa.2020.113572													
J								The impact of stock market price Fourier transform analysis on the Gated Recurrent Unit classifier model	EXPERT SYSTEMS WITH APPLICATIONS										Limit Order Book; Recurrent Neural Networks; Technical indicators; Fourier transform	PREDICTING STOCK; DEEP; INVESTMENT; SYSTEM	In this paper, we suggest new feature extraction models based on the stock market price signal analysis. In particular, we study the behavior observed in signals originating from different sources, such as prices of different Limit Order Book levels and open, close, low, high prices of the preselected time intervals. We apply Fourier transformation to extract new features. Moreover, we evaluate if the performance of the model based on the Gated Recurrent Unit (GRU) architecture is improved when we select features utilizing the proposed methods. Furthermore, we benchmark the performance of new indicators on the GRU model and provide quantified results proving the significant performance improvement obtained by incorporating the suggested features. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0957-4174	1873-6793				NOV 30	2020	159								113565	10.1016/j.eswa.2020.113565													
J								A deep neural network approach towards real-time on-branch fruit recognition for precision horticulture	EXPERT SYSTEMS WITH APPLICATIONS										Precision horticulture; Fruit recognition; Deep CNN; Global average pooling; Classification	LAND-COVER; CLASSIFICATION; COLOR; AGRICULTURE; VISION; RGB	Real-time and accurate on-branch fruit recognition in an uncontrolled/unstructured environment of orchards could facilitate Precision Horticulture (PH) practices. These practices which are based on the site-specific or variety-specific treatment of an orchard include applications like remote recognition of tree species, variety-specific orchard agrochemical applications, orchard yield mapping, robotic fruit picking, fruit tree disease treatment, etc. For this purpose, in the current work a Convolutional Neural Network (CNN) was developed and optimized for fruit recognition based on RGB images. The images from six classes of on-branch fruits i.e. green apples, nectarine, apricot, peach, sour cherry, and amber-colored plums were captured from local orchards at Semnan province, Iran. To avoid over-fitting, the dataset was then augmented to create more training data from existing samples. The model consisted of multiple convolutional, max-pooling, Global Average Pooling (GAP), and fully connected layers. Using GAP instead of the Flatten layer improved the performance of the model by increasing the accuracy as well as significantly reducing the trainable parameters (about 65 times reduction). The optimization phase of the model development was performed by testing four optimizers (RMSprop, SGD, Adam, and Nadam) with three batch sizes (16, 32, and 64) each with 50 epochs. Accordingly, Nadam optimizer with batch size = 32 demonstrated the best results. The best configuration achieved the accuracy of 99.8% and the cross-entropy loss of 0.019 for the test dataset. This result shows that the model is well developed and has good generalization. This reflects the potential of the method for the remote recognition and classification of different varieties of fruits in an orchard regardless of the environmental effects like complex background, variable light, overlaps, and occlusions with other plant parts, etc. The proposed network was also compared with popular structures like VGG11, ResNet50, ResNet152, and YOLOv3. The processing time of this model was about 8 ms per image while it was 351 ms for ResNet152, proving that the proposed network is much better for real-time applications. Consequently, this study presents a robust method for fulfilling the requirements of a PH practice in a high-tech horticulture system. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113594	10.1016/j.eswa.2020.113594													
J								A comparative evaluation of unsupervised deep architectures for intrusion detection in sequential data streams	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Intrusion detection; Anomaly detection; Real-world data	ANOMALY DETECTION; LEARNING APPROACH	Cybersecurity data remains a challenge for the machine learning community as the high volume of traffic makes it difficult to properly disambiguate anomalous from normal behaviour. That decision is the core of an intelligent Intrusion Detection System (IDS), a component responsible for raising alerts whenever a potential threat is detected. However, with high volume data in contemporary systems, these IDSs generate numerous alerts, too large for human operators to exhaustively investigate. Moreover, simply reporting a single possible threat is often not sufficient, since the security analyst has to investigate the alert without any further clues of the underlying cause. In order to combat these issues, we empirically compare popular deep neural learning architectures for the problem of intrusion detection in sequential data streams. Contrary to a majority of research studies, we do not take a classification-based approach that requires labeled examples of hostile attacks. Instead, we adopt an unsupervised anomaly detection approach that aims to model a benign sequential data distribution against which new test instances are compared to. We also examine one additional deep network in the form of an attention model capable of providing explanations in addition to its predictions; such information is of crucial importance to network operators since it provides additional guidance to resolve potential threats. For our experiments, we evaluate the models against a variety of data sets of different complexities, ranging from simple unidimensional (synthetic and Yahoo!) to more complex multi-source (CICIDS2017 and small real-world enterprise network) data streams. In order to facilitate end-user needs, we focus on ranking-based metrics for comparing different deep neural architectures. This evaluation is especially important for security analysts to prioritize their anomaly investigations. Overall, our experiments demonstrate that a variant of a recurrent neural network generally outperforms a popular non-sequential deep autoencoder commonly used for unsupervised anomaly detection. The attentional model did not provide sufficiently good performance and explanations that we discuss in our analysis. Nonetheless, given that the global financial outlays for cybersecurity are calculated in trillions of dollars, our evaluation and identification of the top-performing RNN architectures for anomaly detection in sequential data streams can lead to improved intelligent IDS design, while our contributions of attentional explanation will hopefully lay the foundations for future improvements to the explanatory capability of these intelligent learning-based IDSs. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113577	10.1016/j.eswa.2020.113577													
J								An interactive preference-guided firefly algorithm for personalized tourist itineraries	EXPERT SYSTEMS WITH APPLICATIONS										Interactive multi-objective optimization; Preference disaggregation; Personalized itinerary recommendation; Firefly algorithm	EVOLUTIONARY MULTIOBJECTIVE OPTIMIZATION; LOCAL SEARCH ALGORITHM; TRIP DESIGN; DECOMPOSITION; SET; DOMINANCE; SYSTEM; NUMBER	The present research proposes an interactive optimization framework to aid tourists to organize their trip by generating personalized walking itineraries among several Points of Interest (POIs). The solution of the multi-objective Prize-Collecting Vehicle Routing Problem (MO-PCVRP) is used to simulate this tourist trip design problem. The objectives of the proposed formulation are the minimization of the total distance walked among selected POIs, the minimization of a fixed cost related to the number of the created itineraries, and the maximization of the total satisfaction gained by visiting the selected POIs. The optimization of the MO-PCVRP is conducted by the proposed Preference-Guided Firefly Algorithm (PGFA), which allows for preferences articulated by a decision-maker (DM) to guide the search. The PGFA is incorporated into an interactive framework, where a DM provides his/her preferential information, progressively during the optimization process, by ranking a small representative set of Pareto optimal solutions. The DM's articulated preferences are elicited utilizing a preference disaggregation method, the UTASTAR, which results in a preference model, which is ultimately used to guide the search towards the DM's Region of Interest (ROI) in the Pareto front. The effectiveness and robustness of the proposed interactive PGFA framework are demonstrated over experimental scenarios. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113563	10.1016/j.eswa.2020.113563													
J								An analytical system for evaluating academia units based on metrics provided by academic social network	EXPERT SYSTEMS WITH APPLICATIONS										Analytical system; Comparative analysis; Academic social network; Web scraping; University evaluation; ResearchGate	GOOGLE SCHOLAR; MEDIA; UNIVERSITIES; IMPACT; BRAND	Social networks are becoming more and more popular, not only among young people looking for entertainment, but also among specialists, experts and researchers who wish to establish professional networks, develop business or research projects. They may be useful also for the comparison and evaluation of scientists and research organizations. This study aims to show how to build a framework of an analytical system for evaluation of researchers and research units using the data retrieved from an academic social network. Acquired data are used to find out the main differences between ResarchGate (RG) usage and values of metrics owned by scientists of different gender, scientific title and field of study to find out if various groups of employees can be directly compared. The authors apply web scraping technique for collecting data from university web page (2847 employees) and use R scripts to acquire the metrics form RG portal. Also, data of 1497 researchers and teaching workers from 11 faculties at Polish university were explored. The descriptive statistics, Chi square test, ANOVA and logistic regression were used to analyse the main RG metrics: RG Score, number of publications, reads and citations. Analysis shows the significant differences both in terms of popularity of ResearchGate and values of its main metrics. The research confirmed that 1) the rvest package allows for fast data acquisition from RG, 2) RG metrics can be used by university managers to compare achievements and progress of single researchers, research labs, departments or faculties, 3) Researchers employed at the faculties of formal and natural sciences use RG portal more frequently, possess higher values of RG metrics, therefore different types of workers and various branches of science shouldn't be compared directly. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113608	10.1016/j.eswa.2020.113608													
J								A novel path planning approach for smart cargo ships based on anisotropic fast marching	EXPERT SYSTEMS WITH APPLICATIONS										Path planning; Smart ships; Fast marching method; Anisotropic fast marching method; Navigation brain system (NBS)	SURFACE VEHICLE FORMATIONS; VISCOSITY SOLUTIONS; ALGORITHMS; SIMULATION; MODEL	Path planning is an essential tool for smart cargo ships that navigate in coastal waters, inland waters or other crowded waters. These ships require expert and intelligent systems to plan safe paths in order to avoid collision with both static and dynamic obstacles. This research proposes a novel path planning approach based on the anisotropic fast marching (FM) method to specifically assist with safe operations in complex marine navigation environments. A repulsive force field is specially produced to describe the safe area distribution surrounding obstacles based on the knowledge of human. In addition, a joint potential field is created to evaluate the travel cost and a gradient descent method is used to search for appropriate paths from the start point to the end point. Meanwhile, the approach can be used to constantly optimize the paths with the help of the expert knowledge in collision avoidance. Particularly, the approach is validated and evaluated through simulations. The obtained results show that it is capable of providing a reasonable and smooth path in a crowded waters. Moreover, the ability of this approach exhibits a significant contribution to the development of expert and intelligent systems in autonomous collision avoidance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113558	10.1016/j.eswa.2020.113558													
J								A new prediction method for recommendation system based on sampling reconstruction of signal on graph	EXPERT SYSTEMS WITH APPLICATIONS										Recommendation system; Recommendation technology; Signal processing on graph; Reproducing kernel Hilbert space	REGULARIZATION; ALGORITHM	Recommendation technology is widely used in various e-commerce platforms. Accurately predicting user's preference is the most important goal of recommendation technology. One of the core difficulties of recommendation technology that the rating matrices are seriously sparse. However, the unknown entries in the rating matrix actually contain a lot of useful information for prediction, which are usually discarded in traditional methods. Based on the idea of semi-supervised learning, this paper models the recommendation problem as a signal reconstruction problem on a graph. The new model utilizes both the information of the unlabeled samples and the location information, and thus achieves an excellent predictive performance. Meanwhile, to reduce the computational complexity a strategy is designed skillfully to approximately solve the model. Experimental results shows that the proposed method significantly outperforms the reference methods in predictive accuracy and is robust to the diversity of data sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113587	10.1016/j.eswa.2020.113587													
J								Granular fuzzy pay-off method for real option valuation	EXPERT SYSTEMS WITH APPLICATIONS										Real options; Fuzzy pay-off method; Principle of justifiable granularity; Information granules; Granular computing	GROUP DECISION-MAKING; RESEARCH-AND-DEVELOPMENT; LINGUISTIC INFORMATION; DEVELOPMENT PROJECTS; PORTFOLIO SELECTION; CONSENSUS MODEL; SYSTEMS; OPTIMIZATION	In the last decade, the fuzzy pay-off method has emerged as a widely used alternative approach for real option valuation thanks to its simplicity that makes it easily approachable by practitioners from various application domains. In this study, a new direction for real option valuation is pursued by proposing a granular fuzzy pay-off method. We motivate the proposal by discussing how the extension of the original approach with granular representation can further improve the fuzzy pay-off method. The design of the granular fuzzy pay-off method is founded on the principle of justifiable granularity, which in turn relies on numeric data to build information granules that are semantically sound and experimentally justified. To illustrate the method, a case study in R&D investment in manufacturing is worked out. The extended granular fuzzy pay-off method improves performance and usability in cases with uncertainty. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113597	10.1016/j.eswa.2020.113597													
J								Vague set theory based segmented image fusion technique for analysis of anatomical and functional images	EXPERT SYSTEMS WITH APPLICATIONS										Multimodal image fusion; VS theory based segmentation; Root mean square value of local energy; Local information entropy; Local contrast index; Local standard deviation		The present study focuses on the design of feature based efficient fusion scheme using multiscale shift invariant shearlet transform. For enhancing the obscured but spectacular detailing of brain MRI which is necessary for analyzing the affected tissues, a flexible approach of vague set (VS) theory based segmentation method has been proposed. The existence of non null hesitation in VS theory makes it appropriate for modeling the high degrees of uncertainties/impreciseness presents in MRI. Hence the proposed segmentation method efficiently captures the salient features and fine structures of MRI and makes the irrelevant artifacts smooth. As the relevant local information related to energy activity level, dominant textural variation, spatial structures belonging to edges/contours, likelihood of neighboring contrast distribution, signal complexity are important to produce the fused image, various indices such as root mean square value of local energy (RMSLE), local information entropy (LIE), local contrast index (LCI) and local standard deviation (LSTD) of subband coefficients are captured by placing a 3 x 3 kernel. In this view, principle component analysis (PCA) approach is addressed to produce a composite subband (CSb) image carrying all those local information. Finally, the fused image is constructed based on the strength of CSb for all source images. Experimental results show that the proposed fusion approach is able to integrate utmost information content of the source images and preserve color saliency very efficiently. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113592	10.1016/j.eswa.2020.113592													
J								A novel deep learning framework: Prediction and analysis of financial time series using CEEMD and LSTM	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Long short-term memory; Complementary ensemble empirical mode decomposition; Financial time series; Stock market forecasting; Principal component analysis	EMPIRICAL MODE DECOMPOSITION; CONVOLUTIONAL NEURAL-NETWORKS; TERM-MEMORY NETWORKS; STOCK-PRICE; INDEX; MACHINE; VOLATILITY; CLASSIFICATION; INFORMATION; MARKETS	Deep learning is well-known for extracting high-level abstract features from a large amount of raw data without relying on prior knowledge, which is potentially attractive in forecasting financial time series. Long short-term memory (LSTM) networks are deemed as state-of-the-art techniques in sequence learning, which are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We propose a novel methodology of deep learning prediction, and based on this, construct a deep learning hybrid prediction model for stock markets-CEEMD-PCA-LSTM. In this model, complementary ensemble empirical mode decomposition (CEEMD), as a sequence smoothing and decomposition module, can decompose the fluctuations or trends of different scales of time series step by step, generating a series of intrinsic mode functions (IMFs) with different characteristic scales. Then, with retaining the most of information on raw data, PCA reduces dimension of the decomposed IMFs component, eliminating the redundant information and improving prediction response speed. After that, high-level abstract features are separately fed into LSTM networks to predict closing price of the next trading day for each component. Finally, synthesizing the predicted values of individual components is utilized to obtain a final predicted value. The empirical results of six representative stock indices from three types of markets indicate that our proposed model outperforms benchmark models in terms of predictive accuracy, i.e., lower test error and higher directional symmetry. Leveraging key research findings, we perform trading simulations to validate that the proposed model outperforms benchmark models in both absolute profitability performance and risk-adjusted profitability performance. Furthermore, model robustness test unveils the more stable robustness compared to benchmark models. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 30	2020	159								113609	10.1016/j.eswa.2020.113609													
J								Identity-sensitive loss guided and instance feature boosted deep embedding for person search	NEUROCOMPUTING										Person search; Feature embedding; Loss function	REIDENTIFICATION; NETWORK	Person search aims at detecting and re-identifying pedestrians from whole monitoring images, which is vital for intelligent surveillance. However, this task is still challenging due to the extremely few instances per training identity and inherent fine-grained differences among different identities. To this end, this work proposes an identity-sensitive loss guided and instance feature boosted pipeline to extract deep discriminative feature embedding for person search. First, a prior anchor pre-trained network (PAPN) is designed to obtain proper initial state for the whole deep person search training baseline. Second, a new loss function called instance enhancing loss (IEL) is proposed to learn identity-sensitive features by introducing unlabeled identity information. Specifically, the proposed IEL can selectively utilize unlabeled identities with similar appearances to labeled identities to train the person search network. Third, considering the intra-class compactness of features learned by center loss and contextual inter-class relations, two instance boosting strategies (Boosting) are used to learn more discriminative features. Extensive experiments on two benchmark datasets, namely CUHK-SYSU and PRW, demonstrate the effectiveness of our approach. (C) 2020 The Author(s). Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 20	2020	415						1	14		10.1016/j.neucom.2020.07.062													
J								Co-occurrence graph based hierarchical neural networks for keyphrase generation	NEUROCOMPUTING										Co-occurrence graph; Graph attention networks; Keyphrase generation; Phrase-level representations; Hierarchical recurrent neural networks		More and more attention has been paid to automatic keyphrase generation as it facilitates a wide variety of downstream AI applications, such as information retrieval, text summarization and opinion mining. Although sequence-to-sequence architecture with attention and copy mechanisms (CopyNet) to this task shows promising results, it still suffered from the following shortcomings: (i) it only encodes the keyphrase (usually consists of several words) in word level which can not adequately capture the overall meaning of keyphrase; (ii) it lacks a suitable way to model the correlation among different keyphrases which is very helpful for generating richer and more comprehensive candidate phrases. To overcome these challenges, a novel keyphrase generation model named Hierarchical CopyNet with graph attention networks (HCopy-GAT) is proposed. Firstly, the Hierarchical Recurrent Encode-Decoder neural network (HRED) is employed to learn the expressive embeddings of keyphrases in both word-level and phrase-level. Secondly, the graph attention neural networks (GAT) is applied to model the correlation among different keyphrases. Furthermore, we developed a new dataset named SOFTWARE, which can be taken as a new testbed for keyword generation tasks. With empirical experiments on several real datasets (including our newly built dataset), the proposed HCopy-GAT model outperforms state-of-the-art keyphrase generation models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						15	26		10.1016/j.neucom.2020.07.084													
J								Unsupervised domain adaptation with self-attention for post-disaster building damage detection	NEUROCOMPUTING										Unsupervised domain adaptation; Self-attention; Hurricane damage; Damage assessment		Fast assessment of damaged buildings is important for post-disaster rescue operations. Building damage detection leveraging image processing and machine learning techniques has become a popular research focus in recent years. Although supervised learning approaches have made considerable improvement for damaged building assessment, rapidly deploying supervised classification is still difficult due to the complexity in obtaining a large number of labeled samples in the aftermath of disasters. We propose an unsupervised self-attention domain adaptation (USADA) model, which transforms instances of the source domain to those of the target domain in pixel space, to address the aforementioned issue. The proposed USADA consists of three parts: a set of generative adversarial networks (GANs), a classifier, and a self-attention module. The GAN adapts source domain images to ensure their similarity to target domain images. Once adapted, the classifier can be trained using the adapted images along with the original images of the source domain to classify damaged buildings. The self-attention module is introduced to maintain the foreground of the generated images conditioned on source domain images for generating plausible samples. As a case study, aerial images of Hurricane Sandy, Maria, and Irma, are used as the source and target domain datasets in our experiments. Experimental results demonstrate that classification accuracies of 68.1% and 84.1% are achieved, and our method obtains improvements of 2.0% and 3.6% against pixel-level domain adaptation, which is the basis of our model. (C) 2020 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 20	2020	415						27	39		10.1016/j.neucom.2020.07.005													
J								Diagonal rotor Hopfield neural networks	NEUROCOMPUTING										Complex-valued Hopfield neural networks; Multistate neuron; Rotor Hopfield neural networks; Linear algebra	ASSOCIATIVE MEMORY; STORAGE CAPACITY	A complex-valued Hopfield neural network (CHNN) is a multistate Hopfield model and has been applied to the storage of image data. It has the weak noise tolerance due to the inherent property of rotational invariance. A hyperbolic-valued Hopfield neural network (HHNN) resolves rotational invariance and improves the noise tolerance. A rotor Hopfield neural network (RHNN) is an extension of CHNN and the weights are represented by matrices. It provides excellent noise tolerance by resolving the rotational invariance. However, an RHNN needs double weight parameters of a CHNN, unlike an HHNN. In this work, we propose a diagonal RHNN (DRHNN), which restricts the weights of RHNN to diagonal matrices and reduces the number of weight parameters. The number of weight parameters in a DRHNN, an HHNN, and a CHNN is same. In addition, a DRHNN resolves the rotational invariance and provides excellent noise tolerance like an RHNN. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						40	47		10.1016/j.neucom.2020.07.041													
J								Self-supervised data augmentation for person re-identification	NEUROCOMPUTING										Person re-identification; Self-supervised learning; Data augmentation; Generative adversarial network		Although person re-identification (ReID) has been intensively studied over the past few years, the shortage of annotated training data stands as an obstacle for further performance improvement. To address this issue, many data augmentation methods have been successfully applied to person ReID, such as random scaling, flipping and cropping, which mainly operate on a single image, whilst overlooking the relationship between images. Recently, generative adversarial networks (GANs) have been widely used for data augmentation and smoothly migrated to person ReID. However, the cost of training GAN is expensive and the performance improvement is often limited. Moreover, all these methods focus on augmenting samples for existing IDs. This paper proposes a simple yet effective data augmentation strategy based on self-supervised learning to handle these problems, which consists of the offline ID augmentation that can generate new categories and the online instance augmentation. These two components are integrated i nto a unified framework for boosting the ReID performance. Furthermore, a novel Siamese-like network is developed for ReID in conjunction with the proposed data augmentation method. Extensive experiments on three benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art by a clear margin, which verifies the robustness and the effectiveness of our method. Code will be released at: https://github.com/flychen321/data_aug_reid. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						48	59		10.1016/j.neucom.2020.07.087													
J								E2PAMEA: A fast evolutionary algorithm for extracting fuzzy emerging patterns in big data environments	NEUROCOMPUTING										Emerging pattern mining; Evolutionary fuzzy systems; Multi-objective evolutionary algorithm; Big data	DESCRIPTIVE RULE DISCOVERY; SUBGROUP DISCOVERY; FEATURE-SELECTION; SYSTEMS; MAPREDUCE; METAHEURISTICS; OPTIMIZATION; PERFORMANCE; PREDICTION; ANALYTICS	In this paper, a cooperative-competitive multi-objective evolutionary fuzzy system called E2PAMEA is presented for the extraction of emerging patterns in big data environments. E2PAMEA follows an adaptive schema to automatically employ different genetic operators according to the learning needs, which avoid the tuning of some parameters. It also employs a token-competition-based procedure for updating an elite population where the best set of patterns found so far is stored. In addition, a novel MapReduce procedure for an efficient computation of the evaluation function employed for guiding the search process is proposed. The method, called Bit-LUT employs a pre-evaluation stage where data is represented as a look-up table made of bit sets. This look-up table can be employed later in the chromosome evaluation by means of bitwise operations, reducing the computational complexity of the process. The experimental study carried out shows that E2PAMEA is a promising alternative for the extraction of high-quality emerging patterns in big data. In addition, the proposed Bit-LUT evaluation shows a significant improvement on efficiency with a great scalability capacity on both dimensions of data, which enables the processing of massive datasets faster than other alternatives. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						60	73		10.1016/j.neucom.2020.07.007													
J								Quasi fixed-time synchronization of memristive Cohen-Grossberg neural networks with reaction-diffusion	NEUROCOMPUTING										Quasi fixed-time synchronization; Memristor; Reaction-diffusion; Cohen-Grossberg neural networks	FINITE-TIME; VARYING DELAYS; EXPONENTIAL SYNCHRONIZATION; STABILITY THEOREM; STABILIZATION; DISSIPATIVITY; TERMS	This paper focuses on the quasi-fixed time synchronization of memristive Cohen-Grossberg neural networks with reaction diffusion. First, a new definition of finite/fixed-time attraction set and the concept of quasi finite/fixed-time synchronization are proposed. Then, a quasi fixed-time synchronized lemma is given and an example is used to illustrate the correctness of the lemma. At the same time, the new results given in this article are compared with the existing ones under certain conditions, and the conclusions of this paper are less conservative in remark. In addition, quasi fixed-time synchronization theorem and full synchronization theorem are derived by designing different controllers, finally two examples are given to illustrate the validity of these theorems. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						74	83		10.1016/j.neucom.2020.07.071													
J								Heterogeneous teaching evaluation network based offline course recommendation with graph learning and tensor factorization	NEUROCOMPUTING										Offline course recommendation; Tensor factorization; Teaching evaluation network; Rating prediction; Personalized recommendation; e-learning	MATRIX FACTORIZATION; PREDICTION-APPROACH; SYSTEM; ONTOLOGY	Course recommendation systems are applied to help students with different needs select courses in a large range of course resources. However, a student's needs are not always determined by their personal interests, they are also influenced by teachers, peers etc. Unlike online courses, user behavior and user satisfaction of offline courses often have serious sparse and cold start issues, which cause overfitting problems in previous neural network and matrix factorization (MF) models. Additionally, the interpersonal relations, evaluation text and existing "user-item" formatted rating matrix constitute a multi-source and multi-modal data structure, so a systematic data fusion method is needed to establish recommendations based on these heterogeneous characteristics. Therefore, a hybrid recommendation model by fusing network structured feature with graph neural networks and user interactive activities with tensor factorization was proposed in this paper. First, a graph structured teaching evaluation network is proposed to describe students, courses, and other entities by using the students' rating, commentary text, grading and interpersonal relations. Then, a random walk based neural network is employed to generate the vectorized representation of students by learning their own relational structure. Finally, by recognizing these personalization features as the third dimension of the rating tensor, a Bayesian Probabilistic Tensor Factorization-based tensor factorization is applied to learn and predict students' ratings for classes they have not taken. Experiments on a real-world evaluation of teaching system including 532 participants with 7,453 rating records show that the proposed method outperforms other existing neural network and matrix factorization models including xSVD++, RTTF and DSE with a smaller predictive error as well as better recommendation accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						84	95		10.1016/j.neucom.2020.07.064													
J								Unsupervised domain adaptation with structural attribute learning networks	NEUROCOMPUTING										Domain adaptation; Deep learning; Attribute learning; Graph convolutional networks		Domain adaptation aims at improving the performance on an unknown target domain by transferring the knowledge learned from a related source domain. In this paper, we propose a structural attribute learning network (SAL Net) which can learn transferable domain-invariance features based on the attribute learning. Our proposed SAL Net can learn both the deep visual features which describe the appearance of objects and the semantic attribute features which are robust to the domain shift. To promote the adaptation performance, we construct a structural graph of the visual and semantic attributes by a graph convolutional network(GCN). Our structural attribute learning framework not only learns the domain-invariant attribute features but also extracts the relationship of the features. We perform a set of comparative experiments on the standard domain adaptation benchmarks. The results demonstrate that our proposed method outperforms the previous adaptation methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						96	105		10.1016/j.neucom.2020.07.054													
J								Adaptive multi-teacher multi-level knowledge distillation	NEUROCOMPUTING										Knowledge distillation; Adaptive learning; Multi-teacher		Knowledge distillation (KD) is an effective learning paradigm for improving the performance of light-weight student networks by utilizing additional supervision knowledge distilled from teacher networks. Most pioneering studies either learn from only a single teacher in their distillation learning methods, neglecting the potential that a student can learn from multiple teachers simultaneously, or simply treat each teacher to be equally important, unable to reveal the different importance of teachers for specific examples. To bridge this gap, we propose a novel adaptive multi-teacher multi-level knowledge distillation learning framework (AMTML-KD), which consists two novel insights: (i) associating each teacher with a latent representation to adaptively learn instance-level teacher importance weights which are leveraged for acquiring integrated soft-targets (high-level knowledge) and (ii) enabling the intermediate-level hints (intermediate-level knowledge) to be gathered from multiple teachers by the proposed multi-group hint strategy. As such, a student model can learn multi-level knowledge from multiple teachers through AMTML-KD. Extensive results on publicly available datasets demonstrate the proposed learning framework ensures student to achieve improved performance than strong competitors. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						106	113		10.1016/j.neucom.2020.07.048													
J								Asymmetric CycleGAN for image-to-image translations with uneven complexities	NEUROCOMPUTING										Unpaired Image Translation; CycleGAN; Asymmetric Translation; Average Image Entropy; Edge-retain Prior		CycleGAN is one of the famous and basic methods for unpaired image-to-image translation tasks. Inspired by the experiments of the NIR-RGB translation, which is a kind of translation where images are translated from simple to complex or vice versa, we concluded the definition of asymmetric translation task. Because of the complexity difference between two domains, the complexity inequality in bidirectional translations is significant. We analyzed and witnessed the limitation of the original CycleGAN in asymmetric translation tasks and proposed an Asymmetric CycleGAN model with generators of unequal sizes to adapt to the asymmetric need in asymmetric translations. An empirical metric was also given to determine the asymmetric task from the aspect of image entropy and could be treated as the auxiliary guidance to design the asymmetric generators. Besides, the edge-retain loss between the input and the generated images was introduced to enhance the structural visual quality. Residual-block-net based and U-net based generators were both applied here to verify the Asymmetric CycleGAN. The performance of different depth of generators for Asymmetric CycleGAN was also discussed on the basis of experiments. The qualitative visual evaluation demonstrated that our model had achieved great improvements compared to original CycleGAN. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						114	122		10.1016/j.neucom.2020.07.044													
J								Finite-time distributed cooperative control for heterogeneous nonlinear multi-agent systems with unknown input constraints	NEUROCOMPUTING										Finite-time control; Heterogeneous nonlinear multi-agent systems; Nonaffine pure-feedback form; Input constrains	ADAPTIVE NEURAL-CONTROL; BARRIER LYAPUNOV FUNCTIONS; TRACKING CONTROL; OUTPUT REGULATION; CONSENSUS; STATE; SYNCHRONIZATION; NETWORKS; AGENTS	In this paper, the distributed cooperative control problem of heterogeneous nonlinear multi-agent systems (HNMASs) which are of pure-feedback forms coupled with nonaffine dynamics is considered with momentous emphasis on finite-time convergence. Multiple kinds of input constraints are also considered and can be completely unknown. To achieve the dynamic tracking for the heterogeneous followers with unknown input constrains, there are two consensus theorems are proposed, one of which can be utilized to achieve finite-time convergence. Meanwhile, it is the blend of fraction power feedback method and neural network (NN) technique that let the controlled systems have the finite-time characteristic. In the proposed tracking control protocols, only the local position information from neighbors or leader is required. In the end, the effectiveness of two proposed consensus algorithms is confirmed and compared by numerical simulation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						123	134		10.1016/j.neucom.2020.06.089													
J								Event-triggered reinforcement learning control for the quadrotor UAV with actuator saturation	NEUROCOMPUTING										Quadrotor; UAV; Reinforcement learning; Flight control; Event-triggered control; Actuator saturation	TRACKING CONTROL	This paper proposes an event-triggered reinforcement learning (RL) control strategy to stabilize the quadrotor unmanned aerial vehicle (UAV) with actuator saturation. As the quadrotor UAV equips with a complex dynamic is difficult to be model accurately, a model free reinforcement learning scheme is designed. Due to the practical limitation of actuators, the end of controller is constrained with a bounded function. In order to reduce the calculation consumption for the onboard computer, an event-triggered mechanism is developed, which only update the controller when the triggered condition is satisfied. The proposed controller is implemented with two neural networks which are called critic and actor. Some advanced RL technologies are utilized for speeding up the train process, e.g. off-policy training, experience replay, etc. The stability of closed-loop system is proved by the Lyapunov analysis. The simulation results including a stability task and a tracking task verify the theoretical analysis, in which we find the updating frequency of controller is decreased greatly. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						135	145		10.1016/j.neucom.2020.07.042													
J								Occluded offline handwritten Chinese character inpainting via generative adversarial network and self-attention mechanism	NEUROCOMPUTING										Self-attention mechanism; Generative adversarial network; Occluded offline handwritten Chinese character inpainting		Occluded offline handwritten Chinese characters inpainting is a critical step for handwritten Chinese characters recognition. We propose to apply generative adversarial network and self-attention mechanism to inpaint occluded offline handwritten Chinese characters. First, cyclic loss is used to guarantee the cyclic consistency of the uncorrupted area between corrupted images and original real images instead of masks. Second, self-attention mechanism is combined with generative adversarial network to increase receptive field and explore more Chinese character features. Then an improved character-VGG-19 that is pre-trained with handwritten Chinese character dataset is used to calculate content loss to extract character features more effectively and assist generator to generate realistic characters. Finally, adversarial classification loss is used to make our discriminator classify input images instead of just distinguishing real images from fake images in order to learn the distribution of Chinese characters more effectively. The proposed method is evaluated on an occluded CASIA-HWDB1.1 dataset for three challenging inpainting tasks with different portions of blocks, or pixels randomly missing, or pixels randomly adding. Experimental results show that our method is more effective, compared with several state-of-the-art handwritten Chinese character inpainting methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						146	156		10.1016/j.neucom.2020.07.046													
J								Event-triggered adaptive consensus tracking control for nonlinear switching multi-agent systems	NEUROCOMPUTING										Multi-agent systems; Adaptive control; Consensus tracking; Event-triggered control	STABILITY	This paper focuses on a class of nonlinear switching multi-agent systems about event-triggered adaptive tracking control. Compared with existing results, the event-triggered mechanism is first employed in nonlinear switched multi-agent systems with unknown parameters, and it can reduce the amount of computation and the frequency of the controller updates. By utilizing the backstepping technology, adaptive laws are designed to analyze the unknown parameters. In the sequel, all signals of multi-agent systems are bounded, the tracking errors converge to a small neighborhood of the origin. Finally, simulation results verify the effectiveness of the proposed scheme. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						157	164		10.1016/j.neucom.2020.07.032													
J								Learning sequential and general interests via a joint neural model for session-based recommendation	NEUROCOMPUTING										Session-based recommendation; Behavior modeling; Neural network; Deep learning; Interests Extraction		Session-based recommendation is to predict the next action of a user based on his current behavior session. Recent studies have focused on designing recurrent neural networks to capture representation of users sequential behaviors from a large number of anonymous sessions. Although these neural models have greatly improved recommendation performance compared with traditional approaches, they have overemphasized the sequential information of sessions, that is, the order of items in each browsing session. We argue that besides the order of items, the set of items in each session can also reflect the general interests of current session and can also be exploited to augment recommendations. To this end, this paper proposes a joint neural model, called SGINM, for jointly learning sequential and general interests of each session for session-based recommendation. In SGINM, we design an attentive recurrent neural network which not only captures self-adaptive weighed representation of each subsequence in a session but also learns a global representation for the whole session. Furthermore, the SGINM adopts a multilayer neural network with residual connections to learn the general interests of each session. The SGINM uses a softmax layer to jointly decode the two types of interests and output a ranking vector of recommended items for each new session. The proposed model has been extensively experimented with the benchmark datasets from the RecSys Challenge 2015 and CIKM Cup 2016. Compared with the stateof-the-art models, the proposed SGINM achieves better recommendation performance in terms of higher predicative accuracy and mean reciprocal rank in almost all experiment cases. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						165	173		10.1016/j.neucom.2020.07.039													
J								Design and Application of A Robust Zeroing Neural Network to Kinematical Resolution of Redundant Manipulators Under Various External Disturbances	NEUROCOMPUTING										Zeroing neural network (ZNN); Redundant resolution; Finite-time convergence; Robustness; Robot manipulator; Additive noise	FINITE-TIME CONVERGENCE; DYNAMICS; EQUATION; MOTION; FORMULA	Zeroing neural network (ZNN), as an effective solution tool for time-varying problems, has been widely applied to the kinematical resolution of redundant manipulators. Most of work is devoted to modifying the convergence speed of ZNN in an ideal environment. However, external disturbances are unavoidable in practical solution process of ZNN for the kinematical resolution of redundant manipulators. In this paper, by proposing a robust ZNN (RZNN) model for real-time kinematical resolution, redundant manipulators are capable of achieving path-tracking tasks with high accuracy, even in a noise-disturbed environment. The theoretical analyses about robustness and finite-time convergence are provided to guarantee the superior property of the RZNN model. A numerical example abstracted from the nonlinear mapping of redundant manipulators is first given to demonstrate the effectiveness and advantages of the RZNN model under various external disturbances, by comparing existing two ZNN models. Then, two multi-link robot manipulator-tracking examples are presented to show the applicability of the RZNN model in a noise-disturbed environment, while the existing ZNN model is no longer effective in the same condition. Comparative results validate the effectiveness, high accuracy, and advantages of the RZNN model for the kinematical resolution of redundant manipulators even in a noise-disturbed environment. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						174	183		10.1016/j.neucom.2020.07.040													
J								Quasi-projective synchronization of stochastic complex-valued neural networks with time-varying delay and mismatched parameters	NEUROCOMPUTING										Projective synchronization; Complex-valued neural networks; Stochastic; Parameter mismatch; Time-varying delay	TO-STATE STABILITY; EXPONENTIAL SYNCHRONIZATION	This paper aims to study the issue of stochastic quasi-projective synchronization of complex-valued neural networks with time-varying delays in the case of the simultaneous existence of parameter mismatch and stochastic disturbance. Instead of separating the real and imaginary parts, by constructing a suitable controller, a quasi-projective synchronization criterion of two nonidentical systems is established by combining the complex version It's formula with some inequality techniques in the framework of complex domain. Meanwhile, the bound of synchronization error is estimated. Some sufficient conditions for special cases are also presented. Finally, the effectiveness of the obtained results are verified by a numerical example. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						184	192		10.1016/j.neucom.2020.07.033													
J								RegiNet: Gradient guided multispectral image registration using convolutional neural networks	NEUROCOMPUTING										Image registration; Convolutional neural network; Deep learning; Multimodal; Multispectral	MUTUAL-INFORMATION	Multispectral image registration suffers from severe inconsistency between reference and target images. In this paper, we propose gradient guided multispectral image registration using convolutional neural networks, called RegiNet. We build an end-to-end network that directly produces the registration result from the input image pair. We use a gradient map of the reference image to guide the target image for registration. RegiNet first encodes the reference image and the gradient map of the target image separately, and then concatenates them to register the target image. For loss function, we use a structure loss to effectively capture gradient information from the reference image. Experimental results demonstrate that the proposed method successfully produces registration results as well as outperforms state-of-the-art ones in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						193	200		10.1016/j.neucom.2020.07.066													
J								High-speed rail pole number recognition through deep representation and temporal redundancy	NEUROCOMPUTING										Region-based convolutional neural network; Context information; Object detection; Number recognition; Pole number; High-speed rail	LICENSE PLATE RECOGNITION; EXTRACTION; SEGMENTATION; LOCALIZATION; NETWORKS	Pole number recognition is highly important for the positioning tasks in high-speed rail catenary systems. The complicated working environment poses difficulties for number recognition algorithms, and this situation becomes even more challenging when illumination changes, image blurs and occlusions are considered. In this work, we present a high-accuracy pole number recognition framework including a high-performance cascaded CNN-based Detection and Recognition YOLO (DR-YOLO) and a temporal redundancy approach. First, the DR-YOLO utilizes global features for coarse plate detection and local features for accurate number recognition. Next, context-based combination of adjacent frames utilizes the complementarity and consistency of the same pole number recognition results and generates a unique result. The context-based amendment of adjacent poles utilizes the continuity of adjacent poles and corrects the fault or partial missing results caused by blur or occlusions. Extensive experimental testing is performed on 4 datasets representing high-speed train routine working environments. The reported experimental results validate the effectiveness and efficiency of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						201	214		10.1016/j.neucom.2020.07.086													
J								Optimizing echo state network through a novel fisher maximization based stochastic gradient descent	NEUROCOMPUTING										Echo state network; Hyperparameter optimization; Stochastic gradient descent	OPTIMIZATION; INFORMATION; PROPERTY; SIGNALS	Hyperparameter optimization is a challenging process that has the potential to improve machine learning algorithms. Since it creates a remarkable computational burden for machine learning tasks, there have been few works coping with tuning strategies of a specific algorithm. In this paper, an improved Stochastic Gradient Descent (SGD) based on Fisher Maximization is developed for tuning hyperparameters of an Echo State Network (ESN) which has a wide range of applications. The results of the method are then compared with those of traditional Gradient Descent and Grid Search. According to the obtained results; 1) The scale of the data sets greatly affects the reliability of hyperparameter optimization results; 2) Feature selection is critical in terms of mean error of training when hyperparameter optimization is applied on some methods such as ESN; 3) SGD falls in a good local minima if Fisher Maximization is performed to find a good starting point. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						215	224		10.1016/j.neucom.2020.07.034													
J								EEG-based intention recognition with deep recurrent-convolution neural network: Performance and channel selection by Grad-CAM	NEUROCOMPUTING										EEG; Grad-CAM; Deep recurrent-convolution neural network; Performance; Channel selection; Intention recognition	QUANTITATIVE-ANALYSIS; BCI; CLASSIFICATION; COMMUNICATION	Electroencephalography (EEG) based Brain-Computer Interface (BCI) enables subjects to communicate with the outside world or control equipment using brain signals without passing through muscles and nerves. Many researchers in recent years have studied the non-invasive BCI systems. However, the efficiency of the intention decoding algorithm is affected by the random non-stationary and low signal-to-noise ratio characteristics of the EEG signal. Furthermore, channel selection is another important issue in BCI systems intention recognition. During intention recognition in BCI systems, the unnecessary information produced by redundant electrodes affects the decoding rate and deplete system resources. In this paper, we introduce a recurrent-convolution neural network model for intention recognition by learning decomposed spatio-temporal representations. We apply the novel Gradient-Class Activation Mapping (Grad-CAM) visualization technology to the channel selection. Grad-CAM uses the gradient of any classification, flowing into the last convolutional layer to produce a coarse localization map. Since the pixels of the localization map correspond to the spatial regions where the electrodes are placed, we select the channels that are more important for decision-making. We conduct an experiment using the public motor imagery EEG dataset EEGMMIDB. The experimental results demonstrate that our method achieves an accuracy of 97.36% at the full channel, outperforming many state-of-the-art models and baseline models. Although the decoding rate of our model is the same as the best model compared, our model has fewer parameters with faster training time. After the channel selection, our model maintains the intention decoding performance of 92.31% while reducing the number of channels by nearly half and saving system resources. Our method achieves an optimal trade-off between performance and the number of electrode channels for EEG intention decoding. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						225	233		10.1016/j.neucom.2020.07.072													
J								Finite-time formation-containment tracking for second-order multi-agent systems with a virtual leader of fully unknown input	NEUROCOMPUTING										Time-varying formation; Formation-containment tracking; Finite-time control; Multi-agent systems; Fully unknown input	OUTPUT FORMATION-CONTAINMENT; CONSENSUS CONTROL; SWARM SYSTEMS; DYNAMICS; VEHICLES; DESIGN	This paper studies the finite-time time-varying formation-containment tracking problems for second-order multi-agent systems with directed topology, where the multi-agent system consists of one virtual leader with fully unknown input, several real leaders and followers. In contrast with traditional formation and containment cases studied in most of the existing literature, apart from accomplishing the expected time-varying formation, the real leaders are required to track the trajectory generated by the virtual leader, and the followers need to get into the convex envelope spanned by the real leaders in finite time. The method for solving the finite-time formation-containment tracking problems is divided into the following steps. First, distributed finite-time observers are designed for each real leader and follower, in order to obtain the estimated states of their neighbor agents. Then, formation-containment tracking protocols are presented by using the neighboring information for both real leaders and followers. Furthermore, based on Lyapunov theory, it is proved that the given formation-containment tracking can be realized in finite time by the multi-agent system under the designed protocols. Finally, a case study on multi-robot transport operation is given to verify the effectiveness of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						234	246		10.1016/j.neucom.2020.07.067													
J								Community detection method using improved density peak clustering and nonnegative matrix factorization	NEUROCOMPUTING										Community detection; Nonnegative matrix factorization; Density peak clustering; Random walk; Amendatory PageRank	FAST SEARCH; ALGORITHM; NETWORKS; MODEL; FIND	Community detection in networks is valuable in analyzing, designing, and optimizing complex network. Recently, the nonnegative matrix factorization (NMF) method has successfully uncovered the community structure in the complex networks. However, most of community detection methods based on NMF require the number of the community as a prior information. To address this problem, in this paper, we use the improved density peak clustering (DPC) to obtain the number of centers as the preassigned parameter for nonnegative matrix factorization. The proposed algorithm first calculates the modified PageRank of nodes as the density indexes and then draws a decision graph to obtain the hubs of the network. By means of Markov chain model of a random walk, we execute NMF on one expansion feature matrix. Finally, we compare and analyze the performance of different algorithms on artificial networks and real-world networks. Experimental results indicate that the proposed method is superior to the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						247	257		10.1016/j.neucom.2020.07.080													
J								New optimal method for L-2-L-infinity state estimation of delayed neural networks	NEUROCOMPUTING										Neural networks; Time-varying delay; L-2-L-infinity state estimation; Conservatism	TIME-VARYING DELAYS; IMPROVED STABILITY-CRITERIA; H-INFINITY; SYSTEMS	This proposal considers the issue of L-2-L-infinity state estimation for delayed neural networks. A novel Lyapunov-Krasovskii functional is constructed, which contains augmented s-dependent integral and multiple integral terms. Then, in order to cooperate with the constructed Lyapunov-Krasovskii functional effectively to reduce the conservatism of the result, generalized free-weighting-matrix integral inequality and convex combination method are utilized. Meanwhile, multiple integral terms containing triple integral are utilized when analyzing the L-2-L-infinity performance. As a result, novel delay-dependent conditions are obtained, which can guarantee that the delayed neural network is asymptotically stable with desired performance level. Finally, the given numerical examples show the superiority of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						258	265		10.1016/j.neucom.2020.06.118													
J								A novel quadruple generative adversarial network for semi-supervised categorization of low-resolution images	NEUROCOMPUTING										Generative adversarial networks; Image super-resolution; Image categorization; Semi-supervised learning; Deep learning	SUPERRESOLUTION	In order to make utilization of unlabeled low-resolution (LR) images to shape discriminative models, we present quadruple generative adversarial network (Q-GAN), a game-theoretical framework for implementing semi-supervised categorization of LR images. It can realize photo-realistic image super-resolution (SR) and semi-supervised pattern recognition simultaneously. We consider our pipeline as a four-player optimization-based formulation, which consists of four vital components, i.e., a refiner for image SR and generation, a discriminator for identifying high-resolution (HR) samples and another for identifying true (original) samples, a classifier for label prediction. The refiner and two discriminators characterize the conditional distributions between images and labels, whilst the classifier solely focuses on predicting real image-label pairs. We select those high-quality super-solved images with ground-truth labels for data supplement. We customize the global optimization objective function as well as the training procedure to ensure model approximates the posterior distribution of latent variables given true data in a semi-supervised manner. Experimental results demonstrate that Q-GAN can simultaneously (1) deliver the promising categorization performance among state-of-the-arts, i.e., validation accuracy achieves 92.18% and testing accuracy achieves 90.63%, and (2) recover fine-grained textures with high peak signalto-noise ratios (PNSRs) and structural similarities (SSIMs) from heavily downsampled testing images of hand-crafted dataset and public benchmarks. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 20	2020	415						266	285		10.1016/j.neucom.2020.05.050													
J								Sample complexity of classification with compressed input	NEUROCOMPUTING										Compressed representation; Generalization bound; Information bottleneck	A-PRIORI DISTINCTIONS	One of the most studied problems in machine learning is finding reasonable constraints that guarantee the generalization of a learning algorithm. These constraints are usually expressed as some simplicity assumptions on the target. For instance, in the Vapnik-Chervonenkis (VC) theory the space of possible hypotheses is considered to have a limited VC dimension One way to formulate the simplicity assumption is via information theoretic concepts. In this paper, the constraint on the entropy H(X) of the input variable X is studied as a simplicity assumption. It is proven that the sample complexity to achieve an epsilon-delta Probably Approximately Correct (PAC) hypothesis is bounded by 2(cH(X)/epsilon) +log1/2/alpha epsilon(2) which is sharp up to the 1/epsilon(2) factor (a and c are constants). Moreover, it is shown that if a feature learning process is employed to learn the compressed representation from the dataset, this bound no longer exists. These findings have important implications on the Information Bottleneck (IB) theory which had been utilized to explain the generalization power of Deep Neural Networks (DNNs), but its applicability for this purpose is currently under debate by researchers. In particular, this is a rigorous proof for the previous heuristic that compressed representations are exponentially easier to be learned. However, our analysis pinpoints two factors preventing the IB, in its current form, to be applicable in studying neural networks. Firstly, the exponential dependence of sample complexity on 1/epsilon., which can lead to a dramatic effect on the bounds in practical applications when epsilon is small. Secondly, our analysis reveals that arguments based on input compression are inherently insufficient to explain generalization of methods like DNNs in which the features are also learned using available data. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						286	294		10.1016/j.neucom.2020.07.043													
J								On hyperparameter optimization of machine learning algorithms: Theory and practice	NEUROCOMPUTING										Hyper-parameter optimization; Machine learning; Bayesian optimization; Particle swarm optimization; Genetic algorithm; Grid search	NAIVE BAYES; REGRESSION; SELECTION; SEARCH; RIDGE	Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyperparameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-theart optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						295	316		10.1016/j.neucom.2020.07.061													
J								An adaptive training-less framework for anomaly detection in crowd scenes	NEUROCOMPUTING										Training-less system; Adaptive 3D DCT; RCNN; Saliency guided optical flow; Anomaly detection	EVENT DETECTION	Anomaly detection in crowd videos has become a popular area of research for the computer vision community. Several existing methods have determined anomaly as a deviation from scene normalcy learned via separate training with/without labeled information. However, owing to rare and sparse nature of anomalous events, any such learning can be misleading as there exist no hardcore segregation between anomalous and non-anomalous events. To address such challenge, we propose an adaptive training-less system capable of detecting anomaly on-the-fly. Our solution pipeline consists of three major components, namely, adaptive 3D-DCT model for multi-object detection-based association, local motion descriptor generation through an improved saliency guided optical flow, and anomaly detection based on Earth mover's distance (EMD). The proposed model, despite being training-free, is found to achieve comparable performance with several state-of-the-art methods on publicly available UCSD, UMN, CUHK-Avenue and ShanghaiTech datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						317	331		10.1016/j.neucom.2020.07.058													
J								Hybrid fuzzy integrated convolutional neural network (HFICNN) for similarity feature recognition problem in abnormal netflow detection	NEUROCOMPUTING										Similarity feature recognition; Abnormal netflow detection; Adaptive feature integration convolution neurons (AFICNs); Function mapping label redefinition (FMLR); Fuzzy classification (FC); Inverse function mapping integrated reduction (IFMIR)	FEATURE-SELECTION; LEARNING APPROACH; SYSTEMS; CLASSIFICATION; SEGMENTATION; CONTROLLER; ATTACKS	This paper presents a hybrid fuzzy integrated convolutional neural network (HFICNN) to deal with the similarity feature recognition problem in abnormal netflow detection, which is that different labels with the same netflow eigenvalues are not well disposed by traditional machine learning and deep learning methods. The HFICNN model can convert the same feature individuals with different labels to different feature individuals with different labels by constructing an integration method. Our method recognizes the actual environment and operational complexity, and converts the unique occurrence time feature performance as a new feature for feature recognition. HFICNN integrates time-dependent records into a new record to identify a feature, and converts the integrated records to the original label. The integration process plays a good role and effect in the entire process. HFICNN is realized with the aid of function mapping label redefinition (FMLR), adaptive feature integration convolutional neurons (AFICNs), fuzzy classification (FC), and inverse function mapping integrated reduction (IFMIR). The ICMPv6-based DDoS attacks dataset of a new-generation network is tested, and experimental results show that HFICNN performs better than 10 types of traditional machine learning and two types of deep learning methods on the similarity feature recognition problem, and the HFICNN model is reliable and effective. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						332	346		10.1016/j.neucom.2020.07.076													
J								Information encryption communication system based on the adversarial networks Foundation	NEUROCOMPUTING										Convolutional neural network; Adversarial loss; Encryption system; Chosen ciphertext attack; One Time Pad		We propose an encryption communication system based on the adversarial networks, called Adversar; Network Encryption System (ANES), which contains Sender, Receiver and various Attackers. Using a general loss function, we design three types of ANES with different Attackers, which simulate different password attacks on communications. Through the one-to-one adversarial training between Sender and these all Attackers, ANES can frequently master One Time Pad (OTP) algorithms without human cognition. We find that the more and stronger the opponents ANES encountered, the higher the probability of learning OTP. We also present a more aggressive attack network based on Chosen Ciphertext Attack (CCA) as the antagonist of Sender and Receiver, and finally the system learns OTP with a considerable probability. Experimental results demonstrate that our encryption strategy is comparable to other related works. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						347	357		10.1016/j.neucom.2020.08.041													
J								Towards the representational power of restricted Boltzmann machines	NEUROCOMPUTING										Restricted Boltzmann machines; Representational power; Representational efficiency; Threshold/ReLU neural networks	LEARNING ALGORITHM; DISCRETE; RBM	The restricted Boltzmann machine (RBM), which is a graphical model for binary random variables, has been proven to be a powerful tool in machine learning. However, theoretical foundations for understanding the approximation ability of RBMs are lacking. In this paper, we study the representational power of RBMs, with the focus lying on the sufficient number of hidden units of RBMs required to compute some classes of distributions of interest, with a fixed number of inputs. First, it is constructively shown how RBMs can approximate any distribution that depends on the scalar projection of the inputs onto a given vector up to arbitrary accuracy. Then, for any given distribution, we explore how it can be represented as the form that depends on the scalar projection of the inputs onto some vectors, and then study the properties of these vectors, from which a new proof for the universal approximation theorem of RBMs is deduced. Finally, we investigate the representational efficiency of RBMs by providing a description of all the distributions that can be efficiently computed by RBMs. More specifically, it is shown that a distribution can be computed by a polynomial-size RBM with polynomially bounded parameters, if and only if its mass can be computed by a two-layer feedforward network with threshold/ReLU activation functions, whose size and parameters are polynomially bounded. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						358	367		10.1016/j.neucom.2020.07.090													
J								EEG responses to emotional videos can quantitatively predict big-five personality traits	NEUROCOMPUTING										EEG; Predictive model; Big Five personality; Personality assessment; Emotional videos; Brain-computer interface	MOTIVATIONAL DIRECTION; HUMOR STYLES; EXTROVERSION; METAANALYSIS; SELF; SYNCHRONIZATION; NEUROTICISM; CHILDHOOD; PLEASANT	Recent advances in information technology have suggested the potential possibility to assess an individual's personality automatically. The present study proposed and implemented an EEG-based personality assessment method for quantitative evaluation of people's Big Five personality. EEG data were collected from 66 participants, while they watched a total number of 28 video clips covering 9 typical emotion categories of amusement, joy, inspiration, tenderness, anger, disgust, fear, sadness and neutral. Regression analyses were performed to predict the participants' Big Five personality trait scores using the EEG responses to these emotional video clips. A nested leave-one-out cross-validation procedure was employed with a sparse feature selection strategy to evaluate the out-of-sample personality assessment performance. The established EEG-based regression models could effectively predict the participants' self-reported personality trait scores. The prediction accuracies, measured as the correlations between the EEG-predicted personality trait scores and the self-reported scores, were 0.71, 0.72, 0.86, 0.71, and 0.82 for agreeableness, conscientiousness, neuroticism, openness and extraversion, respectively. A series of tests from both the internal and external validity perspective further showed that a good reliability of the obtained results. These results suggest the proposed method as a promising alternative to conventional personality questionnaires. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						368	381		10.1016/j.neucom.2020.07.123													
J								Composing recipes based on nutrients in food in a machine learning context	NEUROCOMPUTING										Recipe creation; Nutrition; Data mining; Deep learning	MATRIX FACTORIZATION; ALGORITHMS	In the past few decades, numerous studies have demonstrated the effectiveness of diet modification in preventing and controlling different chronic diseases, bringing increased attention to the creation of nutritional recipes. Currently, the development of recipes is primarily based on personal experience rather than nutritional specifications. To this end, this paper first uses kernel canonical correlation analysis to demonstrate the relationship between nutrients in food and recipes. Based on this relationship, a new recipe expression method is put forward, which objectively reflects differing importance of an ingredient in differing recipes. Then, recipes are composed using an auto-encoder in deep learning, and a fusion model of two auto-encoders is proposed for better concocting recipes. This paper uses two machine learning methods, namely, non-negative matrix factorization and two-step regularized least squares, to form recipes. To tackle overfitting and instability in non-negative matrix factorization during the training of recipe model, we introduce the Frobenius norm to redefine the objective function and add non-smooth sparse matrices. Similar food has similar taste, but their nutrients might differ. This paper also considers nutrients as a kernel matrix of the two-step regularized least squares, which can effectively avoid the occurrence of different food combinations with similar taste. Experimental results show that developing recipes based on nutrients in food is feasible and effective in the context of machine learning. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						382	396		10.1016/j.neucom.2020.08.071													
J								Memorized sparse backpropagation	NEUROCOMPUTING										Neural Networks; Backpropagation; Sparse Gradient; Acceleration		Neural network learning is usually time-consuming since backpropagation needs to compute full gradients and backpropagate them across multiple layers. Despite its success of existing works in accelerating propagation through sparseness, the relevant theoretical characteristics remain under-researched and empirical studies found that they suffer from the loss of information contained in unpropagated gradients. To tackle these problems, this paper presents a unified sparse backpropagation framework and provides a detailed analysis of its theoretical characteristics. Analysis reveals that when applied to a multilayer perceptron, our framework essentially performs gradient descent using an estimated gradient similar enough to the true gradient, resulting in convergence in probability under certain conditions. Furthermore, a simple yet effective algorithm named memorized sparse backpropagation (MSBP) is proposed to remedy the problem of information loss by storing unpropagated gradients in memory for learning in the next steps. Experimental results demonstrate that the proposed MSBP is effective to alleviate the information loss in traditional sparse backpropagation while achieving comparable acceleration. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						397	407		10.1016/j.neucom.2020.08.055													
J								InferPy: Probabilistic modeling with deep neural networks made easy	NEUROCOMPUTING										Deep probabilistic modeling; Hierarchical probabilistic models; Neural networks; Bayesian layers; Tensorflow; User-friendly		InferPy is a Python package for probabilistic modeling with deep neural networks. It defines a user-friendly API that trades-off model complexity with ease of use, unlike other libraries whose focus is on dealing with very general probabilistic models at the cost of having a more complex API. In particular, this package allows to define, learn and evaluate general hierarchical probabilistic models containing deep neural networks in a compact and simple way. InferPy is built on top of Tensorflow Probability and Keras. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						408	410		10.1016/j.neucom.2020.07.117													
J								A parallel down-up fusion network for salient object detection in optical remote sensing images	NEUROCOMPUTING										Optical remote sensing images; Salient object detection; Deep learning	OPTIMIZATION	The diverse spatial resolutions, various object types, scales and orientations, and cluttered backgrounds in optical remote sensing images (RSIs) challenge the current salient object detection (SOD) approaches. It is commonly unsatisfactory to directly employ the SOD approaches designed for nature scene images (NSIs) to RSIs. In this paper, we propose a novel Parallel Down-up Fusion network (PDF-Net) for SOD in optical RSIs, which takes full advantage of the in-path low- and high-level features and cross-path multi-resolution features to distinguish diversely scaled salient objects and suppress the cluttered backgrounds. To be specific, keeping a key observation that the salient objects still are salient no matter the resolutions of images are in mind, the PDF-Net takes successive down-sampling to form five parallel paths and perceive scaled salient objects that are commonly existed in optical RSIs. Meanwhile, we adopt the dense connections to take advantage of both low- and high-level information in the same path and build up the relations of cross paths, which explicitly yield strong feature representations. At last, we fuse the multiple-resolution features in parallel paths to combine the benefits of the features with different resolutions, i.e., the high-resolution feature consisting of complete structure and clear details while the low-resolution features highlighting the scaled salient objects. Extensive experiments on the ORSSD dataset demonstrate that the proposed network is superior to the state-of-the-art approaches both qualitatively and quantitatively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 20	2020	415						411	420		10.1016/j.neucom.2020.05.108													
J								Evaluating the feasibility of blockchain in logistics operations: A decision framework	EXPERT SYSTEMS WITH APPLICATIONS										Blockchain; Logistics; Multi-Criteria Decision Making; Intuitionistic Fuzzy AHP; Fuzzy VIKOR	SUPPLY CHAIN; FUZZY VIKOR; HEALTH-CARE; TECHNOLOGY; CHALLENGES; MANAGEMENT; SELECTION; ADOPTION; IMPACT; ROLES	The main purpose of this study is to investigate the feasibility of blockchain technology in logistics industry using a quantitative approach. To this end, a decision framework is proposed based on a multi-criteria decision structure that incorporates AHP into VIKOR under Intuitionistic Fuzzy Theory. This integration presents different solutions and rankings based on different decision-making strategies and also captures uncertainty in the evaluation process. While Intuitionistic Fuzzy AHP calculates the importance weights of the proposed criteria indicated as scalability, privacy, interoperability, audit, latency, visibility, trust, and security, Fuzzy VIKOR ranks the logistics operations demonstrated as materials handling, warehousing, order processing, transportation, packaging, fleet management, labeling, vehicle routing and product returns management. The proposed decision framework was applied in a large-scale logistics company located in Turkey. The findings of this study suggest that while the most important criteria are security, visibility and audit, the most feasible logistics operations proved to be transportation, materials handling, warehousing, order processing and fleet management in a possible blockchain implementation. The decision framework in this study may enable decision makers to evaluate the feasibility of blockchain in logistics operations, which is one of the main research gaps in the current blockchain research. Furthermore, this is the first study that integrates AHP and VIKOR methods under Intuitionistic Fuzzy Theory in the context of blockchain. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113543	10.1016/j.eswa.2020.113543													
J								Fake news detection in multiple platforms and languages	EXPERT SYSTEMS WITH APPLICATIONS										Fake news; Machine learning; Supervised learning	SELECTION	The debate around fake news has grown recently because of the potential harm they can have on different fields, being politics one of the most affected. Due to the amount of news being published every day, several studies in computer science have proposed models using machine learning to detect fake news. However, most of these studies focus on news from one language (mostly English) or rely on characteristics of social media-specific platforms (like Twitter or Sina Weibo). Our work proposes to detect fake news using only text features that can be generated regardless of the source platform and are the most independent of the language as possible. We carried out experiments from five datasets, comprising both texts and social media posts, in three language groups: Germanic, Latin, and Slavic, and got competitive results when compared to benchmarks. We compared the results obtained through a custom set of features and with other popular techniques when dealing with natural language processing, such as bag-of words and Word2Vec. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113503	10.1016/j.eswa.2020.113503													
J								Automatic detection of tuberculosis related abnormalities in Chest X-ray images using hierarchical feature extraction scheme	EXPERT SYSTEMS WITH APPLICATIONS										Tuberculosis; Pneumonia; X-ray images; Lung segmentation; Respiratory illness; Hierarchical classification	LUNG SEGMENTATION; NOISE-REDUCTION; TEXTURE; CLASSIFICATION; COMBINATION; PREDICTION; SYSTEMS	Machine learning techniques have been widely used for abnormality detection in medical images. Chest X-ray images (CXR) are among the non-invasive diagnostic tools used to detect various disease pathologies. The ambiguous anatomical structure of soft tissues is one of the major challenges for segregating normal and abnormal images. The main objective of this study is to mimic the expert radiologist's interpretation procedure in computer-aided diagnosis (CAD) systems. We propose an automatic technique for detection of abnormal CXR images containing one or more pathologies like pleural effusion, infiltration, fibrosis, hila enlargement, dense consolidation, etc. due to tuberculosis (TB). The proposed abnormality detection technique is based on the hierarchical feature extraction scheme in which the features are used in two-level of hierarchy to categorize healthy and unhealthy groups. In level one the handcrafted geometrical features like shape, size, eccentricity, perimeter, etc. and in level 2 traditional first order statistical feature along with texture features like energy, entropy, contrast, correlation, etc. are extracted from segmented lung-fields. Further, a supervised classification approach is employed on the extracted features to detect normal and abnormal CXR images. The performance of the algorithm is validated on a total of 800 CXR images from two public datasets, namely the Montgomery set and Shenzhen set. The obtained results (accuracy = 95.60 +/- 5.07% and area under curve (AUC) = 0.95 +/- 0.06 for Montgomery collection, and accuracy = 99.40 +/- 1.05% and AUC = 0.99 +/- 0.01 for Shenzhen collection) shows the promising performance of the proposed technique for TB detection compared to the existing state of the art approaches. Further, the obtained results are statistically validated using Friedman post-hoc multiple comparison methods, which confirms the significance of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113514	10.1016/j.eswa.2020.113514													
J								An EEG based familiar and unfamiliar person identification and classification system using feature extraction and directed functional brain network	EXPERT SYSTEMS WITH APPLICATIONS										Familiar/unfamiliar person recognition; Electroencephalogram (EEG); Feature extraction; Directed functional connectivity; Brain signal complexity; Classification	FACE RECOGNITION; TRANSFER ENTROPY; NEURAL SYSTEMS; VOICES; CONNECTIVITY; OSCILLATIONS; COMPLEXITY; NAMES	People are extremely proficient at recognizing familiar person, but are much worse at matching unfamiliar one. However, the neural correlation of this proposed difference in neural representations of familiar and unfamiliar identities remains unclear. New methods of EEG data analysis, functional networks and time frequency analyses, are highly recommended to advance the knowledge of those brain mechanisms. Developing an EEG based pattern recognition system could potentially be used to improve the current person recognition strategies. In this study, we designed a multi-channel EEG based pattern recognition system for person recognition. To do this, a new feature extraction method combining directed functional network analysis and signal complexity of EEGs from different brain regions was proposed, which is the main contribution of this paper. The proposed method was tested in an experiment of 20 subjects underlying visual and auditory stimuli simultaneously. The features were calculated in delta, theta, alpha and beta band respectively, then SVM and KNN classifiers were applied to these feature sets and the results showed the recognition accuracies of these four bands are relatively stable with the best accuracy of 90.58% in delta band for SVM. In addition, theta and alpha band also showed good performance for the two classifiers. It indicated delta wave is the best sub band for person perception and SVM is better than KNN in this system. This work is the first time to construct the directed functional network in person recognition study, and it demonstrated the combination of non-linear complexities and network features are efficient for EEG based expert and intelligent system for person recognition. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113448	10.1016/j.eswa.2020.113448													
J								A whale optimization algorithm with chaos mechanism based on quasi-opposition for global optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Whale optimization algorithm; Chaos mechanism; Quasi-opposition based learning; Global optimization problems	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY; INTELLIGENCE; METHODOLOGY	Whale Optimization Algorithm (WOA), as a newly developed meta-heuristic algorithm, performs well in solving optimization problems. A WOA with chaos mechanism based on quasi-opposition (OBCWOA) is proposed in this paper to overcome the slow convergence speed of the original WOA and to avoid being trapped in local optimal solutions when dealing with high-dimensional problems. We applied two strategies to the original WOA: using chaos mechanism to generate initial value to improve convergence speed of the algorithm and using the opposition-based learning method to balance exploration and development ability of the algorithm to help the algorithm jump out of local optimal solutions. The proposed algorithm is compared with other algorithms on unimodal functions, multimodal functions and fixed dimensional multimodal functions, and is applied to a famous engineering design problem. Results show that combination of the two strategies can improve convergence speed and enhance global search ability of the original WOA. OBCWOA proposed in this paper performs better than the other existing algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113612	10.1016/j.eswa.2020.113612													
J								Post Pareto-optimal ranking algorithm for multi-objective optimization using extended angle dominance	EXPERT SYSTEMS WITH APPLICATIONS										Extended angle-based dominance; Knee solutions; Implicit preference; Multi-objective optimization; Solution ranking	PARTICLE SWARM OPTIMIZATION; MANY-OBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHM; SYSTEM OPTIMIZATION; KNEE POINTS; ALLOCATION; SELECTION	This paper presents a solution ranking algorithm to find the outstanding solutions in given set of non-dominated solutions of multi-objective optimization problems, which are the results from either Multi-Objective Evolutionary Algorithms (MOEAs) or exact methods. The algo-rithm enables the decision makers to identify outstanding solutions without a deep understanding of the problem. The algorithm provides a ranking for all solutions so that they can obtain any top K ranked solutions to implement. This novel parameter-free solution ranking approach is based on two concepts: an extended angle-based dominance technique from the algorithm called ADaptive angle-based pruning Algorithm (ADA) for discovering the knee solutions and the inverse-square law of light for enhancing the diversity of solutions. We evaluate the performance of the approach on several well-known test problems against well-known knee finding algorithms as well as on a practical system design and optimization problem to demonstrate the usefulness of the algorithm. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113446	10.1016/j.eswa.2020.113446													
J								Leveraging tacit knowledge for shipyard facility layout selection using fuzzy set theory	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy similarity index; Fuzzy goal programming; Relationship chart; Shipyard layout; Shipbuilding process	ANT COLONY OPTIMIZATION; INNER STRUCTURE WALLS; UNEQUAL-AREA; GENETIC ALGORITHM; HEURISTIC APPROACH; DESIGN; SEARCH; METHODOLOGY; ADJACENCY; FRAMEWORK	A shipyard's layout contributes significantly to manufacturing performance. The practitioners based on experience can provide feasible alternative layouts by considering qualitative factors. However, no study in the shipyard layout context has leveraged their valuable subjective knowledge. This study addresses the above research gap. It proposes a two-stage approach using the Fuzzy similarity index (FSI) and the Fuzzy goal programming model (FGPM). The first stage elicits alternative layouts and relationship (REL) charts from practitioners. REL's fuzzy proximity ratings are assigned by subjective assessment of shipbuilding process flow, inter-shop material flow, distance, cost per unit distance, sharing of material handling equipment. Thereafter, the FSI of each alternative layout with respect to the ideal layout is evaluated. However, implementing the alternative layout with the highest FSI may not be feasible due to practical constraints. Therefore, in the second stage, FGPM is formulated incorporating practical constraints related to site factors, harmful gases emission, environmental, and safety to yield an optimal selection of layout. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				NOV 15	2020	158								113423	10.1016/j.eswa.2020.113423													
J								Semi-supervised learning with generative model for sentiment classification of stock messages	EXPERT SYSTEMS WITH APPLICATIONS										Sentiment analysis; Generative model; Semi-supervised learning; Stock message board	MICROBLOGGING DATA	Classification of investors' sentiments in stock message boards has attracted a great deal of attention. Since the messages are usually short, we propose a semi-supervised learning method to make full use of the features in both train and test messages. The generative emotion model takes message, emotion and words into consideration simultaneously. Based on the facts that words are of different ability in discriminating sentiments, they are categorized into three classes in the model with different emotion strength. Training the generative model can transform the messages into emotion vectors which finally feeds to a sentiment classifier. The experiment results show that the proposed model and learning method are efficient for modeling sentiment in short text, and by properly selecting the amount of train data and the percent of test samples, we can achieve higher classification accuracy than traditional ones. The results indicate that the generative model is effective for short message sentiment classification, and provides a significant approach for the implementation of semi-supervised learning which is a typical expert and intelligent information processing method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113540	10.1016/j.eswa.2020.113540													
J								Bark texture classification using improved local ternary patterns and multilayer neural network	EXPERT SYSTEMS WITH APPLICATIONS										Bark texture classification; Texture analysis; Tree identification; Local ternary patterns; Multi-layer perceptron	BINARY PATTERNS; ROTATION-INVARIANT; SCALE	Tree identification is one of the areas that are regarded by researchers. It is done by human expert with high cost. Experts believe that tree bark has a high relation with species in comparison with other phenotype properties. Repeated textures in the bark is usually various with slight differences. So, lbp-like descriptors used in most recent works. But, most of them do not provide discriminative features. Also some texture descriptors are sensitive to noise and rotation. Local ternary pattern is one of the operators that are resistant to the noise with high discrimination. In most of descriptors, histogram of patterns is used to extract features. But, it is rotation sensitive with high computational complexity. In this paper, the main contribution is to propose a method for bark texture classification with high accuracy based on the improved local ternary patterns (ILTP). In the proposed ILTP, the ternary patterns are coded into two binary patterns, and then each one is classified into two uniform/non-uniform groups. The extracted patterns are labeled according to the degree of uniformity. Finally the occurrence probability of the labels is extracted as features. Also, a multilayer perceptron is designed with four theories in the number of hidden nodes. Experimental results on two benchmark datasets showed that our proposed approach provides higher classification accuracy than most well known methods. Noise-resistant and rotation invariant are other advantages of the presented method. The proposed bark texture classification, because of its high classification accuracy, can be applied in real applications and reduce the financial costs and human risks in the diagnosis of plant species. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113509	10.1016/j.eswa.2020.113509													
J								Understanding the apparent superiority of over-sampling through an analysis of local information for class-imbalanced data	EXPERT SYSTEMS WITH APPLICATIONS										Class imbalance; Sample types; Resampling; Local neighborhood	FEATURE-SELECTION; MINORITY CLASS; ROC CURVE; CLASSIFICATION; PERFORMANCE; SMOTE; IDENTIFICATION; CLASSIFIERS; AREA; ACCURACY	Data plays a key role in the design of expert and intelligent systems and therefore, data preprocessing appears to be a critical step to produce high-quality data and build accurate machine learning models. Over the past decades, increasing attention has been paid towards the issue of class imbalance and this is now a research hotspot in a variety of fields. Although the resampling methods, either by under-sampling the majority class or by over-sampling the minority class, stand among the most powerful techniques to face this problem, their strengths and weaknesses have typically been discussed based only on the class imbalance ratio. However, several questions remain open and need further exploration. For instance, the subtle differences in performance between the over- and under-sampling algorithms are still undercomprehended, and we hypothesize that they could be better explained by analyzing the inner structure of the data sets. Consequently, this paper attempts to investigate and illustrate the effects of the resampling methods on the inner structure of a data set by exploiting local neighborhood information, identifying the sample types in both classes and analyzing their distribution in each resampled set. Experimental results indicate that the resampling methods that produce the highest proportion of safe samples and the lowest proportion of unsafe samples correspond to those with the highest overall performance. The significance of this paper lies in the fact that our findings may contribute to gain a better understanding of how these techniques perform on class-imbalanced data and why over-sampling has been reported to be usually more efficient than under-sampling. The outcomes in this study may have impact on both research and practice in the design of expert and intelligent systems since a priori knowledge about the internal structure of the imbalanced data sets could be incorporated to the learning algorithms. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113026	10.1016/j.eswa.2019.113026													
J								A multi-criteria ratio-based approach for two-stage data envelopment analysis	EXPERT SYSTEMS WITH APPLICATIONS										DEA; Two-stage; Multiple criteria; Discrimination power; Ratio data	MULTIPLE CRITERIA APPROACH; NETWORK DEA APPROACH; EFFICIENCY DECOMPOSITION; ANALYSIS MODELS; PERFORMANCE; SYSTEM; INPUTS	Data Envelopment Analysis (DEA) is a well-known technique for assessing efficiency levels of decision making units (DMUs). Very often, available data may be expressed as ratios and, in such cases, traditional DEA models cannot be applied as long as biased efficiency results are produced, yielding the issues of efficiency underestimation and pseudo-inefficiency. In this paper, a novel two-stage MCDEA-R model to handle ratio data is developed observing three distinct assumptions - black-box, free-link, and fixed-link - offering a multi-criteria decision making (MCDM) perspective to the efficiency assessment problem in productive networks. While the proposed models are tested by evaluating the efficiency levels of a set of 30 bank branches in Iran, their distinctive features are highlighted in terms of previous literature to model ratio data under network structures. Precisely, there were not only gains in terms of mitigating pseudo-inefficiency and lack of discrimination power of weights issues, but there were also actual gains in terms of efficiency reliability as measured by information entropy. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113508	10.1016/j.eswa.2020.113508													
J								T2-FDL: A robust sparse representation method using adaptive type-2 fuzzy dictionary learning for medical image classification	EXPERT SYSTEMS WITH APPLICATIONS										Robust sparse representation; Dictionary learning; Uncertainty; Near-optimal dictionary; Type-2 fuzzy learning; Medical diagnosis	BRAIN-TUMOR SEGMENTATION; K-SVD; DISCRIMINATIVE DICTIONARY; NEURAL-NETWORKS; MR-IMAGES; SYSTEMS; MODEL; IDENTIFICATION; UNCERTAINTY; ALGORITHM	In this paper, a robust sparse representation for medical image classification is proposed based on the adaptive type-2 fuzzy learning (T2-FDL) system. In the proposed method, sparse coding and dictionary learning processes are executed iteratively until a near-optimal dictionary is obtained. The sparse coding step aiming at finding a combination of dictionary atoms to represent the input data efficiently, and the dictionary learning step rigorously adjusts a minimum set of dictionary items. The two-step operation helps create an adaptive sparse representation algorithm by involving the type-2 fuzzy sets in the design process of image classification. Since the existing image measurements are not made under the same conditions and with the same accuracy, the performance of medical diagnosis is always affected by noise and uncertainty. By introducing an adaptive type-2 fuzzy learning method, a better approximation in an environment with higher degrees of uncertainty and noise is achieved. The experiments are executed over two open-access brain tumor magnetic resonance image databases, REMBRANDT and TCGA-LGG, from The Cancer Imaging Archive (TCIA). The experimental results of a brain tumor classification task show that the proposed T2-FDL method can adequately minimize the negative effects of uncertainty in the input images. The results demonstrate the outperformance of T2-FDL compared to other important classification methods in the literature, in terms of accuracy, specificity, and sensitivity. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113500	10.1016/j.eswa.2020.113500													
J								An expert system to discover key congestion points for urban traffic	EXPERT SYSTEMS WITH APPLICATIONS										Key points of congestion; BSSReduce; Digital map; Feature selection; Soft sets; Rough sets	FEATURE-SELECTION; SOFT SET; INFORMATION; EXPRESSWAYS	Discovering key congestion points periodically in traffic jams is a critical issue. It supports road managers to make sense of the situations, and rule out the congestion economically and efficiently. However, city scale and synchronal traffic data bring hardships for such kind of analyses. With recent developments in data science, the availability of traffic conditions data generated by the rising digital map applications makes this issue feasible. Therefore, we firstly propose a digital map data-driven expert system to discover and measure the city-scale key congestion points. It is based on a state-of-the-art feature selection method, BSSReduce (Bijective soft set based feature selection). Data from Baidu Map for Chongqing and Beijing are collected as a case to conduct this study. The results indicate that our proposed method helps the road managers recognize 75 and 300 key congestion points from over 10,000 and 50,000 points of the urban roads each month. The visualized results, as well as the significance measurements, provide road managers an expert system to quickly rule out congestion and work out new solutions to future traffic management. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113544	10.1016/j.eswa.2020.113544													
J								Opposition-based learning Harris hawks optimization with advanced transition rules: principles and analysis	EXPERT SYSTEMS WITH APPLICATIONS										Meta-heuristics; Harris hawks optimizer; Exploration and exploitation; Nature-inspired algorithms	SINE COSINE ALGORITHM; SALP SWARM ALGORITHM; GLOBAL OPTIMIZATION; INSPIRED OPTIMIZER; EVOLUTIONARY; SEARCH	Harris hawks optimizer (HHO) is a recently developed, efficient meta-heuristic optimization approach, which is inspired by the chasing style and collaborative behavior of Harris hawks in nature. However, for some optimization cases, the algorithm suffers from an immature balance between exploitation and exploration. Therefore, in the present study, four effective strategies are introduced into conventional HHO, such as proposing a non-linear energy parameter for the nergy of prey, differor rapid dives, a greedy selection mechanism, and opposition-based learning. These strategies enhance the search-efficiency of HHO and help to alleviate the issues of stagnation at the sub-optimal solution and premature convergence. A well-known collection of 33 benchmark problems is taken to examine the effectiveness of the proposed m-HHO, and the comparison is performed with conventional HHO and other state-of-the-art algorithms. Accordingly, the proposed m-HHO can serve as an effective and efficient optimization tool for global optimization problems. (c) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				NOV 15	2020	158								113510	10.1016/j.eswa.2020.113510													
J								An exact algorithm for the flexible multilevel project scheduling problem	EXPERT SYSTEMS WITH APPLICATIONS										Multilevel projects; Flexible approaches; Scheduling problems	TOOLS	Single project scheduling has received far more attention than have schedules of project portfolios or multi-projects. This lack of scheduling techniques is especially true for flexible portfolios, such as agile, hybrid and extreme project portfolios, which require flexible project structures. While multilevel project scheduling algorithms already exist for deterministic multilevel project structures, they are not able to handle flexible structures. This short paper wants to fill this gap. A matrix-based multilevel multimode project scheduling ((MPSP)-P-4) algorithm is proposed to schedule flexible multilevel projects. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0957-4174	1873-6793				NOV 15	2020	158								113485	10.1016/j.eswa.2020.113485													
J								Random-forest-based real-time contrasts control chart using adaptive breakpoints with symbolic aggregate approximation	EXPERT SYSTEMS WITH APPLICATIONS										Real-time contrasts (RTC); Control chart; Process monitoring; Symbolic aggregate approximation (SAX); Adaptive breakpoints-SAX (ABP-SAX)	STATISTICAL PROCESS-CONTROL; DIMENSIONALITY REDUCTION; CLASSIFICATION; SAX	For high yield management, process monitoring has become an increasingly important task. The real-time contrasts (RTC) control chart uses the real-time classification method for process monitoring and outperforms the existing real-time control chart. The original RTC control chart identifies the cause of faults using a random forest classifier. However, the random forest provides discrete monitoring statistics that could make the overall performance less efficient. To improve the performance of the RTC control chart, we propose a random-forest-based RTC control chart that uses adaptive breakpoints with symbolic aggregate approximation (ABP-SAX). The monitoring statistics of the RTC control chart indicate the process condition, and the quality of the monitoring statistics is determined by the classification performance of the classifier. Therefore, to improve the classification performance of individual decision trees, we proposed ABP-SAX. The original SAX causes time-information loss and distortions in the data slope and pattern. We prevent these problems using the mean squared error to minimize the difference between the represented and original data. After the applied ABP-SAX representation, the raw data are represented by categorical values that preserve information from the original data, and the represented data improve the performance of the RTC control chart. Therefore, the proposed RTC control chart could detect shifts more quickly and identify the cause of the faults. Our improvements can contribute to high yield management and quick response to abnormalities. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113407	10.1016/j.eswa.2020.113407													
J								Markov blanket-based universal feature selection for classification and regression of mixed-type data	EXPERT SYSTEMS WITH APPLICATIONS										Markov blanket; Multivariate feature selection; Conditional independence test; Likelihood-ratio test; Classification; Regression	MUTUAL INFORMATION; ALGORITHM	Feature selection has been successfully applied to improve the quality of data analysis in various expert and intelligent systems. However, because most real-world data nowadays come with mixed features, traditional feature selection approaches that are mainly designed to handle single-type data are not suitable for this situation. In addition, most of existing methods are only applicable to a specific problem, either classification or regression. Therefore, it is an urgent need to develop a universal feature selection method that can be applied to classification and regression with mixed-type data. In response to this, our paper presents a new feature selection method based on a Markov blanket (MB) called Mixed-MB. The key idea behind this is to embed a likelihood ratio-based generalized conditional independence test into an efficient MB search algorithm to find the minimal set of features to fully explain the target variable on mixed-type data. This new MB feature selection method eliminates the weakness of existing MB feature selection method that it only can handle single-type data, while maintaining its strengths such as theoretical soundness, simplicity, speed, and versatility. Experimental results on real-world data sets with mixed features demonstrate that the proposed method is effective for improving the accuracy of prediction models in both classification and regression. It is also shown to be able to yield more accurate results with fewer features than other methods. We believe that Mixed-MB will be widely used in expert and intelligent systems that utilize various data to create value since it can be applied to any type of data and problem. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113398	10.1016/j.eswa.2020.113398													
J								HIN_DRL: A random walk based dynamic network representation learning method for heterogeneous information networks	EXPERT SYSTEMS WITH APPLICATIONS										Dynamic representation learning; Heterogeneous information networks; Meta path; Dynamic random walk		Learning the low-dimensional vector representation of networks can effectively reduce the complexity of various network analysis tasks, such as link prediction, clustering and classification. However, most of the existing network representation learning (NRL) methods are aimed at homogeneous or static networks, while the real-world networks are usually heterogeneous and tend to change dynamically over time, therefore providing an intelligent insight into the evolution of heterogeneous networks is more practical and significant. Based on this consideration, we focus on the dynamic representation learning problem for heterogeneous information networks, and propose a random walk based Dynamic Representation Learning method for Heterogeneous Information Networks (HIN_DRL), which can learn the representation of network nodes at different timestamps. Specifically, we improve the first step of the existing random walk based NRL methods, which generally include two steps: constructing node sequences through random walk process, and then learning node representations by throwing the node sequences into a homogeneous or heterogeneous Skip-Gram model. In order to construct optimized node sequences for evolving heterogeneous networks, we propose a method for automatically extracting and extending meta-paths, and propose a new method for generating node sequences via dynamic random walk based on meta-path and timestamp information of networks. We also propose two strategies for adjusting the quantity and length of node sequences during each random walk process, which makes it more effective to construct the node sequences for heterogeneous information networks at a specific timestamp, thus improving the effect of dynamic representation learning. Extensive experimental results show that compared with the state-of-art algorithms, HIN_DRL achieves better results in Macro-F1, Micro-F1 and NMI for multi-label node classification, multi-class node classification and node clustering on several realworld network datasets. Furthermore, case studies of visualization and dynamic on Microsoft Academic dataset demonstrate that HIN_DRL can learn network representation dynamically and more effectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113427	10.1016/j.eswa.2020.113427													
J								An estimating combination method for interval forecasting of electrical load time series	EXPERT SYSTEMS WITH APPLICATIONS										Interval forecasting; Machine learning method; Distribution estimation; Feature selection; Electrical load time series	ANT COLONY OPTIMIZATION; PREDICTION INTERVALS; QUANTILE REGRESSION; PROBABILISTIC LOAD; NEURAL-NETWORKS; ALGORITHM; HYBRID; PARALLEL; DEMAND	Due to the failure of deterministic point forecasting to capture the uncertainty associated with the original time series, and because it can reflect the range of electrical load fluctuation, the importance of probabilistic interval forecasting has gradually increased. However, the existing theoretical system of interval forecasting is still incomplete, is a complicated process, and has relatively low accuracy. The objective of this study is to propose an interval forecasting approach based on feature selection, the optimized machine learning method, and correction of the Gaussian distribution. By applying this approach, the distribution of predictions can be established to include all the information of prediction intervals at each confidence level, making the best use of the known information. The electrical load time series of Australia are used to examine the effectiveness of the proposed approach, and compared with other models, it is proven to not only simplify the forecasting process and shorten the processing time, but also significantly improve the forecasting efficiency, flexibility, and accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113498	10.1016/j.eswa.2020.113498													
J								Multi-attribute decision making applied to financial portfolio optimization problem	EXPERT SYSTEMS WITH APPLICATIONS										Portfolio optimization; Multiobjective optimization; Multi-attribute decision-making methods	EVOLUTIONARY ALGORITHMS; SELECTION; RISK	This paper proposes an integer multiobjective mean-CVaR portfolio optimization model with variable cardinality constraint and rebalancing and two different methods of decision-maker used to guide and select, according to the decision maker preferences, a solution comes from the non-dominated portfolios generated by a proposed evolutionary algorithm. The decision-making methods were used to approximate investor behavior according to three functions, chosen to represent different investor profiles (conservative, moderate and aggressive). The proposed methods are compared with those found in the literature. Additionally, computational simulations are performed using assets from the Brazilian stock exchange for the period between January 2011 and December 2015. The strategy is that each beginning of the month: the previous portfolio is sold, the optimization is performed, and the decision-making method selects the new portfolio to be purchased. Results of the simulations consider monthly maximum drawdown and cumulative return during the entire study period and show that the optimization model is robust, considering the three simulated profiles. The methods always present cumulative returns above safe investments for the analyzed period, and the aggressive profile obtained bigger gains with greater risk. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113527	10.1016/j.eswa.2020.113527													
J								Fuzzy logic aggregation of crisp data partitions as learning analytics in triage decisions	EXPERT SYSTEMS WITH APPLICATIONS										Learning analytics; Normalized concordance index; Cognitive heuristics; Permutation testing	EMERGENCY; NURSES; HEURISTICS; STRATEGY; JUDGMENT; FLOW	This paper provides an analytical learning system based on Fuzzy Logic AGgregation (FLAG) of crisp data partitions to improve the Triage process in a hospitality emergency department. The method compares patient rankings made by nurses with those made by an Expert to detect points for improvement. Specifically, a normalized concordance index per nurse and the average of them allow for the evaluation of the ability of the nurses of well aggregating the cases in Triage process. The proposed FLAG system is tested through an empirical case study by simulating the patients arriving at two Emergency Departments Triage. The main contribution is the definition of the global performance index combining both the nurse's partitioning concordance with respect to the Expert's one and the accuracy in class assignment. The empirical distribution function of the global concordance index is derived through permutation method. In this way, Kolmogorov-Smirnov testing provides the comparison of the performances of the two healthcare units. The pay-off table concordance-accuracy allows to address improvement actions. Another tool of the system is the correspondence analysis to visualize the accuracy of decisions on the class priority as well as the sharing behaviours that influence the nurse's judgements. All this becomes part of the FLAG learning analytics system which is able to outline critical points by red flags. to improve further assignments and the overall management organization. FLAG system can be adapted in all other situations of risk where cognitive heuristics face an accuracy-effort trade-off such that their simplified decision process leads to reduced accuracy. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113512	10.1016/j.eswa.2020.113512													
J								Improved subspace clustering algorithm using multi-objective framework and subspace optimization	EXPERT SYSTEMS WITH APPLICATIONS										Subspace clustering; Multi-objective Optimization (MOO); Intra-Cluster Compactness (ICC); Feature Non-Redundancy (FNR); Feature Per Cluster (FPC)		Subspace clustering technique divides the data set into different groups or clusters where each cluster comprises of objects that share some similar properties. Again, the feature sets or the subspace features that are used to represent clusters are different for different clusters. Moreover, in subspace clustering, the grouping of similar objects and the subspace feature set representing that group are identified simultaneously. In evolutionary-based machine learning problems, two critical measures to determine the quality of the generated clusters are compactness within and separation between the clusters. However, the distance-based separation between two clusters may not be useful in the context of subspace clustering, as the clusters may belong to two different subspaces. Again, in the case of subspace clustering, the selection of relevant subspace features plays a primary role in generating good quality subspace clusters. Therefore, the proposed approach optimizes the subspace features by considering two new objective functions, feature non-redundancy (FNR) and feature per cluster (FPC) represented in the form of PSMindex. Another objective function, intra-cluster compactness (ICC-index), is modified and used to optimize the compactness among objects within the cluster. Finally, an evolutionary-based multi-objective subspace clustering technique is developed in this paper optimizing these validity indices. A new mutation operator, namely duplication and deletion along with the modified version of the exogenous genetic material uptake, are developed to explore the search space effectively. The developed algorithm is tested on sixteen synthetic data sets and seven standard real-life data sets for identifying different subspace clusters. Again, to show the effectiveness of using multiple objectives, the algorithm is also tested on three big data sets and a MNIST data set. Also, an application of the proposed method is shown in biclustering the gene expression data. The results obtained by the proposed algorithm are compared against some state-of-the-art methods. Experimentation reveals that the proposed algorithm can take advantage of its evolvable genomic structure and the newly defined objective functions on the multi objective based framework. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113487	10.1016/j.eswa.2020.113487													
J								Experimentation and performance in advertising: An observational survey of firm practices on Facebook	EXPERT SYSTEMS WITH APPLICATIONS										Firm experimentation; Exploration; Online advertising; Organizational learning; Reinforcement learning	BUDGET ALLOCATION; EXPLOITATION; EXPLORATION; INFORMATION; ECONOMICS; PRODUCTS; RETURNS	It is widely assumed that firms experiment with their online advertising to identify more profitable approaches to then increase their investment in more profitable advertising, increasing their overall performance. Generalizable evidence on the actual use of such experiment-based learning by firms is sparse. The study herein addresses this shortcoming - detailing the extent to which large advertisers are utilizing experimentation along with evidence on the benefits of doing so. The findings are gleaned from firms' marketing and experimentation practices on a large online advertising platform and indicate that, while experimentation is utilized by some, adoption is far from perfect. Among the few firms making use of experiments, even fewer invest a significant share of their advertising spend in experimentation. This finding is surprising in light of broadly assumed regular experimentation by firms. Experimenting firms further experience higher concurrent and subsequent performance, suggesting that leading firms indeed successfully use experiment-based learning to improve their advertising policies - and that many firms may fall short of their potential by not (yet) using experiments in advertising. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113554	10.1016/j.eswa.2020.113554													
J								Enhancing a Pairs Trading strategy with the application of Machine Learning	EXPERT SYSTEMS WITH APPLICATIONS										Pairs trading; Market neutral; Machine Learning; Deep learning; Unsupervised learning	STATISTICAL ARBITRAGE; COINTEGRATION; OUTRANKING; SELECTION	Pairs Trading is one of the most valuable market-neutral strategies used by hedge funds. It is particularly interesting because it overcomes the arduous process of valuing securities by focusing on relative pricing. By buying a relatively undervalued security and selling a relatively overvalued one, a profit can be made upon the pair's price convergence. However, with the growing availability of data, it became increasingly harder to find rewarding pairs. In this work, we address two problems: (i) how to find profitable pairs while constraining the search space and (ii) how to avoid long decline periods due to prolonged divergent pairs. To manage these difficulties, the application of promising Machine Learning techniques is investi-gated in detail. We propose the integration of an Unsupervised Learning algorithm, OPTICS, to handle problem (i). The results obtained demonstrate the suggested technique can outperform the common pairs' search methods, achieving an average portfolio Sharpe ratio of 3.79, in comparison to 3.58 and 2.59 obtained by standard approaches. For problem (ii), we introduce a forecasting-based trading model, capable of reducing the periods of portfolio decline by 75%. Yet, this comes at the expense of decreasing overall profitability. The proposed strategy is tested using an ARMA model, an LSTM and an LSTM Encoder-Decoder. This work's results are simulated during varying periods between January 2009 and December 2018, using 5-min price data from a group of 208 commodity-linked ETFs, and accounting for transaction costs. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113490	10.1016/j.eswa.2020.113490													
J								A co-optimal coverage path planning method for aerial scanning of complex structures	EXPERT SYSTEMS WITH APPLICATIONS										Coverage path planning; Unmanned aerial vehicle; Particle swarm optimization; Viewpoint quality	OPTIMIZATION	The utilization of unmanned aerial vehicles (UAVs) in survey and inspection of civil infrastructure has been growing rapidly. However, computationally efficient solvers that find optimal flight paths while ensuring high-quality data acquisition of the complete 3D structure remains a difficult problem. Existing solvers typically prioritize efficient flight paths, or coverage, or reducing computational complexity of the algorithm - but these objectives are not co-optimized holistically. In this work we introduce a co-optimal coverage path planning (CCPP) method that simultaneously co-optimizes the UAV path, the quality of the captured images, and reducing computational complexity of the solver all while adhering to safety and inspection requirements. The result is a highly parallelizable algorithm that produces more efficient paths where quality of the useful image data is improved. The path optimization algorithm utilizes a particle swarm optimization (PSO) framework which iteratively optimizes the coverage paths without needing to discretize the motion space or simplify the sensing models as is done in similar methods. The core of the method consists of a cost function that measures both the quality and efficiency of a coverage inspection path, and a greedy heuristic for the optimization enhancement by aggressively exploring the viewpoints search spaces. To assess the proposed method, a coverage path quality evaluation method is also presented in this research, which can be utilized as the benchmark for assessing other CPP methods for structural inspection purpose. The effectiveness of the proposed method is demonstrated by comparing the quality and efficiency of the proposed approach with the state-of-art through both synthetic and real-world scenes. The experiments show that our method enables significant performance improvement in coverage inspection quality while preserving the path efficiency on different test geometries. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113535	10.1016/j.eswa.2020.113535													
J								Maximum-relevance and maximum-diversity of positive ranks: A novel feature selection method	EXPERT SYSTEMS WITH APPLICATIONS										Feature selection; Ranks of positives; Relevance; Redundancy; Diversity; Multivariate filter	MUTUAL INFORMATION; CLASSIFICATION; AREA	With the existing abundance of intelligent and expert systems, there is a need for selecting a subset of highly relevant features with low redundancy. In filter approaches, the feature subsets are iteratively computed by evaluating the candidate features in terms of their relevance with the target class and pair wise redundancies. The use mutual information-based metrics has been extensively studied as an approach to quantifying the relevance and redundancy of candidate features. In this study, a novel filter approach based on ranks of positive instances is proposed. In this approach, redundancy is replaced by diversity to quantify the complementarity of a candidate feature with respect to the already selected subset. Both relevance and diversity are computed in terms of the ranks of positive instances, which is analogous to the computation of the area under the receiver operating characteristic curve (AUC). Experiments conducted on 15 UCI and microarray gene expression data sets have confirmed that the proposed multivariate filter feature selection approach provides better performance scores when compared to other competing multivariate methods as well as benchmark univariate filters. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113499	10.1016/j.eswa.2020.113499													
J								Real-time classification for autonomous drowsiness detection using eye aspect ratio	EXPERT SYSTEMS WITH APPLICATIONS										Real-time drowsiness detection; Computer vision; Machine learning; Support vector machine; Eye aspect ratio; Human reliability	SUPPORT VECTOR MACHINES; DETECTION SYSTEM; COMPUTER VISION; BLINK DETECTION; PERFORMANCE; SLEEPINESS; FATIGUE; ROBUST; RECOGNITION; INHIBITION	Various automated systems require human supervision in complex environments: this can be a monotonous task but still requiring a significant degree of attention. If those tasks are decisive to the process and work safety, then it is imperative that operators maintain adequate levels of alertness to execute necessary actions. Here, we developed a methodology for drowsiness detection based on eye patterns of people monitored by video streams. In contrast to physically intrusive methods based on a biological approach (e.g. electrooculogram), computer vision and machine leaning (ML) were used to create a low-cost realtime system to detect whether a user (operator) is drowsy using a simple web camera. The proposed methodology employs drowsiness rules for blink patterns from neuroscience literature, which allows for automatic alertness supervision of users reducing risks of potential human error and then preventing accidents. Specifically, a temporal element is introduced by concatenating information from several consecutive video frames coupled with the ability of ML models in identifying different eye behavior. Here, multilayer perceptron, random forest, and support vector machines were analyzed: the latter had the overall best performance in terms of average test accuracy (94.9%) and required execution time. The proposed methodology also contains a personal feedback proposal to adapt models for each specific user providing even better results. We validated our model in DROZY - a public database for human drowsiness. Interand intra-subject investigations were conducted considering the Karolinska Sleepiness Scale (KSS) evaluation and the reaction time as performance indicators. In inter-subject analysis, our model did not provide any warning when a subject was awake, but an average of 16.1 warnings were emitted for drowsy subjects with 94.44% accuracy. For intra-subject analysis, our model could detect when subjects were prone to drowsiness. These are interesting and promising results regarding drowsiness detection. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113505	10.1016/j.eswa.2020.113505													
J								A new metaheuristic based on vapor-liquid equilibrium for solving a new patient bed assignment problem	EXPERT SYSTEMS WITH APPLICATIONS										Vapor-liquid equilibrium algorithm; Metaheuristic; Patient bed assignment problem; Constraint optimization problem	AUTONOMOUS SEARCH; ALGORITHM; OPTIMIZATION; MODEL	Bio-inspired computing is an emerging paradigm which is based on the basics and inspiration of natural phenomena to design new and robust competing techniques. Various nature science areas have motivated the inspiration for the design of new intelligent systems. Chemical engineering is one of them. In Chemistry, the vapor-liquid equilibrium process describes the distribution of chemical species combining two essential phases: vapor phase and liquid phase. Using a binary system of compounds is possible to simulate a search process based on the equilibrium between both phases. In this paper, we propose a new algorithm inspired by this chemical phenomenon for solving a new patient bed assignment problem. This problem consists of assigning patients to beds by considering relevant medical requirements trying to maximize the most covered soft constraints. For that, we take a traditional model, and we transform it by using the constraint optimization paradigm. We test our algorithm on 30 benchmarks taken of Chilean health services. To verify results, we perform statistical comparatives with artificial bee algorithm, ant colony optimization, the bat method, cuckoo search, genetic algorithm, particle swarm optimization, and a random strategy. Computational experiments illustrate that the VLE algorithm properly solved 30 instances, finding all global optimal. In nineteen instances, VLE converged towards the best solution in its median value. In ten instances, the median, the average and the best value, all of them achieved the global optimal. Now, when comparing VLE against other techniques, we can note that VLE is sur passed by the artificial bee colony in two instances only. The rest of the results show VLE as a robust algorithm able to suppress classical, such as genetic algorithm and particle swarm optimization. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113506	10.1016/j.eswa.2020.113506													
J								A hybrid recommender system for recommending relevant movies using an expert system	EXPERT SYSTEMS WITH APPLICATIONS										Recommender system; Hybrid recommender system; Expert system; Collaborative-filtering; Content-based filtering; Movies; Rating		Currently, the Internet contains a large amount of information, which must then be filtered to determine suitability for certain users. Recommender systems are a very suitable tool for this purpose. In this paper, we propose a monolithic hybrid recommender system called Predictory, which combines a recommender module composed of a collaborative filtering system (using the SVD algorithm), a content-based system, and a fuzzy expert system. The proposed system serves to recommend suitable movies. The system works with favorite and unpopular genres of the user, while the final list of recommended movies is determined using a fuzzy expert system, which evaluates the importance of the movies. The expert system works with several parameters - average movie rating, number of ratings, and the level of similarity between already rated movies. Therefore, our system achieves better results than traditional approaches, such as collaborative filtering systems, content-based systems, and weighted hybrid systems. The system verification based on standard metrics (precision, recall, F1-measure) achieves results over 80%. The main contribution is the creation of a complex hybrid system in the area of movie recommendation, which has been verified on a group of users using the MovieLens dataset and compared with other traditional recommender systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113452	10.1016/j.eswa.2020.113452													
J								NI-MWMOTE: An improving noise-immunity majority weighted minority oversampling technique for imbalanced classification problems	EXPERT SYSTEMS WITH APPLICATIONS										Imbalanced classification; Noise-immunity; MWMOTE; Clustering; Oversampling	PERFORMANCE; PREDICTION	Oversampling techniques have been favored by researchers because of their simplicity and versatility in dealing with imbalanced classification problems. For oversampling techniques appeared in recent years (e.g. Majority Weighted Minority Oversampling Technique (MWMOTE)), noise processing plays an important role. This is because the processing of noise directly affects the distribution of new synthetic instances. MWMOTE and many other oversampling techniques use knn based noise processing method. While the knn method can effectively handle partial noise when the neighborhood parameter k value is reasonable, it may lead to under-recognition or over-recognition without prior experience. Therefore, we propose an improving noise-immunity majority weighted minority oversampling technique abbreviated NI-MWMOTE. NI-MWMOTE uses an adaptive noise processing scheme, which combines Euclidean distance and neighbor density to rank the probability that suspected noise (knn method) is true noise, and then adaptively selects the best noise processing scheme through iteration and misclassification error. Then, aggregative hierarchical clustering (AHC) method is used to cluster minority instances. And, in each sub-cluster, the sampling size of new samples is adaptively determined by classification complexity and cross-validation. NI-MWMOTE not only avoids the generation of new noise, but also effectively overcomes both between-class imbalances and within-class imbalances. Results demonstrate that NI-MWMOTE achieves significantly better results in most imbalanced datasets than eight popular oversampling algorithms. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113504	10.1016/j.eswa.2020.113504													
J								A decision support system for urban infrastructure inter-asset management employing domain ontologies and qualitative uncertainty-based reasoning	EXPERT SYSTEMS WITH APPLICATIONS										Smart cities; Infrastructure maintenance; Underground utilities; Rule-based system; Reasoning under uncertainty	REHABILITATION; ALGORITHM	Urban infrastructure assets (e.g. roads, water pipes) perform critical functions to the health and well-being of society. Although it has been widely recognised that different infrastructure assets are highly interconnected, infrastructure management in practice such as planning, installation and maintenance are often undertaken by different stakeholders without considering these dependencies due to the lack of relevant data and cross-domain knowledge, which may cause unexpected cascading social, economic and environmental effects. In this paper, we present a knowledge based decision support system for urban infrastructure inter-asset management. By considering various infrastructure assets (e.g. road, ground, cable), triggers (e.g. pipe leaking) and potential consequences (e.g. traffic disruption) as a holistic system, we model each sub-domain using a modular ontology and encapsulate the interdependence between them using a set of rules. Moreover, qualitative likelihood is assigned to each rule by domain experts (e.g. civil engineers) to encode the uncertainty of knowledge, and an inference engine is applied to predict the potential consequences of a given trigger with location specific data and the encoded rules. A web-based prototype system has been developed based on the above concept and demonstrated to a wide range of stakeholders. The system can assist in the process of decision making by aiding data collation and integration, as well as presenting potential consequences of possible triggers, advising on whether additional information is needed or suggesting ways of obtaining such information. The work shows an intelligent approach to integrate and process multi-source data to pioneer a novel way to aid a complex decision process with a high social impact. (C) 2020 The Authors. Published by Elsevier Ltd.																	0957-4174	1873-6793				NOV 15	2020	158								113461	10.1016/j.eswa.2020.113461													
J								Improved reinforcement learning with curriculum	EXPERT SYSTEMS WITH APPLICATIONS										Curriculum learning; Reinforcement learning; Monte Carlo tree search; General game playing	NEURAL-NETWORKS; GAME; GO	Humans tend to learn complex abstract concepts faster if examples are presented in a structured manner. For instance, when learning how to play a board game, usually one of the first concepts learned is how the game ends, i.e. the actions that lead to a terminal state (win, lose or draw). The advantage of learning endgames first is that once the actions leading to a terminal state are understood, it becomes possible to incrementally learn the consequences of actions that are further away from a terminal state - we call this an end-game-first curriculum. The state-of-the-art machine learning player for general board games, AlphaZero by Google DeepMind, does not employ a structured training curriculum. Whilst Deepmind's approach is effective, their method for generating experiences by self-play is resource intensive, costing literally millions of dollars in computational resources. We have developed a new method called the endgame-first training curriculum, which, when applied to the self-play/experience-generati on loop, reduces the required computational resources to achieve the same level of learning. Our approach improves performance by not generating experiences which are expected to be of low training value. The end-gamefirst curriculum enables significant savings in processing resources and is potentially applicable to other problems that can be framed in terms of a game. (c) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113515	10.1016/j.eswa.2020.113515													
J								Construction of partner selection criteria in sustainable supply chains: A systematic optimization model	EXPERT SYSTEMS WITH APPLICATIONS										Sustainable supply chain; Partner selection; Criteria interrelationship analysis; Dempster-Shafer theory; Genetic algorithm	ANALYTIC NETWORK PROCESS; DECISION-MAKING APPROACH; FUZZY DEMATEL; PROGRAMMING APPROACH; ORDER ALLOCATION; NEURAL-NETWORK; MANAGEMENT; PERFORMANCE; FRAMEWORK; TOPSIS	With the recent emphasis on sustainable supply chain management (SSCM) by both companies, consumers and governments, selecting eco-economic partners is more important than ever. Multiple aspects play critical roles in partner selection, including so many social, environmental, and economic attributes, but considering all of them simultaneously has proved impossible due to limitations in terms of evaluating resources and efficient decision making. Hence, a crucial first step of partner selection in an SSCM context is to decide which criteria to focus on and which to disregard. However, previous literature on partner selection usually treat the establishment of criteria as a given, and also neglect the interrelationships between them. To bridge this research gap, the present study develops a systematic four-stage optimization model for constructing partner selection criteria, based on Dempster-Shafer theory, the improved non-dominated sorting genetic algorithm-II (NSGA-II), and the decision-making trial and evaluation laboratory (DEMATEL) method. Our model integrates quantitative and qualitative methodologies to identify, group, filter, and analyze potential alternative criteria for partner selection in sustainable supply chains. We demonstrate the feasibility and effectiveness of our proposed approach with a case study of a Chinese electronic and equipment manufacturer. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113643	10.1016/j.eswa.2020.113643													
J								Rumor detection based on propagation graph neural network with attention mechanism	EXPERT SYSTEMS WITH APPLICATIONS										Rumor detection; Social network; Graph neural network; Social security; Representation learning	FAKE NEWS; VERACITY; TWITTER	Rumors on social media have always been an important issue that seriously endangers social security. Researches on timely and effective detection of rumors have aroused lots of interest in both academia and industry. At present, most existing methods identify rumors based solely on the linguistic information without considering the temporal dynamics and propagation patterns. In this work, we aim to solve rumor detection task under the framework of representation learning. We first propose a novel way to construct the propagation graph by following the propagation structure (who replies to whom) of posts on Twitter. Then we propose a gated graph neural network based algorithm called PGNN, which can generate powerful representations for each node in the propagation graph. The proposed PGNN algorithm repeatedly updates node representations by exchanging information between the neighbor nodes via relation paths within a limited time steps. On this basis, we propose two models, namely GLO-PGNN (rumor detection model based on the global embedding with propagation graph neural network) and ENS-PGNN (rumor detection model based on the ensemble learning with propagation graph neural network). They respectively adopt different classification strategies for rumor detection task, and further improve the performance by including attention mechanism to dynamically adjust the weight of each node in the propagation graph. Experiments on a real-world Twitter dataset demonstrate that our proposed models achieve much better performance than state-of-the-art methods both on the rumor detection task and early detection task. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113595	10.1016/j.eswa.2020.113595													
J								Evolution features and behavior characters of friendship networks on campus life	EXPERT SYSTEMS WITH APPLICATIONS										Evolution feature; Behavior character; Friendship network; Percolation theory	PERCOLATION	Analyzing and mining students' behaviors and interactions from big data is an essential part of education data mining. Based on the data of campus smart cards, which include not only static demographic information but also dynamic behavioral data from more than 30000 anonymous students, in this paper, the evolution features of friendship and the relations between behavior characters and student interactions are investigated. On the one hand, four different evolving friendship networks are constructed by means of the friend ties proposed in this paper, which are extracted from monthly consumption records. In addition, the features of the giant connected components (GCCs) of friendship networks are analyzed via social network analysis (SNA) and percolation theory. On the other hand, two high-level behavior characters, orderliness and diligence, are adopted to analyze their associations with student interactions. Our experiment/empirical results indicate that the sizes of friendship networks have declined with time growth and both the small-world effect and power-law degree distribution are found in friendship networks. Second, the results of the assortativity coefficient of both orderliness and diligence verify that there are strong peer effects among students. Finally, the percolation analysis of orderliness on friendship networks shows that a phase transition exists, which is enlightening in that swarm intelligence can be realized by intervening the key students near the transition point. (C)2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113519	10.1016/j.eswa.2020.113519													
J								Semi-supervised learning for ECG classification without patient-specific labeled data	EXPERT SYSTEMS WITH APPLICATIONS										Semi-supervised learning; Arrhythmia; CNN; ECG classification; Time series signal	HEARTBEAT CLASSIFICATION; BEAT CLASSIFICATION; ARRHYTHMIA DETECTION; MORPHOLOGY; FEATURES	In this paper, we propose a semi-supervised learning-based ECG classification system for detection of supraventricular ectopic beats (SVEB or S beats) and ventricular ectopic beats (VEB or V beats) which does not require manual labeling of the patient-specific ECG data. Owing to inter-subject variability in ECG signal, patient-specific data is usually required to achieve good performance in ECG classification system. However, manual labeling of patient-specific data requires expert intervention, which is costly and time consuming. Our proposed system is based on a 2D convolutional neural network (CNN) with inputs generated from heartbeat triplets. The system also consists of two auxiliary modules: a normal beat estimation module and an iterative beat label update algorithm. The normal beat estimation selects a small amount of patient-specific normal beats accurately from the testing ECG record in an unsupervised manner. These estimated normal beats are used, together with a common pool dataset, to train a preliminary patient-specific CNN classifier which provides initial labels for the testing data. These labels then undergo a semi-supervised iterative update process for improved performance. Our proposed system was evaluated on the MIT-BIH arrhythmia database. The training of our proposed system is fully automatic, and its performance is comparable with several state-of-art supervised methods which require extra manual labeling of patient-specific ECG data. Our proposed system can be a useful tool for batch processing a large amount of ECG data in clinical applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113411	10.1016/j.eswa.2020.113411													
J								Automatic scale estimation for music score images	EXPERT SYSTEMS WITH APPLICATIONS										Optical music recognition; Scale estimation; Scale correction; Music score images analysis	BINARIZATION; RECOGNITION; REMOVAL	Optical Music Recognition (OMR) is the research field focused on the automatic reading of music from scanned images. Its main goal is to encode the content into a digital and structured format with the advantages that this entails. This discipline is traditionally aligned to a workflow whose first step is the document analysis. This step is responsible of recognizing and detecting different sources of information-e.g. music notes, staff lines and text-to extract them and then processing automatically the content in the following steps of the workflow. One of the most difficult challenges it faces is to provide a generic solution to analyze documents with diverse resolutions. The endless number of existing music sources does not meet a standard that normalizes the data collections, giving complete freedom for a wide variety of image sizes and scales, thereby making this operation unsustainable. In the literature, this question is commonly overlooked and a uniform scale is assumed. In this paper, a machine learning-based approach to estimate the scale of music documents with respect to a reference scale is presented. Our goal is to propose a robust and generalizable method to adapt the input image to the requirements of an OMR system. For this, two goal-directed case studies are included to evaluate the proposed approach over common task within the OMR workflow, comparing the behavior with other state-of-the-art methods. Results suggest that it is necessary to perform this additional step in the first stage of the workflow to correct the scale of the input images. In addition, it is empirically demonstrated that our specialized approach is more promising than image augmentation strategies for the multi-scale challenge. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113590	10.1016/j.eswa.2020.113590													
J								Advancement of the search process for digital heritage by utilizing artificial intelligence algorithms	EXPERT SYSTEMS WITH APPLICATIONS										Big data analytics; Digital heritage; Search engine; Machine learning; Neural network; Neural foresting	CONVERGENCE	The increasing amount of pressure to digitalize what we are used to consider a conventional data has created a need to analyze, search and process unique data structures in a timely manner. The progressive world has created a justified need not only for a fast query searches but also the most related and meaningful searches with a minimal guidance. This article explores the potential benefits of using unorthodox solutions, including Artificial Intelligence algorithms in processing big data that are unconventional data structures. One of such big data sources was created as part of digitalization of historic heritage materials, documents and artifacts. The article calls out the benefits and disadvantages of some Artificial Intelligence algorithms and explores ways to use a few of those algorithms for the purposes of the digital heritage. It also offers the solution to maximize the potential of the search engine that could be built for digital heritage or any other unstructured data. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113559	10.1016/j.eswa.2020.113559													
J								Monaural speech enhancement through deep wave-U-net	EXPERT SYSTEMS WITH APPLICATIONS										Speech enhancement; Noise reduction; Deep learning; Signal to Noise Ratio (SNR); Word Error Rate (WER)		In this paper, we present Speech Enhancement through Wave-U-Net (SEWUNet), an end-to-end approach to reduce noise from speech signals. This background context is detrimental to several downstream systems, including automatic speech recognition (ASR) and word spotting, which in turn can negatively impact end-user applications. We show that our proposal does improve signal-to-noise ratio (SNR) and word error rate (WER) compared with existing mechanisms in the literature. In the experiments, network input is a 16 kHz sample rate audio waveform corrupted by an additive noise. Our method is based on the Wave-U-Net architecture with some adaptations to our problem. Four simple enhancements are proposed and tested with ablation studies to prove their validity. In particular, we highlight the weight initialization through an autoencoder before training for the main denoising task, which leads to a more efficient use of training time and a higher performance. Through quantitative metrics, we show that our method is prefered over the classical Wiener filtering and shows a better performance than other state-of-the-art proposals. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113582	10.1016/j.eswa.2020.113582													
J								Anytime automatic algorithm selection for knapsack	EXPERT SYSTEMS WITH APPLICATIONS										Knapsack; Algorithm selection problem; Machine learning techniques; Anytime behaviour approach	BOUNDS	In this paper, we present a new approach for Automatic Algorithm Selection. In this new procedure, we feed the predictor of the best algorithm choice with a runtime limit for the solvers. Hence, the machine learning model should consider and learn from the Anytime Behavior of the solvers, together with features characterizing each instance. For this purpose, we propose a general Framework and apply it to the Knapsack problem. Thus, we created a large and diverse dataset of 15, 000 instances, recorded the anytime behavior of 8 solvers on them and trained and tested three machine learning strategies, collecting the results for different machine learning algorithms. Our results show that, for the majority of the tuples <instance, time>, the solver that computes the best objective value can be predicted. We also make this data publicly available, as a challenge for the community to work in this problem and propose new and better machine learning models and solvers. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113613	10.1016/j.eswa.2020.113613													
J								Stochastic data-driven optimization for multi-class dynamic pricing and capacity allocation in the passenger railroad transportation	EXPERT SYSTEMS WITH APPLICATIONS										Revenue management; Dynamic pricing; Capacity allocation; Data-driven optimization; Stochastic programming	MULTIPRODUCT REVENUE MANAGEMENT; RAILWAY; EXPRESS; TRAIN	As for any passenger transportation service provider, pricing and capacity management are two critical tools for the profitability of a passenger railroad service provider: pricing affects the demand for the services and the capacity management sets the availability of the services in advance. In this study, an expert system is developed as a decision support tool for a passenger railroad service provider's integrated pricing and capacity management problem, which has great significance for the success of the service provider. Considering the demand uncertainty, we first formulate the integrated pricing and capacity management problem as a stochastic nonlinear integer programming (SNLIP) model. This model includes dynamic pricing and dynamic capacity allocation decisions for multiple service classes over a planning horizon in order to maximize profit. Also, several key characteristics of the passenger railroad service operations are captured in the model. Due to inherent demand uncertainty as well as the dynamic nature of the problem, a fast and efficient solution approach is needed. Therefore, a simulation-based procedure embedded in a simulated annealing method is proposed to solve the model. Several real-life cases from Fadak Five-Star Trains (an Iranian luxurious passenger railroad service provider) are presented to demonstrate the model and the solution approach. The results of the case studies show the operational and profitability impacts of using the proposed decision support tool as well as its potential capabilities for practical use by other service providers. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113568	10.1016/j.eswa.2020.113568													
J								Semi-supervised regression trees with application to QSAR modelling	EXPERT SYSTEMS WITH APPLICATIONS										Semi-supervised learning; Regression; Decision trees; Random forests; QSAR	CLASSIFICATION	Despite the ease of collecting abundance of data about various phenomena, obtaining labeled data needed for learning models with high predictive performance remains a difficult and expensive task in many domains. This issue is particularly present in the case of the analysis of scientific data where obtaining labeled data typically requires expensive experiments. Moreover, in the analysis of scientific data, another issue is of fundamental importance: the interpretability of the models and the explainability of their decisions. By taking into account these considerations, we propose a novel semi-supervised method to learn regression trees. Thanks to the semi-supervised machine learning approach, the method is able to exploit information coming not only from labeled data, but also from unlabeled data, thus alleviating the issue of lack of labeled data. The method is based on the predictive clustering trees paradigm that extends regression trees towards structured output prediction. This allows us to obtain interpretable regression trees. The method we propose is particularly suited for the chemoinformatics task of quantitative structure activity relationship (QSAR) modeling, which is the main application context considered in this paper. Specifically, we evaluate the proposed method on 4 QSAR modelling datasets and illustrate its use on a case study of predicting farnesyltransferase inhibitors. Additionally, we also evaluate our approach on 10 benchmark datasets not related to the QSAR modeling problem. The evaluation reveals the following: semi-supervised trees and ensembles thereof have better predictive performance than their supervised counterparts (especially when the number of labeled examples is very small); different datasets and different amounts of labeled data require different amounts of unlabeled data to be included in the learning process; and the learned semi-supervised regression trees can be used to better understand the problem at hand and the way predictions are being made. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113569	10.1016/j.eswa.2020.113569													
J								Toward security monitoring of industrial Cyber-Physical systems via distributed intrusion detection	EXPERT SYSTEMS WITH APPLICATIONS										Industrial Cyber-physical system; Distributed intrusion detection; Sensory system state anomaly monitoring; Adaptive Kalman filter; Recursive Gaussian mixture model; Regularized sparse deep belief network	DATA INJECTION ATTACKS; INTEGRITY ATTACKS; STATE ESTIMATION	Industrial Cyber-physical systems (ICPSs), integrating communication, computation and control of industrial processes are referred to as a core technology to approach the Industry 4.0. Ensuring the ICPS security is of paramount importance in smart manufacturing. Considering the characteristics of large-scale, geographically-dispersed and multi-dimensional heterogeneous, federated and life-critical natures of ICPSs, this paper investigates a hierarchically distributed intrusion detection scheme that seeks to achieve the all-round safety protection of ICPSs according to the system structure and attacking types of each ICPS layer. For physical system-relevant perceptual executive layer, potential and covert attacks are detected by the clustered sensory system state residual anomaly monitoring based on a process noise and measurement noise-adaptive Kalman filter (PNMN-AKF). PNMN-AKF can perform a joint recursive estimation of dynamic system states, time-varying process and measurement noise covariance matrices by the variational Bayes approximation framework. In cyberspace, potential cyber-attacks are detected by the anomaly monitoring of the statistical distribution of the network transmission characteristics of data transmission layer by introducing a forgetting factor-induced recursive Gaussian mixture model (FF-RGMM). In the application control layer, a regularized sparse deep belief network model is introduced to characterize the misuse behavior for detecting potential attacks. Extensive validation and comparative experiments have been conducted on a numerical simulation system and a comprehensive ICPS simulation platform by using OPNET and a commonly-used benchmark simplified Tennessee Eastman process (STEP) based on Matlab/Simulink. Experimental results demonstrate that the proposed hierarchically distributed intrusion detection method can efficiently recognize potential and covert cyber-attacks in each ICPSs link with low false alarm rate and missing detection rate, which lays a foundation for the overall security monitoring of ICPSs. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113578	10.1016/j.eswa.2020.113578													
J								Uncertainty mode selection in categorical clustering using the rough set theory	EXPERT SYSTEMS WITH APPLICATIONS										Unsupervised learning; Categorical clustering; Rough set theory; K-modes; Uncertainty	K-MEANS ALGORITHM; DISSIMILARITY MEASURE; INFORMATION; INITIALIZATION	Clustering is an unsupervised Machine Learning technique widely used to arrange a set of observations into distinct groups called clusters. The problem of categorical clustering has attracted much attention since many real world applications tend to produce such data types. The k-mode was among the first algorithms developed in this context. This algorithms uses the notion of modes to represent the centroids within the clusters. However, its major drawback lies in the random selection of the modes in each iteration during the clustering process. In this paper, we tackled this random selection issue and proposed a new method based on identifying the most adequate modes among a list of candidate ones. The proposed algorithm called Density Rough k-modes (DRk-M) is based on computing the density of each candidate mode to characterize the distribution of the observations around it. Then, we use the Rough Set Theory to deal with the uncertainty involved in this process. The DRk-M was experimented using real world datasets extracted from the UCI (University of California Irvine) Machine Learning Repository, the Global Terrorism Database (GTD) and a set of scrapped Tweets. The DRk-M was compared to many state of the art methods including the k-modes (1998), the Ng's method (2007), Cao's method (2012) and Bai's technique (2014) and it has shown great efficiency. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113555	10.1016/j.eswa.2020.113555													
J								Bee Colony Optimization metaheuristic for fuzzy membership functions tuning	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy logic system; Bee Colony Optimization; Freight train energy consumption	TRANSIT NETWORK DESIGN; TABU SEARCH; INTELLIGENCE; ALGORITHMS	The successfulness of the application of fuzzy logic to real-life problems depends on a number of parameters, such as the number and shapes of fuzzy membership functions, which are usually defined upon intuition or the subjective knowledge of relevant experts. One way to improve the performance of the fuzzy reasoning model is to use heuristic or metaheuristic techniques in order to optimize the shapes of membership functions. In this paper, the Bee Colony Optimization (BCO) algorithm is applied as a tool suitable for the specified optimization problem. The BCO belongs to the group of nature-inspired meta heuristics. The purpose of this paper is to present and discuss a strategy for the adjustment of fuzzy logic membership functions using the variant of the BCO algorithm based on the improvement of complete solutions and show its real-life application to the problem of the estimation of freight train energy consumption. According to the obtained results, it can be concluded that the precision of the developed fuzzy reasoning model is significantly increased after tuning membership functions by the BCO. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113601	10.1016/j.eswa.2020.113601													
J								Convolutional neural network-based safety evaluation method for structures with dynamic responses	EXPERT SYSTEMS WITH APPLICATIONS										Structural health monitoring; Safety evaluation; Strain monitoring; Dynamic structural response; Convolutional neural network	DAMAGE IDENTIFICATION; MODEL	The strain sensors that are used to evaluate structural members have a limited lifespan and thus have shown limitations to perform long-term structural health monitoring (SHM). This study presents a convolutional neural network (CNN)-based strain prediction technique that allows for structural safety evaluations in case of absence or defect of strain sensors. In the proposed method, CNNs were used to establish a relationship between the dynamic structural response and the strain response measured in the structure. A number of dynamic structural responses and the structural member's strain response that are measured before the strain sensor malfunctions are used as input data and output data, respectively, to train the CNNs. The trained CNNs can estimate the strain and evaluate the structural safety even when the later strain measurement response cannot be used. Dynamic acceleration and displacement responses are used as input data in the two CNNs presented in this study, called CNN_A and CNN_D respectively. A numerical study of a beam-like structure and an experimental study which includes shaking table tests on a reinforced concrete frame specimen were conducted to confirm the validity of the strain predictions by the proposed method with CNN_A and CNN_D. The strain prediction performance of the proposed CNNs is compared in these applications. This study also examines the proposed technique's strain prediction performance according to the amount of data used to train the CNNs. In addition, this study discusses influences of variations in the number of locations for measuring the dynamic structural responses that are used as the CNNs' input data on the strain prediction performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113634	10.1016/j.eswa.2020.113634													
J								An intelligent financial portfolio trading strategy using deep Q-learning	EXPERT SYSTEMS WITH APPLICATIONS										Portfolio trading; Reinforcement learning; Deep Q-learning; Deep neural network; Markov decision process	DECISION-SUPPORT; REINFORCEMENT; OPTIMIZATION; SYSTEM; RULES	Portfolio traders strive to identify dynamic portfolio allocation schemes that can allocate their total budgets efficiently through the investment horizon. This study proposes a novel portfolio trading strategy in which an intelligent agent is trained to identify an optimal trading action using deep Q-learning. We formulate a Markov decision process model for the portfolio trading process that adopts a discrete combinatorial action space and determines the trading direction at a prespecified trading size for each asset, thus ensuring practical applicability. Our novel portfolio trading strategy takes advantage of three features to outperform other strategies in real-world trading. First, a mapping function is devised to handle and transform any action that is initially proposed but found to be infeasible into a similar and valuable feasible action. Second, by overcoming the dimensionality problem, this study establishes agent and Q network models to derive a multi-asset trading strategy in the predefined action space. Last, this study introduces a technique that can derive a well-fitted multi-asset trading strategy by designing an agent to simulate all feasible actions in each state. To validate our approach, we conduct backtesting for two representative portfolios and demonstrate superior results over the benchmark strategies. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113573	10.1016/j.eswa.2020.113573													
J								An effective real time GRASP-based metaheuristic: Application to order consolidation and dynamic selection of transshipment points for time-critical freight logistics	EXPERT SYSTEMS WITH APPLICATIONS										Less than truckload operations; Time critical freight logistics; GRASP; Order consolidation and transshipment; CO2 emissions	DELIVERY PROBLEM; NEIGHBORHOOD SEARCH; PICKUP	Time-critical freight logistics is an area within logistics research where the shipper's orders need to be received relatively urgently using a third party logistics (3PL) that provides a quote (bid) to the shipper within a very short period. We solved this 3PL problem by developing an effective meta-heuristic based on the Greedy Randomized Adaptive Search Procedure (GRASP). This is achieved by introducing novel attributes in the construction of the restricted candidate list while incorporating flexible and intelligent rules, some of which are inspired by expert knowledge. The approach performs order consolidation, locates transshipment points and performs an optimal assignment of shipments to the selected consolidation points dynamically and in real time. This intelligent system embeds expert knowledge within the design of neighbourhood reduction schemes and data structures to speed up the search. This is achieved by recording computed data that does not need to be recomputed again while avoiding unnecessary computations of the non-promising alternatives. The performance of this real time optimisation and scheduling tool is tested with a European 3PL company over a 13 weeks period in late 2017 resulting in a significant cost saving and a considerable reduction in CO2 emissions. This powerful decision support system assists the 3PL company in gaining competitive leadership advantage through producing promising quotes that turn customer requests into real customer orders. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113574	10.1016/j.eswa.2020.113574													
J								Pattern recognition and automatic identification of early-stage atrial fibrillation	EXPERT SYSTEMS WITH APPLICATIONS										Atrial fibrillation; Intrinsic time-scale decomposition; Intrinsic entropy; Pattern recognition	DOMINANT FREQUENCY; TIME-SERIES; DYNAMICS	Atrial fibrillation (AF) is a common cardiac arrhythmia and is responsible for a number of complications. While early-stage AF typically lasts only a few episodes and may not be immediately life-threatening, the cardiac arrhythmia favors electrical and structural alteration of the atria that tends to intensify and perpetuate AF even at incipient stage. Therefore, recognition and identification of patterns associated with early-stage AF episodes is demanded for effective treatment and disease management. Nonetheless, the brevity of early-stage AF negates a myriad of conventional models for effective detection, particularly when only a few seconds of recording is available. In this paper, we investigate constructive patterns based upon intrinsic time-scale decomposition (ITD) to parse single-lead ECG signals with short duration collected from wearable devices. ITD provides accurate instantaneous time-frequency-energy characteristics for the nonlinear and nonstationary data, particularly the short-term time series signals. Our model registers average accuracy of 95%, specificity of 96% and sensitivity of 93% for the diagnosis of AF events, handily outperforming wavelet-based algorithm. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113560	10.1016/j.eswa.2020.113560													
J								RecRisk: An enhanced recommendation model with multi-facet risk control	EXPERT SYSTEMS WITH APPLICATIONS										Recommender systems; Trust; Heat equation; Portfolio theory	MODERN PORTFOLIO THEORY; SYSTEMS	Recommender systems (RSs) play a crucial role in helping users quickly find their desired services and promoting sales of service providers in e-commerce. However, service providers intentionally upload cherry-picked service information to mislead RSs for greater profit. Such misleading recommendations pose the risk of degrading user experience and gradually undermine users' confidence in the whole service market over the long term. Worse still, most current expert recommendation methods are more susceptible to such risk because they are heavily dependent on this incomplete service information and assume it is trusted. Therefore, how to satisfy users' service requirements with risks minimized at the same time motivates our work. In this paper, we first discern two risky facets that pose significant challenges to expert recommendation: Sense Drop and Blue Joy. Then we propose a unified framework called RecRisk, which integrates trust, heat equation, and modern portfolio theory to address the above challenges. The main contributions of RecRisk are twofold: (1) To select the services which satisfy the users' preferences, we design a trust-aware heat equation model (TAHE) that combines heat flow theory with trust elements. (2) We develop a flexible model based on modern portfolio theory to weigh users' satisfaction and services' risk facets, and finally recommend a ranked service list to users. Our experimental results demonstrate that RecRisk simultaneously achieves higher recommendation precision and decreases risks when compared to state-of-the-art approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113561	10.1016/j.eswa.2020.113561													
J								Robust scheduling based on extreme learning machine for bi-objective flexible job-shop problems with machine breakdowns	EXPERT SYSTEMS WITH APPLICATIONS										Flexible job-shop problem; Machine breakdowns; Surrogate measure; Extreme learning machine	OPTIMIZATION APPROACH; GENETIC ALGORITHM; SINGLE-MACHINE; FACE	In modern manufacturing systems, a flexible job-shop schedule problem (FJSP) with random machine breakdown has been widely studied. Two objectives, namely makespan and robustness, were simultaneously considered in this study. Maximizing the workload and float time of each operation and the machine breakdowns, one surrogate measure named RMc was developed via an extreme learning machine (ELM) to evaluate robustness. Specifically, this measure determines the impact of float time on the robustness by the probability of machine breakdown and the location of float time. Simultaneously, the impact was automatically adjusted by the ELM. Then, a method combining an improved version of nondominated sorting genetic algorithm II and RMc was proposed to address the bi-objective FJSP. Computational results on the benchmarks show that RMc accurately evaluates the robustness of the schedules with a small amount of computation cost. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113545	10.1016/j.eswa.2020.113545													
J								Graph based feature selection investigating boundary region of rough set for language identification	EXPERT SYSTEMS WITH APPLICATIONS										Language identification; Feature selection; Relative indiscernibility relation; Attribute dependency; Boundary region exploration	FEATURE-EXTRACTION; COMMUNITY STRUCTURE; NETWORK	Language can be chosen to be a species where maximum information can be extracted. In the world, there are many countries, some of which are of numerous types and flavours of regions based on their languages. The challenge is to make the spoken language recognition to be automated through machine learning. The proposed language identification system extracts various features from speech of different languages and constructs a complete weighted graph with extracted features as nodes and similarity among the features as weights of the edges. Similarity values are computed using the concepts of positive region and boundary region of rough set theory and a graph based feature selection algorithm is devised to select only the minimal subset of features relevant to language identification. It is observed that, investigating the boundary region together with the positive region, more valuable information is extracted which helps in selection of more relevant features for language identification. The constructed complete weighted graph is made sparse using Gini index based sparsity measure. As a result, the graph contains only the edges whose terminal nodes are highly similar. Next, a maximal spanning tree of the graph is generated using Prim's algorithm. This tree is a basic structure that provides the maximal similarity among the nodes in the graph. Finally, score of each node is computed based on weights of the edges in the tree and a node with the high est score is selected and removed from the spanning tree. This process of selection and removal of nodes is continued until the graph becomes null. The resultant set of selected nodes is considered as the important feature subset of the audio speeches used for language identification. Experimental results show the effectiveness of the proposed rough set theory based feature selection method. The results also demonstrate the usefulness of investigation of boundary region of rough sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113575	10.1016/j.eswa.2020.113575													
J								Structural order measure of manufacturing systems based on an information-theoretic approach	EXPERT SYSTEMS WITH APPLICATIONS										Manufacturing systems; Structural order parameter; Information entropy; Operational efficiency	COMPLEXITY; MODEL; DESIGN	A manufacturing structure is the material basis of production operation and management, and its rationality is a necessary prerequisite for ensuring highly efficient operation of manufacturing systems. Therefore, how to determine the rationality of manufacturing structures and accurately screen the optimal scheme in multiple alternatives has become an important production decision-making problem with expert and intelligent systems in recent years and needs to be solved urgently. To solve this problem, we put forward a structure entropy model and a structural order parameter in this paper, respectively. First, combined with the structural characteristics of manufacturing systems, the structure entropy model of manufacturing structures with dynamic characteristics is established and specially treated to make it operable in practical applications. Second, the structural order parameter and the definition of the order of manufacturing structures are developed, which are combined with the structure entropy model to realize the quantitative evaluation of the rationality of manufacturing structures and accurate selection of the optimal structure in the alternatives. Finally, in an empirical study, based on the two parameters of connective paths and connective spans, the structure diagrams of different manufacturing systems are made, which can directly reflect the complexity of manufacturing structures and the importance of equipment utilization, and provide another effective way for the assessment and selection of manufacturing structures. The results of the empirical study demonstrate the effectiveness of the developed approaches, which can solve the production choke point that can only rely on qualitative evaluation or trial operation of all schemes before determining the final one. Therefore, our algorithm not only provides theoretical support and decision-making basis for screening manufacturing structures, but also enriches the evaluation and decision-making methods with expert and intelligent systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				NOV 15	2020	158								113636	10.1016/j.eswa.2020.113636													
J								(mu, nu)-pseudo almost periodic solutions of Clifford-valued high-order HNNs with multiple discrete delays	NEUROCOMPUTING										Clifford-valued high-order Hopfield neural network; (mu, nu)-pseudo almost periodic solution; Global exponential stability	HOPFIELD NEURAL-NETWORKS; GLOBAL EXPONENTIAL STABILITY; ANTIPERIODIC SOLUTIONS; EXISTENCE	In this paper, we consider a class of Clifford-valued higher order Hopfield neural networks (HNNs) with multiple discrete delays whose coefficients of the leakage terms are also Clifford numbers. By a direct approach, based on the fixed point theorem and differential inequality techniques, we gain the existence and global exponential stability of (mu, nu)-pseudo almost periodic solutions of the proposed networks. Finally, we use an example to illustrate the feasibility of our results. Our results are new even when the considered HNNs degenerate into real-valued, complex-valued and quaternion-valued ones. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						1	9		10.1016/j.neucom.2020.07.069													
J								Discriminative comparison classifier for generalized zero-shot learning	NEUROCOMPUTING										Generalized zero-shot learning; Weakly-supervised learning; Image classification		Comparison classifier based generalized zero-shot learning (GZSL) helps to achieve knowledge transfer from seen to unseen classes. However, it only utilizes the original semantic features, which are highly related and indistinguishable, to learn an embedding with no consideration of the relationship between them. Moreover, it cannot well encode discriminative information embedded in semantic features. To handle these problems, we present a discriminative comparison classifier for GZSL, which consists of semantic embedding network and relation network. Semantic embedding network takes as input original semantic features and relationship features which can be obtained by clustering, it ensures that the embedding network from semantic space to visual space can learn more discriminative features. Relation network is used to learn relationship between the embedded features and visual features, the validation information will guide embedding network to learn more discriminate features. Moreover, we adopt a novel semantic pivot regularization to keep inter-class discrimination in the visual space. Extensive experiments on several real-world datasets demonstrate the effectiveness of our method over the other state-of-the-arts. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						10	17		10.1016/j.neucom.2020.07.030													
J								InfGCN: Identifying influential nodes in complex networks with graph convolutional networks	NEUROCOMPUTING										Influential nodes identification; Deep learning; Graph convolution networks; Complex networks	IDENTIFICATION	Identifying influential nodes in a complex network is very critical as complex networks are ubiquitous. Traditional methods, such as centrality based methods and machine learning based methods, only consider either network structures or node features to evaluate the significance of nodes. However, the influential importance of nodes should be determined by both network structures and node features. To solve this problem, this paper proposes a deep learning model, named InfGCN, to identify the most influential nodes in a complex network based on Graph Convolutional Networks. InfGCN takes neighbor graphs and four classic structural features as the input into a graph convolutional network for learning nodes' representations, and then feeds the representations into the task-learning layers, comparing the ground truth derived from Susceptible Infected Recovered (SIR) simulation experiments with quantitative infection rate. Extensive experiments on five real-world networks of different types and sizes demonstrate that the proposed model significantly outperforms traditional methods, and can accurately identify influential nodes. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						18	26		10.1016/j.neucom.2020.07.028													
J								Multi-attention deep reinforcement learning and re-ranking for vehicle re-identification	NEUROCOMPUTING										Re-identification; Deep reinforcement learning; Multi-attention; Re-ranking	OBJECT RETRIEVAL; ACCURATE	For solving the vehicle Re-identification (Re-ID) task, we need to focus our attention on the details with arbitrary size in the image, and it's tough to locate these details accurately. In this paper, we propose a Multi-Attention Deep Reinforcement Learning (MADRL) model to focus on multi-attentional subregions that spreading randomly in the image, and extract the discriminative features for the Re-ID task. First, we obtain multiple attentions from the representative features, then group the feature channels into different parts, then train a deep reinforcement learning model to learn more accurate positions of these fine-grained details with different losses. Unlike existing models with complex strategies to keep the patch-matching constrains, our MADRL model can automatically locate the matching patches (multiattentional subregions) in different vehicle images with the same identification (ID). Furthermore, based on the fine-grained attention and global features we re-calculate the distance between the inter- and intra- classes, and we get better re-ranking results. Compared with state-of-the-art methods on three large-scale vehicle Re-ID datasets, our algorithm greatly improves the performance of vehicle Re-ID. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 13	2020	414						27	35		10.1016/j.neucom.2020.07.020													
J								MFRep: Joint user and employer alignment across heterogeneous social networks	NEUROCOMPUTING										Network alignment; Representation learning; Heterogeneous social networks; Multi-entity alignment	IDENTIFICATION	Nowadays people join multiple social networks simultaneously to enjoy a variety of social services. Apart from common users, social networks also share other information entities such as organizations/companies. Aligning common information entities across heterogeneous social networks is challenging, but will facilitate many applications. Existing approaches for network alignment do not fully leverage indirect relations among users (i.e., friends of friends). This leads to relatively poor performance, especially when there are little overlapping social relations between different networks. Meanwhile, the mutual promotion between two kinds of information entity alignment is often neglected. In this paper, we propose a novel Matrix Factorization based Representation learning (MFRep) framework. MFRep investigates the joint learning for user and employer alignment across different networks. (1). We first compute the cross-network similarities in user attributes to extract seed potential anchor users. Its efficiency can be boosted by considering the similarities in employer properties. (2). Considering social relation differences across networks, we construct user relational matrix to preserve multi-step relational information. This embodies the information propagation from known and seed potential anchor users to other potential anchor users. (3). We extend the user relational matrix to preserve consistent associations between users and employers across networks. (4). We perform semi-supervised user representation learning and unsupervised employer representation learning concurrently via efficient matrix decomposition. The correspondence between users/employers across networks can be inferred based on vector relevance. Extensive experiments on real-world social network datasets justify the utility of MFRep, even with dissimilar social structures across networks. MFRep illustrates the mutual promoted alignments of users and employers with a middle degree of scalability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						36	56		10.1016/j.neucom.2020.07.013													
J								Wi-HSNN: A subnetwork-based encoding structure for dimension reduction and food classification via harnessing multi-CNN model high-level features	NEUROCOMPUTING										Food classification; Representation learning; Deep learning; Subnetwork-based multi-layer neural network (SN-MLNN)		Image-based food pattern classification poses new challenges for mainstream computer vision algorithms. Recent works on feature fusion technique have significantly boosted the generalization performances of food categorization tasks. However, the use of representation learning in the training process of feature fusion has rarely been explored. This study addresses the issue through a new supervised subnetwork-based feature encoding and pattern classification model, termed a wide hierarchical subnetwork-based neural network (Wi-HSNN). In particular, Wi-HSNN is a subnet-based iterative training process in which one pair of subnets is added to the framework in each iteration. Furthermore, instead of learning the optimal representations with the whole dataset, this paper introduces a batch-by-batch parallel scheme of Wi-HSNN to process large-scale datasets, such as Place365 set with more than 1.8 million samples. Extensive evaluations on eight benchmark datasets from food classification to scene image recognition demonstrated that the proposed solution has better representation learning capacity compared to existing encoding methods, and achieves stronger performance than existing approaches for food image classification tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						57	66		10.1016/j.neucom.2020.07.018													
J								A holistic representation guided attention network for scene text recognition	NEUROCOMPUTING										Holistic Representation; Convolutional-Attention; Transformer; Scene Text Recognition		Reading irregular scene text of arbitrary shape in natural images is still a challenging problem, despite the progress made recently. Many existing approaches incorporate sophisticated network structures to handle various shapes, use extra annotations for stronger supervision, or employ hard-to-train recurrent neural networks for sequence modeling. In this work, we propose a simple yet strong approach for scene text recognition. With no need to convert input images to sequence representations, we directly connect two-dimensional CNN features to an attention-based sequence decoder which guided by holistic representation. The holistic representation can guide the attention-based decoder focus on more accurate area. As no recurrent module is adopted, our model can be trained in parallel. It achieves 1.5x to 9.4x acceleration to backward pass and 1.3x to 7.9x acceleration to forward pass, compared with the RNN counterparts. The proposed model is trained with only word-level annotations. With this simple design, our method achieves state-of-the-art or competitive recognition performance on the evaluated regular and irregular scene text benchmark datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						67	75		10.1016/j.neucom.2020.07.010													
J								Progressive transduction nonnegative matrix factorization for dimensionality reduction	NEUROCOMPUTING										Nonnegative matrix factorization; Representation learning; Progressive transduction; Data structure; Semi-supervised learning	GRAPH; PARTS	Dimensionality reduction is one important technique to find out meaningful representations of data by discovering the underlying structures of data. As one widely applied method, by learning parts-based representations, nonnegative matrix factorization (NMF) has been widely researched and used to various application fields. Compared with the previous methods, this paper presents a novel semi-supervised NMF learning framework, called progressive transduction NMF (PTNMF), that learns a robustly discriminative representation by introducing a progressive transduction structure. Specifically, a progressive transduction based scheme is employed to gradually update the representations of unlabeled points according to the similarity between the labeled and unlabeled data points. This is helpful to improve the effectiveness of NMF, especially for the case that labeled data is inadequate. The efficiency of the proposed method is discussed both theoretically and empirically. Extensive experiments on several real-world data sets are listed, and the experimental results demonstrate that the proposed algorithm obtains better discriminant ability in comparison to the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						76	89		10.1016/j.neucom.2020.06.115													
J								Exploring a rich spatial-temporal dependent relational model for skeleton-based action recognition by bidirectional LSTM-CNN	NEUROCOMPUTING										Action recognition; Dependent relational model; Spatial-temporal information		With the fast development of effective and low-cost human skeleton capture systems, skeleton-based action recognition has attracted much attention recently. Most existing methods using Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) have achieved promising performance for skeleton-based action recognition. However, these approaches are limited in the ability to explore the rich spatial-temporal relational information. In this paper, we propose a new spatial-temporal model with an end-to-end bidirectional LSTM-CNN (BiLSTM-CNN). First, a hierarchical spatial-temporal dependent relational model is used to explore rich spatial-temporal information in the skeleton data. Then a new framework is proposed to fuse CNN and LSTM. In this framework, the skeleton data are built by the dependent relational model and serve as the input of the proposed network. Then LSTM is used to extract the temporal features, and followed by a standard CNN to explore the spatial information from the output of LSTM. Finally, the experimental results demonstrate the effectiveness of the proposed model on the NTU RGB+D, SBU Interaction and UTD-MHAD dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						90	100		10.1016/j.neucom.2020.07.068													
J								Collaborative Generative Adversarial Network with Visual perception and memory reasoning	NEUROCOMPUTING										Generative adversarial network; Attention mechanism; Data augmentation; Object simulation; Cooperation mechanism	SALIENT OBJECT DETECTION	In order to address such negative issues as GAN's mediocre image quality, high-demand for training samples and computation resources, this paper proposes the collaborative Generative Adversarial Network with Visual perception and memory reasoning (ESA-CGAN). This not only makes use of the vision self-attention mechanism and objects salience model to analyze the global information and detailed features of objects, but also designs cross-correlation self-attention module so as to make a balance between the computation efficiency and computational on the one hand and the statistical efficiency and the ability to simulate remote dependencies on the other hand. Based on convolutional long-term and short-term memory network, this paper optimizes the attention feature map so as to highlight the features of objects themselves and improve their generative abilities. Meanwhile, a cooperative learning mechanism between generators is designed, which combines self-constructed generation model and pre-training generation model to form a generation model group. It not only improves effectively the generation ability of the model, improve the computing efficiency, but also restrains the collapse of the model from another angle. In fact, the model proposed here has completed numerical experiments on multiple common standard datasets and self-configuring datasets, and has made comparisons with several mainstream generation antagonistic network models in terms of the performance of image data augmentation. The experimental results demonstrate that the model has excellent simulation ability to enable itself to effectively realize the augmentation of data, thus making it highly applicable in the future. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						101	119		10.1016/j.neucom.2020.06.037													
J								Multi-scale and multi-branch feature representation for person re-identification	NEUROCOMPUTING										Person re-identification; Computer vision; Deep learning; Multiple scale	NETWORK	Multi-scale feature fusion has been proven effective in substantial person re-identification (ReID) works. However, the existing multi-scale feature fusion is based on features of different semantic levels. We propose a novel multi-scale and multi-branch feature representation for person ReID, namely Ms-Mb. It merges the features of the same semantic level and integrates attention modules to learn robust and representative feature representations. Through the heterogeneous losses supervision, the final feature representation of the image is more discriminative for person ReID. Sufficient ablation study has proven that the multi-scale feature fusion, the attention module and heterogeneous losses training strategy contribute to the performance boost of Ms-Mb. We have conducted experiments on four mainstream benchmarks including Market1501, DukeMTMC-relD, CUHK03 and MSMT17. Extensive experimental results show that our approach Ms-Mb achieves state-of-the-art performances on Market1501 (Rank-1 = 95.8%, mAP = 89.9%), DukeMTMC-relD (Rank-1 = 90.8%, mAP = 82.2%) and MSMT17 (Rank-1 = 81.9%, mAP = 59.3%) without using additional external data or re-ranking. Our approach yields competitive results compared to the state-of-the-art method on CUHK03 (Rank-1 = 75.4%, mAP = 72.9%) and surpasses the other methods by a large margin. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						120	130		10.1016/j.neucom.2020.06.074													
J								Finite/fixed-time synchronization for Markovian complex-valued memristive neural networks with reaction-diffusion terms and its application	NEUROCOMPUTING										Finite/fixed-time synchronization; CVMNNs; Reaction-diffusion terms; Markovian jump parameters; Image encryption	EXPONENTIAL STABILITY; STABILIZATION; MAP	This paper pays attention to the synchronization issue of complex-valued memristive neural networks (CVMNNs) that contain reaction-diffusion terms and Markovian jump parameters. To better meet the needs of some practical projects, the problem of synchronization investigated in this paper is defined in finite time and fixed time; namely, by designing a suitable controller, the synchronization error system can converge to zero in a finite/fixed time. Additionally, by combining Lyapunov-Krasovskii functional theory and algebraic inequality methods, a novel criterion of finite/fixed-time synchronization for proposed drive and response systems is derived. Note that the finite-time and fixed-time synchronization criteria are integrated into a unified theorem. Finally, two examples are provided, one is a simple numerical example to account for the feasibility of main results, the other realizes the application of this paper in establishing a spatiotemporal chaotic cryptosystem for image encryption, and the proposed cryptosystem has been illustrated that it has obvious advantages of large key space and high security by simulation results. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 13	2020	414						131	142		10.1016/j.neucom.2020.07.024													
J								Improving convolutional neural network for text classification by recursive data pruning	NEUROCOMPUTING										Data pruning; Convolutional neural network; Text classification	SENTIMENT ANALYSIS	In spite of the state-of-the-art performance of deep neural networks, shallow neural networks are still the choice in applications with limited computing and memory resources. Convolutional neural network (CNN), in particular the one-convolutional-layer CNN, is a widely-used shallow neural network in natural language processing tasks such as text classification. However, it was found that CNNs may misfit to task-irrelevant words in dataset, which in turn leads to unsatisfactory performance. To alleviate this problem, attention mechanism can be integrated into CNN, but this takes up the limited resources. In this paper, we propose to address the misfitting problem from a novel angle - pruning task-irrelevant words from the dataset. The proposed method evaluates the performance of each convolutional filter based on its discriminative power of the feature generated at the pooling layer, and prunes words captured by the poorly-performed filters. Experiment results show that our proposed model significantly outperforms the CNN baseline model. Moreover, our proposed model produces performance similar to or better than the benchmark models (attention integrated CNNs) while demanding less parameters and FLOPs, and is therefore a choice model for resource limited scenarios, such as mobile applications. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						143	152		10.1016/j.neucom.2020.07.049													
J								Attention-based bidirectional gated recurrent unit neural networks for well logs prediction and lithology identification	NEUROCOMPUTING										GRU neural network; Attention mechanism; Well logs prediction; Lithology identification	LITHOFACIES; MACHINE; SHALE; ROCKS; FIELD; GULF	Many old oilfields have missed or distorted well logs data, which is due to long history of shutdown, poor borehole conditions, damaged instrument, and other reasons. These bring great difficulties to redevelopment of oil and gas field. In this study, a novel method was proposed to complete well logs prediction and quantitative lithology identification. A new model based on gate recurrent unit (GRU) neural network and attention mechanism was constructed in our study. A bidirectional GRU network was designed to extract key features from forward and backward well logs data along depth direction, and attention mechanism was introduced to assign different weights to each hidden layer to improve prediction accuracy. We fully consider the trend of well logs data with depth, the correlation of different log series and the actual depth accumulation effect. Compared with other four traditional intelligent models, experiments on actual reservoir dataset show that the constructed model can obtain lower error and higher fitting degree. Implementation of the proposed method can serve as an economical and reliable alternative for oil and gas industry and provides fast and effective data for further geological research combined with artificial intelligence. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 13	2020	414						153	171		10.1016/j.neucom.2020.07.026													
J								A modified Lanczos Algorithm for fast regularization of extreme learning machines	NEUROCOMPUTING										Extreme Learning machines; Lanczos Algorithm; Regularization; Neural Networks; Regression; Classification	REGRESSION; SYSTEMS; EQUATIONS; ELM	This paper presents a new regularization for Extreme Learning Machines (ELMs). ELMs are Randomized Neural Networks (RNNs) that are known for their fast training speed and good accuracy. Nevertheless the complexity of ELMs has to be selected, and regularization has to be performed in order to avoid under-fitting or overfitting. Therefore, a novel Regularization is proposed using a modified Lanczos Algorithm: Iterative Lanczos Extreme Learning Machine (Lan-ELM). As summarized in the experimental Section, the computational time is on average divided by 4 and the Normalized MSE is on average reduced by 11%. In addition, the proposed method can be intuitively parallelized, which makes it a very valuable tool to analyze huge data sets in real-time. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						172	181		10.1016/j.neucom.2020.07.015													
J								Character-level neural network model based on Nadam optimization and its application in clinical concept extraction	NEUROCOMPUTING										Deep neural networks; Natural language processing; Clinical concept extraction; Named entity recognition	CROWD EVACUATION	Clinical concept extraction aims to quickly and effectively extract available data from complex and diverse clinical information, which is a crucial task for medical diagnosis using electronic medical records. Named entity recognition (NER) accurately marks essential information in clinical records based on the characteristics of the target entity, providing a way to extract clinical concepts. In the clinical concept extraction task, the existing methods are not satisfactory to obtain accurate labelling results in the face of large-scale and complex clinical information. To solve this problem, we improve and optimize a named entity recognition method based on the LSTM-CRF model. First, the improved deep neural network model uses two optional configurations of Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BLSTM) to achieve character-level representation. Then the BLSTM layer obtains the context information of the target word, and the Conditional Random Field (CRF) gives constraints to ensure the standardization of the label. On this basis, Nadam is used to optimize the training process of the network. The experimental results show that our new method has an F1 score of 84.61 on the public dataset of 2010 i2b2/VA concept extraction task, which exceeds the LSTM-CRF model. And the recall of 85.41 is ahead of all the methods evaluated on this data set. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						182	190		10.1016/j.neucom.2020.07.027													
J								Detector neural network vs connectionist ANNs	NEUROCOMPUTING										Connectionist artificial neural network; Detector neural network; Model of dendritic tree of neuron; Neuron-detector; Counter learning	PLASTICITY	Most widely used modern artificial neural networks are based on the connectionist paradigm of building and learning. The authors propose an alternative detector approach. The basis of this approach is the original architecture of the neural network, as well as a new procedure for its learning. The developed neural network is called the detector neural network. This network consists of two layers of neurons. The neurons of the first layer are called neurons-pre-detectors and they do not learn. They are designed to highlight the structural elements of recognizable images, as well as to determine their measured parameters. The types of structural elements and their parameters are set a priori and depend on the type and complexity of recognizable images. Neurons of the second layer can be trained. They recognize individual complex images. These neurons are called neurons-detectors (ND). The model of the ND is significantly different from all known models of neurons and has important features: the presence of a dendritic tree model, a new looks at the role and value of synaptic coupling coefficients, an original approach to the formation of a neuron reaction. The training procedure for the ND is called counter learning. In the process of counter learning, an information model - template of the most simplified image structure of a particular class is formed and remembered by the ND. This template is called a concept. The main role in the formation of the concept is played by the model of the dendritic tree of the neuron. During the classification process, the ND tries to simplify the recognizable image until it coincides with the concept. The article provides a comparative analysis of the detector approach and the connectionist paradigm. The advantages of the detector approach, according to the authors, open up new possibilities in the study of the problem of constructing Artificial General Intelligence. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						191	203		10.1016/j.neucom.2020.07.025													
J								Delay-dependent-stability of stochastic delay coupled systems on networks with regime-switching-diffusions	NEUROCOMPUTING										regime-switching-diffusions; delay-dependent-stability; Lyapunov functional method; asymptotical stability	FINITE-TIME SYNCHRONIZATION; BAM NEURAL-NETWORKS; MULTI-GROUP MODELS; DISPERSAL	In this paper, delay-dependent-stability of stochastic delay coupled systems on networks with regime-switching-diffusions (SDCSRD) is investigated. Here, regime-switching-diffusions and time-varying delay are both considered into stochastic coupled systems on networks. Combining Lyapunov functional method with graph theory, several delay-dependent-stability criteria are derived to ensure H-infinity-stability and asymptotical stability in L-p of SDCSRD. Particularly, as a concrete application of the theoretical results, we study the delay-dependent asymptotical stability in mean square of stochastic delay coupled oscillators with regime-switching-diffusions. Ultimately, two numerical examples are provided to exhibit the effectiveness of the results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						204	214		10.1016/j.neucom.2020.07.017													
J								MGCM: Multi-modal generative compatibility modeling for clothing matching	NEUROCOMPUTING										Multi-modal information; Compatible template generation; Generative compatibility modeling		With the recent prevalence of online fashion-oriented communities and advances in multimedia processing, increasing research interests have been paid to the fashion compatibility modeling, where the compatibility between complementary fashion items (e.g., a top and a bottom) can be assessed automatically. Existing fashion compatibility modeling techniques mainly focus on measuring the compatible preference between fashion items with Deep Neural Networks (DNN), but overlook the generative compatibility modeling. Differently, in this paper, we explore the potential of the Generative Adversarial Network (GAN) in fashion compatibility modeling and thus propose a Multi-modal Generative Compatibility Modeling (MGCM) scheme. In particular, we introduce a multi-modal enhanced compatible template generation network, regularized by the pixel-wise consistency and template compatibility, to sketch a compatible template as the auxiliary link between fashion items. Accordingly, MGCM is able to measure the compatibility between complementary fashion items comprehensively from both item-item and item-template perspectives. Experimental results on two real-world datasets demonstrate the superiority of the proposed scheme over state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						215	224		10.1016/j.neucom.2020.06.033													
J								Neural network method for fractional-order partial differential equations	NEUROCOMPUTING										Fractional partial differential equations; Neural network method; Fractional heat conduction equation; Fractional wave equation	APPROXIMATIONS; INTELLIGENCE; SPACE	In this paper, neural network method is first proposed to solve the fractional-order partial differential equations. The neural network based on the sine and the cosine functions is established on the sample points which are evenly distributed in the solution area. In view of the fractional heat conduction equation and the fractional wave equation, the training error function is established, and the related algorithm for the initial boundary value problem of the above equations is given. Then, the learning rate of the neural network is analyzed. Finally, several numerical examples are given to verify the effectiveness of solving the fractional-order partial differential equations by the present method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						225	237		10.1016/j.neucom.2020.07.063													
J								Optimal state estimation for finite-field networks with stochastic disturbances	NEUROCOMPUTING										Finite-field network; State estimation; Multi-valued Kalman filter; Mean square error	MULTIAGENT SYSTEMS; CONSENSUS NETWORKS; TIME-DELAYS; STABILITY; FILTERS	This paper investigates the optimal state estimation of finite-field networks (FFNs) with stochastic disturbances. The stochastic disturbances are regarded as independent noise process. The optimal state estimation problem of FFNs in the sense of minimum mean square error (MMSE) is proposed, and a necessary and sufficient condition is proposed to find the optimal state estimator by minimizing the mean square error (MSE). Moreover, the multi-valued Kalman filter is established, including prediction step and update step, based on which, both optimal state estimator and MMSE can be calculated effectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						238	244		10.1016/j.neucom.2020.07.065													
J								Remaining useful life prediction of lithium-ion battery with optimal input sequence selection and error compensation	NEUROCOMPUTING										Remaining useful life prediction; Error compensation; Phase space reconstruction; Ensemble empirical mode decomposition; Support vector regression; Genetic algorithm	VECTOR REGRESSION; HEALTH ESTIMATION; PROGNOSTICS; SYSTEMS; STATE	Accurate prediction of remaining useful life (RUL) for lithium-ion battery (LIB) plays a key role in increasing the reliability and safety of battery related industries and facilities. In this paper, RUL prediction of LIB is investigated by employing a hybrid data-driven method based support vector regression (SVR) and error compensation (EC). Firstly, two health indicators (Hls) are established by using capacity and discharging voltage difference of equal time interval (DVD), respectively. Secondly, the ensemble empirical mode decomposition (EEMD) is adopted to preprocess the obtained Hls, which is used to reduce the influence of capacity regeneration and noise. Especially, phase space reconstruction (PSR) with C-C technique is introduced to achieve optimal input sequence selection pattern, it has an important influence on the accuracy of SVR prediction. As an important innovation of the paper, the idea of EC is implemented by combining the predictions of both forecast error and RUL prediction with PSR-SVR. Last but not least, the genetic algorithm (GA) is utilized to optimize the key parameters of SVR so as to achieve more accurate RUL prediction. To verify the effectiveness of the proposed approach, the real data set of LIBs from National Aeronautics and Space Administration (NASA) is carried out, and the dominant is emphasized by comparison with other important methods. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 13	2020	414						245	254		10.1016/j.neucom.2020.07.081													
J								Simplified and yet Turing universal spiking neural P systems with polarizations optimized by anti-spikes	NEUROCOMPUTING										Natural computing; Membrane computing; Neural computation; Spiking neural P system; Computationally complete		Spiking neural P systems with polarizations (PSN P systems) are a class of neural-inspired computation models, where the firing condition of rules is the neuron-associated polarization. It has previously been shown that PSN P systems are Turing universal by using tree types of polarizations, and 164 neurons are needed for constructing a Turing universal PSN P system as a function computing device. In this work, in order to answer the open problem whether this determination mechanism of polarizations can be simplified without the loss of computation power, one more type of object for information encoding, i.e., the anti-spike, is introduced into PSN P systems, thus, PSN P systems with anti-spikes are proposed, abbreviated as PASN P systems. It is proved that two types of polarizations are enough to guarantee the Turing universality of PASN P systems both as number generators and number acceptors. Furthermore, it is demonstrated that 121 neurons are sufficient for a PASN P system with two types of polarizations to achieve universality as a function computing device. These results manifest that anti-spikes are a powerful ingredient of PASN P systems to yield the improvement in computation performance and the reduction in the description complexity necessary to achieve Turing universality. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						255	266		10.1016/j.neucom.2020.07.051													
J								Output-feedback formation tracking control of networked nonholonomic multi-robots with connectivity preservation and collision avoidance	NEUROCOMPUTING										Distributed formation tracking; Connectivity preservation; Collision avoidance; Neural network; Networked mobile robots	DISTRIBUTED FORMATION TRACKING; UNDERWATER VEHICLES SUBJECT; VARYING FORMATION CONTROL; CONSENSUS CONTROL; SURFACE VESSELS; SYSTEMS	This paper studies the problem of adaptive output-feedback formation tracking control for networked uncertain nonholonomic mobile robots (NMRs) with different limited communication distances, while achieving connectivity preservation and collision avoidance. In robot control, equipping all the speed sensors not only increases the cost of the system but also risks losing accuracy and reliability. In this paper, it is assumed that the velocities of the nonholonomic mobile robots in this paper are unknown, they are estimated according to the output of the systems by an adaptive observer via a neural network. The designed dynamic surface based on nonlinear transformation errors can achieve the above three control objectives at the same time, avoiding the situation of using multiple potential functions to solve such problems, which will lead to conflicts in the selection of the design parameters. Compared with the existing references about formation tracking to maintain connectivity and avoid collisions, the main contribution of this paper is to design an observer of robot speed estimation and use only a neural network to estimate the unknown nonlinear term of the system itself. The unknown nonlinearity produced in the process of controller design does not need an additional neural network. Furthermore, the effectiveness of the proposed strategy is verified by simulation examples. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						267	277		10.1016/j.neucom.2020.07.023													
J								Blur detection via deep pyramid network with recurrent distinction enhanced modules	NEUROCOMPUTING										Blur detection; Feature pyramid network; Distinction enhanced block; Boundary-aware penalty; Deep learning		Blur detection aims to detect the regions where the image is blur and identifies the blurred regions accurately. Since the traditional hand-crafted feature based methods usually are not robust enough to handle various complex scenarios, the study of blur detection problem remains to be a challenging task in terms of the accuracy and effectiveness of blur separation. In this paper, instead of seeking a new hand-crafted feature often seen in common approaches, we present a new deep feature-learning method by using feature pyramid network via recurrent Distinction Enhanced Block modules to solve the blur detection problem. We show that the newly introduced Distinction Enhanced Block is able to merge the high-level semantic information with the low-level details effectively, while at the same time, it is capable to keep a desirable identification between the blurry and clear regions. We also develop a new boundary penalty term to refine the boundaries of our results, which leads to more accurate detection and segmentation, compared with current available results in literature. Furthermore, due to the lacking of public datasets for blur detection problems, we have established a new blur detection dataset which may enrich the experimental benchmarks for testing. In terms of efficiency associated with the competitive performance, in particular, our model is shown to have best-in-class evaluation speed, about 0.14 s to evaluate an input image in regular personal laptop. The performance evaluations on both CUHK dataset and our SZU-BD dataset validate that the proposed model outperforms various existing state-of-the-arts methods through commonly used metric indexes. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						278	290		10.1016/j.neucom.2020.06.068													
J								Scene perception guided crowd anomaly detection	NEUROCOMPUTING										Video surveillance; Crowd anomaly detection; Scene perception; Fluid forces	SMOOTHED PARTICLE HYDRODYNAMICS; ABNORMAL EVENT DETECTION; BEHAVIOR DETECTION; VISUAL TRACKING; LOCALIZATION; MODEL; FLOW	Crowd anomaly detection has been a research hotspot in the field of video surveillance in recent years. In most existing methods, the accuracy of anomaly detection dominantly relies on the acquisition of regions of interest (ROI) and feature extraction. However, the randomness of ROI segmentation and crowd group selection usually cannot guarantee a robust performance and thus may lead to false detection sometimes. To address these issues, this paper proposes a scene perception-based approach combining the fluid forces expression and psychological theory. The proposed method firstly introduces a flow field Visualization technology called line integral convolution to segment the moving pedestrians in the scene. Then, a scene perception-guided clustering strategy is proposed to cluster the consistency crowd group. Scene perception strategy is in line with the psychological criteria of human cognition. In clustering, it makes more reasonable use of various attributes of the pedestrians. To ensure a robust detection of the pedestrian group, we propose a fluid feature concept which considers both mass force and surface force. For each consistency group, two types of features including the image appearance feature and fluid feature are combined to describe pedestrian motion. The experimental results show that the proposed method achieves higher accuracy in comparison with some existing methods in terms of both frame-level and pixel-level measurements. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						291	302		10.1016/j.neucom.2020.07.019													
J								Person re-identification from virtuality to reality via modality invariant adversarial mechanism	NEUROCOMPUTING										Person re-identification; Cross-model; Space transformation; Adversarial		Person re-identification based on multi-style images helps in crime scene investigation, where only a virtual image (sketch or portrait) of the suspect is available for retrieving possible identities. However, due to the modality gap between multi-style images, standard model of person re-identification cannot achieve satisfactory performance when directly applied to match the virtual images with the real photographs. To address this problem, we propose a modality invariant adversarial mechanism (MIAM) to remove the modality gap between multi-style images. Specifically, the MIAN consists of two parts: a space transformation module to transfer the multi-style person images to a modality-invariant space, and an adversarial learning module "played" between the category classifier and modality classifier to steer the representation learning. The modality classifier discriminates between the real and virtual images while the category classifier predicts the identities of the input transformed images. We explore the space transformation for data augmentation to further bridge the modality gap and facilitate the performance. Furthermore, we build two new datasets for the multi-style Re-ID to evaluate the performance. Extensive experimental results demonstrate the effectiveness of the proposed method on improving the performance against the existing feature learning networks. Further comparison results conducted on different modules in MIAM show that our approach is of favorable generalization ability on alleviating the modality gap to improve the multi-style Re-ID. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						303	312		10.1016/j.neucom.2020.06.075													
J								Hybrid of human learning optimization algorithm and particle swarm optimization algorithm with scheduling strategies for the flexible job-shop scheduling problem	NEUROCOMPUTING										Human learning algorithm; Adaptive learning system; Particle swarm optimization algorithm; Flexible job-shop scheduling problem; Scheduling strategy; Operations research	SEQUENCE-DEPENDENT SETUP; GENETIC ALGORITHM; ANT COLONY; SEARCH; RULES	The flexible job-shop scheduling problem (FJSP) is a well-known combinational optimization problem. Studying FJSP is essential for promoting production efficiency and effectiveness. Different kinds of improved particle swarm optimization (PSO) algorithms have produced superior results for FJSP in the last few decades. Meanwhile, the human learning optimization (HLO) algorithm, a simple and adaptive learning algorithm for learning system, has helped improve algorithm performance by imitating human learning behavior in recent research. The study proposes a hybrid HLO-PSO algorithm, which utilizes various combinations of the proposed improved PSO and proposed scheduling strategies to solve FJSP under the algorithm architecture of HLO. With the guidance of HLO, the individual learning ability of every particle is further promoted based on the existed advantage of collective action decision of PSO; and with the help of rule-based scheduling strategies, the search capacity of the proposed improved PSO is also further enhanced. By the detailed exposition and analysis, the proposed HLO-PSO is easily implemented and embedded in other production system software or learning system software. Meanwhile, by using it to solve several groups of FJSP instances, the result comparisons with other related algorithms reveal that HLO-PSO can efficiently solve most of single-objective FJSP. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						313	332		10.1016/j.neucom.2020.07.004													
J								Cross-modal image fusion guided by subjective visual attention	NEUROCOMPUTING										Image fusion; Subjective attention; Top-down subjective task; Multi-task auxiliary learning; Deep learning	QUALITY ASSESSMENT; PERFORMANCE; FRAMEWORK	The human visual perception system has very strong robustness and contextual awareness in a variety of image processing tasks. This robustness and the perception ability of contextual awareness is closely related to the characteristics of multi-task auxiliary learning and subjective attention of the human visual perception system. In order to improve the robustness and contextual awareness of image fusion tasks, we proposed a multi-task auxiliary learning image fusion method guided by subjective attention. The image fusion method effectively unifies the subjective task intention and prior knowledge of human brain. In order to achieve our proposed image fusion method, we first analyze the mechanism of multi-task auxiliary learning, build a multi-task auxiliary learning network. Secondly, based on the human visual attention perception mechanism, we introduce the human visual attention network guided by subjective tasks on the basis of the multi-task auxiliary learning network. The subjective intention is introduced by the subjective attention task model, so that the network can fuse images according to the subjective intention. Finally, in order to verify the superiority of our image fusion method, we carried out experiments on the combined vision system image data set, and the infrared and visible image data set for experimental verification. The experimental results demonstrate the superiority of our fusion method over state-of-arts in contextual awareness and robustness. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						333	345		10.1016/j.neucom.2020.07.014													
J								Ultrasound image de-speckling by a hybrid deep network with transferred filtering and structural prior	NEUROCOMPUTING										US image de-speckling; Transfer learning; Gaussian distribution prior; Structural prior; Hybrid neural network	QUANTITATIVE-ANALYSIS; ALGORITHM; NOISE	Deep neural-network has been widely used in natural image denoising. However, due to the lack of label of real ultrasound (US) B-mode image for de-speckling, the deep neural network is greatly restricted in US image de-speckling. In this paper, we propose to use transfer learning and two types of prior knowledge to construct a hybrid neural network structure for de-speckling. Firstly, based on a given US image model, the speckle noise is similar to Gaussian distribution in the logarithmic transformation domain, called Gaussian prior knowledge. The distribution parameters are estimated in the logarithmic transformation domain based on four typical traditional US image de-speckling methods with maximum likelihood estimation. Secondly, depending on the prior parameters, a transferable denoising network is trained with clean natural image dataset. Finally, a VGGNet is used to extract the structure boundaries before and after US image de-speckling based on the transfer network, and we call it structural prior knowledge. The structural boundaries of a US image should be unchanged after the de-speckling, and hence we use this constraint to fine-tune the transfer network. The proposed de-speckling framework is verified on artificially generated phantom (AGP) images and real US images, and the results demonstrate its effectiveness. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						346	355		10.1016/j.neucom.2020.09.002													
J								Poly-GAN: Multi-conditioned GAN for fashion synthesis	NEUROCOMPUTING										Generative adversarial networks; Image alignment; Image stitching; Fashion synthesis		We present Poly-GAN, a novel conditional GAN architecture that is motivated by Fashion Synthesis, an application where garments are automatically placed on images of human models at an arbitrary pose. Poly-GAN allows conditioning on multiple inputs and is suitable for many tasks, including image alignment, image stitching and inpainting. Existing fashion synthesis methods have a similar pipeline where three different networks are used to first align garments with the human pose, then perform stitching of the aligned garment and finally refine the results. Poly-GAN is the first instance where a common architecture is used to perform all three tasks. Our novel architecture enforces the conditions at all layers of the encoder and utilizes skip connections from the coarse layers of the encoder to the respective layers of the decoder. Poly-GAN is able to perform a spatial transformation of the garment based on the RGB skeleton of the model at an arbitrary pose. Additionally, Poly-GAN can perform image stitching, regardless of the garment orientation, and inpainting on the garment mask when it contains irregular holes. Our system achieves state-of-the-art quantitative results on Structural Similarity Index metric and Inception Score metric using the DeepFashion dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						356	364		10.1016/j.neucom.2020.07.092													
J								Adaptive algorithms for synchronization, consensus of multi-agents and anti-synchronization of direct complex networks	NEUROCOMPUTING										Adaptive; Distributed algorithm; Consensus; Synchronization; Anti-synchronization	NEURAL-NETWORKS; SYSTEMS	In this paper, we discuss distributed adaptive algorithms for synchronization of complex networks, consensus of multi-agents with or without pinning controller. The dynamics of individual node is governed by generalized QUAD condition. We design new algorithms, which can keep the left eigenvector of the adaptive coupling matrix corresponding to the zero eigenvalue invariant. Based on this invariance, various distributive adaptive synchronization, consensus, anti-synchronization models are given. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 13	2020	414						365	370		10.1016/j.neucom.2020.07.095													
J								A user-based aggregation topic model for understanding user's preference and intention in social network	NEUROCOMPUTING										User's preference and intention; RNN; Topic model; Social network		In this study, we focus on understanding and mining user's preferences and intentions via user-based aggregation in the context of a social network. Understanding preference and intention in microblog texts is more difficult and challenging than understanding such characteristics in the context of standard text. The main reason is that search history and click history are difficult to obtain due to data privacy in social networks. Meanwhile, the text is sparse, and the number of background topics in social networks is enormous. To overcome the above challenges, we explore an indirect method of user's preference and intention understanding by leveraging a user-based aggregation topic model (UATM). Our UATM aims to mine the distributions of user's preferences and intentions by utilizing user's preference and intention distributions and followees' preference and intention distributions. Furthermore, to alleviate the sparsity problem, we discriminatively model common words and topic words and incorporate a user factor into our model. We combine the recurrent neural network (RNN) and inverse document frequency (IDF) as the weight prior to learn word relationships. Moreover, to further weaken the sparsity of context, we leverage word pairs to model topics for all documents. We also propose a collapsed Gibbs sampling algorithm to infer preference and intention in our UATM. To verify the effectiveness of the proposed method, we collect a Sina Weibo dataset consisting of microblog users and their pushed content to conduct various experiments. Both qualitative and quantitative evaluations demonstrate that our proposed UATM model outperforms several state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						1	13		10.1016/j.neucom.2020.06.099													
J								Event-triggered consensus control of high-order multi-agent systems with arbitrary switching topologies via model partitioning approach	NEUROCOMPUTING										Consensus; Multi-agent systems; Switching topologies; Event-triggered control		This paper investigates the problem of event-triggered consensus control for high-order multi-agent systems with arbitrary switching topologies. To achieve this goal, a novel model partitioning approach is proposed, where the high-order error systems are partitioned into stable subsystems and unstable subsystems through coordinate transformations. By analyzing the unstable subsystems under arbitrary switching, an appropriate controller can be designed to guarantee the subsystems stable. Then the controller gain of the multi-agent systems can be derived through inverse transformations. The designed controller can guarantee the error state convergence under each topology within any time interval, which further ensures the consensus of high-order multi-agent systems under arbitrary switching topologies. Finally, a numerical example is provided to illustrate the feasibility of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						14	22		10.1016/j.neucom.2020.06.058													
J								Hybrid deep neural networks for recommender systems	NEUROCOMPUTING										Recommder systems; Deep Neural Networks; Sparsity; Soft logic; Semi-supervised learning		More recently, deep neural networks received much attention from the recommender systems community where CNNs and RNNs were applied in different recommendation contexts and achieved state-of-the-art results on many publicly available benchmarks. While the success of the neural models came from their high expressiveness, the uninterpretability problem still one of their main drawbacks, which can have a negative side effect on the whole learning process. A good candidate to reduce the uninterpretability is using Probabilistic Soft Logic (PSL), which showed a strong performance in dealing with this problem. Moreover, sparsity is another major problem of the most previous recommendation systems. In this paper, we introduce a new recommender system framework based on the generalized distillation principle that combines two modeling approaches: PSL for the knowledge-driven modeling approach and deep neural networks for the data-driven approach. Experimental results on publicly available data-sets show that our method significantly outperforms the previous state-of-the-art based on deep neural networks emphasizing the utility of PSL rules in handling inconsistencies, reducing the uninterpretability of the neural models and shows promising results in dealing with sparsity. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						23	30		10.1016/j.neucom.2020.06.025													
J								Image captioning with semantic-enhanced features and extremely hard negative examples	NEUROCOMPUTING										Image captioning; Image-text matching; Negative examples		Image captioning is a task to generate natural descriptions of images. In existing image captioning models, the generated captions usually lack semantic discriminability. Semantic discriminability is difficult as it requires the model to capture detailed differences in images. In this paper, we propose an image captioning framework with semantic-enhanced features and extremely hard negative examples. These two components are combined in a Semantic-Enhanced Module. The semantic-enhanced module consists of an image-text matching sub-network and a Feature Fusion layer which provides semantic-enhanced features of rich semantic information. Moreover, in order to improve the semantic discriminability, we propose an extremely hard negative mining method which utilize the extremely hard negative examples to improve the latent alignment between visual and language information. Experimental results on MSCOCO and Flickr30K show that our proposed framework and training method can simultaneously improve the performance of image-text matching and image captioning, achieving competitive performance against state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						31	40		10.1016/j.neucom.2020.06.112													
J								PCSGAN: Perceptual cyclic-synthesized generative adversarial networks for thermal and NIR to visible image transformation	NEUROCOMPUTING										Image transformation; Thermal; Near-InfraRed; Perceptual loss; Adversarial loss; Cyclic-synthesized loss; Generative adversarial network	TRANSLATION	In many real world scenarios, it is difficult to capture the images in the visible light spectrum (VIS) due to bad lighting conditions. However, the images can be captured in such scenarios using Near-Infrared (NIR) and Thermal (THM) cameras. The NIR and THM images contain the limited details. Thus, there is a need to transform the images from THM/NIR to VIS for better understanding. However, it is non-trivial task due to the large domain discrepancies and lack of abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to transform the images from one domain to another domain. Most of the available GAN based methods use the combination of the adversarial and the pixel-wise losses (like L-1 or L-2) as the objective function for training. The quality of transformed images in case of THM/NIR to VIS transformation is still not up to the mark using such objective function. Thus, better objective functions are needed to improve the quality, fine details and realism of the transformed images. A new model for THM/NIR to VIS image transformation called Perceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is introduced to address these issues. The PCSGAN uses the combination of the perceptual (i.e., feature based) losses along with the pixel-wise and the adversarial losses. Both the quantitative and qualitative measures are used to judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR scene datasets. The proposed PCSGAN outperforms the state-of-the-art image transformation models, including Pix2pix, DuaIGAN, CycIeGAN, PS2GAN, and PAN in terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is available at https://github.com/KishanKancharagunt a/PCSGAN. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						41	50		10.1016/j.neucom.2020.06.104													
J								vtGraphNet: Learning weakly-supervised scene graph for complex visual grounding	NEUROCOMPUTING										Visual grounding; Referring expression comprehension	LABEL IMAGE CLASSIFICATION; ATTENTION	As a challenging cross-modal task, current visual grounding is usually addressed by directly analyzing the unstructured scene and matching the query text with all region proposals, which is prone to errors, especially when the scene and/or query text are complex. In this paper, we study such complex visual grounding problem and propose to build a query dependent visual-textual (VT) scene graph to jointly understand the image and query text. To avoid the difficulty of obtaining ground-truth scene graphs, we propose vtGraphNet to effectively learn the bi-modal scene graph in a weakly-supervised way, where the only supervision is the manually annotated grounding region. Specifically, we first use an ARU Tagging model to sequentially tag every query word as either an attribute, a relationship or an auxiliary. If a word is tagged as attribute, we develop an attribute-assigning model to associate it to a region proposal. If a word is tagged as relationship, we develop a relationship-referring model to associate it to a pair of region proposals. A simple yet effective graph consistency loss function is constructed to constrain the above associations to form a feasible compact VT scene graph, from which discriminative region features can be extracted and used to locate the grounding object by classification. Extensive experiments on benchmark datasets validate the superiority of our approach in handling both simple and complex visual grounding tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						51	60		10.1016/j.neucom.2020.06.091													
J								Weakly-supervised multi-label learning with noisy features and incomplete labels	NEUROCOMPUTING										Multi-label learning; Weakly-supervised; Noisy features and incomplete labels; Low-rank and sparse decomposition; Label correlations		Weakly-supervised multi-label learning has emerged as a hot topic more recently. Most existing methods deal with such problem by learning from the data where the label assignments are incomplete while the feature information is ideal. However, in many real applications, due to the influence of occlusion, illumination and low-resolution, the acquired features are often noisy, which may reduce the robustness of the learning model. In this paper, to overcome the above shortcoming, we propose a novel weakly-supervised multi-label learning framework called WML-LSC, where the low-rank and sparse constrain schemes are jointly incorporated to capture the desired feature information. Specifically, we first decompose the observed feature matrix into an ideal feature matrix and an outlier matrix. Considering that similar instances usually share similar visual characteristics, we constrain the ideal feature matrix to be low-rank. Meanwhile, a reasonable assumption is that the noise is sparse compared with the feature matrix, which leads outlier matrix to be sparse. In addition, a linear self-recovery model is adopted to reconstruct the incomplete label assignment matrix by exploiting label correlations. Finally, the desired model is trained on the ideal feature matrix and the refined label matrix. Extensive experimental results demonstrate that our proposed method can achieve superior and comparable performance against state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						61	71		10.1016/j.neucom.2020.06.101													
J								Subspace learning for unsupervised feature selection via adaptive structure learning and rank approximation	NEUROCOMPUTING										Subspace learning; Adaptive structure learning; Rank constraint; Projection matrix; Feature selection	FEATURE SUBSET; REDUNDANCY	Traditional unsupervised feature selection methods usually construct a fixed similarity matrix. This matrix is sensitive to noise and becomes unreliable, which affects the performance of feature selection. The researches have shown that both the global reconstruction information and local structure information are important for feature selection. To solve the above problem effectively and make use of the global and local information of data simultaneously, a novel algorithm is proposed in this paper, called subspace learning for unsupervised feature selection via adaptive structure learning and rank approximation (SLASR). Specifically, SLASR learns the manifold structure adaptively, thus the preserved local geometric structure can be more accurate and more robust to noise. As a result, the learning of the similarity matrix and the low-dimensional embedding is completed in one step, which improves the effect of feature selection. Meanwhile, SLASR adopts the matrix factorization subspace learning framework. By minimizing the reconstruction error of subspace learning residual matrix, the global reconstruction information of data is preserved. Then, to guarantee more accurate manifold structure of the similarity matrix, a rank constraint is used to constrain the Laplacian matrix. Additionally, the l(2,1/2) regularization term is used to constrain the projection matrix to select the most sparse and robust features. Experimental results on twelve benchmark datasets show that SLASR is superior to the six comparison algorithms from the literature. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 6	2020	413						72	84		10.1016/j.neucom.2020.06.111													
J								Internal reinforcement adaptive dynamic programming for optimal containment control of unknown continuous-time multi-agent systems	NEUROCOMPUTING										Optimal containment control; Multi-agent system; Internal reinforcement learning; Adaptive dynamic programming; Neural network	DIFFERENTIAL GRAPHICAL GAMES; TRACKING CONTROL; DISTURBANCE REJECTION; CONSENSUS; NETWORKS; LEADER	In this paper, a novel control scheme is developed to solve an optimal containment control problem of unknown continuous-time multi-agent systems. Different from traditional adaptive dynamic programming (ADP) algorithms, this paper proposes an internal reinforcement ADP algorithm (IR-ADP), in which the internal reinforcement signals are added in order to facilitate the learning process. Then a distributed containment control law is designed for each agent with the internal reinforcement signal. The convergence of this IR-ADP algorithm and the stability of the closed-loop multi-agent system are analyzed theoretically. For the implementation of the optimal controllers, three neural networks (NNs), namely internal reinforcement NNs, critic NNs and actor NNs, are utilized to approximate the internal reinforcement signals, the performance indices and optimal control laws, respectively. Finally, some simulation results are provided to demonstrate the effectiveness of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						85	95		10.1016/j.neucom.2020.06.106													
J								Disturbance observer-based output feedback control for uncertain QUAVs with input saturation	NEUROCOMPUTING										QUAV; State observer; Disturbance observer; Neural network; Input saturation	ATTITUDE TRACKING CONTROL; QUADROTOR; SPACECRAFT; POSITION; VEHICLES; FLIGHT	This paper investigates the problem of disturbance observer-based output feedback control for quadrotor unmanned aerial vehicles with external disturbances, uncertainties and input saturation. Different from the existing results, we propose a novel control algorithm towards attitude system and position system. First, considering that not all signals can be measured directly by sensors, we construct a full-order state observer to estimate the unmeasureable state variables. Second, a nonlinear disturbance observer is designed to estimate the unknown external disturbance encountered by quadrotor during flight. Then, novel second order sliding mode surfaces are proposed to design the attitude controller and position controller, which can effectively attenuate the chattering effect without sacrificing the robustness of the controller. In addition, an auxiliary system is developed to eliminate the influence caused by input saturation. Moreover, by using the Lyapunov stability theory, it is proved that the designed attitude and position controllers can ensure that all signals of the resulting closed-loop system are uniformly bounded. Finally, a simulation example is given to verify the effectiveness of proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						96	106		10.1016/j.neucom.2020.06.096													
J								LILPA: A label importance based label propagation algorithm for community detection with application to core drug discovery	NEUROCOMPUTING										Community detection; Label propagation; Node importance; Node attraction; Label importance; Core drug discovery	NETWORKS; EVOLUTIONARY; INFORMATION	Community is an important feature of complex networks. Many label propagation based algorithms are proposed to detect communities in networks because of their high efficiency, however, most of their results are unstable due to the randomness of the node order of label update and the order of label choice. In this paper, a novel label propagation algorithm, Label Importance based Label Propagation Algorithm (LILPA), is proposed to discover communities by adopting fixed label update order based on the ascending order of node importance, utilizing label importance based on node importance and node attraction when labels are launched to other nodes and employing label update process based on node importance, node attraction and label importance for improving the instability and enhancing its accurate and efficiency. Meanwhile, Core Drug Discovery for Indications (CDDI) is a popular research field in Traditional Chinese Medicine (TCM). Then we apply LILPA in a drug network to discover drug communities and core drugs for treating different indications in TCM. Experimental results on 16 synthetic and 10 real-world networks demonstrate that LILPA obtains better accuracy and stability than state-of-the-art approaches. In addition, LILPA can discover effective core drugs in drug networks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						107	133		10.1016/j.neucom.2020.06.088													
J								Robust optimal feedback control design for uncertain systems based on artificial neural network approximation of the Bellman's value function	NEUROCOMPUTING										Dynamic programming; Differential neural networks; Hamilton-Jacobi-Bellman equation; Min-Max optimal control	UNIVERSAL APPROXIMATION; EQUATION; BOUNDS	In this study, a local approximated solution for the Hamilton-Jacobi-Bellman equation based on differential neural networks is proposed. The approximated Value function is used to obtain a feedback control law for a class of uncertain dynamical systems. The approach to deal with the uncertainties of the dynamical system is a min-max method that yields obtaining a robust-like solution for an optimal control problem. The proposed cost functional is presented in the Bolza form and the necessary and sufficient conditions for getting the min-max optimal solution with the designed robust version of the dynamic programming approach are provided. Moreover, the effect of the neural network approximation is studied. All the analysis considers a smoothness assumption regarding the Value function. The practical stability of the system with the neural network feedback optimal control is formally demonstrated by means of the Lyapunov method. Finally, the performance of the control law is compared in simulation with a classical optimal controller. The proposed controller overcomes the results produced by the classical controller, which is confirmed by the functional evaluation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						134	144		10.1016/j.neucom.2020.06.085													
J								SAANet: Siamese action-units attention network for improving dynamic facial expression recognition	NEUROCOMPUTING										Facial expression recognition; Metric Learning; Action-units attention; Sampling strategy		Facial expression recognition (FER) has a wide variety of applications ranging from human-computer interaction, robotics to health care. Although FER has made significant progress with the success of Convolutional Neural Network (CNN), it is still challenging especially for the video-based FER due to the dynamic changes in facial actions. Since the specific divergences exists among different expressions, we introduce a metric learning framework with a siamese cascaded structure that learns a fine-grained distinction for different expressions in video-based task. We also develop a pairwise sampling strategy for such metric learning framework. Furthermore, we propose a novel action-units attention mechanism tailored to FER task to extract spatial contexts from the emotion regions. This mechanism works as a sparse self-attention fashion to enable a single feature from any position to perceive features of the action-units (AUs) parts (eyebrows, eyes, nose, and mouth). Besides, an attentive pooling module is designed to select informative items over the video sequences by capturing the temporal importance. We conduct the experiments on four widely used datasets (CK+, Oulu-CASIA, MMI, and AffectNet), and also do experiment on the wild dataset AFEW to further investigate the robustness of our proposed method. Results demonstrate that our approach outperforms existing state-of-the-art methods. More in details, we give the ablation study of each component. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						145	157		10.1016/j.neucom.2020.06.062													
J								Noise-tolerant neural algorithm for online solving time-varying full-rank matrix Moore-Penrose inverse problems: A control-theoretic approach	NEUROCOMPUTING										Recurrent neural network model; Time-varying full-rank matrix; Moore-Penrose inverse problems; Exponential convergence; Redundant robot manipulators	OPTIMIZATION; REDUNDANT; NETWORK	In this paper, zeroing neural network models are redesigned and analyzed from a control-theoretical framework for online solving time-varying full-rank Moore-Penrose inversions. To solve time-varying full-rank Moore-Penrose inverse problems with different noises in real time, some modified zeroing neural network models are developed, analyzed and investigated from the perspective of control. Furthermore, the proposed zeroing neural network models globally converge to the theoretical solution of the full-rank Moore-Penrose inverse problem without noises, and exponentially converge to the exact solution in the presence of noises, which are demonstrated theoretically. Moreover, in comparison with existing models, numerical simulations are provided to substantiate the feasibility and superiority of the proposed modified neural network for online solving time-varying full-rank Moore-Penrose problems with inherent tolerance to noises. In addition, the numerical results infer that different activation functions can be applied to accelerate the convergence speed of the zeroing neural network model. Finally, the proposed zeroing neural network models are applied to the motion generation of redundant robot manipulators, which illustrates its high efficiency and robustness. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						158	172		10.1016/j.neucom.2020.06.050													
J								Finite-time synchronization of fully complex-valued networks with or without time-varying delays via intermittent control	NEUROCOMPUTING										Complex network; Complex variable; Finite-time Synchronization; Intermittent control	DYNAMICAL NETWORKS; PROJECTIVE SYNCHRONIZATION; EXPONENTIAL SYNCHRONIZATION; IMPULSIVE SYNCHRONIZATION; CLUSTER SYNCHRONIZATION; FEEDBACK; SYSTEMS	This paper explores finite-time (F-T) synchronization for a type of fully complex-valued (C-V) networks with or without delays by proposing intermittent control schemes but without using the ordinary separation technique. Above all, the vector signum function in complex field is proposed as the generalization of real-valued counterpart, which is critical to the control design in complex field and the construction of Lyapunov functions. To realize F-T synchronization, several complex-valued intermittent controllers, only dependent of the information of controlled nodes, are designed on the addressed complex-valued networks, which are different from the design on the divided real-valued subsystems. In the theoretical analysis, some nontrivial Lyapunov functionals and functions are constructed by virtue of the proposed signum functio n and some inequalities are developed in complex domain to establish criteria of F-T synchronization. Finally, some numerical examples are performed to reveal the effectiveness of the derived results and the practicality of the proposed control strategies. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						173	184		10.1016/j.neucom.2020.06.057													
J								On the investigation of activation functions in gradient neural network for online solving linear matrix equation	NEUROCOMPUTING										Gradient neural network; Linear matrix equation; Activation function; Fixed-time convergence	FINITE-TIME CONVERGENCE; SYLVESTER EQUATION; DYNAMICS; DESIGN; MODELS	In this paper, we investigate different activation functions (AFs) on convergence performance of a gradient-based neural network (GNN) for solving linear matrix equation, AXB + X = C. It is observed that, by employing different AFs, i.e., linear, power-sigmoid, sign-power, and general sign-bi-power functions, the presented GNN model can achieve different convergence performance. More specifically, if linear function is employed, the GNN model can achieve exponential convergence; if the power-sigmoid function is employed, superior convergence can be achieved as compared to the linear case; while if the sign-power and general sign-bi-power functions are employed, the GNN model can achieve finite- and fixed-time convergence, respectively. Detailed theoretical proofs are offered to demonstrate these facts. Besides, the exponential convergence rate and the upper bounds of finite and fixed convergence time are also theoretically estimated. Finally, two illustrative examples are performed to further substantiate the aforementioned theoretical results and the effectiveness of the presented GNN model for solving the linear matrix equation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						185	192		10.1016/j.neucom.2020.06.097													
J								Hub-based subspace clustering	NEUROCOMPUTING										Hubness; Subspace clustering; Graph-based meta-features; Selective sampling		Data often exists in subspaces embedded within a high-dimensional space. Subspace clustering seeks to group data according to the dimensions relevant to each subspace. This requires the estimation of subspaces as well as the clustering of data. Subspace clustering becomes increasingly challenging in high dimensional spaces due to the curse of dimensionality which affects reliable estimations of distances and density. Recently, another aspect of high-dimensional spaces has been observed, known as the hubness phenomenon, whereby few data points appear frequently as nearest neighbors of the rest of the data. The distribution of neighbor occurrences becomes skewed with increasing intrinsic dimensionality of the data, and few points with high neighbor occurrences emerge as hubs. Hubs exhibit useful geometric properties and have been leveraged for clustering data in the full-dimensional space. In this paper, we study hubs in the context of subspace clustering. We present new characterizations of hubs in relation to subspaces, and design graph-based meta-features to identify a subset of hubs which are well fit to serve as seeds for the discovery of local latent subspaces and clusters. We propose and evaluate a hubness-driven algorithm to find subspace clusters, and show that our approach is superior to the baselines, and is competitive against state-of-the-art subspace clustering methods. We also identify the data characteristics that make hubs suitable for subspace clustering. Such characterization gives valuable guidelines to data mining practitioners. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						193	209		10.1016/j.neucom.2020.06.098													
J								Expectation-Maximization algorithm for finite mixture of alpha-stable distributions	NEUROCOMPUTING										Alpha-stable mixture model; Alpha-stable distribution; Expectation-Maximization algorithm	CLASSIFICATION; PARAMETERS; NUMBER	A Gaussian Mixture Model (GMM) is a parametric probability density function built as a weighted sum of Gaussian distributions. Gaussian mixtures are used for modelling the probability distribution in many fields of research nowadays. Nevertheless, in many real applications, the components are skewed or heavy tailed. For that reason, it is useful to model the mixtures as components with alpha-stable distribution. In this work, we present a mixture of skewed alpha-stable model where the parameters are estimated using the Expectation-Maximization algorithm. As the Gaussian distribution is a particular limiting case of alpha-stable distribution, the proposed model is a generalization of the widely used GMM. The proposed algorithm is much faster than the parameter estimation of the alpha-stable mixture model using a Bayesian approach and Markov chain Monte Carlo methods. Therefore, it is more suitable to be used for large vector observations. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						210	216		10.1016/j.neucom.2020.06.114													
J								Fusing motion patterns and key visual information for semantic event recognition in basketball videos	NEUROCOMPUTING										Event classification; Sports video analysis; Global & local motion separation; Motion patterns; Key visual information	CAMERA MOTION	Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						217	229		10.1016/j.neucom.2020.07.003													
J								Adaptive weighted motion averaging with low-rank sparse for robust multi-view registration	NEUROCOMPUTING										Point cloud; Multi-view registration; Motion averaging; Low-rank and sparse matrix; Lagrange multiplier	3D; PROJECTION	Motion averaging has recently been introduced as an effective means to tackle the registration of multiview range scans. This approach can view parts of pair-wise motions with high reliability as an input to estimate the global motions for a multi-view registration. However, reliable pair-wise motions are not easy to confirm in most practical applications without prior knowledge. In this paper, we propose an adaptive low-rank sparse (LRS) weighted motion averaging method for a robust and accurate multiview registration, which can directly reconstruct high-quality 3D shape models from a set of unordered range scans. Specifically, we first introduce LRS matrix decomposition to automatically compute the initial global motions. The LRS matrix decomposition can recover the initial global models through the full exploration of a set of pair-wise motions. Subsequently, we extend the motion averaging with an adaptive weight computation by developing an optimization strategy using the Lagrange multiplier method, which can adaptively compute the weights of the reliability for each pair-wise relative motion. Accordingly, the proposed method can recover accurate and robust global motions in a set of iterations through weighted motion averaging. Experimental results on several public datasets demonstrate the excellent performance of the proposed method in comparison with state-of-the-art multi-view registration and 3D scene reconstruction. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						230	239		10.1016/j.neucom.2020.06.102													
J								Sequential online prediction in the presence of outliers and change points: An instant temporal structure learning approach	NEUROCOMPUTING										Online prediction; Change point detection; Outlier detection; Streaming data; Regime shift; Instant learning	INFERENCE; MODEL	In this paper, we consider sequential online prediction (SOP) for streaming data in the presence of outliers and change points. We propose an INstant TEmporal structure Learning (INTEL) algorithm to address this problem. Our INTEL algorithm is developed based on a full consideration of the duality between online prediction and anomaly detection. We first employ a mixture of weighted Gaussian process models (WGPs) to cover the expected possible temporal structures of the data. Then, based on the rich modeling capacity of this WGP mixture, we develop an efficient technique to instantly learn (capture) the temporal structure of the data that follows a regime shift. This instant learning is achieved only by adjusting one hyper-parameter of the mixture model. A weighted generalization of the product of experts (POE) model is used for fusing predictions yielded from multiple GP models. An outlier is declared once a real observation seriously deviates from the fused prediction. If a certain number of outliers are consecutively declared, then a change point is declared. Extensive experiments are performed using a diverse of real datasets. Results show that the proposed algorithm is significantly better than benchmark methods for SOP in the presence of outliers and change points. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						240	258		10.1016/j.neucom.2020.07.011													
J								Deep neural network to extract high-level features and labels in multi-label classification problems	NEUROCOMPUTING										Deep neural networks; Multi-label classification; High-level features; High-level labels; Association-based pooling		Pooling layers help reduce redundancy and the number of parameters in deep neural networks without the need of performing additional learning processes. Although these operators are able to deal with both single-label and multi-label problems they are specifically aimed at reducing feature space. However, in the case of multi-label data, this should also be done in the label space. On the other hand, in spite of their success, existing pooling operators are not ideal when handling (multi-label) datasets that do not have an explicit topological organization. In this paper, we present a deep neural architecture using bidirectional association-based pooling layers to extract high-level features and labels in multi-label classification problems. Our approach uses an association function to detect distinct pairs of neurons that will be aggregated into pooled neurons. In the first pooling layer, our proposal computes the Pearson correlation among the variables as the basis to quantify the association values. In addition, we propose an iterative procedure that allows estimating the association degree among pooled neurons in deeper layers without the need of recomputing the correlation matrix. The main advantage of this deep neural architecture is that it allows extracting high-level features and labels on datasets with no specific topological organization. The numerical results show that our bidirectional neural network helps reduce the number of problem features and labels while preserving network's discriminatory power. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						259	270		10.1016/j.neucom.2020.06.117													
J								Projective parameter transfer based sparse multiple empirical kernel learning Machine for diagnosis of brain disease	NEUROCOMPUTING										Multiple empirical kernel learning machine; Sparse learning; Parameter-based transfer learning; Projective model	TRANSCRANIAL SONOGRAPHY; PARKINSONS-DISEASE; DOMAIN ADAPTATION; CLASSIFICATION; INFORMATION	Single-modal neuroimaging-based diagnosis for brain diseases is a main routine due to the lack of advanced imaging devices, especially in rural hospitals. Transfer learning (TL) has demonstrate edits effectiveness in improving the performance of a computer-aided diagnosis (CAD) system, which transfers knowledge from the model of a related imaging modality (source domain (SD)) to that of the diagnosis modality (target domain (TD)). Multiple empirical kernel learning machine (MEKLM) is a newly proposed classifier with superior performance to the conventional classifiers with implicit kernel mapping. In this work, we propose a novel projective model(PM) based sparse MEKLM(PM-SMEKLM) algorithm to learn a cross-domain transformation by PM in way of the parameter-based TL, and then apply it to the neuroimaging-based CAD for brain diseases. Sparse learning is integrated into MEKLM to further select effective information in SD for knowledge transfer and enhance the generalization ability of the classifier in TD. The projection matrix in PM and sparse representations in MEKLM are jointly learned, which effectively improves the performance of PM-SMEKLM. Three experiments are conducted on two neuroimaging datasets for the diagnosis of Alzheimer's disease and Parkinson's disease, respectively. Experimental results show that the proposed PM-SMEKLM algorithm outperforms all the compared algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						271	283		10.1016/j.neucom.2020.07.008													
J								Gradient preconditioned mini-batch SGD for ridge regression	NEUROCOMPUTING										Mini-batch SGD; Regularized loss minimization; Ridge regression; Gradient preconditioning; Random projection; Linear sketching		Data preconditioning technique, which reduces the condition number of the problem by a linear transformation of the data matrix, is typically used to accelerate the convergence of the first-order optimization methods for regularized loss minimization. One obvious limitation of the technique is exceedingly expensive of computational cost for the large-scale problems, especially an ocean of samples. In this paper, we have a gradient preconditioning trick and combine it with mini-batch SGD. The proposed gradient preconditioned mini-batch SGD algorithm boosts indeed the convergence with lower computational cost than that of the data preconditioning technique for ridge regression. Concretely, we use recent random projection and linear sketching methods to randomly low rank approximate the data matrix, then we can achieve a appropriate preconditioner through numerical linear algebra. Finally, we apply obtained preconditioner to the gradient to reduce computational cost. The experimental results on both synthetic data and real data sets validate the feasibility and effectiveness of our trick and algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						284	293		10.1016/j.neucom.2020.06.092													
J								Efficient neural network compression via transfer learning for machine vision inspection	NEUROCOMPUTING										Deep learning; Industrial image inspection; Neural network compression; Transfer learning	ARCHITECTURES	Several practical difficulties arise when trying to apply deep learning to image-based industrial inspection tasks: training datasets are difficult to obtain, each image must be inspected in milliseconds, and defects must be detected with 99% or greater accuracy. In this paper we show how, for image-based industrial inspection tasks, transfer learning can be leveraged to address these challenges. Whereas transfer learning is known to work well only when the source and target domain images are similar, we show that using ImageNet-whose images differ significantly from our target industrial domain-as the source domain, and performing transfer learning, works remarkably well. For one benchmark problem involving 5,520 training images, the resulting transfer-learned network achieves 99.90% accuracy, compared to only a 70.87% accuracy achieved by the same network trained from scratch. Further analysis reveals that the transfer-learned network produces a considerably more sparse and disentangled representation compared to the trained-from-scratch network. The sparsity can be exploited to compress the transfer-learned network up to 1/128 the original number of convolution filters with only a 0.48% drop in accuracy, compared to a drop of nearly 5% when compressing a trained-from-scratch network. Our findings are validated by extensive systematic experiments and empirical analysis. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						294	304		10.1016/j.neucom.2020.06.107													
J								Gradient-based discriminative modeling for blind image deblurring	NEUROCOMPUTING										Image deblurring; Blind deconvolution; Discriminative prior; Blur kernel estimation; Low-illumination	MOTION; DECONVOLUTION; SHAKEN	Blind image deconvolution is a fundamental task in image processing, computational imaging, and computer vision. It has earned intensive attention in the past decade since the seminal work of Fergus et al. [1] for camera shake removal. In spite of the recent great progress in this field, this paper aims to formulate the blind problem with a simpler modeling perspective. What is more important, the newly proposed approach is expected to achieve comparable or even better performance towards the real blurred images. Specifically, the core critical idea is the proposal of a pure gradient-based discriminative prior for accurate and robust blur kernel estimation. Numerous experimental results on both the benchmark datasets and real-world blurred images in various imaging scenarios, e.g., natural, manmade, low-illumination, text, or people, demonstrate well the effectiveness and robustness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						305	327		10.1016/j.neucom.2020.06.093													
J								Deep reinforcement learning based lane detection and localization	NEUROCOMPUTING										Lane detection; Deep reinforcement learning; Lane localization; Q-Learning	ROAD DETECTION; FRAMEWORK; TRACKING	Recently, deep-learning based lane detection methods effectively boost the development of Advanced Driver Assistance Systems (ADAS) and Self-Driving Systems. However, these methods only detect lane lines with sketchy bounding boxes while ignore the shape of specific curved lanes. To address the above problems, this paper introduces deep reinforcement learning into cursory lane detection models for accurate lane detection and localization. This model consists of two stages, namely the bounding box detector and landmark point localizer. To be specific, a bounding box level convolution neural network lane detector outputs the preliminary location of lanes in the form of bounding boxes. Then, a reinforcement based Deep Q-Learning Localizer (DQLL) accurately localizes the lanes as a group of landmarks to achieve better representation of curved lanes. Moreover, a pixel-level lane detection dataset named NWPU Lanes Dataset is constructed and released. It contains a variety of real traffic scenes and accurate masks of the lane lines. This approach achieves competitive performance in the released dataset and TuSimple Lane dataset. Furthermore, the codes and dataset will be released on https://github.com/tuzixini/DQLL. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						328	338		10.1016/j.neucom.2020.06.094													
J								Leader-following consensus of multi-agent systems under antagonistic networks	NEUROCOMPUTING										Multi-agent systems; Signed graph; Bipartite consensus; Adaptive regulation	DYNAMICAL-SYSTEMS; SYNCHRONIZATION	In this paper, we address the leader-following consensus of multi-agent systems (MASs) under a class of antagonistic networks. Different from the traditional literature, in our model, whether a follower can access the leader depends on a matrix instead of a scalar. For antagonistic and directed networks with structurally balanced properties, sufficient conditions are established under which the MASs will achieve bipartite consensus. Detailed algorithms are proposed to design the gain matrices and the coupling strength. What is more, we design adaptive laws to self-tune the value of the coupling strength. Simulation results are also given to verify the effectiveness of our methods. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 6	2020	413						339	347		10.1016/j.neucom.2020.07.006													
J								End-to-end masked graph-based CRF for joint slot filling and intent detection	NEUROCOMPUTING										Slot filling; Graph-based CRF; Intent detection; Mask mechanism; End-to-end structure		Slot filling and intent detection are the basic and crucial fields of natural language processing (NLP) for understanding and analyzing human language, owing to their wide applications in real-world scenarios. Most existing methods of slot filling and intent detection tasks utilize linear chain conditional random field (CRF) for only optimizing slot filling, no matter the method is a pipeline or a joint model. In order to describe and exploit the implicit connections which indicate the appearance compatibility of different tag pairs, we introduce a graph-based CRF for a joint optimization of tag distribution of the slots and the intents. Instead of applying the complex inference algorithm of traditional graph-based CRF, we use an end-to-end method to implement the inference, which is formulated as a specialized multi-layer graph convolutional network (GCN). Furthermore, mask mechanism is introduced to our model for addressing multi-task problems with different tag-sets. Experimental results show the superiority of our model com- pared with other alternative methods. Our code is available at https://github.com/tomsonsgs/e2e-mask-graph-crf. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						348	359		10.1016/j.neucom.2020.06.113													
J								FSD-10: A fine-grained classification dataset for figure skating	NEUROCOMPUTING										Action recognition; Figure Skating Dataset; Fine-grained sports content analysis; Keyframe based temporal segment network	ACTION RECOGNITION	Action recognition is an important and challenging problem in video analysis. Although the past decade has witnessed progress in action recognition with the development of deep learning, such process has been slow in competitive sports content analysis. To promote the research on action recognition from competitive sports video clips, we introduce a Figure Skating Dataset (FSD-10) for fine-grained sports content analysis. To this end, we collect 1484 clips from the worldwide figure skating championships in 2017-2018, which consist of 10 different actions in men/ladies programs. Each clip is at a rate of 30 frames per second with resolution 1080 x 720, which are annotated by experts. To build a baseline for action recognition in figure skating, we evaluate state-of-the-art action recognition methods on FSD-10. Motivated by the idea that domain knowledge is of great concern in sports field, we propose a key-frame based temporal segment network (KTSN) for classification and achieve remarkable performance. Experimental results demonstrate that FSD-10 is an ideal dataset for benchmarking action recognition algorithms, as it requires to accurately extract action motions rather than action poses. We hope FSD-10, which is designed to have a large collection of finegrained actions, can serve as a new challenge to develop more robust and advanced action recognition models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						360	367		10.1016/j.neucom.2020.06.108													
J								A novel generalization of the natural residual function and a neural network approach for the NCP	NEUROCOMPUTING										Complementarity functions; Natural residual function; Nonlinear complementarity problem	COMPLEMENTARITY-PROBLEMS; NEWTON	The natural residual (NR) function is a mapping often used to solve nonlinear complementarity problems (NCPs). Recently, three discrete-type families of complementarity functions with parameter p >= 3 (where p is odd) based on the NR function were proposed. Using a neural network approach based on these families, it was observed from some preliminary numerical experiments that lower values of p provide better convergence rates. Moreover, higher values of p require larger computational time for the test problems considered. Hence, the value p = 3 is recommended for numerical simulations, which is rather unfortunate since we cannot exploit the wide range of values for the parameter p of the family of NCP functions. This paper is a follow-up study on the aforementioned results. Motivated by previously reported numerical results, we formulate a continuous-type generalization of the NR function and two corresponding symmetrizations. The new families admit a continuous parameter p > 0, giving us a wider range of choices for p and smooth NCP functions when p > 1. Moreover, the generalization subsumes the discrete-type generalization initially proposed. The numerical simulations show that in general, increased stability and better numerical performance can be achieved by taking values of p in the interval (1,3). This is indeed a significant improvement of preceding studies. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						368	382		10.1016/j.neucom.2020.06.059													
J								Spatial attention based visual semantic learning for action recognition in still images	NEUROCOMPUTING										Still image-based action recognition; Spatial attention; Semantic parts; Deep learning	MODEL	Visual semantic parts play crucial roles in still image-based action recognition. A majority of existing methods require additional manual annotations such as human bounding boxes and predefined body parts besides action labels to learn action related visual semantic parts. However, labeling these manual annotations is rather time-consuming and labor-intensive. Moreover, not all manual annotations are effective when recognizing a specific action. Some of them can be irrelevant and even misguided. To address these limitations, this paper proposes a multi-stage deep learning method called Spatial Attention based Action Mask Networks (SAAM-Nets). The proposed method does not need any additional annotations besides action labels to obtain action-specific visual semantic parts. Instead, we propose a spatial attention layer injected in a convolutional neural network to create a specific action mask for each image with only action labels. Moreover, based on the action mask, we propose a region selection strategy to generate a semantic bounding box containing action-specific semantic parts. Furthermore, to effectively combine the information of the whole scene and the sematic box, two feature attention layers are adopted to obtain more discriminative representations. Experiments on four benchmark datasets have demonstrated that the proposed method can achieve promising performance compared with state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						383	396		10.1016/j.neucom.2020.07.016													
J								Fast algorithm with theoretical guarantees for constrained low-tubal-rank tensor recovery in hyperspectral images denoising	NEUROCOMPUTING										Tensor tubal rank; Non-convex optimization; Bilateral random tensors projections; Hyperspectral images	NONCONVEX; MINIMIZATION	Hyperspectral images (HSIs) are unavoidably degraded by mixed noise, including Gaussian noise and sparse noise. In this paper, we consider a constrained tubal rank and sparsity model (CTSD) to tackle the HSIs mixed noise removal, which characterizes the clean HSIs via the low-tubal-rank constraint and the sparse noise via the l(0) and l(infinity) norm constraints, respectively. Due to the strong non-convexity, the CTSD model is challenging to solve. To tackle the CTSD, we develop the proximal alternating minimization (PAM) algorithm via the exact tensor singular value decomposition (t-SVD) and establish the global convergence under mild assumptions. Since the t-SVD is computationally expensive, especially for large scale images, we further design an efficient inexact PAM algorithm via an inexact t-SVD. The inexact PAM enjoys two advantages: (1) The computational complexity for SVDs of the inexact PAM (O(rn(1)n(2)n(3))) is about twofold faster than that of the exact PAM (O(min(n(1), n(2))n(1)n(2)n(3))) for r << min(n(1), n(2)); (2) The accuracy of the inexact PAM is theoretically guaranteed. Extensive experiments on HSIs denoising demonstrate that the exact and inexact methods both outperform comparing methods in quantitative evaluation metrics and visual effects, and the inexact PAM can compromise between the accuracy and efficiency for large scale HSIs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						397	409		10.1016/j.neucom.2020.07.022													
J								Resilient model-free adaptive control for cyber-physical systems against jamming attack	NEUROCOMPUTING										Model-free adaptive control; Cyber-physical systems (CPSs); Jamming attacks; Predictive compensation algorithm	SYNCHRONIZATION CONTROL; NETWORKS	This paper considers the resilient tracking control problem for nonlinear unknown cyber-physical systems (CPSs) subject to jamming attacks in the wireless transmission channel. First, a novel model-free adaptive control (MFAC) framework against jamming attacks is established. A Bernoulli distribution process is used to describe the happen behavior of jamming attacks. Second, a n-steps predictive compensation algorithm is designed in the controller side to reduce the effect of jamming attacks. Then, the model-free adaptive controller is designed such that the tracking error of the nonlinear system is stochastic stable, and all the analysis are based on input/output data. Simulation results demonstrate the effectiveness of our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						422	430		10.1016/j.neucom.2020.04.043													
J								Syntactically-informed word representations from graph neural network	NEUROCOMPUTING										Natural language processing; Contextual word representation; Word representation; Word embedding; Syntactic word representation		Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT. The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs). The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than fine-tuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. (C) 2020 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				NOV 6	2020	413						431	443		10.1016/j.neucom.2020.06.070													
J								Short-term traffic flow prediction: From the perspective of traffic flow decomposition	NEUROCOMPUTING										Traffic flow decomposition; Periodicity; Volatility; Time-series analysis; Supervised learning	LEARNING ALGORITHM; NEURAL-NETWORK	Some researchers treat traffic flow as an entirety while predicting short-term traffic flow. Through analyzing real-world traffic flow, we have found that urban traffic shows a stable changing process along with random disturbs. An alternative way is to decompose traffic flow into two components: periodicity and volatility. We propose a hybrid method named Time-Series Analysis and Supervised-Learning (TSA-SL) for short-term traffic flow prediction from the perspective of traffic flow decomposition. In the method, period traffic flow is modeled with a typical TSA method called Fourier Transform (FT), where periodic behaviors are described as the combination of sines and cosines. The volatility of the current location is determined by its surroundings, so spatial-temporal correlations are extracted as input features of SL. Then, three hybrid prediction models, including FT-SVR, FT-GBRT and FT-LSTM, are built with proposed TSA-SL. In the experiment, an Electronic Registration Identification (ERI) dataset including massive real-world individual trajectories is employed. Comparing with classical baseline models, our proposed TSA-SL method has certain superiority. Furthermore, we decompose traffic flow into different components in terms of traveling purposes and vehicle types. The experimental results show that our method performs better in predicting partial traffic flow than predicting all traffic flow. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						444	456		10.1016/j.neucom.2020.07.009													
J								Kinematic skeleton graph augmented network for human parsing	NEUROCOMPUTING										Image segmentation; Human parsing; Deeplab V3+; Kinematic skeleton graph; Human parsing dataset	SEGMENTATION	Human parsing, which is a task of labeling pixels in human images into different fine-grained semantic parts, has achieved significant progress during the past decade. However, there are still several challenges in human parsing, due to occlusions, varying poses and similar appearance between the left/right parts. To tackle these problems, a Human Kinematic Skeleton Graph Layer (HKSGL) is proposed to augment regular neural networks with human kinematic skeleton information. The HKSGL has two major components: kinematic skeleton graph and interconnected modular neural layer. The kinematic skeleton graph is a user pre-defined skeleton graph, which models the interconnections between different semantic parts. Then the skeleton graph is passed to the interconnected modular neural layer which is composed of a set of modular blocks corresponding to different semantic parts. The HKSGL is a lightweight, low costs layer which can be easily attached to any existing neural networks. To demonstrate the power of the HKSGL, a new dataset on human parsing in occlusions is also collected, termed the RAP-Occ. Extensive experiments have been performed on four datasets on human parsing, including the LIP, the CIHP, the ATR and the RAP-Occ. And two popular baselines, i.e., the Deeplab V3+ and the CE2P, are agumented by the proposed HKSGL. Competitive performance of the augmented models has been achieved in comparison with state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						457	470		10.1016/j.neucom.2020.07.002													
J								Probabilistic regressor chains with Monte Carlo methods	NEUROCOMPUTING										Multi-output regression; Multi-label classification; Regressor chains; Classifier chains; Monte Carlo methods; Particle filters	CLASSIFIER CHAINS; PREDICTION	A large number and diversity of techniques have been offered in the literature in recent years for solving multi-label classification tasks, including classifier chains where predictions are cascaded to other models as additional features. Chaining methods have often providing state of the art results, and the idea of extending it to multi-output regression has already been trialed. However, these 'regressor chains' have seen limited applicability, on account of yielding relatively little predictive performance compared to individual regression models, and also limited interpretability. In this work we identify and discuss the main limitations of regressor chains, including an analysis of different base models, loss functions, explainability, and other desiderata of real-world applications. We develop and examine techniques to overcome these limitations. In particular we present Monte Carlo schemes in the framework of probabilistic chains. We show they can be effective, flexible and useful in different areas. Overall, we also place regressor chains in context among general multi-output learning with continuous outputs, and in doing this shed additional light on the applicability of chaining to machine learning tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						471	486		10.1016/j.neucom.2020.05.024													
J								Local k-NNs pattern in Omni-Direction graph convolution neural network for 3D point clouds	NEUROCOMPUTING										3D point cloud; Spatial layout; Omni-Directional k-NNs pattern; Graph convolution neural network	IMAGE	Effective representation of objects in irregular and unordered point clouds is one of the core challenges in 3D vision. Transforming point cloud into regular structures, such as 2D images and 3D voxels, are not ideal. It either obscures the inherent geometry information of 3D data or results in high computational complexity. Learning permutation invariance feature directly from raw 3D point clouds using deep neural network is a trend, such as PointNet and its variants, which are effective and computationally efficient. However, these methods are weak to reveal the spatial structure of 3D point clouds. Our method is delicately designed to capture both global and local spatial layout of point cloud by proposing a Local k-NNs Pattern in Omni-Direction Graph Convolution Neural Network architecture, called LKPO-GNN. Our method converts the unordered 3D point cloud into an ordered 1D sequence, to facilitate feeding the raw data into neural networks and simultaneously reducing the computational complexity. LKPO-GNN selects multi-directional k-NNs to form the local topological structure of a centroid, which describes local shapes in the point cloud. Afterwards, GNN is used to combine the local spatial structures and represent the unordered point clouds as a global graph. Experiments on ModelNet40, ShapeNetPart, ScanNet, and S3DIS datasets demonstrate that our proposed method outperforms most existing methods, which verifies the effectiveness and advantage of our work. Additionally, a deep analysis towards illustrating the rationality of our approach, in terms of the learned the topological structure feature, is provided. Source code is available at https://github.com/zwj12377/LKPO-GNN.git. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						487	498		10.1016/j.neucom.2020.06.095													
J								Sampled-data synchronization of delayed multi-agent networks and its application to coupled circuit	NEUROCOMPUTING										Lyapunov method; Linear matrix inequality (LMI); Multi-agent networks; Synchronization; Time-delay	CHAOTIC NEURAL-NETWORKS; CONSENSUS PROBLEMS; STATE SYNCHRONIZATION; ASYMPTOTIC STABILITY; SYSTEMS; ALGORITHM; PASSIVITY; DISCRETE; TOPOLOGY	This paper deals with the synchronization control of general chaotic delayed neural networks via sampled-data controllers. The synchronization problem is reduced to a stabilization one, and then a Lyapunov-Krasovskii function (LKF) is proposed to obtain the sufficient condition for the asymptotic stability. Moreover, the sufficient condition is presented by linear matrix inequalities (LMIs), which are easy to solve by existing software. Three numerical examples including electrical circuits are provided to confirm validity of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				NOV 6	2020	413						499	511		10.1016/j.neucom.2020.05.060													
J								Contelog: A declarative language for modeling and reasoning with contextual knowledge	KNOWLEDGE-BASED SYSTEMS										Knowledge-base systems; Contextual reasoning; Context; Contextual knowledge base systems; Knowledge representation; Declarative semantics; Datalog; Modularity; Reuse	SYSTEM	Context-awareness is at the core of many modern-day applications in safety and secure-critical domains. In existing context-aware systems knowledge and context are not formally integrated, and consequently adaptation behaviors for safety-criticality cannot be formally reasoned. In modern day smart systems, such as healthcare and advanced manufacturing, context-awareness must be combined with contextual reasoning in order that new knowledge can be inferred and based on which strategic decisions can be made. Consequently, a rigorous approach is essential to represent contextual domain knowledge and inference rules in order to combine the logic of domain-based decision rules with contextual constraints for contextual reasoning, and decision making. In this paper we address this later issue and introduce a formal approach to achieve contextual reasoning. The framework that we create, called Contelog, conservatively extends the syntax and semantics of Datalog to reason with contextual knowledge. In this setting, contextual knowledge is reusable on its own. The significance is that by fixing the contextual knowledge, goal-specific analysis rules may be changed, and vice versa. By providing a theory of context, independent of the logic of the rules, we have developed a simple and sound context calculus using which it is possible to export knowledge reasoned in one context to another context. Query processing and implementation of Contelogprograms convince us that it has the capabilities to reason in systems where perception and cognition are formally combined for problem solving. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106403	10.1016/j.knosys.2020.106403													
J								Heuristic algorithms based on deep reinforcement learning for quadratic unconstrained binary optimization	KNOWLEDGE-BASED SYSTEMS										Unconstrained binary quadratic programming; Heuristic algorithm; Deep reinforcement learning; Neural network	SEARCH	The unconstrained binary quadratic programming (UBQP) problem is a difficult combinatorial optimization problem that has been intensively studied in the past decades. Due to its NP-hardness, many heuristic algorithms have been developed for the solution of the UBQP. These algorithms are usually problem-tailored, which lack generality and scalability. To address these issues, a heuristic algorithm based on deep reinforcement learning (DRLH) is proposed in this paper. It features in inputting specific features and using a neural network model called NN to guild the selection of variable at each solution construction step. Also, to improve the algorithm speed and efficiency, two algorithm variants named simplified DRLH (DRLS) and DRLS with hill climbing (DRLS-HC) are developed as well. These three algorithms are examined through extensive experiments in comparison with famous heuristic algorithms from the literature. Experimental results show that the DRLH, DRLS, and DRLS-HC outperform their competitors in terms of both solution quality and computational efficiency. Precisely, the DRLH achieves the best-quality results, while DRLS offers a high-quality solution in a very short time. By adding a hill-climbing procedure to DRLS, the resulting DRLS-HC algorithm is able to obtain almost the same quality result as DRLH with however 5 times less computing time on average. We conducted additional experiments on large-scale instances and various data distributions to verify the generality and scalability of the proposed algorithms, and the results on benchmark instances indicate the ability of the algorithms to be applied to practical problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106366	10.1016/j.knosys.2020.106366													
J								Novel multi-label feature selection via label symmetric uncertainty correlation learning and feature redundancy evaluation	KNOWLEDGE-BASED SYSTEMS										Multi-label feature selection; Symmetric uncertainty; Fuzzy mutual information; Feature redundancy evaluation	ATTRIBUTE SELECTION; MUTUAL INFORMATION; CLASSIFICATION; RECOGNITION; REDUCTION	Multi-label data with high dimensionality, widely existed in the real world, bring many challenges to the applications of machine learning, pattern recognition and other fields. Scholars have proposed some multi-label feature selection methods from various aspects. However, there are few studies on the feature selection for multi-label data based on fuzzy mutual information, and most existing methods neglect the correlation between labels. In this study, we propose two novel multi-label feature selection approaches via label symmetric uncertainty correlation and feature redundancy evaluation. Firstly, we propose the concept of symmetric uncertainty correlation between labels via fuzzy mutual information, and design a label importance weight based on label symmetric uncertainty correlation learning. Further, we define a label similarity relation matrix on multi-label space via the label importance weight. Secondly, we define the symmetric uncertainty correlation between features and labels, and propose the first multi-label feature selection approach. Thirdly, considering the above-proposed method can only get a feature sequence and does not remove the redundancy features, we further propose an improved multi-label removing-redundancy feature selection approach through introducing feature redundancy evaluation. Finally, comprehensive experiments are executed to demonstrate the performance of our methods. The results illustrate that our study is better than other representative feature selection methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106342	10.1016/j.knosys.2020.106342													
J								Ensemble transfer CNNs driven by multi-channel signals for fault diagnosis of rotating machinery cross working conditions	KNOWLEDGE-BASED SYSTEMS										Fault diagnosis; Rotating machinery; Ensemble transfer CNN; Multi-channel signals; Decision fusion	CONVOLUTIONAL NEURAL-NETWORK; ROLLING BEARING; CLASSIFICATION; ALGORITHM; FRAMEWORK; SPECTRUM	Automatic and reliable fault diagnosis of rotating machinery cross working conditions is of practical importance. For this purpose, ensemble transfer convolutional neural networks (CNNs) driven by multi-channel signals are proposed in this paper. Firstly, a series of source CNNs modified with stochastic pooling and Leaky rectified linear unit (LReLU) are pre-trained using multi-channel signals. Secondly, the learned parameter knowledge of each individual source CNN is transferred to initialize the corresponding target CNN which is then fine-tuned by a few target training samples. Finally, a new decision fusion strategy is designed to flexibly fuse each individual target CNN to obtain the comprehensive result. The proposed method is used to analyze multi-channel signals measured from rotating machinery. The comparison result shows the superiorities of the proposed method over the existing deep transfer learning methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106396	10.1016/j.knosys.2020.106396													
J								An efficient metaheuristic for the K-page crossing number minimization problem	KNOWLEDGE-BASED SYSTEMS										Metaheuristics; Variable neighborhood search; Graph layout problems; Book drawing	VARIABLE NEIGHBORHOOD SEARCH; QUALITY	Graph layout problems refer to a family of optimization problems with relevant applications in VLSI design, information retrieval, numerical analysis, computational biology, or graph theory. In this paper, we focus on a variant where the graph is mapped over a specific structure referred to as book, which consists of a spine and a number of pages. Vertices of the graph are allocated in the spine, which is usually represented as a line, and edges are assigned to pages of the book, which are represented as half-planes that have the spine as its boundary. The experience shows that one of the main quality desired for layout of graphs is the minimization of edge crossing. The K-page crossing number minimization problem (KPMP) aims to reduce as much as possible the number of crossings between edges belonging to the same page. We propose the application of the VNS metaheuristic to efficiently solve the KPMP. Experiments show a remarkable improvement over the state-of-the-art procedures for a large set of instances, leading the proposed VNS algorithm as a promising strategy to solve this family of book drawing problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106352	10.1016/j.knosys.2020.106352													
J								HeTROPY: Explainable learning diagnostics via heterogeneous maximum-entropy and multi-spatial knowledge representation	KNOWLEDGE-BASED SYSTEMS										Causal reasoning; Knowledge representation; Learning diagnostics; Relation prediction		Autonomous learning diagnostics, where the students' strengths and weaknesses are disclosed from their observed performance data, is a challenging task in e-learning systems. Current student knowledge models can alleviate some of the problems in learning (i.e. predicting student performance) but they neglect learning diagnostics, which is based on causal reasoning. To this end, we propose a novel heterogeneous attention interpreter with a maximum entropy regularizer on top of a student knowledge model to achieve explainable learning diagnostics. Our model segregates the impact of the homogeneous knowledge points, while promoting the heterogeneous relatives by maximizing their chance to contribute to the prediction. We also propose a multi-spatial knowledge representation that is readily generalizable to other data-driven educational tasks. Extensive experiments on real-world datasets reveal that the proposed method is able to enhance the model's explanatory power, hence increases the trustworthiness towards learning diagnostics. It also brings notable improvement in accuracy in the student performance prediction task. The findings in this paper are adoptable to various types of e-learning systems to assist teachers to gain insights into student learning states and diagnose learning problems. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				NOV 5	2020	207								106389	10.1016/j.knosys.2020.106389													
J								Gamma distribution-based sampling for imbalanced data	KNOWLEDGE-BASED SYSTEMS										Imbalanced data; Sampling; Gamma distribution	KERNEL DENSITY-ESTIMATION; SMOTE; CHALLENGES; MACHINE	Imbalanced class distribution is a common problem in a number of fields including medical diagnostics, fraud detection, and others. It causes bias in classification algorithms leading to poor performance on the minority class data. In this paper, we propose a novel method for balancing the class distribution in data through intelligent resampling of the minority class instances. The proposed method is based on generating new minority instances in the neighborhood of the existing minority points via a gamma distribution. Our method offers a natural and coherent approach to balancing the data. We conduct a comprehensive numerical analysis of the new sampling technique. The experimental results show that the proposed method outperforms the existing state-of-the-art methods for imbalanced data. Concretely, the new sampling technique produces the best results on 12 out of 24 real life as well as synthetic datasets. For comparison, the SMOTE method achieves the top score on only 1 dataset. We conclude that the new technique offers a simple yet effective sampling approach to balance data. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106368	10.1016/j.knosys.2020.106368													
J								An efficient multi-label learning method with label projection	KNOWLEDGE-BASED SYSTEMS										Multi-label classification; Variational inference; SVMs; Label projection; Labels relationship	CLASSIFICATION; CLASSIFIERS; ENSEMBLES; SELECTION	Multi-label classification (MLC) is a problem that each given sample is associated with more than one label simultaneously. There is a variety of application in our daily life, such as text categorization and image annotation. To date, many methodologies are proposed to do a multi-label learning task. According to the MLC setting, we put forward an MLC method called TPMLC (an MLC method with Two Parts) and propose a uniform loss function based on the variational inference with Bayesian and Gaussian distribution assumption. Moreover, this uniform loss function is composed of two parts. On one hand, the first part is about the determination of the relationship between sample and multiple labels, so we adopt a set of multiple support vector machines (SVMs) to determine this relationship. On the other hand, the second part in this uniform loss function is about the determination of the relationship among multiple labels, and thus we construct a projection matrix model to determine this relationship. Furthermore, this uniform loss function is optimized simultaneously, such that the two kinds of relationships can be optimized at the same time. Besides, we also present the convergence analysis and computational complexity analysis of the method. After that, in the experiment part, the comparison of TPMLC with state-of-the-art approaches manifests the feasibility and the competitive performance in classification. In addition, the statistic results show that the proposed method performs better than the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106298	10.1016/j.knosys.2020.106298													
J								Visual Question Answering via Combining Inferential Attention and Semantic Space Mapping	KNOWLEDGE-BASED SYSTEMS										Visual Question Answering; Inferential attention; Semantic space mapping		Visual Question Answering (VQA) has emerged and aroused widespread interest in recent years. Its purpose is to explore the close correlations between the image and question for answer inference. We have two observations about the VQA task: (1) the number of newly defined answers is ever-growing, which means that answer prediction on pre-defined labeled answers may lead to errors, as some unlabeled answers may be the right choice to the question-image pairs; (2) in the process of answering visual questions, the gradual change of human attention has an important guiding role in exploring the correlations between images and questions. Based on these observations, we propose a novel model for VQA, i.e., combining Inferential Attention and Semantic Space Mapping (IASSM). Specifically, our model has two salient aspects: (1) a semantic space shared by both the labeled and unlabeled answers is constructed to learn new answers, where the joint embedding of a question and the corresponding image is mapped and clustered around the answer exemplar; (2) a novel inferential attention model is designed to simulate the learning process of human attention to explore the correlations between the image and question. It focuses on the more important question words and image regions associated with the question. Both the inferential attention and the semantic space mapping modules are integrated into an end-to-end framework to infer the answer. Experiments performed on two public VQA datasets and our newly constructed dataset show the superiority of IASSM compared with existing methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106339	10.1016/j.knosys.2020.106339													
J								Several alternative term weighting methods for text representation and classification	KNOWLEDGE-BASED SYSTEMS										Unsupervised term weighting; Supervised term weighting; Text representation; Text classification; Nonlinear transformation	FEATURE-SELECTION; NAIVE BAYES; FREQUENCY; SCHEMES; FRAMEWORK	Text representation is one kind of hot topics which support text classification (TC) tasks. It has a substantial impact on the performance of TC. Although the most famous TF-IDF is specially designed for information retrieval rather than TC tasks, it is highly useful in the field of TC as a term weighting method to represent text contents. Inspired by the IDF part of TF-IDF which is defined as the logarithmic transformation, we proposed several alternative methods in this study to generate unsupervised term weighting schemes that can offset the drawback confronting TF-IDF. Moreover, owing to TC tasks are different from information retrieval, representing test texts as a vector in an appropriate way is also essential for TC tasks, especially for supervised term weighting approaches (e.g., TF-RF), mainly due to these methods need to use category information when weighting the terms. But most of current schemes do not clearly explain how to represent test texts with their schemes. To explore this problem and seek a reasonable solution to these schemes, we analyzed a classic unsupervised term weighting method and three typical supervised term weighting methods in depth to illustrate how to represent test texts. To investigate the effectiveness of our work, three sets of experiments are designed to compare their performance. Comparisons show that our proposed methods can indeed enhance the performance of TC, and sometimes even outperform existing supervised term weighting methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106399	10.1016/j.knosys.2020.106399													
J								The Structured Smooth Adjustment for Square-root Regularization: Theory, algorithm and applications	KNOWLEDGE-BASED SYSTEMS										High-dimensional data; Noise level; Oracle inequality; Sparsity; Structured Smooth Adjustment for Square-root Regularization	VARIABLE SELECTION; LASSO; REGRESSION; RECOVERY	In this paper, a novel method called Structured Smooth Adjustment for Square-root Regularization (SSASR) is proposed to simultaneously select grouped variables and encourage piecewise smoothness within each group. This approach is based on square-root regularization with a joint l(2,1) norm regularizer that, like the group lasso, shrinks a group of coefficients to identically zero and, additionally, involves an additional IGTV regularizer to enforce certain structural constraints - instead of pure sparsity - on the coefficients. We show the SSASR estimator can achieve optimal estimation and prediction, which is adaptive to the unknown noise level, under some mild conditions on the design matrix. To implement, an efficient algorithm termed Scaled Dual Forward-backward Splitting is proposed with proved convergence. Furthermore, we carry out an experimental evaluation on both synthetic data and real data obtained from glioblastoma multiforme samples and gray images. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106278	10.1016/j.knosys.2020.106278													
J								Near real-time topic-driven rumor detection in source microblogs	KNOWLEDGE-BASED SYSTEMS										Rumor detection; Topic vector; Latent dirichlet allocation (LDA); Source microblogs		Rumors can be propagated across online microblogs at a relatively low cost, but result in a series of major problems in our society. Traditional rumor detection approaches focus on exploring various propagation patterns or data interactions between a source microblog and its subsequent reactions. It is obvious that this causes missing interaction on rumor detection, especially in the absence of retweets or reactions. According to the communication theory of Allport and Postman (1947), Chorus (1953) and Rosnow (1988), the topic of a post can help determine its potential of being a rumor or not. Therefore, we develop a novel topic-driven rumor detection (TDRD) framework to determine whether a post is a rumor only according to its source microblog. Specifically, we first automatically perform topic classification on source microblogs, and then we successfully incorporate the predicted topic vector of the source microblogs into rumor detection. Our extensive experimental results demonstrate that our TDRD significantly outperforms state-of-the-art methods on both two English and two Chinese benchmark datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106391	10.1016/j.knosys.2020.106391													
J								Discriminative and informative joint distribution adaptation for unsupervised domain adaptation	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Joint distribution adaptation; Maximum margin criterion; Row-sparsity	TRANSFER SUBSPACE; LOW-RANK; FRAMEWORK; KERNEL; REGULARIZATION; REPRESENTATION	Domain adaptation learning is proposed as an effective technology for leveraging rich supervision knowledge from the related domain(s) to learn a reliable classifier for a new domain. One popular kind of domain adaptation methods is based on feature representation. However, such methods fail to consider the within-class and between-class relations after obtaining the new representation. In addition, they do not consider the negative effects of features that might be redundant or irrelevant to the final classification. To this end, a novel domain-invariant feature learning method based on the maximum margin criterion and sparsity technique for unsupervised domain adaptation is proposed in this paper, referred to as discriminative and informative joint distribution adaptation (DIJDA). Specifically, DIJDA adopts the maximum margin criterion in the adaptation process such that the transformed samples are near to those in the same class but segregated from those in different classes. As a result, the discriminative knowledge referred from source labels can be transferred to target domain effectively. Moreover, DIJDA imposes a row-sparsity constraint on the transformation matrix, which enforces rows of the matrix corresponding to inessential feature attributes to be all zero. Therefore, the most informative feature attributes can be extracted. Compared with several state-ofthe-art methods, DIJDA substantially improves the classification results on five widely used benchmark datasets, which demonstrates the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106394	10.1016/j.knosys.2020.106394													
J								Robust discriminant feature selection via joint L-2,L-1-norm distance minimization and maximization	KNOWLEDGE-BASED SYSTEMS										Feature selection; L-2,L-1-norm; Discriminative Feature Selection; Robustness	ALGORITHM	Discriminative Feature Selection (DFS) is an algorithm, proposed recently for effective feature selection by considering both joint linear discriminant analysis and row sparsity regularization. However, this method is not robust enough to protect the data from outliers, because it utilizes the squared L-2-norm distance metric. To overcome this problem, we present in this paper, a novel discriminative feature selection algorithm, which uses the robust L-2,L-1-norm for measuring the distances in DFS. Although the algorithm is apparently simple, it should not be considered trivial because of its non-convexity. Also, we present an analysis of the convergence, both theoretical and empirical. More importantly, we proposed an iterative algorithm to achieve optimal results. Experimental results, using various data sets, demonstrate the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106090	10.1016/j.knosys.2020.106090													
J								A memory network based end-to-end personalized task-oriented dialogue generation	KNOWLEDGE-BASED SYSTEMS										Task-oriented dialogue system; Dialogue generation; Personalized response	SYSTEM	Building a personalized task-oriented dialogue system is an important but challenging task. Significant success has been achieved in the template selection responses. However, preparing a massive response template is time-consuming and human-labor intensive. In this paper, we propose an end-to-end framework based on memory networks for response generation in a personalized task-oriented dialogue system. Our model consists of three parts: a retrieval module, a memory encoder network and a memory decoder network. Retrieval module employs the user utterances and user attributes to collect relevant responses from other users. Memory encoder is trained with textual features to obtain dialogue representation. Memory decoder is composed of an RNN and a rule-memory network for response generation. Experiments on the benchmark dataset show that our model achieves better performance than strong baselines in personalized task-oriented dialogue generation. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106398	10.1016/j.knosys.2020.106398													
J								Cross-sentence N-ary relation classification using LSTMs on graph and sequence structures	KNOWLEDGE-BASED SYSTEMS										N-ary relation classification; Graph Neural Network; Cross-sentence; Dependency tree; Graph attention; N-gram		Relation classification is an important semantic processing task in the field of Natural Language Processing (NLP). The past works mainly focused on binary relations in a single sentence. Recently, cross-sentence N-ary relation classification, which detects relations among n entities across multiple sentences, has been arousing people's interests. The dependency tree based methods and some Graph Neural Network (GNN) based methods have been carried out to convey rich structural information. However, it is challenging for researchers to fully use the relevant information while ignore the irrelevant information from the dependency trees. In this paper, we propose a Graph Attention-based LSTM (GA LSTM) network to make full use of the relevant graph structure information. The dependency tree of multiple sentences is divided into many subtrees whose root node is a word in the sentence and the leaf nodes are regarded as the neighborhood. A graph attention mechanism is used to aggregate the local information in the neighborhood. Using this network, we identify the relevant information from the dependency tree. On the other hand, because the GNNs highly depend on the graph structure of the sentence and lack context sequence structural information, their effectiveness to the task is limited. To tackle this problem, we propose an N-gram Graph LSTM (NGG LSTM) network, which updates the hidden states by aggregating graph neighbor node information and the inherent sequence structural information of sentence. The experimental results show that our methods outperform most of the existing methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				NOV 5	2020	207								106266	10.1016/j.knosys.2020.106266													
J								Combining experts' causal judgments	ARTIFICIAL INTELLIGENCE										Causality; Intervention; Combining causal judgments; Complexity		Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts' opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts' causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being compatible, and show how compatible causal models can be merged. We then use it as the basis for combining experts' causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103355	10.1016/j.artint.2020.103355													
J								Fixed point semantics for stream reasoning	ARTIFICIAL INTELLIGENCE										Dynamic data; Answer set programming; Stream reasoning	KNOWLEDGE REPRESENTATION; LOGIC	Reasoning over streams of input data is an essential part of human intelligence. During the last decade stream reasoning has emerged as a research area within the AI-community with many potential applications. In fact, the increased availability of streaming data via services like Google and Facebook has raised the need for reasoning engines coping with data that changes at high rate. Recently, the rule-based formalism LARS for non-monotonic stream reasoning under the answer set semantics has been introduced. Syntactically, LARS programs are logic programs with negation incorporating operators for temporal reasoning, most notably window operators for selecting relevant time points. Unfortunately, by preselecting fixed intervals for the semantic evaluation of programs, the rigid semantics of LARS programs is not flexible enough to constructively cope with rapidly changing data dependencies. Moreover, we show that defining the answer set semantics of LARS in terms of FLP reducts leads to undesirable circular justifications similar to other ASP extensions. This paper fixes all of the aforementioned shortcomings of LARS. More precisely, we contribute to the foundations of stream reasoning by providing an operational fixed point semantics for a fully flexible variant of LARS and we show that our semantics is sound and constructive in the sense that answer sets are derivable bottom-up and free of circular justifications. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103370	10.1016/j.artint.2020.103370													
J								Negotiating team formation using deep reinforcement learning	ARTIFICIAL INTELLIGENCE										Multi-agent systems; Team formation; Coalition formation; Reinforcement learning; Deep learning; Cooperative games; Shapley value	COOPERATIVE GAME-THEORY; COLLUSION; AGENTS; POWER; NASH; IDENTIFICATION; CONVERGENCE; PRICE; PLAY	When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103356	10.1016/j.artint.2020.103356													
J								Price of Pareto Optimality in hedonic games	ARTIFICIAL INTELLIGENCE										Pareto optimality; Price of Pareto Optimality; Coalition formation; Hedonic games	STABILITY	The Price of Anarchy measures the welfare loss caused by selfish behavior: it is defined as the ratio of the social welfare in a socially optimal outcome and in a worst Nash equilibrium. Similar measures can be derived for other classes of stable outcomes. We observe that Pareto optimality can be seen as a notion of stability: an outcome is Pareto optimal if and only if it does not admit a deviation by the grand coalition that makes all players weakly better off and some players strictly better off. Motivated by this observation, we introduce the concept of Price of Pareto Optimality: this is an analogue of the Price of Anarchy, with the worst Nash equilibrium replaced with the worst Pareto optimal outcome. We then study this concept in the context of hedonic games, and provide lower and upper bounds on the Price of Pareto Optimality in three classes of hedonic games: additively separable hedonic games, fractional hedonic games, and modified fractional hedonic games. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103357	10.1016/j.artint.2020.103357													
J								Utilitarian welfare and representation guarantees of approval-based multiwinner rules	ARTIFICIAL INTELLIGENCE										Computational social choice; Multiwinner voting; Approval preferences	OPTIMAL SOCIAL CHOICE; PROPORTIONAL REPRESENTATION; SET	To choose a suitable multiwinner voting rule is a hard and ambiguous task. Depending on the context, it varies widely what constitutes the choice of an "optimal" subset of alternatives. In this paper, we provide a quantitative analysis of multiwinner voting rules using methods from the theory of approximation algorithms-we estimate how well multiwinner rules approximate two extreme objectives: a representation criterion defined via the Approval Chamberlin-Courant rule and a utilitarian criterion defined via Multiwinner Approval Voting. With both theoretical and experimental methods, we classify multiwinner rules in terms of their quantitative alignment with these two opposing objectives. Our results provide fundamental information about the nature of multiwinner rules and, in particular, about the necessary tradeoffs when choosing such a rule. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103366	10.1016/j.artint.2020.103366													
J								Probability pooling for dependent agents in collective learning	ARTIFICIAL INTELLIGENCE										Probability pooling; Copulas; Collective learning; Dependent agents	INFORMATION	The use of copulas is proposed as a way of modelling dependencies between different agents' probability judgements when carrying out probability pooling. This is combined with an established Bayesian model in which pooling is viewed as a form of updating on the basis of probability values provided by different individuals. Adopting the Frank family of copulas we investigate the effect of different assumed levels of comonotonic dependence between individuals, in the context of a collective learning problem in which a population of agents must reach consensus on which of two mutually exclusive and exhaustive hypotheses is true. In this scenario agents receive evidence from two sources; directly from the environment and also from other agents in the form of probability judgements. They then apply Bayesian updating to the former and probability pooling to the latter. We carry out multi-agent simulation experiments and show that optimal population level performance is obtained under the assumption of some degree of comonotonicity between agents, and consequently show that the standard assumption of agent independence is suboptimal. This is found to be particularly true of scenarios where there is a large amount of noise and very low amounts of direct evidence. Finally, we investigate dynamic environments in which the true state of the world changes and show that identifying the optimal level of agent dependency has an even greater effect on performance than for static environments in which the true state remains constant. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103371	10.1016/j.artint.2020.103371													
J								Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations	ARTIFICIAL INTELLIGENCE										Explainable AI; Reinforcement learning; Interestingness elements; Autonomy; Video highlights; Visual explanations		We propose an explainable reinforcement learning (XRL) framework that analyzes an agent's history of interaction with the environment to extract interestingness elements that help explain its behavior. The framework relies on data readily available from standard RL algorithms, augmented with data that can easily be collected by the agent while learning. We describe how to create visual summaries of an agent's behavior in the form of short video-clips highlighting key interaction moments, based on the proposed elements. We also report on a user study where we evaluated the ability of humans to correctly perceive the aptitude of agents with different characteristics, including their capabilities and limitations, given visual summaries automatically generated by our framework. The results show that the diversity of aspects captured by the different interestingness elements is crucial to help humans correctly understand an agent's strengths and limitations in performing a task, and determine when it might need adjustments to improve its performance. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103367	10.1016/j.artint.2020.103367													
J								Knowledge-based programs as succinct policies for partially observable domains	ARTIFICIAL INTELLIGENCE										Planning under uncertainty; Contingent planning; Epistemic logic; Knowledge-based programs; Belief tracking	MARKOV-PROCESSES; BELIEF TRACKING; COMPLEXITY	We suggest to express policies for contingent planning by knowledge-based programs (KBPs). KBPs, introduced by Fagin et al. (1995) [32], are high-level protocols describing the actions that the agent should perform as a function of their current knowledge: branching conditions are epistemic formulas that are interpretable by the agent. The main aim of our paper is to show that KBPs can be seen as a succinct language for expressing policies in single-agent contingent planning. KBP are conceptually very close to languages used for expressing policies in the partially observable planning literature: like them, they have conditional and looping structures, with actions as atomic programs and Boolean formulas on beliefs for choosing the execution path. Now, the specificity of KBPs is that branching conditions refer to the belief state and not to the observations. Because of their structural proximity, KBPs and standard languages for representing policies have the same power of expressivity: every standard policy can be expressed as a KBP, and every KBP can be "unfolded" into a standard policy. However, KBPs are more succinct, more readable, and more explainable than standard policies. On the other hand, they require more online computation time, but we show that this is an unavoidable tradeoff. We study knowledge-based programs along four criteria: expressivity, succinctness, complexity of online execution, and complexity of verification. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				NOV	2020	288								103365	10.1016/j.artint.2020.103365													
J								A Cerebellar Computational Mechanism for Delay Conditioning at Precise Time Intervals	NEURAL COMPUTATION											PURKINJE-CELL ACTIVITY; EYEBLINK; TRACE; CORTEX; REPRESENTATION; RELIABILITY; MEMORY; MODEL	The cerebellum is known to have an important role in sensing and execution of precise time intervals, but the mechanism by which arbitrary time intervals can be recognized and replicated with high precision is unknown. We propose a computational model in which precise time intervals can be identified from the pattern of individual spike activity in a population of parallel fibers in the cerebellar cortex. The model depends on the presence of repeatable sequences of spikes in response to conditioned stimulus input. We emulate granule cells using a population of Izhikevich neuron approximations driven by random but repeatable mossy fiber input. We emulate long-term depression (LTD) and long-term potentiation (LTP) synaptic plasticity at the parallel fiber to Purkinje cell synapse. We simulate a delay conditioning paradigm with a conditioned stimulus (CS) presented to the mossy fibers and an unconditioned stimulus (US) some time later issued to the Purkinje cells as a teaching signal. We show that Purkinje cells rapidly adapt to decrease firing probability following onset of the CS only at the interval for which the US had occurred. We suggest that detection of replicable spike patterns provides an accurate and easily learned timing structure that could be an important mechanism for behaviors that require identification and production of precise time intervals.																	0899-7667	1530-888X				NOV	2020	32	11					2069	2084		10.1162/neco_a_01318													
J								Reverse-Engineering Neural Networks to Characterize Their Cost Functions	NEURAL COMPUTATION											FREE-ENERGY PRINCIPLE; LOCAL LEARNING RULE; ACTIVE INFERENCE; BRAIN; INFORMATION; PLASTICITY; DEPENDENCE; NEURONS	This letter considers a class of biologically plausible cost functions for neural networks, where the same cost function is minimized by both neural activity and plasticity. We show that such cost functions can be cast as a variational bound on model evidence under an implicit generative model. Using generative models based on partially observed Markov decision processes (POMDP), we show that neural activity and plasticity perform Bayesian inference and learning, respectively, by maximizing model evidence. Using mathematical and numerical analyses, we establish the formal equivalence between neural network cost functions and variational free energy under some prior beliefs about latent states that generate inputs. These prior beliefs are determined by particular constants (e.g., thresholds) that define the cost function. This means that the Bayes optimal encoding of latent or hidden states is achieved when the network's implicit priors match the process that generates its inputs. This equivalence is potentially important because it suggests that any hyperparameter of a neural network can itself be optimized-by minimization with respect to variational free energy. Furthermore, it enables one to characterize a neural network formally, in terms of its prior beliefs.																	0899-7667	1530-888X				NOV	2020	32	11					2085	2121		10.1162/neco_a_01315													
J								Closed-Loop Deep Learning: Generating Forward Models With Backpropagation	NEURAL COMPUTATION											REPRESENTATIONS	A reflex is a simple closed-loop control approach that tries to minimize an error but fails to do so because it will always react too late. An adaptive algorithm can use this error to learn a forward model with the help of predictive cues. For example, a driver learns to improve steering by looking ahead to avoid steering in the last minute. In order to process complex cues such as the road ahead, deep learning is a natural choice. However, this is usually achieved only indirectly by employing deep reinforcement learning having a discrete state space. Here, we show how this can be directly achieved by embedding deep learning into a closed-loop system and preserving its continuous processing. We show in z-space specifically how error backpropagation can be achieved and in general how gradient-based approaches can be analyzed in such closed-loop scenarios. The performance of this learning paradigm is demonstrated using a line follower in simulation and on a real robot that shows very fast and continuous learning.																	0899-7667	1530-888X				NOV	2020	32	11					2122	2144		10.1162/neco_a_01317													
J								Assessing Goodness-of-Fit in Marked Point Process Models of Neural Population Coding via Time and Rate Rescaling	NEURAL COMPUTATION											TESTING MULTIVARIATE UNIFORMITY	Marked point process models have recently been used to capture the coding properties of neural populations from multiunit electrophysiological recordings without spike sorting. These clusterless models have been shown in some instances to better describe the firing properties of neural populations than collections of receptive field models for sorted neurons and to lead to better decoding results. To assess their quality, we previously proposed a goodness-of-fit technique for marked point process models based on time rescaling, which for a correct model produces a set of uniform samples over a random region of space. However, assessing uniformity over such a region can be challenging, especially in high dimensions. Here, we propose a set of new transformations in both time and the space of spike waveform features, which generate events that are uniformly distributed in the new mark and time spaces. These transformations are scalable to multidimensional mark spaces and provide uniformly distributed samples in hypercubes, which are well suited for uniformity tests. We discuss the properties of these transformations and demonstrate aspects of model fit captured by each transformation. We also compare multiple uniformity tests to determine their power to identify lack-of-fit in the rescaled data. We demonstrate an application of these transformations and uniformity tests in a simulation study. Proofs for each transformation are provided in the appendix.																	0899-7667	1530-888X				NOV	2020	32	11					2145	2186		10.1162/neco_a_01321													
J								Inferring Neuronal Couplings From Spiking Data Using a Systematic Procedure With a Statistical Criterion	NEURAL COMPUTATION											FUNCTIONAL CONNECTIVITY; NETWORKS; MODEL; PATTERNS	Recent remarkable advances in experimental techniques have provided a background for inferring neuronal couplings from point process data that include a great number of neurons. Here, we propose a systematic procedure for pre- and postprocessing generic point process data in an objective manner to handle data in the framework of a binary simple statistical model, the Ising or generalized McCulloch-Pitts model. The procedure has two steps: (1) determining time bin size for transforming the point process data into discrete-time binary data and (2) screening relevant couplings from the estimated couplings. For the first step, we decide the optimal time bin size by introducing the null hypothesis that all neurons would fire independently, then choosing a time bin size so that the null hypothesis is rejected with the strict criteria. The likelihood associated with the null hypothesis is analytically evaluated and used for the rejection process. For the second postprocessing step, after a certain estimator of coupling is obtained based on the preprocessed data set (any estimator can be used with the proposed procedure), the estimate is compared with many other estimates derived from data sets obtained by randomizing the original data set in the time direction. We accept the original estimate as relevant only if its absolute value is sufficiently larger than those of randomized data sets. These manipulations suppress false positive couplings induced by statistical noise. We apply this inference procedure to spiking data from synthetic and in vitro neuronal networks. The results show that the proposed procedure identifies the presence or absence of synaptic couplings fairly well, including their signs, for the synthetic and experimental data. In particular, the results support that we can infer the physical connections of underlying systems in favorable situations, even when using a simple statistical model.																	0899-7667	1530-888X				NOV	2020	32	11					2187	2211		10.1162/neco_a_01324													
J								Repetitive Control for Multi-Joint Arm Movements Based on Virtual Trajectories	NEURAL COMPUTATION											EQUILIBRIUM TRAJECTORIES; STIFFNESS	According to the neuromuscular model of virtual trajectory control, the postures and movements of limbs are performed by shifting the equilibrium positions determined by agonist and antagonist muscle activities. In this study, we develop virtual trajectory control for the reaching movements of a multi-joint arm, introducing a proportional-derivative feedback control scheme. In virtual trajectory control, it is crucial to design a suitable virtual trajectory such that the desired trajectory can be realized. To this end, we propose an algorithm for updating virtual trajectories in repetitive control, which can be regarded as a Newton-like method in a function space. In our repetitive control, the virtual trajectory is corrected without explicit calculation of the arm dynamics, and the actual trajectory converges to the desired trajectory. Using computer simulations, we assessed the proposed repetitive control for the trajectory tracking of a two-link arm. Our results confirmed that when the feedback gains were reasonably high and the sampling time was sufficiently small, the virtual trajectory was adequately updated, and the desired trajectory was almost achieved within approximately 10 iterative trials. We also propose a method for modifying the virtual trajectory to ensure that the formation of the actual trajectory is identical even when the feedback gains are changed. This modification method makes it possible to execute flexible control, in which the feedback gains are effectively altered according to motion tasks.																	0899-7667	1530-888X				NOV	2020	32	11					2212	2236		10.1162/neco_a_01322													
J								Bicomplex Projection Rule for Complex-Valued Hopfield Neural Networks	NEURAL COMPUTATION											ASSOCIATIVE MEMORY	A complex-valued Hopfield neural network (CHNN) with a multistate activation function is a multistate model of neural associative memory. The weight parameters need a lot of memory resources. Twin-multistate activation functions were introduced to quaternion- and bicomplex-valued Hopfield neural networks. Since their architectures are much more complicated than that of CHNN, the architecture should be simplified. In this work, the number of weight parameters is reduced by bicomplex projection rule for CHNNs, which is given by the decomposition of bicomplex-valued Hopfield neural networks. Computer simulations support that the noise tolerance of CHNN with a bicomplex projection rule is equal to or even better than that of quaternion- and bicomplex-valued Hopfield neural networks. By computer simulations, we find that the projection rule for hyperbolic-valued Hopfield neural networks in synchronous mode maintains a high noise tolerance.																	0899-7667	1530-888X				NOV	2020	32	11					2237	2248		10.1162/neco_a_01320													
J								ReLU Networks Are Universal Approximators via Piecewise Linear or Constant Functions	NEURAL COMPUTATION											DEEP	This letter proves that a ReLU network can approximate any continuous function with arbitrary precision by means of piecewise linear or constant approximations. For univariate function f(x), we use the composite of ReLUs to produce a line segment; all of the subnetworks of line segments comprise a ReLU network, which is a piecewise linear approximation to f(x). For multivariate function f(x), ReLU networks are constructed to approximate a piecewise linear function derived from triangulation methods approximating f(x). A neural unit called TRLU is designed by a ReLU network; the piecewise constant approximation, such as Haar wavelets, is implemented by rectifying the linear output of a ReLU network via TRLUs. New interpretations of deep layers, as well as some other results, are also presented.																	0899-7667	1530-888X				NOV	2020	32	11					2249	2278		10.1162/neco_a_01316													
J								Effect of Top-Down Connections in Hierarchical Sparse Coding	NEURAL COMPUTATION											SET	Hierarchical sparse coding (HSC) is a powerful model to efficiently represent multidimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest interconnecting these subproblems as in predictive coding (PC) theory, which adds top-down connections between consecutive layers. In this study, we introduce a new model, 2-layer sparse predictive coding (2L-SPC), to assess the impact of this interlayer feedback connection. In particular, the 2L-SPC is compared with a hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and a 2-layer Hi-La networks are trained on four different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge and generates a refined representation in the second layer compared to the Hi-La model. Third, we show that the 2L-SPC top-down connection accelerates the learning process of the HSC problem. Finally, the analysis of the emerging dictionaries shows that the 2L-SPC features are more generic and present a larger spatial extension.																	0899-7667	1530-888X				NOV	2020	32	11					2279	2309		10.1162/neco_a_01325													
J								An adaptive data detection algorithm based on intermittent chaos with strong noise background	NEURAL COMPUTING & APPLICATIONS										Intermittent chaos; Adaptive step size; Line spectrum detection; Low SNR; <mml; math><mml; mrow><mml; mn>1</mml; mn><mml; mfrac><mml; mn>1</mml; mn><mml; mn>2</mml; mn></mml; mfrac></mml; mrow></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$1; frac{1}{2}$$; end{document}<inline-graphic xlink; href="521_2018_3839_Article_IEq1; gif"; >-dimension spectrum		In order to realize the signal detection under the condition of lower SNR, this paper introduced the adaptive phase length based on the Duffing chaotic system and verified the measured signal at the optimal excitation frequency. The existence of the target signal was judged by observing whether there are two consecutive intermittent chaos in the time domain. Then the envelope of the intermittent chaos was obtained by Hilbert transform. Finally, the exact value of envelope spectrum was obtained by using the one-and-half-dimension spectrum, which can calculate the precise value of the frequency of the signal to be measured. The experimental results showed that the proposed algorithm can achieve a lower SNR than the conventional detection. Compared with the general chaotic detection, this algorithm can realize smart self-adaptation. It is unnecessary to specify different excitation frequencies and chaotic thresholds for different frequencies to be measured. In addition to the existence of the target signal judgment, the algorithm can also achieve accurate calculation of the frequency of the signal to be measured.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16755	16762		10.1007/s00521-018-3839-9													
J								Stacking model of multi-label classification based on pruning strategies	NEURAL COMPUTING & APPLICATIONS										Multi-label classification; Label dependence; Feature selection; ReliefF		Exploiting dependencies between the labels is the key of improving the performance of multi-label classification. In this paper, we divide the utilizing methods of label dependence into two groups from the perspective of different ways of problem transformation: label grouping method and feature space extending method. As to the feature space extending method, we find that the common problem is how to measure the dependencies between labels and to select proper labels to add to the original feature space. Therefore, we propose a ReliefF-based pruning model for multi-label classification (ReliefF-based stacking, RFS). RFS measures the dependencies between labels in a feature selection perspective and then selects the more relative labels into the original feature space. Experimental results of 9 multi-label benchmark datasets shows that RFS is more effective compared to other advanced multi-label classification algorithms.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16763	16774		10.1007/s00521-018-3888-0													
J								A sonar image segmentation algorithm based on quantum-inspired particle swarm optimization and fuzzy clustering	NEURAL COMPUTING & APPLICATIONS										Sonar image segmentation; Particle swarm optimization; Quantum-inspired; Fuzzy clustering		The underwater sonar image has the characteristics of complex background and heavy noise pollution. By using multi-bit quantum system to encode particles, a new sonar image segmentation algorithm based on quantum-inspired particle swarm and fuzzy clustering is proposed. Based on its own optimal particles and global optimal particles, the rotation angle is determined. By calculating the variance of the particle group fitness, the multi-bit quantum revolving gate is used to update the particle position in real time. The output of improved particle swarm optimization is used to initialize the K-mean clustering center to converge to the global optimal solution. Based on the idea of fuzzy membership matrix in FCM, combined with the isolated spatial information characteristics of the noise, the sonar image segmentation and the denoising are carried out. The experimental results show that the proposed algorithm can improve the global search ability of particle swarm optimization effectively. It is better than the quantum fuzzy clustering and quantum genetic algorithm in image segmentation. The analysis results of multiple real underwater sonar images show that the new optimization algorithm has faster convergence speed, better optimizing capacity and better segmentation results for sonar images.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16775	16782		10.1007/s00521-018-3890-6													
J								A novel numerical optimization algorithm inspired from garden balsam	NEURAL COMPUTING & APPLICATIONS										Artificial intelligent; Evolutionary computing; Swarm intelligence; Garden balsam optimization algorithm; Function optimization	LEARNING-BASED OPTIMIZATION; ARTIFICIAL BEE COLONY; FUZZY NEURAL-NETWORK; SEARCH ALGORITHM; IMMUNE-SYSTEM; EVOLUTION; DESIGN	This paper introduces a new evolutionary computing method inspired by the seed transmission process of garden balsam. Garden balsam, a beautiful and attractive flower, randomly ejects the seeds within a certain range by virtue of mechanical force originating from cracking of mature seed pods, which is different from natural expansion of most species of plants. The seeds scattered to suitable growth area will have greater reproductive capacity in the next generation, followed by iteration until the most suitable point for growth in a particular space is eventually found. This phenomenon can more intuitively show the process of searching the problem solution space in the optimization problem. The garden balsam optimization algorithm proposed in this paper incorporates two different types of search processes and has a mechanism to maintain population diversity. Through the optimization experiment on 24 constrained optimization problems, the results obtained by using this algorithm are compared with those of some known meta-heuristic search algorithms. The statistical analysis of the experimental results has been implemented by Friedman rank test and Holm-Sidak test. The comparison results verify the effectiveness of the algorithm.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16783	16794		10.1007/s00521-018-3905-3													
J								Surface EMG data aggregation processing for intelligent prosthetic action recognition	NEURAL COMPUTING & APPLICATIONS										Data aggregation; Signal processing; Support vector machine; Generalized regression neural network	FEATURE-EXTRACTION; SIMULATION; SIGNAL; TEMPERATURE; PARAMETERS; DESIGN	In the current development and design of sports rehabilitation equipment or biomimetic prostheses, in addition to pay attention to the development and design of the structure, the more core is how to realize the accurate and effective control of the rehabilitation equipment or intelligent prosthesis, and the current research is based on data process and pattern recognition. This paper designs nine kinds of actions that can react effectively to the function of the hand and extracts the original EMG signals, which are based on the sEMG of the forearm muscles of human hand movement, and uses the 20-order comb filter and wavelet threshold to preprocess the signal, and uses the root-mean-square, wavelength and nonlinear characteristics sample entropy in time domain as three eigenvalues to construct the input feature vectors of the subsequent action classifier. Finally, the recognition of the hand movements is realized successfully through GRNN and SVM. The recognition rate is 98.64% in SVM classifier and 96.27% in GRNN classifier. Experimental results show that the SVM classifier is better than the GRNN classifier.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16795	16806		10.1007/s00521-018-3909-z													
J								Fractional stochastic resonance multi-parameter adaptive optimization algorithm based on genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Real-time adaptive adjustment; Bistable system; Simulation; Weak signal detection	ANOMALOUS DIFFUSION; BISTABLE SYSTEM	The output effect of fractional-order stochastic resonance (FOSR) system is affected by many factors such as input system parameters and noise intensity. In practice, many tests are needed to adjust parameters to achieve the optimal effect, and this way of "trial and error" greatly limits the application prospect of FOSR. Based on genetic algorithm, a suitable adaptive function was established to adjust the multiple parameters, including the fractional order, system parameters, and the input noise intensity of the fractional bistable system. Simulation results showed that the algorithm can achieve joint optimization of these parameters. It was proved that this algorithm is conducive to the real-time adaptive adjustment of the FOSR system in practical applications and conducive to the application and extension of FOSR in weak signal detection and other fields. The proposed algorithm has certain practical value.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16807	16818		10.1007/s00521-018-3910-6													
J								A machine learning-based scheme for the security analysis of authentication and key agreement protocols	NEURAL COMPUTING & APPLICATIONS										Authentication protocols; Machine learning; Formal analysis of protocol security; Protocol dataset		This paper proposes a novel machine learning-based scheme for the automatic analysis of authentication and key agreement protocols. Considering the traditional formal protocol analysis schemes, their analysis accuracies depend heavily on the prior knowledge possessed by the analyst and the subjective understanding of the protocol. The rapid development of artificial intelligence in security field shows that the ideal way to get rid of the dependency is to use machine learning. Hence, we elaborately compare more than 2000 protocol analysis results and select 500 most representative ones of them to build a protocol dataset. Combining the protocol representation method of traditional schemes, these selected protocols are expressed as weight matrixes based on security components. Furthermore, a machine learning-based security analysis model is proposed to automatically find the attacks of the protocol. For now, three types of attacks against authentication and key agreement protocols can be identified based on our model. And experiment results show that it can reach almost 72% upper-bound performance. From the derivative of the accuracy curves, it can be inferred that the performance of our scheme will definitely get better as the dataset expands.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16819	16831		10.1007/s00521-018-3929-8													
J								Advances in plant nutrition diagnosis based on remote sensing and computer application	NEURAL COMPUTING & APPLICATIONS										Plant nutrition diagnosis; Hyperspectral; Canopy color; Satellite remote sensing; UAV	LEAF CHLOROPHYLL CONTENT; CANOPY NITROGEN; SPECTRAL INDEXES; REFLECTANCE; WHEAT; INVERSION; MODEL; FIELD; DEFICIENCIES; INDICATOR	Hyperspectral remote sensing, visible light remote sensing and canopy color analysis have been widely concerned for rapid diagnosis of crop growth and nutrition. They are expected to develop into potential nondestructive diagnostic techniques for crop nitrogen nutrition in the new era on account of the advantages of stable, rapid, convenient and nondestructive results, together with the good correlation between canopy color parameter NRI and plant nitrogen nutrition index and yield satisfying the demand for nondestructive diagnosis of nitrogen nutrition, and their feasibility to monitor plant growth status and nitrogen nutrition level in real time and quickly. At present, with the rapid development of remote sensing satellite, unmanned aerial vehicles remote sensing and Internet of things, remote sensing will be more and more widely used in plant nutrition diagnosis.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16833	16842		10.1007/s00521-018-3932-0													
J								Prediction of corn price fluctuation based on multiple linear regression analysis model under big data	NEURAL COMPUTING & APPLICATIONS										Univariate nonlinear regression analysis; Big data; Multiple regression analysis; Price forecast	FORECAST	This paper mainly analyzes the changing trend of corn price and the factors that affect the price of corn. Using the data and regression analysis, the univariate nonlinear and multivariate linear regression models are established to predict the corn price, respectively. First, this paper establishes a univariate nonlinear regression model with time as the independent variable, and corn price is used as the dependent variable through the analysis of the trend of big data related to Chinese corn price from 2005 to 2016 by MATLAB, which is the computer-based analysis and processing method. The variation of the maize price with time was fitted. To a certain extent, the price trend of corn is predicted. However, the estimated price of corn in 2017 with this model will deviate from the actual value. According to the changes of related policies in our country, we analyzed the deviation of the original model, and the relationship between supply and demand is the main underlying factor that affects the price of corn. This paper selects maize-related big data from 2005 to 2016, we set its production consumption, import and export volume as independent variables, and we still use maize price as the dependent variable to establish a multiple linear regression model. At this stage, the time series analysis of the independent variable has obtained the forecast value of each independent variable in 2017, and then the model is used to predict the corn in 2017 more accurately.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16843	16855		10.1007/s00521-018-03970-4													
J								Intelligent active fault-tolerant system for multi-source integrated navigation system based on deep neural network	NEURAL COMPUTING & APPLICATIONS										Active fault-tolerant; One-class SVM; Fault detection; Deep neural network; State estimation		This paper proposes an intelligent active fault-tolerant system based on deep neural network. That is, an active fault-tolerant integrated navigation system is established by adding neural network to the fault-tolerant integrated navigation system based on one-class support vector machine fault detection algorithm. When there is no fault, the neural network trains each sub-filter; when there is a fault, the neural network which has been in the training state will predict the fault time data and use the neural network prediction data to replace the fault data into the main filter for fusion. It can be seen from the simulation analysis that the system can detect the fault of the navigation sub-filtering system well, and when the fault occurs, the prediction data of the neural network is used for information fusion. Simulation results show that the system can provide stable and reliable navigation under the condition of time-varying system and observation noise and complex environment.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16857	16874		10.1007/s00521-018-03975-z													
J								Model predictive and adaptive neural sliding mode control for three-dimensional path following of autonomous underwater vehicle with input saturation	NEURAL COMPUTING & APPLICATIONS										Autonomous underwater vehicle; Path following; MPC; SMC; RBFNN	UNDERACTUATED SURFACE VESSELS; TRAJECTORY-TRACKING; CONTROL DESIGN; ROBUST; FEEDBACK; SHIPS	With model uncertainties and input saturation, a novel control method is developed to steer an underactuated autonomous underwater vehicle that realizes the following of the planned path in three-dimensional (3D) space. Firstly, Serret-Frenet frame is applied as virtual target, and the path following errors model in 3D is built. Secondly, the control method which includes kinematic controller and dynamic controller was presented based on cascade control strategy. The kinematic controller, which is responsible for generating a series of constrained velocity signals, is designed based on model predictive control. The adaptive radial basis function neural network is used to estimate the model uncertainty caused by hydrodynamic parameters. Moreover, sliding mode control technology is applied in the design of dynamic controller to improve its robustness. Then, the control effect is compared with that of LOS guidance law and PID controller by simulation experiment. The comparison results show that the proposed algorithm can improve path following effect and reduce input saturation.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16875	16889		10.1007/s00521-018-03976-y													
J								An adaptive artificial-fish-swarm-inspired fuzzy C-means algorithm	NEURAL COMPUTING & APPLICATIONS										Cluster analysis; Data aggregation; Classifier; Global optimization	FCM CLUSTERING-ALGORITHM; CLASSIFICATION; SELECTION	Fuzzy C-means (FCM) is a classical algorithm of cluster analysis which has been applied to many fields including artificial intelligence, pattern recognition, data aggregation and their applications in software engineering, image processing, IoT, etc. However, it is sensitive to the initial value selection and prone to get local extremum. The classification effect is also unsatisfactory which limits its applications severely. Therefore, this paper introduces the artificial-fish-swarm algorithm (AFSA) which has strong global search ability and adds an adaptive mechanism to make it adaptively adjust the scope of visual value, improves its local and global optimization ability, and reduces the number of algorithm iterations. Then it is applied to the improved FCM which is based on the Mahalanobis distance, named as adaptive AFSA-inspired FCM(AAFSA-FCM). The optimal solution obtained by adaptive AFSA (AAFSA) is used for FCM cluster analysis to solve the problems mentioned above and improve clustering performance. Experiments show that the proposed algorithm has better clustering effect and classification performance with lower computing cost which can be better to apply to every relevant area, such as IoT, network analysis, and abnormal detection.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16891	16899		10.1007/s00521-018-03977-x													
J								Intelligent predicting of salt pond's ion concentration based on support vector regression and neural network	NEURAL COMPUTING & APPLICATIONS										Ion concentration; Support vector regression; Neural network; AdaBoost; Gradient boosting; Extra trees; Potash fertilizer	ALGORITHM	The constant dynamic changes in salt pond make it difficult to achieve accurate prediction of ion concentration. It is of great significance to get the accurate prediction of potassium ion concentration in salt pools for the actual production of potash fertilizer. In this paper, some machine learning methods, such as support vector regression (SVR), AdaBoost regressor, K neighbor regressor, gradient boosting regressor, extra trees regressor and neural network regressor, have been used to build the prediction models. In the experiment, the MSE andR(2)of the K(+)concentration by using SVR in test data set reach 0.26385 and 0.9414, which are better than other models. Therefore, the SVR model has high research value in the field of salt pool ion concentration prediction.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16901	16915		10.1007/s00521-018-03979-9													
J								Research on gesture recognition of smart data fusion features in the IoT	NEURAL COMPUTING & APPLICATIONS										Gesture recognition; Fusion features; Smart data aggregation; Hu moment; SVM	INTELLIGENT CONTROL; SIMULATION; DESIGN; ROBOTS; SPACE; HAND	With the rapid development of Internet of things technology, the interaction between people and things has become increasingly frequent. Using simple gestures instead of complex operations to interact with the machine, the fusion of smart data feature information and so on has gradually become a research hotspot. Considering that the depth image of the Kinect sensor lacks color information and is susceptible to depth thresholds, this paper proposes a gesture segmentation method based on the fusion of color information and depth information; in order to ensure the complete information of the segmentation image, a gesture feature extraction method based on Hu invariant moment and HOG feature fusion is proposed; and by determining the optimal weight parameters, the global and local features are effectively fused. Finally, the SVM classifier is used to classify and identify gestures. The experimental results show that the proposed fusion features method has a higher gesture recognition rate and better robustness than the traditional method.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16917	16929		10.1007/s00521-019-04023-0													
J								NCMS: Towards accurate anchor free object detection through l(2) norm calibration and multi-feature selection	COMPUTER VISION AND IMAGE UNDERSTANDING										Object detection; Norm calibration; Feature selection		We present simple and flexible drop-in modules in feature pyramids for general object detection, which can be easily generalized to other anchor-free detectors without introducing extra parameters, and only involves negligible computational cost on training and testing. The proposed detector, called NCMS, inserts a simple norm calibration (NC) operation between the feature pyramids and detection head to alleviate and balance the norm bias caused by feature pyramid network (FPN). Furthermore, the NCMS leverages an enhanced multi-feature selective strategy (MS) during training to assign the ground-truth to particular feature pyramid levels as supervisions, in order to obtain more discriminative representation for objects. By generalizing to the state-of-the-art FSAF module (Zhu et al., 2019), our NCMS improves it by 1.6% on COCO val set without bells and whistles. The resulting best model achieves 44.0% mAP with single-model and single-scale testing, which is a fairly competitive result.																	1077-3142	1090-235X				NOV	2020	200								103050	10.1016/j.cviu.2020.103050													
J								E-ProSRNet: An enhanced progressive single image super-resolution approach	COMPUTER VISION AND IMAGE UNDERSTANDING										CNN; Single image super-resolution; Residual network; Computationally efficient; Charbonnier loss function	INTERPOLATION	In recent years, the convolutional neural networks (CNNs) have been successfully applied to single image super-resolution (SISR) task. However, most of the CNN based SISR methods obtain better performance with a huge amount of training parameters which increases the computational complexity of their SISR models. Such SR networks suffer from a heavy burden on computational assets and as a result, they are no longer appropriate for many real-world applications. Hence, in the computer vision community, it is an interest to endorse an SR approach which makes use of less number of training parameters with better SR performance. In this paper, we propose a computationally efficient SR approach called enhanced progressive super-resolution network i.e., E-ProSRNet. This approach is the enhanced version of our base proposed model called ProSRNet. In E-ProSRNet model, we propose a novel enhanced parallel densely connected residual network (E-PDRN) which helps to extract rich features in the low-resolution (LR) observation. The SR performance of proposed E-ProSRNet model is better than that of ProSRNet and it uses a less number of training parameters when compared to that of ProSRNet model. The experimental analysis on common testing benchmark datasets shows that the proposed E-ProSRNet sets new state-of-the-art performance on SISR task for upscaling factor x4. The E-ProSRNet method obtains better SR performance when compared to that obtained using proposed ProSRNet as well as the other state-of-the-art methods with significant reduction in the computational complexity.																	1077-3142	1090-235X				NOV	2020	200								103038	10.1016/j.cviu.2020.103038													
J								Learning deep edge prior for image denoising	COMPUTER VISION AND IMAGE UNDERSTANDING										Denoising; Variational model; Total variation; Edge prior; CNN; Interpretability	RESTORATION; ALGORITHM; RECOVERY	Image restoration is an important technique to deal with the degradation of the image. This paper presents an efficient and trusty denoising scheme, which combines the convolutional neural network (CNN) technique with the traditional variational model, to offer interpretable and high quality reconstructions. In this scheme, CNN, which has proven effectiveness in feature extraction tasks, is adopted to obtain the designed edge features from the noisy images, to be the prior of the reconstruction through an edge regularization. In the proposed denoising model, the total variation (TV) regularization is also adopted for its superior performance in allowing the sharp edges. The solution of the proposed model is obtained by using the Bregman splitting method, with the existence and the uniqueness of the solution also analyzed in this paper. Extensive experiments show that the two regularizations combined in the proposed model are able to fix the staircasing defects effectively and retrieve the fine textures in the recovered images as well, which outperforms the state-of-the-art interpretable denoising methods. Moreover, the proposed edge regularization can be easily extended into other kinds of noise or other restoration tasks, which implies the strong adaptivity of the proposed scheme.																	1077-3142	1090-235X				NOV	2020	200								103044	10.1016/j.cviu.2020.103044													
J								Cross-spectral stereo matching for facial disparity estimation in the dark	COMPUTER VISION AND IMAGE UNDERSTANDING										Facial disparity estimation; Multi-spectral transfer; Deep learning	NETWORKS; SHAPE	Numerous applications on human faces hinge on depth information. Often, facial stereo matching provides an opportunity to estimate disparity without active projectors. However, existing algorithms are less effective at night due to unclear texture and severe noises in RGB images. In this paper, we address this problem by estimating facial disparity maps from NIR-RGB pairs. We develop a neural network composed of a multispectral transfer network (MSTN) and a disparity estimation network (DEN). MSTN is used to produce a pseudo-NIR image aligned with the RGB view using a spatially weighted sum on the NIR one by a kernel prediction network (KPN). As the pseudo-NIR and the NIR images share the same appearance, the facial disparity map is predicted by the proposed DEN with the same-spectral stereo pair. The whole network can be trained in an end-to-end manner and the experimental results demonstrate that it performs favorably against state-of-the-art algorithms on both synthetic and real data.																	1077-3142	1090-235X				NOV	2020	200								103046	10.1016/j.cviu.2020.103046													
J								Texture collinearity foreground segmentation for night videos	COMPUTER VISION AND IMAGE UNDERSTANDING										Background subtraction; Foreground segmentation; Change detection; GMM; MOG; Night videos; Texture features; Texture matching	GAUSSIAN MIXTURE MODEL; BACKGROUND SUBTRACTION; OBJECT DETECTION; DETECTION ALGORITHMS; DENSITY-ESTIMATION; NEURAL-NETWORK; COLOR; BENCHMARK; EFFICIENT; TRACKING	One of the most difficult scenarios for unsupervised segmentation of moving objects is found in nighttime videos where the main challenges are the poor illumination conditions resulting in low-visibility of objects, very strong lights, surface-reflected light, a great variance of light intensity, sudden illumination changes, hard shadows, camouflaged objects, and noise. This paper proposes a novel method, coined COLBMOG (COLlinearity Boosted MOG), devised specifically for the foreground segmentation in nighttime videos, that shows the ability to overcome some of the limitations of state-of-the-art methods and still perform well in daytime scenarios. It is a texture-based classification method, using local texture modeling, complemented by a color-based classification method. The local texture at the pixel neighborhood is modeled as an..-dimensional vector. For a given pixel, the classification is based on the collinearity between this feature in the input frame and the reference background frame. For this purpose, a multimodal temporal model of the collinearity between texture vectors of background pixels is maintained. COLBMOG was objectively evaluated using the ChangeDetection.net (CDnet) 2014, Night Videos category, benchmark. COLBMOG ranks first among all the unsupervised methods. A detailed analysis of the results revealed the superior performance of the proposed method compared to the best performing state-of-the-art methods in this category, particularly evident in the presence of the most complex situations where all the algorithms tend to fail.																	1077-3142	1090-235X				NOV	2020	200								103032	10.1016/j.cviu.2020.103032													
J								Open cross-domain visual search	COMPUTER VISION AND IMAGE UNDERSTANDING												This paper addresses cross-domain visual search, where visual queries retrieve category samples from a different domain. For example, we may want to sketch an airplane and retrieve photographs of airplanes. Despite considerable progress, the search occurs in a closed setting between two pre-defined domains. In this paper, we make the step towards an open setting where multiple visual domains are available. This notably translates into a search between any pair of domains, from a combination of domains or within multiple domains. We introduce a simple - yet effective - approach. We formulate the search as a mapping from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Open cross-domain visual search is then performed by searching in the common semantic space, regardless of which domains are used as source or target. Domains are combined in the common space to search from or within multiple domains simultaneously. A separate training of every domain-specific mapping function enables an efficient scaling to any number of domains without affecting the search performance. We empirically illustrate our capability to perform open cross-domain visual search in three different scenarios. Our approach is competitive with respect to existing closed settings, where we obtain state-of-the-art results on several benchmarks for three sketch-based search tasks.																	1077-3142	1090-235X				NOV	2020	200								103045	10.1016/j.cviu.2020.103045													
J								Two-stream deep sparse network for accurate and efficient image restoration	COMPUTER VISION AND IMAGE UNDERSTANDING										Two-stream sparse network; Image restoration; Image super-resolution; Image denoising		Deep convolutional neural network (CNN) has achieved great success in image restoration. However, previous methods ignore the complementarity between low-level and high-level features, thereby leading to limited image reconstruction quality. In this paper, we propose a two-stream sparse network (TSSN) to explicitly learn shallow and deep features to enforce their respective contribution to image restoration. The shallow stream learns shallow features (e.g., texture edges), and the deep stream learns deep features (e.g., salient semantics). In each stream, sparse residual block (SRB) is proposed to efficiently aggregate hierarchical features by constructing sparse connections among layers in the local block. Spatial-wise and channel-wise attention are used to fuse the shallow and deep stream which recalibrates features by weight assignment in both spatial and channel dimensions. A novel loss function called Softmax-L-1 loss is proposed to increase penalties of pixels that have large L-1 loss (i.e., hard pixels). TSSN is evaluated with three representative IR applications, i.e., single image super-resolution, image denoising and JPEG deblocking. Extensive experiments demonstrate that TSSN outperforms most of state-of-the-art methods on benchmark datasets on both quantitative metric and visual quality.																	1077-3142	1090-235X				NOV	2020	200								103029	10.1016/j.cviu.2020.103029													
J								Fine-grained facial landmark detection exploiting intermediate feature representations	COMPUTER VISION AND IMAGE UNDERSTANDING										Facial landmark detection; Face alignment; Feature map reuse	FACE ALIGNMENT; NETWORK	Facial landmark detection has been an active research subject over the last decade. In this paper, we present a new approach for Fine-grained Facial Landmark Detection (FFLD) improving on the precision of the detected points. A high spatial precision of facial landmarks is crucial for many applications related to aesthetic rendering, such as face modeling, face animation, virtual make-up, etc. In this paper, we present an approach that improves the detection precision. Since most facial landmarks are positioned on visible boundary lines, we train a model that encourages the detected landmarks to stay on these boundaries. Our proposed Convolutional Neural Networks (CNN) effectively exploits lower-level feature maps containing abundant boundary information. To this end, beside the main CNN predicting facial landmark positions, we use several additional components, called CropNets. CropNet receives patches cropped from feature maps at different stages of this CNN, and estimate fine corrections of its predicted positions. We also introduce a novel robust spatial loss function based on pixel-wise differences between patches cropped from predicted and ground-truth positions. To further improve the landmark localization, our framework uses several loss functions optimizing the precision at several stages in different ways. Extensive experiments show that our framework significantly increases the local precision of state-of-the-art deep coordinate regression models.																	1077-3142	1090-235X				NOV	2020	200								103036	10.1016/j.cviu.2020.103036													
J								Pointly-supervised scene parsing with uncertainty mixture	COMPUTER VISION AND IMAGE UNDERSTANDING										Scene parsing; Bayesian neural networks; Uncertainty; Gamma mixture; Weakly-supervised learning		Pointly-supervised learning is an important topic for scene parsing, as dense annotation is extremely expensive and hard to scale. The state-of-the-art method harvests pseudo labels by applying thresholds upon softmax outputs (logits). There are two issues with this practice: (1) Softmax output does not necessarily reflect the confidence of the network output. (2) There is no principled way to decide on the optimal threshold. Tuning thresholds can be time-consuming for deep neural networks. Our method, by contrast, builds upon uncertainty measures instead of logits and is free of threshold tuning. We motivate the method with a large-scale analysis of the distribution of uncertainty measures, using strong models and challenging databases. This analysis leads to the discovery of a statistical phenomenon called uncertainty mixture. Specifically speaking, for each independent category, the distribution of uncertainty measures for unlabeled points is a mixture of two components (certain v.s. uncertain samples). The phenomenon of uncertainty mixture is surprisingly ubiquitous in real-world datasets like PascalContext and ADE20k. Inspired by this discovery, we propose to decompose the distribution of uncertainty measures with a Gamma mixture model, leading to a principled method to harvest reliable pseudo labels. Beyond that, we assume the uncertainty measures for labeled points are always drawn from the certain component. This amounts to a regularized Gamma mixture model. We provide a thorough theoretical analysis of this model, showing that it can be solved with an EM-style algorithm with convergence guarantee. Our method is also empirically successful. On PascalContext and ADE20k, we achieve clear margins over the baseline, notably with no threshold tuning in the pseudo label generation procedure. On the absolute scale, since our method collaborates well with strong baselines, we reach new state-of-the-art performance on both datasets.																	1077-3142	1090-235X				NOV	2020	200								103040	10.1016/j.cviu.2020.103040													
J								Computational Intelligence Techniques for Combating COVID-19: A Survey	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										Diseases; COVID-19; Pandemics; Viruses (medical); Data analysis; Computational intelligence; Coronaviruses; Research initiatives		Computational intelligence has been used in many applications in the fields of health sciences and epidemiology. In particular, owing to the sudden and massive spread of COVID-19, many researchers around the globe have devoted intensive efforts into the development of computational intelligence methods and systems for combating the pandemic. Although there have been more than 200,000 scholarly articles on COVID-19, SARS-CoV-2, and other related coronaviruses, these articles did not specifically address in-depth the key issues for applying computational intelligence to combat COVID-19. Hence, it would be exhausting to filter and summarize those studies conducted in the field of computational intelligence from such a large number of articles. Such inconvenience has hindered the development of effective computational intelligence technologies for fighting COVID-19. To fill this gap, this survey focuses on categorizing and reviewing the current progress of computational intelligence for fighting this serious disease. In this survey, we aim to assemble and summarize the latest developments and insights in transforming computational intelligence approaches, such as machine learning, evolutionary computation, soft computing, and big data analytics, into practical applications for fighting COVID-19. We also explore some potential research issues on computational intelligence for defeating the pandemic.																	1556-603X	1556-6048				NOV	2020	15	4					10	22		10.1109/MCI.2020.3019873													
J								A Bayesian Updating Scheme for Pandemics: Estimating the Infection Dynamics of COVID-19	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										COVID-19; Pandemics; Epidemics; Data assimilation; Bayes methods; Statistical analysis; Monitoring; Computational modeling	EPIDEMIC; CHINA	Epidemic models play a key role in understanding and responding to the emerging COVID-19 pandemic. Widely used compartmental models are static and are of limited use to evaluate intervention strategies of combatting the pandemic. Applying the technology of data assimilation, we propose a Bayesian updating approach for estimating epidemiological parameters using observable information to assess the impacts of different intervention strategies. We adopt a concise renewal model and propose new parameters by disentangling the reduction of instantaneous reproduction number Rt into mitigation and suppression factors to quantify intervention impacts at a finer granularity. A data assimilation framework is developed to estimate these parameters including constructing an observation function and developing a Bayesian updating scheme. A statistical analysis framework is built to quantify the impacts of intervention strategies by monitoring the evolution of the estimated parameters. We reveal the intervention impacts in European countries and Wuhan and the resurgence risk in the United States.																	1556-603X	1556-6048				NOV	2020	15	4					23	33		10.1109/MCI.2020.3019874													
J								COVID-19 Time Series Forecast Using Transmission Rate and Meteorological Parameters as Features	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										COVID-19; Time series analysis; Recurrent neural networks; Temperature measurement; Systematics; Globalization; India; China; Pandemics; Predictive models	COINTEGRATION	The number of confirmed cases of COVID-19 has been ever increasing worldwide since its outbreak in Wuhan, China. As such, many researchers have sought to predict the dynamics of the virus spread in different parts of the globe. In this paper, a novel systematic platform for prediction of the future number of confirmed cases of COVID-19 is proposed, based on several factors such as transmission rate, temperature, and humidity. The proposed strategy derives systematically a set of appropriate features for training Recurrent Neural Networks (RNN). To that end, the number of confirmed cases (CC) of COVID-19 in three states of India (Maharashtra, Tamil Nadu and Gujarat) is taken as a case study. It has been noted that stationary and nonstationary parts of the features improved the prediction of the stationary and non-stationary trends of the number of confirmed cases, respectively. The new platform has general application and can be used for pandemic time series forecasting.																	1556-603X	1556-6048				NOV	2020	15	4					34	50		10.1109/MCI.2020.3019895													
J								Meaningful Big Data Integration for a Global COVID-19 Strategy	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										COVID-19; Pandemics; Public healthcare; Big Data; Data analysis; Monitoring; Globalization		With the rapid spread of the COVID-19 pandemic, the novel Meaningful Integration of Data Analytics and Services (MIDAS) platform quickly demonstrates its value, relevance and transferability to this new global crisis. The MIDAS platform enables the connection of a large number of isolated heterogeneous data sources, and combines rich datasets including open and social data, ingesting and preparing these for the application of analytics, monitoring and research tools. These platforms will assist public health author ities in: (i) better understanding the disease and its impact; (ii) monitoring the different aspects of the evolution of the pandemic across a diverse range of groups; (iii) contributing to improved resilience against the impacts of this global crisis; and (iv) enhancing preparedness for future public health emergencies. The model of governance and ethical review, incorporated and defined within MIDAS, also addresses the complex privacy and ethical issues that the developing pandemic has highlighted, allowing oversight and scrutiny of more and richer data sources by users of the system.																	1556-603X	1556-6048				NOV	2020	15	4					51	61		10.1109/MCI.2020.3019898													
J								Road detection based on simultaneous deep learning approaches	ROBOTICS AND AUTONOMOUS SYSTEMS										Visual perception; Data combination; Deep learning; Computer vision; Road map detection; Road lane lines detection; Road segmentation; Driving assistance		One of the most important challenges for Autonomous Driving and Driving Assistance systems is the detection of the road to perform or monitor navigation. Many works can be found in the literature to perform road and lane detection, using both algorithmic processing and learning based techniques. However, no single solution is mentioned to be applicable in any circumstance of mixed scenarios of structured, unstructured, lane based, line based or curb based limits, and other sorts of boundaries. So, one way to embrace this challenge is to have multiple techniques, each specialized on a different approach, and combine them to obtain the best solution from individual contributions. That is the central concern of this paper. By improving a previously developed architecture to combine multiple data sources, a solution is proposed to merge the outputs of two Deep Learning based techniques for road detection. A new representation for the road is proposed along with a workflow of procedures for the combination of two simultaneous Deep Learning models, based on two adaptations of the ENet model. The results show that the overall solution copes with the alternate failures or under -performances of each model, producing a road detection result that is more reliable than the one given by each approach individually. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103605	10.1016/j.robot.2020.103605													
J								Joint semantic segmentation of road objects and lanes using Convolutional Neural Networks	ROBOTICS AND AUTONOMOUS SYSTEMS										Semantic segmentation; Neural networks; Lane segmentation; Object segmentation		This paper presents a multi-task instance segmentation neural network able to provide both road lane and road participants detection. The multi-task approach, ERFNet-based, allows feature sharing and reduces the computational requirements of the overall detection architecture, allowing real time performance even in configurations with limited hardware. The proposed method includes an ad-hoc training procedure and automatic dataset creation mechanism that is also introduced in this paper. The proposed solution has been tested and validated through a newly generated public dataset derived from the BDD100K of 19K images, and in real scenarios. The results obtained prove the viability of the work for road application and its real time performance. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103623	10.1016/j.robot.2020.103623													
J								Associated Reality: A cognitive Human-Machine Layer for autonomous driving	ROBOTICS AND AUTONOMOUS SYSTEMS										ADAS and autonomous vehicle; Cooperative-ITS; Human-Machine system; Augmented Local Dynamic Map; Cognitive system; Cognitive landmark		Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles (AV) are cooperative systems and processes that use: artificial intelligence, cognitive methods, cloud technolo-gies, cooperative vehicle-to-everything-communications (V2X), software-hardware platforms, sensor platforms and incipient intelligent transport infrastructures, to get self-driving systems and smart connected mobility services. This paper, to support automated driving systems (assisted, semi-autonomous and fully autonomous vehicles), introduces a cognitive layer called Associated Reality to enhance the involved information, knowledge and communication processes. The architecture defined includes an augmented Local Dynamic Map, with complementary layers, and an augmented Graph Database, with complementary semantic-cognitive relations, for the considered purpose, in cooperative human- machine and machine-machine systems. Virtual augmented landmarks are defined to improve the connectivity and intelligence of the involved spatial-information systems. Different structure landmarks and sequence landmarks (which includes regular, repetitive and periodic landmarks) are defined, categorized and used in diverse visual localization and mapping scenarios, for autonomous driving. In this paper, it is also shown, as a proof-of-concept for vehicle localization and mapping in road tunnels, the visual detection of different sequences of periodic luminaires, using YOLO v3 for the corresponding LED lights detection, or a specific alternative procedure developed with very low computational cost. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103624	10.1016/j.robot.2020.103624													
J								Improving robot dual-system motor learning with intrinsically motivated meta-control and latent-space experience imagination	ROBOTICS AND AUTONOMOUS SYSTEMS										Meta-control; Arbitration; Experience imagination; Intrinsic motivation; Reinforcement learning; Robotic grasping	NETWORK	Combining model-based and model-free learning systems has been shown to improve the sample efficiency of learning to perform complex robotic tasks. However, dual-system approaches fail to consider the reliability of the learned model when it is applied to make multiple-step predictions, resulting in a compounding of prediction errors and performance degradation. In this paper, we present a novel dual-system motor learning approach where a meta-controller arbitrates online between model-based and model-free decisions based on an estimate of the local reliability of the learned model. The reliability estimate is used in computing an intrinsic feedback signal, encouraging actions that lead to data that improves the model. Our approach also integrates arbitration with imagination where a learned latent-space model generates imagined experiences, based on its local reliability, to be used as additional training data. We evaluate our approach against baseline and state-of-the-art methods on learning vision-based robotic grasping in simulation and real world. The results show that our approach outperforms the compared methods and learns near-optimal grasping policies in denseand sparse-reward environments. (C) 2020 The Authors. Published by Elsevier B.V.																	0921-8890	1872-793X				NOV	2020	133								103630	10.1016/j.robot.2020.103630													
J								Comparative study of self tuning, adaptive and multiplexing FTC strategies for successive failures in an Octorotor UAV	ROBOTICS AND AUTONOMOUS SYSTEMS										Fault-tolerant control; Sliding mode control; Robust control; Multiplexing; Control allocation; Actuator redundancy; Adaptive control; UAV	CONTROL ALLOCATION; TOLERANT; PERFORMANCE	This paper presents three fault-tolerant control (FTC) strategies for a coaxial octorotor unmanned aerial vehicle (UAV) regarding motor failures. The first FTC is based on a control mixing strategy which consists of a set of control laws designed offline, each one dedicated to a specific fault situation. The second FTC, a robust adaptive sliding mode control allocation is presented, where the control gains of the controller are adjusted online in order to redistribute the control signals among the healthy motors in order to stabilize the overall system. The third FTC strategy is a new strategy proposed in this article, which is based on a self-tuning sliding mode control (STSMC) where the control gains are readjusted based on the detected error to maintain the stability of the system. Multiple indoor experiments on an octorotor UAV are conducted to show and compare the effectiveness and the behavior of each FTC scheme after successive faults are injected. More specifically, we inject complete actuator's failures into the top four motors of our octorotor. Every strategies show good fault tolerance results, although the control mixing method performs slightly better overall while the adaptive method performs slightly worse. However, the control mixing method requires a huge design effort to take into account as much situations as possible, while the adaptive method and the STSMC only require to determine a few gains. The adaptive method do not need fault detection to operate, but it thus does not provide information on the system's health without an additional fault identification and diagnosis mechanism, while both the control mixing method and the STSMC provide such information. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103602	10.1016/j.robot.2020.103602													
J								Geometric and constrained control for a string of tethered drones	ROBOTICS AND AUTONOMOUS SYSTEMS											SYSTEMS	In this study, we present a novel concept of a multi tethered drone system. The system includes an arbitrary number of drones connected serially to an active ground station. The considered drones are of quadrotor type. Utilizing a unique pulley-gimbal mechanism, each drone can freely move along the tether, and its state is measured with respect to the ground station without the use of standard onboard inertial sensors or GPS. The proposed system can be thought of as a robotic arm where each tether section acts as a variable-length link and each drone is a joint actuator. We model the coupled behavior of the ground station and the string, taking into account an arbitrary number of drones. Then, a controller that combines tools from geometric-control and Model Predictive Control is suggested. The developed model and control approach are also applicable for other swarm applications where the position of agents is to be controlled to a string-like form. Finally, the concept is demonstrated using numerical simulations and an initial experiment, which illustrate its potential effectiveness. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103609	10.1016/j.robot.2020.103609													
J								Autonomous drone race: A computationally efficient vision-based navigation and control strategy	ROBOTICS AND AUTONOMOUS SYSTEMS										Micro aerial vehicle; Visual navigation; Autonomous drone race	TRAJECTORY GENERATION; QUADROTOR	Drone racing is becoming a popular sport where human pilots have to control their drones to fly at high speed through complex environments and pass a number of gates in a pre-defined sequence. In this paper, we develop an autonomous system for drones to race fully autonomously using only onboard resources. Instead of commonly used visual navigation methods, such as simultaneous localization and mapping and visual inertial odometry, which are computationally expensive for micro aerial vehicles (MAVs), we developed the highly efficient snake gate detection algorithm for visual navigation, which can detect the gate at 20 HZ on a Parrot Bebop drone. Then, with the gate detection result, we developed a robust pose estimation algorithm which has better tolerance to detection noise than a state-of-the-art perspective-n-point method. During the race, sometimes the gates are not in the drone's field of view. For this case, a state prediction-based feed-forward control strategy is developed to steer the drone to fly to the next gate. Experiments show that the drone can fly a half-circle with 1.5 m radius within 2 s with only 30 cm error at the end of the circle without any position feedback. Finally, the whole system is tested in a complex environment (a showroom in the faculty of Aerospace Engineering, TU Delft). The result shows that the drone can complete the track of 15 gates with a speed of 1.5 m/s which is faster than the speeds exhibited at the 2016 and 2017 IROS autonomous drone races. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103621	10.1016/j.robot.2020.103621													
J								Robot exploration of indoor environments using incomplete and inaccurate prior knowledge	ROBOTICS AND AUTONOMOUS SYSTEMS										Robot exploration; Floor plan; Exploration strategy; Prior knowledge	SIMULTANEOUS LOCALIZATION; STRATEGIES; SLAM	Exploration is a task in which autonomous mobile robots incrementally discover features of interest in initially unknown environments. We consider the problem of exploration for map building, in which a robot explores an indoor environment in order to build a metric map. Most of the current exploration strategies used to select the next best locations to visit ignore prior knowledge about the environments to explore that, in some practical cases, could be available. In this paper, we present an exploration strategy that evaluates the amount of new areas that can be perceived from a location according to a priori knowledge about the structure of the indoor environment being explored, like the floor plan or the contour of external walls. Although this knowledge can be incomplete and inaccurate (e.g., a floor plan typically does not represent furniture and objects and consequently may not fully mirror the structure of the real environment), we experimentally show, both in simulation and with real robots, that employing prior knowledge improves the exploration performance in a wide range of settings. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103622	10.1016/j.robot.2020.103622													
J								FUHAR: A transformable wheel-legged hybrid mobile robot	ROBOTICS AND AUTONOMOUS SYSTEMS										Mobile robot; Transformable wheel-legged robot; Motion analysis; Obstacle avoidance; Dynamic model	DESIGN; LOCOMOTION; OPTIMIZATION; PLATFORM	This paper introduces a mobile robot with a new type of transformable wheel legs that can be used for flat and rough terrain. It integrates the stability and maneuverability of a wheeled robot and the legged robot's obstacle climbing capacity using a transformable mechanism with wheel legs. With a transformation structure based on a four-bar mechanism, these two modes can be easily changed. This paper analyzes the movements for the proposed robot in wheeled and legged mode. Dynamic modeling and design of a control system were obtained. Then, the obstacle climbing strategies under legged modes were carried out. Finally, on the basis of the simulation, a prototype of the proposed robot was designed and produced. The results from the experiments validate the efficiency of the designed hybrid mobile robot. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103627	10.1016/j.robot.2020.103627													
J								Cross-entropy based stochastic optimization of robot trajectories using heteroscedastic continuous-time Gaussian processes	ROBOTICS AND AUTONOMOUS SYSTEMS										Robot motion planning; Trajectory optimization; Continuous-time Gaussian processes; Stochastic optimization; Cluttered environments	MOTION; NAVIGATION	High dimensional robot motion planning has recently been approached with trajectory optimization methods that efficiently minimize a suitable objective function in order to generate robot trajec-tories that are both optimal and feasible. However, finding a globally optimal solution is often an insurmountable problem in practice and state-of-the-art trajectory optimization methods are thus prone to local minima, mainly in cluttered environments. In this paper, we propose a novel trajectory planning algorithm that employs stochastic optimization in order to find a collision-free trajectory generated from a continuous-time Gaussian process (GP). The contributions of the proposed motion planning method stem from introducing the heteroscedasticity of the GP, together with exploited sparsity for efficient covariance estimation, and a cross-entropy based stochastic optimization for importance sampling based trajectory optimization. We evaluate the proposed method on three simulated scenarios: a maze benchmark, a 7 DOF robot arm planning benchmark and a 10 DOF mobile manipulator trajectory planning example and compare it to a state-of-the-art GP trajectory optimization method, namely the Gaussian process motion planner 2 algorithm (GPMP2). Our results demonstrate the following: (i) the proposed method yields a more thorough exploration of the solution space in complex environments than GPMP2, while having comparable execution time, (ii) the introduced heteroscedasticity generates GP priors better suited for collision avoidance and (iii) the proposed method has the ability to efficiently tackle high-dimensional trajectory planning problems. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103618	10.1016/j.robot.2020.103618													
J								Three level sequence-based Loop Closure Detection	ROBOTICS AND AUTONOMOUS SYSTEMS										Loop Closure Detection; SLAM; Mobile robotics	LOCALIZATION; ROBUST	The recognition of previously visited places, known as Loop Closure Detection (LCD), composes one of the problems widely studied in robotics: simultaneous localization and mapping (SLAM). In this paper we propose a three level hierarchy based LCD method. In our serialized approach, in the First Level, a sequence of the most recently visited places is used as query to search for candidate sequences in our topological map composed by previously visited places. After that, at the Second Level, the method selects the most similar sequence to the query among all candidate sequences which is temporally consistent with the previous LCD method response. Then, at the Third Level, we match the image sequences belonging to the query sequence to the candidate sequence selected in the Second Level. The method is evaluated in different and challenging public datasets, and presents expressive results that overcome the LCD state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103620	10.1016/j.robot.2020.103620													
J								Surveillance planning with safe emergency landing guarantee for fixed-wing aircraft	ROBOTICS AND AUTONOMOUS SYSTEMS										Unmanned aerial vehicle; Surveillance planning; Emergency landing guarantee	CURVATURE	In this paper, we study Emergency Landing Aware Surveillance Planning (ELASP) to determine a cost-efficient trajectory to visit a given set of target locations such that a safe emergency landing is possible at any point of the multi-goal trajectory. The problem is motivated to guarantee a safe mission plan in a case of loss of thrust for which it is desirable to have a safe gliding trajectory to a nearby airport. The problem combines computational challenges of the combinatorial multi-goal planning with demanding motion planning to determine safe landing trajectories for the curvature-constrained aerial vehicle. The crucial property of safe landing is a minimum safe altitude of the vehicle that can be found by trajectory planning to nearby airports using sampling-based motion planning such as RRT*. A trajectory is considered safe if the vehicle is at least at the minimum safe altitude at any point of the trajectory. Thus, a huge number of samples have to be evaluated to guarantee the safety of the trajectory, and an evaluation of all possible multi-goal trajectories is quickly computationally intractable. Therefore, we propose to utilize a roadmap of safe altitudes combined with the estimation of the trajectory lengths to evaluate only the most promising candidate trajectories. Based on the reported results, the proposed approach significantly reduces the computational burden and enables a solution of ELASP instances with tens of locations in units of minutes using standard single-core computational resources. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				NOV	2020	133								103644	10.1016/j.robot.2020.103644													
J								n-Dimensional (S, N)-implications	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										n-Dimensional intervals; Fuzzy-implications; (S,N)-Implications; n-Dimensional fuzzy sets; Decision-making problems	FUZZY-SETS	The n-dimensional fuzzy logic (n-DFL) has been contributed to overcome the insufficiency of traditional fuzzy logic in modeling imperfect and imprecise information, coming from different opinions of many experts by considering the possibility to model not only ordered but also repeated membership degrees. Thus, n-DFL provides a consolidated logical strategy for applied technologies since the ordered evaluations provided by decision makers impact not only by selecting the best solutions for a decision making problem, but also by enabling their comparisons. In such context, this paper studies the n-dimensional fuzzy implications (n-DI) following distinct approaches: (i) analytical studies, presenting the most desirable properties as neutrality, ordering, (contra-)symmetry, exchange and identity principles, discussing their interrelations and exemplifications; (ii) algebraic aspects mainly related to left- and right-continuity of representable n-dimensional fuzzy t-conorms; and (iii) generating n-DI from existing fuzzy implications. As the most relevant contribution, the prospective studies in the class of n-dimensional interval (S,N)-implications include results obtained from t-representable n-dimensional conorms and involutive n-dimensional fuzzy negations. And, these theoretical results are applied to model approximate reasoning of inference schemes, dealing with based-rule in n-dimensional interval fuzzy systems. A synthetic case-study illustrates the solution for a decision-making problem in medical diagnoses. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						1	26		10.1016/j.ijar.2020.07.002													
J								Fuzzy beta-covering approximation spaces	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy beta-covering approximation space; Fuzzy beta-minimal description; Fuzzy beta-maximal description; I-reduct; Lattice	ROUGH SET MODELS; ATTRIBUTE REDUCTION; OPERATORS	All fuzzy covering-based rough set models are constructed under a corresponding fuzzy covering approximation space (FCAS). The fuzzy beta-covering approximation space (beta-FCAS) is a generalization of the FCAS through replacing the value 1 with a parameter beta. In other words, the beta-FCAS is the basis of studying fuzzy covering-based rough sets and their applications. Therefore, it is necessary to study some questions in the fuzzy covering approximation space, such as problems of reduction, relationships among some basic concepts and relationships between two fuzzy covering approximation spaces. In this article, we investigate the questions in the beta-FCAS further. Firstly, the definitions of I-irreducible element and I-reduct are presented, which can be seen as the complement of the existing notions. Then, relationships among some concepts in the beta-FCAS are investigated, such as the relationship between fuzzy beta-minimal description and beta-reduct, and the relationship between fuzzy beta-covering and its I-reduct. Thirdly, inspired by some concepts in the beta-FCAS, we present some new concepts between two beta-FCASs and their properties. Based on these new concepts, we present some conditions such that two fuzzy beta-coverings have the same reduct (or I-reduct). Finally, based on the results above, seven derived beta-FCASs are investigated further and a corresponding lattice of them is proposed. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						27	47		10.1016/j.ijar.2020.07.009													
J								Couple fuzzy covering rough set models and their generalizations to CCD lattices	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy beta-covering; 1st couple fuzzy beta-covering rough set models; 2nd couple fuzzy beta-covering rough set models; 1st couple L-fuzzy beta-covering rough set models; 2nd couple L-fuzzy beta-covering rough set models	APPROXIMATION OPERATORS; REDUCTION	We make gradual generalizations in this paper, from the concepts of twin approximation operators in covering rough set theory to the concepts of couple fuzzy covering rough set models in fuzzy rough set theory, and further to the concepts of couple L-fuzzy covering rough set models in L-fuzzy rough set theory. Given a fuzzy covering approximation space (U, (C) over tilde) and beta is an element of (0, 1], for each x is an element of U, we divide (C) over tilde into two parts T-x = [(C) over tilde) (i) is an element of (C) over tilde): (C) over tilde) (i) (x) >= beta} and perpendicular to(x) = [(C) over tilde) (i) is an element of (C) over tilde): (C) over tilde) (i) (x) < beta]. To fully describe x from positive aspect and negative aspect, both the two parts T-x and perpendicular to(x) are important, especially the combination of them. So, in this paper, based on the two parts, we define 1st couple beta-fuzzy covering rough set models [(<(P-)over tilde>, (P+) over tilde), ((Q) over tilde-, (Q) over tilde+)] and 2nd couple fuzzy beta-covering rough set models [((P-) over tilde, (P+) over tilde), ((Q-) over tilde, (Q+) over tilde)]. Both the two types of couple fuzzy beta-covering rough set models are generalizations of the twin approximations defined in covering rough set theory. Since each pair of operators in these models are not only closely related but complementary, they can be used to analyze and solve practical problems from positive and negative aspects so as to make a crucial decision. So we then give some examples to show their practical value. The relationships between our models and some other models introduced in previous literature are investigated, and the matrix methods are given to calculate the related approximations and to describe the relationships between every couple models. To generalize the couple fuzzy beta-covering rough set models to the CCD lattice are of a bit complicated, because any two elements in the lattice cannot be compared with each other generally. After some effort we successfully construct the ideal couple L-fuzzy beta-covering rough set models in L-fuzzy rough set theory which are just the generalizations of the two types of couple fuzzy beta-covering rough set models. We also obtain the lattice matrix representations to calculate the related approximations. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						48	69		10.1016/j.ijar.2020.08.003													
J								Deterministic and stochastic damage detection via dynamic response analysis	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Deterministic and stochastic parameter calibration; Wave propagation in elastic solids; Markov Chain Monte Carlo; Bayesian inference; Damage detection	LONGITUDINAL-WAVE PROPAGATION; GEOMETRIC ERGODICITY; IDENTIFICATION; CONVERGENCE; ATTENUATION; HASTINGS; BEAMS	The paper proposes a method of damage detection in elastic materials, which is based on analyzing the time-dependent (dynamic) response of the material excited by an acoustic signal. A case study is presented consisting of experimental measurements and their mathematical analysis. The decisive parameters (wave speed and damping coefficient) of a mathematical model of the acoustic wave are calibrated by comparing the measurement data with the numerically evaluated exact solution predicted by the mathematical model. The calibration is done both deterministically by minimizing the square error over time and stochastically by a Bayesian approach, implemented through the Metropolis-Hastings algorithm. The resulting posterior distribution of the parameters can be used to construct a Bayesian test for damage. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						70	83		10.1016/j.ijar.2020.08.008													
J								Dynamic reliability analysis of nonlinear structures using a Duffing-system-based equivalent nonlinear system method	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Nonlinear system; Dynamic reliability; Equivalent nonlinear system; Random response; First excursion failure mechanism	LINEARIZATION METHOD; RANDOM VIBRATION; LINEAR METHOD; STIFFNESS; CRITERION	To improve the analysis accuracy of dynamic reliability of a nonlinear system, an equivalent nonlinear system method is presented. In this method, general nonlinear systems are converted to equivalent Duffing nonlinear systems according to the minimum mean square error criterion, whose exact analytical solution of the random steady-state responses can be determined by a FokkerPlanckKolmogorov equation. Then the exact results of stochastic responses are used to analyze structural dynamic reliability. To use the equivalent nonlinear system method to analyze structural dynamic reliability is not only convenient for calculation but highly accurate. In addition, the presented equivalent nonlinear system has a parameter s, which controls the degree of nonlinearity. Thus, it is easy to obtain the corresponding analysis results from converting the original system to equivalent nonlinear systems with different degrees of nonlinearity by changing the value of s. Especially, when s is zero, the equivalent nonlinear system will degenerate to the linear system, and leading to the analysis results of the equivalent linearization method. An example shows that the analysis results of the proposed equivalent nonlinear system method are reliable, and the calculation appears to be more accurate than that of the equivalent linearization method. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						84	97		10.1016/j.ijar.2020.08.006													
J								Algebraic aspects and coherence conditions for conjoined and disjoined conditionals	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Coherence; Conditional random quantities; Conjunction and disjunction of conditionals; Decomposition formula; Conditional constituents; Inclusion-exclusion formula	QUASI CONJUNCTION; PROBABILITIES; LOGIC; DISJUNCTION; INCLUSION; OBJECTS; RULES	We deepen the study of conjoined and disjoined conditional events in the setting of coherence. These objects, differently from other approaches, are defined in the framework of conditional random quantities. We show that some well known properties, valid in the case of unconditional events, still hold in our approach to logical operations among conditional events. In particular we prove a decomposition formula and a related additive property. Then, we introduce the set of conditional constituents generated by n conditional events and we show that they satisfy the basic properties valid in the case of unconditional events. We obtain a generalized inclusion-exclusion formula and we prove a suitable distributivity property. Moreover, under logical independence of basic unconditional events, we give two necessary and sufficient coherence conditions. The first condition gives a geometrical characterization for the coherence of prevision assessments on a family F constituted by n conditional events and all possible conjunctions among them. The second condition characterizes the coherence of prevision assessments defined on J U K, where K is the set of conditional constituents associated with the conditional events in F. Then, we give a further theoretical result and we examine some examples and counter-examples. Finally, we make a comparison with other approaches and we illustrate some theoretical aspects and applications. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						98	123		10.1016/j.ijar.2020.08.004													
J								Notes on the lattice of fuzzy rough sets with crisp reference sets	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy rough set; Lower and upper approximation; Fuzzy equivalence; Uncertain knowledge; Regular double Stone lattice; Dually well-ordered set		Since the theory of rough sets was introduced by Zdzislaw Pawlak, several approaches have been proposed to combine rough set theory with fuzzy set theory. In this paper, we examine one of these approaches, namely fuzzy rough sets with crisp reference sets, from a lattice-theoretic point of view. We connect the lower and upper approximations of a fuzzy relation R to the approximations of the core and support of R. We also show that the lattice of fuzzy rough sets corresponding to a fuzzy equivalence relation R and the crisp subsets of its universe is isomorphic to the lattice of rough sets for the (crisp) equivalence relation E, where E is the core of R. We establish a connection between the exact (fuzzy) sets of R and the exact (crisp) sets of the support of R. (C) 2020 The Authors. Published by Elsevier Inc.																	0888-613X	1873-4731				NOV	2020	126						124	132		10.1016/j.ijar.2020.08.007													
J								Thirty years of credal networks: Specification, algorithms and complexity	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Imprecise probabilities; Probabilistic graphical models	HIDDEN MARKOV-MODELS; PROBABILISTIC NETWORKS; INFERENCE; SETS; DISTRIBUTIONS; INDEPENDENCE; EXPECTATIONS; UNCERTAINTY; CLASSIFIERS; FRAMEWORK	Credal networks generalize Bayesian networks to allow for imprecision in probability values. This paper reviews the main results on credal networks under strong independence, as there has been significant progress in the literature during the last decade or so. We focus on computational aspects, summarizing the main algorithms and complexity results for inference and decision making. We address the question "What is really known about strong extensions of credal networks?" by looking at theoretical results and by presenting a short summary of real applications. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						133	157		10.1016/j.ijar.2020.08.009													
J								Efficient algorithms for robustness analysis of maximum a posteriori inference in selective sum-product networks	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Robust statistics; Sensitivity analysis; Sum-product networks; Tractable probabilistic models		Sum-Product Networks (SPN) are deep probabilistic models with demonstrated excellent performance in several machine learning tasks. As with many other probabilistic models, performing Maximum-A-Posteriori inference in SPNs is NP-hard. Selective SPNs are a subclass of SPNs that allow for efficient Maximum-A-Posteriori inference and closed-form parameter learning. Due to the high number of parameters, SPNs learned from data can produce unreliable and overconfident inferences, especially for instances with low statistical support. This issue can be partially mitigated by performing a robustness analysis of inferences with respect to small changes in the parameters. In this work, we address the problem of assessing the robustness of Maximum-A-Posteriori inferences produced with Selective SPNs to global perturbations of the parameters. We consider such an inference robust if it remains the single maximizer under small perturbations of the model parameters. We present efficient algorithms and an empirical analysis with realistic problems involving missing data completion and multilabel classification. The experiments show that our criteria are informative with respect to the inference accuracy, suggesting that it indeed discriminate robust and non-robust instances. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						158	180		10.1016/j.ijar.2020.07.008													
J								Semiring programming: A semantic framework for generalized sum product problems	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Weighted model counting; Declarative languages; Semantic abstractions; Semiring frameworks	CONSTRAINT; INFERENCE; SAT	To solve hard problems, AI relies on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming. Although it is widely accepted that solving real-world problems requires an integration amongst these, contemporary representation methodologies offer little support for this. In an attempt to alleviate this situation, we position and motivate a new declarative programming framework in this paper. We focus on the semantical foundations in service of providing abstractions of well-known problems such as SAT, Bayesian inference, generative models, learning and convex optimization. Programs are understood in terms of first-order logic structures with semiring labels, which allows us to freely combine and integrate problems from different AI disciplines and represent non-standard problems over unbounded domains. Thus, the main thrust of this paper is to view such well-known problems through a unified lens in the hope that appropriate solver strategies (exact, approximate, portfolio or hybrid) may emerge that tackle real-world problems in a principled way. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						181	201		10.1016/j.ijar.2020.08.001													
J								Efficient approaches for maintaining dominance-based multigranulation approximations with incremental granular structures	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Incremental learning; Data mining; Ordered data; Knowledge acquisition; Multigranulation	PROBABILISTIC ROUGH SETS; UPDATING APPROXIMATIONS; ATTRIBUTE REDUCTION; DYNAMIC DATA; FEATURE-SELECTION; 3-WAY DECISIONS; MODEL; MAINTENANCE; KNOWLEDGE; FRAMEWORK	In practical decision making applications, it is computationally time-consuming to maintain multigranulation approximations from scratch in dynamic ordered decision information systems (ODISs) with incremental granular structures consisting of the changing of granular structures by adding granular structures, or by adding an attribute set into each granular structure. The time consumed in the process of maintaining approximations from scratch makes it natural to take into account incremental strategies in order to reduce computational complexity in dynamic multigranulation contexts. To address this challenge, we propose two matrix-based incremental strategies that can dynamically update the lower and upper approximations of each decision class with incremental granular structures in dominance-based multigranulation rough sets (DMGRSs). Moreover, the corresponding incremental algorithms are designed for handling dynamic multi-source ordered data. Ultimately, empirical experiments conducted on UCI data sets depict that the proposed algorithms exhibit a better computational performance compared with the matrix-based static algorithm. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						202	227		10.1016/j.ijar.2020.08.005													
J								The dynamic update method of attribute-induced three-way granular concept in formal contexts	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Attribute-induced; Dynamic formal context; Granular computing; The extension and connotation; Three-way granular concept	INFORMATION FUSION; DECISION; REDUCTION	Granular computing is becoming a very hot research field, which has received extensive attention in recent years. It helps us to analyze and solve problems better by dividing complex problems into several simpler ones. Three-way granular concept is an important concept proposed by combining granular computing, formal concept analysis and three-way decision. Using traditional updating methods of three-way granular concepts, a lot of time and space resources are needed when multiple attributes or objects are deleted in formal context. In order to improve the efficiency and flexibility of obtaining three-way concepts, this paper discusses a novel dynamic update method of three-way granular concepts. In this paper, we firstly introduce the related knowledge of three-way granular concepts. Secondly, the update rules of the extension and connotation of attribute-induced three-way granular concepts are discussed in the dynamic formal context to construct three-way granular concepts. Moreover, we develop a method for establishing attribute-induced three-way granular concept by dynamic changes in the case of deleting multiple objects and attributes in the formal context. Furthermore, we design four algorithms to compare between the proposed approaches and traditional updating ways of three-way granular concepts. Finally, the validity of dynamic update method of attribute-induced three-way granular concept is verified through the experimental evaluation using six datasets coming from the University of California-Irvine (UCI) repository. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						228	248		10.1016/j.ijar.2019.12.014													
J								A probabilistic deontic argumentation framework	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Probabilistic argumentation; Deontic argumentation	ABSTRACT ARGUMENTATION; STRUCTURED ARGUMENTATION; SUPPORT; POSITIVISM; LAW	What does it mean that something is probably obligatory? And how does it relate to the probability that it is permitted or prohibited? In this paper, we provide a possible answer by merging deontic argumentation and probabilistic argumentation into a probabilistic deontic argumentation framework. This framework allows us to specify a semantics for the probability of deontic statuses. The deontic argumentation part builds on standard concepts from the study of computational models of argument: rule-based arguments, argumentation graphs, argument labelling semantics and statement labelling semantics. We then encapsulate this deontic composition with the approach of probabilistic labellings to probabilistic argumentation, in order to associate deontic statements with probability values. The framework is illustrated with a scenario featuring a violation and a contrary-to-duty obligation. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						249	271		10.1016/j.ijar.2020.08.012													
J								A lattice-based representation of independence relations for efficient closure computation	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Independence relations; Representation; Lattice-based partitioning; Fast-closure computation; Algorithm engineering	CONDITIONAL-INDEPENDENCE	Independence relations in general include exponentially many statements of independence, that is, exponential in the number of variables involved. These relations are typically characterised however, by a small set of such statements and an associated set of derivation rules. While various computational problems on independence relations can be solved by manipulating these smaller sets without the need to explicitly generate the full relation, existing algorithms for constructing these sets are associated with often prohibitively high running times. In this paper, we introduce a lattice structure for organising sets of independence statements and show that current algorithms are rendered computationally less demanding by exploiting new insights in the structural properties of independence gained from this lattice organisation. By means of a range of experimental results, we subsequently demonstrate that through the lattice organisation indeed a substantial gain in efficiency is achieved for fast-closure computation of semi-graphoid independence relations in practice. (C) 2020 Published by Elsevier Inc.																	0888-613X	1873-4731				NOV	2020	126						272	289		10.1016/j.ijar.2020.08.002													
J								Extended belief rule-based model for environmental investment prediction with indicator ensemble selection	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Extended belief rule-based model; Indicator ensemble selection; Environmental investment prediction; White-box design; Knowledge enhanced data analytics	EFFICIENCY; ENERGY; SYSTEM	Environmental investment prediction is an effective solution to reduce the wasteful investments of environmental management. Since environmental management involves diverse environmental indicators, investment prediction modeling usually causes the curse of dimensionality and uses irrelevant indicators. A common solution to solve these problems is the use of indicator selection methods to select representative indicators. However, different indicator selection methods have their relative strengths and weaknesses, resulting in different selected indicators and information loss of real representative indicators. Hence, in the present work, a new environmental investment prediction model is proposed on the basis of extended belief rule-based (EBRB) model along with the indicator ensemble selection (IES) and is called IES-EBRB model. The EBRB model is a white-box designed decision-making model and has the specialty on using prior knowledge to enhance data analytics for autonomous decision making; and the IES is an extension of ensemble learning to cooperatively integrate different kinds of indicator selection methods for selecting representative indicators. In a case study, the real world environment data from 2005 to 2018 of 31 provinces in China are applied to verify the effectiveness and accuracy of the IES-EBRB model. Results show that the IES-EBRB model not only can obtain desired environmental investments, but also produces satisfactory accuracy compared to some existing investment prediction models. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						290	307		10.1016/j.ijar.2020.08.013													
J								Partial-overall dominance three-way decision models in interval-valued decision systems	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decisions; Interval-valued decision systems; Dominance degree; Dominance relation	THEORETIC ROUGH SETS; INTUITIONISTIC FUZZY; CLASSIFICATION; APPROXIMATION	Three-way decisions are a generalization of classical decision theory and receive increasing attentions from various fields to handle decision-making problems, especially when involving in incomplete information. An interval is a typical notion of information representation with incompleteness and uncertainty. To measure the dominance degree of one interval dominating or being dominated by another is a hot issue. In this paper, a novel dominance measure is constituted by considering both lengths and locations of intervals. The proposed dominance measure distinguishes two overlapped or coincided intervals, and is able to quantize the separation degree of disjoint intervals. A notion of variable precision overall dominance relation is introduced by integrating both the attribute-wise evaluation information and overall dominance degree of objects. Based on the constituted dominance relation, two three-way decision models are presented in interval-valued decision systems with categorical and interval-valued decision attributes, respectively. Numerical examples of three-way decisions and simulated experiments on classification over two synthetic and four benchmark datasets are presented. Experimental results indicate that the proposed model can produce lower classification errors in comparison with existing methods. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				NOV	2020	126						308	325		10.1016/j.ijar.2020.08.014													
J								Plant species identification based on modified local discriminant projection	NEURAL COMPUTING & APPLICATIONS										Plant species identification; Maximum margin criterion (MMC); Local discriminant projection (LDP); Modified LDP (MLDP)	GENERAL FRAMEWORK; RECOGNITION; FEATURES	Plant species identification based on plant leaves is important for biological science, ecological science, and agricultural digitization. Because of the complexity and variation of the plant leaves, many classical plant species identification algorithms using plant leaf images are not enough for practical application. A modified local discriminant projection (MLDP) algorithm is proposed for plant species identification. MLDP aims to extract discriminant features for plant species identification by taking class label information into account based on the property of locality preserving. The MLDP can preserve the local geometrical structure of leaves and extract the strong discriminative ability. The experimental results on the public ICL leaf image database show the effectiveness and feasibleness of the proposed method.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16329	16336		10.1007/s00521-018-3746-0													
J								VRKSHA: a novel tree structure for time-profiled temporal association mining	NEURAL COMPUTING & APPLICATIONS										Mining association; Time-stamped temporal database; Multi-tree structure; IDS	SIMILARITY MEASURE; DISCOVERY; PATTERNS; TRENDS	Mining association patterns from a time-stamped temporal database distributed over finite time slots is implicitly associated with task of scanning the input database. Finding supports of itemsets requires scanning the input database. Database scan can be performed in either snapshot or lattice-based approach. Sequential and SPAMINE methods for similarity-profiled association pattern mining originally proposed by Jin Soung Yoo and Sashi Sekhar are based on the snapshot database scan and lattice scan, respectively. Snapshot database scan involves scanning multi-time slot database time slot by time slot. The major limitation of Sequential method is the requirement to retain original temporal database in the disk for finding itemset support computations. In this paper, a novel multi-tree structure called VRKSHA is proposed that eliminates the need to store the original temporal database in the memory and also eliminates the need to retain database in memory. The basic idea is to generate a compressed time-stamped temporal tree and use this multi-tree structure to obtain true supports of temporal itemsets for a given time slot. Discovery of similar temporal itemsets is based on finding distance between temporal itemset and reference w.r.t each time slot and validating whether the computed distance satisfies specified user dissimilarity threshold. A pattern is pruned if the dissimilarity condition fails at any given time slot well before computing true support of itemset w.r.t all time slots. The advantage of proposed Sequential approach is from the fact that it is a single database scan approach excluding the initial database scan performed for computing true supports of singleton items. VRKSHA overcomes the major limitation of retaining database in memory that is required by SPAMINE, G-SPAMINE, MASTER algorithms. Experiment results prove that computational time and memory consumed by VRKSHA are significantly very much better than by approaches such as Naive, Sequential, SPAMINE, and G-SPAMINE. To the best of our survey and knowledge, VRKSHA is the pioneering work to introduce and propose a compressed tree-based data structure for mining similarity-profiled temporal association patterns in the area of time-profiled temporal association mining.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16337	16365		10.1007/s00521-018-3776-7													
J								Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks	NEURAL COMPUTING & APPLICATIONS										Adaptive P2P systems; Multiagent information fusion system; Holon formation; Uncertain information handling; Holonic information fusion		There has recently been a major advance with respect to how information fusion is performed. Information fusion has gone from being conceived as a purely hierarchical procedure, as is the case of traditional military applications, to now being regarded collaboratively, as holonic fusion, which is better suited for civil applications and edge organizations. The above paradigm shift is being boosted as information fusion gains ground in different non-military areas, and human-computer and machine-machine communications, where holarchies, which are more flexible structures than ordinary, static hierarchies, become more widespread. This paper focuses on showing how holonic structures tend to be generated when there are constraints on resources (energy, available messages, time, etc.) for interactions based on a set of fully intercommunicating elements (peers) whose components fuse information as a means of optimizing the impact of vagueness and uncertainty present message exchanges. Holon formation is studied generically based on a multiagent system model, and an example of its possible operation is shown. Holonic structures have a series of advantages, such as adaptability, to sudden changes in the environment or its composition, are somewhat autonomous and are capable of cooperating in order to achieve a common goal. This can be useful when the shortage of resources prevents communications or when the system components start to fail.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16367	16385		10.1007/s00521-018-3818-1													
J								Human activity recognition via optical flow: decomposing activities into basic actions	NEURAL COMPUTING & APPLICATIONS										Action recognition; Motion descriptor; Optical flow; Decomposing activities	MOTION; HISTOGRAMS	Recognizing human activities using automated methods has emerged recently as a pivotal research theme for security-related applications. In this research paper, an optical flow descriptor is proposed for the recognition of human actions by considering only features derived from the motion. The signature for the human action is composed as a histogram containing kinematic features which include the local and global traits. Experimental results performed on the Weizmann and UCF101 databases confirmed the potentials of the proposed approach with attained classification rates of 98.76% and 70%, respectively, to distinguish between different human actions. For comparative and performance analysis, different types of classifiers including Knn, decision tree, SVM and deep learning are applied to the proposed descriptors. Further analysis is performed to assess the proposed descriptors under different resolutions and frame rates. The obtained results are in alignment with the early psychological studies reporting that human motion is adequate for the perception of human activities.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16387	16400		10.1007/s00521-018-3951-x													
J								Complex environment image recognition algorithm based on GANs and transfer learning	NEURAL COMPUTING & APPLICATIONS										Image recognition; Transfer learning; Neural network structure; Feature extraction		With the rapid development of the global economy, people's living standards have gradually improved, and the way of transportation has undergone earth-shaking changes. This has created a series of social and environmental issues while greatly facilitating people's lives and work. Excessive numbers of cars have caused urban traffic problems such as traffic pollution, traffic jams and traffic accidents. The strong economic strength has made the automobile industry appear in a booming stage and made it enter the automobile society earlier. The license plate is a vehicle-passed ID card, so the detection and identification of the license plate has become one of the important research directions in today's society. Although the resolution of today's surveillance equipment is getting higher, there are still quite a few monitors with low resolution. Conventional license plate detection and recognition algorithms are challenged in low-resolution video processing. In this paper, we introduce the theory and methodology of GAN and transfer learning, which are applied to deal with the license plate image recognition under several complex environments. The experimental results show that the method adopted in this paper has higher recognition rate and robustness.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16401	16412		10.1007/s00521-019-04018-x													
J								Intelligent employment rate prediction model based on a neural computing framework and human-computer interaction platform	NEURAL COMPUTING & APPLICATIONS										Human-computer interaction; Neural computing framework; Intelligent employment rate prediction; Data mining	ENCRYPTION	An intelligent employment rate prediction model based on a neural computing framework and human-computer interaction platform is demonstrated in this manuscript. Predictive analytics is the future of things, and its significance is manifested in two main aspects: understanding the future so that people can prepare for its arrival, and predicting the current decision so that people can understand the possible consequences, and by the consequences of the analysis to determine the current decision, and strive to make the current decision. However, there are lots of challenges for the prediction tasks. The novelty of this research is mainly concentrated on two major aspects: (1) the neural network model is optimized and enhanced. The proposed nerve tree network model is essentially based on a tree-structured code for a multi-layered feed-forward sparse neural network; with the tree-structured code, the nerve tree network model does not require interconversion between its genotype and phenotype in the coding and decoding operations, and also effectively reduces the computing time. (2) The human-computer interaction is integrated to construct a user-friendly system. In interactive technology, the interactive contact surface and the model, interactive methods and social acceptance have also given rise to many questions that must be solved and problems that require further research and technological innovation. Through numerical verification, the performance of the proposed framework is validated, and the simulation proves the overall performance of the proposed model. Compared with other models, the proposed algorithms can achieve higher prediction accuracy.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16413	16426		10.1007/s00521-019-04019-w													
J								Novel QoS optimization paradigm for IoT systems with fuzzy logic and visual information mining integration	NEURAL COMPUTING & APPLICATIONS										Internet of Things; Information gathering; Visual information; Data mining		The Internet of Things is a new round of information technology revolution after computers, the Internet and mobile communications. Internet of Things technology is an important means to improve the level of social information, which will have a profound impact on economic development and social life. IoT can stimulate the economy, increase employment, improve efficiency and make people's lives and work more convenient. Since fuzzy control can make good use of expert fuzzy information and effectively deal with the complex process of modeling, fuzzy control has received extensive attention once it has been proposed. Fuzzy logic system has become a research hotspot in academic and application fields due to its wide application. Fuzzy system identification includes structure identification and parameter identification. Fuzzy cognitive graph is a kind of soft computing method. It has stronger semantics than neural network because of its intuitive expression ability and powerful reasoning ability. Due to the widespread popularity of visual data acquisition devices, people can use the device to capture a large number of videos and images and spread them over the network in daily learning, production, life, work and entertainment. Computer science and technology, information computing technology, automated detection technology and Internet of Things technology contribute to the research of visual information data. In this paper, we conduct research on the novel QoS optimization paradigm for the IoT systems based on fuzzy logic and visual information mining integration. The experimental results show that the proposed optimization scheme has higher robustness.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16427	16443		10.1007/s00521-019-04020-3													
J								Crowd density estimation in still images using multiple local features and boosting regression ensemble	NEURAL COMPUTING & APPLICATIONS										Crowd counting; Machine learning; Texture features; Regression ensemble	HISTOGRAMS; RETRIEVAL	Crowd density estimation is a challenging research problem in computer vision and has many applications in commercial and defense sectors. Various crowd density estimation methods have been proposed by researchers in the past, but there is an utmost need for accurate, robust and efficient crowd density estimation techniques for its practical implementation. In this paper, we propose a fine-tuned and computationally economical, ensemble regression-based machine learning model for crowd density estimation. The WorldExpo'10 dataset has been used for experimental analysis and model performance evaluation. We extract variety of features intexture-based featuressuch as gray-level co-occurrence matrix, local binary pattern and histogram of oriented gradients,structure-based featuressuch as perimeter pixel and the orientation of pixels, andsegment-based handcrafted featuresfrom each patch of the image and use an optimum combination of these features as input to the regression model. To achieve optimized memory utilization and faster speed, principal component analysis is employed to reduce the dimensions of the lengthy feature vector. Extensive experiments on different fronts ranging from the model hyperparameter optimization, features optimization and features selection were conducted, and at each step, we selected the most favorable results as input to the optimized model. The performance of the model is evaluated based on two popular metrics, i.e., mean absolute error and mean squared error. The comparative analysis shows that the proposed system outperforms the former methods tested on the WorldExpo'10 dataset.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16445	16454		10.1007/s00521-019-04021-2													
J								A QoS optimization system for complex data cross-domain request based on neural blockchain structure	NEURAL COMPUTING & APPLICATIONS										Neural network; Blockchain; Network structure; Cross-domain request for complex data	ALGORITHM	Various computing environments are constantly emerging which are using different technology platforms and security mechanisms from each other. The development of network technology makes cross-domain access between various systems necessary, which requires seamless information sharing and data exchange between systems, thus eliminating the phenomenon of information islands. With its unique consensus mechanism and compatible encryption algorithms, it has gradually attracted attention in various fields. Many people believe that blockchain technology is a revolution in Internet technology in the future, which is a huge innovation in information infrastructure technology as well. In order to ensure a high QoS of the data grid, the system needs to overcome many unstable factors of the network and the grid nodes. Resource (capability) reservation, copy deployment, buffer mechanism, parallel data transfer, and data storage and recovery are the main means to solve such problems. Above all, this paper proposed a QoS optimization system for complex data cross-domain request based on neural blockchain structure. Experimental results show that the proposed method has higher robustness and efficiency.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16455	16469		10.1007/s00521-019-04062-7													
J								CanSuR: a robust method for staining pattern recognition of HEp-2 cell IIF images	NEURAL COMPUTING & APPLICATIONS										HEp-2 cell staining pattern recognition; Local binary pattern; Canonical correlation analysis; Support vector machine	ANTINUCLEAR AUTOANTIBODIES; TEXTURAL FEATURES; CLASSIFICATION	The recognition of staining patterns present in human epithelial type 2 (HEp-2) cells helps to diagnose connective tissue disease. In this context, the paper introduces a robust method, termed as CanSuR, for automatic recognition of antinuclear autoantibodies by HEp-2 cell indirect immunofluorescence (IIF) image analysis. The proposed method combines the advantages of a new sequential supervised canonical correlation analysis (CCA), introduced in this paper, with the theory of rough hypercuboid approach. While the proposed CCA efficiently combines the local textural information of HEp-2 cells, derived from various scales of rotation-invariant local binary patterns, the relevant and significant features of HEp-2 cell for staining pattern recognition are extracted using rough hypercuboid approach. Finally, the support vector machine, with radial basis function kernel, is used to recognize one of the known staining patterns present in IIF images. The effectiveness of the proposed staining pattern recognition method, along with a comparison with related approaches, is demonstrated on MIVIA, SNP and ICPR HEp-2 cell image databases. An important finding is that the proposed method performs significantly better than state-of-the art methods, on three HEp-2 cell image databases with respect to both classification accuracy and F1 score.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16471	16489		10.1007/s00521-019-04108-w													
J								Smart IoT information transmission and security optimization model based on chaotic neural computing	NEURAL COMPUTING & APPLICATIONS										Chaotic neural network; Computational model; Internet of Things; Intelligent system; Optimization algorithm; Information transmission; Network security		The improvement of human quality of life is inseparable from the support of information technology, and the development of information technology has made human life more convenient. The current era is the information age, and the level of informatization has gradually become one of the important indicators to measure the comprehensive level of a country. The emergence of the Internet of Things has led to rapid development of technologies such as data perception, wireless data transmission, and intelligent information processing. With the increasing use of information transmission, people gradually realize the impact of security issues on themselves and society. In this paper, a smart IoT information transmission and security optimization model based on chaotic neural computing model is proposed. Simulation and analysis show that the proposed algorithm can ensure the availability and confidentiality of data at the same time.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16491	16504		10.1007/s00521-019-04162-4													
J								ARDIS: a Swedish historical handwritten digit dataset	NEURAL COMPUTING & APPLICATIONS										Handwritten digit recognition; ARDIS dataset; Machine learning methods; Benchmark	RECOGNITION; CLASSIFICATION; SEGMENTATION; BENCHMARKING; NETWORKS; ONLINE; SVM	This paper introduces a new image-based handwritten historical digit dataset named Arkiv Digital Sweden (ARDIS). The images in ARDIS dataset are extracted from 15,000 Swedish church records which were written by different priests with various handwriting styles in the nineteenth and twentieth centuries. The constructed dataset consists of three single-digit datasets and one-digit string dataset. The digit string dataset includes 10,000 samples in red-green-blue color space, whereas the other datasets contain 7600 single-digit images in different color spaces. An extensive analysis of machine learning methods on several digit datasets is carried out. Additionally, correlation between ARDIS and existing digit datasets Modified National Institute of Standards and Technology (MNIST) and US Postal Service (USPS) is investigated. Experimental results show that machine learning algorithms, including deep learning methods, provide low recognition accuracy as they face difficulties when trained on existing datasets and tested on ARDIS dataset. Accordingly, convolutional neural network trained on MNIST and USPS and tested on ARDIS provide the highest accuracies58.80% respectively. Consequently, the results reveal that machine learning methods trained on existing datasets can have difficulties to recognize digits effectively on our dataset which proves that ARDIS dataset has unique characteristics. This dataset is publicly available for the research community to further advance handwritten digit recognition algorithms.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16505	16518		10.1007/s00521-019-04163-3													
J								Features denoising-based learning for porosity classification	NEURAL COMPUTING & APPLICATIONS										Reservoir characterization; Recurrent neural network; Classification; Petrophysical properties estimation	EMPIRICAL MODE DECOMPOSITION; SEISMIC ATTRIBUTES; NEURAL-NETWORKS; SAND	Reservoir characterization is one of the most challenging tasks that help in modeling different lithological properties like porosity, permeability and fluid saturation using seismic readings like velocity profile, impedance, etc. Such a model is required for field development, placing new wells and prediction management. Seismic attributes are being progressively utilized for the tasks of model building, exploration and properties estimation from the data. However, these tasks become very complex due to the nonlinear and heterogeneous nature of subsurface properties. In this context, present work proposes a recurrent neural network-based learning framework to classify porosity using seismic attributes as predictor variables. The approach begins by calculating different seismic attributes from the data. From the initially calculated attribute set, features that are to be used for classification are selected by using two different strategies. Firstly, the seismic attributes having good correlation strength with reservoir porosity are extracted. Subsequently, generative topographic map is utilized to select the significant features. The final reduced features set obtained by the integrated result of above two strategies is then fed as an input to the empirical mode decomposition (EMD) algorithm. The denoised features resulting from the EMD algorithm are used to train the classification models. Further, a comparison is carried out between the proposed classification framework(EMD+RNN)and other supervised classifiers to show the performance of the proposed framework.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16519	16532		10.1007/s00521-019-04165-1													
J								Granulated deep learning and Z-numbers in motion detection and object recognition	NEURAL COMPUTING & APPLICATIONS										Deep learning; Granular computing; Rough sets; Video tracking; Object recognition; Z-numbers	ROUGH ENTROPY; TRACKING	The article deals with the problems of motion detection, object recognition, and scene description using deep learning in the framework of granular computing and Z-numbers. Since deep learning is computationally intensive, whereas granular computing, on the other hand, leads to computation gain, a judicious integration of their merits is made so as to make the learning mechanism computationally efficient. Further, it is shown how the concept of z-numbers can be used to quantify the abstraction of semantic information in interpreting a scene, where subjectivity is of major concern, through recognition of its constituting objects. The system, thus developed, involves recognition of both static objects in the background and moving objects in foreground separately. Rough set theoretic granular computing is adopted where rough lower and upper approximations are used in defining object and background models. During deep learning, instead of scanning the entire image pixel by pixel in the convolution layer, we scan only the representative pixel of each granule. This results in a significant gain in computation time. Arbitrary-shaped and sized granules, as expected, perform better than regular-shaped rectangular granules or fixed-sized granules. The method of tracking is able to deal efficiently with various challenging cases, e.g., tracking partially overlapped objects and suddenly appeared objects. Overall, the granulated system shows a balanced trade-off between speed and accuracy as compared to pixel level learning in tracking and recognition. The concept of using Z-numbers, in providing a granulated linguistic description of a scene, is unique. This gives a more natural interpretation of object recognition in terms of certainty toward scene understanding.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16533	16548		10.1007/s00521-019-04200-1													
J								Handwriting perceptual classification and synthesis using discriminate HMMs and progressive iterative approximation	NEURAL COMPUTING & APPLICATIONS										Cursive handwriting synthesis; Embedded hidden Markov models; Visual perceptual codes; Control points; Progressive iterative interpolation	HIDDEN MARKOV-MODELS; GENERATION; DRAWINGS	This paper handles the problem of online handwriting synthesis. Indeed, this work presents a probabilistic model using the embedded hidden Markov models (HMMs) for the classification and modeling of perceptual sequences. At first, we start with a vector of perceptual points as input seeking a class of basic shape probability as output. In fact, these perceptual points are necessary for the drawing and the recovering of each basic shape where each one is designed with an HMM built and trained with its components. Each path through these possibilities of control points represents an observation that serves as input for the following step. Secondly, the already detected sequences of observations which represent a segment formed an initial HMM and the concatenation of multiple ones leads to a global HMM. To classify a global HMM, we should codify it by searching the best path of initial HMM. The best path is obtained by computing the maximum of likelihood of the different basic shapes. In order to synthesize the handwritten trace, and to recover the best control points sequences, we investigated the progressive iterative approximation. The performance of the proposed model was assessed using samples of scripts extracted from IRONOFF and MAYASTROON databases. In fact, these samples served for the generation of the set of control points used for the HMMs training models. In experiments, good quantitative agreement and approximation were found for the generated trajectories and a more reduced representation of the scripts models was designed.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16549	16570		10.1007/s00521-019-04206-9													
J								Weakly supervised multi-scale recurrent convolutional neural network for co-saliency detection and co-segmentation	NEURAL COMPUTING & APPLICATIONS										Co-salient object detection; Image co-segmentation; Recurrent convolutional neural network; Visual attention; Weakly supervised training	REGION DETECTION	A new approach involving multi-scale recurrent convolutional neural network (RCNN) has been proposed for co-saliency object detection. The proposed approach involves careful separation of foreground and background superpixel regions from a single image taken from a related group of images in order to train an RCNN to extract the common salient object regions. The one-dimensional convolutional neural network (CNN) is trained using superpixels extracted from several multi-scaled images derived from a single image in every group. The output of the CNN is fed into the recurrent neural network to classify the common object superpixel properties from the remaining images. The superpixel feature training-based RCNN approach addresses two challenges: It requires a small training dataset of about 38 representative images. Further, the use of 1-dimensional superpixel features to train the RCNN results in faster training. The proposed approach delivers accurate identification and segmentation of the common salient object from an image group even under extreme background conditions and object pose variations. The approach has been extensively evaluated using public domain datasets, such as imagepair, iCoseg-sub and iCoseg. The proposed approach delivers higher accuracy,F-measure and lower mean absolute error compared to several state-of-the-art approaches.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16571	16588		10.1007/s00521-019-04265-y													
J								A GPU-based hybrid jDE algorithm applied to the 3D-AB protein structure prediction	SWARM AND EVOLUTIONARY COMPUTATION										GPU Computing; Structural bioinformatics; Evolutionary algorithms; Hybrid algorithms; High-performance computing	BEE COLONY ALGORITHM; DIFFERENTIAL EVOLUTION; OPTIMIZATION; SEARCH	The Protein Structure Prediction (PSP) problem is one of the most significant open problems in bioinformatics. In the AB off-lattice model, the protein sequence is labeled as 'A' or 'B' according to the amino acid classification of being hydrophobic or hydrophilic. It has been widely explored in the literature because polarity is one of the main driving forces behind protein structure definition. This work provides a high-performance hybrid algorithm to approach the 3D-AB off-lattice model through Graphics Processing Units (GPUs). The proposed hybrid algorithm, named cuHjDE-3D, is a self-adaptive Differential Evolution (DE) that uses the jDE mechanism to self-adapt the DE parameters and employs the Hooke-Jeeves Direct Search (HJDS) as the exploitation routine. The experiments were conducted on real protein sequences from the Protein Data Bank (PDB) and compared against state-of-the-art algorithms from the related literature concerning the 3D-AB off-lattice model. Moreover, we provide a methodology to compare a 3D-AB predicted conformation with its native conformation from the PDB repository using the RMSD metric. The obtained results highlight the optimization potential of the proposed method. Also, the GPU running time analysis reports the positive impact of using a massively parallel architecture, with speedups up to 277x , promoting the necessary scalability to handle the 3D-AB model.																	2210-6502	2210-6510				NOV	2020	58								100711	10.1016/j.swevo.2020.100711													
J								Using decomposition-based multi-objective evolutionary algorithm as synthetic example optimization for self-labeling	SWARM AND EVOLUTIONARY COMPUTATION										Self-labeling; Semi-supervised learning; MOEA/D; Mahalanobis distance; Synthetic example optimization	CLASSIFICATION; SOFTWARE; MOEA/D; TOOL	Existing a lot of unlabeled data and few labeled data is one of the most common problems in real datasets. Semi-supervised classification methods can well handle such a problem and have a desirable performance. Among them, one of the most successful methods in dealing with shortage of labeled data is self-labeled technique. One of the difficulties of this technique is wrong data labeling in iterative process of self-labeling. The main reasons are 1) existing outlier and noisy data, 2) inappropriate distribution of labeled data in problem space, and 3) shortage of labeled data in order to make diversity in learning hypotheses. In this paper, a method is developed so as to generate synthetic labeled using decomposition-based multi-objective evolutionary algorithm as synthetic example optimization for self-labeling called DMSS. In DMSS, the synthetic labeled datasets with high diversity and high classification accuracy are generated and then added to the labeled datasets for better training of the algorithm. In already conducted researches, the diversity of the generated data and their distribution in problem space have not been well investigated. The proposed method is a data preparation method which can be employed in all self-labeled techniques. To do so, the proposed method is applied over four self-labeled algorithms having different features and their performances are then evaluated using 25 pattern datasets. The obtained results show high performance of the DMSS with regard to the classification accuracy compared to the existing methods in the literature. Also, the outcomes of conducted non-parametric statistical tests show that the proposed method significantly outperforms the other existing methods in the literature, as well.																	2210-6502	2210-6510				NOV	2020	58								100736	10.1016/j.swevo.2020.100736													
J								Population sizing of cellular evolutionary algorithms	SWARM AND EVOLUTIONARY COMPUTATION										Spatially structured evolutionary algorithms; Cellular evolutionary algorithms; Optimal population size; Event takeover values	NETWORKS	Cellular evolutionary algorithms (cEAs) are a particular type of EAs in which a communication structure is imposed to the population and mating restricted to topographically nearby individuals. In general, these algorithms have longer takeover times than panmictic EAs and previous investigations argue that they are more efficient in escaping local optima of multimodal and deceptive functions. However, most of those studies are not primarily concerned with population size, despite being one of the design decisions with a greater impact in the accuracy and convergence speed of population-based metaheuristics. In this paper, optimal population size for cEAs structured by regular and random graphs with different degree is estimated. Selecto-recombinative cEAs and standard cEAs with mutation and different types of crossover were tested on a class of functions with tunable degrees of difficulty. Results and statistical tests demonstrate the importance of setting an appropriate population size. Event Takeover Values (ETV) were also studied and previous assumptions on their distribution were not confirmed: although ETV distributions of panmictic EAs are heavy-tailed, log-log plots of complementary cumulative distribution functions display no linearity. Furthermore, statistical tests on ETVs generated by several instances of the problems conclude that power law models cannot be favored over log-normal. On the other hand, results confirm that cEAs impose deviations to distribution tails and that large ETVs are less probable when the population is structured by graphs with low connectivity degree. Finally, results suggest that for panmictic EAs the ETVs' upper bounds are approximately equal to the optimal population size.																	2210-6502	2210-6510				NOV	2020	58								100721	10.1016/j.swevo.2020.100721													
J								Solving energy-efficient distributed job shop scheduling via multi-objective evolutionary algorithm with decomposition	SWARM AND EVOLUTIONARY COMPUTATION										Distributed job shop; Energy-efficient multi-objective scheduling; Collaborative search; Adaptive selection; Energy adjustment strategy	GENETIC ALGORITHM; OPTIMIZATION; HEURISTICS; MAKESPAN; SEARCH; MOEA/D	The energy-efficient distributed job shop scheduling problem (EEDJSP) is studied in this paper with the criteria of minimizing both makespan and energy consumption. A mathematical model is presented and an effective modified multi-objective evolutionary algorithm with decomposition (MMOEA/D) is proposed. First, the encoding scheme and decoding scheme are designed based on the characteristics of the EEDJSP. Second, several initialization rules are fused together to produce a diverse population with certain diversity. Third, a collaborative search is proposed to exchange the information between individuals for exploring good solutions. Fourth, three problem-specific local intensification heuristics are designed. Moreover, an adaptive selection strategy is proposed to adjust the utilization of local search operators dynamically. Besides, an energy adjustment strategy is designed for further improvement. We carry out extensive numerical tests with the benchmarking instances. The effectiveness of local intensification as well as energy adjustment strategy is verified via the statistical comparisons. It also shows that the MMOEA/D outperforms other algorithms.																	2210-6502	2210-6510				NOV	2020	58								100745	10.1016/j.swevo.2020.100745													
J								Multi-population meta-heuristics for production scheduling: A survey	SWARM AND EVOLUTIONARY COMPUTATION										Production scheduling; Artificial bee colony; Imperialist competitive algorithm; Shuffled frog-leaping algorithm	ARTIFICIAL BEE COLONY; IMPERIALIST COMPETITIVE ALGORITHM; FROG-LEAPING ALGORITHM; TOTAL FLOWTIME MINIMIZATION; MACHINE ORDER ACCEPTANCE; DEPENDENT SETUP TIMES; JOB-SHOP; HYBRID FLOWSHOP; ASSEMBLY FLOWSHOP; GENETIC ALGORITHM	Production scheduling is one of the most critical issues in manufacturing and production systems and has markedly positive impact on the performances of manufacturing. In the past decades, various scheduling problems have been extensively solved by multi-population meta-heuristics, which are artificial bee colony (ABC), imperialist competitive algorithm (ICA) and shuffled frog-leaping algorithm (SFLA); however, the related works of multi-population meta-heuristics to scheduling are hardly reviewed. In this paper, we provide an extensive recall on solving production scheduling based on ABC, ICA or SFLA. A new classification on scheduling problems is first given, then three algorithms are described and their applications to scheduling are summarized in a systematic way; finally, the main conclusions are drawn and some future research directions are presented.																	2210-6502	2210-6510				NOV	2020	58								100739	10.1016/j.swevo.2020.100739													
J								Multi-objective self-organizing optimization for constrained sparse array synthesis	SWARM AND EVOLUTIONARY COMPUTATION										MOEAs; Genetic programming; Sparse plane array	PARTICLE SWARM OPTIMIZATION; KRILL HERD ALGORITHM; ANTENNA DESIGN; EVOLUTIONARY ALGORITHMS; STRATEGY; MOEA/D	Sparse span array is a critical communication technology for detecting microwave signal, yet it is difficult to simultaneously satisfy both reducing antenna elements' number and maintaining maximum side lobe level. Towards this problem, we propose a multi-objective optimization approach for self-organizing limited-area sparse span array, termed MOSSA. Overall, a uniform framework of multi-objective sparse span array is proposed. Specially, two objectives, number of selected antenna and peak side lobe level, are established for exploring the optimal array distribution in the framework. Based on the framework, for the problem of global-optimum array distribution, we propose a multi-objective particle swarm optimization searching pattern and design a MOSSA algorithm; Furthermore, for the problem of flexibly-adjusted self-organizing array structure, we present a multiobjective genetic programming searching pattern and design a MOSSA-gp algorithm. Moreover, a limited-region mode supplements to the framework. Finally, combination decision strategy assists users to screen out suitable solutions under the guidance of fuzzy-range indexes and then select the optimal solution by a triangle-approximating approach based on minimum Manhattan distance. Numerous experiments demonstrate that the proposed MOSSA outperforms other state-of-the-art algorithms in terms of both antenna elements' number and maximum side lobe level.																	2210-6502	2210-6510				NOV	2020	58								100743	10.1016/j.swevo.2020.100743													
J								Population size in Particle Swarm Optimization	SWARM AND EVOLUTIONARY COMPUTATION										Particle swarm optimization; Swarm size; Population size; Swarm intelligence; Real-world problems; Metaheuristics	CONVERGENCE; ALGORITHM; SELECTION	Particle Swarm Optimization (PSO) is among the most universally applied population-based metaheuristic optimization algorithms. PSO has been successfully used in various scientific fields, ranging from humanities, engineering, chemistry, medicine, to advanced physics. Since its introduction in 1995, the method has been widely investigated, which led to the development of hundreds of PSO versions and numerous theoretical and empirical findings on their convergence and parameterization. However, so far there is no detailed study on the proper choice of PSO swarm size, although it is widely known that population size crucially affects the performance of metaheuristics. In most applications, authors follow the initial suggestion from 1995 and restrict the population size to 20-50 particles. In this study, we relate the performance of eight PSO variants to swarm sizes that range from 3 up to 1000 particles. Tests are performed on sixty 10- to 100-dimensional scalable benchmarks and twentytwo 1- to 216-dimensional real-world problems. Although results do differ for the specific PSO variants, for the majority of considered PSO algorithms the best performance is obtained with swarms composed of 70-500 particles, indicating that the classical choice is often too small. Larger swarms frequently improve efficiency of the method for more difficult problems and practical applications. For unimodal problems slightly lower swarm sizes are recommended for the majority of PSO variants, but some would still perform best with hundreds of particles.																	2210-6502	2210-6510				NOV	2020	58								100718	10.1016/j.swevo.2020.100718													
J								Semantic approximation for reducing code bloat in Genetic Programming	SWARM AND EVOLUTIONARY COMPUTATION										Genetic Programming; Semantic approximation; Code bloat	OPERATOR EQUALIZATION; TOURNAMENT SELECTION; CROSSOVER; TUTORIAL	Code bloat is a phenomenon in Genetic Programming (GP) characterized by the increase in individual size during the evolutionary process without a corresponding improvement in fitness. Bloat negatively affects GP performance, since large individuals are more time consuming to evaluate and harder to interpret In this paper, we propose two approaches for reducing GP code bloat based on a semantic approximation technique. The first approach replaces a random subtree in an individual by a smaller tree of approximate semantics. The second approach replaces a random subtree by a smaller tree that is semantically approximate to the desired semantics. We evaluated the proposed methods on a large number of regression problems. The experimental results showed that our methods help to significantly reduce code bloat and improve the performance of GP compared to standard GP and some recent bloat control methods in GP. Furthermore, the performance of the proposed approaches is competitive with the best machine learning technique among the four tested machine learning algorithms.																	2210-6502	2210-6510				NOV	2020	58								100729	10.1016/j.swevo.2020.100729													
J								An Adaptive Memetic Approach for Heterogeneous Vehicle Routing Problems with two-dimensional loading constraints	SWARM AND EVOLUTIONARY COMPUTATION										Memetic algorithm; Adaptive algorithm; Vehicle routing; Multi-methods	BEE COLONY ALGORITHM; EVOLUTIONARY ALGORITHM; OPTIMIZATION	The heterogeneous fleet vehicle routing problem with two-dimensional loading constraints (2L- HFVRP) is a complex variant of the classical vehicle routing problem. 2L-HFVRP seeks for minimal cost set of routes to serve a set of customers using a fleet of vehicles of different capacities, fixed and variable operating costs, different dimensions, and restricted loading constraints. To effectively deal with the 2L-HFVRP, we propose a two-stage method that successively calls the routing stage and the packing stage. For the routing stage, we propose an adaptive memetic approach that integrates new multi-parent crossover operators with multi-local search algorithms in an adaptive manner. A time-varying fitness function is proposed to avoid prematurity and improve search performance. An adaptive quality-and-diversity selection mechanism is devised to control the application of the memetic operators and the local search algorithms. In the packing stage, five heuristics are adopted and hybridised to perform the packing process. Experiments on a set of 36 2L-HFVRP benchmark instances demonstrate that the proposed method provides highly competitive results in comparison with state-of-the-art algorithms. In particular, the proposed method obtains the best results for several instances.																	2210-6502	2210-6510				NOV	2020	58								100730	10.1016/j.swevo.2020.100730													
J								Deterministic scaffold assembly by self-reconfiguring micro-robotic swarms	SWARM AND EVOLUTIONARY COMPUTATION										Modular robotic swarm; Self-reconfiguration; Large-scale swarm coordination; Distributed algorithm; Scaffolding	SYSTEMS; MOTION; MODULE	The self-reconfiguration of large swarms of modular robotic units from one object into another is an intricate problem whose critical parameter that must be optimized is the time required to perform a transformation. Various optimizations methods have been proposed to accelerate transformations, as well as techniques to engineer the shape itself, such as scaffolding which creates an internal object structure filled with holes for easing the motion of modules. In this paper, we propose a novel deterministic and distributed method for rapidly constructing the scaffold of an object from an organized reserve of modules placed underneath the reconfiguration scene. This innovative scaffold design is parameterizable and has a face-centered-cubic lattice structure made from our rotating-only micro-modules. Our method operates at two levels of planning, scheduling the construction of components of the scaffold to avoid deadlocks at one level, and handling the navigation of modules and their coordination to avoid collisions in the other. We provide an analysis of the method and perform simulations on shapes with an increasing level of intricacy to show that our method has a reconfiguration time complexity of O((3)root N) time steps for a subclass of convex shapes, with N the number of modules in the shape. We then proceed to explain how our solution can be further extended to any shape.																	2210-6502	2210-6510				NOV	2020	58								100722	10.1016/j.swevo.2020.100722													
J								S-commerce: Influence of Facebook likes on purchases and recommendations on a linked e-commerce site	DECISION SUPPORT SYSTEMS										Social commerce; Facebook likes; Social networking site (SNS); Social proof; Attitude transfer; Controlled experiments	WORD-OF-MOUTH; SOCIAL COMMERCE; MODERATING ROLE; CONSUMER-BEHAVIOR; BOX-OFFICE; IMPACT; REVIEWS; PRODUCT; POPULARITY; INTENTIONS	Social networking site (SNS) driven e-commerce, the latest social commerce (s-commerce) phenomenon, gains prominence with the introduction of the call-to-action feature. The call-to-action feature on any sponsored post or advertisement on SNS redirects the user to a linked e-commerce website that offers the product. Information cues available on the SNS are expected to influence user decision making on the linked e-commerce site. Set in the context of Facebook driven e-commerce, this study explores how likes on Facebook influence user's purchase and recommendation decisions on a linked e-commerce website. Using controlled experiments we find that a higher volume of likes on Facebook leads to a higher likelihood of purchasing and recommending a product on the linked e-commerce site. This effect is found to be mediated by the user's initial product attitude formed on Facebook. An additional analysis examining the strength of the influence reveals that the mere presence of likes is not sufficient to impact the user's decision making. In fact, a low volume of likes elicits user behavior similar to absence of likes. The influence is effective only if the number of likes is substantially high. The findings of the study add to the s-commerce literature by establishing the inter-site influence of Facebook likes on user's purchase and post-purchase decisions and providing empirical evidence of the efficacy of SNS-driven e-commerce.																	0167-9236	1873-5797				NOV	2020	138								113383	10.1016/j.dss.2020.113383													
J								The effects of bidder factors on online bidding strategies: A motivation-opportunity-ability (MOA) model	DECISION SUPPORT SYSTEMS										Online auctions; Bidding strategy; Risk preference; E-commerce; Motivation-opportunity-ability (MOA)	TIME PRESSURE; E-COMMERCE; SELLER STRATEGIES; AUCTIONS; INFORMATION; USER; IMPACT; EXPERIENCE; CONSUMER; INTERNET	The use and popularity of online auctions is growing all over the world. Bidding strategies are important because they are related to an auction's final price and ultimately its revenue. This study investigates the bidding strategies adopted by online bidders and the factors of the bidders, including bidding motivations, time availability, bidding experience, and risk aversion. We use the data from China to test the model and identify three bidding strategies in single-unit auctions: agent bidding, snipe bidding, and ratchet bidding. By running logistic regression, we find that hedonic motivations, utilitarian motivations, time availability, bidding experience, and risk preference all influence online bidding strategies. We also conduct pairwise comparisons of bidding strategies based on these factors and a simulation experiment to compare the benefits brought by different bidding strategies. We conclude by discussing the implications for both research and practice.																	0167-9236	1873-5797				NOV	2020	138								113397	10.1016/j.dss.2020.113397													
J								Fast and frugal heuristics for portfolio decisions with positive project interactions	DECISION SUPPORT SYSTEMS										Decision making; Decision analysis; Portfolio selection; Heuristics; Behavioral decision making	RESOURCE-ALLOCATION BEHAVIOR; SELECTION; MODELS; OPTIMIZATION; INFORMATION	We consider portfolio decision problems with positive interactions between projects. Exact solutions to this problem require that all interactions are assessed, requiring time, expertise and effort that may not always be available. We develop and test a number of fast and frugal heuristics - psychologically plausible models that limit the number of assessments to be made and combine these in computationally simple ways - for portfolio decisions. The proposed "add-the-best" family of heuristics constructs a portfolio by iteratively adding a project that is best in a greedy sense, with various definitions of "best". We present analytical results showing that information savings achievable by heuristics can be considerable; a simulation experiment showing that portfolios selected by heuristics can be close to optimal under certain conditions; and a behavioral laboratory experiment demonstrating that choices are often consistent with the use of heuristics. Add-the-best heuristics combine descriptive plausibility with effort-accuracy trade-offs that make them potentially attractive for prescriptive use.																	0167-9236	1873-5797				NOV	2020	138								113399	10.1016/j.dss.2020.113399													
J								Improving healthcare access management by predicting patient no-show behaviour	DECISION SUPPORT SYSTEMS										Analytics; No-show prediction; Healthcare access; Design science research	DESIGN SCIENCE RESEARCH; NEURAL-NETWORK; MISSED APPOINTMENTS; SUPPORT-SYSTEM; NON-ATTENDANCE; OUTPATIENT; REGRESSION; IMPACT; NONATTENDANCE; METHODOLOGY	Low attendance levels in medical appointments have been associated with poor health outcomes and efficiency problems for service providers. To address this problem, healthcare managers could aim at improving attendance levels or minimizing the operational impact of no-shows by adapting resource allocation policies. However, given the uncertainty of patient behaviour, generating relevant information regarding no-show probabilities could support the decision-making process for both approaches. In this context many researchers have used multiple regression models to identify patient and appointment characteristics than can be used as good predictors for no-show probabilities. This work develops a Decision Support System (DSS) to support the implementation of strategies to encourage attendance, for a preventive care program targeted at underserved communities in Bogota, Colombia. Our contribution to literature is threefold. Firstly, we assess the effectiveness of different machine learning approaches to improve the accuracy of regression models. In particular, Random Forest and Neural Networks are used to model the problem accounting for non-linearity and variable interactions. Secondly, we propose a novel use of Layer-wise Relevance Propagation in order to improve the explainability of neural network predictions and obtain insights from the modelling step. Thirdly, we identify variables explaining no-show probabilities in a developing context and study its policy implications and potential for improving healthcare access. In addition to quantifying relationships reported in previous studies, we find that income and neighbourhood crime statistics affect no-show probabilities. Our results will support patient prioritization in a pilot behavioural intervention and will inform appointment planning decisions.																	0167-9236	1873-5797				NOV	2020	138								113398	10.1016/j.dss.2020.113398													
J								The crowd against the few: Measuring the impact of expert recommendations	DECISION SUPPORT SYSTEMS										recommender systems; filter bubble; experts; user behavior; diversity	SERENDIPITY; DIVERSITY	A large amount of research on recommender systems has focused on improving the accuracy of suggestions in offline settings. However, this focus and the commonly used techniques can lead to a "filter bubble", severely limiting the diversity of content discovered by users. Several offline studies show that this can be mitigated by using experts for recommendation. In contrast to standard recommender systems, experts are able to generate more diverse recommendations and increase the novelty of given suggestions. They can be used in missing-data or cold-start scenarios and reduce noise in the users' ratings. This paper examines the impact of employed experts' recommendations on user behavior for a real-world recommender system on a popular video-on-demand website, provided by a large television network. We study whether the potential benefits of experts lead to differences in user behavior, user perceptions and properties of given recommendations (e.g., diversity). We find that enriching a state-of-the-art system with the suggestions of employed experts can significantly increase platform use. Even though expert recommendations are used less frequently and are less successful than expected, users watch a greater number of clips, use more recommendations, and come back to the website more frequently when they receive expert suggestions. When searching for other influencing factors, we find that experts generate more diverse recommendations and improve the taste coverage of the system keeping user satisfaction unaffected. In summary, our results show large benefits of using employed experts and have implications for the design and use of recommender systems in real-world scenarios.																	0167-9236	1873-5797				NOV	2020	138								113345	10.1016/j.dss.2020.113345													
J								A note on big data analytics capability development in supply chain	DECISION SUPPORT SYSTEMS										Big data; Analytics; Capability development; Qualitative study; Supply chain	FIRM PERFORMANCE; INFORMATION-TECHNOLOGY; PREDICTIVE ANALYTICS; DATA QUALITY; MANAGEMENT; OPPORTUNITIES; CHALLENGES; COMPETITION; LOGISTICS; NETWORKS	Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and infra-firm power dynamics play an important role in building BDA capability and using BDA technologies.																	0167-9236	1873-5797				NOV	2020	138								113382	10.1016/j.dss.2020.113382													
J								A decision support framework for home health care transportation with simultaneous multi-vehicle routing and staff scheduling synchronization	DECISION SUPPORT SYSTEMS										Shared healthcare mobility; Home health care; Decision support system; Hybrid genetic algorithm; Synchronization; Vehicle routing	STOCHASTIC TRAVEL; TIME WINDOWS; SERVICE; ASSIGNMENT; ALGORITHM; OPTIMIZATION; DELIVERY; SYSTEM; MODEL	Due to the ageing population and the prevalence of chronic diseases, Home Health Care (HHC) practices are significantly increasing in developed countries to provide coordinated health related services to patients at their homes. Accordingly, the scope of HHC services is also expanding from typical nursing and postoperative care at home to cover all types of needs of elderly patients (e.g., personal care, drug delivery and meal services). This paper aims to address the pressing demand for HHC services and develop a novel and effective mathematical model and solution methodology for supporting health care service delivery decisions. Our decision support framework captures the real needs of HHC services, including the challenges of creating simultaneous schedules and route plans for a set of HHC staff and Home Delivery Vehicles (HDVs) under the requirements of synchronization between HHC staff and HDVs visits, multiple visits to patients, multiple routes of HDVs and pickup/delivery visits related precedence for HDVs. A Mixed Integer Linear Programming (MILP) model is developed to characterize the optimization problem. Considering the computational complexity of the problem, a Hybrid Genetic Algorithm (HGA) is proposed to suggest HHC planning decisions. The model formulation and proposed HGA are examined on real-life instances for demonstrating its practicality and randomly generated test instances for assessing the scalability of the proposed approach. The results show the effectiveness and efficiency of our solution methodology. Experimental results indicate that the proposed algorithm provided a good performance even with an increasing number of required synchronized services, whereas the heuristic tactics facilitate the HGA to produce better-quality solutions in a significantly shorter time. Our framework is expected to contribute to an important aspect of shared healthcare mobility.																	0167-9236	1873-5797				NOV	2020	138								113361	10.1016/j.dss.2020.113361													
J								Automated dynamic approach for detecting ransomware using finite-state machine	DECISION SUPPORT SYSTEMS										Cybersecurity; Intrusion/anomaly detection; Malware mitigation; Ransomware		Ransomware is a type of malware that affects the victim data by modifying, deleting, or blocking their access. In recent years, ransomware attacks have resulted in critical data and financial losses to individuals and industries. These disruptions force the need for developing effective anti-ransomware methods in the research community. However, most of the existing techniques are designed to detect a specific ransomware variant instead of providing a generic solution mainly because of the obfuscation techniques used by ransomware or the use of static analysis methods. In this context, this paper proposes a novel ransomware-detection technique that identifies ransomware attacks by evaluating the current state of a computer system with knowledge of a ransomware attack. The finite-state machine model is used to synthesise the knowledge of the ransomware attack with respect to the victim machine. The proposed method monitors the changes happening in the computer system in terms of utilisation, persistence, and lateral movement of its resources to detect ransomware attacks. The experimental results demonstrate that the proposed method can accurately detect attacks from different ransomware variants with significantly few false predictions.																	0167-9236	1873-5797				NOV	2020	138								113400	10.1016/j.dss.2020.113400													
J								The reliability analysis of rating systems in decision making: When scale meets multi-attribute additive value model	DECISION SUPPORT SYSTEMS										Rating systems; Multi-attribute additive value model; Asymmetric information; Reliability; Consistency	RESPONSE CATEGORIES; FEATURE-SELECTION; CLASSIFICATION; WELFARE; MANIPULATION; AGGREGATION; RANKINGS; NUMBER	A rating system (RS) comprises a rating metric defined by a discrete set of integers contained in an interval (e.g., [0, N]), and an aggregation rule. RSs are widely used in various fields to capture and summarize individuals' opinions on alternatives. In this paper we argue that the multi-attribute additive value model (MAVM) should be used as a benchmark to analyze the reliability of RSs, and present some tight bounds on the parameter N and overall rating scores, which guarantee the consistency between the RS [0, N] and MAVM at the ranking and rating levels. Interestingly, the tight bounds at the rating level are twice as large as those at the ranking level. The results in this paper can provide new insights about the reliability analysis of RSs.																	0167-9236	1873-5797				NOV	2020	138								113384	10.1016/j.dss.2020.113384													
J								Antisocial online behavior detection using deep learning	DECISION SUPPORT SYSTEMS										Antisocial online behavior; Natural language processing; Text classification; Deep learning; Cyberbullying; Attention mechanism	NEURAL-NETWORKS; TEXT; ANALYTICS	Digitalization shifts human communication to online platforms, which has many benefits but also builds up a space for antisocial online behavior (AOB) such as harassment, insult and other forms of hateful textual content. Online platforms have good reasons to monitor and moderate such content. The paper examines the viability of automatic content monitoring using deep machine learning and natural language processing (NLP). More specifically, we consolidate prior work in the field of antisocial online behavior detection and compare relevant approaches to recent NLP models in an empirical study. Covering important methodological advancements in NLP including bidirectional encoding, attention, hierarchical text representations, and pre-trained transformer-based language models, and extending previous approaches by introducing a pseudo-sentence hierarchical attention network, the paper provides a comprehensive summary of the state-of-affairs in NLP-based AOB detection, clarifies the detection accuracy that is attainable with today's technology, discusses whether this degree is sufficient for deploying deep learning-based text screening systems, and approaches the interpretability topic.																	0167-9236	1873-5797				NOV	2020	138								113362	10.1016/j.dss.2020.113362													
J								A Crowdsourcing Framework for Collecting Tabular Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Crowdsourcing; Computers; Cleaning; Data models; Inference algorithms; Estimation; Crowdsourcing; tabular data; truth inference; task assignment	TRUTH DISCOVERY	In crowdsourcing, human workers are employed to tackle problems that are traditionally difficult for computers (e.g., data cleaning, missing value filling, and sentiment analysis). In this paper, we study the effective use of crowdsourcing in filling missing values in a given relation (e.g., a table containing different attributes of celebrity stars, such as nationality and age). A task given to a worker typically consists of questions about the missing attribute values (e.g., What is the age of Jet Li?). Although this problem has been studied before, existing work often treats related attributes independently, leading to suboptimal performance. In this paper, we present T-Crowd, which is a crowdsourcing system that considers attribute relationships. Particularly, T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. The attribute relationship information is used to guide task allocation to workers. Our solution seamlessly supports categorical and continuous attributes. Our extensive experiments on real and synthetic datasets show that T-Crowd outperforms state-of-the-art methods, improving the quality of truth inference and reducing the monetary cost of crowdsourcing.																	1041-4347	1558-2191				NOV 1	2020	32	11					2060	2074		10.1109/TKDE.2019.2914903													
J								A Framework for Supervised Classification Performance Analysis with Information-Theoretic Methods	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Entropy; Mutual information; Proposals; Tools; Performance analysis; Performance evaluation; classification algorithms; information entropy; mutual information; formal concept analysis	ENTROPY-BASED EVALUATION; CONFUSIONS	We introduce a framework for the evaluation of multiclass classifiers by exploring their confusion matrices. Instead of using error-counting measures of performance, we concentrate in quantifying the information transfer from true to estimated labels using information-theoretic measures. First, the Entropy Triangle allows us to visualize the balance of mutual information, variation of information, and the deviation from uniformity in the true and estimated label distributions. Next, the Entropy-Modified Accuracy allows us to rank classifiers by performance while the Normalized Information Transfer rate allows us to evaluate classifiers by the amount of information accrued during learning. Finally, if the question rises to elucidate which errors are systematically committed by the classifier, we use a generalization of Formal Concept Analysis to elicit such knowledge. All such techniques can be applied either to artificially or biologically embodied classifiers-e.g., human performance on perceptual tasks. We instantiate the framework in a number of examples to provide guidelines for the use of these tools in the case of assessing single classifiers or populations of them-whether induced with the same technique or not-either on single tasks or in a set of them. These include well-known UCI tasks and the more complex KDD cup 99 competition on Intrusion Detection.																	1041-4347	1558-2191				NOV 1	2020	32	11					2075	2087		10.1109/TKDE.2019.2915643													
J								A General Centrality Framework-Based on Node Navigability	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Measurement; Navigation; Eigenvalues and eigenfunctions; Task analysis; Indexes; Social networking (online); Computer science; Centrality; node ranking in graphs; graph navigability	SMALL-WORLD; WEB; NETWORKS; DYNAMICS; SEARCH; COMMUNICABILITY; NAVIGATION	Centrality metrics are a popular tool in Network Science to identify important nodes within a graph. We introduce the Potential Gain as a centrality measure that unifies many walk-based centrality metrics in graphs and captures the notion of node navigability, interpreted as the property of being reachable from anywhere else (in the graph) through short walks. Two instances of the Potential Gain (called the Geometric and the Exponential Potential Gain) are presented and we describe scalable algorithms for computing them on large graphs. We also give a proof of the relationship between the new measures and established centralities. The geometric potential gain of a node can thus be characterized as the product of its Degree centrality by its Katz centrality scores. At the same time, the exponential potential gain of a node is proved to be the product of Degree centrality by its Communicability index. These formal results connect potential gain to both the "popularity" and "similarity" properties that are captured by the above centralities.																	1041-4347	1558-2191				NOV 1	2020	32	11					2088	2100		10.1109/TKDE.2019.2947035													
J								A Novel Trust Model Based Overlapping Community Detection Algorithm for Social Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Social networking (online); Detection algorithms; Computational modeling; Network topology; Peer-to-peer computing; Topology; Clustering algorithms; Community detection; social network; trust; coarse cluster	COMPLEX NETWORKS; DISCOVERY	With the fast advances in Internet technologies, social networks have become a major platform for social interaction, lifestyle demonstration, and message dissemination. Effective community detection in social networks helps to assess public sentiment, identify community leaders, and produce personalized recommendation. While different community detection approaches have been proposed in the literature, the trust model based detection schemes model user interactions as trust transfer, which helps to capture the implicit relation in the network. Unfortunately, trust model based detection schemes face a cold start problem, i.e., they cannot accurately model newly joined users as these users have few interactions for a duration after joining the network. In this paper, we propose TLCDA, a novel trust model based community detection algorithm. By enhancing the traditional trust computation with inter-node relation strength and similarity in social networks, TLCDA detects communities through coarse-grained K-Mediods clustering. Our evaluation on real social networks shows that the communities detected by TLCDA exhibit superior preference cohesion while satisfying the topology cohesion.																	1041-4347	1558-2191				NOV 1	2020	32	11					2101	2114		10.1109/TKDE.2019.2914201													
J								A Variational Bayesian Framework for Cluster Analysis in a Complex Network	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Complex networks; Clustering algorithms; Bayes methods; Task analysis; Analytical models; Social networking (online); Stochastic processes; Complex network; cluster analysis; node attributes; Bayesian model; variational inference	COMMUNITY DETECTION; MODEL	A complex network is a network with non-trivial topological structures. It contains not just topological information but also attribute information available in the rich content of nodes. Concerning the task of cluster analysis in a complex network, model-based algorithms are preferred over distance-based ones, as they avoid designing specific distance measures. However, their models are only applicable to complex networks where the attribute information is composed of attributes in binary form. To overcome this disadvantage, we introduce a three-layer node-attribute-value hierarchical structure to describe the attribute information in a flexible and interpretable manner. Then, a new Bayesian model is proposed to simulate the generative process of a complex network. In this model, the attribute information is generated by following the hierarchical structure while the links between pairwise nodes are generated by a stochastic blockmodel. To solve the corresponding inference problem, we develop a variational Bayesian algorithm called TARA, which allows us to identify functionally meaningful clusters through an iterative procedure. Our extensive experiment results show that TARA can be an effective algorithm for cluster analysis in a complex network. Moreover, the parallelized version of TARA makes it possible to perform efficiently at its tasks when applied to large complex networks.																	1041-4347	1558-2191				NOV 1	2020	32	11					2115	2128		10.1109/TKDE.2019.2914200													
J								Deep Learning Driven Venue Recommender for Event-Based Social Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Deep learning; History; Heterogeneous networks; Social networking (online); Metadata; Computational modeling; Numerical models; Deep learning; venue recommender system; transfer learning; EBSN	RECURRENT NEURAL-NETWORK; MODEL	Event-based online social platforms, such as Meetup and Plancast, have experienced increased popularity and rapid growth in recent years. In EBSN setup, selecting suitable venues for hosting events, which can attract a great turnout, is a key challenge. In this paper, we present a deep learning based venue recommendation system DeepVenue which provides context driven venue recommendations for the Meetup event-hosts to host their events. The crux of the proposed model relies on the notion of similarity between multiple Meetup entities such as events, venues, groups, etc. We develop deep learning techniques to compute a compact descriptor for each entity, such that two entities (say, venues) can be compared numerically. Notably, to mitigate the scarcity of venue related information in Meetup, we leverage on the cross domain knowledge transfer from popular LBSN service Yelp to extract rich venue related content. For hosting an event, the proposed DeepVenue model computes a success score for each candidate venue and ranks those venues according to the scores and finally recommend the top k venues. Our rigorous evaluation on the Meetup data collected for the city of Chicago shows that DeepVenue significantly outperforms the baselines algorithms. Precisely, for 84 percent of events, the correct hosting venue appears in the top 5 of the DeepVenue recommended list.																	1041-4347	1558-2191				NOV 1	2020	32	11					2129	2143		10.1109/TKDE.2019.2915523													
J								Detecting Communities with Multiplex Semantics by Distinguishing Background, General, and Specialized Topics	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Semantics; Bayes methods; Multiplexing; Inference algorithms; Probabilistic logic; Complex networks; Community detection; Bayesian probabilistic model; multiplex semantics; variational inference	NETWORKS	Finding semantic communities using network topology and contents together is a hot topic in community detection. Existing methods often use word attributes in an indiscriminate way to help finding communities. Through analysis we find that, words in networked contents often embody a hierarchical semantic structure. Some words reflect a background topic of the whole network with all communities, some imply the high-level general topic covering several topic-related communities, and some imply the high-resolution specialized topic to describe each community. Ignoring such semantic structures often leads to defects in depicting networked contents where deep semantics are not fully utilized. To solve this problem, we propose a new Bayesian probabilistic model. By distinguishing words from either a background topic or some two-level topics (i.e., general and specialized topics), this model not only better utilizes the networked contents to help finding communities, but also provides a clearer multiplex semantic community interpretation. We then give an efficient variational algorithm for model inference. The superiority of this new approach is demonstrated by comparing with ten state-of-the-art methods on nine real networks and an artificial benchmark. A case study is further provided to show its strong ability in deep semantic interpretation of communities.																	1041-4347	1558-2191				NOV 1	2020	32	11					2144	2158		10.1109/TKDE.2019.2937298													
J								Entropy-based Sampling Approaches for Multi-Class Imbalanced Problems	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Entropy; Earth Observing System; Information entropy; Uncertainty; Measurement uncertainty; Data mining; Imbalanced learning; oversampling; undersampling; hybrid sampling; entropy	F-MEASURE; SMOTE; FRAMEWORK; MODEL	In data mining, large differences between multi-class distributions regarded as class imbalance issues have been known to hinder the classification performance. Unfortunately, existing sampling methods have shown their deficiencies such as causing the problems of over-generation and over-lapping by oversampling techniques, or the excessive loss of significant information by undersampling techniques. This paper presents three proposed sampling approaches for imbalanced learning: the first one is the entropy-based oversampling (EOS) approach; the second one is the entropy-based undersampling (EUS) approach; the third one is the entropy-based hybrid sampling (EHS) approach combined by both oversampling and undersampling approaches. These three approaches are based on a new class imbalance metric, termed entropy-based imbalance degree (EID), considering the differences of information contents between classes instead of traditional imbalance-ratio. Specifically, to balance a data set after evaluating the information influence degree of each instance, EOS generates new instances around difficult-to-learn instances and only remains the informative ones. EUS removes easy-to-learn instances. While EHS can do both simultaneously. Finally, we use all the generated and remaining instances to train several classifiers. Extensive experiments over synthetic and real-world data sets demonstrate the effectiveness of our approaches.																	1041-4347	1558-2191				NOV 1	2020	32	11					2159	2170		10.1109/TKDE.2019.2913859													
J								Fast Discrete Collaborative Multi-Modal Hashing for Large-Scale Multimedia Retrieval	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Semantics; Optimization; Collaboration; Correlation; Training; Binary codes; Complexity theory; Multi-modal hashing; pair-wise semantics; computation and memory efficiency; fast discrete optimization		Many achievements have been made on learning to hash for uni-modal and cross-modal retrieval. However, it is still an unsolved problem that how to directly and efficiently learn discriminative discrete hash codes for the multimedia retrieval, where both query and database samples are represented with heterogeneous multi-modal features. With this motivation, we propose a Fast Discrete Collaborative Multi-modal Hashing (FDCMH) method in this paper. We first propose an efficient collaborative multi-modal mapping that first transforms heterogeneous multi-modal features into the unified factors to exploit the complementarity of multi-modal features and preserve the semantic correlations in multiple modalities with linear computation and space complexity. Such shared factors also bridge the heterogeneous modality gap and remove the inter-modality redundancy. Further, we develop an asymmetric hashing learning module to simultaneously correlate the learned hash codes with low-level data distribution and high-level semantics. In particular, this design could avoid the challenging symmetric semantic matrix factorization and O(n(2)) memory cost (n is the number of training samples). It can support both computation and memory efficient discrete hash optimization. Experiments on several public multimedia retrieval datasets demonstrate the superiority of the proposed approach compared with state-of-the-art hashing techniques, in terms of both model learning efficiency and retrieval accuracy.																	1041-4347	1558-2191				NOV 1	2020	32	11					2171	2184		10.1109/TKDE.2019.2913388													
J								Ghost Imputation: Accurately Reconstructing Missing Data of the Off Period	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Sensors; Indexes; Uncertainty; Data mining; Computational complexity; Computer science; Batteries; Imputation; multivariate; pattern mining	MAXIMUM-LIKELIHOOD; PROGRAM; MOBILE	Noise and missing data are intrinsic characteristics of real-world data, leading to uncertainty that negatively affects the quality of knowledge extracted from the data. The burden imposed by missing data is often severe in sensors that collect data from the physical world, where large gaps of missing data may occur when the system is temporarily off or disconnected. How can we reconstruct missing data for these periods? We introduce an accurate and efficient algorithm for missing data reconstruction (imputation), that is specifically designed to recover off-period segments of missing data. This algorithm, Ghost, searches the sequential dataset to find data segments that have a prior and posterior segment that matches those of the missing data. If there is a similar segment that also satisfies the constraint - such as location or time of day - then it is substituted for the missing data. A baseline approach results in quadratic computational complexity, therefore we introduce a caching approach that reduces the search space and improves the computational complexity to linear in the common case. Experimental evaluations on five real-world datasets show that our algorithm significantly outperforms four state-of-the-art algorithms with an average of 18 percent higher F-score.																	1041-4347	1558-2191				NOV 1	2020	32	11					2185	2197		10.1109/TKDE.2019.2914653													
J								On Nearby-Fit Spatial Keyword Queries	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Approximation algorithms; Spatial databases; Games; Sports; Indexes; History; Shape; Spatial keyword queries; proximity queries; spatial database; location-based services	SEARCH	Geo-textual data is ubiquitous nowadays, where each object has a location and is associated with some keywords. Many types of queries based on geo-textual data, termed as spatial keyword queries, have been proposed, and are to find optimal object(s) in terms of both its (their) location(s) and keywords. In this paper, we propose a new type of query called nearby-fit spatial keyword query (NSKQ), where an optimal object is defined based not only on the location and the keywords of the object itself, but also on those of the objects nearby. For example, in an application of finding a hotel, not only the location of a hotel but also the objects near the hotel (e.g., shopping malls, restaurants, and bus stops nearby) might need to be taken into consideration. The query is proved to be NP-hard, and in order to perform the query efficiently, we developed two approximate algorithms with small constant approximation factors equal to 1.155 and 1.79. We conducted extensive experiments based on both real and synthetic datasets, which verified our algorithms.																	1041-4347	1558-2191				NOV 1	2020	32	11					2198	2212		10.1109/TKDE.2019.2915295													
J								Real-Time Ambulance Redeployment: A Data-Driven Approach	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Real-time systems; Hospitals; Safety; Indexes; Optimal matching; Urban areas; Urban computing; emergency medical services; ambulance redeployment; data and knowledge management	LOCATION; ALLOCATION; RELOCATION; MODEL	Emergency Medical Services (EMS) are of great importance to saving people's lives from emergent accidents and diseases by efficiently picking up patients using ambulances. The transporting capability of an EMS system (e.g., defined as the average pickup time of patients) significantly depends on the real-time redeployment strategy of ambulances. That is, which station should an ambulance be redeployed to, after it becomes available (after it transports a patient to a hospital or after it finishes the in-site treatment for a patient)? However, it is a challenging task concerning with the multiple data D1-D5 as detailed in Introduction. To this end, in this paper, we propose a data-driven real-time ambulance redeployment approach that redeploys an ambulance to a proper station after it becomes available, so as to optimize the transporting capability of an EMS system, considering the aforementioned multiple data D1-D5. Specifically, the proposed approach is comprised of two stages to well consider the D1-D5. First, we propose a method (a safety time-based urgency index) to incorporate D1, D2, and D3 into each ambulance station's urgency degree (D*). Second, we propose an optimal matching algorithm to combine D*, D4, and D5 into the redeployment of the current available ambulance. Experimental results using data collected in real world demonstrate the significant advantages of our approach over many baselines. Comparing with baselines, our approach can save similar to 4 minutes (similar to 35 percent) of the average pickup time for each patient, improve the ratio of patients picked up within 10 minutes from 0.684 and 0.803 (similar to 17 percent), and largely enhance the survival rate of patients (similar to 12 percent for patients in category A1 and similar to 17 percent for patients in A2).																	1041-4347	1558-2191				NOV 1	2020	32	11					2213	2226		10.1109/TKDE.2019.2914206													
J								Reference-Based Framework for Spatio-Temporal Trajectory Compression and Query Processing	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Trajectory; Query processing; Indexes; Measurement; Roads; Computer science; Compression algorithms; Reference trajectory; spatio-temporal trajectory; compression	TREE	The pervasiveness of GPS-enabled devices and wireless communication technologies results in massive trajectory data, incurring expensive cost for storage, transmission, and query processing. To relieve this problem, in this paper we propose a novel framework for compressing trajectory data, REST (Reference-based Spatio-temporal trajectory compression), by which a raw trajectory is represented by concatenation of a series of historical (sub-)trajectories (called reference trajectories) that form the compressed trajectory within a given spatio-temporal deviation threshold. In order to construct a reference trajectory set that can most benefit the subsequent compression, we propose three kinds of techniques to select reference trajectories wisely from a large dataset such that the resulting reference set is more compact yet covering most footprints of trajectories in the area of interest. To address the computational issue caused by the large number of combinations of reference trajectories that may exist for resembling a given trajectory, we propose efficient greedy algorithms that run in the blink of an eye and dynamic programming algorithms that can achieve the optimal compression ratio. Compared to existing work on trajectory compression, our framework has few assumptions about data such as moving within a road network or moving with constant direction and speed, and better compression performance with fairly small spatio-temporal loss. In addition, by indexing the reference trajectories directly with an in-memory R-tree and building connections to the raw trajectories with inverted index, we develop an extremely efficient algorithm that can answer spatio-temporal range queries over trajectories in their compressed form. Extensive experiments on a real taxi trajectory dataset demonstrate the superiority of our framework over existing representative approaches in terms of both compression ratio and efficiency.																	1041-4347	1558-2191				NOV 1	2020	32	11					2227	2240		10.1109/TKDE.2019.2914449													
