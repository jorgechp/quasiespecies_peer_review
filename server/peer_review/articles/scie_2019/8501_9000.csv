PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Generalized hesitant fuzzy rough sets (GHFRS) and their application in risk analysis	SOFT COMPUTING										Hesitant fuzzy sets; Rough sets; Generalized rough sets; Generalized hesitant fuzzy rough sets	GROUP DECISION-MAKING; 3-WAY DECISIONS; APPROXIMATIONS; SIMILARITY	As a generalization of fuzzy rough sets, the concept of generalized hesitant fuzzy rough sets (GHFRS) is presented in this paper. It is an endeavor to define rough approximations of a collection of hesitant fuzzy sets over a given universe. To this end, elements of the universe are initially clustered using a set-valued map, and then, hesitant fuzzy sets are aggregated by using lower and upper approximation operators. These operators produce hesitant fuzzy sets which aggregate hesitant fuzzy elements. Structural and topological properties associated with GHFRS have been examined. The model is further employed to design a three-way decision analysis technique which preserves many properties of classical techniques but needs less effort and computation. Unlike the existing approaches, the alternatives can be clustered and selected jointly by using a set-valued mapping. This feature makes its application area broader. Moreover, this method is applied to an example, where risk analysis issue is discussed for the selection of energy projects.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14005	14017		10.1007/s00500-020-04776-0		FEB 2020											
J								A computational model of task allocation in social insects: ecology and interactions alone can drive specialisation	SWARM INTELLIGENCE										Task allocation; Game theory; Simulation	DIVISION-OF-LABOR; APIS-MELLIFERA; INFORMATION-TRANSFER; TEMPORAL POLYETHISM; EVOLUTIONARY GAMES; GENETIC DIVERSITY; AGE POLYETHISM; WORKER SIZE; COLONY; ORGANIZATION	Social insects allocate their workforce in a decentralised fashion, addressing multiple tasks and responding effectively to environmental changes. This process is fundamental to their ecological success, but the mechanisms behind it are not well understood. While most models focus on internal and individual factors, empirical evidence highlights the importance of ecology and social interactions. To address this gap, we propose a game theoretical model of task allocation. Our main findings are twofold: Firstly, the specialisation emerging from self-organised task allocation can be largely determined by the ecology. Weakly specialised colonies in which all individuals perform more than one task emerge when foraging is cheap; in contrast, harsher environments with high foraging costs lead to strong specialisation in which each individual fully engages in a single task. Secondly, social interactions lead to important differences in dynamic environments. Colonies whose individuals rely on their own experience are predicted to be more flexible when dealing with change than colonies relying on social information. We also find that, counter to intuition, strongly specialised colonies may perform suboptimally, whereas the group performance of weakly specialised colonies approaches optimality. Our simulation results fully agree with the predictions of the mathematical model for the regions where the latter is analytically tractable. Our results are useful in framing relevant and important empirical questions, where ecology and interactions are key elements of hypotheses and predictions.																	1935-3812	1935-3820				JUN	2020	14	2					143	170		10.1007/s11721-020-00180-4		FEB 2020											
J								Neural network prediction of bioleaching of metals from waste computer printed circuit boards using Levenberg-Marquardt algorithm	COMPUTATIONAL INTELLIGENCE										electronic waste; printed circuit boards; bioleaching; artificial neural networks; genetic algorithm	THIOBACILLUS-FERROOXIDANS; HEAVY-METALS; RECOVERY; COPPER; OPTIMIZATION; PROGRESS	The applicability of artificial neural network (ANN) to predict the bioleaching of metals using from computer printed circuit boards (CPCB) and the influence of process parameters were studied. The influence of process parameters initial pH (1.6-2.4), pulp density (2%-13%), and the initial volume of Inoculum (5%-25%) were investigated on the rate of bioleaching of metals from CPCB. Network inputs were fed as initial pH, pulp density, and inoculum volume and with the extraction of Cu, Ag, and Au as output. The ANN was developed using the Levenberg-Marquardt algorithm and trained for modeling and prediction. The most fitting architectures for Cu, Ag, and Au were [4-5-5-2-1], [4-7-5-2-1], [4-7-1-1-1] trained with Levenberg-Marquardt algorithm, respectively. The R values were observed to be 0.996, 0.997, and 0.993 for Cu, Ag, and Au extraction predictions, respectively. The genetic algorithm model defined by ANN was used to achieve maximum extraction rates for Cu, Au, and Ag. The predicted data showed that there is a great capability of using ANN for the prediction of Cu, Ag, and Au extraction from CPCB through bioleaching process. Hence, the ANN model can be used to control the operational conditions for improved metals extraction through bioleaching.																	0824-7935	1467-8640															10.1111/coin.12288		FEB 2020											
J								Energy-efficient routing technique for reliable data transmission under the background of big data for disaster region	COMPUTATIONAL INTELLIGENCE										big data; big data-based clustering technique; environment data; hybrid metaheuristic optimization; Internet of things (IoT)	DATA ANALYTICS; ENVIRONMENT	Big data analysis and cloud computing are gaining much interest in various applications including disaster management. One of the major difficulties in the process of exchanging environmental data in the disaster affected areas has been considered as one of the emerging areas of research. This research focuses on maintaining the environmental data information management of the disaster affected areas, where the intermediate node has been used to transmit the information during transmission and an optimized routing has been used to create efficient data transmission, such as temperature, pressure, humidity, and the level of pollution within the network. The intermediate node may also be hacked during data processing. In this article, the efficient big data-based clustering technique has been proposed. In this research, the information is grouped into a cluster in every comparable node and the energy consumption has been efficiently managed with the hybrid metaheuristic optimization-based effective routing technique. The system excellence has been evaluated using the energy utilization factor, packet delivery ratio, and attack-free routing effectiveness metrics to handle environmental information on disaster affected areas.																	0824-7935	1467-8640															10.1111/coin.12294		FEB 2020											
J								L-1 norm based pedestrian detection using video analytics technique	COMPUTATIONAL INTELLIGENCE										HOG; human detection; pedestrian detection; SVM; twin background model		Pedestrian detection from images of the visible spectrum is a high relevant area of research given its potential impact in the design of pedestrian protection systems. In general, detection is made with two different phases, feature extraction and classification. Also, features for detection of pedestrian are already are available such as optimal feature model. But still required is an improvement in detection by reducing the execution time and false positive. The proposed model has three different phases, that is, background subtraction, feature extraction, and classification. In spite of giving entire information into feature extraction, the system gives only a useful information (foreground image) by twin background model. Then the foreground image moves to the feature extraction and classifies the pedestrian. For feature extraction, histogram of orientation gradient (HOG) L-1 normalization has been used. This will increase the detection accuracy and reduce the computation time of a process. In addition, false positive rate has been minimized.																	0824-7935	1467-8640															10.1111/coin.12292		FEB 2020											
J								Small universal asynchronous spiking neural P systems with multiple channels	NEUROCOMPUTING										Spiking neural P systems; Asynchronous system; Multiple channels; Turing completeness	RULES	Researchers have proposed spiking neural P systems with multiple channels (SNP-MC systems), as a variant of spiking neural P systems (SN P systems), with channel labels distinguishing different synapses. This work focuses on small universal SNP-MC systems working on asynchronously mode, where the use of enabled rules is not obligatory. We construct an asynchronous SNP-MC system using only 38 neurons and preserving its universality for computing functions. It is also proved, as small universal number generators, an asynchronous SNP-MC system needs 41 neurons. In comparing with the existing literature, asynchronous SNP-MC system needs fewer neurons than any other small asynchronous SN P system and even some synchronous SN P systems. The results show that our use of multiple channels well compensates for the computing lost when removing synchronization from SNP-MC systems. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						1	8		10.1016/j.neucom.2019.06.104													
J								Single image dehazing based on fusion strategy	NEUROCOMPUTING										Deep learning; Image dehazing; Fusion strategy; Derived maps	WEATHER; RESTORATION; VISIBILITY	In this paper, we propose a deep convolutional network for single image dehazing based on derived image fusion strategy. Instead of estimating the transmission map and atmospheric light as previously performed, we directly generate a haze-free image by the proposed end-to-end trainable neural network. We derive five maps from the original hazy image based on the characteristics of the hazy scene to improve the dehazing performance. First, the exposure map (EM) and saliency map (SM) complement each other to focus on details in far-away and near-region scenes. Second, the white balance map (BM) and gamma correction map (GM) are employed to recover the latent colour and intensity components of the scene. Finally, the haze veil map (VM) is introduced to enhance the global image contrast. To efficiently blend the five derived maps, we propose a U-shaped deep convolutional network consisting of encoder and decoder layers to generate a haze-free image. The convolutional layers transferred from the pretrained ResNet50 are used as encoder layers for hierarchical feature extraction. Two efficient blocks, named the cascaded residual block and the channel compression block, are proposed in the network for better dehazing performance. The final dehazed result is generated by combining the significant features of the different derived maps. Additionally, perceptual loss is introduced for better visual quality. The experimental results for both synthetic and natural hazy images demonstrate that our algorithm performs comparably or even better than state-of-the-art methods in terms of the peak signal-to-noise ratio (PSNR), structure similarity (SSIM) and visual quality. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						9	23		10.1016/j.neucom.2019.09.094													
J								Data-Independent Feature Learning with Markov Random Fields in Convolutional Neural Networks	NEUROCOMPUTING										Convolutional neural networks; Image representation; Markov random fields; Gibbs distribution; Self-organising maps; Image classification; Image features	SPATIAL-INTERACTION; CONVERGENCE; MODELS; SPACE	In image classification, deriving robust image representations is a key process that determines the performance of vision systems. Numerous image features and descriptors have been developed manually over the years. As an alternative, however, deep neural networks, in particular convolutional neural networks (CNNs), have become popular for learning image features or representations from data and have demonstrated remarkable performance in many real-world applications. But CNNs often require huge amount of labelled data, which may be prohibitive in many applications, as well as long training times. This paper considers an alternative, data-independent means of obtaining features for CNNs. The proposed framework makes use of the Markov random field (MRF) and self-organising map (SOM) to generate basic features and model both intra- and inter-image dependencies. Various MRF textures are synthesized first, and are then clustered by a convolutional translation-invariant SOM, to form generic image features. These features can be directly applied as early convolutional filters of the CNN, leading to a new way of deriving effective features for image classification. The MRF framework also offers a theoretical and transparent way to examine and determine the influence of image features on performance of CNNs. Comprehensive experiments on the MNIST, rotated MNIST, CIFAR-10 and CIFAR-100 datasets were conducted with results outperforming most state-of-the-art models of similar complexity. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						24	35		10.1016/j.neucom.2019.03.107													
J								Efficient representations of EEG signals for SSVEP frequency recognition based on deep multiset CCA	NEUROCOMPUTING										Brain-computer interface; Electroencephalogram; Multiset canonical correlation analysis; Steady-state visual evoked potential	CANONICAL CORRELATION-ANALYSIS; BCI	Canonical correlation analysis (CCA) has been widely used for frequency recognition in steady-state visual evoked potential (SSVEP) based brain-computer interfaces (BCIs). However, linear CCA-based methods may be insufficient given the complexity of EEG signals. A nonlinear feature extraction method based on deep multiset CCA (DMCCA) is proposed for SSVEP recognition to fully utilize the real EEG and constructed sine-cosine signals. In DMCCA, neural networks are trained to learn the nonlinear representations of multiple sets of EEG signals at the same frequency by maximizing the overall correlation within the representations and reference signals. Therefore, reference signals are augmented with the extracted features for frequency recognition. Finally, the proposed method is evaluated using SSVEP signals collected from 10 subjects. DMCCA-based method outperforms others in terms of classification accuracy compared with CCA- and multiset CCA-based methods. The proposed DMCCA-based method has substantial potential for improving the recognition performance of SSVEP signals. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						36	44		10.1016/j.neucom.2019.10.049													
J								Eye localization based on weight binarization cascade convolution neural network	NEUROCOMPUTING										Eye localization; Convolution neural network; Binary; Image processing; Coarse-to-fine	FACE ALIGNMENT; RECOGNITION; HYBRID	Eye localization is a key step in the field of face recognition and analysis, which is the premise and breakthrough of drowsiness estimation and auxiliary driving. An eye localization method based on Weight Binarization Cascade Convolution Neural Network (WBCCNN) is proposed, in which the WBCCNN is composed of four levels and the weight is constrained by binarization. It predicts eye positions from coarse-to-fine to improve the performance of eye localization, and binary network not only helps to reduce the storage size of the model, but also speeds up the operation. Experiments on eye localization are performed using Labeled Faces in the Wild (LFW), BioID, and Labeled Face Parts in the Wild (LFPW) Databases, from which the results show that the average detection errors of left eye and right eye by our method are 0.66% and 0.71% on LFW, respectively. The operation speed of binary network is approximately as twice as that of non-binary. In addition, our method requires less storage capacity, which maintains higher performance on BioID and LFPW, compared to some state-of-the-art works. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						45	53		10.1016/j.neucom.2019.10.048													
J								Bayesian neural multi-source transfer learning	NEUROCOMPUTING										Bayesian methods; Transfer learning; Neural networks; Data fusion; Bayesian neural networks	NETWORKS; LANGEVIN; RECOGNITION	Although the use of deep learning and neural networks techniques are gaining popularity, there remain a number of challenges when multiple sources of information and data need to be combined. Although transfer learning and data fusion methodologies try to address this challenge, they lack robust uncertainty quantification which is crucial for decision making. Bayesian inference provides a rigorous approach for uncertainty quantification in decision making. Uncertainty quantification using Bayesian inference takes into consideration uncertainty associated with model parameters, as well as, the uncertainty in combining multiple sources of data. In this paper, we present a Bayesian framework for transfer learning using neural networks that considers single and multiple sources of data. We use existence of prior distributions to define the dependency between different data sources in a multi-source Bayesian transfer learning framework. We use Markov Chain Monte-Carlo method to obtain samples from the posterior distribution that consider the knowledge from the source datasets as priors. The results show that the framework provides a robust probabilistic approach for decision making with accuracy that is similar to gradient-based learning methods. Moreover, the results are comparable to related machine learning methods used for transfer learning in the literature. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						54	64		10.1016/j.neucom.2019.10.042													
J								Streaking artifacts suppression for cone-beam computed tomography with the residual learning in neural network	NEUROCOMPUTING										Streaking artifact; Convolution neural network; Residual learning; Computed tomography	VIEW IMAGE-RECONSTRUCTION; RAY CT RECONSTRUCTION; LOW-DOSE CT; DEEP; PICCS	This study aims to address and test a new residual learning algorithm in neural network applied to the projection data to generate high qualified imaging by reducing the streaking artifacts in cone-beam computed tomography (CBCT). Since the streaking artifacts have a large relationship with the noise on the projection, a residual objective upon Poisson noise corresponding to the image was proposed. As the prior, the convolution neural network (CNN) was constructed to residual learning based on the simulated label and exploited to eliminate the artifacts in the slice. To illustrate the robustness and applicability of CNN, the proposed method is evaluated using CBCT images. For the simulated projection, the PSNR and SSIM of the proposed method were dramatically increased by 15.4% and 85.9% of that with raw projection; for the true projection, the PSNR and SSIM were increased by 14.9% and 56.2%, respectively. Study results show effective results, and the proposed method is practical and attractive as a preferred solution to CT streaking artifacts suppression. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						65	78		10.1016/j.neucom.2019.09.087													
J								Unsupervised feature selection via adaptive hypergraph regularized latent representation learning	NEUROCOMPUTING										Unsupervised feature selection; Hypergraph learning; Latent representation learning; Local structure preservation	STRUCTURE PRESERVATION; GRAPH	Due to the rapid development of multimedia technology, a large number of unlabelled data with high dimensionality need to be processed. The high dimensionality of data not only increases the computation burden of computer hardware, but also hinders algorithms to obtain optimal performance. Unsupervised feature selection, which is regarded as a means of dimensionality reduction, has been widely recognized as an important and challenging pre-step for many machine learning and data mining tasks. However, we observe that there are at least two issues in previous unsupervised feature selection methods. Firstly, traditional unsupervised feature selection algorithms usually assume that the data instances are identically distributed and there is no dependency between them. However, the data instances are not only associated with high dimensional features but also inherently interconnected with each other. Secondly, the traditional similarity graph used in previous methods can only describe the pair-wise relations of data, but cannot capture the high-order relations, so that the complex structures implied in the data cannot be sufficiently exploited. In this work, we propose a robust unsupervised feature selection method which embeds the latent representation learning into feature selection. Instead of measuring the feature importances in original data space, the feature selection is carried out in the learned latent representation space which is more robust to noises. In order to capture the local manifold geometrical structure of original data in a high-order manner, a hypergraph is adaptively learned and embedded into the resultant model. An efficient alternating algorithm is developed to optimize the problem. Experimental results on eight benchmark data sets demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						79	97		10.1016/j.neucom.2019.10.018													
J								Selective ensemble of multiple local model learning for nonlinear and nonstationary systems	NEUROCOMPUTING										Nonlinear and time-varying system; Online modeling and prediction; Local model learning; Selective ensemble	ADAPTIVE SOFT SENSOR; VALUED B-SPLINE; NEURAL-NETWORKS; TRACKING CONTROL; IDENTIFICATION; REGRESSION; ALGORITHM; MACHINE	This paper proposes a selective ensemble of multiple local model learning for modeling and identification of nonlinear and nonstationary systems, in which the set of local linear models are self adapted to capture the newly emerging process characteristics and the prediction of the process output is also self adapted based on an optimally selected ensemble of subset linear local models. Specifically, our selective ensemble of multiple local model learning approach performs the model adaptation at two levels. At the level of local model adaptation, a newly emerging process state in the incoming data is automatically identified and a new local linear model is fitted to this newly emerged process state. At the level of online prediction, a subset of candidate local linear models are optimally selected and the prediction of the process output is computed as an optimal linear combiner of the selected subset local linear models. Two case studies involving chaotic time series prediction and modeling of a real-world industrial microwave heating process are used to demonstrate the effectiveness of our proposed approach, in comparison with other existing methods for modeling and identification of nonlinear and time-varying systems. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 22	2020	378						98	111		10.1016/j.neucom.2019.10.015													
J								Impact of fully connected layers on performance of convolutional neural networks for image classification	NEUROCOMPUTING										Convolutional neural networks; Fully connected layers; Image classification; Shallow vs deep CNNs; Wider vs deeper datasets		The Convolutional Neural Networks (CNNs), in domains like computer vision, mostly reduced the need for handcrafted features due to its ability to learn the problem-specific features from the raw input data. However, the selection of dataset-specific CNN architecture, which mostly performed by either experience or expertise is a time-consuming and error-prone process. To automate the process of learning a CNN architecture, this paper attempts at finding the relationship between Fully Connected (FC) layers with some of the characteristics of the datasets. The CNN architectures, and recently datasets also, are categorized as deep, shallow, wide, etc. This paper tries to formalize these terms along with answering the following questions. (i) What is the impact of deeper/shallow architectures on the performance of the CNN w.r.t. FC layers?, (ii) How the deeper/wider datasets influence the performance of CNN w.r.t. FC layers?, and (iii) Which kind of architecture (deeper/shallower) is better suitable for which kind of (deeper/wider) datasets. To address these findings, we have performed experiments with four CNN architectures having different depths. The experiments are conducted by varying the number of FC layers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny ImageNet, and CRCHistoPhenotypes to justify our findings in the context of image classification problem. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						112	119		10.1016/j.neucom.2019.10.008													
J								Mechanisms of dynamical complexity changes in patterns of sensory neurons under antinociceptive effect emergence	NEUROCOMPUTING										Bifurcation analysis; Bursting activity; Nociceptive neuron model; Dynamical complexity	ROOT GANGLION NEURONS; MEMBRANE-POTENTIAL OSCILLATIONS; NA(V)1.7 SODIUM-CHANNELS; MIXED-MODE OSCILLATIONS; SUBTHRESHOLD OSCILLATIONS; BIFURCATION-ANALYSIS; CHRONIC COMPRESSION; NEURAL INFORMATION; POTASSIUM CHANNELS; HOPF-BIFURCATION	The aim is to elucidate mechanisms of changes in dynamical complexity of impulse activity patterns of sensory neurons during the emergence of the antinociceptive response on the painful stimulus. To solve the problem we used the method of bifurcation analysis that enabled us to determine relations between parameters of the nociceptive neuron model and a type of the solution before and during the emergence of the antinociceptive effect. We have shown that the mechanism of the suppression of ectopic bursting discharges is the base for the changes in the dynamical complexity of patterns in the nociceptive neurons. Under the conditions of the potassium blocking, the molecular mechanism of suppression of these discharges can be connected entirely with the modification of the activation gating system of slow Na(V)1.8 sodium channels by the specific action of comenic acid that is a drug substance of a new non-opioid analgesic anoceptin. The obtained results explain one of the possible molecular mechanisms of the analgesic suppression of the neuropathic pain. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						120	128		10.1016/j.neucom.2019.10.004													
J								Reducibilities of hyperbolic neural networks	NEUROCOMPUTING										Complex-valued neural networks; Clifford algebra; Hyperbolic numbers; Reducibility; Singularity	HIERARCHICAL STRUCTURES; LOCAL MINIMA; COMPLEX; UNIQUENESS	Clifford algebra includes the real and complex numbers. The hyperbolic numbers also belong to Clifford algebra. In recent years, neural networks (NNs) are extended using Clifford algebra, and hyperbolic NNs have been proposed. Since the hyperbolic numbers have zero divisors, it is difficult to analyze the hyperbolic NNs. Thus, the reducibilities of hyperbolic NNs have never been revealed. In this work, the reducibilities of hyperbolic NNs are studied. The reducibilities are tightly related to learning process. In the real-valued and complex-valued NNs, there exist three types of reducibilities. In the hyperbolic NNs, there exists another type of reducibilities, and it has been difficult to determine all the reducibilities of hyperbolic NNs. It is proved that hyperbolic NNs have another reducibility, referred to as hyperbola-reducibility, and all the reducibities of hyperbolic NNs are determined. In addition, the inherent singularities of hyperbolic NNs are revealed. These facts are expected to improve the learning process of hyperbolic NNs in future. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						129	141		10.1016/j.neucom.2019.07.101													
J								A Pareto-smoothing method for causal inference using generalized Pareto distribution	NEUROCOMPUTING										Causality; Causal inference; Machine learning; Treatment effect; Importance sampling	MODEL; OVERLAP	Causal inference aims to estimate the treatment effect of an intervention on the target outcome variable and has received great attention across fields ranging from economics and statistics to machine learning. Observational causal inference is challenging because the pre-treatment variables may influence both the treatment and the outcome, resulting in confounding bias. The classic inverse propensity weighting (IPW) estimator is theoretically able to eliminate the confounding bias. However, in observational studies, the propensity scores used in the IPW estimator must be estimated from finite observational data and may be subject to extreme values, leading to the problem of highly variable importance weights, which consequently makes the estimated causal effect unstable or even misleading. In this paper, by reframing the IPW estimator in the importance sampling framework, we propose a Pareto-smoothing method to tackle this problem. The generalized Pareto distribution (GPD) from extreme value theory is used to fit the upper tail of the estimated importance weights and to replace them using the order statistics of the fitted GPD. To validate the performance of the new method, we conducted extensive experiments on simulated and semi-simulated datasets. Compared with two existing methods for importance weight stabilization, i.e., weight truncation and self-normalization, the proposed method generally achieves better performance in settings with a small sample size and high-dimensional covariates. Its application on a real-world heath dataset indicates its utility in estimating causal effects for program evaluation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						142	152		10.1016/j.neucom.2019.09.095													
J								Robust optimal graph clustering	NEUROCOMPUTING										Graph-based clustering; Sparse reconstruction; Similarity graph; Adaptive neighbors	IMAGE	Most graph-based clustering methods separate the graph construction and clustering into two independent processes. The manually pre-constructed graph may not be suitable for the subsequent clustering. Moreover, as real world data generally contains noises and outliers, the similarity graph directly learned from them will be unreliable and further impair the subsequent clustering performance. To tackle the problems, in this paper, we propose a novel clustering framework where a robust graph is learned with noise removal, and simultaneously, with desirable clustering structure. To this end, we first learn a discriminative representation of data samples via sparse reconstruction. Then, a robust graph is automatically constructed with adaptive neighbors to each data sample. Simultaneously, a reasonable rank constraint is imposed on the Laplacian matrix of similarity graph to pursue the ideal clustering structure, where the number of connected components in the learned graph is exactly equal to the number of clusters. We finally derive an alternate optimization algorithm guaranteed with convergence to solve the formulated unified learning framework to achieve better prediction accuracy. Experiments on both synthetic and real datasets demonstrate the superior performance of the proposed method compared with several state-of-the-art clustering techniques. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						153	165		10.1016/j.neucom.2019.07.102													
J								Mask-CDNet: A mask based pixel change detection network	NEUROCOMPUTING										Mask-CDNet; Pixel-level change; Unaligned images	IMAGES; CLASSIFICATION; MODEL	Change detection between multi-temporal images becomes a core technique widely used in various fields. But there still exist some challenging issues in the field of change detection. This thesis mainly focuses on the issue that two compared images captured at different times are hard to be precisely aligned due to the fact that the camera is mounted on a moving platform. This issue will lead to the problem that it is difficult to determine the change area between a pair of roughly registered images in pixel-level. To conquer this problem, a novel deep learning based change detection framework, consisting of two collaborative modules, is proposed to improve the estimation accuracy and computation efficiency. The first module is mainly used to roughly predict change areas and match information of two unaligned compared images in the absence of ground truth. The second module aims to refine the change areas and make results more accurate and interpretative. Because of the designed two modules, the framework could not only deal with two roughly registered images, but also be robust to uninteresting change coming from noise or arbitrary spurious differences. Although it is a two-stage method, the proposed framework is trained end-to-end. Meanwhile, the design of pure network make it efficient. Experiments evaluated on PCD-2015 dataset and AICD 2012 dataset demonstrate the method outperforms existing literatures. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						166	178		10.1016/j.neucom.2019.10.022													
J								Abstractive meeting summarization by hierarchical adaptive segmental network learning with multiple revising steps	NEUROCOMPUTING										Abstractive meeting summarization; Attention; Reinforcement learning		The Abstractive meeting summarization is a challenging problem in natural language understanding, which automatically generates the condensed summary covering the important points in the meeting conversation. However, the existing abstractive summarization works mainly focus on the structured text documents, which may be ineffectively applied to the meeting summarization task due to the lack of modeling the unstructured long-form conversational contents. In this paper, we consider the problem of abstractive meeting summarization from the viewpoint of hierarchical adaptive segmental encoder-decoder network learning. We propose the hierarchical neural encoder based on adaptive recurrent networks to learn the semantic representation of meeting conversation with adaptive conversation segmentation. We then devise a multi-step revising mechanism to refine the learned semantic representation. We finally develop the reinforced decoder network to generate the high-quality summaries for abstractive meeting summarization. We conduct the extensive experiments on the well-known AMI meeting conversation dataset to validate the effectiveness of our proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						179	188		10.1016/j.neucom.2019.10.019													
J								Efficient precise weight tuning protocol considering variation of the synaptic devices and target accuracy	NEUROCOMPUTING										Conductance variation; Neural networks; Synaptic device; Weight mapping; Weight tuning protocol	MEMORY	In off-chip training, we can improve the inference accuracy of hardware-based neural networks by reducing the conductance (weight) variation of synaptic devices through precise program/erase control in weight mapping. However, the precise weight tuning protocol (PWTP) requires a significant amount of time because it requires repeated read-verify-write cycles for each synapse device. In this paper, we propose an efficient PWTP method to significantly reduce the weight mapping time by greatly reducing the number of synaptic devices to which PWTP should be applied. In the proposed method, the effect of weight variation of synaptic devices on the inference accuracy of neural networks depends largely on which layer the synaptic devices belong to. Using our layer-selection method, the required percentage of PWTP-applied synaptic devices is can be reduced by up to 2600 times compared to that of the conventional method where PWTP is applied to all or part of the synaptic devices which are simply ranked by the weight magnitude. Also, three criteria of variation sensitivity are evaluated and compared in the method of selecting synaptic devices to which PWTP is applied within the selected layer. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						189	196		10.1016/j.neucom.2019.09.099													
J								Video summarization via block sparse dictionary selection	NEUROCOMPUTING										Block-sparsity; Video summarization; Sparse representation; Dictionary selection	RECOVERY; SIGNALS	The explosive growth of video data has raised new challenges for many video processing tasks such as video browsing and retrieval, hence, effective and efficient video summarization (VS) is urgently demanded to automatically summarize a video into a succinct version. Recent years have witnessed the advancements of sparse representation based approaches for VS. However, video frames are analyzed individually for keyframe selection in existing methods, which could lead to redundancy among selected keyframes and poor robustness to outlier frames. Due to that adjacent frames are visually similar, candidate keyframes often occur in temporal blocks, in addition to sparse presence. Therefore, in this paper, the block-sparsity of candidate keyframes is taken into consideration, by which the VS problem is formulated as a block sparse dictionary selection model. Moreover, a simultaneous block version of Orthogonal Matching Pursuit (SBOMP) algorithm is designed for model optimization. Two keyframe selection strategies are also explored for each block. Experimental results on two benchmark datasets, namely VSumm and TVSum datasets, demonstrate that the proposed SBOMP based VS method clearly outperforms several state-of-the-art sparse representation based methods in terms of F-score, redundancy among keyframes and robustness to outlier frames. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						197	209		10.1016/j.neucom.2019.07.108													
J								Alpha divergence minimization in multi-class Gaussian process classification	NEUROCOMPUTING										Gaussian processes; Expectation propagation; alpha-divergences; Approximate inference; Variational inference		This paper analyzes the minimization of alpha-divergences in the context of multi-class Gaussian process classification. For this task, several methods are explored, including memory and computationally efficient variants of the Power Expectation Propagation algorithm, which allow for efficient training using stochastic gradients and mini-batches. When these methods are used for training, very large datasets (several millions of instances) can be considered. The proposed methods are also very general as they can interpolate between other popular approaches for approximate inference based on Expectation Propagation (EP) (alpha -> 1) and Variational Bayes (VB) (alpha -> 0) simply by varying the alpha parameter. An exhaustive empirical evaluation analyzes the generalization properties of each of the proposed methods for different values of the alpha parameter. The results obtained show that one can do better than EP and VB by considering intermediate values of alpha. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						210	227		10.1016/j.neucom.2019.09.090													
J								Some necessary and sufficient conditions for containment of second-order multi-agent systems with sampled position data	NEUROCOMPUTING										Multi-agent system; Containment control; Sampled position data; Communication width; Sampling period	DOUBLE-INTEGRATOR DYNAMICS; CONSENSUS; CONTROLLABILITY	The containment control problem using only sampled position data for second-order multi-agent systems (MAS) is studied in this paper. We first consider the delay-free case of the containment control protocol and obtain a necessary and sufficient condition regarding the network structure and the system parameters. Then, we obtain a similar necessary and sufficient condition for the case with time delay of the containment control protocol. Finally, several numerical simulations are performed to verify the correctness of the theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						228	237		10.1016/j.neucom.2019.10.031													
J								Passive browser identification with multi-scale Convolutional Neural Networks	NEUROCOMPUTING										Browser fingerprinting; Convolutional Neural Networks (CNNs); Net flow	TRACKING; WEB	Browser identification is the act of recognizing web traffic through surveillance despite the use of encryption or anonymizing software. Although previous work has reported some promising results, browser fingerprinting is still an emerging technique and has not reached an acceptable level of performance. This paper presents a novel approach by using deep-convolutional-neural-network-based (deep CNN) learning model to extract the complete shape of traffic I/O graph signal in obtaining stable traffic characteristics, employing nonlinear multi-class classification algorithms to perform the task of browser identification. The approach is evaluated on a new dataset collected across a large number of websites. Extensive experimental results show that traffic characteristics which are learned from I/O graph by deep CNN are much more stable and discriminative than the metrics those are obtained from the early studies, and the approach achieves a practically useful level of performance with significant precision and recall. Additional experiments on the depth of deep CNN are provided to further examine the applicability of our approach. Our dataset is publicly available to facilitate future research. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						238	247		10.1016/j.neucom.2019.10.028													
J								Prediction of blood glucose concentration for type 1 diabetes based on echo state networks embedded with incremental learning	NEUROCOMPUTING										Blood glucose prediction; Continuous glucose monitoring systems (CGMS); Echo state networks (ESN); Incremental learning; Feedback network	MODEL; INFORMATION; FRAMEWORK; ACCURACY; BATCH	Valid prediction of blood glucose concentration can help people to manage diabetes mellitus, alert hypoglycemia/hyperglycemia, exploit artificial pancreas, and plan a treatment program. Along the development of continuous glucose monitoring system (CGMS), the massive historical data require a new modeling framework based on a data-driven perspective. Studies indicate that the glucose time series (i.e., CGMS readings) involve chaotic properties; therefore, echo state networks (ESN) and its improved variants are proposed to establish subject-specific prediction models owing to their superiority in processing chaotic systems. This study mainly has two innovations: (1) a novel combination of incremental learning and ESN is developed to obtain a suitable network structure through partial optimization of parameters; (2) a feedback ESN is proposed to excavate the relationship of different predictions. These methods are assessed on ten patients with diabetes mellitus. Experimental results substantiate that the proposed methods achieve superior prediction performance in terms of four evaluation metrics compared with three conventional methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						248	259		10.1016/j.neucom.2019.10.003													
J								Deep neural networks compression learning based on multiobjective evolutionary algorithms	NEUROCOMPUTING										Neural network compression; Evolutionary optimization; Pareto-optimal solutions; Expensive optimization	EFFICIENT GLOBAL OPTIMIZATION	This work addresses the problem of deep neural network compression, which is a promising technique to reduce the number of network parameters and/or speed up the network evaluation process significantly. Most of the existing methods rely on domain experts' experience for the selection of hyperparameters such as the rank and sparsity ratio of the weight matrix in order to get an appropriate compression result without serious performance downgrade. However, they usually suffer from heavy computational loads due to the large number of tests in revealing the best hyperparameters. In this work, we propose an efficient approach to network compression from the perspective of multiobjective evolution. The contributions in the paper are twofolds: (1) We build a multiobjective compression learning model that considers the model classification error rate and compression rate as two objectives in the optimization, which can provide a compromise of the tradeoffs between these two objectives. (2) A mechanism for approximate compressed model generation is devised in the framework of expensive multiobjective optimization, which is able to reduce the high model training costs involved in the optimization process. Experiments are carried out to confirm the superiority of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						260	269		10.1016/j.neucom.2019.10.053													
J								Diverse frequency band-based convolutional neural networks for tonic cold pain assessment using EEG	NEUROCOMPUTING										Tonic cold pain classification; EEG Convolutional Neural Networks (ConvNets); Deep learning; Pattern recognition	MECHANISMS; POWER; STIMULATION; PREDICTION; RESPONSES; HAND	The purpose of this study is to present a novel classification framework, called diverse frequency band-based Convolutional Neural Networks (DFB-based ConvNets), which can objectively identify tonic cold pain states. To achieve this goal, scalp EEG data were recorded from 32 subjects under cold stimuli conditions. The proposed DFB-based ConvNets model is capable of classifying three classes of tonic pain: No pain, Moderate Pain, and Severe Pain. Firstly, the proposed method utilizes diverse frequency band-based inputs to learn temporal representations from different frequency bands of Electroencephalogram (EEG) which are expected to have more discriminative power. Then the derived features are concatenated to form a feature vector, which is fed into a fully-connected network for performing the classification task. Experimental results demonstrate that the proposed method successfully discriminates the tonic cold pain states. To show the superiority of the DFB-based ConvNets classifier, we compare our results with the state-of-the-art classifiers and show it has a competitive classification accuracy (97.37%). Moreover, these promising results may pave the way to use DFB-based ConvNets in clinical pain research. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						270	282		10.1016/j.neucom.2019.10.023													
J								Aggregating diverse deep attention networks for large-scale plant species identification	NEUROCOMPUTING										Large-scale plant species identification; Plant taxonomy; Attention-based hierarchical multi-task learning; Fusion	CLASSIFICATION	In this paper, a novel fusion method is proposed to deal with large-scale plant species identification by aggregating diverse outputs from multiple deep networks, where each deep network focus on one subset of the whole plant species. Firstly, a fixed plant taxonomy is constructed for organizing large number of fine-grained plant species hierarchically and it is further used as a guideline to help generating diverse but overlapped task groups. Secondly, an attention-based deep hierarchical multi-task learning (AHMTL) algorithm is proposed to recognize fine-grained plant species belonging to the same task group effectively by learning more discriminative deep features and classifiers jointly. Finally, we fuse all outputs from multiple deep networks to obtain the final high-level feature representation and give the prediction probability for each plant species. The experimental results have proved the effectiveness of our proposed method on large-scale plant species identification. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						283	294		10.1016/j.neucom.2019.10.077													
J								An expectation-maximization based single-beacon underwater navigation method with unknown ESV	NEUROCOMPUTING										Single-beacon navigation; Expectation-maximization; Effective sound velocity; Kalman filter	ACOUSTIC NAVIGATION; AUV NAVIGATION; LOCALIZATION; ALGORITHM; IDENTIFICATION; SYSTEMS; FILTER	Navigation performance in a single-beacon underwater navigation system considerably depends on the accuracy of the slant-range measurement. Ranges are usually obtained based on a presumed or known effective sound velocity (ESV). Because it is difficult to accurately determine the ESV between the pinger and the receiver, traditional methods are usually affected by large-range measurement errors that lead to large positioning errors. In this study, we use the expectation maximization (EM) method, which is widely used for parameter identification, to estimate the unknown ESV by treating it as a model parameter. We propose an EM-based, single-beacon navigation method that incorporates the Kalman filter into the EM frame. Numerical examples using simulated and field data indicate that navigation accuracy can be significantly improved when the proposed EM-based method is implemented, and the estimated ESV is in good agreement with its true value. (C) 2019 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 22	2020	378						295	303		10.1016/j.neucom.2019.10.066													
J								Event-triggered ADP control of a class of non-affine continuous-time nonlinear systems using output information	NEUROCOMPUTING										Event-triggered approach; Observer; Adaptive dynamic programming; Non-affine system; Neural network	FEEDBACK; DYNAMICS; TRACKING	An event-triggered adaptive dynamic programming (ADP) approach is proposed for a class of non-affine continuous-time nonlinear systems with unknown internal states. A neural networks (NNs)-based observer is designed to reconstruct internal states of the system using output information, and then, by the estimation signals, an output feedback ADP control approach is developed in an event-triggered manner. The proposed approach samples the states and updates the control signal only when the triggered condition is violated, and critic NNs are designed to approximate the performance index. Compared with the traditional ADP one under a fixed sampling mechanism, the event-triggered control approach reduces the computation resource and transmission load in the learning process. The stability analysis of the closed-loop system is provided based on the Lyapunov's theorem. Two simulation results also verify the theoretical claims. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						304	314		10.1016/j.neucom.2019.08.097													
J								Semantic-based padding in convolutional neural networks for improving the performance in natural language processing. A case of study in sentiment analysis	NEUROCOMPUTING										Natural language processing; Convolutional neural networks; Padding		In this work, a methodology for applying semantic-based padding in Convolutional Neural Networks for Natural Language Processing tasks is proposed. Semantic-based padding takes advantage of the unused space required for having a fixed-size input matrix in a Convolutional Network effectively, using words present in the sentence. The methodology proposed has been evaluated intensively in Sentiment Analysis tasks using a variety of word embeddings. In all the experimentation carried out the proposed semantic-based padding improved the results achieved when no padding strategy is applied. Moreover, when the model used a pre-trained word embeddings, the performance of the state of the art has been surpassed. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 22	2020	378						315	323		10.1016/j.neucom.2019.08.096													
J								Integral sliding mode synchronization control for Markovian jump inertial memristive neural networks with reaction-diffusion terms	NEUROCOMPUTING										IMNNs; Markovian jump parameters; Reaction-diffusion terms; Sliding mode control scheme; Algebraic inequalities	FINITE-TIME STABILIZATION; PASSIVITY ANALYSIS; VARYING DELAYS; STABILITY; SYSTEMS	This paper is concerned with the synchronization problem for a class of inertial memristive neural networks (IMNNs), where the Markovian jump parameters and reaction-diffusion terms are involved. First, by choosing a suitable variable substitution, the original second-order differential systems can be transformed into the first-order ones. Then, based on the sliding mode control scheme, a specific sliding mode function that contains some mode-dependent integral terms and an integral term of the symbol function is proposed, so that the computational difficulties in handling Markovian jump IMNNs with reaction-diffusion terms can be solved. The synchronization criterion in forms of algebraic inequalities can be obtained by the ingenious use of some inequality techniques. Finally, three examples are provided to verify the feasibility, practicability and superiority of the proposed theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						324	334		10.1016/j.neucom.2019.10.047													
J								Automatic fetal brain extraction from 2D in utero fetal MRI slices using deep neural network	NEUROCOMPUTING										Fetal MRI; Brain extraction; Fully connected network; Deep learning; Residual learning block; Segmentation	SEGMENTATION; IMAGES; ATLAS; HASTE	Background: In utero fetal MRI has been developing in common medical prenatal practice for nearly two decades. But the applications and research on fetal MRI still lag behind due to the lack of specialized image processing and analysis tools. Brain extraction, as an initial preprocessing step for many brain MRI-based processing methods, is an important basis for accurate fetal MRI analysis. However, it is very challenging to automatically extract fetal brains from fetal MRI due to the large variation in fetal brains across different gestational weeks and complex maternal tissues surrounding the fetal brains. Method: We proposed a novel two-step framework using the deep learning method for solving the challenging problem of automatic fetal brain extraction in 2D in utero fetal MRI slices. The proposed framework consisted of two fully convolutional network (FCN) models, i.e., a shallow FCN and an extra deep multi-scale FCN (M-FCN). The first shallow FCN rapidly located the fetal brain and extracted the region of interest (ROI) containing the brain. Then, within the brain ROI, the M-FCN further refined the segmentation and produced the final brain mask by leveraging the multi-scale information and residual learning blocks. Dilated convolutional layers were employed in both FCNs to control the size of feature maps and increase the field of view. Result: Eighty-eight 2D fetal MRIs were collected for experiments. We compared our method with the state-of-the-art methods on extracting fetal brains. It has been evaluated that our proposed framework outperformed the other methods in both fetal brain localization and segmentation tasks. With the proposed method, we located the fetal brain with an accuracy of 100%. The brain segmentation performance was measured based on the overlap between the automatic segmentations and the manual segmentations. Our proposed method achieved an average of 0.958 Dice score, 0.950 sensitivity rate, and 0.968 precision on the testing dataset, and it took an average of 6 s to process one fetal MRI stack on a work-station with TITAN X GPU and i7-6700 CPU. Conclusion: In this paper, we proposed an effective and efficient deep learning framework for automatic fetal brain extraction from fetal MRI. It has been validated with solid experiments that the proposed method can be used as a practical and useful tool in clinical practice and neuroscience research. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 22	2020	378						335	349		10.1016/j.neucom.2019.10.032													
J								Effective piecewise planar modeling based on sparse 3D points and convolutional neural network	NEUROCOMPUTING										Urban scene; Piecewise planar stereo; Markov Random Field; Image over-segmentation; Convolutional neural network	SUPERPIXELS	Piecewise planar stereo methods can approximately reconstruct the complete structures of a scene by overcoming challenging difficulties (e.g., poorly textured regions) that pixel-level stereo methods cannot resolve. In this paper, a novel plane assignment cost is first constructed by incorporating scene structure priors and high-level image features obtained by convolutional neural network (CNN). Then, the piecewise planar scene structures are reconstructed in a progressive manner that jointly optimizes image regions (or superpixels) and their associated planes, followed by a global plane assignment optimization under a Markov Random Field (MRF) framework. Experimental results on a variety of urban scenes confirm that the proposed method can effectively reconstruct the complete structures of a scene from only sparse three-dimensional (3D) points with high efficiency and accuracy and can achieve superior results compared with state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						350	363		10.1016/j.neucom.2019.10.026													
J								3D-SSD: Learning hierarchical features from RGB-D images for amodal 3D object detection	NEUROCOMPUTING										Hierarchical feature fusion; Multi-layer prediction; Real-time 3D detector		This paper aims at developing a faster and more accurate solution to the amodal 3D object detection problem for indoor scenarios. The solution is achieved through a novel neural network structure which takes a pair of RGB-D images as input and delivers oriented 3D bounding boxes as the output. Such network, named 3D-SSD, has two components: hierarchical feature fusion and multi-layer prediction. The hierarchical feature fusion combines multi-scale appearance and geometric features learned from RGB-D images, which is later utilized in the multi-layer prediction for object detection. Both the accuracy and the efficiency can be improved by exploiting 2.5D representations in a synergistic way. To specifically address the shape variance of different objects, a set of 3D anchor boxes with varying physical sizes are attached to every location on the prediction layers. While testing, the category scores for 3D anchor boxes are generated with adjusted positions, sizes and orientations, leading to the final detections using non-maximum suppression. Comprehensive experiments have been performed on publicly accessible dataset of SUN RGB-D and NYUV2. The results show the proposed algorithm is the first 3D detector that runs in near real-time on the challenging datasets with competitive performance to the state-of-the-art methods. The 3D-SSD gets 37.1% mAP on the SUN RGB-D dataset at around 5.6 fps, which outperforms the state-of-the-art Deep Sliding Shape by 10.2% mAP and around 109 x faster. For an efficient model setting with a rate of 9.3 fps, 3D-SSD still gets an accuracy of 37% on mAP. Further, experiments also suggest the proposed approach achieves comparable accuracy and is about 477 x faster than the state-of-art method on the NYUv2 dataset even with a smaller input image size. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 22	2020	378						364	374		10.1016/j.neucom.2019.10.025													
J								Enhancing multi-view clustering through common subspace integration by considering both global similarities and local structures	NEUROCOMPUTING										Multi-view learning; Subspace learning; Graph learning; Clustering; Integration		Multi-view clustering seeks to partition objects based on various observations by utilizing cross-views to provide a complementary description of the same objects. It remains challenging to effectively fuse the multi-view data with various dimensions as well as different structures into a new yet highly informative form, thus facilitating adequate assignment of the objects. To tackle the issue, we propose a common subspace integration (CSI) model. The CSI actively learns a common subspace by jointly preserving the local geometry of each view, while incorporating a global partition information to enhance its separability during the learning process. It can be easily generalized to its kernel version, thereby popularizing its general usages. An effective alternative optimization scheme is designed to solve the proposed model. Extensive experiments on six real-world datasets were conducted to demonstrate its superiority by comparing with the twelve state-of-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						375	386		10.1016/j.neucom.2019.10.014													
J								Deep Gabor convolution network for person re-identification	NEUROCOMPUTING										Person re-identification; Gabor convolution; Resnet-50; Gabor filter	FEATURES	Person re-identification is an import problem in computer vision fields and more and more deep neural network models have been developed for representation learning in this task due to their good performance. However, compared with hand-crafted feature representations, deep learned features cannot not be interpreted easily. To meet this demand, motivated by the Gabor filters' good interpretability and the deep neural network models' reliable learning ability, we propose a new convolution module for deep neural networks based on Gabor function (Gabor convolution). Compared with classical convolution module, every parameter in the proposed Gabor convolution kernel has a specific meaning while classical one has not. The Gabor convolution module has a good texture representation ability and is effective when it is embedded in the low layers of a network. Besides, in order to make the proposed Gabor module meaningful, a new loss function designed for this module is proposed as a regularizer of total loss function. By embedding the Gabor convolution module to the Resnet-50 network, we show that it has a good representation learning ability for person re-identification. And experiments on three widely used person re-identification datasets show favorable results compared with the state-of-the-arts. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						387	398		10.1016/j.neucom.2019.10.083													
J								Global exponential dissipativity of neutral-type BAM inertial neural networks with mixed time-varying delays	NEUROCOMPUTING										BAM inertial neural network; Dissipativity; Globally exponentially attractive set; Neutral-type; Mixed time-varying delay; Inequality	STABILITY ANALYSIS; DISCRETE; SYNCHRONIZATION; STABILIZATION; SYSTEMS; LEAKAGE	This paper considers the global exponential dissipativity of neutral-type BAM inertial neural networks with mixed time-varying delays. Firstly, we transform the proposed BAM inertial neural networks to usual one. Secondly, by establishing a new neutral-type differential inequality and employing Lyapunov method and analytical techniques, some novel sufficient conditions in accordance with algebraic and linear matrix inequalities are obtained for the global exponential dissipativity of the addressed neural networks. Moreover, the globally exponentially attractive sets and the exponential convergence rate index are also assessed. Finally, the effectiveness of the obtained results is illustrated by some examples with numerical simulations. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						399	412		10.1016/j.neucom.2019.10.082													
J								Off-policy synchronous iteration IRL method for multi-player zero-sum games with input constraints	NEUROCOMPUTING										Adaptive dynamic programming; Zero-sum game; Integral reinforcement learning; Off-policy; Constrained optimal control	NONLINEAR-SYSTEMS; STABILITY	In this paper, a novel synchronous off-policy method is given to solve multi-player zero-sum (ZS) game under the condition that the knowledge of system data are completely unknown, the actuators of controls are constrained and the disturbances are bounded simultaneously. The cost functions are built by non-quadratic functions to reflect the constrained properties of inputs. The integral reinforcement learning (IRL) technology is employed to solve Hamilton-Jacobi-Bellman equation, so that the system dynamics are not necessary anymore. The obtained value function is proved to converge to the optimal game values. And the equivalent of traditional policy iteration (PI) algorithm and the proposed algorithm is given in solving the multi-player ZS game with constrained inputs. Three neural networks in this paper are utilized, the critic neural network (CNN) to approach the cost function, the action neural network (ANN) to approach the control policies and the disturbance neural networks (DNN) to approach the disturbances are utilized. Finally, a simulation example is given to demonstrate the convergence and performance of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						413	421		10.1016/j.neucom.2019.10.075													
J								Non-overlapping classification of hyperspectral imagery based on set-to-sets distance	NEUROCOMPUTING										Set to sets distance; Collaborative representation; Hyperspectral imagery; Spectral-spatial classification	SPECTRAL-SPATIAL CLASSIFICATION; SPARSE-REPRESENTATION; COLLABORATIVE REPRESENTATION; EXTINCTION PROFILES; SEGMENTATION	Spectral-spatial classification methods can improve the classification accuracy of hyperspectral imagery (HSI) dramatically. However, this improvement is caused in part by the overlap between training set and test set. In this paper, a novel non-overlapping spectral-spatial classification framework based on set-to-sets distance is proposed. First, each class is sampled using a controlled random sampling method, which is regarded as a training set. The image is segmented into many superpixels and each superpixel is taken as a test set. In order to ensure that the test set and the training set are not overlapped with each other, the training pixels contained in each test superpixel are deleted. Then, the training set of each class is compressed into a more compact set to reduce the computational complexity and kernel trick is used to make samples be approximately linearly separable. Finally, each test set is modeled as a convex hull, this hull is represented collaboratively with all training sets. With the resolved representation coefficients, the distance between the test set and each training set can be calculated for classification. Experimental results based on three real HSI data sets demonstrate the superiority of the proposed method to state-of-the-art algorithms under the non-overlapping sampling strategy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						422	434		10.1016/j.neucom.2019.10.071													
J								Beyond EM: A faster Bayesian linear regression algorithm without matrix inversions	NEUROCOMPUTING										Bayesian linear regression; Gaussian prior; Matrix-inversion-free		The Bayesian linear regression is a useful tool for many scientific communities. This paper presents a novel algorithm for solving the Bayesian linear regression problem with Gaussian priors, which shares the same spirit as the gradient based methods. In addition, the standard scheme for this task, the Expectation Maximization (EM) algorithm, involves matrix inversions but our proposed algorithm is free of. Numerical experiments demonstrate that the proposed algorithm performs as well as the gradient based and EM algorithms in term of precision, but runs significantly faster than the gradient based and EM algorithms. Due to its matrix-inversion-free nature, the algorithm of this paper is a viable alternative to the competing methods available in the literature. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						435	440		10.1016/j.neucom.2019.10.061													
J								A structure-guided approach to the prediction of natural image saliency	NEUROCOMPUTING										Visual saliency; Eye-tracking dataset; Scene structure	EYE-MOVEMENTS; SCENE; ATTENTION; FEATURES; OBJECTS; SEARCH; MODEL	The structure of a scene provides global contextual information in directing gaze and complements local object information in saliency prediction. In this study, we explore how visual attention can be affected by scene structures, namely openness, depth and perspective. We first build an eye tracking dataset with 2500 natural scene images and collect gaze data via both eye tracking and mouse tracking. We make observations on scene layout properties and propose a set of scene structural features relating to visual attention. The set of complementary features are then integrated for saliency prediction. Our features are independent of and can work together with many computational modules, and this work demonstrates the use of Multiple kernel learning (MKL) as an example to integrate the features at low- and high-levels. Experimental results demonstrate that our model outperforms existing methods and our scene structural features can improve the performance of other saliency models in outdoor scenes. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						441	454		10.1016/j.neucom.2019.09.085													
J								DCGSA: A global self-attention network with dilated convolution for crowd density map generating	NEUROCOMPUTING										Crowd density map; Convolutional neural networks; Self-attention; Global context; Dilated convolution		Due to non-uniform density and variations in scale and perspective, estimating crowd count in crowded scenes in different degree is an extremely challenging task. The deep learning models mostly use pooling operation so that the density map of original resolution is obtained through the last upsampling. This paper aims to solve the problem of losing local spatial information by pooling in density map estimation. Therefore, we propose a dilated convolution neural network with global self-attention, named DCGSA. Especially, we introduce a Global Self-Attention module (GSA) to provide global context as guidance of low-level features to select person location details and a Pyramid Dilated Convolution module (PDC) that extracts channel-wise and pixel-wise features more precisely. Extensive experiments on several crowd datasets show that our method achieves lower crowd counting error and better density maps compared to the recent state-of-the-art methods. In particular, our method also performs well on the sparse dataset UCSD. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 22	2020	378						455	466		10.1016/j.neucom.2019.10.081													
J								SAUV-A Bio-Inspired Soft-Robotic Autonomous Underwater Vehicle	FRONTIERS IN NEUROROBOTICS										exoskeleton; genetic algorithm; compliant structure; shock absorbability; cave diving	TRACKING CONTROL; DESIGN; TARGET	Autonomous and remotely operated underwater vehicles allow us to reach places which have previously been inaccessible and perform complex repair, exploration and analysis tasks. As their navigation is not infallible, they may cause severe damage to themselves and their often fragile surroundings, such as flooded caves, coral reefs, or even accompanying divers in case of a collision. In this study, we used a shallow neural network, consisting of interlinking PID controllers, and trained by a genetic algorithm, to control a biologically inspired AUV with a soft and compliant exoskeleton. Such a compliant structure is a versatile and passive solution which reduces the accelerations induced by collisions to 56% of the original mean value acting upon the system, thus, notably reducing the stress on its components and resulting reaction forces on its surroundings. The segmented structure of this spherical exoskeleton protects the encased system without limiting the use of cameras, sensors or manipulators.																	1662-5218					FEB 21	2020	14								8	10.3389/fnbot.2020.00008													
J								Explicit feedback meet with implicit feedback in GPMF: a generalized probabilistic matrix factorization model for recommendation	APPLIED INTELLIGENCE										Recommendation system; Explicit and implicit feedback; Reliability score; Review network; Probabilistic matrix factorization		Recommender Systems focus on implicit and explicit feedback or parameters of users for better rating prediction. Most of the existing recommender systems use only one type of feedback ignoring the other one. Based on the availability of resources, we may consider more number of feedback of both the types to predict user's rating for a particular item more accurately. However to the best of our knowledge, there is no generalized model that is fitted for multiple parameters or feedback. In this paper, we have proposed a Generalized Probabilistic Matrix Factorization (GPMF) model which uses multiple parameters of both the types for recommendation. To build GPMF, first we develop three models focusing on users' crucial side information. First model PMFE (P robabilistic M atrix F actorization with E xplicit_Feedback) is proposed based on an explicit feedback of users and second one is PMFI (P robabilistic M atrix F actorization with I mplicit_Feedback), where an implicit feedback is considered. The last one is PMFEI (P robabilistic M atrix F actorization with E xplicit and I mplicit_Feedback), where both explicit and implicit feedback are considered. Extensive experiments on real world datasets show that PMFEI performs better compare to baselines. PMFEI model also performs better compare to baselines for cold-start users and cold-start items also. In our experimental section, it is shown that GPMF performs better when we consider both explicit and implicit feedback. The effectiveness of each parameter is not same for recommendation. Using GPMF we can estimate the effectiveness of a parameter. Based on this effectiveness, we can add or remove more parameters for better rating prediction.																	0924-669X	1573-7497				JUN	2020	50	6					1955	1978		10.1007/s10489-020-01643-1		FEB 2020											
J								An intuitionistic linguistic MCDM model based on probabilistic exceedance method and evidence theory	APPLIED INTELLIGENCE										Fuzzy linguistic model; Multi-criteria decision making; Non-scalar criteria satisfaction; Intuitionistic fuzzy sets; Choquet probabilistic exceedance method	DECISION-MAKING; FUZZY-SETS; TERM SETS; DIVERGENCE MEASURE; AGGREGATION; OPERATORS; NUMBERS	The optimization in multi-criteria decision making under uncertain conditions has attracted more and more scholars in recent years. However, it is still an open issue that how to better evaluate the satisfaction with more complex objects. Since the great performance of intuitionistic fuzzy set on handling the uncertain information, in this paper, a new fuzzy linguistic model for non-scalar criteria satisfaction expressed via intuitionistic fuzzy sets is proposed, which makes experts evaluate more objectively. Moreover, a corresponding aggregation approach based on the Choquet probabilistic exceedance method is also proposed. After a series of calculation processes, the final aggregated results embodied by intuitionistic fuzzy sets (IFSs) can be obtained. Then by converting them into the belief intervals, the best alternative can be selected more objectively. In addition, two real-life applications are shown to demonstrate the practicality of proposed method.																	0924-669X	1573-7497				JUN	2020	50	6					1979	1995		10.1007/s10489-020-01638-y		FEB 2020											
J								A graph-based taxonomy of citation recommendation models	ARTIFICIAL INTELLIGENCE REVIEW										Recommender systems; Research paper recommender systems; Algorithms taxonomy; Information retrieval; Survey; Neural networks	SYSTEMS; REPRESENTATION; NETWORKS	Recommender systems have been used since the beginning of the Web to assist users with personalized suggestions related to past preferences for items or products including books, movies, images, research papers and web pages. The availability of millions research articles on various digital libraries makes it difficult for a researcher to find relevant articles to his/er research. During the last years, a lot of research have been conducted through models and algorithms that personalize papers recommendations. With this survey, we explore the state-of-the-art citation recommendation models which we categorize using the following seven criteria: platform used, data factors/features, data representation methods, methodologies and models, recommendation types, problems addressed, and personalization. In addition, we present a novel k-partite graph-based taxonomy that examines the relationships among surveyed algorithms and corresponding k-partite graphs used. Moreover, we present (a) domain's popular issues, (b) adopted metrics, and (c) commonly used datasets. Finally, we provide some research trends and future directions.																	0269-2821	1573-7462				OCT	2020	53	7					5217	5260		10.1007/s10462-020-09819-4		FEB 2020											
J								Enhancing a machine learning binarization framework by perturbation operators: analysis on the multidimensional knapsack problem	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Combinatorial Optimisation; Machine Learning; Metaheuristics; KNN; K-means; Knapsack	ARTIFICIAL ALGAE ALGORITHM; METAHEURISTIC OPTIMIZATION; CLUSTERING-ALGORITHM; SYSTEM	Solving combinatorial optimization problems is of great interest in the areas of computer science and operations research. Optimization algorithms and particularly metaheuristics are constantly improved in order to reduce execution times, increase the quality of solutions and address larger instances. In this work, an improvement of the binarization framework which uses the K-means technique is developed. To achieve this, a perturbation operator based on the K-nearest neighbor technique is incorporated into the framework with the aim of generating more robust binarized algorithms. The technique of K-nearest neighbors is used for improving the properties of diversification and intensification of metaheuristics in its binary version. The contribution of the K-nearest neighbors perturbation operator to the final results is systematically analyzed. Particle Swarm Optimization and Cuckoo Search are used as metaheuristic techniques. To verify the results, the well-known multidimensional knapsack problem is tackled. A computational comparison is made with the state-of-the-art of metaheuristic techniques that use general mechanisms of binarization. The results show that our improved framework produces consistently better results. In this sense, the contribution of the operator which uses the K-nearest neighbors technique is investigated finding that this operator contributes significantly to the quality of the results.																	1868-8071	1868-808X				SEP	2020	11	9					1951	1970		10.1007/s13042-020-01085-8		FEB 2020											
J								L-BiX: incremental sliding-window aggregation over data streams using linear bidirectional aggregating indexes	KNOWLEDGE AND INFORMATION SYSTEMS										Sliding window; Aggregation; Incremental computation; Streams	KEYWORD SEARCH; COMPUTATION; MODEL	The number of real-time information sources, or so-called streams, has rapidly increased, leading to a greater demand for complex analyses over streams. Although many stream analysis methods exist, aggregation is fundamental to ascertain higher levels of knowledge from raw data. In particular, sliding-window aggregation, where aggregations over sliding windows are repeatedly computed, is useful in many real-life applications. Two stacks is the state-of-the-art method to compute sliding-window aggregations incrementally with a O(1) time complexity. However, its performance seriously degrades as the window size increases due to the high overhead to maintain its index. To address this problem, this paper proposes a linear bidirectional index (L-BiX) that exploits two kinds of partial aggregations. Specifically, forward (old-to-new) and backward (new-to-old) aggregations allow efficient computations in an incremental manner. The proposed algorithm requires the same time complexity as two stacks (O(1)). Our experimental evaluation shows that the throughput of L-BiX can be faster by up to 1.71 times than that of two stacks with a 50% smaller memory footprint.																	0219-1377	0219-3116				AUG	2020	62	8					3107	3131		10.1007/s10115-020-01444-5		FEB 2020											
J								Ontologies for the Virtual Materials Marketplace	KUNSTLICHE INTELLIGENZ										Ontology; Data management; Interoperability; Semantic technology; EMMO		The Virtual Materials Marketplace (VIMMP) project, which develops an open platform for providing and accessing services related to materials modelling, is presented with a focus on its ontology development and data technology aspects. Within VIMMP, a system of marketplace-level ontologies is developed to characterize services, models, and interactions between users; the European Materials and Modelling Ontology is employed as a top-level ontology. The ontologies are used to annotate data that are stored in the ZONTAL Space component of VIMMP and to support the ingest and retrieval of data and metadata at the VIMMP marketplace frontend.																	0933-1875	1610-1987				SEP	2020	34	3			SI		423	428		10.1007/s13218-020-00648-9		FEB 2020											
J								Computationally efficient and secure anonymous authentication scheme for IoT-based mobile pay-TV systems	COMPUTATIONAL INTELLIGENCE										anonymity; authentication; bilinear pairing; elliptic curve cryptography; integrity; mobile pay-TV system	ACCESS-CONTROL; PRIVACY	In the next few years, the mobile pay-TV systems will be very popular due to their extensive applications. Providing security and privacy are the most challenging issues in the secure development of mobile pay-TV systems. To avoid unauthorized access to mobile pay-TV services, it is very important to authenticate the mobile users and the head end system (HES) in an anonymous manner. Even though several authentication schemes were proposed to provide anonymous authentication, the previously proposed schemes are not fit for mobile pay-TV applications due to their high computational complexity. Hence, a computationally efficient anonymous authentication scheme is proposed in this article for secure service provision in mobile pay-TV systems. The proposed authentication scheme can effectively authenticate both the mobile users and the HES with low computational cost in an anonymous manner. In addition, an anonymous batch authentication scheme is also proposed in this article to authenticate a batch of users in the subscription phase to alleviate the authentication burden of the HES. The security analysis section shows that the proposed scheme is more efficient in terms of security and the performance analysis section shows the strength of this article in terms of computational cost, while performing anonymous authentication in mobile pay-TV systems.																	0824-7935	1467-8640				AUG	2020	36	3					994	1009		10.1111/coin.12295		FEB 2020											
J								Security challenges in internet of things: Distributed denial of service attack detection using support vector machine-based expert systems	COMPUTATIONAL INTELLIGENCE										distributed denial of service (DDoS) attack; internet of things (IoT); software-defined network (SDN); support vector machine (SVM)		The rapid development of internet of things (IoT) is to be the next generation of the IoT devices are a simple target for attackers due to the lack of security. Attackers can easily hack the IoT devices that can be used to form botnets, which can be used to launch distributed denial of service (DDoS) attack against networks. Botnets are the most dangerous threat to the security systems. Software-defined networking (SDN) is one of the developing filed, which introduce the capacity of dynamic program to the network. Use the flexibility and multidimensional characteristics of SDN used to prevent DDoS attacks. The DDoS attack is the major attack to the network, which makes the entire network down, so that normal users might not avail the services from the server. In this article, we proposed the DDoS attack detection model based on SDN environment by combining support vector machine classification algorithm is used to collect flow table values in sampling time periods. From the flow table values, the five-tuple characteristic values extracted and based on it the DDoS attack can be detected. Based on the experimental results, we found the average accuracy rate is 96.23% with a normal amount of traffic flow. Proposed research offers a better DDoS detection rate on SDN.																	0824-7935	1467-8640															10.1111/coin.12293		FEB 2020											
J								Observer-Based Interval Type-2 Fuzzy Logic Control for Nonlinear Networked Control Systems with Delays	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Interval type-2 fuzzy logic control; Nonlinear networked control system; Observer-based control	CONTINUOUS BIOREACTOR	One of the popular area of research for the past decade in the academia as well as in the industry is networked control systems (NCS). Variable time delays induced by the network and data packet dropout during transmission of data are among the problems encountered in these type of systems. Researchers proposed and developed various control strategies over the past years to deal with the above-mentioned problem. Fuzzy logic control (FLC) is a widely used technique for dealing with control problems, and the most commonly used one is type-1 FLC. However, the interval type-2 (IT2) is proven to be better at handling uncertainties compared to the type-1 FLC. In this paper, a nonlinear NCS with delays is considered, and an observer-based IT2 FLC is designed in order to improve the control of NCS due to the presence of uncertainties and delays. The developed scheme includes the design of a state feedback controller based on IT2 FLC, and an observer-based IT2 FLC using Lyapunov-Krasovskii theory. To investigate the effectiveness of the proposed scheme, simulations are performed considering various network delays. Firstly, the results of IT2 FLCs are compared with that of type-1 FLCs and improvement is observed. Secondly, the newly developed observer-based IT2 FLC is compared with the IT2 state feedback FLC developed in the literature. Results show faster and more effective response using the newly developed technique.																	1562-2479	2199-3211				MAR	2020	22	2			SI		380	399		10.1007/s40815-020-00799-9		FEB 2020											
J								A Decision-Making Model Under Probabilistic Linguistic Circumstances with Unknown Criteria Weights for Online Customer Reviews	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Online reviews; Sentiment analysis; Probabilistic linguistic term sets; MULTIMOORA	SENTIMENT ANALYSIS; TERM SETS; MULTIMOORA; SELECTION; QUALITY; OPTIMIZATION; INTENTION; BRAND	Online customer reviews (OCRs) provide much information about products or service, but the mass of information increases the difficulty for customers to make decisions. Thus, we establish a multi-criteria decision making (MCDM) model to evaluate products or service. To analyze OCRs, the sentiment analysis (SA) is introduced to identify the sentiment orientation of reviews. Considering that the textual information in OCRs is linguistic information, probabilistic linguistic term sets (PLTSs) are applied to present the results of the SA. A process of extracting probabilistic linguistic information based on SA from OCRs is also presented. Then, for the MCDM problems with unknown criteria weights, we combine the PP (projection pursuit) method and the MULTIMOORA (multiplicative multi-objective optimization by ratio analysis) method, and develop an extended method (named as the PP-MULTIMOORA method). The projection pursuit (PP) method is developed to derive objective criteria weights and the MULTIMOORA method is to derive final rankings of products or service. Finally, we apply the proposed model to a case of evaluating doctors' service quality and further conduct a comparative analysis to illustrate the effectiveness of our work.																	1562-2479	2199-3211				APR	2020	22	3					777	789		10.1007/s40815-020-00812-1		FEB 2020											
J								Multiple Attribute Group Decision Making Method Based on Intuitionistic Fuzzy Einstein Interactive Operations	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intuitionistic fuzzy numbers; Einstein interactive operational rules; Weighted averaging operator; Power average operator; Multiple attribute group decision making	AGGREGATION OPERATORS; DEMATEL METHOD; SETS; ENTROPY; NUMBERS; MODEL	The intuitionistic fuzzy numbers (IFNs) have been extensively studied in recent years. However, the traditional operational rules (ORs) of the IFNs still have some drawbacks in solving the practical decision-making problems. Einstein t-conorm and t-norm (TAT) are an important and typical class of the TAT, but the ORs for the IFNs based on the Einstein TAT (ETAT) cannot consider the interaction between the membership degree (MD) and the non-membership degree (N-MD), they may get the unreasonable evaluation results in some realistic decision-making situations. So this paper proposes some new Einstein interactive ORs for the IFNs, then, it further presents the intuitionistic fuzzy Einstein interactive weighted averaging (IFEIWA) operator to overcome above existing drawbacks, and some properties of this operator are proved. Simultaneously, in order to eliminate the effects of the existing biases of some decision experts in the process of evaluating attributes, this paper proposes the intuitionistic fuzzy Einstein interactive power averaging (IFEIPA) operator and the intuitionistic fuzzy Einstein interactive weighted power averaging (IFEIWPA) operator based on the revised power weighted averaging operator, and then gives their some desirable properties. Further, by using the IFEIPA operator and the IFEIWPA operator, this paper presents a novel method for the multi-attribute group decision making (MAGDM) problems to solve practical decision-making problems. Lastly, this paper uses some actual application examples to verify the applicability and validity of the proposed MAGDM method, and then demonstrates the superiority of novel method by detailed comparison analysis with other typical methods.																	1562-2479	2199-3211				APR	2020	22	3					790	809		10.1007/s40815-020-00809-w		FEB 2020											
J								A Decoupled Method for Credibility-Based Design Optimization with Fuzzy Variables	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Credibility-based design optimization; Fuzzy variables; Decoupled method; Inverse most credible point; Inverse credibility analysis	RELIABILITY ASSESSMENT METHOD; SINGLE-LOOP METHOD; SEQUENTIAL OPTIMIZATION; FAILURE CREDIBILITY; ROBUST DESIGN; SIMULATION; INDEX	Fuzzy uncertainty (FU) exists widely in engineering applications, but there lack design optimization methods under FU, thus a credibility-based design optimization (CBDO) is focused to obtain the safety design under FU in this paper. Firstly, the concepts of credibility index and most credible point (MCP) are presented to measure the safety degree under FU, where the credibility index and the MCP, respectively, show similar properties as the reliability index and the most probable point under random uncertainty. Secondly, the inverse MCP (IMCP) is defined with respect to the required credibility, and the detailed method is established for searching IMCP, on which the performance measure approach (PMA) can be combined to solve the CBDO. Since the PMA combined with the IMCP includes a time-consuming double-loop strategy, the sequential optimization and credibility assessment (SOCA) is proposed to decouple the double-loop strategy thirdly. In the SOCA, a shifting vector constructed by the IMCP is used to transform the credibility constraint into an equivalent deterministic one, on which the double-loop strategy can be avoided to reduce the computational cost for solving the CBDO. One numerical example and two engineering examples fully illustrate the efficiency and accuracy of the SOCA.																	1562-2479	2199-3211				APR	2020	22	3					844	858		10.1007/s40815-020-00813-0		FEB 2020											
J								Transportation of water-based trapped bolus of SWCNTs and MWCNTs with entropy optimization in a non-uniform channel	NEURAL COMPUTING & APPLICATIONS										Peristalsis flow; Viscosity (temperature dependent); Radiative heat flux; Effective heat transfer rate (i; e; Nusselt number); Trapped bolus; Entropy generation	MIXED CONVECTION; HEAT-TRANSFER; GENERATION; FLOW; NANOFLUID; FLUID	Here, we communicate the peristalsis of single- and multi-walled CNTs through nonlinear porous, non-uniform propagating channel boundaries. Non-uniform channel boundaries are of asymmetric characteristics. Flow equations are modeled by taking variable viscosity, linear and nonlinear porous medium (i.e., Darcy and Darcy-Forchheimer), nonlinear thermal radiation, mixed convection, heat generation and absorption aspects. Convective heat transfer aspects are at the convectively heated surface. The entropy generation rate is modeled via the second law of thermodynamics. Modeled equations are simplified with the help of long wavelength assumption. Dimensionless system of equations with respective boundary conditions is solved numerically via built-in shooting algorithm in Mathematica 8 software. Further, these numerical results are directly received in the form of curves. Such curves are made for velocity, temperature, pressure gradient, trapping, entropy rate and Bejan number. Heat transfer rate at lower and upper walls is achieved through bar charts.																	0941-0643	1433-3058				SEP	2020	32	17					13565	13576		10.1007/s00521-020-04766-1		FEB 2020											
J								Intelligent tutoring system using expert knowledge and Kohonen maps with automated training	NEURAL COMPUTING & APPLICATIONS										Intelligent tutoring systems; Hybrid ITS; Didactic transposition of contents; Artificial neural networks; Self-organizing maps	PERSONALIZATION	This paper presents an intelligent tutoring system (ITS) model that is capable of driving the didactic transposition of contents. Initially, the tutoring system reactions bases its behavior on rules defined by an expert teacher; after this, a neural network that learns from the student's behavior when they are studying adjusts these rules. This way, the neural network improves the teacher's rules and, consequently, defines a learning strategy that is more adaptive and reactive to the student's profile. Thus, it is possible to offer the student a personalized and individualized education form. The model is able to guide the student throughout the didactic transposition of contents, aiding the consolidation of desired competencies established on educational propositions. This work shows the development process of the ITS, including the expert guidance system and the hybrid system, which improves the expert rules from SOM neural network use. The obtained results indicate that the application of hybrid technology in ITSs is feasible because, for defining the teaching strategies, it incorporates the teacher's knowledge and by neural network use, it assimilates the students' learning process behavior. The results show that proposed model has great agreement between the actions of the "ITS" and the students' actions. The model showed satisfactory performance when compared to other systems proposed in the literature that use connectionist approach in its conception.																	0941-0643	1433-3058				SEP	2020	32	17					13577	13589		10.1007/s00521-020-04767-0		FEB 2020											
J								Improved butterfly optimisation algorithm based on guiding weight and population restart	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Butterfly optimisation algorithm; meta-heuristic; swarm intelligence algorithm; guiding weight; population restart	SALP SWARM ALGORITHM; STRATEGY	Butterfly Optimisation Algorithm (BOA) is a kind of meta-heuristic swarm intelligence algorithm based on butterfly foraging strategy, but it still needs to be improved in the aspects of convergence speed and accuracy when solving with high-dimensional optimisation problems. In this paper, an improved butterfly optimisation algorithm is proposed, in which guiding weight and population restart strategy are applied to the original algorithm. By adding guiding weight to the global search equation, the convergence speed and accuracy of the algorithm are improved, and the possibility of jumping out of the local optimal solution is increased by the population restart strategy. In order to verify the performance of the proposed algorithm, 24 benchmark functions commonly used for optimisation algorithm experiments are applied in this paper, including 12 unimodal functions and 12 multimodal functions. Experimental results show that the proposed algorithm improves the convergence speed, accuracy and the ability to jump out of the local optimal solution.																	0952-813X	1362-3079															10.1080/0952813X.2020.1725651		FEB 2020											
J								Vertex rough graphs	COMPLEX & INTELLIGENT SYSTEMS										Rough set; Edge rough graph; Vertex rough graph; Rough membership function		This article introduces the notion of vertex rough graph and discusses certain basic graph theoretic definitions and examples. Adjacency of vertices is used to create a matrix corresponding to a vertex rough graph. Also, the membership function of a vertex rough graph is introduced with the help of Pawlak's Rough set theory, and using this certain results are obtained. The concepts of rough precision and rough similarity degree are extended to vertex rough graphs.																	2199-4536	2198-6053				JUL	2020	6	2					347	353		10.1007/s40747-020-00133-8		FEB 2020											
J								An optimal mobile data gathering in small scale WSN by power saving adaptive clustering techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Mobile data gathering; Fuzzy; Cluster head selection; Route distance; Heterogeneous network; Power; Clustering		Nature consists of enormous and various physical and phenomenon, like lightweight, temperature, motion, seismol waves, and plenty of others. For observation and cashing in on the environment it's necessary to collect the knowledge concerning the phenomenon. Wireless device networks facilitate U.S. in sensing the environment and in obtaining info concerning the natural discernible occurrences. It needs communication protocols to diminish the power consumption. In wireless sensor networks, power is the key one among the foremost necessary resources since every node gathers processes and passes on knowledge to its base station. In general, most of the works in sensor networks are done using static nodes and single base station. Recent researches use mobile knowledge gathering strategies and are planned to prolong the operation time of device networks. One or additional mobile collectors are wont to gather detected knowledge from device nodes at short transmission ranges. This paper presents a novel algorithm for cluster head selection and provides best visiting points and knowledge gathering path for a mobile sink among clusters. With shaping associate best cluster and knowledge gathering path, this methodology improves the information assortment performance yet because the network life extension of device in small scale networks. The performance has been evaluated using LTE and WiFi networks. Also, quality measures for each network have been computed and presented.																	1868-5137	1868-5145															10.1007/s12652-020-01757-x		FEB 2020											
J								Improving monarch butterfly optimization through simulated annealing strategy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Monarch butterfly optimization; Simulated annealing; Benchmark problems; Neighborhood search	TABU SEARCH; ALGORITHM	Currently, a novel of meta-heuristic algorithm called monarch butterfly optimization (MBO) is presented for solving machine learning and continuous optimization problems. It has been proved experimentally that MBO is superior to artificial bee colony algorithm (ABC), ant colony optimization algorithm (ACO), Biogeography-based optimization (BBO), differential evolution algorithm (DE) and simple genetic algorithm (SGA) algorithms on most test functions. This paper presents a new version of MBO with simulated annealing (SA) strategy called SAMBO. The SA strategy is put in the migration operator and butterfly adjusting operator. So the newly proposed algorithm has two features: One is that the algorithm accepts all the butterfly individuals whose fitness are better than their parents. The other is that the algorithm randomly selects some individuals which are worse than their parents to disturbance the convergence of algorithm. In this way, the SAMBO algorithm can escape from local optima. Finally, the experiments are carried on 14 continuous nonlinear functions, and results show that SAMBO method exceeds the MBO algorithm on most test functions.																	1868-5137	1868-5145															10.1007/s12652-020-01702-y		FEB 2020											
J								Wajsberg algebras arising from binary block codes	SOFT COMPUTING										BCK bounded commutative algebras; MV-algebras; Wajsberg algebras; Block codes		In this paper, we presented some connections between BCK commutative bounded algebras, MV-algebras, Wajsberg algebras and binary block codes. Using connections between these three algebras, we will associate to each of them a binary block code and, in some circumstances, we will prove that the converse is also true.																	1432-7643	1433-7479				APR	2020	24	8			SI		6047	6058		10.1007/s00500-019-04653-5		FEB 2020											
J								The zero-divisor graphs of MV-algebras	SOFT COMPUTING										MV-algebra; MV-semiring; Zero-divisor; Zero-divisor graph; Diameter		In this paper, we will introduce and study the zero-divisor graphs of MV-algebras. Let (A,circle plus,*,0) be an MV-algebra, and (A,circle dot,0) be the associated semigroup. Define the zero-divisor graph Gamma(A) of A to be the simple graph with vertices V(Gamma(A))={x is an element of A|(there exists y is an element of A\{0})x circle dot y=0}, and edges E(Gamma(A))={the edge with endsxandy|(x not equal y,x,y is an element of A)x circle dot y=0}. We show that Gamma(A) is connected with diam(Gamma(A))<= 3, where diam(Gamma(A)) denotes the diameter of Gamma(A). Moreover, we characterize A with diam(Gamma(A)) equal to 0, 1, 2 or 3. Finally, using the zero-divisor graph, we classify all MV-algebras of cardinality up to seven.																	1432-7643	1433-7479				APR	2020	24	8			SI		6059	6068		10.1007/s00500-020-04738-6		FEB 2020											
J								Quasicomplemented residuated lattices	SOFT COMPUTING										Quasi complemented residuated lattice; Disjunctive residuated lattice; alpha-Filter	ALPHA-FILTERS; IDEALS	In this paper, the class of quasicomplemented residuated lattices is introduced and investigated, as a subclass of residuated lattices in which any prime filter not containing any dense element is a minimal prime filter. The notion of a disjunctive residuated lattice is introduced, and it is observed that a residuated lattice is Boolean if and only if it is disjunctive and quasicomplemented. Finally, some characterizations for quasicomplemented residuated lattices are given by means of the new notion of alpha-filters.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6591	6602		10.1007/s00500-020-04778-y		FEB 2020											
J								Residents' perceptions of smart energy metres	EXPERT SYSTEMS										intention to use; perceived risks; smart electricity monitor; smart energy controller; smart grids; smart metres	ACCEPTANCE; DEPLOYMENT; BUILDINGS; ADOPTION	Smart metres are a form of expert system with performance features beyond energy-consumption record keeping, to include monitoring, analysing, and estimating metre readings. Although smart metres have great capabilities, this technology is still in its infancy in many developing countries, and little is known about the kinds of risks associated with smart metres from residents' perspectives. This research therefore aims to fill this gap by examining the influence of four different types of perceived risk on residents' intentions to use smart metres in Jordan. By following a quantitative approach, 242 survey responses were tested by using structural equation modelling-partial least squares. The statistical results indicated that perceived security and technical risks have a significant and negative impact on residents' intentions to use smart metres. However, perceived privacy and health risks, surprisingly, were found to have no significant negative influence on intention to use. Theoretical and practical implications are indicated, and directions of future research are subsequently specified.																	0266-4720	1468-0394															10.1111/exsy.12500		FEB 2020											
J								An algorithm to compare two-dimensional footwear outsole images using maximum cliques and speeded-up robust feature	STATISTICAL ANALYSIS AND DATA MINING										forensic science; image comparison; machine learning; maximum clique; SURF	RETRIEVAL	Footwear examiners are tasked with comparing an outsole impression (Q) left at a crime scene with an impression (K) from a database or from the suspect's shoe. We propose a method for comparing two shoe outsole impressions that relies on robust features (speeded-up robust feature; SURF) on each impression and aligns them using a maximum clique (MC). After alignment, an algorithm we denote MC-COMP is used to extract additional features that are then combined into a univariate similarity score using a random forest (RF). We use a database of shoe outsole impressions that includes images from two models of athletic shoes that were purchased new and then worn by study participants for about 6 months. The shoes share class characteristics such as outsole pattern and size, and thus the comparison is challenging. We find that the RF implemented on SURF outperforms other methods recently proposed in the literature in terms of classification precision. In more realistic scenarios where crime scene impressions may be degraded and smudged, the algorithm we propose-denoted MC-COMP-SURF-shows the best classification performance by detecting unique features better than other methods. The algorithm can be implemented with the R-package shoeprintr.																	1932-1864	1932-1872				APR	2020	13	2					188	199		10.1002/sam.11449		FEB 2020											
J								Examples of Gibsonian Affordances in Legged Robotics Research Using an Empirical, Generative Framework	FRONTIERS IN NEUROROBOTICS										robot; affordance; legged; generative; reactive	CENTRAL PATTERN GENERATORS; ADAPTIVE-BEHAVIOR; LOCOMOTION; INTELLIGENCE; WALKING; MODELS; SYSTEM	Evidence from empirical literature suggests that explainable complex behaviors can be built from structured compositions of explainable component behaviors with known properties. Such component behaviors can be built to directly perceive and exploit affordances. Using six examples of recent research in legged robot locomotion, we suggest that robots can be programmed to effectively exploit affordances without developing explicit internal models of them. We use a generative framework to discuss the examples, because it helps us to separate-and thus clarify the relationship between-description of affordance exploitation from description of the internal representations used by the robot in that exploitation. Under this framework, details of the architecture and environment are related to the emergent behavior of the system via a generative explanation. For example, the specific method of information processing a robot uses might be related to the affordance the robot is designed to exploit via a formal analysis of its control policy. By considering the mutuality of the agent-environment system during robot behavior design, roboticists can thus develop robust architectures which implicitly exploit affordances. The manner of this exploitation is made explicit by a well constructed generative explanation.																	1662-5218					FEB 20	2020	14								12	10.3389/fnbot.2020.00012													
J								A Credibilistic Fuzzy DEA Approach for Portfolio Efficiency Evaluation and Rebalancing Toward Benchmark Portfolios Using Positive and Negative Returns	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Portfolio selection; Portfolio efficiency evaluation; VaR; CVaR; Data envelopment analysis; Range directional model; Credibilistic theory; Negative returns	DATA ENVELOPMENT ANALYSIS; PERFORMANCE EVALUATION; SELECTION MODEL; MUTUAL FUNDS; OPTIMIZATION; OUTPUTS; RISK	This paper proposes a data envelopment analysis (DEA)-based portfolio efficiency evaluation approach integrated with a rebalancing method to help investors acquire efficient portfolios. Two fuzzy portfolio selection models with value at risk (VaR) and conditional value at risk (CVaR) as objectives are proposed under the credibilistic framework. The models are constrained by realistic constraints of short selling/no short selling, capital budget, bounds on investment in an asset, and minimum return desired by the investor. These models are used to compute the benchmark portfolios, which constitute the portfolio efficient frontier. Furthermore, random sample portfolios are generated individually for each model in compliance with their constraints. These random sample portfolios are evaluated in terms of their relative efficiency with risk (VaR or CVaR) as an input and return as an output using DEA. Bearing in mind the volatile nature of the investment market, negative returns are also considered for portfolio efficiency evaluation using the range directional model. Moreover, an efficiency frontier improvement algorithm is used to rebalance the inefficient random portfolios to make them efficient. The proposed approach provides an alternative to the investors to acquire benchmark portfolios using the traditional portfolio selection models. A detailed numerical illustration and an out of sample analysis with the Nifty 50 index from the National Stock Exchange, India, are presented to substantiate the proposed approach.																	1562-2479	2199-3211				APR	2020	22	3					824	843		10.1007/s40815-020-00801-4		FEB 2020											
J								Obtaining Efficient Solutions of Interval Multi-objective Linear Programming Problems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Efficient solution; Interval multi-objective linear programming; Uncertainty; epsilon-constraint; Lexicographic; Weighted sum	MINIMAX REGRET SOLUTION; OPTIMIZATION; COEFFICIENTS	In this paper, we consider interval multi-objective linear programming (IMOLP) models which are used to deal with uncertainties of real-world problems. So far, a variety of approaches for obtaining efficient solutions (ESs) of these problems have been developed. In this paper, we propose a new and two generalized methods. In the new method, converting IMOLP into an interval linear programming (ILP) and then obtaining its optimal solutions (OSs), ESs of the IMOLP are determined. This method has several advantages: (i) This method is the only method which obtains a solution box for IMOLP models. (ii) The solving process is not time consuming. (iii) The number of ESs is higher than for other methods. (V) The method is applicable for large-scale problems. Also, we generalize the epsilon-constraint and lexicographic methods which are used for obtaining ESs of the multi-objective linear programming (MOLP) models which do not have any problems such as lengthy and time-consuming and are applicable for large-scale problems. Some examples were solved to show the efficiency of the proposed methods. Finally, by the proposed method, we solve the IMOLP model corresponding to the problem of the facilities and non-return funds in a bank.																	1562-2479	2199-3211				APR	2020	22	3					873	890		10.1007/s40815-020-00800-5		FEB 2020											
J								Power Load Forecast Based on Fuzzy BP Neural Networks with Dynamical Estimation of Weights	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Power load forecast; Fuzzy BP neural networks; Cubature Kalman filter; Sage-Husa filtering; Observable degree; Variational Bayesian	ENERGY-MANAGEMENT; KALMAN FILTER; ALGORITHM; SYSTEMS; STORAGE; MODEL; PREDICTION	The short-term power load forecast is deeply studied by integrating fuzzy BP neural networks and composite adaptive filtering in this paper. Due to the difficulty on accurate modeling of complex factors in smart grid, the fuzzy technology is introduced to deal with the uncertain factors. Meanwhile, data-driven method, such as neural networks, is used as the basic frame of the power load forecast by combing the fuzzy technology. Thereby, to design a highly effective training scheme becomes the main problem on the weights of the BP-NNs. To realize better weights training, the combined filtering technology is fully adopted by fusing adaptive filtering methods based on observable degree (OD) analysis, Sage-Husa adaptive technology, and variational Bayesian method. Thereby, a novel power load forecast algorithm is proposed based on the fuzzy BP-NNs and the combined adaptive cubature Kalman filter. Experiment based on practical power load data is presented to validate the effectiveness of the proposed short-term power load algorithm.																	1562-2479	2199-3211				APR	2020	22	3					956	969		10.1007/s40815-019-00796-7		FEB 2020											
J								Fuzzy Interest Rate Term Structure Equation	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Zero coupon bond; Fuzzy process; Liu process; Term structure equation; Affine term structure		Interest rate is an important part of option pricing model, which attracts many scholars' attention. In this paper, the fuzzy interest rate model is established under the framework of credibility theory, and the fuzzy term structure equations of zero coupon bonds and coupon bonds are derived. In order to solve the fuzzy term structure equations, the conditions of bond price possessing affine term structure in fuzzy environment is verified. As an illustration, based on a special fuzzy interest rate model, the pricing formula of zero coupon bond is obtained by using the fuzzy term structure equation and affine term structure.																	1562-2479	2199-3211				APR	2020	22	3					999	1006		10.1007/s40815-020-00810-3		FEB 2020											
J								Neural Fuzzy Control of the Indoor Air Quality Onboard a RO-RO Ship Garage	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Indoor Air Quality; Carbon monoxide; Artificial neural network; Fuzzy controller; Neuro-fuzzy controller; Energy saving	CONTROL STRATEGIES; CARBON-MONOXIDE; CLIMATE CONTROL; VENTILATION; BUILDINGS; NETWORKS; COMFORT	Air quality control is necessary to improve the environmental condition in particular Indoor Air Quality (IAQ), which has a direct impact on a living organism. In marine application, the International Maritime Organization (IMO) made standards to determine the gas emission limits and specify the air circulation in IAQ in addition to employing in energy-efficient and energy management system which leads to reduce emission and enhance the environmental condition by efficient and economical way. Applying advanced energy management policies and control could be achieved by energy consumption. In this paper, an intelligent neuro-fuzzy controller has been designed to model and control carbon monoxide (CO) concentration for a real case study of a high-speed craft passenger ship with a vehicle garage onboard for a liner between Egypt and Saudi Arabia ports. The relation between emitted CO and the number of cars and their positions has been modelled using artificial neural network (ANN). The ANN model has been built and validated based on real measurements of CO at different ventilation conditions of the case study. Different fuzzy controllers, fixed and adaptive, are designed to control CO during loading and unloading states. Scaling factors of fuzzy controller are adapted using two different ways namely Supervisor Fuzzy Controller (SFC) and Particle Swamp Optimization (PSO). Simulation results have analysed the proposed control system at different conditions. The obtained outcomes manifest the fact that the controller tends to work robustly and efficiently to maintain CO at the permissible allowed range.																	1562-2479	2199-3211				APR	2020	22	3					1020	1035		10.1007/s40815-020-00804-1		FEB 2020											
J								Deep neural network based Rider-Cuckoo Search Algorithm for plant disease detection	ARTIFICIAL INTELLIGENCE REVIEW										Plant disease; Piecewise fuzzy C-means clustering; Deep belief network; Classification; Texture features	LEAF DISEASES; IDENTIFICATION; OPTIMIZATION; CLASSIFICATION; SEGMENTATION; RECOGNITION	Agriculture is the main source of wealth, and its contribution is essential to humans. However, several obstacles faced by the farmers are due to different kinds of plant diseases. The determination and anticipation of plant diseases are the major concerns and should be considered for maximizing productivity. This paper proposes an effective image processing method for plant disease identification. In this research, the input image is subjected to the pre-processing phase for removing the noise and artifacts present in the image. After obtaining the pre-processed image, it is subjected to the segmentation phase for obtaining the segments using piecewise fuzzy C-means clustering (piFCM). Each segment undergoes a feature extraction phase in which the texture features are extracted, which involves information gain, histogram of oriented gradients (HOG), and entropy. The obtained texture features are subjected to the classification phase, which uses the deep belief network (DBN). Here, the proposed Rider-CSA is employed for training the DBN. The proposed Rider-CSA is designed by integrating the rider optimization algorithm (ROA) and Cuckoo Search (CS). The experimental results proved that the proposed Rider-CSA-DBN outperformed other existing methods with maximal accuracy of 0.877, sensitivity of 0.862, and the specificity of 0.877, respectively.																	0269-2821	1573-7462				OCT	2020	53	7					4993	5018		10.1007/s10462-020-09813-w		FEB 2020											
J								End-to-end multitask Siamese network with residual hierarchical attention for real-time object tracking	APPLIED INTELLIGENCE										Real-time object tracking; Deep networks; Attention mechanism; Correlation filters		Object tracking with deep networks has recently achieved substantial improvement in terms of tracking performance. In this paper, we propose a multitask Siamese neural network that uses a residual hierarchical attention mechanism to achieve high-performance object tracking. This network is trained offline in an end-to-end manner, and it is capable of real-time tracking. To produce more efficient and generative attention-aware features, we propose residual hierarchical attention learning using residual skip connections in the attention module to receive hierarchical attention. Moreover, we formulate a multitask correlation filter layer to exploit the missing link between context awareness and regression target adaptation, and we insert this differentiable layer into a neural network to improve the discriminatory capability of the network. The results of experimental analyses conducted on the OTB, VOT and TColor-128 datasets, which contain various tracking scenarios, demonstrate the efficiency of our proposed real-time object-tracking network.																	0924-669X	1573-7497				JUN	2020	50	6					1908	1921		10.1007/s10489-019-01605-2		FEB 2020											
J								Modified dynamic fuzzy c-means clustering algorithm - Application in dynamic customer segmentation	APPLIED INTELLIGENCE										Dynamic fuzzy c-means; Dynamic customer segmentation; Xie-Beni index; Customer segmentation; Modified dynamic fuzzy c-means	RELATIONSHIP MANAGEMENT; CLASSIFICATION; MIGRATION; KNOWLEDGE; FRAMEWORK; PATTERNS; NETWORK; MODEL; INDEX	The dynamic customer segmentation (DCS) is a useful tool for managers in implementing marketing strategies by observing dynamic changes that are happening in the customer segments over time. The Crespo's dynamic fuzzy c-means (CDFCM) is one of the clustering algorithms introduced in the literature for DCS. We have suggested modifications to the CDFCM algorithm owing to certain shortcomings found in it, resulting in the modified dynamic fuzzy c-means (MDFCM) algorithm. To show the performance of the MDFCM algorithm, extensive experiments were carried out in comparison with the CDFCM algorithm using a retail supermarket dataset with eleven new data updates. To validate the results of the MDFCM algorithm, the fuzzy clustering evaluation measures such as Xie-Beni (XB) index, within sum of squared error (WSSE), root mean squared error (RMSE), Kwon index, and Tang index are utilized. The experimental results show that MDFCM is the most effective clustering algorithm for DCS, and the results are tested statistically to show its significance. The MDFCM algorithm is further compared with another successful algorithm available in the literature called Fathabadi's dynamic fuzzy c-means (FDFCM). To show the usefulness of the MDFCM algorithm, a DCS framework is proposed and it has been demonstrated through a case study.																	0924-669X	1573-7497				JUN	2020	50	6					1922	1942		10.1007/s10489-019-01626-x		FEB 2020											
J								Clustering association rules to build beliefs and discover unexpected patterns	APPLIED INTELLIGENCE										Unexpected pattern mining; Pattern clustering; Belief system; Association rule mining	UNDIRECTED DISCOVERY; EXCEPTION RULES; INTERESTINGNESS	Interesting pattern discovery is an important topic in data mining research. Many different definitions have been proposed to describe whether a pattern is interesting. Among these many definitions, unexpectedness has shown to be a highly promising measure. Mining unexpected patterns allows one to identify a failing in prior knowledge and may suggest an aspect of the data that deserves further investigation. Unexpected patterns are typically mined using belief-driven methods, but these require an established belief system. Prior studies have manually built their own partial belief systems to apply their method, but these remain laborious to create. In this study, we propose a novel approach that is able to automatically detect beliefs from data, which can in turn be used to reveal unexpected patterns. Central to this approach is a clustering-based method in which clusters represent beliefs and outliers are potential unexpected patterns. We also propose a pattern representation that captures the semantic relation between patterns rather than the lexical difference. An experimental evaluation on different datasets and a comparison to some other methods demonstrate the effectiveness of the proposed method, as well as the relevance of the discovered patterns.																	0924-669X	1573-7497				JUN	2020	50	6					1943	1954		10.1007/s10489-020-01651-1		FEB 2020											
J								Generative Attention Learning: a "GenerAL" framework for high-performance multi-fingered grasping in clutter	AUTONOMOUS ROBOTS										Grasping; Multi-fingered hands; Deep reinforcement learning; Visual attention		Generative Attention Learning (GenerAL) is a framework for high-DOF multi-fingered grasping that is not only robust to dense clutter and novel objects but also effective with a variety of different parallel-jaw and multi-fingered robot hands. This framework introduces a novel attention mechanism that substantially improves the grasp success rate in clutter. Its generative nature allows the learning of full-DOF grasps with flexible end-effector positions and orientations, as well as all finger joint angles of the hand. Trained purely in simulation, this framework skillfully closes the sim-to-real gap. To close the visual sim-to-real gap, this framework uses a single depth image as input. To close the dynamics sim-to-real gap, this framework circumvents continuous motor control with a direct mapping from pixel to Cartesian space inferred from the same depth image. Finally, this framework demonstrates inter-robot generality by achieving over 92% real-world grasp success rates in cluttered scenes with novel objects using two multi-fingered robotic hand-arm systems with different degrees of freedom.																	0929-5593	1573-7527				JUL	2020	44	6					971	990		10.1007/s10514-020-09907-y		FEB 2020											
J								Tensorized Multi-view Subspace Representation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multi-view representation learning; Subspace clustering; Low-rank tensor; Constraint matrix	MATRIX FACTORIZATION; SPARSE; ALGORITHM	Self-representation based subspace learning has shown its effectiveness in many applications. In this paper, we promote the traditional subspace representation learning by simultaneously taking advantages of multiple views and prior constraint. Accordingly, we establish a novel algorithm termed as Tensorized Multi-view Subspace Representation Learning. To exploit different views, the subspace representation matrices of different views are regarded as a low-rank tensor, which effectively models the high-order correlations of multi-view data. To incorporate prior information, a constraint matrix is devised to guide the subspace representation learning within a unified framework. The subspace representation tensor equipped with a low-rank constraint models elegantly the complementary information among different views, reduces redundancy of subspace representations, and then improves the accuracy of subsequent tasks. We formulate the model with a tensor nuclear norm minimization problem constrained with l(2,1)-norm and linear equalities. The minimization problem is efficiently solved by using an Augmented Lagrangian Alternating Direction Minimization method. Extensive experimental results on diverse multi-view datasets demonstrate the effectiveness of our algorithm.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2344	2361		10.1007/s11263-020-01307-0		FEB 2020											
J								Application of decision making model for leakage reduction to economic project in water distribution systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Decision making model; Water distribution systems; Benefit cost analysis; Life cycle cost		Water distribution systems in Korea that are responsible for maintaining a stable supply of tap water and ensuring water quality are experiencing many problems, such as pipe leakage, corrosion, and aging of pipes. There have been many recommendations for improved pipe network assessment, diagnosis, design, operation, maintenance, etc. However, improvements to the water pipe network have been limited. Additionally, leaks in the tap water supply have caused deterioration of waterworks facilities. To solve this problem, water distribution maintenance projects have focused on rehabilitation and replacement of old water pipes using pipe installation age as a factor. Evaluation methods using hydraulic pressure, such as the demand energy ratio, did not previously exist in Korean water distribution systems. In this study, we have proposed a decision-making model that considers the economic efficiency of waterworks improvement projects, using a non-revenue water (NRW) multiple regression equation. Rather than placing priority of water distribution pipe rehabilitation and replacement projects on those systems with the oldest pipe installations, additionally we have proposed using cost benefit (C/B) and life cycle cost (LCC) analysis, which can be evaluated and qualified economically through the demand energy ratio, to set priority. As project priority changes are made according to the proposed decision-making model, it is also important optimize parameters according to specific waterworks facilities operations and management.																	1868-5137	1868-5145															10.1007/s12652-019-01634-2		FEB 2020											
J								Realistic ranking of exclusive supplier strategies based on the evaluation of real value of the risks in the supply chain	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Risk assessment; Supply chain; Interdependency effects; Real value of risk; Three-dimensional integrated mean; Exclusive supplier	MULTIOBJECTIVE APPROACH; DECISION-MAKING; DEMAND; DISRUPTION; MITIGATION; VISIBILITY; SELECTION; MODEL	Risks play an important role in supply chain vulnerability, performance maintenance, and competitive advantage. Through adopting strategies by the exclusive supplier, the potential risks in the supply chain become actual. The risk interdependencies are not involved in the effectiveness of selecting the appropriate strategy. Since a risk may be influenced by another risk in a desirable or undesirable way, estimation of the real risk value is necessary. In this paper, first, the real weight of the actual risks for each strategy is specified through a new approach. Then, considering risk interdependency effects, the amount of increase or decrease in a risk value is determined by proposing an improved DEMATEL method. The real value of each risk is a combination of factors such as real weight, actual value, and sensitivity to other risk effects. Finally, the value of objective functions for each strategy is determined and all strategies will be re-ranked by the proposed approach (three-dimensional integrated mean method). However, the proposed method is used for ranking of Exclusive Supplier Strategies or decision in this paper, but it can be used for many decision-making procedures.																	1868-5137	1868-5145															10.1007/s12652-020-01725-5		FEB 2020											
J								Kernalized average entropy and density based spatial clustering with noise	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Spatial; Candidate region detection; Kernalized; Average entropy; Density-based spatial clustering	IMAGES	Image clustering is one of the key technologies for image processing. Most image clustering methods based on density algorithms encountered with challenges including robust salient region extraction, cluster centre identification and noise. To address these issues, different density-based clustering methods were used. To accommodate this constraint, this paper proposes a method named Kernalized Average Entropy and Density-based Spatial Clustering (KAE-DSC). The KAE-DSC method efficiently extracts the robust salient region for texture and image database using Spatial Candidate Region Detection. Finally, Spatial Density-based Clustering with Noise is used to differentiate between core, non-core and noisy region for grouping similar region. By identifying the three different regions, efficient prediction is said to be made for the given texture and medical images. KAE-DSC method is compared with several other methods based on several evaluation indexes to testify that the proposed KAE-DSC is outperformed. Abundant experimental results proved that KAE-DSC is robust to parameters, which can automatically determine the number of clustering and improve the accuracy of clustering with minimum time complexity.																	1868-5137	1868-5145															10.1007/s12652-020-01741-5		FEB 2020											
J								Real-time maximally stable homogeneous regions	JOURNAL OF REAL-TIME IMAGE PROCESSING										Quasi-flat zone hierarchy; Component-trees; Multi-valued component-trees; Image segmentation; Object tracking	SCALE-SPACE REPRESENTATION; IMAGE; EXTRACTION; OPERATORS; TREE	In this paper, we present the concept of maximally stable homogeneous regions (MSHR). MSHR are conceptually very similar to maximally stable extremal regions but can be segmented in images with an arbitrary number of channels. The computation of the presented MSHR relies on the construction of a quasi-flat zone hierarchy. We present a fast algorithm for computing the hierarchy that overcomes the runtime restrictions of existing approaches. The proposed algorithm can construct the quasi-flat zone hierarchy efficiently in real time, scales linearly in the number of pixels and, in practice, sub-linearly in the number of channels. In the experiments, we display how MSHR can be used to improve the results of optical character recognition systems and to perform 3D object segmentation. We further demonstrate the universality and speed of the proposed algorithm for three example applications: image segmentation, object tracking, and image filtering.																	1861-8200	1861-8219															10.1007/s11554-020-00951-6		FEB 2020											
J								A multimodal auditory equal-loudness comparison of air and bone conducted sounds	JOURNAL ON MULTIMODAL USER INTERFACES										Auditory conduction equivalency; Bone conduction; Equal-loudness curves	HEARING; INTELLIGIBILITY; THRESHOLDS; VIBRATOR	The term 'multimodal' typically refers to the combination of two or more sensory modalities; however, through the advancement of technology, modality variations within specific sensory systems are being discovered and compared in regards to physiological perception and response. The ongoing evaluation of air vs bone conduction auditory perception modalities is one such comparison. Despite an increased awareness of the potential benefits of utilizing bone conduction pathways, a complete understanding of the human auditory system, more specifically, the relationship between air conducted and bone conducted sound remains a critical deficiency hindering the development of advanced multimodal auditory displays. Conduction equivalency ratios (CERs), which were defined as the difference in sound intensity levels (in dB) between equally loud signals transmitted in air conduction (AC) (sound field) and bone conduction (BC) modes provided a link between these two modes of hearing by determining the relationship between spectral content of AC and BC sound. The current report aims to describe, in depth, the establishment of such CERs at three BC transducer contact locations on a listener's head over a range of audible frequencies presented over three signal intensities within a controlled free-field listening environment. Results indicated the AC-BC relationship is not unique and depends on sound intensity, frequency, and BC transducer location. In addition, in terms of head sensitivity, results support similar findings which indicate that the Mastoid and Condyle locations can be considered interchangeable in terms of their frequency-related sensitivity while the Forehead was found to be considerably less sensitive compared to the other locations.																	1783-7677	1783-8738				JUN	2020	14	2			SI		199	206		10.1007/s12193-020-00320-4		FEB 2020											
J								A novel improved symbiotic organisms search algorithm	COMPUTATIONAL INTELLIGENCE										CEC2015; function optimization; self-adaption; symbiosis organisms search algorithm; three-way mutualism phase; weighted reflection	DIFFERENTIAL EVOLUTION ALGORITHM; ENGINEERING OPTIMIZATION; SELF-ADAPTATION; PARAMETERS	For last two decades, nature-inspired metaheuristic algorithms together with their modified, improved, and hybrid versions have been gaining huge popularity in the field of optimization in solving continuous and complex real-life optimization problems. In this work, a novel improved symbiosis organism search (SOS) algorithm, called self-adaptive beneficial factor-based improved SOS (SaISOS, in short) is suggested. The self-adaptive benefit factors and a modified mutualism phase (called "Three-way mutualism phase") have been introduced here to upgrade the performance of SOS algorithm. A random weighted reflection coefficient and a new control operator have also been introduced. To validate the proposed algorithm and to compare its performance with other state-of-the-art algorithms, 15 IEEE-CEC 2015 functions have been employed and the experimental results confirm that SaISOS provides competitive results on most occasions. Also, the proposed algorithm is used to solve five real-world optimization problems. Considering the average output, it is observed that the proposed method performs significantly better in solving the real-world problems compared to the alternative state-of-the art techniques considered in this work.																	0824-7935	1467-8640															10.1111/coin.12290		FEB 2020											
J								On the number of fuzzy subgroups of dicyclic groups	SOFT COMPUTING										Equivalence relation; Fuzzy subgroup; Chain of subgroups; Automorphism group	NON-ABELIAN GROUPS; EQUIVALENCE RELATION	One of the significant aspects of fuzzy group theory is classification of the fuzzy subgroups of finite groups under a suitable equivalence relation. In this paper, we determine the number of distinct fuzzy subgroups of dicyclic groups in some particular cases by the new equivalence relation introduced by T & x1ce;rn & x1ce;uceanu. In this case, the corresponding equivalence classes of fuzzy subgroups of a group G are closely connected to the automorphism group and the chains of subgroups of G. In fact, this new equivalence relation generalizes the natural equivalence relation defined on the lattice of fuzzy subgroups.																	1432-7643	1433-7479				APR	2020	24	8			SI		6183	6191		10.1007/s00500-020-04761-7		FEB 2020											
J								Improving Autonomous Exploration Using Reduced Approximated Generalized Voronoi Graphs	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Autonomous robotic exploration; Image thinning algorithm-reduced approximated GVG	MAPS; ALGORITHM; COVERAGE; PATH	Autonomous robotic exploration has been extensively applied in many tasks, such as mobile mapping and indoor searching. One of the most challenging issues is to locate the Next-Best-View and to guide robots through a previously unknown environment. Existing methods based on generalized Voronoi graphs (GVGs) have presented feasible solutions but require excessive computation to construct GVGs from metric maps, and the GVGs are usually redundant. This paper proposes an improving method based on reduced approximated GVG (RAGVG), which provides a topological representation of the explored space with a smaller graph. Additionally, a fast and robust image thinning algorithm for constructing RAGVGs from metric maps is presented, and an autonomous robotic exploration framework using RAGVGs is designed. The proposed method is validated with three known common data sets and two simulations of autonomous exploration tasks. The experimental results show that the proposed algorithm is efficient in constructing RAGVGs, and the simulations indicate that the mobile robot controlled by the RAGVG-based exploration method reduced the total time by approximately 20% for the given tasks.																	0921-0296	1573-0409				JUL	2020	99	1					91	113		10.1007/s10846-019-01119-6		FEB 2020											
J								Collision Avoidance of Redundant Robotic Manipulators Using Newton's Method	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Collision avoidance; Newton; Optimization; Hessian	FIELD-BASED NAVIGATION; ROADMAPS	This study investigates the application of Newton method to the problems of collision avoidance and path planning for robotic manipulators, especially robots with high Degrees of Freedom (DOF). The proposed algorithm applies to the potential fields method, where the Newton technique is used for performing the optimization. As compared to classical gradient descent method this implementation is mathematically elegant, enhances the performance of motion generation, eliminates oscillations, does not require gains tuning, and gives a faster convergence to the solution. In addition, the paper presents a computationally efficient symbolic formula for calculating the Hessian with respect to joint angles, which is essential for achieving realtime performance of the algorithm in high DOF configuration spaces. The method is validated successfully in simulation environment. Results for different methods (Newton, gradient descent and gradient descent with momentum) are compared in terms of quality of the path generated, oscillations, minimum distance to obstacles and convergence rate.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		673	681		10.1007/s10846-020-01159-3		FEB 2020											
J								Machine hermeneutics, postphenomenology, and facial recognition technology	AI & SOCIETY										Machine hermeneutics; Material hermeneutics; Perception; Don Ihde; Artificial intelligence		I would like to introduce the notion of machine hermeneutics in this paper. The notion refers to hermeneutical activity performed by machines. Machines are now capable of making the very interpretive tasks, using artificial intelligence algorithms based on the technology of machine learning that used to be the exclusive domain of human beings. In making this claim, I am not talking about possible conscious machines of the future, but those existing here and now. With facial recognition algorithms, for example, machines are now performing routinely what must be regarded as hermeneutical analyses with astounding accuracy and power. Thus, machine hermeneutics supplements Don Ihde's notion of material hermeneutics. In the latter, it is still human beings who do the interpretation, through the lenses provided the natural sciences; in this case, the natural sciences, or the technology afforded by the sciences, intervene between the human being and the world. In machine hermeneutics, on the contrary, the intervening comes in two layers. On the one hand, there is the usual intervention that Ihde talks about, but on the other, the artificial intelligence algorithm performs its own kind of intervention and interpretation, presenting an already interpreted result to the human beings, who then perceive it through the aid of the usual intervention such as the normal eyeglasses. Then the paper discusses the problem of how to justify the kind of perception that undergoes this process. In what sense can it be said that the algorithm is performing the right action, i.e., one such that the process comes up with a right picture of the world? I contend that this does not merely consist of technical excellence for the technology involved, but also ethical excellence. The two cannot be considered one apart from the other.																	0951-5666	1435-5655															10.1007/s00146-020-00951-x		FEB 2020											
J								Efficient meta-heuristic approaches in solving minimal exposure path problem for heterogeneous wireless multimedia sensor networks in internet of things	APPLIED INTELLIGENCE										Internet of things; Heterogeneous wireless multimedia sensor network; Directional sensing coverage model; Minimal exposure path; Evolution algorithm; Particle swarm optimization algorithm	COVERAGE PROBLEMS	One of the well-known methods for evaluating Heterogeneous wireless multimedia sensor networks (HWMSNs) in Internet of Things have drawn attention of the research community because this type of networks possesses great advantages of both coverage and performance. One of the most fundamental issues in HWMSNs is the barrier coverage problem which evaluates the surveillance capability of the network systems, especially those designed for security purposes. Among multiple approaches to solve this issue, finding the minimal exposure path (MEP), which corresponds to the worst-case coverage of the network is the most popular and efficient way. However, the MEP problem in HWMSNs (hereinafter heterogeneous multimedia MEP or HM-MEP) is specifically complex and challenging with the unique features of the HWMSNs. Thus, the problem is then converted into numerical functional extreme with high dimension, non-differential and non-linearity. Adapting to these features, two efficient meta-heuristic algorithms, Hybrid Evolutionary Algorithm (HEA) and Gravitation Particle Swarm Optimization (GPSO) are proposed for solving the problem. The HEA is a hybrid evolutionary algorithm in combination with local search while the GPSO is a novel particle swarm optimization based on the gravity force theory. Experimental results on extensive instances indicate that the proposed algorithms are suitable for the HM-MEP problem and perform well in term of both solution accuracy and computation time compared to existing approaches.																	0924-669X	1573-7497				JUN	2020	50	6					1889	1907		10.1007/s10489-019-01628-9		FEB 2020											
J								Computer-Aided Dementia Diagnosis Based on Hierarchical Extreme Learning Machine	COGNITIVE COMPUTATION										Dementia disease; Computer-aided diagnosis; CSP; Brain functional network; H-ELM	EEG CLASSIFICATION; ALZHEIMERS-DISEASE; NETWORK	The deep learning-based computer-aided diagnosis (CADx) approaches of dementia often require a lot of manual intervention. Although deep learning has a good effect on feature extraction, the current deep learning methods usually need to set a large number of parameters manually, which is time consuming. Hierarchical extreme learning machine (H-ELM) needs only less manual intervention and can extract features by a multi-layer feature representation framework, which is much faster than the traditional deep learning methods. A CADx framework based on H-ELM, named DCADx, is proposed. As common spatial pattern (CSP) and brain functional network (BFN) have been proven to have better de-redundancy effects on brain data, the DCADx contains two different data redundancy reduction methods: (1) CSP-based DCADx (i.e., DCADx-CSP model) and (2) BFN-based DCADx (i.e., DCADx-BFN model). The experimental evaluation proved the effectiveness of the proposed algorithms. The DCADx-CSP model obtained 83.2% on Alzheimer's disease and 82.5% on Parkinson's disease. The DCADx-BFN obtained 89.3% on Alzheimer's disease and 88.7% on Parkinson's disease. DCADx can make full use of the feature expression ability of H-ELM to achieve better performance. CSP and BFN can reduce the redundancy to enhance the diagnostic accuracy further.																	1866-9956	1866-9964															10.1007/s12559-019-09708-1		FEB 2020											
J								Robust and sparse multigroup classification by the optimal scoring approach	DATA MINING AND KNOWLEDGE DISCOVERY										High dimensional data; Linear discriminant analysis; Penalization; Robustness; Supervised classification; Variable selection	DISCRIMINANT-ANALYSIS; ALGORITHMS; REGRESSION; SQUARES; LASSO	We propose a robust and sparse classification method based on the optimal scoring approach. It is also applicable if the number of variables exceeds the number of observations. The data are first projected into a low dimensional subspace according to an optimal scoring criterion. The projection only includes a subset of the original variables (sparse modeling) and is not distorted by outliers (robust modeling). In this low dimensional subspace classification is performed by minimizing a robust Mahalanobis distance to the group centers. The low dimensional representation of the data is also useful for visualization purposes. We discuss the algorithm for the proposed method in detail. A simulation study illustrates the properties of robust and sparse classification by optimal scoring compared to the non-robust and/or non-sparse alternative methods. Three real data applications are given.																	1384-5810	1573-756X				MAY	2020	34	3					723	741		10.1007/s10618-019-00666-8		FEB 2020											
J								Analysis of net asset value prediction using low complexity neural network with various expansion techniques	EVOLUTIONARY INTELLIGENCE										Net asset value (NAV) prediction; Functional link artificial neural network (FLANN); Trigonometric expansion; Legendre polynomials expansion; Chebyshev polynomials expansion; Power series expansion		Net asset value (NAV) is a crucial measure that reveals the financial condition of a mutual fund and its prediction becomes important in decision making especially for fund managers. For this purpose, several works have been done using FLANN for NAV prediction. However, when it comes to FLANN model, the choice of various parameters and expansion functions has huge impact on the performance of the prediction must be considered. This study focuses on the objective of finding the optimal parameters for each model built with one type of functional expansion and compare them to find the most suitable for NAV prediction. Comparisons made on the learning rate, the sliding widow's size, the number of expansions, reveal that these parameters must be found by heuristic tests for each expansion. The analysis on the number of days ahead of prediction shows that Legendre expansion is more appropriate for short term prediction whereas Power series expansion gives good results for both short and long-term prediction. In case of Convergence, Power series followed by Chebyshev expansion converge faster than the other models.																	1864-5909	1864-5917															10.1007/s12065-020-00365-0		FEB 2020											
J								Supervised learning as an inverse problem based on non-smooth loss function	KNOWLEDGE AND INFORMATION SYSTEMS										Inverse problem; Supervised learning; Non-smooth loss function; Optimization; Slanting function; Airfoil self-noise; ECG signals	BIG DATA; MACHINE; OPTIMIZATION; STABILITY	This paper is concerned by solving supervised machine learning problem as an inverse problem. Recently, many works have focused on defining a relationship between supervised learning and the well-known inverse problems. However, this connection between the learning problem and the inverse one has been done in the particular case where the inverse problem is reformulated as a minimization problem with a quadratic cost functional (L2 cost functional). Although, it is well known that the cost functional can be L1 or any positive function that measures the gap between the predicted data and the observed one. Indeed, the use of L1 loss function for supervised learning problem gives more consistent results (see Rosasco et al. in Neural Comput 16:1063-1076, 2004). This strengthens the idea of reformulating the inverse problem, associated to machine learning problem, into a minimization problem using L1 functional. However, the L1 loss function is non-differentiable, which precludes the use of standard optimization tools. To overcome this difficulty, we propose in this paper a new technique of approximation based on the reformulation of the associated inverse problem into a minimizing one of a slanting cost functional Chen et al. (MIS Q Manag Inf Syst 36:1165-1188, 2012), which is solved using Tikhonov regularization and Newton's method. This approach leads to an efficient numerical algorithm allowing us to solve supervised learning problem in the most general framework. To confirm this, we present some numerical results showing the efficiency of the proposed approach. Furthermore, the numerical experiment validation is made through academic and real-life data. Thus, the comparison with existing methods and numerical stability of the algorithm is presented in order to show that our approach is better in terms of convergence speed and quality of predicted models.																	0219-1377	0219-3116				AUG	2020	62	8					3039	3058		10.1007/s10115-020-01439-2		FEB 2020											
J								Decision support for personalized hospital choice using the DEX hierarchical model with SMAA	KNOWLEDGE AND INFORMATION SYSTEMS										Patient hospital choice; DEX; SMAA; Uncertainty; Diversity; Decision making	MULTICRITERIA ACCEPTABILITY ANALYSIS; PATIENT CHOICE; QUALITY; HEALTH; INFORMATION; APPOINTMENT; PREDICTION; BEHAVIOR; MEDIA; COST	Despite an ever-growing personalized demand for patients' hospital choice, little systematic work has examined the decision process that considers the diversity of medical service demand. In this paper, we develop an intelligence decision framework to explore multi-source uncertain information in hospital choice. The framework employs a novel SMAA-DEX method to generate a ranking list of hospital alternatives based on Decision EXpert (DEX) hierarchical model and stochastic multicriteria acceptability analysis (SMAA) in personalized hospital choice. To conduct the multi-source information fusion under uncertainty, the SMAA-DEX method produces the central weight vector considering the ordinal weight and random weight and estimates the holistic acceptability indices for each alternative. By collecting hospital statistics, third-party evaluations and personal patient information in the real world, we verify our method for personalized hospital choice in terms of different preferences such as distance, ranking and income. The results of the experiments demonstrate the effectiveness of the proposed approach, which not only effectively processes various types of hospital choice, but also accomplishes uncertain reasoning of multi-source online information.																	0219-1377	0219-3116				AUG	2020	62	8					3059	3082		10.1007/s10115-020-01448-1		FEB 2020											
J								Risk analysis method of bank microfinance based on multiple genetic artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Multiple inheritance; Artificial neural network; Bank microloan; Risk assessment	POVERTY REDUCTION; INSTITUTIONS; PERFORMANCE; ALGORITHM	As a supplement to the financing of small- and medium-sized enterprises, bank microfinance companies are nonbank financial institutions, and it has played an active role in maintaining the stability of financial markets. However, in the course of the operation of microfinance companies, due to the lack of careful management and risk control, the problem of risk management has become increasingly prominent. The purpose of this paper is to study the microfinance risk based on polygenic artificial neural network, and it provides theory and practice application for risk management of microcredit enterprises. Taking the risk management of China's microfinance companies as the research object, on the basis of previous studies, this paper analyzes the risks of bank microfinance companies. Secondly, the basic theory of neural network model and its transformation function are introduced, and the learning method of neural network. At the same time, the learning algorithm of neural network and its improved algorithm are mainly introduced. It lays a theoretical foundation for the follow-up empirical research. Then, through the empirical study of data-based risk assessment of microcredit of farmers, the sample data are divided into training samples and test samples for comparison. Then, we use MATLAB software to establish a neural network model for farmers' microcredit risk assessment. Finally, in order to make the neural network model of farmers' credit risk assessment better popularize and apply, and effectively reduce the credit risk of farmers' microcredit. The corresponding policy suggestions are put forward, which proves the validity and applicability of the neural network in the field of farmers' microcredit risk assessment. It provides a good basis for rural credit cooperatives to identify the credit risk of farmers.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5367	5377		10.1007/s00521-019-04683-y		FEB 2020											
J								PTL-LTM model for complex action recognition using local-weighted NMF and deep dual-manifold regularized NMF with sparsity constraint	NEURAL COMPUTING & APPLICATIONS										Complex action recognition; Transfer learning; Nonnegative matrix factorization; Manifold structure	NONNEGATIVE MATRIX FACTORIZATION; REPRESENTATION; JOINT	Complex action recognition possesses significant academic research value, potential commercial value and broad market application prospect. For improving its performance, a local-weighted nonnegative matrix factorization with rank regularization constraint (LWNMF_RC) is firstly presented, which removes complex background and then obtains motion salient regions. Secondly, a dual-manifold regularized nonnegative matrix factorization with sparsity constraint (DMNMF_SC) is proposed, which not only considers the short-term and middle-term temporal dependencies implied in data manifold, but also mines the geometric structure hidden in feature manifold. In addition, the introduction of sparsity constraint makes features possess better discriminativeness. Thirdly, a deep DMNMF_SC method is constructed, which acquires more hierarchical and discriminative features. Finally, a long-term temporal memory model with probability transfer learning (PTL-LTM) is proposed, which accurately memorizes the long-term temporal dependency among multiple simple action segments and, meanwhile, makes full use of the probability features of rich labeled simple actions and then applies the knowledge learned from simple actions for complex action recognition. Consequently, the performance is effectively improved.																	0941-0643	1433-3058				SEP	2020	32	17					13759	13781		10.1007/s00521-020-04783-0		FEB 2020											
J								Multi-view convolutional neural network with leader and long-tail particle swarm optimizer for enhancing heart disease and breast cancer detection	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Leader and long-tail; Particle swarm optimization; Parameter optimization; Heart disease; Breast cancer	LEFT-VENTRICLE SEGMENTATION; COMPUTER-AIDED DETECTION; CLASSIFICATION; MASS	As the core of deep learning methodologies, convolutional neural network (CNN) has received wide attention in the area of image recognition. In particular, it requires very precise, accurate and fine recognition power for medical imaging processing. Numerous promising prospects of CNN applications with medical prognosis and diagnosis have been reported in the related works, and the common goal among the literature is mainly to analyze the insights from the finest details of medical images and build a more suitable model with maximum accuracy and minimum error. Thus, a novel CNN model is proposed with the characteristics of multi-view feature preprocessing and swarm-based parameter optimization. Additional information of extra features from multi-view is discovered potentially for training, and simultaneously, the most optimal set of CNN parameters are provided by our proposed leader and long-tail-based particle swarm optimization. The purpose of such a hybrid method is to achieve the highest possibility of target recognition in medical images. Preliminary experiments over cardiovascular and mammogram datasets related to heart disease prediction and breast cancer classification, respectively, are designed and conducted, and the results indicate encouraging performance compared to other existing CNN model optimization methods.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15469	15488		10.1007/s00521-020-04769-y		FEB 2020											
J								Artificial intelligence based commuter behaviour profiling framework using Internet of things for real-time decision-making	NEURAL COMPUTING & APPLICATIONS										Traffic analysis; Flow profiling; Trajectory analysis; Bluetooth data; Human behaviour; IoT	TRAFFIC FLOW; ROAD; WEATHER; TRANSPORTATION; ACCIDENTS	Road traffic environments are highly dynamic and volatile with a multitude of roadside and external environmental factors contributing to its dynamicity. Apart from infrastructure-related means such as traffic lights, planned and unplanned road events and different road networks, a core component which contributes towards the traffic environment is the human factor which is heavily overlooked in the current studies. Due to diverse travel patterns of day-to-day activities, the commuter behaviour is directly depicted in traffic patterns providing an opportunity to further explore human behaviours using road traffic. Conducting such analysis would reveal different commuter behavioural patterns that can be used for optimization and timely management of operations. However, to conduct such real-time behaviour analysis, large volumes of high-frequency data are required with high granularity, as well as, a suitable technology to manage such data. Addressing these needs, we propose an environment-driven commuter behavioural model that can be used to elucidate diverse behaviours in road traffic environments. We conceptualized, designed and developed an artificial intelligence based commuter behaviour profiling framework to detect diverse commuter behavioural profiles, fluctuating and routine patterns among commuters using traffic flow profiling and travel trajectory analysis. We evaluated the framework using 190 million data points captured from the Bluetooth sensor network of the Melbourne arterial road network, in the state of Victoria in Australia. The results demonstrate that traffic flow profiling of the proposed framework can provide insights on recurrent commuter behaviours that are distinct to a selected area with a high granularity. Moreover, traffic trajectory analysis provides insights on non-recurrent behaviours such as accidents with regard to how such incidents impact the dynamics of the network and how the impact is propagated through the network. Besides road traffic management, the proposed framework will enable real-time decision-making when planning road infrastructure and support decision-making of government and business entities to optimize operations.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16057	16071		10.1007/s00521-020-04736-7		FEB 2020											
J								Competitive equilibrium for almost all incomes: existence and fairness	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Competitive equilibrium; Fair division; Subgame perfect equilibrium; Picking sequence; Maximin share; Indivisible goods; Indivisible chores	RANDOM SERIAL DICTATORSHIP; INDIVISIBLE GOODS; ALLOCATION; DIVISION; COMPLEXITY; ENVY	Competitive equilibrium (CE) is a fundamental concept in market economics. Its efficiency and fairness properties make it particularly appealing as a rule for fair allocation of resources among agents with possibly different entitlements. However, when the resources are indivisible, a CE might not exist even when there is one resource and two agents with equal incomes. Recently, Babaioff and Nisan and Talgam-Cohen (2017-2019) have suggested to consider the entire space of possible incomes, and check whether there exists a CE for almost all income-vectors-all income-space except a subset of measure zero. They proved various existence and non-existence results, but left open the cases of four goods and three or four agents with monotonically-increasing preferences. This paper proves non-existence in both these cases, thus completing the characterization of CE existence for almost all incomes in the domain of monotonically increasing preferences. Additionally, the paper provides a complete characterization of CE existence in the domain of monotonically decreasing preferences, corresponding to allocation of chores. On the positive side, the paper proves that CE exists for almost all incomes when there are four goods and three agents with additive preferences. The proof uses a new tool for describing a CE, as a subgame-perfect equilibrium of a specific sequential game. The same tool also enables substantially simpler proofs to the cases already proved by Babaioff et al. Additionally, this paper proves several strong fairness properties that are satisfied by any CE allocation, illustrating its usefulness for fair allocation among agents with different entitlements.																	1387-2532	1573-7454				FEB 20	2020	34	1							26	10.1007/s10458-020-09444-z													
J								Density estimates on the unit simplex and calculation of the mode of a sample	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										aggregation functions; delaunay triangulation; density estimate; interval valued fuzzy sets; mode; nearest neighbours	FUZZY MEASURES	This paper addresses reliable and efficient calculation of the mode of a multivariate sample, which is a classical fusion function. In particular, we focus on the inputs given on the unit simplex, when aggregating elements of Atanassov intuitionistic fuzzy sets, interval-valued fuzzy sets and their extensions, as well as compositional data. We outline the use of a specially designed 2-additive fuzzy measures and the Choquet integral for the purposes of reducing computational complexity in higher dimensions. We present computational analysis and benchmark four different methods of density-based mode estimation.																	0884-8173	1098-111X				MAY	2020	35	5					850	868		10.1002/int.22227		FEB 2020											
J								A modified soft-likelihood function based on POWA operator	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										information fusion; OWA; POWA; power average (PA) operator; soft-likelihood function	AGGREGATION; ENTROPY	Information fusion is an important research direction. In this field, there are plenty of ways to combine evidence. Initially, Yager proposed a soft-likelihood function based on the ordered weighted average (OWA) operator to effectively fuse compatible probabilistic evidence. Recently, Song et al proposed a new soft-likelihood function based on the power ordered weighted average (POWA) operator. However, through analysis, we find Song et al's method has the following two shortcomings: (a) The weight of POWA cannot comprehensively reflect the relation between probability and OWA operator. (b) The soft-likelihood function does not reflect the preferences of decision makers. To overcome the above problem, we propose a modified soft-likelihood function. The effectiveness of the proposed method is demonstrated from the perspective of theoretical analysis and numerical examples.																	0884-8173	1098-111X				MAY	2020	35	5					869	890		10.1002/int.22228		FEB 2020											
J								Comparison of variable selection methods in partial least squares regression	JOURNAL OF CHEMOMETRICS										PLS; variable selection	MULTIVARIATE CALIBRATION; TARGET PROJECTION; PLS; ELIMINATION; PREDICTION; COMPONENTS	Through the remarkable progress in technology, it is getting easier and easier to generate vast amounts of variables from a given sample. The selection of variables is imperative for data reduction and for understanding the modeled relationship. Partial least squares (PLS) regression is among the modeling approaches that address high throughput data. A considerable list of variable selection methods has been introduced in PLS. Most of these methods have been reviewed in a recently conducted study. Motivated by this, we have therefore conducted a comparison of available methods for variable selection within PLS. The main focus of this study was to reveal patterns of dependencies between variable selection method and data properties, which can guide the choice of method in practical data analysis. To this aim, a simulation study was conducted with data sets having diverse properties like the number of variables, the number of samples, model complexity level, and information content. The results indicate that the above factors like the number of variables, number of samples, model complexity level, information content and variant of PLS methods, and their mutual higher-order interactions all significantly define the prediction capabilities of the model and the choice of variable selection strategy.																	0886-9383	1099-128X				JUN	2020	34	6							e3226	10.1002/cem.3226		FEB 2020											
J								Fast image encryption algorithm with high security level using the Bulban chaotic map	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image encryption; Bulban map; Chaos theory; Real-time communication	TRANSFORM	In the last decades, a big number of image encryption schemes have been proposed. Most of these schemes reach a high-security level, however, their slow speeds due to their complex process make them unusable in real-time applications. Motivated by this, we propose a new efficient and high-speed image encryption scheme based on the Bulban chaotic map. Unlike most of the existing schemes, we make a wisely use of this simple chaotic map to generate only a few numbers of random rows and columns. Moreover, to further increase the speed, we raise the processing unit from the pixel level to the row/column level. Security of the new scheme is achieved through a substitution-permutation network, where we apply a circular shift of rows and columns to break the strong correlation of adjacent pixels. Then, we combine the XOR operation with the Modulo function to mask the pixels values and prevent any leak of information. High-security tests and simulation analysis have been carried out to demonstrate that the scheme is extremely secure and highly fast for real-time image processing at 80 fps (frames per second).																	1861-8200	1861-8219															10.1007/s11554-020-00948-1		FEB 2020											
J								Transductive multi-label learning from missing data using smoothed rank function	PATTERN ANALYSIS AND APPLICATIONS										Semi-supervised learning; Matrix completion; Smoothed rank function; Multi-label learning; Constrained rank minimization	MATRIX COMPLETION; CLASSIFICATION	In this paper, we propose two new algorithms for transductive multi-label learning from missing data. In transductive matrix completion (MC), the challenge is prediction while the data matrix is partially observed. The joint MC and prediction tasks are addressed simultaneously to enhance accuracy in comparison with separate tackling of each. In this setting, the labels to be predicted are modeled as missing entries inside a stacked matrix along the feature-instance data. Assuming the data matrix is of low rank, we propose a new recommendation method for transductive MC by posing the problem as a minimization of the smoothed rank function with non-affine constraints, rather than its convex surrogate. We provide convergence analysis for the proposed algorithms and illustrate their low computational complexity and robustness in comparison with other methods. The simulations are conducted on well-known real datasets in two different scenarios of randomly missing pattern with and without block-loss. The simulations reveal our methods accuracy is superior to state-of-the-art methods up to 10% in low observation rates for the scenario without block-loss. The accuracy of the proposed methods in the scenario with block-loss is comparable to the state-of-the-art while the complexity is reduced up to four times.																	1433-7541	1433-755X				AUG	2020	23	3					1225	1233		10.1007/s10044-020-00869-6		FEB 2020											
J								"Solving discounted {0-1} knapsack problems by a discrete hybrid teaching-learning-based optimization algorithm"	APPLIED INTELLIGENCE										Discounted {0-1} knapsack problem; Teaching-learning-based optimization algorithm; Self-learning; Crossover operator	MULTIOBJECTIVE OPTIMIZATION; SEARCH ALGORITHM	The discounted {0-1} knapsack problem (D{0-1}KP) is a kind of knapsack problem with group structure and discount relationships among items. It is more challenging than the classical 0-1 knapsack problem. A more effective hybrid algorithm, the discrete hybrid teaching-learning-based optimization algorithm (HTLBO), is proposed to solve D{0-1}KP in this paper. HTLBO is based on the framework of the teaching-learning-based optimization (TLBO) algorithm. A two-tuple consisting of a quaternary vector and a real vector is used to represent an individual in HTLBO and that allows TLBO to effectively solve discrete optimization problems. We enhanced the optimization ability of HTLBO from three aspects. The learning strategy in the Learner phase is modified to extend the exploration capability of HTLBO. Inspired by the human learning process, self-learning factors are incorporated into the Teacher and Learner phases, which balances the exploitation and exploration of the algorithm. Two types of crossover operators are designed to enhance the global search capability of HTLBO. Finally, we conducted extensive experiments on eight sets of 80 instances using our proposed approach. The experiment results show that the new algorithm has higher accuracy and better stability than do previous methods. Overall, HTLBO is an excellent approach for solving the D{0-1}KP.																	0924-669X	1573-7497				JUN	2020	50	6					1872	1888		10.1007/s10489-020-01652-0		FEB 2020											
J								Model-based exception mining for object-relational data	DATA MINING AND KNOWLEDGE DISCOVERY										Outlier detection; Exception mining; Statistical-relational learning; Bayesian network; Likelihood ratio; Network data	OUTLIER DETECTION; INFORMATION; DISCOVERY; PATTERN	This paper develops model-based exception mining and outlier detection for the case of object-relational data. Object-relational data represent a complex heterogeneous network, which comprises objects of different types, links among these objects, also of different types, and attributes of these links. We follow the well-established exceptional model mining (EMM) framework, which has been previously applied for subgroup discovery in propositional data; our novel contribution is to develop EMM for relational data. EMM leverages machine learning models for exception mining: An object is exceptional to the extent that a model learned for the object data differs from a model learned for the general population. In relational data, EMM can therefore be used for detecting single outlier or exceptional objects. We combine EMM with state-of-the-art statistical-relational model discovery methods for constructing a graphical model (Bayesian network), that compactly represents probabilistic associations in the data. We investigate several outlierness metrics, based on the learned object-relational model, that quantify the extent to which the association pattern of a potential outlier object deviates from that of the whole population. Our method is validated on synthetic data sets and on real-world data sets about soccer and hockey matches, IMDb movies and mutagenic compounds. Compared to baseline methods, the EMM approach achieved the best detection accuracy when combined with a novel outlinerness metric. An empirical evaluation on soccer and movie data shows a strong correlation between our novel outlierness metric and success metrics: Individuals that our metric marks out as unusual tend to have unusual success.																	1384-5810	1573-756X				MAY	2020	34	3					681	722		10.1007/s10618-020-00677-w		FEB 2020											
J								Theorem Proving for Pointwise Metric Temporal Logic Over the Naturals via Translations	JOURNAL OF AUTOMATED REASONING										Metric temporal logic; Theorem proving; Modelling	VERIFICATION; COMPLEXITY	We study translations from metric temporal logic (MTL) over the natural numbers to linear temporal logic (LTL). In particular, we present two approaches for translating from MTL to LTL which preserve the ExpSpace complexity of the satisfiability problem for MTL. In each of these approaches we consider the case where the mapping between states and time points is given by (i) a strict monotonic function and by (ii) a non-strict monotonic function (which allows multiple states to be mapped to the same time point). We use this logic to model examples from robotics, traffic management, and scheduling, discussing the effects of different modelling choices. Our translations allow us to utilise LTL solvers to solve satisfiability and we empirically compare the translations, showing in which cases one performs better than the other. We also define a branching-time version of the logic and provide translations into computation tree logic.																	0168-7433	1573-0670				DEC	2020	64	8					1553	1610		10.1007/s10817-020-09541-4		FEB 2020											
J								The contribution of linked open data to augment a traditional data warehouse	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Linked open data; Traditional DW augmentation; Value	METRICS; ONTOLOGY; QUALITY; DESIGN; MODEL; ETL	The arrival of Big Data has contributed positively to the evolution of the data warehouse (DW ) technology. This gives birth of augmented DW s that aim at maximizing the effectiveness of existing ones. Various augmentation scenarios have been proposed and adopted by firms and industry covering several aspects such as new data sources (e.g., Linked Open Data (LOD), social, stream and IoT data), data ingestion, advanced deployment infrastructures, programming paradigms, data visualization. These scenarios allow companies reaching valuable decisions. By examining traditional DW s, we realized that they do not fulfill all decision-maker requirements since data sources alimenting a target DW are not rich enough to capture Big Data. The arrival of LOD era is an excellent opportunity to enrich traditional DW s with a new V dimension: Value. In this paper, we first conceptualize the variety of internal and external sources and study its effect on the ETL phase to ease the value capturing. Secondly, a Value-driven approach for the DW design is discussed. Thirdly, three realistic scenarios for integrating LOD in the DW landscape are given. Finally, experiments are conducted showing the added value by augmenting the existing DW environment with LOD.																	0925-9902	1573-7675				DEC	2020	55	3					397	421		10.1007/s10844-020-00594-w		FEB 2020											
J								A research survey: heuristic approaches for solving multi objective flexible job shop problems	JOURNAL OF INTELLIGENT MANUFACTURING										Flexible job shop; Heuristics; Multi objective; Metaheuristics	MULTIOBJECTIVE GENETIC ALGORITHM; SWARM OPTIMIZATION ALGORITHM; DISCRETE HARMONY SEARCH; BEE COLONY ALGORITHM; SCHEDULING PROBLEM; TABU SEARCH; EVOLUTIONARY ALGORITHMS; ROBUSTNESS MEASURES; PARETO-OPTIMALITY; DISPATCHING RULES	Flexible job shop scheduling problem is a relaxation of the job shop scheduling problem and is one of the well-known combinatorial optimization problems that has wide applications in the industrial fields such as production management, supply chain, transport systems, manufacturing systems. In recent years, many researches have been carried out with different approaches-ranging from mathematical models to heuristic methods-to solve multi objective flexible job shop scheduling problems (FJSSP). This study aims to present the forms of scrutiny of multi-objective FJSSPs and various heuristic techniques used to solve problems in the last decade. This review will allow the reader to select specific methods and follow the guidelines set forth in their future research.																	0956-5515	1572-8145															10.1007/s10845-020-01547-4		FEB 2020											
J								A comprehensive investigation into sclera biometrics: a novel dataset and performance study	NEURAL COMPUTING & APPLICATIONS										Ocular biometrics; Sclera recognition; Dataset; Identity recognition; Sclera segmentation	IRIS RECOGNITION; SEGMENTATION	The area of ocular biometrics is among the most popular branches of biometric recognition technology. This area has long been dominated by iris recognition research, while other ocular modalities such as the periocular region or the vasculature of the sclera have received significantly less attention in the literature. Consequently, ocular modalities beyond the iris are not well studied and their characteristics are today still not as well understood. While recent needs for more secure authentication schemes have considerably increased the interest in competing ocular modalities, progress in these areas is still held back by the lack of publicly available datasets that would allow for more targeted research into specific ocular characteristics next to the iris. In this paper, we aim to bridge this gap for the case of sclera biometrics and introduce a novel dataset designed for research into ocular biometrics and most importantly for research into the vasculature of the sclera. Our dataset, called Sclera Blood Vessels, Periocular and Iris (SBVPI), is, to the best of our knowledge, the first publicly available dataset designed specifically with research in sclera biometrics in mind. The dataset contains high-quality RGB ocular images, captured in the visible spectrum, belonging to 55 subjects. Unlike competing datasets, it comes with manual markups of various eye regions, such as the iris, pupil, canthus or eyelashes and a detailed pixel-wise annotation of the complete sclera vasculature for a subset of the images. Additionally, the datasets ship with gender and age labels. The unique characteristics of the dataset allow us to study aspects of sclera biometrics technology that have not been studied before in the literature (e.g. vasculature segmentation techniques) as well as issues that are of key importance for practical recognition systems. Thus, next to the SBVPI dataset we also present in this paper a comprehensive investigation into sclera biometrics and the main covariates that affect the performance of sclera segmentation and recognition techniques, such as gender, age, gaze direction or image resolution. Our experiments not only demonstrate the usefulness of the newly introduced dataset, but also contribute to a better understanding of sclera biometrics in general.																	0941-0643	1433-3058															10.1007/s00521-020-04782-1		FEB 2020											
J								New method for interval-valued hesitant fuzzy decision making based on preference relations	SOFT COMPUTING										Decision making; IVHFPR; Acceptably multiplicative consistency; Consensus; Optimization model	MULTIPLICATIVE CONSISTENCY	Interval-valued hesitant fuzzy preference relations (IVHFPRs) are powerful to express the judgments of decision makers (DMs) such as hesitancy and uncertainty. This paper continues to research decision making with IVHFPRs. To do this, it first defines a multiplicative consistency index for interval fuzzy preference relations (IFPRs). Then, it gives a new (acceptably) multiplicative consistency concept for IVHFPRs. After that, optimization models for judging the acceptably multiplicative consistency of IVHFPRs are built. When the multiplicative consistency is unacceptable, optimization models for improving the multiplicative consistency level and for minimizing the number of adjusted judgments are constructed, respectively. Based on the acceptably multiplicative consistency analysis, an algorithm for decision making with IVHFPRs is presented. Furthermore, formulae for determining the weights of the DMs and for measuring the consensus are offered. Based on the acceptably multiplicative consistency and consensus analysis, a new method for group decision making with IVHFPRs is proposed. Finally, an example about selecting the reservoir operation schemes is offered to show the efficiency of the new method and to compare with previous research.																	1432-7643	1433-7479				SEP	2020	24	17					13381	13399		10.1007/s00500-020-04756-4		FEB 2020											
J								Inverse-adaptive multilayer T-S fuzzy controller for uncertain nonlinear system optimized by differential evolution algorithm	SOFT COMPUTING										Inverse-adaptive multilayer T-S fuzzy controller (IFC plus AF); Uncertain nonlinear system; Hybrid adaptive-optimal control; Differential evolution (DE) algorithm; Lyapunov stability principle	DESIGN; TRACKING	This paper initiatively proposes a novel inverse-adaptive multilayer T-S fuzzy controller (IFC + AF) optimized with differential evolution (DE) soft computing algorithm available for a class of robust control methods applied in uncertain nonlinear SISO and MISO systems. First, a novel multilayer T-S fuzzy model is created by combined multiple simple T-S fuzzy models with a sum function in the output. Then, the parameters of multilayer T-S fuzzy model are optimally identified using DE algorithm to create offline the inverse nonlinear system regarding uncertain system parameters. Second, an adaptive fuzzy-based sliding-mode surface is innovatively designed to guarantee that the closed-loop system is asymptotically stable using Lyapunov stability principle. Moreover, necessary benchmark tests are investigated in MATLAB/Simulink platform, including the spring-mass-damper SMD system and the fluid level of a double tank with uncertain parameters, in order to illustrate the effectiveness and the feasibility of the proposed IFC + AF control scheme. The IFC + AF control algorithm is adequately investigated with various control coefficients and is strictly compared with the advanced adaptive fuzzy control and the inverse fuzzy control (IFC) approaches. Simulation and experiment results are satisfactorily investigated and demonstrate the feasibility and performance of the proposed IFC + AF control method.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14073	14089		10.1007/s00500-020-04782-2		FEB 2020											
J								PedestriANS: a bipedal robot with adaptive morphology	ADAPTIVE BEHAVIOR										Actuator network system; bipedal robot; adaptive morphology; locomotion; whole body dynamics		In diverse situations, humans produce natural and adaptable bipedal locomotion by cooperatively manipulating the interactions among the different parts of their bodies and the environment. Therefore, to realize a robot with adaptable behavior, it should be enabled to adjust its morphology accordingly in response to environmental changes. From this perspective, this study introduces the development of a bipedal robot with adaptive morphology. By implementing an actuator network system (ANS), the robot is able to manipulate the physical characteristics of its legs and the way they interact with each other. Two experiments have been conducted: main and supplementary experiments. The main experiment examined how effective is adjusting the robot's morphology on changing the robot's behavior. The experiment was conducted on different ground materials and under different connection patterns between the robot's legs. During the experiment, the robot's behavior was evaluated in reference to four aspects: walking style, stability, speed, and moving direction. The supplementary experiment took the results of the main experiment and used it to improve the robot's behavior during locomotion. The robot was enabled to automatically switch between the different connection patterns of the ANS, which in turn changed the interaction between the robot's legs and generated a more suitable dynamics for the surrounding environment.																	1059-7123	1741-2633														UNSP 1059712320905177	10.1177/1059712320905177		FEB 2020											
J								ECG sonification to support the diagnosis and monitoring of myocardial infarction	JOURNAL ON MULTIMODAL USER INTERFACES										Electrocardiogram; Sonification; Process monitoring; Myocardial infarction	ST-SEGMENT	This paper presents the design and evaluation of four sonification methods to support monitoring and diagnosis in Electrocardiography (ECG). In particular we focus on an ECG abnormality called ST-elevation which is an important indicator of a myocardial infarction. Since myocardial infarction represents a life-threatening condition it is of essential value to detect an ST-elevation as early as possible. As part of the evaluated sound designs, we propose two novel sonifications: (i) Polarity sonification, a continuous parameter-mapping sonification using a formant synthesizer and (ii) Stethoscope sonification, a combination of the ECG signal and a stethoscope recording. The other two designs, (iii) the water ambience sonification and the (iv) morph sonification, were presented in our previous work about ECG sonification (Aldana Blanco AL, Steffen G, Thomas H (2016) In: Proceedings of Interactive Sonification Workshop (ISon). Bielefeld, Germany). The study evaluates three components across the proposed sonifications (1) detection performance, meaning if participants are able to detect a transition from healthy to unhealthy states, (2) classification accuracy, that evaluates if participants can accurately classify the severity of the pathology, and (3) aesthetics and usability (pleasantness, informativeness and long-term listening). The study results show that the polarity design had the highest accuracy rates in the detection task whereas the stethoscope sonification obtained the better score in the classification assignment. Concerning aesthetics, the water ambience sonification was regarded as the most pleasant. Furthermore, we found a significant difference between sound/music experts and non-experts in terms of the error rates obtained in the detection task using the morph sonification and also in the classification task using the stethoscope sonification. Overall, the group of experts obtained lower error rates than the group of non-experts, which means that further training could improve accuracy rates and, particularly for designs that rely mainly on pitch variations, additional training is needed in the non-experts group.																	1783-7677	1783-8738				JUN	2020	14	2			SI		207	218		10.1007/s12193-020-00319-x		FEB 2020											
J								Sparse-FCM and deep learning for effective classification of land area in multi-spectral satellite images	EVOLUTIONARY INTELLIGENCE										Multi-classification; Land use classification; Multi-spectral satellite images; DBN; Harmony search algorithm	COVER; ALGORITHM	Remote sensing plays a major role in crop classification, land use classification, and land cover classification such that the information for the classification is assured with the help of the satellite images. This paper concentrates on the land use classification and proposes an optimization algorithm, called Firefly Harmony Search (FHS) for training the Deep Belief Neural Network (DBN). The FHS algorithm is the integration of the Firefly Algorithm and Harmony Search Algorithm (HSA), which tunes the weights of DBN to perform the multi-class classification. For the effective classification, the multispectral image is subjected to the sparse Fuzzy C-Means to form segments such that the feature extraction is effective, free from dimensionality issues and computational complexities. The features extracted from the segments of the multi-spectral images include vegetation indices and statistical features. Then, these features are fed to the DBN, which is tuned using the FHS algorithm for performing the land use classification. Experimentation using four datasets proves the effectiveness of the proposed multi-class classification approach. The accuracy, sensitivity, and specificity of the method are found to be 0.9317, 0.9568, and 0.0379, respectively, that is effective over the existing land use classification methods.																	1864-5909	1864-5917															10.1007/s12065-020-00362-3		FEB 2020											
J								A fuzzy adaptive gravitational search algorithm for two-dimensional multilevel thresholding image segmentation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image segmentation; Two-dimension multilevel thresholding; Fuzzy adaptive gravitational search algorithm; Tsallis entropy	OPTIMIZATION; ENTROPY; KAPURS; GSA	Two-dimensional (2D) multilevel thresholding is an important technique for noisy image segmentation which has drawn much attention during the past few years. The conventional image segmentation methods are efficient for 2D bi-level thresholding. However, the computational complexity grows exponentially when extended to 2D multilevel thresholding since they search the optimal thresholds by exhaustive strategy. To tackle this problem, a fuzzy adaptive gravitational search algorithm (FAGSA) using Tsallis entropy as its objective function has been presented to find the optimal 2D multilevel thresholds in this paper. In the FAGSA, fuzzy logic controllers are designed to tune the control parameters. The state-of-the-art heuristic algorithms are compared with this proposed algorithm. Both test images and noisy images are utilized in the experiments to evaluate the performance of the involved algorithms. The experimental results significantly demonstrate the superiority of our algorithm in terms of the objective function value, image quality measures and time consumption.																	1868-5137	1868-5145															10.1007/s12652-020-01777-7		FEB 2020											
J								An efficient adaptive thresholding function optimized by a cuckoo search algorithm for a despeckling filter of medical ultrasound images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Thresholding function; Despeckling; Ultrasound image; Cuckoo search algorithm	SPECKLE; NOISE; WAVELET; DESIGN	In this research paper, we propose an efficient adaptive thresholding function optimized by a cuckoo search algorithm for despeckling filters to avoid boundaries created by more than a few current despeckling filters. The speckle noise contamination caused at some points in ultrasound picture acquisition systems compromises the level of its visuality, which presents a diagnostic challenge for medical doctors. Therefore, to enhance the visual quality, despeckling filters are normally employed in the processing of such pictures. However, several disadvantages have developed within current despeckling filters, which have discouraged the utilization of modern despeckling filters for minimizing impacts from speckle noise. The proposed despeckling filter was developed through a combination of an adaptive thresholding function and a cuckoo search algorithm. Specifically, the aforementioned cuckoo search algorithm optimizes the coefficients of the thresholding function. Additionally, the proposed approach it can be used in a no-reference as well as in a full-reference image quality assessment which the assessment of PSNR, SSIM, MSE and MAE values was used to evaluate the proposed despeckling filter, as well as others, in medical ultrasound images. The findings from the research show that the visual outcome acquired by the proposed filter is inferior to those of filters with a despeckling effect. In addition, the numerical results revealed that the proposed despeckling filter was effective in creating ultrasound imageries for clinical use.																	1868-5137	1868-5145															10.1007/s12652-020-01743-3		FEB 2020											
J								Single Image Blind Deblurring Based on Salient Edge-Structures and Elastic-Net Regularization	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Single image blind deblurring; Salient edge-structures; Elastic-net regularization; Kernel and latent image estimation	DECONVOLUTION; MINIMIZATION; ALGORITHM	In single image blind deblurring, the blur kernel and latent image are estimated from a single observed blurry image. The associated mathematical problem is ill-posed, and an acceptable solution is difficult to obtain without additional priors or heuristics. Inspired by the nonlocal self-similarity in image denoising problem, we introduce elastic-net regularization as a rank prior to improve the estimation of the intermediate image. Furthermore, it is well known that salient edge-structures can provide reliable information for kernel estimation. Therefore, we propose a new blind image deblurring method by combining the salient edge-structures and the elastic-net regularization. The salient edge-structures are selected from the intermediate image and used to guide the estimation of the blur kernel. Then, we employ the elastic-net regularization and edge-structures to further estimate intermediate latent image, by retaining the dominant edge and removing slight texture, for a better kernel estimation. Finally, quantitative and qualitative evaluations are conducted by comparing the results with those obtained by state-of-the-art methods. We conclude that the proposed method performs favorably when considering both synthetic and real blurry images.																	0924-9907	1573-7683				OCT	2020	62	8					1049	1061		10.1007/s10851-020-00949-6		FEB 2020											
J								Multi-weighted Complex Structure on Fractional Order Coupled Neural Networks with Linear Coupling Delay: A Robust Synchronization Problem	NEURAL PROCESSING LETTERS										Robust synchronization; Fractional order; Coupled neural networks; Kronecker product; Linear coupling delays	FINITE-TIME SYNCHRONIZATION; ADAPTIVE PINNING SYNCHRONIZATION; DYNAMICAL NETWORKS; DIFFERENTIAL-EQUATIONS; UNCERTAIN PARAMETERS; INTERMITTENT CONTROL; PASSIVITY ANALYSIS; VARYING DELAYS; LEAKAGE DELAY; STABILITY	This sequel is concerned with the analysis of robust synchronization for a multi-weighted complex structure on fractional-order coupled neural networks (MWCFCNNs) with linear coupling delays via state feedback controller. Firstly, by means of fractional order comparison principle, suitable Lyapunov method, Kronecker product technique, some famous inequality techniques about fractional order calculus and the basis of interval parameter method, two improved robust asymptotical synchronization analysis, both algebraic method and LMI method, respectively are established via state feedback controller. Secondly, when the parameter uncertainties are ignored, several synchronization criterion are also given to ensure the global asymptotical synchronization of considered MWCFCNNs. Moreover, two type of special cases for global asymptotical synchronization MWCFCNNs with and without linear coupling delays, respectively are investigated. Ultimately, the accuracy and feasibility of obtained synchronization criteria are supported by the given two numerical computer simulations.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2453	2479		10.1007/s11063-019-10188-5		FEB 2020											
J								Design of highly effective multilayer feedforward neural network by using genetic algorithm	EXPERT SYSTEMS										activation functions; genetic algorithm; learning parameters; multilayer feedforward neural networks	ACTIVATION FUNCTIONS; LEARNING RATE; BACKPROPAGATION	This paper presents a highly effective and precise neural network method for choosing the activation functions (AFs) and tuning the learning parameters (LPs) of a multilayer feedforward neural network by using a genetic algorithm (GA). The performance of the neural network mainly depends on the learning algorithms and the network structure. The backpropagation learning algorithm is used for tuning the network connection weights, and the LPs are obtained by the GA to provide both fast and reliable learning. Also, the AFs of each neuron in the network are automatically chosen by a GA. The present study consists of 10 different functions to accomplish a better convergence of the desired input-output mapping. Test studies are performed to solve a set of two-dimensional regression problems for the proposed genetic-based neural network (GNN) and conventional neural network having sigmoid AFs and constant learning parameters. The proposed GNN has also been tested by applying it to three real problems in the fields of environment, medicine, and economics. Obtained results prove that the proposed GNN is more effective and reliable when compared with the classical neural network structure.																	0266-4720	1468-0394				AUG	2020	37	4			SI				e12532	10.1111/exsy.12532		FEB 2020											
J								Bi-objective web service composition problem in multi-cloud environment: a bi-objective time-varying particle swarm optimisation algorithm	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Multi-objective optimisation; web service composition; multi-cloud market	FRAMEWORK; AWARE	Cloud computing became an inevitable information technology industry. Despite its several plus points such as economy of scale and rapid elasticity, it suffers from vendor lock-in, resource limitation and cybersecurity attacks in which it leads business discontinuity or even business failure. Multi-cloud, on the other hand, can be trustable paradigm to obviate obstacles such as aforesaid unpleasant features of a single cloud. One of the biggest challenges is to know which cloud is commensurate with user's business process with regards to security objectives. To this end, the new method is presented to quantify the amount of cloud security risk (CSR) in regards to user's business process. Therefore, in this paper, the web service composition problem is formulated to bi-objective optimisation problem with service cost and multi-cloud risk viewpoints in ever-increasing multi-cloud environment (MCE) in which each provider has its variable pricing policy and different security level. It is obviously an NP-Hard problem. To solve the combinatorial problem, we develop a bi-objective time-varying particle swarm optimisation (BOTV-PSO) algorithm. The parameters are tuned based on elapsed time so a good balance between exploration and exploitation is achieved. To illustrate the effectiveness of proposed algorithm, we defined several scenarios and compared the performance of proposed algorithm with multi-objective GA-based (MOGA) optimiser, a single objective genetic algorithm (SOGA) that only optimises cost function and neglects CSR, and multi-objective simulated annealing algorithm (MOSA). The experimental results showed the superiority of proposed BOTV-PSO against other approaches in terms of convergence, diversity, fitness, performance, and even scalability.																	0952-813X	1362-3079															10.1080/0952813X.2020.1725652		FEB 2020											
J								Evolving dictionary based sentiment scoring framework for patient authored text	EVOLUTIONARY INTELLIGENCE										Sentiment analysis; Medical text; Patient authored text (PAT); Dictionary based approach		In recent days, the Government and other organizations are focusing on providing better health care to people. Understanding the patients experience of care-received is key for providing better health care. With prevailing usage of social media applications, patients are expressing their experience over social media. This patient authored text is a free-unstructured data which is available over social media in large chunks. To extract the sentiments from this huge data, a domain-specific dictionary is required to get better accuracy. The proposed approach defines a new domain-specific dictionary and uses this in sentiment scoring to enhance the overall sentiment classification on patient authored text. We conducted experiments on the proposed approach using NHS Choices dataset and compared it with popular classifiers like linear regression, stochastic gradient descent, dictionary-based approaches: VADER and AFINN. The results prove that the proposed approach is an effective strategy for sentiment analysis over patient authored text which helps in improving the classification accuracy.																	1864-5909	1864-5917															10.1007/s12065-020-00366-z		FEB 2020											
J								Improved convolutional neural network based histopathological image classification	EVOLUTIONARY INTELLIGENCE										Convolution neural network; Histopathological image classification; Machine learning	BREAST-CANCER DIAGNOSIS	Histopathological image classification is one of the important application areas of medical imaging. However, an accurate and efficient classification is still an open-ended research due to the complexity in histopathological images. For the same, this paper presents an efficient architecture of convolutional neural network for the classification of histopathological images. The proposed method consists of five subsequent blocks of layers, each having convolutional, drop-out, and max-pooling layers. The performance of the introduced classification system is validated on colorectal cancer histology image dataset which consists of RGB-colored images belonging to eight different classes. The experimental results confirm the higher performance of the proposed convolutional neural network against existing different machine learning models with the lowest error rate of 22.7%.																	1864-5909	1864-5917															10.1007/s12065-020-00367-y		FEB 2020											
J								Multi-layer manifold learning with feature selection	APPLIED INTELLIGENCE										Data embedding; Feature selection; Feature extraction; Manifold learning; Face recognition; Pattern classification	LOCALITY PRESERVING PROJECTIONS; FEATURE-EXTRACTION; DIMENSIONALITY REDUCTION; FRAMEWORK; PLUS	Many fundamental problems in machine learning require some form of dimensionality reduction. To this end, two different strategies were used: Manifold Learning and Feature Selection. Manifold learning (or data embedding) attempts to compute a subspace from original data by feature recombination/transformation. Feature selection aims to select the most relevant features in the original space. In this paper, we propose a novel cooperative Manifold learning-Feature selection that goes beyond the simple concatenation of these two modules. Our basic idea is to transform a given shallow embedding to a deep variant by computing a cascade of embeddings in which each embedding undergoes feature selection and elimination. We use filter approaches in order to efficiently select irrelevant features at any stage of the process. For a case study, our proposed framework was used with two typical linear embedding algorithms: Local Discriminant Embedding (LDE) (a supervised technique) and Locality Preserving Projections (LPP) (unsupervised technique) on four challenging face databases and it has been conveniently compared with other cooperative schemes. Moreover, a comparison with several state-of-the-art manifold learning methods is provided. As it is exhibited by our experimental study, the proposed framework can achieve superior learning performance with respect to classic cooperative schemes and to many competing manifold learning methods.																	0924-669X	1573-7497				JUN	2020	50	6					1859	1871		10.1007/s10489-019-01563-9		FEB 2020											
J								Reduction of Onset Delay in Functional Near-Infrared Spectroscopy: Prediction of HbO/HbR Signals	FRONTIERS IN NEUROROBOTICS										hemodynamic response; prediction; tracking; vector phase analysis; brain-machine interface (BMI); functional near-infrared spectroscopy (fNIRS)	BRAIN-COMPUTER INTERFACES; INITIAL DIP; SYSTEM-IDENTIFICATION; NEURONAL-ACTIVITY; MICROCIRCULATION; CLASSIFICATION; NIRS; PHASE; MOTOR; FMRI	An intrinsic problem when using hemodynamic responses for the brain-machine interface is the slow nature of the physiological process. In this paper, a novel method that estimates the oxyhemoglobin changes caused by neuronal activations is proposed and validated. In monitoring the time responses of blood-oxygen-level-dependent signals with functional near-infrared spectroscopy (fNIRS), the early trajectories of both oxy- and deoxy-hemoglobins in their phase space are scrutinized. Furthermore, to reduce the detection time, a prediction method based upon a kernel-based recursive least squares (KRLS) algorithm is implemented. In validating the proposed approach, the fNIRS signals of finger tapping tasks measured from the left motor cortex are examined. The results show that the KRLS algorithm using the Gaussian kernel yields the best fitting for both Delta HbO (i.e., 87.5%) and Delta HbR (i.e., 85.2%) at q = 15 steps ahead (i.e., 1.63 s ahead at a sampling frequency of 9.19 Hz). This concludes that a neuronal activation can be concluded in about 0.1 s with fNIRS using prediction, which enables an almost real-time practice if combined with EEG.																	1662-5218					FEB 18	2020	14								10	10.3389/fnbot.2020.00010													
J								Text-mining-based Fake News Detection Using Ensemble Methods	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Fake news; social media; stylometric features; word vectors; ensemble methods		Social media is a platform to express one's views and opinions freely and has made communication easier than it was before. This also opens up an opportunity for people to spread fake news intentionally. The ease of access to a variety of news sources on the web also brings the problem of people being exposed to fake news and possibly believing such news. This makes it important for us to detect and flag such content on social media. With the current rate of news generated on social media, it is difficult to differentiate between genuine news and hoaxes without knowing the source of the news. This paper discusses approaches to detection of fake news using only the features of the text of the news, without using any other related metadata. We observe that a combination of stylometric features and text-based word vector representations through ensemble methods can predict fake news with an accuracy of up to 95.49%.																	1476-8186	1751-8520				APR	2020	17	2					210	221		10.1007/s11633-019-1216-5		FEB 2020											
J								The MetaCoq Project	JOURNAL OF AUTOMATED REASONING										Proof assistants; Meta-Programming; Program certification	PARAMETRICITY	The MetaCoq project aims to provide a certified meta-programming environment in Coq. It builds on Template-Coq, a plugin for Coq originally implemented by Malecha (Extensible proof engineering in intensional type theory, Harvard University, , 2014), which provided a reifier for Coq terms and global declarations, as represented in the Coq kernel, as well as a denotation command. Recently, it was used in the CertiCoq certified compiler project (Anand et al., in: CoqPL, Paris, France, , 2017), as its front-end language, to derive parametricity properties (Anand and Morrisett, in: CoqPL'18, Los Angeles, CA, USA, 2018). However, the syntax lacked semantics, be it typing semantics or operational semantics, which should reflect, as formal specifications in Coq, the semantics of Coq 's type theory itself. The tool was also rather bare bones, providing only rudimentary quoting and unquoting commands. We generalize it to handle the entire polymorphic calculus of cumulative inductive constructions, as implemented by Coq, including the kernel's declaration structures for definitions and inductives, and implement a monad for general manipulation of Coq 's logical environment. We demonstrate how this setup allows Coq users to define many kinds of general purpose plugins, whose correctness can be readily proved in the system itself, and that can be run efficiently after extraction. We give a few examples of implemented plugins, including a parametricity translation and a certified extraction to call-by-value lambda-calculus. We also advocate the use of MetaCoq as a foundation for higher-level tools.																	0168-7433	1573-0670				JUN	2020	64	5			SI		947	999		10.1007/s10817-019-09540-0		FEB 2020											
J								Energy allocation and payment: a game-theoretic approach	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Mechanism design; Cooperation game theory; 91-XX; 91Axx	DEMAND-SIDE MANAGEMENT; SHAPLEY VALUE	Nowadays, energy represents the most important resource; however, we need to face several energy-related rising issues, one main concern is how energy is consumed. In particular, how we can stimulate consumers on a specific behaviour. In this work, we present a model facing energy allocation and payment. Thus, we start with the explanation of the first step of our work concerning a mechanism design approach for energy allocation among consumers. More in details, we go deep into the formal description of the energy model and users' consumption profiles. We aim to select the optimal consumption profile for every user avoiding consumption peaks when the total required energy could exceed the energy production. The mechanism will be able to drive users in shifting energy consumptions in different hours of the day. The next step concerns a payment estimation problem which involves a community of users and an energy distributor (or producer). Our aim is to compute payments for every user in the community according to the single user's consumption, the community's consumption and the available energy. By computing community-dependent energy bills, our model stimulates a users' virtuous behaviour, so that everyone approaches the production threshold as close as possible. Our payment function distributes incentives if the consumption is lower than the produced energy and penalties when the consumption exceeds the resources threshold, satisfying efficiency and fairness properties both from the community (efficiency as an economic equilibrium among sellers and buyers) and the single user (fairness as an economic measure of energy good-behaving) points of view.																	1012-2443	1573-7470				JUL	2020	88	7			SI		793	816		10.1007/s10472-019-09685-z		FEB 2020											
J								Development of a Novel Robotic Rehabilitation System With Muscle-to-Muscle Interface	FRONTIERS IN NEUROROBOTICS										wearable robot; human-human interface; electromyogram; functional electrical stimulation; lower limb rehabilitation	ELECTRICAL-STIMULATION; ASSISTED THERAPY; UPPER-LIMB; STROKE; COMPLIANT; PATTERNS; MOTION	In this study, we developed a novel robotic system with a muscle-to-muscle interface to enhance rehabilitation of post-stroke patients. The developed robotic rehabilitation system was designed to provide patients with stage appropriate physical rehabilitation exercise and muscular stimulation. Unlike the position-based control of conventional bimanual robotic therapies, the developed system stimulates the activities of the target muscles, as well as the joint movements of the paretic limb. The robot-assisted motion and the electrical stimulation on the muscles of the paretic side are controlled by on-line comparison of the motion and the muscle activities between the paretic and unaffected sides. With the developed system, the rehabilitation exercise can be customized and modulated depending on the patient's stage of motor recovery after stroke. The system can be operated in three different modes allowing both passive and active exercises. The effectiveness of the developed system was verified with healthy human subjects, where the subjects were paired to serve as the unaffected side and the paretic side of a hemiplegic patient.																	1662-5218					FEB 18	2020	14								3	10.3389/fnbot.2020.00003													
J								Design and Performance Evaluation of a Novel Wearable Parallel Mechanism for Ankle Rehabilitation	FRONTIERS IN NEUROROBOTICS										ankle rehabilitation; parallel robot; mechanical design; performance indices; performance evaluation	ROBOT-AIDED NEUROREHABILITATION; FOOT ORTHOSIS; DROP	Repetitive and intensive physiotherapy is indispensable to patients with ankle disabilities. Increasingly robot-assisted technology has been employed in the treatment to reduce the burden of the therapists and the related costs of the patients. This paper proposes a configuration of a wearable parallel mechanism to supplement the equipment selection for ankle rehabilitation. The kinematic analysis, i.e., the inverse position solution and Jacobian matrices, is elaborated. Several performance indices, including the reachable workspace index, motion isotropy index, force transfer index, and maximum torque index, are developed based on the derived kinematic solution. Moreover, according to the proposed kinematic configuration and wearable design concept, the mechanical structure that contains a basic machine-drive system and a multi-model position/force data collection system is designed in detail. Finally, the results of the performance evaluation indicate that the wearable parallel robot possesses sufficient motion isotropy, high force transfer performance, and large maximum torque performance within a large workspace that can cover all possible range of motion of human ankle complex, and is suitable for ankle rehabilitation.																	1662-5218					FEB 18	2020	14								9	10.3389/fnbot.2020.00009													
J								Multi-task Generative Adversarial Network for Detecting Small Objects in the Wild	INTERNATIONAL JOURNAL OF COMPUTER VISION										Small object detection; Small face detection; Super-resolution network; Multi-task generative adversarial network; COCO; WIDER FACE		Object detection results have been rapidly improved over a short period of time with the development of deep convolutional neural networks. Although impressive results have been achieved on large/medium sized objects, the performance on small objects is far from satisfactory and one of remaining open challenges is detecting small object in unconstrained conditions (e.g. COCO and WIDER FACE benchmarks). The reason is that small objects usually lack sufficient detailed appearance information, which can distinguish them from the backgrounds or similar objects. To deal with the small object detection problem, in this paper, we propose an end-to-end multi-task generative adversarial network (MTGAN), which is a general framework. In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each inputted image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training process. Extensive experiments on the challenging COCO and WIDER FACE datasets demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods by a large margin.																	0920-5691	1573-1405				JUN	2020	128	6					1810	1828		10.1007/s11263-020-01301-6		FEB 2020											
J								Unified Binary Generative Adversarial Network for Image Retrieval and Compression	INTERNATIONAL JOURNAL OF COMPUTER VISION										Binary codes; Image retrieval; Image compression; Generative adversarial network	QUANTIZATION; CODES	Binary codes have often been deployed to facilitate large-scale retrieval tasks, but not that often for image compression. In this paper, we propose a unified framework, BGAN+, that restricts the input noise variable of generative adversarial networks to be binary and conditioned on the features of each input image, and simultaneously learns two binary representations per image: one for image retrieval and the other serving as image compression. Compared to related methods that attempt to learn a single binary code serving both purposes, we demonstrate that choosing for two codes leads to more effective representations due to less concessions needed when balancing the requirements. The added value of using a unified framework compared to two separate frameworks lies in the synergy in data representation that is beneficial for both learning processes. When devising this framework, we also address another challenge in learning binary codes, namely that of learning supervision. While the most striking successes in image retrieval using binary codes have mostly involved discriminative models requiring labels, the proposed BGAN+ framework learns the binary codes in an unsupervised fashion, yet more effectively than the state-of-the-art supervised approaches. The proposed BGAN+ framework is evaluated on three benchmark datasets for image retrieval and two datasets on image compression. The experimental results show that BGAN+ outperforms the existing retrieval methods with significant margins and achieves promising performance for image compression, especially for low bit rates.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2243	2264		10.1007/s11263-020-01305-2		FEB 2020											
J								Anchor-Based Self-Ensembling for Semi-Supervised Deep Pairwise Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION										Anchor; Self-ensembling; Deep pairwise hasing; Semi-seupervised; Large-scale	IMAGE RETRIEVAL; SCENE	Deep hashing has attracted considerable attention to tackle large-scale retrieval tasks, because of automatic and powerful feature extraction of convolutional neural networks and the gain of hashing in computation and storage costs. Most current supervised deep hashing methods only utilize the semantic information of labeled data without exploiting unlabeled data. However, data annotation is expensive and thus only scarce labeled data are available, which are difficult to represent the true distribution of all data. In this paper, we propose a novel semi-supervised deep pairwise hashing method to leverage both labeled and unlabeled data to learn hash functions. Our method utilizes the transduction of anchors to preserve the pairwise similarity relationship among both labeled and unlabeled samples. Additionally, to explore the semantic similarity information hidden in unlabeled data, it adopts self-ensembling to create strong ensemble targets for latent binary vectors of training samples and form a consensus predicting similarity relationship to multiple anchors. Unlike previous pairwise based hashing methods without maintaining the relevance among similar neighbors, we further explain and exhibit the capability of our method on preserving their relevance through calculating their similarities to anchors. Finally, extensive experiments on benchmark databases demonstrate the superior performance of the proposed method over recent state-of-the-art hashing methods on multiple retrieval tasks. The source codes of the proposed method are available on: .																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2307	2324		10.1007/s11263-020-01299-x		FEB 2020											
J								On spatial keyword covering	KNOWLEDGE AND INFORMATION SYSTEMS										mCK query; Spatial keyword cover; Spatial keyword search	SEARCH	This article introduces and solves a spatial keyword cover problem (SK- Cover for short), which aims to identify the group of spatio-textual objects covering all the keywords in a query andminimizing a distance cost function that leads to fewer objects in the answer set. In a broad sense, SK- Cover has been actively studied in the literature of spatial keyword search, such as the m-closest keywords query and the collective spatial keyword query. However, these existing works focus on minimizing only the largest pairwise distance even though the actual spatial cost is highly influenced by the number of objects in the answer group. Motivated by this, the present article further generalizes the problem definition in such a way that the total cost takes the cardinality of the group as well as the spatial distance. We prove that SKCover is not only NP-hard but also does not allow an approximation better than O(log |T |) in polynomial time, where T is the set of query keywords. We first establish an O(log |T |)approximation algorithm, which is asymptotically optimal in terms of the approximability of SK- Cover, together with effective accessing strategies and pruning rules to improve the overall efficiency and scalability. Despite the NP-hardness of SK- Cover, this article also develops exact solutions that find the optimal group of objects in a reasonably fast manner in practice, especially when it is required to cover a relatively small number of query keywords. In addition to our algorithmic results, we empirically show that our approximation algorithm always achieves the best accuracy and the efficiency comparable to that of a state-of-the-art algorithm intended for mCK, a problem similar to yet theoretically easier than SK- Cover, and also demonstrate that our exact algorithm using the proposed approximation scheme runs much faster than the baseline algorithm adapted from the existing solution for mCK.																	0219-1377	0219-3116				JUL	2020	62	7					2577	2612		10.1007/s10115-020-01446-3		FEB 2020											
J								Random walk-based entity representation learning and re-ranking for entity search	KNOWLEDGE AND INFORMATION SYSTEMS										Linked Data; Graph analysis; Entity representation learning; PageRank-based re-ranking; Random walk with restart; Entity search		Linked Data (LD) has become a valuable source of factual records, and entity search is a fundamental task in LD. The task is, given a query consisting of a set of keywords, to retrieve a set of relevant entities in LD. The state-of-the-art approaches for entity search are based on information retrieval techniques. This paper first examines these approaches with a traditional evaluation metric, recall@k, to reveal their potential for improvement. To obtain evidence for the potentials, an investigation is carried out on the relationship between queries and answer entities in terms of path lengths on a graph of LD. On the basis of the investigation, learning representations of entities are dealt with. The existing methods of entity search are based on heuristics that determine relevant fields (i.e., predicates and related entities) to constitute entity representations. Since the heuristics require burdensome human decisions, this paper is aimed at removing the burden with a graph proximity measurement. To this end, in this paper, RWRDoc is proposed. It is an RWR (random walk with restart)-based representation learning method that learns representations of entities by using weighted combinations of representations of reachable entities w.r.t. RWR. RWRDoc is mainly designed to improve recall scores; therefore, as shown in experiments, it lacks capability in ranking. In order to improve the ranking qualities, this paper proposes a personalized PageRank-based re-ranking method, PPRSD (Personalized PageRank-based Score Distribution), for the retrieved results. PPRSD distributes relevance scores calculated by text-based entity search methods in a personalized PageRank manner. Experimental evaluations showcase that RWRDoc can improve search qualities in terms of recall@1000 and PPRSD can compensate for RWRDoc's insufficient ranking capability, and the evaluations confirmed this compensation.																	0219-1377	0219-3116				AUG	2020	62	8					2989	3013		10.1007/s10115-020-01445-4		FEB 2020											
J								From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the World of AI	KUNSTLICHE INTELLIGENZ											CARLO TREE-SEARCH; DEEP NEURAL-NETWORKS; LEVEL; GO; NEUROEVOLUTION	This paper reviews the field of Game AI, which not only deals with creating agents that can play a certain game, but also with areas as diverse as creating game content automatically, game analytics, or player modelling. While Game AI was for a long time not very well recognized by the larger scientific community, it has established itself as a research area for developing and testing the most advanced forms of AI algorithms and articles covering advances in mastering video games such as StarCraft 2 and Quake III appear in the most prestigious journals. Because of the growth of the field, a single review cannot cover it completely. Therefore, we put a focus on important recent developments, including that advances in Game AI are starting to be extended to areas outside of games, such as robotics or the synthesis of chemicals. In this article, we review the algorithms and methods that have paved the way for these breakthroughs, report on the other important areas of Game AI research, and also point out exciting directions for the future of Game AI.																	0933-1875	1610-1987															10.1007/s13218-020-00647-w		FEB 2020											
J								Selective Embedding with Gated Fusion for 6D Object Pose Estimation	NEURAL PROCESSING LETTERS										Deep learning; RGB-D; Pose estimation; Gate fusion; Local features; Point clouds	EFFICIENT RANSAC; RECOGNITION	Deep learning method for 6D object pose estimation based on RGB image and depth (RGB-D) has been successfully applied to robot grasping. The fusion of RGB and depth is one of the most important difficulties. Previous works on the fusion of these two features are mostly concatenated together without considering the different contributions of the two types of features to pose estimation. We propose a selective embedding with gated fusion structure called SEGate, which can adjust the weights of RGB and depth features adaptively. Furthermore, we aggregate the local features of point clouds according to the distance between them. More specifically, the close point clouds contribute a lot to local features, while the distant point clouds contribute a little. Experiments show that our approach achieves the state-of-art performance in both LineMOD and YCB-Video datasets. Meanwhile, our approach is more robust to the pose estimation of occluded objects.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2417	2436		10.1007/s11063-020-10198-8		FEB 2020											
J								Constrained PSO Based Center Selection for RBF Networks Under Concurrent Fault Situation	NEURAL PROCESSING LETTERS										Radial basis function (RBF); Center selection; Concurrent fault situation; Fault tolerance	DESIGN; PREDICTION	In the training of radial basis function (RBF) networks, one important issue is to select RBF centers before constructing the networks. Most existing center selection methods are designed for the fault-free situation only. However, as the implementation of the networks may be perturbed by faults, these algorithms may lead to networks with degraded performance. This paper considers the center selection problem for RBF networks under the concurrent fault situation where multiplicative weight noise and open weight fault exist simultaneously. In particular, we introduce a binary label vector indicating the centers selected from training samples. Using the label vector, the fault-tolerant RBF model under the concurrent fault situation is reformulated as a constrained optimization problem, so that fault-tolerance can be considered in the procedure of center selection. To solve this constrained optimization problem, a constrained particle swarm optimization based algorithm is developed to select centers and train the network simultaneously. Simulation results show that the proposed algorithm is superior than state-of-the-art center selection algorithms.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2437	2451		10.1007/s11063-020-10202-1		FEB 2020											
J								Optimal control of DAB converter backflow power based on phase-shifting strategy	SOFT COMPUTING										DC-DC converter; Phase-shift control; Optimization; Backflow power	DC-DC CONVERTER; REACTIVE POWER; DESIGN	In order to solve the problems such as high backflow power and current stress of bidirectional DC-DC converter under the traditional single phase-shifting control, an optimized dual phase-shifting control method is proposed. Compared with the conventional phase-shifting control, this method can not only reduce the loss of the converter but also increase the flexibility of the control and realize full voltage range transmission. Firstly, the working principle of dual phase-shifting control and the mathematical model of backflow power are analyzed in detail. Then, the different operating range of return power according to the range of transmission power and voltage transformation ratio are derived. So as to achieve the goal of optimal operation of return power in full mode, the optimal solution of local return power and the corresponding combination of phase-shifting angle are found out. Finally, the superiorities of the control strategy are verified by the experimental results.																	1432-7643	1433-7479				APR	2020	24	8			SI		6031	6038		10.1007/s00500-020-04715-z		FEB 2020											
J								A simple two-phase differential evolution for improved global numerical optimization	SOFT COMPUTING										Evolutionary algorithm; Differential evolution; SHADE; L-SHADE; jSO	ALGORITHM; ENSEMBLE; MUTATION; CROSSOVER	In the evolutionary computing community, differential evolution (DE) is well appreciated as a simple yet versatile population-based, non-convex optimizer designed for continuous optimization problems. A simple two-phase DE algorithm is presented in this article, which aims to identify promising basins of attraction on a non-convex functional landscape in the first phase, and starting from those previously identified search regions, a success history-based switch parameter DE is employed to further fine tune the search process leading to the optima of the landscape. Our proposed framework has been validated on the well-known IEEE Congress on Evolutionary Computation (CEC) benchmark suites (CEC 2013, 2014 and 2017). Results of the proposed method are compared with corresponding CEC winners (SHADE for CEC 2013, L-SHADE for CEC 2014 and jSO for CEC 2017).																	1432-7643	1433-7479				APR	2020	24	8			SI		6151	6167		10.1007/s00500-020-04750-w		FEB 2020											
J								Homographic pun location using multi-dimensional semantic relationships	SOFT COMPUTING										Homographic pun; Pun location; Multi-dimensional semantic; SemEval2017 Task7	TRANSPARENCY; HUMOR	Homographic pun has been developed into a new research area as an important branch of humor research, being a common source of humor in jokes and other comedic works. Pun word is the key to better understand homographic pun. However, in order to construct automatic model for locating the pun from homographic pun, it remains difficult challenges because of the ambiguity and confusion. In this paper, we firstly introduce several multi-dimensional semantic relationships of homographic pun based on the relevant theory and then employ a novel effective un-supervised semantic similarity match approach MSRLP that depending on the multi-dimensional semantic relationships to locate the pun in a homographic pun. Performance evaluation demonstrates that our presented approach significantly achieves the state-of-the-art performance on the public SemEval2017 Task7 dataset, outperforming a number of strong baselines by at least 3.67% in F1-score measure.																	1432-7643	1433-7479				AUG	2020	24	16					12163	12173		10.1007/s00500-019-04654-4		FEB 2020											
J								On some pseudometrics in the intuitionistic fuzzy environment	SOFT COMPUTING										Pseudometric space; Intuitionistic fuzzy sets; Intuitionistic fuzzy numbers; Interval-valued intuitionistic fuzzy sets	DISTANCE MEASURE; SIMILARITY MEASURES; SETS	Three novel pseudometrics on the set of intuitionistic fuzzy numbers, called intuitionistic fuzzy cumulative pseudometrics, are first proposed. Then, the unified method to calculate the distances between intuitionistic fuzzy sets, interval-valued intuitionistic fuzzy sets and high-dimensional intuitionistic fuzzy sets based on such pseudometric is presented, and corresponding proofs are given.																	1432-7643	1433-7479															10.1007/s00500-020-04705-1		FEB 2020											
J								An MDD-based SAT encoding for pseudo-Boolean constraints with at-most-one relations	ARTIFICIAL INTELLIGENCE REVIEW										Pseudo-Boolean; Decision diagram; At-most-one; Exactly-one; Implication chain; SAT; Encodings	PROJECT SCHEDULING PROBLEM	Pseudo-Boolean (PB) constraints are ubiquitous in Constraint Satisfaction Problems. Encoding such constraints to SAT has proved to be an efficient solving approach. A commonly used technique for encoding PB constraints consists in representing the constraint as a Binary Decision Diagram (BDD), and then encoding this BDD to SAT. A key point in this technique is to obtain small BDD representations to generate small SAT formulas. In many problems, some subsets of the Boolean variables occurring in a PB constraint may also have other constraints imposed on them. In this work we introduce a way to take advantage of those constraints in order to obtain smaller SAT encodings. The main idea is that decision diagrams may be smaller if they avoid to represent truth assignments that are already forbidden by some other constraints. In particular, we present encodings for monotonic decreasing PB constraints, in conjunction with other constraints such as at-most-one, exactly-one and implication chains on subsets of their variables. We provide empirical evidence of the usefulness of this technique to reduce the size of the encodings as well as the solving time.																	0269-2821	1573-7462				OCT	2020	53	7					5157	5188		10.1007/s10462-020-09817-6		FEB 2020											
J								Error dynamic shaping in HIV optimized drug delivery control	EVOLVING SYSTEMS										HIV; Error dynamic shaping; Drug delivery	NONLINEAR-SYSTEMS; INFECTION; MODEL; HIV/AIDS; IDENTIFIABILITY; OBSERVER; THERAPY	Recently the use of control theory methods in drug delivery problems for HIV infection models has been considered. The treatment goal is the regulation of infected CD4+ T cells concentration to a predefined value by controlling the amount of medicine taken by the patient. In this field, optimal drug delivery, uncertain parameters and nonlinear behavior of the patient models are challenges, which are considered in previous works. In this study, error dynamic shaping (EDS) has been applied to the nonlinear HIV model with uncertain parameters to suppress the state of variables to follow the desired paths. Also, the genetic algorithm has been used to determine free parameters in the controller to minimize the dose of medicine. Robustness of the closed-loop and constrained signal control have been analyzed in this paper. The results indicate that the superiority of the proposed controller in the robustness of the model in comparison with previous studies.																	1868-6478	1868-6486															10.1007/s12530-020-09329-2		FEB 2020											
J								Nonlinear time-series modeling of feed drive system based on motion states classification	JOURNAL OF INTELLIGENT MANUFACTURING										Feed drive system; Motion states classification; NAR-LSTM network; Time-series modeling; Prediction	CONTOUR ERROR REDUCTION; NETWORK; TEMPERATURE	This paper proposed a novel modeling method using the running process data, i.e., the reference input positions and the actual output positions, based on the Naive Bayes method and a nonlinear autoregressive long-short term memory network (i.e., the NAR-LSTM) to address the nonlinear time-series modeling problem for increasing the prediction accuracy of the model of CNC machine feed drive system. A Naive Bayes based motion states classifier (i.e., the NB-MSC) is proposed to automatically classify the motion states with knowledge of dynamic characteristics of the feed drive system for constructing the submodels of different motion states (startup state, reverse state, etc.). In addition, a model based multi-objective optimization method is presented to extract samples for the training of the NB-MSC. Then by modifying the basic long-short term memory (LSTM) network, the NAR-LSTM network is proposed to construct those submodels. Compared to the existing modeling methods via dynamic analysis, the proposed method is better because it can achieve higher prediction accuracy on highly nonlinear motion states such as the reverse state. To validate the proposed methods, a set of experiments are conducted to prove the feasibility of the feed drive model as well as the advantages of the NB-MSC and the NAR-LSTM network in improving the modeling performance.																	0956-5515	1572-8145															10.1007/s10845-020-01546-5		FEB 2020											
J								A Visual Interface Tool for Development of Quadrotor Control Strategies	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned aerial vehicles; Nonlinear control systems; Graphical user interfaces	TRAJECTORY GENERATION; TRACKING; FLIGHT	Quadrotor control is an exciting research area due to its inherent non-linearity, the variety of tasks to be performed, and the wide scope of control strategies. Despite several works have been published, some aspects must be considered before implementation: How a quadrotor will operate in challenging trajectories, how to define trajectory limits, or how changing the device size will affect its performance. A complete user-friendly development platform is presented, where any device may be tested just by setting a few parameters. Typing a set of waypoints and their corresponding times, the tool calculates the optimal trajectory with minimum snap. For the defined waypoints, the control gains are tuned for a chosen control strategy using Particle Swarm Optimization (PSO). Three control strategies are available, but the tool was developed in modular form and opened to others. After tuned the gains the control performance may be visually evaluated through a graphical user interface (GUI). The quadrotor model considers the effect of air drag and propellers' speed dynamics. Measurement noise and limits for propeller speeds were also considered. Results demonstrate how this tool helps to initiate undergraduate and beginner graduate students in this research area, testing new control strategies, defining limits in challenging trajectories, and determining quadrotor characteristics for specific applications. The tool is available in GitHub (Martins and Rendon 2019).																	0921-0296	1573-0409															10.1007/s10846-019-01142-7		FEB 2020											
J								Using eye-tracking into decision makers evaluation in evolutionary interactive UA-FLP algorithms	NEURAL COMPUTING & APPLICATIONS										User ergonomics; User fatigue; Engineering design; UA-FLP; Artificial neural networks	FACILITY LAYOUT PROBLEM; ANT COLONY OPTIMIZATION; TABU SEARCH; GENETIC ALGORITHM; HEURISTIC ALGORITHM; SINGLE; DESIGN; REPRESENTATION; EXPERTISE; INPUT	Unequal area facility layout problem is an important issue in the design of industrial plants, as well as other fields such as hospitals or schools, among others. While participating in an interactive designing process, the human user is required to evaluate a high number of proposed solutions, which produces them fatigue both mental and physical. In this paper, the use of eye-tracking to estimate user's evaluations from gaze behavior is investigated. The results show that, after a process of training and data taking, it is possible to obtain a good enough estimation of the user's evaluations which is independent of the problem and of the users as well. These promising results advice to use eye-tracking as a substitute for the mouse during users' evaluations.																	0941-0643	1433-3058				SEP	2020	32	17					13747	13757		10.1007/s00521-020-04781-2		FEB 2020											
J								Two approximation algorithms for probabilistic coalition structure generation with quality bound	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multi-agent systems; Cooperative game theory; Coalition structure generation; Uncertainty; Bounded approximation algorithm	COOPERATIVE GAMES	How to form effective coalitions is an important issue in multi-agent systems. Coalition Structure Generation is a fundamental problem whose formalization can encompass various applications related to multi-agent cooperation. involves partitioning a set of agents into coalitions such that the social surplus (i.e., the sum of the values of all coalitions) is maximized. In traditional , we are guaranteed that all coalitions will be successfully established, that is, the attendance rate of each agent for joining any coalition is assumed to be 1.0. Having the real world in mind, however, it is natural to consider the uncertainty of agents' availabilities, e.g., an agent might be available only two or three days a week because of his/her own schedule. Probabilistic Coalition Structure Generation is an extension of where the attendance type of each agent is considered. The aim of this problem is to find the optimal coalition structure which maximizes the sum of the expected values of all coalitions. In , since finding the optimal coalition structure easily becomes intractable, it is important to consider approximation algorithms, i.e., to consider a trade-off between the quality of the returned solution and tractability. In this paper, a formal framework for is introduced. Approximation algorithms for called Bounded Approximation Algorithm based on Attendance Types and Involved are then presented. We prove a priori bounds on the quality of the solution returned by and with respect to the optimum and perform experimental evaluations on a number of benchmarks.																	1387-2532	1573-7454				FEB 18	2020	34	1							25	10.1007/s10458-020-09449-8													
J								Improvement of battery lifetime in software-defined network using particle swarm optimization based cluster-head gateway switch routing protocol with fuzzy rules	COMPUTATIONAL INTELLIGENCE										battery lifetime; CGSR; fuzzy rules; network lifetime; PSO; software-defined networks	ALGORITHM	The software-defined network (SDN) is one of the network architectures, in which the data plane and control plane is divided from each other, and the network can be handled using a sensibly centralized controller and this method is adopted to reconfigure the wireless sensor network automatically. In this article, to implement the SDN in MANET, in which control nodes can be chosen in SDN dynamically for the activation of MANET function to allocate the works to other mobile nodes to the base station. However, in the field of mobile ad hoc networks, the network lifetime, and battery lifetime is one of the major problems and the energy consumption can play a significant rule for the transmission of data in the SDN. Therefore, in this article, particle swarm optimization (PSO) based CGSR (cluster-head gateway switch routing protocol) algorithm with fuzzy rules is proposed to increase the network lifetime of battery powered mobile nodes by reducing the energy consumptions of each node in software-defined MANET. In this proposed method, a routing method that can permit various mobile nodes with low battery power to transmits the data from source node to base station. We design a PSO based CGSR routing protocol by selecting the routing mobile nodes using fuzzy rules for packet transmission. In CGSR process, the formation of cluster and selection of cluster head is executed depending on the particle swarm optimization method. This proposed routing protocol can be used to enhance the battery lifetime by extension of the network lifetime with numerical analysis for efficient route node selection.																	0824-7935	1467-8640				MAY	2020	36	2					813	823		10.1111/coin.12271		FEB 2020											
J								A connectionist account of the relational shift and context sensitivity in the development of generalisation	CONNECTION SCIENCE										Analogy; similarity; relational shift; distributed connectionist model; generalisation; statistical learning	CAUSAL RELATIONS; INDUCTION; ANALOGY; KNOWLEDGE	Similarity-based generalisation is fundamental to human cognition, and the ability to draw analogies based on relational similarities between superficially different domains is crucial for reasoning and inference. Learning to base generalisation on shared relations rather than (or in the face of) shared perceptual features has been identified as an important developmental milestone. However, recent research has highlighted the context-sensitivity of generalisation: children and adults use perceptual similarity to make inferences in some cases and relational similarity in others, a finding that suggests people track the predictive validity of different types of inferences. Here we demonstrate that this pattern of behaviour naturally emerges over the course of development in a domain-general statistical learning model that employs distributed, sub-symbolic representations. We suggest that this model offers a parsimonious account of the development of context-sensitive, similarity-based generalisation and may provide several advantages over other popular structured or symbolic approaches to modelling relational inference.																	0954-0091	1360-0494															10.1080/09540091.2020.1728519		FEB 2020											
J								Prediction of component concentrations in sodium aluminate liquor using stochastic configuration networks	NEURAL COMPUTING & APPLICATIONS										Stochastic configuration networks; Industrial data modelling; Component concentrations; Sodium aluminate liquor	ELECTRICAL-CONDUCTIVITY; PRIOR KNOWLEDGE; TEMPERATURE; HYDROXIDE; MODEL	Online measuring of component concentrations in sodium aluminate liquor is essential and important to Bayer alumina production process. They are the basis of closed- loop control and optimization and affect the final product quality. There are three main components in sodium aluminate liquor, termed caustic hydroxide, alumina and sodium carbonate (their concentrations are represented by c(K), c(A) and c(C), respectively). They are obtained off-line by titration analysis and suffered from larger time delays. To solve this problem, a hybrid model for cK and cA is proposed by combining a mechanism model and a stochastic configuration network (SCN) compensation model. An SCN-based model for c(C) is also proposed using the estimated values of c(K) and c(A) from the hybrid model. A real-world application conducted in Henan Branch of China Aluminum Co. Ltd demonstrates the effectiveness of the proposed modelling techniques. Experimental results show that our proposed method performs favourably in terms of the prediction accuracy, compared against the regress model, BP neural networks, RBF neural networks and random vector functional link model.																	0941-0643	1433-3058				SEP	2020	32	17					13625	13638		10.1007/s00521-020-04771-4		FEB 2020											
J								System-Level Non-interference of Constant-Time Cryptography. Part II: Verified Static Analysis and Stealth Memory	JOURNAL OF AUTOMATED REASONING										Non-interference; Cache-based attacks; Constant-time cryptography; Stealth memory; Coq	TIMING-ATTACK; COUNTERMEASURES; AES	This paper constitutes the second part of a paper published in Barthe et al. (J Autom Reason, 2017. 10.1007/s10817-017-9441-5). Cache-based attacks are a class of side-channel attacks that are particularly effective in virtualized or cloud-based environments, where they have been used to recover secret keys from cryptographic implementations. One common approach to thwart cache-based attacks is to use constant-time implementations, i.e. those which do not branch on secrets and do not perform memory accesses that depend on secrets. However, there is no rigorous proof that constant-time implementations are protected against concurrent cache-attacks in virtualization platforms with shared cache. We propose a new information-flow analysis that checks if an x86 application executes in constant-time, and show that constant-time programs do not leak confidential information through the cache to other operating systems executing concurrently on virtualization platforms. Our static analysis targets the pre-assembly language of the CompCert verified compiler. Its soundness proof is based on a connection between CompCert semantics and our idealized model of virtualization, and uses isolation theorems presented in Part I. We then extend our model of virtualization platform and our static analysis to accommodate stealth memory, a countermeasure which provisions a small amount of private cache for programs to carry potentially leaking computations securely. Stealth memory induces a weak form of constant-time, called S-constant-time, which encompasses some widely used cryptographic implementations. Our results provide the first rigorous analysis of stealth memory and S-constant-time, and the first tool support for checking if applications are S-constant-time. We formalize our results using the Coq proof assistant and we demonstrate the effectiveness of our analyses on cryptographic implementations, including PolarSSL AES, DES and RC4, SHA256 and Salsa20.																	0168-7433	1573-0670				DEC	2020	64	8					1685	1729		10.1007/s10817-020-09548-x		FEB 2020											
J								Coordination of production planning and distribution in closed-loop supply chains	NEURAL COMPUTING & APPLICATIONS										Closed-loop supply chains; Production routing problem; Closed-loop supply chain integration; Meta-heuristics	INTEGRATED PRODUCTION; MULTIPRODUCT PRODUCTION; ROUTING PROBLEM; INVENTORY; OPTIMIZATION; ALGORITHM; DELIVERY; RECOVERY; NETWORK; SEARCH	A closed-loop supply chain structure organises material and information flows from origin points to consumption points, including production, recycling, disposal, and other reverse logistic activities. Some integration problems arise with this structure including production, inventory, location, routing, distribution, collection, recycling, and routing. The integration problems that are facing scientific researchers include inventory routing, location routing, and location inventory. This study considers the integration problem of a closed-loop supply chain for the production, distribution, collection, and recycling quantities, along with the distribution and collection routes for each time period of a finite planning horizon. We refer to this problem as the "Closed-Loop Supply Chain Integrated Production-Inventory-Distribution-Routing Problem" (CLSC-PRP). A mathematical model is proposed that is the first to determine both quantities and routes for the CLSC-PRP simultaneously. As the problem is known to be NP-hard in terms of computational complexity, a simulated annealing-based decomposition heuristic is developed for solving large-scale CLSC-PRP instances. The results of the proposed mathematical model for the CLSC-PRP are compared with the results of the developed heuristic and two separate models that manage forward and backward production routing problems. An extensive comparative study indicated the following: (i) the proposed model was able to reduce the cost required for operating the total supply chain by an average of 12%, along with providing a positive impact on the environment and (ii) the proposed heuristic is able to generate solutions that are close to optimal in most cases.																	0941-0643	1433-3058				SEP	2020	32	17					13605	13623		10.1007/s00521-020-04770-5		FEB 2020											
J								Machine learning algorithms for improving security on touch screen devices: a survey, challenges and new perspectives	NEURAL COMPUTING & APPLICATIONS										Machine learning algorithms; Deep learning; Mobile phone touch screen; Android; Support vector machine; Command attention; Security	SUPPORT VECTOR MACHINE; CONTINUOUS AUTHENTICATION; RANDOM FOREST; CLASSIFICATION; INPUT	Mobile phone touch screen devices are equipped with high processing power and high memory. This led to users not only storing photos or videos but stored sensitive application such as banking applications. As a result of that the security system of the mobile phone touch screen devices becomes sacrosanct. The application of machine learning algorithms in enhancing security on mobile phone touch screen devices is gaining a tremendous popularity in both academia and the industry. However, notwithstanding the growing popularity, up to date no comprehensive survey has been conducted on machine learning algorithms solutions to improve the security of mobile phone touch screen devices. This survey aims to connect this gap by conducting a comprehensive survey on the solutions of machine learning algorithms to improve the security of mobile phone touch screen devices including the analysis and synthesis of the algorithms and methodologies provided for those solutions. This article presents a comprehensive survey and a new taxonomy of the state-of-the-art literature on machine learning algorithms in improving the security of mobile phone touch screen devices. The limitation of the methodology in each article reviewed is pointed out. Challenges of the existing approaches and new perspective of future research directions for developing more accurate and robust solutions to mobile phone touch screen security are discussed. In particular, the survey found that exploring of different aspects of deep learning solutions to improve the security of mobile phone touch screen devices is under-explored.																	0941-0643	1433-3058				SEP	2020	32	17					13651	13678		10.1007/s00521-020-04775-0		FEB 2020											
J								Impact of finite wavy wall thickness on entropy generation and natural convection of nanofluid in cavity partially filled with non-Darcy porous layer	NEURAL COMPUTING & APPLICATIONS										Entropy generation; Natural convection; Nanofluid-porous composite; Forchheimer model; Wavy solid wall; FEA	UPPER HORIZONTAL SURFACE; ALUMINA-WATER NANOFLUID; HEAT-TRANSFER; THERMAL-CONDUCTIVITY; MIXED CONVECTION; SQUARE CAVITY; FLOW; ENCLOSURE; TEMPERATURE; REVOLUTION	This paper investigates the natural convection inside a partially layered porous cavity with a heated wavy solid wall; the geometry is encountered in compact heat exchangers. Alumina nanoparticles are included in the water to enhance the heat exchange process. The incidental entropy generation is also studied to evaluate the thermodynamic irreversibility. The nanofluid flow is taken as laminar and incompressible while the advection inertia effect in the porous layer is taken into account by adopting the Darcy-Forchheimer model. The problem is explained in the dimensionless form of the governing equations and solved by the finite element method. The Darcy number (Da), porosity of the porous layer (epsilon), number of undulations (N), and the nanoparticles volume fraction (phi) are varied to assess the heat transfer and the incidental entropy generation. It is found that the waviness of the solid wall augments the average Nusselt number and minimizes the generation of entropy. The results show for some circumstances that the Nusselt number is augmented by 43.8% when N is raised from 0 (flat solid wall) to 4. It is also found that the porosity of the porous layer is a more crucial parameter than its permeability, where a 37.4% enhancement in the Nusselt number is achieved when the porosity is raised from 0.2 to 0.8.																	0941-0643	1433-3058				SEP	2020	32	17					13679	13699		10.1007/s00521-020-04776-z		FEB 2020											
J								UAM-RDE: an uncertainty analysis method for RSSI-based distance estimation in wireless sensor networks	NEURAL COMPUTING & APPLICATIONS										Uncertainty analysis; Distance estimation; Received signal strength indicator; Wireless sensor network		Distance estimation between sensor nodes is crucial to localization and object tracking in a wireless sensor network. The received signal strength indicator is widely used for wireless distance estimation, because of its advantages, including availability, low cost, flexibility, and so on. As is well known, RSSI measurement values are extremely susceptible to the surroundings, resulting in uncertain distance estimations. Without confidential information, the distance estimation results could not provide valuable priori information for follow-up processing methods and applications, such as localization, navigation, and guidance. In this paper, we propose a new uncertainty analysis method for RSSI-based distance estimation (UAM-RDE) to study the uncertainty propagation in RSSI-based distance estimations. In UAM-RDE, we explore the uncertainty propagation mechanism from the input to the output of the RSSI-based distance estimation, including uncertainty factor analysis, sensitivity analysis, propagation, and synthesis of uncertainty. The simulations and experimental results validate and demonstrate the feasibility of UAM-RDE.																	0941-0643	1433-3058				SEP	2020	32	17					13701	13714		10.1007/s00521-020-04777-y		FEB 2020											
J								From static to dynamic word representations: a survey	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Word representation; Static embedding; Dynamic embedding; Cross-lingual embedding		In the history of natural language processing (NLP) development, the representation of words has always been a significant research topic. In this survey, we provide a comprehensive typology of word representation models from a novel perspective that the development from static to dynamic embeddings can effectively address the polysemy problem, which has been a great challenge in this field. Then the survey covers the main evaluation metrics and applications of these word embeddings. And, we further discuss the development of word embeddings from static to dynamic in cross-lingual scenario. Finally, we point out some open issues and future works.																	1868-8071	1868-808X				JUL	2020	11	7					1611	1630		10.1007/s13042-020-01069-8		FEB 2020											
J								An adaptive kernelized rank-order distance for clustering non-spherical data with high noise	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Unsupervised learning; Clustering; Rank-order; Kernel similarity; Non-spherical data; Noise	LINKAGE	Clustering is a fundamental research topic in unsupervised learning. Similarity measure is a key factor for clustering. However, it is still challenging for existing similarity measures to cluster non-spherical data with high noise levels. Rank-order distance is proposed to well capture the structures of non-spherical data by sharing the neighboring information of the samples, but it cannot well tolerate high noise. In order to address above issue, we propose KROD, a new similarity measure incorporating rank-order distance with Gaussian kernel. By reducing the noise in the neighboring information of samples, KROD improves rank-order distance to tolerate high noise, thus the structures of non-spherical data with high noise levels can be well captured. Then, KROD strengthens these captured structures by Gaussian kernel so that the samples in the same cluster are closer to each other and can be easily clustered correctly. Experiment illustrates that KROD can effectively improve existing methods for discovering non-spherical clusters with high noise levels. The source code can be downloaded from .																	1868-8071	1868-808X				AUG	2020	11	8					1735	1747		10.1007/s13042-020-01068-9		FEB 2020											
J								The efficient fast-response content-based image retrieval using spark and MapReduce model framework	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MapReduce; Hadoop; CBIR; KNN algorithm; Spark	FEATURES	Content Based Image Retrieval (CBIR) is a way of querying image databases. CBIR looks at visual properties of an image as "search terms" and returns pictures from a database that share the same or almost similar visual properties. Most CBIR systems in the literature works by extracting the image color, texture and shape features before comparing them with those in the database and then compute the distance between features of images for retrieval purposes. In this proposed work, we use a MapReduce model framework to index the large-scale images and Spark has been used as a proportionate method of retrieving the index, which runs on the higher layer of MapReduce and Hadoop distributed file system (HDFS) environment. HDFS provides an in-memory data storage and fast retrieval mechanism using the indexing process. The image retrieval is performed in alignment with the K-Nearest Neighbour's model using Apache implementation. The processing time has been evaluated with the Hadoop framework in CBIR. The proposed approach takes 10% less time to index images than the distributed image segmentation method discussed in the literature.																	1868-5137	1868-5145															10.1007/s12652-020-01775-9		FEB 2020											
J								Cloud computing using load balancing and service broker policy for IT service: a taxonomy and survey	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Load balancing; Cloud service broker; Cloud resource scheduling	MIN-MIN ALGORITHM; OF-THE-ART; SCHEDULING ALGORITHMS; RESOURCE-MANAGEMENT; GENETIC ALGORITHM; VIRTUAL MACHINES; OPTIMIZATION; FRAMEWORK; INFRASTRUCTURE; INTEGRATION	Cloud computing is the big boom technology in IT industry infrastructure. Many people are moving to cloud computing because of dynamic allocation of resources and reduction in cost. Cloud computing delivers infrastructure, software, and platforms as a service to all consumers. But still, it has numerous issues related to performance unpredictability, resource sharing, security, storage capacity, availability of resources on each requirement, data confidentiality and many more. Load balancing and service brokering are the two main key areas, which ensures reliability, scalability, minimize response time, maximize throughput and cost in the cloud environment. These are the main things we have to focus to improve the performance of the computation. This survey paper presents a comparative and comprehensive study of various load balancing algorithm used in the load balancer and brokering policy used for each service and their scheduling types. The objectives of this survey is to (1) Determine, illustrate, compare and analyze newer methods developed for load balancing and service brokering (the most notable problem) by systematically reviewing papers from the year 2015 to 2018; (2) Classify and analyze techniques based on the key parameters in cloud computing techniques; (3) Ultimately set an updated, thorough and rigorous discussion on load balancing and service broker techniques so as to motivate and direct with valuable references for future research development and direction.																	1868-5137	1868-5145															10.1007/s12652-020-01747-z		FEB 2020											
J								Quality of service (QoS) and priority aware models for energy efficient and demand routing procedure in mobile ad hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										EEAODR; MANETS; Proactive & reactive routing method; QoS; Efficient way		Mobile ad hoc networks (MANETs) make them invigorate class of remote correspondence framework which can change positions in systems and unusual changes in arranging topology. AODV directing calculation demonstrates its preferences for contrast with other open methodologies yet additionally have a few downsides, for example, high overheads, all vitality utilization in a broad system that create the need of change. The introduced algorithm energy efficient ad-hoc on-demand routing (EEAODR) in this algorithm mainly focus traditional AODV giving responsibilities to keep up vitality stack among organizing hub for enhancing the system consistency. Also, because of high versatility, the steering conventions that are composed by the engineering of wired or cell systems are not adequate for Portable Impromptu Systems and perform inadequately. In this conventions set the base level of vitality way at whatever point a hub achieved the base level point. In this method found the least ideal way and dynamic node for foundation steering way. To demonstrate the centrality of the new proposed scheme, those are reenacted by customary, present day calculation above network system with different parameters and came about showing strength the outlined procedure.																	1868-5137	1868-5145															10.1007/s12652-020-01769-7		FEB 2020											
J								Toward intelligent continuous assistance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial intelligence; Human-assistive robotics; Human-robot interaction; Ambient intelligence	ELDERLY-PEOPLE; ENVIRONMENT; PERSPECTIVE; HELP; SOAR; HOME	Technology supported assistance is a research area dedicated to support both older adults and, at some level, their caregivers in a variety of situations and contexts. A number of projects doing detailed evaluation both with robots and/or ICT-based intelligent devices have identified as open challenges the need to guarantee both continuity and variability of service according to context interpretation. This paper starts from the willingness to study how both continuity and variability can be pursued by leveraging and integrating results from research areas like artificial intelligence (AI), cognitive systems, psychology and sensor networks. Some of these technological skills are needed for example by an assistive robot and still represent open challenges in AI. This paper presents a medium term research initiative aiming at synthesizing an enhanced (intelligent) control architecture for assistive robots that take advantage from the continuous flow of information provided by a sensor network. The paper presents two main results: (a) starting from the analysis of requirements coming from the real world, it envisages a conceptual cognitive architecture highlighting the functional requirements and the key capabilities characterizing an "ideal" intelligent assistive robot; (b) it presents a prototype of a testbed architecture called KOaLa (Knowledge-based cOntinuous Loop) which integrates sensor data representation, knowledge reasoning and decision making capabilities showing its novelty in a realistic scenario.																	1868-5137	1868-5145															10.1007/s12652-020-01766-w		FEB 2020											
J								Fuzzy approximations of a multiplicative inverse cubic functional equation	SOFT COMPUTING										Multiplicative inverse function; Multiplicative inverse functional equation; Multiplicative inverse quadratic functional equation; Generalized Hyers-Ulam-Rassias stability	ULAM-RASSIAS STABILITY	The aim of this study is to introduce a new multiplicative inverse cubic functional equation, to accomplish its general solution, to employ Hyers' method for solving its stability problems in Felbin's type fuzzy normed linear spaces, to present an apt example for justifying its stability result is invalid for singular case and to elucidate its interpretation through an application in electromagnetism.																	1432-7643	1433-7479				SEP	2020	24	17					13285	13292		10.1007/s00500-020-04741-x		FEB 2020											
J								An integrated fuzzy-genetic failure mode and effect analysis for aircraft wing reliability	SOFT COMPUTING										Reliability maintenance; FMEA; Genetic algorithm; Simulated Annealing algorithms	OPTIMIZATION	The purpose of this paper is to propose a model for assessing the hazard system based on the failure structure of aircraft wing. The model considers reliability maintenance and repair factors. Failure Mode and Effect Analysis (FMEA) is one of the well-known methods of quality management being used for continuous improvement in product or process plans. One serious issue of FMEA is the definition of the risk priorities of failure modes. This paper proposes an analytical method based on failure modes and failure analysis in criticality and suggests a fuzzy evaluation method for evaluating the risk level of the wing of aircraft. The analytical technique includes qualitatively analyzing the operation, failure modes, failure cause, failure rate, and severity through FMEA and quantitatively assessing the safety by using the fuzzy evaluation method. Fuzzy logic and Genetic Algorithms are integrated using a risk-cost model based on FMEA and comparisons with Simulated Annealing algorithms.																	1432-7643	1433-7479				SEP	2020	24	17					13401	13412		10.1007/s00500-020-04757-3		FEB 2020											
J								Ensemble learning with recursive feature elimination integrated software effort estimation: a novel approach	EVOLUTIONARY INTELLIGENCE										Ensemble learning; Recursive feature elimination; Software effort estimation; Machine learning		To develop software, estimating actual effort is important for any organization as there is no chance of getting either overestimation or underestimation. Due to the overestimation of effort, there may be an immediate need to compromise with the quality and testing. Similarly, underestimation may lead to allocating more resource. Compared to some of the early developed estimation techniques, machine learning based approaches are keen to estimate the effort more accurately due to their dynamic adaptivity with any type of data. With the rapid development of software products, many methods fail to satisfy the objective of development in an effective way. In this paper, a novel model based on ensemble learning and recursive feature elimination based method has been proposed to estimate the effort. With the feature ranking and selection method, the proposed method is able to estimate the efforts with the parameters like size and cost. Simulation results are encouraging with the proposed method with COCOMO II dataset.																	1864-5909	1864-5917															10.1007/s12065-020-00360-5		FEB 2020											
J								Dynamic hesitant fuzzy Bayesian network and its application in the optimal investment port decision making problem of "twenty-first century maritime silk road"	APPLIED INTELLIGENCE										Hesitant fuzzy set; Bayesian network; Dynamic decision making; Port investment	CORRELATION-COEFFICIENTS; RISK; OPTIMIZATION; UNCERTAINTY; PREDICTION; MARINE; MODEL; SETS	Most traditional decision making methods cannot deal with intensive uncertain data that varies with time effectively, and they only derive the static risk analysis by different aggregation operators. There is little research of the dynamic decision making problems with massive uncertain information. To manage with different kinds of uncertain knowledge and hesitancy in the dynamic decision-making process, this paper combines the advantages of hesitant fuzzy sets (HFSs) in depicting information and Bayesian Network (BN) in uncertain reasoning. Considering the uncertain information of risk factors varies with time, the concept of Dynamic Hesitant Fuzzy Bayesian Network (DHFBN) is proposed to deal with dynamic decision making problems under the hesitant fuzzy environment. Then, an improved Particle Swarm Optimization (PSO) algorithm and the Expectation-Maximization (EM) algorithm are adopted for the structure learning and parameters learning of DHFBN respectively. Based on the learned optimal DHFBN, a dynamic reasoning and prediction method is developed. Furthermore, a case about the optimal port investment decision making problem of "21st Century Maritime Silk Road" is presented to illustrate the application of the proposed method. Finally, we also conduct a comparative experiment to testify the validity and advantages of the method in detail.																	0924-669X	1573-7497				JUN	2020	50	6					1846	1858		10.1007/s10489-020-01647-x		FEB 2020											
J								Mathematical programming approach to formulate intuitionistic fuzzy regression model based on least absolute deviations	FUZZY OPTIMIZATION AND DECISION MAKING										Fuzzy regression model; Intuitionistic fuzzy number; Mathematical programming; Distance criterion		Fuzzy regression models are widely used to investigate the relationship between explanatory and response variables for many decision-making applications in fuzzy environments. To include more fuzzy information in observations, this study uses intuitionistic fuzzy numbers (IFNs) to characterize the explanatory and response variables in formulating intuitionistic fuzzy regression (IFR) models. Different from traditional solution methods, such as the least-squares method, in this study, mathematical programming problems are built up based on the criterion of least absolute deviations to establish IFR models with intuitionistic fuzzy parameters. The proposed approach has the advantages that the model formulation is not limited to the use of symmetric triangular IFNs and the signs of the parameters are determined simultaneously in the model formulation process. The prediction performance of the obtained models is evaluated in terms of similarity and distance measures. Comparison results of the performance measures indicate that the proposed models outperform an existing approach.																	1568-4539	1573-2908				JUN	2020	19	2					191	210		10.1007/s10700-020-09315-y		FEB 2020											
J								Unsupervised rotating machinery fault diagnosis method based on integrated SAE-DBN and a binary processor	JOURNAL OF INTELLIGENT MANUFACTURING										Fault diagnosis; Sparse autoencoder; Deep belief network; Bearing fault; Gear pitting fault	DEEP BELIEF NETWORK; SPARSE AUTOENCODER; REPRESENTATION	In recent years, deep learning based diagnostic approaches have become more attractive. However, most of these methods are supervised diagnostic approaches. Developing a supervised diagnostic model requires a large number of labeled training data. And it is time consuming and labor intensive to obtain labeled data for a variety of systems and working conditions. Therefore, an unsupervised diagnostic model that does not require labeled training data is more desirable. This paper proposes an unsupervised diagnostic model by integrating a sparse autoencoder, a deep belief network, and a binary processor. In comparison with the existing unsupervised methods, the proposed method does not need to perform statistical features extraction, and directly uses the normalized frequency domain signals as the inputs. Moreover, in the proposed diagnostic model, the input data is passed through layer by layer without fine-tuning, which is completely unsupervised process. The proposed methods have been validated with bearing fault datasets and gear pitting fault datasets. The validation results show that the proposed method has a higher accuracy for both bearing and gear pitting fault diagnosis.																	0956-5515	1572-8145															10.1007/s10845-020-01543-8		FEB 2020											
J								Geodesic Analysis in Kendall's Shape Space with Epidemiological Applications	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Longitudinal modeling; Shape trajectory; Riemannian metric; Principal geodesic analysis; Geodesic regression; Parallel transport; Jacobi fields	PARALLEL TRANSPORT; RIEMANNIAN-MANIFOLDS; DEFORMATION; REGRESSION	We analytically determine Jacobi fields and parallel transports and compute geodesic regression in Kendall's shape space. Using the derived expressions, we can fully leverage the geometry via Riemannian optimization and thereby reduce the computational expense by several orders of magnitude over common, nonlinear constrained approaches. The methodology is demonstrated by performing a longitudinal statistical analysis of epidemiological shape data. As an example application, we have chosen 3D shapes of knee bones, reconstructed from image data of the Osteoarthritis Initiative. Comparing subject groups with incident and developing osteoarthritis versus normal controls, we find clear differences in the temporal development of femur shapes. This paves the way for early prediction of incident knee osteoarthritis, using geometry data alone.																	0924-9907	1573-7683				MAY	2020	62	4					549	559		10.1007/s10851-020-00945-w		FEB 2020											
J								New Set of Non-separable Orthogonal Invariant Moments for Image Recognition	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Image classification; Orthogonal invariant moments; Rotation; scaling and translation invariants; Pattern recognition; Image retrieval	PATTERN-RECOGNITION; REPRESENTATION	It is known that the rotation, scaling and translation invariant property of image moments has a high significance in image recognition. For this reason, the seven invariant moments presented by Hu are widely used in the field of image analysis. These moments are of finite order; therefore, they do not comprise a complete set of image descriptors. For this reason, we introduce in this paper another series of invariant moments of infinite order, which are based on normalized central moments. The non-orthogonal property of these moments causes the redundancy of information. To overcome this problem, we propose a new construction technique of non-separable orthogonal polynomials in two variables based on a recurrence formula and we present a new set of orthogonal moments, which are invariant to translation, scaling and rotation. The presented approaches are tested in several well-known computer vision datasets including moment's invariability, image retrieval and classification of objects, this latter based on fuzzy K-means clustering algorithm. The performance of these invariant moments for classification and image retrieval is compared with some recent invariant moments such as invariants of multi-channel orthogonal radial-substituted Chebyshev moments, invariants of quaternion radial-substituted Chebyshev moments, invariants of rotational moments in Radon space and Legendre-Fourier moments in Radon space. The experimental results made using four databases of images, namely Columbia Object Image Library (COIL-20) database, MPEG7-CE shape database, COIL-100 database and ORL database, show that our orthogonal invariant moments have done better than the other descriptors tested.																	0924-9907	1573-7683				MAY	2020	62	4					606	624		10.1007/s10851-020-00948-7		FEB 2020											
J								Computing and testing Pareto optimal committees	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Committee selection; Multiwinner voting; Pareto optimality; Algorithms and complexity; Set extensions	PROPORTIONAL REPRESENTATION; COMPLEXITY	Selecting a set of alternatives based on the preferences of agents is an important problem in committee selection and beyond. Among the various criteria put forth for desirability of a committee, Pareto optimality is a minimal and important requirement. As asking agents to specify their preferences over exponentially many subsets of alternatives is practically infeasible, we assume that each agent specifies a weak order on single alternatives, from which a preference relation over subsets is derived using some preference extension. We consider five prominent extensions (responsive, downward lexicographic, upward lexicographic, best, and worst). For each of them, we consider the corresponding Pareto optimality notion, and we study the complexity of computing and verifying Pareto optimal outcomes. For each of the preference extensions, we give a complete characterization of the complexity of testing Pareto optimality when preferences are dichotomous or linear. We also consider strategic issues: for four of the set extensions, we present a linear-time, Pareto optimal and strategyproof algorithm that even works for weak preferences.																	1387-2532	1573-7454				FEB 17	2020	34	1							24	10.1007/s10458-020-09445-y													
J								Robust H-infinity performance for discrete time T-S fuzzy switched memristive stochastic neural networks with mixed time-varying delays	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Switched; memristive; H-infinity performance; Lyapunov-Krasovskii functional; neural networks	STABILITY ANALYSIS; SYSTEMS; SYNCHRONIZATION; CRITERIA; LEAKAGE	In this paper, we study the robust H-infinity performance for discrete-time T-S fuzzy switched memristive stochastic neural networks with mixed time-varying delays and switching signal design. The neural network under consideration is subject to time-varying and norm bounded parameter uncertainties. Decomposing of the delay interval approach is employed in both the discrete delays and distributed delays. By constructing a proper Lyapunov-Krasovskii functional (LKF) with triple summation terms and using an improved summation inequality techniques. Sufficient conditions are derived in terms of linear matrix inequalities (LMIs) to guarantee the considered discrete-time neural networks to be exponentially stable. Finally, numerical examples with simulation results are given to illustrate the effectiveness of the developed theoretical results.																	0952-813X	1362-3079															10.1080/0952813X.2020.1725649		FEB 2020											
J								Finding weaknesses in networks using Greedy Randomized Adaptive Search Procedure and Path Relinking	EXPERT SYSTEMS										critical nodes; GRASP; metaheuristic; Path Relinking; alpha-separator problem	SCATTER SEARCH	In recent years, the relevance of cybersecurity has been increasingly evident to companies and institutions, as well as to final users. Because of that, it is important to ensure the robustness of a network. With the aim of improving the security of the network, it is desirable to find out which are the most critical nodes in order to protect them from external attackers. This work tackles this problem, named the alpha-separator problem, from a heuristic perspective, proposing an algorithm based on the Greedy Randomized Adaptive Search Procedure (GRASP). In particular, a novel approach for the constructive procedure is proposed, where centrality metrics derived from social network analysis are used as a greedy criterion. Furthermore, the quality of the provided solutions is improved by means of a combination method based on Path Relinking (PR). This work explores different variants of PR, also adapting the most recent one, Exterior PR, for the problem under consideration. The combination of GRASP + PR allows the algorithm to obtain high-quality solutions within a reasonable computing time. The proposal is supported by a set of intensive computational experiments that show the quality of the proposal, comparing it with the most competitive algorithm found in the state of art.																	0266-4720	1468-0394														e12540	10.1111/exsy.12540		FEB 2020											
J								Building blocks for commodity augmented reality-based molecular visualization and modeling in web browsers	PEERJ COMPUTER SCIENCE										Molecular modeling; Integrative modeling; Virtual reality; Augmented reality; Chemistry; Education; Molecular visualization	VIRTUAL-REALITY; SIMULATIONS; SCIENCE; PROTEIN; HAND	For years, immersive interfaces using virtual and augmented reality (AR) for molecular visualization and modeling have promised a revolution in the way how we teach, learn, communicate and work in chemistry, structural biology and related areas. However, most tools available today for immersive modeling require specialized hardware and software, and are costly and cumbersome to set up. These limitations prevent wide use of immersive technologies in education and research centers in a standardized form, which in turn prevents large-scale testing of the actual effects of such technologies on learning and thinking processes. Here, I discuss building blocks for creating marker-based AR applications that run as web pages on regular computers, and explore how they can be exploited to develop web content for handling virtual molecular systems in commodity AR with no more than a webcam- and internet-enabled computer. Examples span from displaying molecules, electron microscopy maps and molecular orbitals with minimal amounts of HTML code, to incorporation of molecular mechanics, real-time estimation of experimental observables and other interactive resources using JavaScript. These web apps provide virtual alternatives to physical, plastic-made molecular modeling kits, where the computer augments the experience with information about spatial interactions, reactivity, energetics, etc. The ideas and prototypes introduced here should serve as starting points for building active content that everybody can utilize online at minimal cost, providing novel interactive pedagogic material in such an open way that it could enable mass-testing of the effect of immersive technologies on chemistry education.																	2376-5992					FEB 17	2020									e260	10.7717/peerj-cs.260													
J								Attribute based honey encryption algorithm for securing big data: Hadoop distributed file system perspective	PEERJ COMPUTER SCIENCE										Big data; Data security; And encryption-decryption; HDFS; Hadoop; Cloud storage	FRAMEWORK	Hadoop has become a promising platform to reliably process and store big data. It provides flexible and low cost services to huge data through Hadoop Distributed File System (HDFS) storage. Unfortunately, absence of any inherent security mechanism in Hadoop increases the possibility of malicious attacks on the data processed or stored through Hadoop. In this scenario, securing the data stored in HDFS becomes a challenging task. Hence, researchers and practitioners have intensified their efforts in working on mechanisms that would protect user's information collated in HDFS. This has led to the development of numerous encryption-decryption algorithms but their performance decreases as the file size increases. In the present study, the authors have enlisted a methodology to solve the issue of data security in Hadoop storage. The authors have integrated Attribute Based Encryption with the honey encryption on Hadoop, i.e., Attribute Based Honey Encryption (ABHE). This approach works on files that are encoded inside the HDFS and decoded inside the Mapper. In addition, the authors have evaluated the proposed ABHE algorithm by performing encryption-decryption on different sizes of files and have compared the same with existing ones including AES and AES with OTP algorithms. The ABHE algorithm shows considerable improvement in performance during the encryption-decryption of files.																	2376-5992					FEB 17	2020									e259	10.7717/peerj-cs.259													
J								AlphaLogger: detecting motion-based side-channel attack using smartphone keystrokes	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smartphone security; Keystroke inference; Side-channel attacks; Machine learning; Motion sensor		Due to the advancement in technologies and excessive usability of smartphones in various domains (e.g., mobile banking), smartphones became more prone to malicious attacks.Typing on the soft keyboard of a smartphone produces different vibrations, which can be abused to recognize the keys being pressed, hence, facilitating side-channel attacks. In this work, we develop and evaluate AlphaLogger- an Android-based application that infers the alphabet keys being typed on a soft keyboard. AlphaLogger runs in the background and collects data at a frequency of 10Hz/sec from the smartphone hardware sensors (accelerometer, gyroscope and magnetometer) to accurately infer the keystrokes being typed on the soft keyboard of all other applications running in the foreground. We show a performance analysis of the different combinations of sensors. A thorough evaluation demonstrates that keystrokes can be inferred with an accuracy of 90.2% using accelerometer, gyroscope, and magnetometer.																	1868-5137	1868-5145															10.1007/s12652-020-01770-0		FEB 2020											
J								Convolutional neural networks based focal loss for class imbalance problem: a case study of canine red blood cells morphology classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Imbalanced data; Red blood cell classification; Erythrocytes classification; Medical data; Deep learning	DATASETS; SMOTE	Morphologies of red blood cells are normally interpreted by a pathologist. It is time-consuming and laborious. Furthermore, a misclassified red blood cell morphology will lead to false disease diagnosis and improper treatment. Thus, a decent pathologist must truly be an expert in classifying red blood cell morphology. In the past decade, many approaches have been proposed for classifying human red blood cell morphology. However, those approaches have not addressed the class imbalance problem in classification. A class imbalance problem-a problem where the numbers of samples in classes are very different-is one of the problems that can lead to a biased model towards the majority class. Due to the rarity of every type of abnormal blood cell morphology, the data from the collection process are usually imbalanced. In this study, we aimed to solve this problem specifically for classification of dog red blood cell morphology by using a Convolutional Neural Network (CNN)-a well-known deep learning technique-in conjunction with a focal loss function, adept at handling class imbalance problem. The proposed technique was conducted on a well-designed framework: two different CNNs were used to verify the effectiveness of the focal loss function and the optimal hyperparameters were determined by fivefold cross-validation. The experimental results show that both CNNs models augmented with the focal loss function achieved higher F1-scores, compared to the models augmented with a conventional cross-entropy loss function that does not address class imbalance problem. In other words, the focal loss function truly enabled the CNNs models to be less biased towards the majority class than the cross-entropy did in the classification task of imbalanced dog red blood cell data.																	1868-5137	1868-5145															10.1007/s12652-020-01773-x		FEB 2020											
J								Screen-Light Decomposition Framework for Point-of-Gaze Estimation Using a Single Uncalibrated Camera and Multiple Light Sources	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Adaptive; Estimation; Normalized; Space; eye; Tracking; Calibration; Framework	CROSS-RATIO; TRACKING; EYE; SYSTEMS	The use of a single uncalibrated camera is desirable for eye tracking to reduce the overall complexity and cost of the system. Quite often, at least one external light source is used to enhance image quality and generate a corneal reflection used as a reference point to estimate the point-of-gaze (PoG). Though the use of more than one light source has shown to enhance accuracy and robustness to head motion, it is unlikely that all corneal reflections appear in the eye images during natural eye movements. In this paper, we introduce the Screen-Light Decomposition (SLD) framework as a generalized model for PoG estimation using a single uncalibrated camera and a variable number of light sources. SLD synthesizes existing uncalibrated video-based eye trackers and can be used as a modeling tool to compare and design eye trackers. We have used the framework to design a novel eye-tracking technique, called SAGE, for single normalized space adaptive gaze estimation, that can gracefully degrade the gaze tracker performance when one or more corneal reflections are not detected, even during the calibration procedure. Results from an user experiment are presented to demonstrate its improved performance over other designs.																	0924-9907	1573-7683				MAY	2020	62	4					585	605		10.1007/s10851-020-00947-8		FEB 2020											
J								Boolean lifting property in quantales	SOFT COMPUTING										Lifting property; Quantale; Reticulation; Boolean center; Normal; Semilocal	PRIME SPECTRUMS; REPRESENTATIONS	In ring theory, the lifting idempotent property (LIP) is related to some important classes of rings: clean rings, exchange rings, local and semilocal rings, Gelfand rings, maximal rings, etc. Inspired by LIP, lifting properties were also defined for other algebraic structures: MV-algebras, BL-algebras, residuated lattices, abelian l-groups, congruence distributive universal algebras, etc. In this paper, we define a lifting property (LP) in commutative coherent integral quantales, structures that are a good abstraction for lattices of ideals, filters and congruences. LP generalizes all the lifting properties existing in the literature. The main tool in the study of LP in a quantale A is the reticulation of A, a bounded distributive lattice whose prime spectrum is homeomorphic to the prime spectrum of A. The principal results of the paper include a characterization theorem for quantales with LP and a characterization theorem for hyperarchimedean quantales.																	1432-7643	1433-7479				APR	2020	24	8			SI		6169	6181		10.1007/s00500-020-04752-8		FEB 2020											
J								Knowledge-enhanced temporal word embedding for diachronic semantic change estimation	SOFT COMPUTING										Co-occurrence; Context; Dependency relation; Syntactic pattern; Semantic change; Word embedding		Historical, social and linguistic factors cause semantic changes that can narrow, broaden or completely alter the meanings of words. Frequency, syntactic and semantic variations are to be studied to examine such changes. Syntactic changes cannot be observed in many cases, if words have no POS variation. Context and connotation contribute more to semantic alteration. In addition, words have similar or related meanings in certain contexts, and the context is considered with diverse features such as co-occurrence and word association. The semantic change is generally related to the variation in n-gram context with a maximum of 5 g. However, distant context terms also play a prominent role in semantic change. There is also a link between the type of change and the use of lexical relations. This paper builds a knowledge-enhanced temporal word embedding model that utilizes 'word-centric dependency relations' for capturing context words irrespective of their n-gram position and 'syntactic patterns for lexical relations' for determining the type of semantic change. The joint learning of contexts with both dependency and lexical relations from diachronic corpora is performed to obtain temporal word embedding vectors. The proposed model outperforms other n-gram-based approaches when evaluated with standard diachronic corpora.																	1432-7643	1433-7479				SEP	2020	24	17					12901	12918		10.1007/s00500-020-04714-0		FEB 2020											
J								Energy demand forecasting using a novel remnant GM(1,1) model	SOFT COMPUTING										Energy demand; Grey prediction; GM(1; 1); Forecasting; Residual model	GREY PREDICTION MODEL; ELECTRICITY CONSUMPTION; CHINA	Grey prediction models play a significant role in forecasting energy demand, particularly the GM(1,1) model. To increase the prediction accuracy of the original GM(1,1) model, the corresponding residual GM(1,1) model is often recommended. However, the original and residual models that form the basis of the remnant grey prediction model are usually set up independently. In this work, we use a neural network to determine the degree to which a predicted value obtained from the original GM(1,1) model can be modified. A distinctive feature of our proposed prediction model is that the residual model is leveraged by providing a new adjustment mechanism for predicted values to maximize the prediction accuracy. The independent creation of a residual model is no longer required for the proposed model. The prediction accuracy of the proposed prediction models is verified using real energy demand cases. Experimental results showed that the proposed remnant GM(1,1) models perform well in comparison with other remnant GM(1,1) variants.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13903	13912		10.1007/s00500-020-04765-3		FEB 2020											
J								A selective ensemble preprocessing strategy for near-infrared spectral quantitative analysis of complex samples	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Preprocessing method; Ensemble; Full factorial design; Multivariate calibration; Near-infrared spectroscopy; Partial least squares	MULTIVARIATE CALIBRATION; SPECTROSCOPY; IDENTIFICATION; MODELS; DISCRIMINATION; WAVELET	Preprocessing of raw near-infrared (NIR) spectra is typically required prior to multivariate calibration since the measured spectra of complex samples are often subject to overwhelming background, light scattering, varying noises and other unexpected factors. Various preprocessing methods have been developed aimed at removing or reducing the interference of these effects. However, it is usually difficult to determine the best preprocessing method for a given data. Instead of selecting the best one, a selective ensemble preprocessing strategy is proposed for NIR spectral quantitative analysis. Firstly, numerous preprocessing methods and their combinations are obtained by full factorial design in order of baseline correction, scattering correction, smoothing and scaling. Then partial least squares (PLS) model is built for each preprocessing method. The models which have better predictions than PLS are selected and their predictions are averaged as the final prediction. The performance of the proposed method was tested with corn, blood and edible blend oil samples. Results demonstrate that the selective ensemble preprocessing method can give comparative or even better results than the traditional selected best preprocessing method. Therefore, in the framework of selective ensemble preprocessing, more accurate calibration can be obtained without searching the best preprocessing method.																	0169-7439	1873-3239				FEB 15	2020	197								103916	10.1016/j.chemolab.2019.103916													
J								Chagas disease vectors identification using visible and near-infrared spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Species determination; Triatominae; Chagas disease; Machine learning; Classification; VIS-NIRS	HEMIPTERA REDUVIIDAE TRIATOMINAE; BRASILIENSIS SPECIES COMPLEX; AGE; INFESTANS; SORDIDA; ORIGIN; REVALIDATION; REGRESSION; GUASAYANA; RHODNIUS	Chagas disease, caused by the parasite Trypanosoma cruzi, is widespread in Latin America, where the disease remains one of the major public health problems. This condition is mostly transmitted by triatomines which are haematophagous insects all their life. With 154 species described in the world, the correct determination of the species involved in the transmission is crucial to develop efficient control strategies. This can be achieved by taxonomic keys (available only for adult stages, nymphal instars must be reared), or by molecular techniques. Both are time and/or money consuming, showing the needs of new identification tools, especially for nymphal instars which are the most frequently found on the field. Visible and near-infrared spectroscopy (VIS-NIR), used successfully these last years in various organisms' determination, was applied on a sample of three species from Bolivia: Triatoma infestans, Triatoma sordida and Triatoma guasayana. The spectrum of the dorsal part of the head from nymphal instars and adult stages was taken for each specimen of each species. Different methods of preprocessing and selection of variables (wavelengths) were tested to find the best model of classification for the three species. Each model was evaluated by different indices: accuracy, specificity, and F1 score. The comparison of the performance of each model evidenced that the best results were obtained when using a short spectrum (400-2000 nm) without pre-processing. A total of 32 components were retained by tuning, and 933 wavelengths were kept by the backward feature selection algorithm. Applying it on a new sample of insects, this model showed a global accuracy of 97.2% (95.0-98.6). The F1 score was greater than 0.95, and the specificity greater than 0.94 for all the species. For the first time, a tool is available to quickly identify and with a high accuracy nymphal instars and adults of triatomines.																	0169-7439	1873-3239				FEB 15	2020	197								103914	10.1016/j.chemolab.2019.103914													
J								Deep learning for geographical discrimination of Panax notoginseng with directly near-infrared spectra image	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Residual convolutional neural network; Deep teaming; Panax notoginseng; Geographical trace; Spectra discrimination	SPECTROSCOPY; PHARMACOLOGY; STRATEGY	Herbal materials have been widely used as functional food by a certain group of people for a potentially positive effect on body health regulation. Panax notoginseng as a crude material of functional food has long medical and cultivation history for more than 400 years in China and other countries. However, the quality was fluctuated with their geographical origins and Wenshan Autonomous Prefecture was regarded as the geo-authentic location with high properties. Therefore, rapid detection method is necessary for consumer to discriminate their authentic origins. In our study, 258 near infrared spectra of root powder of P. notoginseng from five main cultivation areas were used for discrimination analysis. A deep learning strategy (residual convolutional neural network) was established with 80% spectra images. Therein, the discrimination of geographical origins of the herb was first to be reported using directly spectra images instead of data matric from these spectra. The results indicated that these samples could be correctly classified as their respective categories with 100% accuracy in training set and 91% accuracy in test set. Finally, 22 samples were accurately discriminated in 25 samples of prediction set. In general, residual convolutional neural network using direct spectra image would be a feasible strategy for geographical traceability in further discrimination research.																	0169-7439	1873-3239				FEB 15	2020	197								103913	10.1016/j.chemolab.2019.103913													
J								A deep learning just-in-time modeling approach for soft sensor based on variational autoencoder	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Variational autoencoder; Just-in-time learning; Kullback Leibler divergence; Gaussian process regression; Soft sensor	PRINCIPAL COMPONENT REGRESSION; INDUSTRIAL-PROCESSES; OPTIMIZATION; DIAGNOSIS; FRAMEWORK	This paper presents a variational autoencoder-based just-in-time (JIT) learning framework for soft sensor modeling. Just-in-Time learning is often applied for soft sensor modeling in industrial processes. However, traditional just-in-time learning methods measure the similarity based on Euclidean distance, which has not taken into consideration the uncertainty in variables. To improve traditional just-in-time learning methods, in the proposed approach, the variational autoencoder is employed to extract features from input data set containing noise. Each feature variable is expressed by a Gaussian distribution. Then, by using the distribution of each feature variable, Kullback-Leibler divergence is employed to evaluate the similarity between the historical samples and a query sample. Furthermore, historical samples that are most similar to the query samples based on the values of the Kullback-Leibler divergence are selected for modeling. Finally, Gaussian process regression as a nonlinear regression model, is used to model the relationship between the selected input samples and the corresponding output samples, and then make a prediction. A numerical example as well as application on a practical debutanizer industrial process demonstrates the effectiveness of the proposed method.																	0169-7439	1873-3239				FEB 15	2020	197								103922	10.1016/j.chemolab.2019.103922													
J								Bilinear and trilinear modelling of three-way data obtained in two factor designed metabolomics studies	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Omics data analysis; Experimental design; Two-factor interaction; Three-way data analysis; Trilinear and bilinear modelling	MULTIVARIATE CURVE RESOLUTION; ROTATION AMBIGUITIES; COMPONENT ANALYSIS; VARIANCE; RESPONSES; SAMPLES; TOOL	Metabolomic studies of biological samples using experimentally designed experiments at different levels produce large multivariate datasets which can be arranged in three-way data structures and modelled using bilinear and trilinear factor decomposition methods. The goal of these studies is the discovery of the hidden sources of data variability to facilitate their biochemical interpretation. In this paper, the relationship between the effects of the experimental design factors, the structure of the generated three-way datasets and their more appropriate modelling (bilinear or trilinear) are investigated. As example of study, the effects of the dose of a chemical drug on the changes over time in the concentration of lipids in multiple samples of a biological organism are investigated in detail. Different scenarios are considered depending on the type of effects and interactions between the experimental factors. The optimal data modelling results are obtained in case of having reproducible multiplicative effects between the experimental design factors, because in this case the data decomposition can be performed using a trilinear model and the correct lipid profiles are recovered. In the other data scenarios, even in the presence of only additive effects and no interaction between design factors, the correct recovery of the different lipid profiles describing the behavior of the system is not guaranteed and the subsequent rotation ambiguities associated to the bilinear model decompositions can still be present.																	0169-7439	1873-3239				FEB 15	2020	197								103917	10.1016/j.chemolab.2019.103917													
J								Rapid discrimination of Salvia miltiorrhiza according to their geographical regions by laser induced breakdown spectroscopy (LIBS) and particle swarm optimization-kernel extreme learning machine (PSO-KELM)	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Laser-induced breakdown spectroscopy; Particle swarm optimization; Kernel extreme learning machine; Salvia miltiorrhiza; Classification	QUANTITATIVE-ANALYSIS; CLASSIFICATION; ORIGINS; DANSHEN; ROOTS	Laser-induced breakdown spectroscopy (LIBS) coupled with particle swarm optimization-kernel extreme learning machine (PSO-KELM) method was developed for classification and identification of six types Salvia miltiorrhiza samples in different regions. The spectral data of 15 Salvia miltiorrhiza samples were collected by LIBS spectrometer. An unsupervised classification model based on principal components analysis (PCA) was employed first for the classification of Salvia miltiorrhiza in different regions. The results showed that only Salvia miltiorrhiza samples from Gansu and Sichuan Province can be easily distinguished, and the samples in other regions present a bigger challenge in classification based on PCA. A supervised classification model based on KELM was then developed for the classification of Salvia miltiorrhiza, and two methods of random forest (RF) and PSO were used as the variable selection method to eliminate useless information and improve classification ability of the KELM model. The results showed that PSO-KELM model has a better classification result with a classification accuracy of 94.87%. Comparing the results with that obtained by particle swarm optimization-least squares support vector machines (PSO-LSSVM) and PSO-RF model, the PSO-KELM model possess the best classification performance. The overall results demonstrate that LIBS technique combined with PSO-KELM method would be a promising method for classification and identification of Salvia miltiorrhiza samples in different regions.																	0169-7439	1873-3239				FEB 15	2020	197								103930	10.1016/j.chemolab.2020.103930													
J								RBPro-RF: Use Chou's 5-steps rule to predict RNA-binding proteins via random forest with elastic net	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										RNA-Binding proteins; Feature extraction; SMOTE; Elastic net; Random forest	AMINO-ACID-COMPOSITION; SUPPORT VECTOR MACHINES; FLEXIBLE WEB SERVER; SUBCELLULAR-LOCALIZATION; DIMENSIONALITY REDUCTION; ENSEMBLE CLASSIFIER; PSEUDO; SITES; DNA; SEQUENCE	RNA-proteins interaction is essential for the regulation of gene expression, cell defense and developmental regulation and other life activities, so applying machine learning to predict RNA-binding proteins (RBPs) has become a research hotspot in bioinformatics. We propose a new method to predict RNA-binding proteins called RBPro-RF. First, the feature vectors of the protein sequence are extracted by fusing composition-transition-distribution (C-T-D), pseudo-amino acid composition (PseAAC) and position-specific scoring matrix-400 (PSSM-400). Secondly, the synthetic minority oversampling technique (SMOTE) and the edited nearest neighbor (ENN) are employed to balance samples. Then, elastic net (EN) is used to eliminate redundant features and retain the important features to represent RBPs. Finally, the optimal feature vectors are input into random forest classifier to predict RBPs. Ten-fold cross-validation indicates the ACC and MCC of the training set are 97.43% and 0.933, respectively. In addition, the accuracies of three independent test sets Human, S. cerevisiae and A. thaliana are 95.63%, 88.82%, and 92.35%, respectively, which are superior to the state-of-the-art prediction methods. In summary, experimental results show that our method can significantly improve the accuracy of RNA-binding proteins prediction. The source code and all datasets are available at https://github.com/QUST-AIBBDRC/RBPro-RF/.																	0169-7439	1873-3239				FEB 15	2020	197								103919	10.1016/j.chemolab.2019.103919													
J								A spatial-temporal LWPLS for adaptive soft sensor modeling and its application for an industrial hydrocracking process	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft sensor; JITL framework; LWPLS; Hydrocracking process; Quality prediction	PARTIAL LEAST-SQUARES; REGRESSION; DESIGN; SYSTEM	Locally weighted partial least squares (LWPLS) is a widely used just-in-time learning (JITL) modeling algorithm for adaptive soft sensor development. In LWPLS, spatial variable distance is used to measure similarity and assign weights for historical samples, which is very effective to handle process time-varying problems of abrupt changes. However, the gradual process changes are not effectively handled in traditional LWPLS. To cope with this problem, a novel similarity is proposed for temporal distance measurement by introducing a temporal variable of sampling instant, in which newest sampled data can get large weights since they represent the more recent process state. Then, both spatial and temporal similarities are considered to construct a spatial-temporal adaptive LWPLS modeling framework in this paper. The effectiveness of the proposed algorithm is validated on an industrial hydrocracking process.																	0169-7439	1873-3239				FEB 15	2020	197								103921	10.1016/j.chemolab.2019.103921													
J								Revealing informative metabolites with random variable combination based on model population analysis for metabolomics data	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Metabolomics; Variable selection; Biomarker discovery; Informative metabolites; Variable combination; Model population analysis	PARTIAL LEAST-SQUARES; SELECTION; REGRESSION; STRATEGY; TOOL	The discovery of biomarker is a critical and essential step in metabolomics research. With the increasing complexity of metabolomics data generated by high resolution instruments, it is always an urgent need for chemometricians or statisticians to develop a method to efficiently reveal informative metabolites (variables). Based on the framework of model population analysis, a strategy coupled with partial least squares discriminant analysis (PLS-DA), called revealing informative metabolites iteratively (RIMI), was proposed in this study. For the sake of considering the synergetic effect of multiple variables, a vast population of random variable combinations are generated. It is worth pointing out that only the variable combinations with higher model accuracy are used to make paired models in order to statistically assess the importance of each variable in accordance with its beneficial contribution to classification model performance. Four types of variables which include strongly informative, weakly informative, noise and interfering variables, are then identified based on the difference and its significance of the area under the receiver operating characteristic curve (AUROC) values of exclusion and inclusion of each variable. With this definition, unbeneficial variables, including noise and interfering variables, were eliminated iteratively in a mild way. Strongly and weakly informative variables regarded as beneficial variables, are retained, and their P values of t-test are used to reveal the best variable subset. Due to the advantage in exploring useful information from a vast number of variable combinations with good performance, when applied to two metabolomics datasets, RIMI has greatly improved the accuracy value of classification model compared to other methods as the results show. It is indicated that RIMI has efficiently revealed informative metabolites and is regarded as a good alternative for biomarker discovery.																	0169-7439	1873-3239				FEB 15	2020	197								103920	10.1016/j.chemolab.2019.103920													
J								Improving the PSO method for global optimization problems	EVOLVING SYSTEMS										Particle swarm optimization; Stochastic methods; Termination rules	PARTICLE SWARM OPTIMIZATION; ALGORITHM; MINIMA; GA	The paper introduces two modifications for the well-known PSO method to solve global optimization problems. The first modification deals with the termination of the method and the second with the bounding of the so-called velocity in order to prevent the method from creating particles outside the domain range of the objective function. The modified method was tested on a series of global optimization problems from the relevant literature and the results are reported.																	1868-6478	1868-6486															10.1007/s12530-020-09330-9		FEB 2020											
J								Complex network analysis of three-way decision researches	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Three-way decision; Granular computing; Rough set; Complex networks; Knowledge discovery	THEORETIC ROUGH SET; APPROXIMATE CONCEPT CONSTRUCTION; ATTRIBUTE REDUCTION; CONFLICT-ANALYSIS; MAKING APPROACH; MODEL; SYSTEMS; SPACES; WEB; GRANULATION	In this paper, complex networks are used to analyze the dataset of three-way decision articles published before December 18, 2019 and downloaded from ISI Web of Science. The scientific collaboration network, university collaboration network, networks of scientific papers (i.e., citation network, bibliographic coupling network, co-citation network) and keywords network are constructed to reveal the relationships between authors, affiliations, papers and keywords, respectively. Some interesting results are obtained and used to answer the following questions: (1) which authors play a key role in developing three-way decision; (2) which affiliations actively promote the development of three-way decision; (3) which papers are important or influential in the field of three-way decision; (4) what are the closely related research issues around three-way decision.																	1868-8071	1868-808X				MAY	2020	11	5					973	987		10.1007/s13042-020-01082-x		FEB 2020											
J								Optimal Nonlinear PID Control of a Micro-Robot Equipped with Vibratory Actuator Using Ant Colony Algorithm: Simulation and Experiment	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Micro-robot; Nonlinear PID control; Vibrating motors; Ant colony optimization	DESIGN; OPTIMIZATION	In this paper, an optimal nonlinear control scheme based on the application of ant colony optimization (ACO) is applied to a micro-robot equipped with vibratory actuators. Accordingly, two small vibrating motors are utilized to run the micro-robot and the motion principle of stick-slip is used for locomotion purpose. First, a dynamic model of the micro-robot is derived considering the stiffness of the robot's legs. Then, the influences of robot mass and length of legs on micro-robot motion are studied using simulation. Next, an optimal linear PID control scheme is applied to the micro-robot system. However, it is found that this control method does not have an acceptable performance when friction is low or the system is under disturbance. Consequently, an ACO-based optimal nonlinear PID control is proposed to cope with the mentioned drawbacks as the main contribution of the paper. Afterwards, the performance of both control techniques is compared through simulation. Finally, the micro-robot is developed and experimentally evaluated. It is found that the experimental results are in a good agreement with some of the simulation outcomes through which the validity of the mathematical scheme as well as the feasibility of design is affirmed.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		773	796		10.1007/s10846-020-01165-5		FEB 2020											
J								Robust hand gesture recognition system based on a new set of quaternion Tchebichef moment invariants	PATTERN ANALYSIS AND APPLICATIONS										Hand gesture recognition; Tchebichef moments; Moment invariants; Quaternion algebra; Complex background; RST invariants	CONTOUR FRAGMENTS; BAG; CLASSIFICATION; FRAMEWORK; NETWORK	Hand gesture recognition is a challenging task due to the complexity of hand movements and to the variety among the same gesture performed by distinct subjects. Recent technologies, such as Kinect sensor, provide new opportunities, allowing to capture both RGB and depth (RGB-D) images, which offer high discriminant information for efficient hand gesture recognition. In the aspect of feature extraction, the traditional methods process the RGB and depth information independently. In this paper, we propose a robust hand gesture recognition system based on a new feature extraction method, fusing RGB images and depth information simultaneously, by using the quaternion algebra that provide a more robust and holistical representation. In fact, we introduce, for the first time, a novel type of feature extraction method, named quaternion Tchebichef moment invariants. The novelty of the proposed method in this paper lies in the direct derivation of invariants from their orthogonal moments, based on the algebraic properties of the discrete Tchebichef polynomials. The proposed approach based on quaternion algebra is suggested to process the four components holistically, for a robust and efficient hand gesture recognition system. The obtained experimental and theoretical results demonstrate that the present approach is very effective for addressing the problem of hand gesture recognition and have proved its robustness against geometrical distortion, noisy conditions and complex background compared to the state of the art, indicating that it could be highly useful for many computer vision applications.																	1433-7541	1433-755X				AUG	2020	23	3					1337	1353		10.1007/s10044-020-00866-9		FEB 2020											
J								Neural network with adaptive evolutionary learning and cascaded support vector machine for fault localization and diagnosis in power distribution system	EVOLUTIONARY INTELLIGENCE										Fault diagnosis; Fault signal classification; Adaptive evolutionary learning; Cascade support vector machine; Neural network	CLASSIFICATION; LOCATION; ALGORITHM; SCHEME	Fault diagnosis and classification in electric power system is necessary to maintain a protected operation of power system. The classification of this signal is complex due to the large dataset, computational complexity and limited real time performance. This paper focuses on the detection and classification of electric power transmission using neural network with adaptive evolutionary learning and cascade support vector machine (SVM) with wavelet descriptors of the signal to overcome such limitations. Initially the wavelet decomposed fault signals are extracted from the simulated signals. The received signal consists of normal signals and fault signals such as transient, sag and swells signals respectively. The wavelet descriptors of different datasets are applied to the cascade SVM for better classification. This real experiment of this paper shows that this cascade SVM provides good generalization and much fast speed compared with traditional SVMs.																	1864-5909	1864-5917															10.1007/s12065-020-00359-y		FEB 2020											
J								Handling Sequence-dependent Setup Time Flexible Job Shop Problem with Learning and Deterioration Considerations using Evolutionary Bi-level Optimization	APPLIED ARTIFICIAL INTELLIGENCE											SCHEDULING PROBLEM; TABU SEARCH; ALGORITHM	Bi-level optimization is a challenging research area that has received significant attention from researchers to model enormous NP-hard optimization problems and real-life applications. In this paper, we propose a new evolutionary bi-level algorithm for Flexible Job Shop Problem with Sequence-Dependent Setup Time (SDST-FJSP) and learning/deterioration effects. There are two main motivations behind this work. On the one hand, learning and deterioration effects might occur simultaneously in real-life production systems. However, there are still ill posed in the scheduling area. On the other hand, bi-level optimization was presented as an interesting resolution scheme easily applied to more complex problems without additional modifications. Motivated by these issues, we attempt in this work to solve the FJSP variant using the bi-level programming framework. We suggest firstly a new bi-level mathematical formulation for the considered FJSP; then we propose a bi-level evolutionary algorithm to solve the problem. The experimental study on well-established benchmarks assesses and validates the advantage of using a bi-level scheme over the compared approaches in this research area to solve such NP-hard problem.																	0883-9514	1087-6545				MAY 11	2020	34	6					433	455		10.1080/08839514.2020.1723871		FEB 2020											
J								Deep learning approaches for anomaly-based intrusion detection systems: A survey, taxonomy, and open issues	KNOWLEDGE-BASED SYSTEMS										Intrusion detection; Anomaly detection; Deep learning	NETWORK	The massive growth of data that are transmitted through a variety of devices and communication protocols have raised serious security concerns, which have increased the importance of developing advanced intrusion detection systems (IDSs). Deep learning is an advanced branch of machine learning, composed of multiple layers of neurons that represent the learning process. Deep learning can cope with large-scale data and has shown success in different fields. Therefore, researchers have paid more attention to investigating deep learning for intrusion detection. This survey comprehensively reviews and compares the key previous deep learning-focused cybersecurity surveys. Through an extensive review, this survey provides a novel fine-grained taxonomy that categorizes the current state-of-the-art deep learning-based IDSs with respect to different facets, including input data, detection, deployment, and evaluation strategies. Each facet is further classified according to different criteria. This survey also compares and discusses the related experimental solutions proposed as deep learning-based IDSs. By analysing the experimental studies, this survey discusses the role of deep learning in intrusion detection, the impact of intrusion detection datasets, and the efficiency and effectiveness of the proposed approaches. The findings demonstrate that further effort is required to improve the current state-of-the art. Finally, open research challenges are identified, and future research directions for deep learning-based IDSs are recommended. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105124	10.1016/j.knosys.2019.105124													
J								Aedes mosquito detection in its larval stage using deep neural networks	KNOWLEDGE-BASED SYSTEMS										Mosquito larval surveillance; Deep neural networks; Automatic segmentation; Vector control; Dengue fever; Zika fever	DENGUE; ZIKA	Dengue, Chikungunya and Zika viruses cause dangerous infections in tropical and subtropical regions throughout the world. The World Health Organization estimates that one out of every three persons in the entire human population is in danger of contracting one of these diseases from a single mosquito bite. Currently, these viral infections are not preventable by vaccines and there is not a direct treatment that can effectively diminish the viral infection, which causes a wide range of pathologies, including severe joint pain, internal blood loss, permanent neurological damage in unborn children and even death. Due to this grim scenario, the best and maybe the only line of defense against these diseases is the effective surveillance, control and suppression of the mosquitoes that transmit these viruses: Aedes aegypti and Aedes albopictus. In this paper, we present a complete solution that is capable of identifying the Aedes aegypti and Aedes albopictus mosquito in the larval stage, which is easily disposable, restricted to water bodies, and incapable of transmitting diseases according to the Centers for Disease Control and Prevention (CDC). Our proposal is based on deep neural networks (DNN) that effectively recognize larval samples with an accuracy of 94.19%, which is better than other state-of-the-art automatic methods. Additionally, the capabilities of our proposed DNN allow us to automatically crop the region of interest (ROI) with an accuracy of 92.85% and then automatically classify the region as Aedes positive or Aedes negative, without further human intervention and in less than a second, accelerating the response time for biological control from days to seconds. Our proposal includes hardware designs that allow inexpensive implementation, making it suitable for isolated communities, underdeveloped countries, and rural or urban areas. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								104841	10.1016/j.knosys.2019.07.012													
J								On representation of fuzzy measures for learning Choquet and Sugeno integrals	KNOWLEDGE-BASED SYSTEMS										Aggregation functions; Fuzzy measures; Capacities; Choquet integral; Sugeno integral; Multicriteria decision making	AGGREGATION	This paper examines the marginal contribution representation of fuzzy measures, used to construct fuzzy measure from empirical data through an optimization process. We show that the number of variables can be drastically reduced, and the constraints simplified by using an alternative representation. This technique makes optimizing fitting criteria more efficient numerically, and allows one to tackle learning problems with higher number of correlated decision criteria. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105134	10.1016/j.knosys.2019.105134													
J								Mining distinct and contiguous sequential patterns from large vehicle trajectories	KNOWLEDGE-BASED SYSTEMS										Vehicle trajectory; Data mining; Sequential pattern mining; Contiguous patterns	SAFETY JUSTIFICATION; EXPLORATION; GROWTH	We focus on the problem of using contiguous SPM to extract succinct, redundancy controlled patterns from large vehicle trajectories. Although there exist several techniques to reduce the contiguous sequential pattern output such as closed and max SPM, they still produce massive redundant pattern outputs when the input sequence database is sufficiently large and homogeneous - as is often the case for vehicle trajectories. Therefore, in this work we propose DC-SPAN: a distinct contiguous SPM algorithm. DC-SPAN mines a set of sequential patterns where the maximum redundancy of the pattern output is controlled by a user-specified parameter. Through various experiments using real world trajectory datasets we show DC-SPAN effectively controls the redundancy of the pattern output with trade-offs in pattern distinctness. Additionally, our experiments also indicate that DC-SPAN efficiently computes these patterns, incurring only a marginal running time cost over existing state-of-the-art contiguous SPM approaches. Lastly, due to the less redundant and more succinct pattern output we also briefly explore visualisation as a useful technique to interpret the discovered vehicle routes. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105076	10.1016/j.knosys.2019.105076													
J								Cascaded dual-scale crossover network for hyperspectral image classification	KNOWLEDGE-BASED SYSTEMS										Algorithms; Data processing; Hyperspectral image classification; Residual learning; Convolutional neural network		In recent years, deep neural networks have exhibited numerous advantages in hyperspectral image classification (HIC). However, owing to the limited number of training samples of hyperspectral images (HSIs), the network structure should not be designed too deep to retard the overfitting phenomenon. This study proposes a cascaded dual-scale crossover network for HIC, which not only could extract rich features, but also does not make the network deeper. It continuously connects two different cascaded dual-scale crossover blocks, and automatically extracts the spectral-spatial features of HSIs. Moreover, for the limited training samples, the proposed network could flexibly capture more discriminant contextual features by using different spectral-size and spatial-size convolution kernels. Furthermore, two different cross-merge methods are designed to improve the information flow and contrast of the images to obtain parts of interest for the images. Two skip structures are also used for alleviating overfitting and accelerating the network training. Additional experimental results on some datasets, including Indian Pines, Kennedy Space Center, and University of Pavia, verify the feasibility of the proposed network. Namely, the classification accuracy of the proposed network is superior to that of other existing networks. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105122	10.1016/j.knosys.2019.105122													
J								DISL: Deep Isomorphic Substructure Learning for network representations	KNOWLEDGE-BASED SYSTEMS										Deep learning; Network representations; Isomorphic substructures; Probability-guided random walk; Convolutional neural networks	GRAPH; SYSTEM	The analysis of complex networks based on deep learning has drawn much attention recently. Generally, due to the scale and complexity of modern networks, traditional methods are gradually losing the analytic efficiency and effectiveness. Therefore, it is imperative to design a network analysis model which caters to the massive amount of data and learns more comprehensive information from networks. In this paper, we propose a novel model, namely Deep Isomorphic Substructure Learning (DISL) model, which aims to learn network representations from patterns with isomorphic substructures. Specifically, in DISL, deep learning techniques are used to learn a better network representation for each vertex (node). We provide the method that makes the isomorphic units self-embed into vertex-based subgraphs whose explicit topologies are extracted from raw graph-structured data, and design a Probability-guided Random Walk (PRW) procedure to explore the set of substructures. Sequential samples yielded by PRW provide the information of relational similarity, which integrates the information of correlation and co-occurrence of vertices and the information of substructural isomorphism of subgraphs. We maximize the likelihood of the preserved relationships for learning the implicit similarity knowledge. The architecture of the Convolutional Neural Networks (CNN5) is redesigned for simultaneously processing the explicit and implicit features to learn a more comprehensive representation for networks. The DISL model is applied to several vertex classification tasks for social networks. Our results show that DISL outperforms the challenging state-of-the-art Network Representation Learning (NRL) baselines by a significant margin on accuracy and weighted-F-1 scores over the experimental datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105086	10.1016/j.knosys.2019.105086													
J								HDSM: A distributed data mining approach to classifying vertically distributed data streams	KNOWLEDGE-BASED SYSTEMS										Distributed data stream mining; Vertically-distributed data; Online classification	ENSEMBLE; CLASSIFICATION	The rise in the Internet of Things (IoT) and other sensor networks has created many vertically-distributed and high-velocity data streams that require specialized algorithms for true distributed data mining. This paper proposes a novel Hierarchical Distributed Stream Miner (HDSM) that learns relationships between the features of separate data streams with minimal data transmission to central locations. Experimental evaluation demonstrates significant improvements in classification accuracy over previously proposed distributed stream-mining approaches while minimizing data transmission and computational costs. HDSM's potential for dynamically trading off accuracy with computational resource costs is also demonstrated. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105114	10.1016/j.knosys.2019.105114													
J								Incremental updating approximations for double-quantitative decision-theoretic rough sets with the variation of objects	KNOWLEDGE-BASED SYSTEMS										Double-quantitative decision-theoretic rough sets; Concept approximations; Incremental learning	DYNAMIC MAINTENANCE; ATTRIBUTE REDUCTION; KNOWLEDGE; ALGORITHM	Double-quantitative decision-theoretic rough sets (Dq-DTRS) provide more comprehensive description methods for rough approximations of concepts, which lay foundations for the development of attribute reduction and rule extraction of rough sets. Existing researches on concept approximations of Dq-DTRS pay more attention to the equivalence class of each object in approximating a concept, and calculate concept approximations from the whole data set in a batch. This makes the calculation of approximations time consuming in dynamic data sets. In this paper, we first analyze the variations of equivalence classes, decision classes, conditional probability, internal grade and external grade in dynamic data sets while objects vary sequentially or simultaneously over time. Then we propose the updating mechanisms for the concept approximations of two types of Dq-DTRS models from incremental perspective in dynamic decision information systems with the sequential and batch variations of objects. Meanwhile, we design incremental sequential insertion, sequential deletion, batch insertion, batch deletion algorithms for two Dq-DTRS models. Finally, we present experimental comparisons showing the feasibility and efficiency of the proposed incremental approaches in calculating approximations and the stability of the incremental updating algorithms from the perspective of the runtime under different inserting and deleting ratios and parameter values. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105082	10.1016/j.knosys.2019.105082													
J								Parameter tuning for meta-heuristics	KNOWLEDGE-BASED SYSTEMS										Parameter tuning; Gravitational search algorithm (GSA); Gravitational constant; Meta-heuristics	GRAVITATIONAL SEARCH ALGORITHM; PARTICLE SWARM OPTIMIZATION; SYMBIOTIC ORGANISMS SEARCH; EVOLUTIONARY; QUALITY; DESIGN; GSA	These days meta-heuristic algorithms are gaining lot of popularity. The performance of the meta-heuristics depends upon the suitable selection of user dependent parameters. Finding the most suitable values for the parameters (fine tuning) is a challenging problem. This paper proposes a generalized strategy to find the most suitable value of any parameter for a meta-heuristic algorithm. The approach is based on the relation between algorithm's performance and functional landscape. The proposed approach is evaluated by applying it to a recent meta-heuristic algorithm, Gravitational Search Algorithm (GSA). The parameter a which plays a vital role in the convergence of GSA search process, is fine tuned using the proposed strategy. Obtained values of a, change the nature of gravitational coefficient G from monotonic to non-monotonic for a cluster free diversified search. The proposed strategy is tested over CEC-2015 test suite. Various statistical tests have been applied to compare the obtained results with recent variants of GSA and other state-of-the-art meta-heuristics. Results confirm that the parameters obtained using proposed approach significantly improve the results. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105094	10.1016/j.knosys.2019.105094													
J								Multi-graph fusion for multi-view spectral clustering	KNOWLEDGE-BASED SYSTEMS										Multi-view learning; Spectral clustering; Graph fusion	ALGORITHM	A panoply of multi-view clustering algorithms has been developed to deal with prevalent multi-view data. Among them, spectral clustering-based methods have drawn much attention and demonstrated promising results recently. Despite progress, there are still two fundamental questions that stay unanswered to date. First, how to fuse different views into one graph. More often than not, the similarities between samples may be manifested differently by different views. Many existing algorithms either simply take the average of multiple views or just learn a common graph. These simple approaches fail to consider the flexible local manifold structures of all views. Hence, the rich heterogeneous information is not fully exploited. Second, how to learn the explicit cluster structure. Most existing methods do not pay attention to the quality of the graphs and perform graph learning and spectral clustering separately. Those unreliable graphs might lead to suboptimal clustering results. To fill these gaps, in this paper, we propose a novel multi-view spectral clustering model which performs graph fusion and spectral clustering simultaneously. The fusion graph approximates the original graph of each individual view but maintains an explicit cluster structure. Experiments on four widely used data sets confirm the superiority of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105102	10.1016/j.knosys.2019.105102													
J								Interactive double states emotion cell model for textual dialogue emotion prediction	KNOWLEDGE-BASED SYSTEMS										Textual dialogue; Emotion recognition; Emotion prediction; Human-computer interaction; Data-driven; IDS-ECM; Macro-average F1 score	RECOGNITION	Daily dialogues are full of emotions that control the trends of dialogues and influence the attitudes of interlocutors toward each other, and understanding the human emotions in dialogues is of great significance in emotional comfort, human-computer interaction and intelligent question-answering. This paper defines a new task called emotion prediction in textual dialogue. Different from the text emotion recognition task, which derives the current emotional state of interlocutor from the utterance, emotion prediction aims at predicting the future emotional state of interlocutor before the interlocutor utters something. Moreover, this paper summarizes and explains three notable characteristics of emotional propagation in text dialogue: context dependence, persistence and contagiousness. By considering these characteristics, a fully data-driven interactive double states emotion cell model (IDS-ECM) is proposed. The model has two layers. The first layer automatically extracts the emotional information of historical dialogue and is used to describe the contextual dependence of the textual dialogue emotion. The second layer models the change process of interlocutors' emotional states during the dialogue and depicts the persistence and contagiousness of emotions. Experimental results on two manually annotated datasets show that the proposed model is superior to the baseline in the macroaveraged F1 evaluation metric and that the proposed model can simulate the emotional changes in the process of dialogue so as to predict the emotions with high accuracy. The experimental results also reveal the communication differences between different emotional categories in dialogue, which is of guiding significance for future research. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105084	10.1016/j.knosys.2019.105084													
J								A two-stage dynamic influence model-achieving decision-making consensus within large scale groups operating with incomplete information	KNOWLEDGE-BASED SYSTEMS										Large scale group decision making; Incomplete opinions; Consensus reaching process; Social network; Opinion evolution	SOCIAL NETWORK; REACHING PROCESS; TRUST; MECHANISM; FRAMEWORK	The decision making environment has been dramatically affected by rapid developments in society and the economy. Large-scale group decision making (LSGDM) based on social network has become a vital research topic in the field of decision making science. In this paper, we propose a novel framework based on social network to manage the consensus reaching process (CRP) for LSGDM faced with incomplete information. In this framework, the large-scale group is first classified into several smaller sub-groups using a sub-group detection algorithm, based on the social network. Then, we propose an estimating method based on a collaborative filtering algorithm for estimating the missing preference information of the opinion leaders in each sub-group. The two-stage dynamic influence model for handling the consensus reaching process in LSGDM begins when the LSGDM is transformed into several smaller sub-group decision processes. In the first stage, a consensus model, based on opinion evolution, is proposed for the CRP within each sub-group. In the second stage, we consider each sub-group as a decision making unit. By focusing on the consensus problem across the sub-groups, we develop a novel opinion-leaders feedback strategy in order to help the sub-groups revise their opinions, working toward consensus. We provide an example of an application of our process to illustrate the validity of the proposed model for managing the CRP in LSGDM. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 15	2020	189								105132	10.1016/j.knosys.2019.105132													
J								Deep learning approach on information diffusion in heterogeneous networks	KNOWLEDGE-BASED SYSTEMS										Heterogeneous networks; Information diffusion; Topic diffusion; Cascade prediction; Network representation learning; Deep learning	MODEL	There are many real-world complex systems with multi-type interacting entities that can be regarded as heterogeneous networks including human connections and biological evolutions. One of the main issues in such networks is to predict information diffusion such as shape, growth and size of social events and evolutions in the future. While there exist a variety of works on this topic mainly using a threshold-based approach, they suffer from the local viewpoint on the network and sensitivity to the threshold parameters. In this paper, information diffusion is considered through a latent representation learning of the heterogeneous networks to encode in a deep learning model. To this end, we propose a novel meta-path representation learning approach, Heterogeneous Deep Diffusion(HDD), to exploit meta-paths as main entities in networks. At first, the functional heterogeneous structures of the network are learned by a continuous latent representation through traversing meta-paths with the aim of global end-to-end viewpoint. Then, the well-known deep learning architectures are employed on our generated features to predict diffusion processes in the network. The proposed approach enables us to apply it on different information diffusion tasks such as topic diffusion and cascade prediction. We demonstrate the proposed approach on benchmark network datasets through the well-known evaluation measures. The experimental results show that our approach outperforms the earlier state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105153	10.1016/j.knosys.2019.105153													
J								A new fast search algorithm for exact k-nearest neighbors based on optimal triangle-inequality-based check strategy	KNOWLEDGE-BASED SYSTEMS										Exact k-nearest neighbors; Fast search algorithm; Clustering; Triangle inequality; Optimal check strategy		The k-nearest neighbor (KNN) algorithm has been widely used in pattern recognition, regression, outlier detection and other data mining areas. However, it suffers from the large distance computation cost, especially when dealing with big data applications. In this paper, we propose a new fast search (FS) algorithm for exact k-nearest neighbors based on optimal triangle-inequality-based (OTI) check strategy. During the procedure of searching exact k-nearest neighbors for any query, the OTI check strategy can eliminate more redundant distance computations for the instances located in the marginal area of neighboring clusters compared with the original TI check strategy. Considering the large space complexity and extra time complexity of OTI, we also propose an efficient optimal triangle-inequality-based (EOTI) check strategy. The experimental results demonstrate that our proposed two algorithms (OTI and EOTI) achieve the best performance compared with other related KNN fast search algorithms, especially in the case of dealing with high-dimensional datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105088	10.1016/j.knosys.2019.105088													
J								Discovering process models for the analysis of application failures under uncertainty of event logs	KNOWLEDGE-BASED SYSTEMS										Application logs; Process discovery; Conformance checking; Uncertainty; Failure detection	ANOMALY DETECTION; SOFTWARE; OPERATIONS	Computer applications, such as servers, databases and middleware, ubiquitously emit execution traces stored in log files. The use of logs for the analysis of application failures is known since the early days of computers. Field data studies have shown that application logs are fraught with uncertainty, i.e., missing or noisy events in the logs. A body of research that has dealt successfully with uncertainty in event logs is process mining from the business process management community, specifically by discovering process models. The literature has shown the value of process mining across several domains, but as yet there is no study that quantifies possible improvements from using process models, and the impact of uncertainty in the context of application failures. This work addresses the use of process mining for detecting failures from application logs. First, process models are discovered from logs; then conformance checking is used to detect deviations from the models. We contribute to knowledge engineering research with a systematic measurement study that quantifies the failure detection capability of conformance checking in spite of missing events, and its accuracy with respect to process models obtained from noisy logs. Analysis is done with a dataset of 55,462 execution traces from three independent real-life applications. We obtain a mixed answer depending on the application under test; our measurements provide insights into the use of process mining for failure analysis. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105054	10.1016/j.knosys.2019.105054													
J								CNAVER: A Content and Network-based Academic VEnue Recommender system	KNOWLEDGE-BASED SYSTEMS										Venue recommender system; Social network analysis; Meta-path analysis; Random walk with restart (RWR); Graph clustering; Rank-based fusion	JOURNALS; INFORMATION; CENTRALITY; KNOWLEDGE; DESIGN	The phenomenon of rapidly developing academic venues poses a significant challenge for researchers: how to recognize the ones that are not only in accordance with one's scholarly interests but also of high significance? Often, even a high-quality paper is rejected because of a mismatch between the research area of the paper and the scope of the journal. Recommending appropriate scholarly venues to researchers empowers them to recognize and partake in important academic conferences and assists them in getting published in impactful journals. A venue recommendation system becomes helpful in this scenario, particularly when exploring a new field or when further choices are required. We propose CNAVER: A Content and Network-based Academic VEnue Recommender system. It provides an integrated framework employing a rank-based fusion of paper-paper peer network (PPPN) model and venue-venue peer network (VVPN) model. It only requires the title and abstract of a paper to provide venue recommendations, thus assisting researchers even at the earliest stage of paper writing. It also addresses cold start issues such as the involvement of an inexperienced researcher and a novel venue along with the problems of data sparsity, diversity, and stability. Experiments on the DBLP dataset exhibit that our proposed approach outperforms several state-of-the-art methods in terms of precision, nDCG, MRR, accuracy, F - measure(macro), average venue quality, diversity, and stability. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105092	10.1016/j.knosys.2019.105092													
J								F-Mapper: A Fuzzy Mapper clustering algorithm	KNOWLEDGE-BASED SYSTEMS										Topological data analysis; Mapper; Fuzzy c-Means; F-Mapper; Clustering	TOPOLOGY	Using topology in data analysis, known as Topological Data Analysis (TDA), is now a promising new area of data mining research. One of the important and foundational tools of TDA is the Mapper algorithm. During the past two decades, this algorithm has proven its useful and robust abilities in extracting insights and meaningful information from high-dimensional datasets. Nevertheless, several alterations in the choices of parameters, such as lens, cover and clustering, can be used to develop this algorithm. In this paper, we propose the F-Mapper algorithm, based on the foundation of the Mapper algorithm, to solve the problem of automating when dividing cover intervals with an arbitrary percentage of overlap. To clarify the efficiency of this enhanced algorithm, experiments were carried out on three datasets, including the Unit Circle, Reaven and Miller Diabetes, and NKI Breast Cancer. The experimental results will be analyzed and compared with those of the original method, the Mapper algorithm, through the output image and silhouette coefficient score in the evaluation of clustering. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105107	10.1016/j.knosys.2019.105107													
J								Managing minority opinions in micro-grid planning by a social network analysis-based large scale group decision making method with hesitant fuzzy linguistic information	KNOWLEDGE-BASED SYSTEMS										Micro-grid planning; Large-scale group decision making; Social network analysis; Minority opinions; Hesitant fuzzy linguistic term sets; Consensus	CONSENSUS REACHING PROCESS; MODEL; MECHANISM	The growth of global electricity demand has put forward higher requirements for power distribution networks. The high cost of the large-scale power system and the voice for the use of renewable energy impel the birth of the micro-grid which plays a complementary role in the power generation of large-scale power system. The construction of micro-grid planning is complex and many stakeholders' opinions should be considered for a comprehensive evaluation. Furthermore, the development of social big data techniques, such as e-marketplace and e-democracy, makes experts have social relationships among them. This study aims to develop a consensus model to manage minority opinions for large-scale group decision making with social network analysis for micro-grid planning. To deal with the vague and uncertain features in complex micro-grid planning problems, experts are supposed to use hesitant fuzzy linguistic term sets to express their opinions. A social network analysis-based clustering method is introduced to classify experts. Besides, in a large-scale group decision making problem, the opinions of experts should be fully considered, especially the minority opinions. This model considers the minority opinions in a micro-grid planning problem and provides an approach to manage these opinions. Finally, we use an illustrative example concerning the micro-grid planning decision making in Ali district in Tibet to demonstrate the effectiveness and practicability of the proposed model. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105060	10.1016/j.knosys.2019.105060													
J								A T1OWA fuzzy linguistic aggregation methodology for searching feature-based opinions	KNOWLEDGE-BASED SYSTEMS										Sentiment analysis; Feature opinion aggregation; Linguistic model; T1OWA aggregation operator	SENTIMENT ANALYSIS PROBLEM; INFORMATION; EXTRACTION; OPERATORS; REVIEWS	Online services such as Amazon, Tripadvisor, Ebay, etc., allow users to express sentiments about different products or services. Not only that, in some cases it is also possible to express sentiments about the different features characterizing those products or services. Most users express sentiments about individual features by using numerical values, which sometimes do not allow users to reflect properly what they are meaning and therefore they are misleading. To overcome this key issue and make users' opinions in online services more comprehensive, a new methodology for representing sentiments using linguistic term sets instead of numerical values is presented. In addition, this methodology will allow to implement importance degrees on the different features characterizing users' opinions. From both sentiments and importance of the features, the most important opinions for each user is derived via an aggregation step based on the Type-1 Ordered Weighted Averaging (T1OWA) operator, which is able to aggregate the corresponding fuzzy set representations of linguistic terms. Furthermore, the final output of the T1OWA based-search process can easily be interpreted by users because it is always of the same type (fuzzy) and defined in the same domain of the original fuzzy linguistic labels. A case study is presented where the T1OWA operator methodology is used to assess different opinions according to different user profiles. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105131	10.1016/j.knosys.2019.105131													
J								MapReduce based improved quick reduct algorithm with granular refinement using vertical partitioning scheme	KNOWLEDGE-BASED SYSTEMS										Rough sets; MapReduce; Apache spark; Reduct; Horizontal partitioning; Vertical partitioning; Feature subset selection	ATTRIBUTE REDUCTION; ROUGH	In the last few decades, rough sets have evolved to become an essential technology for feature subset selection by way of reduct computation in categorical decision systems. In recent years with the proliferation of MapReduce for distributed/parallel algorithms, several scalable reduct computation algorithms have been developed in this field for large-scale decision systems using MapReduce. The existing MapReduce based reduct computation approaches use horizontal partitioning (division in object space) of the dataset into the nodes of the cluster, requiring a complicated shuffle and sort phase. In this work, we propose an algorithm MR_IQRA_VP which is designed using vertical partitioning (division in attribute space) of the dataset with a simplified shuffle and sort phase of the MapReduce framework. MR_IQRA_VP is a distributed/parallel implementation of the Improved Quick Reduct Algorithm (IQRA_IG) and is implemented using iterative MapReduce framework of Apache Spark. We have done an extensive comparative study through experimentation on benchmark decision systems using existing horizontal partitioning based reduct computation algorithms. Through experimental analysis, along with theoretical validation, we have established that MR_IQRA_VP is suitable and scalable to datasets of larger size attribute space and moderate object space prevalent in the areas of Bioinformatics and Web mining. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105104	10.1016/j.knosys.2019.105104													
J								Hybrid neural conditional random fields for multi-view sequence labeling	KNOWLEDGE-BASED SYSTEMS										Conditional random fields; Sequence labeling; Multi-view learning; Neural network; Dynamic programming	BACKPROPAGATION	In traditional machine learning, conditional random fields (CRF) is the mainstream probability model for sequence labeling problems. CRF considers the relation between adjacent labels other than decoding each label independently, and better performance is expected to achieve. However, there are few multiview learning methods involving CRF which can be directly used for sequence labeling tasks. In this paper, we propose a novel multi-view CRF model to label sequential data, called MVCRF, which well exploits two principles for multi-view learning: consensus and complementary. We first use different neural networks to extract features from multiple views. Then, considering the consistency among the different views, we introduce a joint representation space for the extracted features and minimize the distance between the two views for regularization. Meanwhile, following the complementary principle, the features of multiple views are integrated into the framework of CRF. We train MVCRF in an endto-end fashion and evaluate it on two benchmark data sets. The experimental results illustrate that MVCRF obtains state-of-the-art performance: F-1 score 95.44% for chunking on CoNLL-2000, 95.06% for chunking and 96.99% for named entity recognition (NER) on CoNLL-2003. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105151	10.1016/j.knosys.2019.105151													
J								Consistency and consensus-driven models to personalize individual semantics of linguistic terms for supporting group decision making updates with distribution linguistic preference relations	KNOWLEDGE-BASED SYSTEMS										Group decision making (GDM); Numerical scale model; Personalized individual semantics; Consensus measure; Distribution linguistic preference relation (DLPR)	NUMERICAL SCALE; SETS; INFORMATION; TAXONOMY; GDM	Distribution linguistic preference relations (DLPRs) that model linguistic expressions with the aid of probabilistic distributions of multiple linguistic terms provide an effective tool to accurately elicit the preferences of decision makers (DMs) in linguistic decisions. Meanwhile, numerical scale models have been suitable choices for DMs to handle computing with words when solving linguistic decision problems. This study focuses on improving the group decision making (GDM) with DLPRs via the help of numerical scale models by filling the following gap. It is obvious that words might exhibit different meanings for different people. DMs may have a varying understanding of a given linguistic term in real-world fuzzy linguistic GDM. Setting personalized semantics of the linguistic terms for each DM becomes a critical task in GDM with DLPRs. To do this, we first define an improved numerical scale model to facilitate the linkages between DLPRs and numerical fuzzy preference relations. Then an additive consistency and a multiplicative consistency of DLPRs are analyzed, and the corresponding consistency indices are provided to measure the consistency levels of DLPRs. Based on them, we develop two consistency-driven optimization models to personalize numerical scales for linguistic terms with individual DLPRs. Next, we develop an approach for addressing GDM with DLPRs. In the proposed approach, a dissimilarity-based consensus measure is designed. To determine a group numerical scale for the linguistic terms with the corresponding group DLPR, two consistency and consensus-driven optimization models are constructed. Finally, illustrative examples are analyzed using the proposed approach to demonstrate its applicability and validity. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105078	10.1016/j.knosys.2019.105078													
J								An approach to generalizing the handling of preferences in argumentation-based decision-making systems	KNOWLEDGE-BASED SYSTEMS										Knowledge-based systems; Reasoning server; Defeasible argumentation; Criterion combination	RECOMMENDER SYSTEMS; LOGIC	As a practical mechanism for formalizing commonsense reasoning, argumentation has shown its potential for applications in diverse areas, many related to decision-making in knowledge-based systems. Following this line, and for helping users in making a better and informed decision, different recommender systems proposals have been developed in the argumentation literature. We will use recommender systems as a good example where to exercise our proposal. In particular, the role of preference criterion in argumentation-based recommender systems which is used to compare competing arguments is central to the user's query answering process where if the criterion does not adjust to the represented domain, the system could fail by being undecided too often. Therefore, having tools that allow to select and change the argument comparison mechanism has to be used become a central issue. Argumentation-based recommender systems that offer these tools provide an interesting ability that can be used for improving the reasoning capabilities in this type of systems. This work introduces an approach to handle multiple argument preference criteria in argumentation-based recommender systems and general knowledge-based decision support systems. More precisely, the proposal allows changing the information that a criterion can use in the argument comparison process and specify how several criteria can be simultaneously used in such process as well; to achieve that goal, a set of operators to combine several criteria is presented. The knowledge representation and reasoning is performed in Defeasible Logic Programming, a defeasible argumentation formalism based on logic programming. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105112	10.1016/j.knosys.2019.105112													
J								Active learning through label error statistical methods	KNOWLEDGE-BASED SYSTEMS										Active learning; Clustering; Label error statistical model; Probabilistic lipschitzness	DENSITY	Clustering-based active learning splits data into a number of blocks and queries the labels of the most critical instances. An active learner must decide how to choose these critical instances and how to split the blocks. In this paper, we present theoretical and practical statistical methods for analyzing the relationship between the label error and the neighbor radius, and design new split and selection strategies to handle these two issues. First, we define statistical functions for the label error based on a single instance and instance pairs. Second, we build practical statistical models, calculate empirical label errors, and guide the block splitting process. Third, using these practical models, we develop a center-and-edge instance selection strategy for choosing critical instances. Fourth, we design a new algorithm called active learning through label error statistical methods (ALSE). Learning experiments were performed with 20 datasets from various domains. The results of significance tests verify the effectiveness of ALSE and its superiority over state-of-the-art active learning algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105140	10.1016/j.knosys.2019.105140													
J								CSAN: A neural network benchmark model for crime forecasting in spatio-temporal scale	KNOWLEDGE-BASED SYSTEMS										Crime situation awareness; Crime forecasting; Variational auto-encoder; Sequence generation; Spatio-temporal analysis	PREDICTION; TUTORIAL; FUSION	Understanding the evolving discipline of crime situations is a long-standing but significant problem. Former methods prefer the stochastic modeling of the crime phenomenon in physics or statistical equations, which are elegant in theoretical explanations but less efficient in real applications. Recently, some data-driven models, especially neural network models, are illustrating promising performance in capturing dynamics of the complex phenomenon, and available massive dataset enables the task-beneficial information utilization. However, there exist several difficulties in regional crime situation awareness, including the high dimensionality, the intractable correlations as well as information redundancies in spatio-temporal dataset. To achieve efficient information processing and disentangle relationships from a recent crime dataset of fifteen years, we construct the crime situation awareness network (CSAN) as a new benchmark forecasting model via integrating structures of variational auto-encoders and context-based sequence generative neural network. Final experiments demonstrate that CSAN mostly outperforms other commonly-used spatio-temporal forecasting algorithms, such as Conv-LSTM, in regional multi-type crime frequency prediction. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 15	2020	189								105120	10.1016/j.knosys.2019.105120													
J								Cost-sensitive semi-supervised selective ensemble model for customer credit scoring	KNOWLEDGE-BASED SYSTEMS										Cost-sensitive learning; Credit scoring; Semi-supervised learning; Selective ensemble	HYBRID MODEL; CLASSIFICATION; CLASSIFIERS	Only a few customers can be labeled in realistic credit-scoring problems, while many other customers cannot. Further, satisfactory performance is difficult, as traditional supervised learning methods can only use labeled samples to build credit-scoring models. Semi-supervised learning (SSL) can use both labeled and unlabeled samples to solve this problem, but existing credit-scoring research has primarily constructed single semi-supervised models. This study introduces SSL, cost-sensitive learning, a group method of data handling (GMDH), and an ensemble learning technique to propose a GMDH-based cost-sensitive semi-supervised selective ensemble (GCSSE) model. This involves two stages: (1)First, train an ensemble model composed of N base classifiers on the initial training set L with class labels, use it to selectively label the samples from the dataset U without class labels, add them with their predicted labels to the training set, and update the N base classifiers on the new training set; (2)Second, classify L and the test set using the respective trained base classifiers, and construct a cost-sensitive GMDH neural network to obtain the selective ensemble classification results for the test set. Experimental comparisons of five public customer credit score datasets and an empirical analysis of a real customer credit score dataset suggest that this model exhibits the best overall credit-scoring performance compared with one supervised ensemble model and three semi-supervised ensemble models. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105118	10.1016/j.knosys.2019.105118													
J								DeepLN: A framework for automatic lung nodule detection using multi-resolution CT screening images	KNOWLEDGE-BASED SYSTEMS										Lung nodules detection; Multi-model ensemble; Multi-resolution CT screening images	SUBSOLID PULMONARY NODULES; MANAGEMENT; CANCER	Computed tomography (CT) is an important and valuable tool for detecting and diagnosing lung cancer at an early stage. Commonly, CT screenings with lower dose and resolution are used for preliminary screening. In particular, many hospitals in smaller towns only provide CT screenings at low resolution. However,when patients are diagnosed with suspected cancer, they are transferred or recommended to larger hospitals for more sophisticated examinations with high-resolution CT scans. Therefore, multi-resolution CT images deserve attention and are critical in clinical practice. Currently, the available open source datasets only contain high-resolution CT screening images. To address this problem, a multi-resolution CT screening image dataset called the DeepLNDataset is constructed. A three-level labeling criterion and a semi-automatic annotation system are presented to guarantee the correctness and efficiency of lung nodule annotation. Moreover, a novel framework called DeepLN is proposed to detect lung nodules in both low-resolution and high-resolution CT screening images. The multi-level features are extracted by a neural-network based detector to locate the lung nodules. Hard negative mining and a modified focal loss function are employed to solve the common category imbalance problem. A novel non-maximum suppression based ensemble strategy is proposed to synthesize the results from multiple neural network models trained on CT image datasets of different resolutions. To the best of our knowledge, this is the first work that considers the influence of multiple resolutions on lung nodule detection. The experimental results demonstrate that the proposed method can address this issue well. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				FEB 15	2020	189								105128	10.1016/j.knosys.2019.105128													
J								One-step Kernel Multi-view Subspace Clustering	KNOWLEDGE-BASED SYSTEMS										Subspace clustering; Multi-view data; One-step strategy; Kernelized model; Simplex constraint; Rank constraint	FEATURE-SELECTION; ALGORITHM; ROBUST	Multi-view subspace clustering is essential to many scientific problems. However, most existing methods suffer from three aspects of issues. First, these methods usually adopt a two-step framework, lacking the ability to achieve an optimal common affinity matrix across multiple views. Second, these methods are intended to solve the clustering problem in linear subspaces but may fail in practice as most real-world data sets may exhibit non-linear structures. Third, most existing subspace-based methods force the negative elements in the coefficient matrix to be positive, which may damage the inherent correlation among the data. To address above issues, we propose a novel approach termed One-step Kernel Multi-view Subspace Clustering (OKMSC). The common affinity matrix is learned from all views under one-step framework, which integrates the nonnegative and discriminative property of affinity matrix into the computation. Further, a kernelized model is designed to address the nonlinear multi-view clustering problem. And an iterative optimization method is designed to solve the objective function in this model. Extensive experiments have validated the superiority of the proposed method over several state-of-art clustering methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105126	10.1016/j.knosys.2019.105126													
J								Granular structure-based incremental updating for multi-label classification	KNOWLEDGE-BASED SYSTEMS										Incremental learning; Multi-label classification; Granular structure system; Three-way decisions	ATTRIBUTE REDUCTION APPROACH; FEATURE-SELECTION; 3-WAY DECISION; SET; MODEL	Incremental learning is an efficient computational paradigm of acquiring approximate knowledge of data in dynamic environment. Most of the research focuses on knowledge updating for single-label classification, whereas incremental mechanism for multi-label classification is of preliminary nature. This leads to considerable computation complexity to maintain desired performance. To address this challenge, we formulate a granular structure system (GSS). The proposed granular structure system in bottom-up way provides a systematic view on label-specific based classification. We demonstrate that the three-way selective ensemble (TSEN) model, a state-of-the-art solution for multi-label classification, is compatible with GSS in granulation. An incremental mechanism of GSS is introduced for both label-specific feature generation and optimization, and an incremental three-way selective ensemble algorithm for multiple instances immigration (IMOTSEN) is presented. Experiments completed on six datasets show that the proposed algorithm can maintain considerable classification performance while significantly accelerating the knowledge (GSS) updating. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105066	10.1016/j.knosys.2019.105066													
J								Assignment of attribute weights with belief distributions for MADM under uncertainties	KNOWLEDGE-BASED SYSTEMS										Evidential reasoning; Belief distribution; Entropy weight assignment method; Interval value; Interval belief degree; Incompleteness	EVIDENTIAL REASONING APPROACH; GROUP DECISION-MAKING; FEEDBACK MECHANISM; OBJECTIVE WEIGHTS; POINT ALLOCATION; SOCIAL NETWORK; RULE; SYSTEM; COMBINATION; INFORMATION	Multiple attribute decision making (MADM) problems often consist of various types of quantitative and qualitative attributes. Quantitative attributes can be assessed by accurate numerical values, interval values or fuzzy numbers, while qualitative attributes can be evaluated by belief distributions, linguistic variables or intuitionistic fuzzy sets. However, the determination of attribute weights is still an open issue in MADM problems until now. In the traditional objective weight assignment method, attributes are usually assessed by accurate values. In this paper, an entropy weight assignment method is proposed to dealing with the situation where the assessment of attributes can contain uncertainties, e.g., interval values, or contain both uncertainties and incompleteness, e.g., belief distributions. The advantage of the proposed method lies in that uncertainties and incompleteness contained in the interval numerical values or belief distributions can be preserved in the generated weights. Specifically, several pairs of programming models to generate the weights of attributes are constructed in three different circumstances: (1) quantitative attribute expressed by interval values; (2) incomplete belief distribution with accurate belief degrees; and (3) belief distribution constituted by interval belief degrees. The evidential reasoning approach is then utilized to aggregate the distributions of attributes based on the generated attribute weights. The normalized interval weight vector is defined, and the characteristics of the weight assignment method are discussed. The proposed method has been experimented with real data to illustrate its advantages and the potential in supporting MADM with uncertain and incomplete information. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				FEB 15	2020	189								105110	10.1016/j.knosys.2019.105110													
J								Context constraint and pattern memory for long-term correlation tracking	NEUROCOMPUTING										Correlation filters; Long-term tracking; Spatio-temporal weights; Gaussian mixture model; Average peak-to-correlation energy		This paper proposes a novel correlation tracking framework with context constraint and pattern memory (CCPM) to address scale variation, heavy occlusion and out-of-view in long-term tracking tasks. Taking the spatial structure into account, we learn multi-channel correlation filters by training the background-aware samples constrained by Gaussian function and utilize the scaling pool technique to estimate the target scale variation. Then a novel online recovery mechanism, including the weighted fern classifier and the pattern-memorized template matcher, is applied to strengthen the ability to re-capture the target in case of tracking failure. The spatio-temporal weights used in the fern classifier can effectively suppress the significant position drift caused by the distortion of the target, whereas the pattern-memorized templates compressed by Gaussian mixture model (GMM) can reduce capacity usage and computational cost. Finally, we focus on the model corruption problem, the average peak-to-correlation energy (APCE) is exploited to identify the reliable parts of the tracking trajectory, and we update the model adaptively in term of the feedback from high-confidence tracking results. Extensive experimental results demonstrate that the proposed algorithm performs favorably in OTB2013 data set against several state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						1	15		10.1016/j.neucom.2019.10.021													
J								Neural network-based adaptive funnel sliding mode control for servo mechanisms with friction compensation	NEUROCOMPUTING										Neural network (NN); Adaptive control; Funnel function; Servo mechanism; Sliding mode control (SMC)	NONLINEAR-SYSTEMS; MOTION CONTROL; TRACKING CONTROL; FEEDBACK SYSTEMS; INPUT	In this paper, a novel adaptive funnel sliding mode control scheme is proposed for servo mechanisms with friction compensation. A continuously differentiable friction model is employed to capture the unknown friction dynamics. The friction nonlinearities and unknown dynamics are estimated by using neural network (NN). Moreover, a modified funnel variable, which relaxes limitation in original funnel control (e.g., systems with relative degree 1 or 2), is developed using the tracking error to replace the scaling factor, which is used to design the sliding mode surface. Then, a novel adaptive funnel sliding mode control scheme is proposed for servo mechanisms to improve the transient performance. The effectiveness of the developed control method is validated via experimental results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						16	26		10.1016/j.neucom.2019.10.006													
J								Video salient object detection via spatiotemporal attention neural networks	NEUROCOMPUTING										Video salient objects detection; Motion information; Spatiotemporal feature fusion; Coarse pixel-wise labels	SEGMENTATION	Recently, deep convolutional neural networks have been widely introduced into image salient object detection and achieve good performance in this community. However, as the complexity of video scenes, video salient object detection with deep learning models is still a challenge. The specific difficulties come from two aspects. First of all, the deep networks on image saliency detection cannot capture robust motion cues in video sequences. Secondly, as for the spatiotemporal fusing features, the existing methods simply exploit element-wise addition or concatenation, which not fully explores the contextual information and complementary correlation, thus they cannot produce more robust spatiotemporal features. To address these issues, we propose a two-stream based spatiotemporal attention neural network (STAN) for video salient object detection. We amply extract the motion information in terms of long short term memory (LSTM) network and 3D convolutional operation from optical flow-based prior and video sequences. Moreover, an attentive module is designed to integrate the different types of spatiotemporal feature maps by learning the corresponding weights. Meanwhile, in order to generate sufficient pixel-wise annotated video frames, we manually generate lots of coarse labels, which are well utilized to train a robust saliency prediction network. Experiments on the widely used challenging datasets (e.g., FBMS and DAVIS) prove that the proposed STAN has competitive performances among salient object detection methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 15	2020	377						27	37		10.1016/j.neucom.2019.09.064													
J								Clothescounter: A framework for star-oriented clothes mining from videos	NEUROCOMPUTING										Clothes clustering; Clothes detection; Video advertising; Fashion data mining	ALGORITHM; DBSCAN	This paper presents a novel framework, ClothesCounter, which aims to automatically identify clothes worn by certain stars in videos. At first, several deep convolutional neural networks (CNN) models were utilized to preprocess video data in order to detect clothing images from original video frames, including human body detection, human posture selection, human pose estimation, face verification, and clothing detection. We then propose a method for extracting features of clothing images based on triplet loss that can map clothing images into a compact feature space. In the learned feature space, we present a two-stage clustering algorithm that does not require the number of clusters. Our framework was examined in a large-scale video dataset. Experimental results demonstrate the feasibility and effectiveness of our proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						38	48		10.1016/j.neucom.2019.09.023													
J								Advanced stability criteria for static neural networks with interval time-varying delays via the improved Jensen inequality	NEUROCOMPUTING										Static neural networks; Interval time-varying delay; Improved Jensen inequality; Lyapunov-Krasovskii functional; Stability criteria	DEPENDENT STABILITY; EXPONENTIAL STABILITY; PASSIVITY; SYSTEMS	This paper investigates the asymptotic stability of static neural network (SNN) with interval time-varying delay. A proper Lyapunov-Krasovskii functional (LKF) is presented to coordinate with the features of the improved Jensen inequality. Based on the new proposed LKF together with the improved Jensen inequality and a parameter-dependent matrix inequality, new sufficient conditions ensuring the delay-dependent stability are derived in terms of linear matrix inequality (LMI). By considering a marginal augmented vector with two complementary integral terms to yield more crossing information, a further enhanced stability criterion is obtained. The validity of the theorectical results is demonstrated by two numerical examples. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						49	56		10.1016/j.neucom.2019.10.034													
J								LMI-based criterion on stochastic ISS property of delayed high-order neural networks with explicit gain functions and simply event-triggered mechanism	NEUROCOMPUTING										Input-to-state stability; High-order neural networks; Markovian jumping; Lyapunov function method; Time-delays; Neumann boundary	TO-STATE STABILITY; MULTIAGENT SYSTEMS; NONLINEAR-SYSTEMS; INPUT; SYNCHRONIZATION	In this paper, by transforming the high-order system into the system with vector-matrix form, the authors employed variational methods in Sobolev spaces, Lyapunov function method, Dynkin's formula and comparison principle to deduce the stochastic input-to-state stability in mean square on the high-order time-delays reaction-diffusion neural networks under the concise event-triggered mechanism. Remarkably, it is the first paper in which the LMI-based criterion of input-to-state stability of time-delays high-order reaction-diffusion neural networks with event-triggered control is derived, in which the diffusion item plays its role. Not similarly as those of previous literature, the sufficient conditions of the main result in this paper are not involved to Lyapunov function, which implies that the conditions of this paper is easier to be verified than ever. With the help of computer Matlab LMI toolbox, a numerical example illustrates the effectiveness of proposed methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 15	2020	377						57	63		10.1016/j.neucom.2019.10.030													
J								Multi-scene ancient chinese text recognition	NEUROCOMPUTING										Multi-scene ancient chinese text recognition (MACR); Synthetic dataset; Multi-model ensemble (MME)		Multi-scene ancient Chinese text recognition (MACR) is a challenging task for ordinary people without relevant professional knowledge. Due to language barriers and the lack of related open datasets, there is little research on MACR. In this paper, a multi-scene ancient Chinese text (MACT) dataset, formed by handwritten text, calligraphy, natural scene text in ancient fonts, is established that includes synthetic samples generated for training and real scene samples collected for testing. We first perform experiments on some CNN structures as the baseline method, and the top-1 recognition result, 66.94%, is approximately 13.96% higher than subjective human recognition results. Furthermore, based on these models and confidence score from the baseline, a multi-model ensemble (MME) method is proposed, which adopts auxiliary datasets and a feature extraction method to augment data before training, utilizes different hyper-parameters to optimize in training, and integrates multiple models to improve recognition accuracy. The top-1 accuracy results of the MME method reach 73.36% and other top-n results also greatly surpass the baseline results. The MACT dataset is publicly available on the website(1). (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						64	72		10.1016/j.neucom.2019.10.029													
J								Comparison of end-to-end and hybrid deep reinforcement learning strategies for controlling cable-driven parallel robots	NEUROCOMPUTING										Deep reinforcement learning; End-to-end DRL strategy; Hybrid DRL strategy; Deep deterministic policy gradient; Cable-driven parallel robot	FORCE-CLOSURE ANALYSIS; DESIGN	Deep reinforcement learning (DRL) has been proven effective in learning policies of high-dimensional states and actions. Recently, a variety of robot manipulation tasks have been accomplished using end-to-end DRL strategies. An end-to-end DRL strategy accomplishes a robot manipulation task as a black box. On the other hand, a robot manipulation task can be divided into multiple subtasks and accomplished by non-learning-based approaches. A hybrid DRL strategy integrates DRL with non-learning-based approaches. The hybrid DRL strategy accomplishes some subtasks of a robot manipulation task by DRL and the rest subtasks by non-learning-based approaches. However, the effects of integrating DRL with non-learning-based approaches on the learning speed and the robustness of DRL to model uncertainties have not been discussed. In this study, an end-to-end DRL strategy and a hybrid DRL strategy are developed and compared in controlling a cable-driven parallel robot. This study shows that, by integrating DRL with non-learning-based approaches, the hybrid DRL strategy learns faster and is more robust to model uncertainties than the end-to-end DRL strategy. This study demonstrates that, by taking advantages of both learning and non-learning-based approaches, the hybrid DRL strategy provides an alternative to accomplish a robot manipulation task. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						73	84		10.1016/j.neucom.2019.10.020													
J								Non-sparse label specific features selection for multi-label classification	NEUROCOMPUTING										Multi-label learning; Numerical label; Label specific features	SUPPORT VECTOR MACHINES	In multi-label learning, extracting specific features for each label is a new strategy to construct discriminative classification models in recent years. Existing approaches usually consider that each label is only associated with a small subset of the original features. But in some applications this sparsity assumption does not hold, where non-sparse label specific features are more discriminative than sparse label specific features. In this paper, a novel feature selection-based approach is proposed to extract non-sparse label specific features. Firstly, we translate the logic labels to the numerical ones to convey more semantic information and embed the label correlations. Secondly, a linear regression is modeled to describe the discrimination of label specific features based on the numerical labels. To our best knowledge, it is one of the first attempts to utilize the numerical labels for extracting label specific features. Comprehensive experiments on several multi-label data sets clearly manifest that the superiority of our proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						85	94		10.1016/j.neucom.2019.10.016													
J								Sample selection-based hierarchical extreme learning machine	NEUROCOMPUTING										Sample selection; Fuzzy C-means clustering; Condensed nearest neighbour; Hierarchical extreme learning machine	ELM; ALGORITHM; DEEP	Large amounts of training data in machine learning can keep the accuracy high to a certain extent, but the time costs are high because of the exorbitant amount of data and their dimensionality. Therefore, how to simultaneously select the most useful training data set and extract the main features of the samples, especially for image data, are essential problems that urgently need to be solved in the field of large-scale machine learning. Herein, a training sample selection method that is based on the fuzzy c-means clustering algorithm (FCM) is proposed for the problems. It first utilises condensed nearest neighbour (CNN) to make a preliminary selection of training samples. Then, it utilises the FCM to get the centres of the selected data, and, finally, it effectively condenses the sample using a compression parameter. Meanwhile, considering the critical influence of the sample features on the classification model, this paper selects the hierarchical extreme learning machine (H-ELM) model to better solve the classification task. Based on this, the paper presents the FCM-CNN-H-ELM framework for data classification, which combines FCM-Based CNN and H-ELM. The results of the experiments show that the proposed training sample selection method and classification framework can guarantee consistent, even higher, prediction results with a small number of training samples, and significantly reduce the training time. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						95	102		10.1016/j.neucom.2019.10.013													
J								MLP neural network-based recursive sliding mode dynamic surface control for trajectory tracking of fully actuated surface vessel subject to unknown dynamics and input saturation	NEUROCOMPUTING										Trajectory tracking; Recursive sliding mode control; Input saturation; The Nussbaum function; The minimal learning parameter	PATH-FOLLOWING CONTROL; DESIGN; SHIPS; DSC	In this work, we present a MLP neural network-based recursive sliding mode dynamic surface control scheme for a fully actuated surface vessel with uncertain dynamics and external disturbances, where the control input are required to be constrained. First of all, the minimum learning parameter (MLP) neural networks (NN)-based are designed to enhance the robustness against model uncertainties. Subsequently, an adaptive law is employed to compensate neural networks approximation errors and disturbances. The recursive sliding mode control method combined with dynamic surface control (DSC) is designed to eliminate repeated derivative of virtual control laws and enhance systems robustness. A smooth hyperbolic tangent function is incorporated with the control scheme to reduce the risk of actuator saturation. At the same time, the Nussbaum function is used to compensate for the saturation function and ensure the stability of system. We show that under the proposed control method, despite the presence of system uncertainties and disturbances, the tracking errors can converge into arbitrarily small neighborhoods around zero, while the constraint requirements on the control force and torque will not be violated. By using the Lyapunov function, it is proven that the proposed control method can guarantee the uniform boundedness of all the closed loop signals. Finally, simulation results further demonstrate the effectiveness of the proposed method. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						103	112		10.1016/j.neucom.2019.08.090													
J								Sentence guided object color change by adversarial learning	NEUROCOMPUTING										Generative adversarial networks (GANs); Sentence; Color change		In this paper, we propose a novel problem that is sentence guided object color change. Based on an original sentence and the corresponding image, by modifying this original sentence, the object color of corresponding image is changed along with the modified target sentence. This problem has two difficulties: (1) How to design a model to learn the change of sentence? (2) How to balance the relationship between the local object and the whole image during learning process? Confronted with these difficulties, as far as we know, few existing methods deal with them effectively. Therefore, we propose a new cascaded model to solve this problem based on generative adversarial networks (GANs). We employ the adversarial game to build a cascaded model, which learns the changed information of a sentence. Then, we specially design a penalty balance term to balance the relationship between local object and entire image in the generated image. Finally, experimental results on the flower and bird datasets demonstrate the validity of the proposed model. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						113	121		10.1016/j.neucom.2019.10.012													
J								A multi-task learning model with adversarial data augmentation for classification of fine-grained images	NEUROCOMPUTING										Deep neural networks; Generative adversarial network; Data augmentation; Multi-task learning		A large set of training samples is a prerequisite to effectively learn deep neural networks for image classification. When the labeled samples are scarce, it often fails to produce a promising model. Data augmentation is a widely used technique to overcome the issue, which enlarges the training samples with label invariant transformations, e.g., rotation, flip and random crop etc. However, the diversity of images generated by standard data augmentation is quite limited and thus the final improvement on classification accuracy is not much, especially for fine-grained classification problem. In this paper, we propose a two-stage generative adversarial network, namely Fine-grained Conditional Adversarial Network (F-CGAN), which can produce class-dependent synthetic images with fine-grained details. Moreover, to leverage the synthetic images for fine-grained classification, we develop a multi-task learning classifier, which categorizes training images and synthetic images simultaneously. Experimental results on CUB Birds and Stanford Dogs data sets show that the proposed method indeed improves the classification accuracy. (c) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				FEB 15	2020	377						122	129		10.1016/j.neucom.2019.10.002													
J								Resonance transmission of multiple independent signals in cortical networks	NEUROCOMPUTING										Stochastic resonance; Cortical networks; Multiple signal transmission; Excitation-inhibition gain	STOCHASTIC RESONANCE; NEURAL-NETWORKS; FIRING-RATE; OSCILLATIONS; INHIBITION; COMMUNICATION; INTERNEURONS; INFORMATION; CORTEX; SYNCHRONIZATION	Multiple sensory signal, such as various visual stimuli, can be simultaneously transmitted and processed through different cortical areas. Using a biologically inspired model, we investigate the mechanism underlying such multiplexing in cortex. The network model is comprised of five feedforward-connected neural population of excitatory (E) and inhibitory (I) spiking neurons, each representing a cortical area. Numerical results indicate that multiple input signals are independently transmitted through feedforward neural networks via stochastic resonance (SR), and the transmission of information between adjacent cortical areas is altered by E-I relation of local cortical circuits. Due to different intrinsic properties of neurons, excitatory population can induce stochastic resonance to respond to low-frequency signal, and inhibitory neurons tend to respond to high-frequency signal through resonance. The E-I coupled neural ensemble shows selectivity for different input signal, which is gated by the gain between excitatory and inhibitory neurons. To be specific, neural network is inclined to gate-on signal with low-frequency when the excitation exceeds inhibition, whereas the high-frequency signal is selected. Moreover, neural signal can be transmitted from area to area only when the input frequency coincides with the inherent frequency of receivers in posterior areas, thus the independent conduction pathway is established through resonance. The transmission efficiency largely depends on the gain between excitatory and inhibitory input. Mean-field theory is further applied to validate the multiplexing in cortical networks and demonstrate the effective transmission of multiple information via SR in cortical networks. (R) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						130	144		10.1016/j.neucom.2019.10.037													
J								Adaptive neural output feedback fault tolerant control for a class of uncertain nonlinear systems with intermittent actuator faults	NEUROCOMPUTING										Intermittent actuator faults; Adaptive neural control; Command filtered backstepping; Nonlinear system	DYNAMIC SURFACE CONTROL; FAILURE COMPENSATION; FLIGHT CONTROL	In real applications, actuators of control systems frequently encounter unknown intermittent faults during operation while effectively handing such faults is still a challenge. In this paper, an adaptive neural output feedback fault tolerant control (FTC) scheme based on the command filtered backstepping is developed for a class of uncertain nonlinear systems to address this challenge. In this scheme, a stable nonlinear observer is designed to estimate the system states and neural networks with random hidden nodes are utilized in this observer to approximate unknown functions. A projection algorithm is adopted to estimate system unknown parameters such that the boundedness of parameter estimates is guaranteed. It is proved that the boundedness of all signals in the closed-loop system can be ensured by the proposed modified Lyapunov function. Also the ultimate bound of the tracking error depends on design parameters, adjustable jumping amplitude of Lyapunov function and minimum fault time interval. A truncated L-2 bound isestablishedbyiterative calculationtoillustrate thatthetransienttracking errorperformance is determined by design parameters in the controller and observer. Applications on two simulation examples validate the effectiveness of the proposed scheme. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						145	158		10.1016/j.neucom.2019.09.040													
J								Finite-time synchronization of memristive Cohen-Grossberg neural networks with time delays	NEUROCOMPUTING										Memristor; Finite-time synchronization; Cohen-Grossberg neural networks; Time delays	STABILITY ANALYSIS; EXPONENTIAL SYNCHRONIZATION; ANTI-SYNCHRONIZATION	This paper discusses the finite-time synchronization of memristor-based Cohen-Grossberg neural networks with time-varying delays. By using nonlinear transformation, an equivalent system is obtained from the discussed neural network. By investigating the finite-time synchronization of the alternative system, some finite-time synchronization criteria of the considered memristor-based Cohen-Grossberg neural network can be obtained. In the whole process, the error state vertical bar e(t)vertical bar are divided into two procedures: vertical bar e(t)vertical bar moving from initial value to 1, next, vertical bar e(t)vertical bar from 1 to 0. Especially, the error state variables e(t) move to 1 in a finite time, and move to 0 in a fixed time. Compared to the classical Lyapunov asymptotic convergence, the finite-time convergence of this paper is a new way to study the synchronization problems. Finally, numerical simulations are presented to display the obtained theoretical results. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						159	167		10.1016/j.neucom.2019.10.036													
J								Adaptive passivity and synchronization of coupled reaction-diffusion neural networks with multiple state couplings or spatial diffusion couplings	NEUROCOMPUTING										Passivity; Synchronization; Coupled reaction-diffusion neural networks; Multiple state couplings; Multiple spatial diffusion couplings	H-INFINITY SYNCHRONIZATION; TIME-VARYING DELAYS; DYNAMICAL NETWORKS; COMPLEX NETWORKS; PINNING CONTROL; STRATEGIES; STABILITY; WEIGHTS; TERMS; ARRAY	In this paper, two types of coupled reaction-diffusion neural networks with multiple state couplings or spatial diffusion couplings are presented. By selecting appropriate adaptive control schemes and employing inequality techniques, several passivity conditions for these network models are given. In addition, two sufficient conditions for ensuring the synchronization of the proposed network models are also established by exploiting the output-strictly passivity. Finally, we give two numerical examples to verify the effectiveness of the derived criteria. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						168	181		10.1016/j.neucom.2019.10.027													
J								Unsupervised selective rank fusion for image retrieval tasks	NEUROCOMPUTING										Content-based image retrieval; Unsupervised late fusion; Rank-aggregation; Correlation measure; Effectiveness estimation	QUERY DIFFICULTY PREDICTION; RE-RANKING; DIFFUSION PROCESS; SHAPE; SIMILARITY; SCALE; COLOR; CLASSIFICATION; REPRESENTATION; DESCRIPTORS	Several visual features have been developed for content-based image retrieval in the last decades, including global, local and deep learning-based approaches. However, despite the huge advances in features development and mid-level representations, a single visual descriptor is often insufficient to achieve effective retrieval results in several scenarios. Mainly due to the diverse aspects involved in human visual perception, the combination of different features has been establishing as a relevant trend in image retrieval. An intrinsic difficulty consists in the task of selecting the features to combine, which is often supported by supervised learning approaches. Therefore, in the absence of labeled data, selecting features in an unsupervised way is a very challenging, although essential task. In this paper, an unsupervised framework is proposed to select and fuse visual features in order to improve the effectiveness of image retrieval tasks. The framework estimates the effectiveness and correlation among features through a rank-based analysis and uses a list of ranker pairs to determine the selected features combinations. High-effective retrieval results were achieved through a comprehensive experimental evaluation conducted on 5 public datasets, involving 41 different features and comparison with other methods. Relative gains up to +55% were obtained in relation to the highest effective isolated feature. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						182	199		10.1016/j.neucom.2019.09.065													
J								A structured perspective of volumes on active learning	NEUROCOMPUTING										Active learning; Version space; Hypothesis; Hypersphere; Outer volume; Inner volume	VECTOR MACHINES	We approximate the version space which covers all feasible classification hypotheses into a structured geometric hypersphere against agnostic distribution. We present a structured perspective that divides the available active learning (AL) sampling approaches into two kinds of strategies: Outer Volume Sampling and Inner Volume Sampling. For the outer volume, it is represented by a circumscribed hypersphere which would exclude any outlier (non-promising) hypothesis from the version space globally. While for the inner volume, it is represented by many inscribed hyperspheres, which cover most of hypotheses within the outer volume. To enhance the performance of AL, we aggregate the two kinds of volumes to eliminate their sampling biases via finding the optimal inscribed hyperspheres in the enclosing space of outer volume. We then propose a Volume-based Model for the AL sampling without any distribution assumption. To generalize our theoretical model, in a non-linear feature space, spanned by kernel, we use sequential optimization to globally optimize the original space to a sparse space by halving the size of the kernel space. Then, the expectation maximization (EM) model which returns the local center helps us to find a local representation. To describe this process, we propose an easy-to-implement algorithm called Volume-based AL (VAL). Empirical evaluation on a various set of structured clustering and unstructured handwritten digit data sets have demonstrated that, employing our proposed model can accelerate the decline of the prediction error rate with fewer sampling number compared with the other algorithms. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						200	212		10.1016/j.neucom.2019.10.056													
J								A literature survey on kinship verification through facial images	NEUROCOMPUTING										Kinship verification; Survey; Computer vision	FACE RECOGNITION; REPRESENTATION; SELECTION; OBJECT	Kinship verification is an emerging task in computer vision which aims at finding out whether there is a kin relation between given identities through their facial images. Applications of kinship verification include image annotation, children adoption, and social media analysis, etc. However, kinship verification through facial images is challenging because facial images usually contain high intra-variances, which vary from genetic, age and gender difference. Over the past decade, more and more effective methods have emerged. This paper aims at categorizing and evaluating these methods systematically. We attach great importance to the difficulties in practical applications of kinship verification, and review the prominent algorithms from the perspective of learning more efficient models with more diverse kin relations. Then we further show how to develop an efficient and robust kinship verification system. Finally we present several potential directions for future research. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						213	224		10.1016/j.neucom.2019.09.089													
J								A penalty-like neurodynamic approach to constrained nonsmooth distributed convex optimization	NEUROCOMPUTING										Constrained distributed optimization; Neurodynamic approach; Differential inclusion	RECURRENT NEURAL-NETWORK; MULTIAGENT SYSTEMS; ALGORITHM	A nonsmooth distributed optimization problem subject to affine equality and convex inequality is considered in this paper. All the local objective functions in the distributed optimization problem possess a common decision variable. And taking privacy into consideration, each agent doesn't share its local information with other agents, including the information about the local objective function and constraint set. To cope with this distributed optimization, a neurodynamic approach based on the penalty-like methods is proposed. It is proved that the presented neurodynamic approach is convergent to an optimal solution to the considered distributed optimization problem. The proposed neurodynamic approach in this paper has lower model complexity and computational load via reducing auxiliary variables. In the end, two illustrative examples are given to show the effectiveness and practical application of the proposed neural network. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						225	233		10.1016/j.neucom.2019.10.050													
J								Observer-based consensus for linear multi-agent systems with intermittent communication	NEUROCOMPUTING										Multi-agent systems; Consensus; Intermittent communication; Observer; Sampled data	DISTRIBUTED CONSENSUS; NETWORKS; SYNCHRONIZATION; DYNAMICS; FEEDBACK; DELAY	This paper considers an observer-based consensus problem of linear multi-agent systems with intermittent communication. An observer-based protocol with both continuous and sampled data is proposed. Consensus criteria and protocol design are obtained by matrix theory. Compared with the previous protocol, it is shown that our protocol can solve the consensus problem with lower communication time rate especially when the time interval without communication is sufficiently short. Finally, several examples are provided to show the effectiveness of the derived results. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						234	242		10.1016/j.neucom.2019.10.039													
J								Integral reinforcement learning-based online adaptive event-triggered control for non-zero-sum games of partially unknown nonlinear systems	NEUROCOMPUTING										Event-triggered control (ETC); Integral reinforcement learning (IRL); Adaptive dynamic programming (ADP); Adaptive critic design; Non-zero-sum (NZS) games	OPTIMAL TRACKING CONTROL; DIFFERENTIAL-GAMES; FEEDBACK CONTROL; TIME; ALGORITHM; HORIZON; INPUT	This paper develops an integral reinforcement learning (IRL)-based adaptive control method for the multi-player non-zero-sum (NZS) games of the nonlinear continuous-time systems with partially unknown dynamics, in the context of event-triggered mechanism. With the principle of IRL method, the requirement for the system drift dynamics is relaxed in the controller design. Moreover, different from the conventional iteration computation methods, the algorithm developed in this work is implemented in an online adaptive fashion, which provides a new way to combine the IRL algorithm and the event-triggered control framework in solving the NZS game issues. In the event-based algorithm, a state-dependent triggering condition is presented, which not only guarantees the closed-loop system stability, but also reduces the computation and communication loads of the controlled plant. By means of Lyapunov theorem, the uniform ultimate boundedness (UUB) properties of the system states and the critic weight estimation errors have been proved. Finally, two numerical examples are utilized to demonstrate the efficacy of the proposed method. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						243	255		10.1016/j.neucom.2019.09.088													
J								Stereoscopic video saliency detection based on spatiotemporal correlation and depth confidence optimization	NEUROCOMPUTING										Stereoscopic video; Saliency detection; Spatiotemporal correlation; Cepth confidence optimization	VISUAL-ATTENTION; OBJECT DETECTION; MODEL; SEGMENTATION; CONTRAST; IMAGE	Effective stereoscopic video saliency detection is challenging due to the diversity of visual contrast and the variability of object motion. In this paper, we propose a novel stereoscopic video saliency detection method based on spatiotemporal correlation and depth confidence optimization. First, spatial saliency is obtained by color contrast computation and further enhancing spatial correlation between neighbors. Second, temporal saliency is calculated by propagating motion information between frames. The special propagation in sequential frames and reverse sequential frames augments the temporal correlation. Furthermore, a depth confidence optimization is proposed to fuse the spatial saliency and temporal saliency to generate optimal saliency map adaptively. Experimental results on three datasets demonstrate the efficiency of the proposed saliency method. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						256	268		10.1016/j.neucom.2019.10.024													
J								A self-organizing developmental cognitive architecture with interactive reinforcement learning	NEUROCOMPUTING										Cognitive development; Online learning; Self-organizing neural network; Object recognition; Interactive reinforcement learning	NEURAL-NETWORK; KNOWLEDGE; ROBOTICS; MODEL; POWER	Developmental cognitive systems can endow robots with the abilities to incrementally learn knowledge and autonomously adapt to complex environments. Conventional cognitive methods often acquire knowledge through passive perception, such as observing and listening. However, this learning way may generate incorrect representations inevitably and cannot correct them online without any feedback. To tackle this problem, we propose a biologically-inspired hierarchical cognitive system called Self-Organizing Developmental Cognitive Architecture with Interactive Reinforcement Learning (SODCA-IRL). The architecture introduces interactive reinforcement learning into hierarchical self-organizing incremental neural networks to simultaneously learn object concepts and fine-tune the learned knowledge by interacting with humans. In order to realize the integration, we equip individual neural networks with a memory model, which is designed as an exponential function controlled by two forgetting factors to simulate the consolidation and forgetting processes of humans. Besides, an interactive reinforcement strategy is designed to provide appropriate rewards and execute mistake correction. The feedback acts on the forgetting factors to reinforce or weaken the memory of neurons. Therefore, correct knowledge is preserved while incorrect representations are forgotten. Experimental results show that the proposed method can make effective use of the feedback from humans to improve the learning effectiveness significantly and reduce the model redundancy. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						269	285		10.1016/j.neucom.2019.07.109													
J								Tensor cross-view quadratic discriminant analysis for kinship verification in the wild	NEUROCOMPUTING										Kinship verification; Cross-view; Tensor XQDA	PRINCIPAL COMPONENT ANALYSIS; FACE; DEEP; RECOGNITION; FRAMEWORK; FEATURES; SUBJECT	This paper presents a new Tensor Cross-view Quadratic Discriminant Analysis (TXQDA) method based on the XQDA method for kinship verification in the wild. Many researchers used metric learning methods and have achieved reasonably good performance in kinship verification, none of these methods looks at the kinship verification as a cross-view matching problem. To tackle this issue, we propose a tensor cross-view method to train multilinear data using local histograms of local features descriptors. Therefore, we learn a hierarchical tensor transformation to project each pair face images into the same implicit feature space, in which the distance of each positive pair is minimized and that of each negative pair is maximized. Moreover, TXQDA was proposed to separate the multifactor structure of face images (i.e. kinship, age, gender, expression, illumination and pose) from different dimensions of the tensor. Thus, our TXQDA achieves better classification results through discovering a low dimensional tensor subspace that enlarges the margin of different kin relation classes. Experimental evaluation on five challenging databases namely Cornell KinFace, UB KinFace, TSKinFace, KinFaceW-II and FIW databases, show that the proposed TXQDA significantly outperforms the current state of the art. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						286	300		10.1016/j.neucom.2019.10.055													
J								Fixed pattern noise reduction for infrared images based on cascade residual attention CNN	NEUROCOMPUTING										Fixed pattern noise reduction; Convolution neural network; Multi-scale convolution; Visual attention mechanism	ADAPTIVE NONUNIFORMITY CORRECTION; ALGORITHM	Existing fixed pattern noise reduction (FPNR) methods are easily affected by the motion state of the scene and working condition of the image sensor, which leads to over smooth effects, ghosting artifacts as well as slow convergence rate. To address these issues, we design an innovative cascade convolution neural network (CNN) model with residual skip connections to realize single frame blind FPNR operation without any parameter tuning. Moreover, a coarse-fine convolution (CF-Conv) unit is introduced to extract complementary features in various scales and fuse them to pick more spatial information. Inspired by the success of the visual attention mechanism, we further propose a particular spatial-channel noise attention unit (SCNAU) to separate the scene details from fixed pattern noise more thoroughly and recover the real scene more accurately. Experimental results on test data demonstrate that the proposed cascade CNN-FPNR method outperforms the existing FPNR methods in both of visual effect and quantitative assessment. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						301	313		10.1016/j.neucom.2019.10.054													
J								Event-triggered ISS-modular neural network control for containment maneuvering of nonlinear strict-feedback multi-agent systems	NEUROCOMPUTING										Containment maneuvering; MIMO strict-feedback system; Event-triggered modular-ISS neural network control; Predictor	AUTONOMOUS UNDERWATER VEHICLES; DYNAMIC SURFACE CONTROL; TRACKING CONTROL; LEADER	The paper is concerned with containment maneuvering of nonlinear multi-input multi-output (MIMO) strict-feedback multi-agent systems under the distributed directed communication. Followers are driven into a convex hull spanned by all virtual leaders guided by desired parameterized paths. Multi-agent systems are driven to achieve the desired behavior. A containment maneuvering controller is developed based on an event-triggered modular-ISS neural network control method to decouple estimation and control. Specifically, in the estimation loop, an RBF-network-based predictor module is constructed to design the adaptation law, where an RBF network is employed to identify the uncertain nonlinear dynamics. An event-triggered communication mechanism is introduced to decrease the burden of followers communication. In the control loop, a control module is constructed by using the output of predictor module and a third-order linear tracking differentiator. An event-triggered actuation mechanism is introduced to reduce the actuator burden of followers. A path update law is constructed to achieve coordination among leaders. An event-triggered communication scheme is introduced for leaders to decrease the communication burden of leaders. The input-to-state stability of the total cascade system is demonstrated by using the cascade stability theorem. Simulation results are provided to substantiate the efficacy of the proposed containment maneuvering controller for nonlinear MIMO strict-feedback multi-agent systems. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						314	324		10.1016/j.neucom.2019.10.043													
J								Joint extraction of entities and relations by a novel end-to-end model with a double-pointer module	NEUROCOMPUTING										Double-pointer module; Convolution neural network; Self-attention; Relation extraction; Joint learning		Joint extraction of entities and relations is to detect entities and recognize semantic relations simultaneously. However, some existing joint models predict relations on words, instead of entities. These models cannot make full use of the entity information when predicting relations, which will affect relation extraction. We propose an end-to-end model with a double-pointer module that can jointly extract whole entities and relations. The double-pointer module is combined with multiple decoders to predict the start and end positions of the entity in the input sentence. In addition, in order to learn the relevance between long-distance entities effectively, the multi-layer convolution and self-attention mechanism are used as an encoder, instead of Bi-RNN. We conduct experiments on two public datasets and our models outperform the baseline methods significantly. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						325	333		10.1016/j.neucom.2019.09.097													
J								Impulsive synchronization of two coupled delayed reaction-diffusion neural networks using time-varying impulsive gains	NEUROCOMPUTING										Impulsive synchronization; Reaction-diffusion neural networks; Time-varying impulsive gains; Impulse-time-dependent discretized; Lyapunov functions	H-INFINITY SYNCHRONIZATION; EXPONENTIAL SYNCHRONIZATION; INTERMITTENT SYNCHRONIZATION; ADAPTIVE SYNCHRONIZATION; L-2-GAIN ANALYSIS; MIXED DELAYS; STABILITY; SYSTEMS; TERMS; STABILIZATION	In this paper, the problem of impulsive synchronization of two coupled delayed reaction-diffusion neural networks under aperiodic discrete measurements is revisited. Different from the previous static impulsive gain based impulsive synchronization strategy, a novel impulsive synchronization strategy using sampling-interval-dependent impulsive gains is proposed. The time-varying impulsive synchronization gains are able to adapt to the variation of sampling intervals, and thus can improve the synchronization performance. The stability analysis of the resultant synchronization error system is performed by applying an impulse-time-dependent discretized Lyapunov functions based method. Sufficient conditions for the existence of desired impulsive synchronization controllers are derived in terms of a set of linear matrix inequalities (LMIs). These conditions allow to synthesize time-varying impulsive gains. A numerical example is presented to demonstrate the effectiveness of the developed methodology. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						334	344		10.1016/j.neucom.2019.08.098													
J								RADC-Net: A residual attention based convolution network for aerial scene classification	NEUROCOMPUTING										Convolutional neural network (CNN); Residual attention; Dense connection; Enhanced classification layer; Aerial image	ENSEMBLE	With rapid development of satellite and airplane platforms, aerial image has become more and more accessible. Aerial image scene classification plays an important role in many remote sensing and geo-science applications. Deep learning methods, especially convolutional neural network (CNN), have boosted the performance of aerial image scene classification significantly because of the strong feature representation ability. However, the distribution of geo-spatial objects and the spatial arrangement of aerial image scenes is often more complicated than natural image scenes. While current CNNs usually highlight the global semantics, more local semantics and more discriminative features need to be preserved to deal with the aforementioned challenges in aerial scenes. In this paper, we propose a residual attention based dense connected convolutional neural network (RADC-Net) to tackle this problem. This framework contains three dense block each followed by an attention block and an enhanced classification layer. Firstly, we simplify the current dense connection structure so that our dense block has much fewer parameters while maintaining discriminative convolutional feature representation ability. Then, we propose a novel residual attention block for our framework to highlight the local semantics relevant to the aerial scenes. Finally, we introduce an enhanced classification layer in our framework to further refine the extracted convolutional features and highlight local semantic information. We comprehensively evaluate the performance of our proposed RADC-Net on three publicly available benchmark datasets. Experimental results demonstrate that our proposed RADC-Net outperforms some state-of-the-art methods with much fewer parameters. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				FEB 15	2020	377						345	359		10.1016/j.neucom.2019.11.068													
J								Visual Sentiment Prediction with Attribute Augmentation and Multi-attention Mechanism	NEURAL PROCESSING LETTERS										Visual sentiment analysis; Sentiment classification; Attribute detection; Multi-attention mechanism		Recently, many methods that exploit attention mechanism to discover the relevant local regions via visual attributes, have demonstrated promising performance in visual sentiment prediction. In these methods, accurate detection of visual attributes is of vital importance to identify the sentiment relevant regions, which is crucial for successful assessment of visual sentiment. However, existing work merely utilize basic strategies on convolutional neural network for visual attribute detection and fail to obtain satisfactory results due to the semantic gap between visual features and subjective attributes. Moreover, it is difficult for existing attention models to localize subtle sentiment relevant regions, especially when the performance of attribute detection is relatively poor. To address these problems, we first design a multi-task learning based approach for visual attribute detection. By augmenting the attributes with sentiments supervision, the semantic gap can be effectively reduced. We then develop a multi-attention model for jointly discovering and localizing multiple relevant local regions given predicted attributes. The classifier built on top of these regions achieves a significant improvement in visual sentiment prediction. Experimental results demonstrate the superiority of our method against previous approaches.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2403	2416		10.1007/s11063-020-10201-2		FEB 2020											
J								Encoding features from multi-layer Gabor filtering for visual smoke recognition	PATTERN ANALYSIS AND APPLICATIONS										Smoke recognition; Gabor transform; Gabor convolutional network; Local binary pattern (LBP); Basic grouping method; Aggregation function	LOCAL BINARY PATTERNS; FACE RECOGNITION; IMAGE; REPRESENTATION; NORMALIZATION; DESCRIPTOR; HISTOGRAM; ADABOOST; MODEL	It is a challenging task to accurately recognize smoke from visual scenes due to large variations in smoke shape, color and texture. To improve recognition accuracy, we propose a framework mainly with a robust local feature extraction module based on Gabor convolutional networks. We first propose a Gabor convolutional network, each layer of which mainly consists of Gabor convolution and feature fusion. To fuse feature maps generated by Gabor convolution, we present three Basic Grouping Methods, which divide the feature maps into several groups along orientation axis, scale axis and both of them. To avoid exponential growth of feature maps and preserve discriminative information simultaneously, we propose three element-wise aggregation functions, which are mean, min and max, to combine feature maps in each group. To further improve efficiency, we use local binary pattern to encode hidden and output maps of Gabor convolutional layers. In addition, we use a weight vector to optimize concatenation of histograms for further improvement. Experiments show that our method achieves very outstanding results on smoke, texture and material image datasets. Although the feature extraction step of our method is training-free, our framework consistently outperforms state-of-the-art methods on small smoke datasets, even including deep learning-based methods.																	1433-7541	1433-755X				AUG	2020	23	3					1117	1131		10.1007/s10044-020-00864-x		FEB 2020											
J								An instance and variable selection approach in pixel-based classification for automatic white blood cells segmentation	PATTERN ANALYSIS AND APPLICATIONS										Instance and variable selection; Random Forest; Data reduction; Small target detection; Automatic segmentation; Pixel-based classification; White blood cells	COLOR IMAGE SEGMENTATION; ALGORITHM; CLASSIFIERS; FRAMEWORK	Instance and variable selection involve identifying a subset of instances and variables such that the learning process will use only this subset with better performances and lower cost. Due to the huge amount of data available in many fields, data reduction is considered as an NP-hard problem. In this paper, we present a simultaneous instance and variable selection approach based on the Random Forest-RI ensemble methods in the aim to discard noisy and useless information from the original data set. We proposed a selection principle based on two concepts: the ensemble margin and the importance variable measure of Random Forest-RI. Experiments were conducted on cytological images for the automatic segmentation and recognition of white blood cells WBC (nucleus and cytoplasm). Moreover, in order to explore the performance of our proposed approach, experiments were carried out on standardized datasets from UCI and ASU repository, and the obtained results of the instances and variable selection by the Random Forest classifier are very encouraging.																	1433-7541	1433-755X				NOV	2020	23	4					1709	1726		10.1007/s10044-020-00873-w		FEB 2020											
J								From Chance to Serendipity: Knowledge Workers' Experiences of Serendipitous Social Encounters	ADVANCES IN HUMAN-COMPUTER INTERACTION											DIRECTIONS	Serendipity refers to uncontrolled circumstances that lead to unexpected yet fortunate discoveries. The phenomenon has been studied extensively in relation to information retrieval. However, serendipity in the context of social encounters has been the subject of few empirical studies. In professional life, social serendipity might result in benefits such as fruitful collaboration, successful recruitment, discovery of novel information, and acquisition of crucial new perspectives from peers. Despite the potential significance of serendipity, particularly for knowledge work, there is a lack of empirical understanding of related subjective experiences and the role of technology within the process of encountering unsought findings. This qualitative study investigates knowledge workers' detailed narratives of serendipitous social encounters and the related factors through an analysis of 37 responses to an international online survey. We provide a detailed account of the experiential characteristics and contextual qualities of the reported instances of social serendipity. Finally, we discuss the seemingly minor role of technology in social serendipity and research avenues to computationally enhance social serendipity.																	1687-5893	1687-5907				FEB 14	2020	2020								1827107	10.1155/2020/1827107													
J								A single-end directional relaying scheme for double-circuit transmission line using fuzzy expert system	COMPLEX & INTELLIGENT SYSTEMS										Currents; Faults; Fuzzy system	PROTECTION; LOCATION	The faults occurring in different sections are difficult to identify using the traditional techniques. This paper investigates a fuzzy expert system for directional relaying, classification, and location of faults in double-circuit transmission lines. The current magnitudes measured at only one terminal of the double-circuit transmission line are used to compute discreet Fourier coefficients. Thus, this scheme does not involve any communication channel. The presented fuzzy expert system is achieved from the structure of MAMDANI system in LabVIEW software. Test case studies show the effectiveness of the presented scheme. The simulation results attest that the directional relaying, classification, and location estimation is very accurate. This scheme is adaptive to the change of fault location, fault resistance, fault inception angle, and fault type.																	2199-4536	2198-6053				JUL	2020	6	2					335	346		10.1007/s40747-020-00131-w		FEB 2020											
J								A technical view on neural architecture search	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Neural architecture search; AutoML; Deep learning; Machine learning	MODEL SELECTION; ALGORITHMS; NETWORKS	Due to the discovery of innovative and practical neural architectures, deep learning has achieved bright successes in many fields, such as computer vision, natural language processing, recommendation systems, etc. To reach high performance, researchers have to adjust neural architectures and choose training tricks very carefully. The manual trial-and-error process for discovering the best neural network configuration consumes plenty of manpower. The neural architecture search (NAS) aims to alleviate this issue by automatically configuring neural networks. Recently, the rapid development of NAS has shown significant achievements. Novel neural network architectures that outperform the state-of-the-art handcrafted networks have been discovered in image classification benchmarks. In this paper, we survey NAS from a technical view. By summarizing the previous NAS approaches, we drew a picture of NAS for readers including problem definition, search approaches, progress towards practical applications and possible future directions. We hope that this paper can help beginners start their researches on NAS.																	1868-8071	1868-808X				APR	2020	11	4					795	811		10.1007/s13042-020-01062-1		FEB 2020											
J								A q-rung orthopair fuzzy multi-criteria group decision making method for supplier selection based on a novel distance measure	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										q-Rung orthopair fuzzy set; Distance measure; Supplier selection; q-ROF TOPSIS; q-ROF ELECTRE	SIMILARITY MEASURES; SETS; TOPSIS; CRITERIA; OPERATORS; VENDOR	Supplier selection and evaluation is a crucial decision-making issue to establish an effective supply chain. Higher-order fuzzy decision-making methods have become powerful tools to support decision-makers in solving their problems effectively by reflecting uncertainty in calculations better than crisp sets in the last 3 decades. The q-rung orthopair fuzzy (q-ROF) sets which are the general form of both intuitionistic and Pythagorean fuzzy sets, have been recently introduced to provide decision-makers more freedom of expression than other fuzzy sets. In this paper, we introduce q-ROF TOPSIS and q-ROF ELECTRE as two separate methods and new approaches for group decision making to select the best supplier. As the existing distance measures in q-rung orthopair fuzzy environment have some drawbacks and generate counter-intuitive results, we propose a new distance measure along with its proofs to use in both q-ROF TOPSIS and q-ROF ELECTRE methods. Moreover, a comparison study is conducted to illustrate the superiority of the proposed distance measure. Subsequently, a comprehensive case study is performed with q-ROF TOPSIS and q-ROF ELECTRE methods separately to choose the best supplier for a construction company by rating the importance of criteria and alternatives under q-ROF environment. Finally, a comparison and parameter analysis are performed among the proposed q-ROF TOPSIS and q-ROF ELECTRE methods and existing q-ROF decision-making methods to demonstrate the effectiveness of our proposed methods.																	1868-8071	1868-808X				AUG	2020	11	8					1749	1780		10.1007/s13042-020-01070-1		FEB 2020											
J								Dynamic uncertain causality graph based on cloud model theory for knowledge representation and reasoning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										DUCG; Knowledge representation and reasoning; Cloud model; Uncertain information; TOPSIS	FUZZY PETRI NETS; AGGREGATION OPERATORS; METHODOLOGY; SETS	The dynamic uncertain causality graph (DUCG), which has been widely applied in many fields, is an important modelling technique for knowledge representation and reasoning. However, the extant DUCG models have been criticized because they cannot precisely represent experts' knowledge owing to the ignorance of the fuzziness and randomness of uncertain knowledge. In response, we propose a new type of DUCG model called the cloud reasoning dynamic uncertain causality graph (CDUCG). The CDUCG model, which is based on cloud model theory, can handle with the fuzziness and randomness of uncertain information simultaneously. Moreover, an inference algorithm based on the combination of CDUCG and the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) is proposed to implement fuzzy knowledge inference effectively and thus make the expert systems more dependable and intelligent. Finally, illustrative examples and an industrial application concerning root cause analysis of aluminum electrolysis are provided to demonstrate the proposed CDUCG model. And experimental results show that the new CDUCG model is flexible and reliable for knowledge representation and reasoning.																	1868-8071	1868-808X				AUG	2020	11	8					1781	1799		10.1007/s13042-020-01072-z		FEB 2020											
J								Dynamic consent management for clinical trials via private blockchain technology	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dynamic consent management; Trust; Blockchain; Clinical trials	INFORMED-CONSENT	Clinical trials (CTs) are essential for the advancement of medical research, paving the way for the development and adoption of new treatments, and contributing to the evolution of healthcare. An essential factor for the success of a CT is the appropriate management of its participants and their personal data. According to the current regulations, collecting and using personal data from participants must comply with rigorous standards. Therefore, healthcare institutes need to obtain freely given, specific, informed, and unambiguous consent before being able to collect the data. Some of the major limitations of the current technological solutions are the lack of control over the granularity of consent grants, as well as the difficulty of handling dynamic changes of consent over time. In this paper, we present SCoDES, an approach for trusted and decentralized management of dynamic consent in clinical trials, based on blockchain technology (BCT). The usage of blockchain provides a set of features that allow maintaining consent information with trust guarantees while avoiding the need for a dedicated or centralized third trusted party. We provide a full implementation of SCoDES, made available as a self-contained infrastructure, with the possibility to interact with external services, and using hyperledger as a blockchain framework.																	1868-5137	1868-5145															10.1007/s12652-020-01761-1		FEB 2020											
J								A novel approach based on BSPCI for quantifying functional connectivity pattern of the brain's region for the classification of epileptic seizure	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electroencephalography (EEG); Feature extraction; PLV (phase locking value); Bi-spectral phase concurrence index (BSPCI); Classification	EEG; PREDICTION; SYNCHRONY; COHERENCE; SPECTRA	Epilepsy seizure is brain neurological abnormality which arises from the sudden deviation of the electrical interaction in the brain. Electroencephalography (EEGs) are obtained from 22 subjects with epileptic seizure and non-epileptic seizure states recorded using 23 channels with a sampling frequency of 256 Hz. The functional connectivity of the brain region can be exacted from the features obtained from EEG signals by measuring phase locking value (PLV). The neuronal connection in the brain can be expressed in terms of phase synchrony. Despite of the fact that brain states should be characterize independently based upon its extracted features. Therefore, a novel functional connectivity index (FCI) feature is proposed, namely Bi-Spectral Phase Concurrence Index (BSPCI). It is used to represent the spectral information with third cumulant order correlation functions of the EEG signal. In this paper, three FCI features were measured namely, the magnitude's mean of the bi-spectral, the normalized entropy of bi-spectrum (NE1) and the normalized entropy of squared bi-spectrum (NE2) from the BSPCI. Rank sum test based on the Wilcoxon approach is used to find the set of statistical difference between quantitative features extracted from EEG signals. The results provide evidence that the FCI will have an impact in separating the difference among seizure states of various epileptic seizure patients. On the part of reducing a large number of the feature vector, feature selection is performed by utilizing the sequential forward selection method. PLV is measured for quantifying the obtained phase synchrony of EEG signal. For the classification of epileptic seizure, Support Vector Machine is utilized which gain a large accuracy for the proposed bi-spectral analysis method when compared with Incremental Gradient Descent (IGD), Logistic Regression (LR) and Multilayer Perceptrons (MLP). The result is compared with IGD, LR, and MLP for obtaining better performance rate and the classification is 98.79% for the proposed work.																	1868-5137	1868-5145															10.1007/s12652-020-01774-w		FEB 2020											
J								Implementation of enhanced blowfish algorithm in cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless communication; Security; Privacy and trust; Advanced network architecture; Blowfish encryption; Optimal key selection; IDA; Communication	BIG DATA; STORAGE	In the recent years, increase in the mobile devices and cloud processing the data stored in the cloud in the form of pictures, messages and texts. Cloud service providers cannot trust entirely to authorize the services used by the clients. For enhancing the security in the cloud, new cyber security model is introduced with optimal key selection. In this k-medoid clustering algorithm is used to cluster the information which is secret. It is based on the data distance measure. The data is encrypted and stored in the cloud using blowfish encryption. To improve the accuracy improved dragonfly algorithm is used.																	1868-5137	1868-5145															10.1007/s12652-020-01765-x		FEB 2020											
J								Sustainable supplier selection by a new possibilistic hierarchical model in the context of Z-information	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sustainability; Supplier Selection; Fuzzy sets; Z-number	DECISION-MAKING; FUZZY; CHAIN; CRITERIA	In recent years, supplier selection has been significantly important with respect to dimensions and criteria of sustainability. Organizations need to try to choose their suppliers based on how well their performances are in each of the economic, social, and environmental criteria. On the other hand, since the methods of supplier selection depend on the experts' opinions, which have the potential of uncertainty and ambiguity, using Fuzzy sets to evaluate the criteria can be useful. Apart from considering experts' opinions on a fuzzy basis, probabilities are considered in experts' opinions via Z-numbers in order to increase the reliability of the data and the results. In this paper, after reviewing the literature, identifying the sustainability criteria, and step-by-step explaining the presented method, a numerical example is studied for more clarification. Moreover, the results of the conventional fuzzy sets are obtained and have been compared with those considering the probabilities (Z-number) leading to the conclusion that applying the experts' opinions will be effective in ranking the suppliers.																	1868-5137	1868-5145															10.1007/s12652-020-01751-3		FEB 2020											
J								Error recognition of robot kinematics parameters based on genetic algorithms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Genetic algorithms; Robot kinematics; Parameter error recognition technology; Six-DOF parallel robot; Least square method	QUADRUPED ROBOT; WALKING	With the development of modern industrial technologies and intelligent control technology, the requirement for the accuracy of the robot's terminal attitude is getting higher and higher. Therefore, the technology of kinematics parameter identification, robot calibration becomes more and more important. In order to achieve the accurate calibration of the end position of the robot, it is necessary to identify and analyze its errors in order to improve its accuracy. In this paper, the error model of 6-DOF parallel manipulator is constructed based on vector matrix analysis, and the mapping relationship between attitude error and structural parameter error is obtained. In order to solve the problem that the attitude accuracy of the end of a 6-DOF parallel robot decreases, this paper creatively proposes a genetic algorithm to optimize the accuracy of the 6-DOF parallel robot. At the same time, this paper also improves the genetic algorithm by eliminating the similar individuals in the population of the algorithm through the application of the crowding mechanism, thus avoiding the premature convergence of the genetic algorithm. Finally, the proposed algorithm is compared with the least squares method. The simulation results show that the proposed genetic algorithm-based robot kinematics parameter error identification algorithm has obvious advantages.																	1868-5137	1868-5145															10.1007/s12652-020-01781-x		FEB 2020											
J								Towards an automated decision support system for the identification of additive manufacturing part candidates	JOURNAL OF INTELLIGENT MANUFACTURING										Additive manufacturing; Machine learning; Candidate identification; Conceptual design	SPARE PARTS; ENVIRONMENTAL-IMPACT; PROCESS SELECTION; DESIGN; OPTIMIZATION; RECOGNITION; COMPLEXITY; VECTOR; MODEL	As additive manufacturing (AM) continues to mature, an efficient and effective method to identify parts which are eligible for AM as well as gaining insight on what values it may add to a product is needed. Prior methods are naturally developed and highly experience-dependent, which falls short for its objectiveness and transferability. In this paper, a decision support system (DSS) framework for automatically determining the candidacy of a part or assembly for AM applications is proposed based on machine learning (ML) and carefully selected candidacy criteria. With the goal of supporting efficient candidate screening in the early conceptual design stage, these criteria are further individually decoded to decisive parameters which can be extracted from digital models or resource planning databases. Over 200 existing industrial examples are manually collected and labelled as training data; meanwhile, multiple regression algorithms are tested against each AM potential to find better predictive performance. The proposed DSS framework is implemented as a web application with integrated cloud-based database and ML service, which allows advantages of easy maintenance, upgrade, and retraining of ML models. Two case studies of a hip implant and a throttle pedal are used as demonstrating examples. This preliminary work provides a promising solution for lowering the requirements of non-AM experts to find suitable AM candidates.																	0956-5515	1572-8145															10.1007/s10845-020-01545-6		FEB 2020											
J								On the minimal number of generators of endomorphism monoids of full shifts	NATURAL COMPUTING										Full shift; Endomorphisms; Automorphisms; Cellular automata; Minimal number of generators	FINITE-SEMIGROUPS; RANKS	For a group G and a finite set A, denote by Endo(A(G)) the monoid of all continuous shift commuting self-maps of AG and by Auto(A(G)) its group of units. We study the minimal cardinality of a generating set, known as the rank, of Endo(A(G)) and Auto(A(G)). In the first part, when G is a finite group, we give upper and lower bounds for the rank of Auto(A(G)) in terms of the number of conjugacy classes of subgroups of G. In the second part, we apply our bounds to show that if G has an infinite descending chain of normal subgroups of finite index, then Endo(A(G)) is not finitely generated; such is the case for wide classes of infinite groups, such as infinite residually finite or infinite locally graded groups.																	1567-7818	1572-9796															10.1007/s11047-020-09785-4		FEB 2020											
J								An Intuitive End-to-End Human-UAV Interaction System for Field Exploration	FRONTIERS IN NEUROROBOTICS										UAV; intuitive interaction; pose estimation; super-twisting; extended state observer; back-stepping	ATTITUDE STABILIZATION	This paper presents an intuitive end-to-end interaction system between a human and a hexacopter Unmanned Aerial Vehicle (UAV) for field exploration in which the UAV can be commanded by natural human poses. Moreover, LEDs installed on the UAV are used to communicate the state and intents of the UAV to the human as feedback throughout the interaction. A real time multi-human pose estimation system is built that can perform with low latency while maintaining competitive performance. The UAV is equipped with a robotic arm, kinematic and dynamic attitude models for which are provided by introducing the center of gravity (COG) of the vehicle. In addition, a super-twisting extended state observer (STESO)-based back-stepping controller (BSC) is constructed to estimate and attenuate complex disturbances in the attitude control system of the UAV, such as wind gusts, model uncertainties, etc. A stability analysis for the entire control system is also presented based on the Lyapunov stability theory. The pose estimation system is integrated with the proposed intelligent control architecture to command the UAV to execute an exploration task stably. Additionally, all the components of this interaction system are described. Several simulations and experiments have been conducted to demonstrate the effectiveness of the whole system and its individual components.																	1662-5218					FEB 14	2020	13								117	10.3389/fnbot.2019.00117													
J								Difficult first strategy GP: an inexpensive sampling technique to improve the performance of genetic programming	EVOLUTIONARY INTELLIGENCE										Difficult first strategy; Genetic programming; Pre-processing; Sampling techniques; Dataset sampling; Machine learning	SUBSET-SELECTION; FITNESS	Genetic programming (GP) is a top performer in solving classification and clustering problems, in general and symbolic regression problems, in particular. GP has produced impressive results and has outperformed human generated results for 76 different problems taken from 22 different fields. There remain a number of significant open issues despite its impressive results. Among them are high computational cost, premature convergence and high error rate. These issues must be addressed for GP to realise its full potential. In this paper a simple and cost effective technique called Difficult First Strategy-GP (DFS-GP) is proposed to address the aforementioned problems. The proposed technique involves pre-processing and sampling steps. In the pre-processing step, difficult to evolve data points by GP from the given data set are marked and in the sampling step, they are introduced in the evolutionary run by using two newly defined sampling techniques, called difficult points first and difficulty proportionate selection. These techniques are biased towards selecting difficult data points during the initial stage of a run and of easy points in the latter stage of a run. This ensures that GP does not ignore difficult-to-evolve data points during a run. Experiments have shown that GP coupled with DFS avoids premature convergence and attained higher fitness than standard GP using same fitness evaluations. Performance of the proposed technique was evaluated on three commonly known metrics, which are convergence speed, fitness and variance in the best results. Our results have shown that the proposed setups had achieved 10-15% better fitness values than Standard GP. Furthermore, the proposed setups had consistently generated better quality solutions on all the problems and utilized 30-50% less computations to match the best performance of Standard GP.																	1864-5909	1864-5917				DEC	2020	13	4					537	549		10.1007/s12065-020-00355-2		FEB 2020											
J								Optimal UPQC location in power distribution network via merging genetic and dragonfly algorithm	EVOLUTIONARY INTELLIGENCE										UPQC placement; Power quality improvement; Genetic algorithm; DragonFly; Crossover; UPQC cost; Voltage stability	QUALITY CONDITIONER; OPTIMAL-DESIGN; IMPLEMENTATION; OPTIMIZATION; ALLOCATION; PLACEMENT; VOLTAGE; SYSTEM	Nowadays, flexible alternating current transmission system devices, particularly unified power quality conditioner (UPQC) are found to have significant impacts on stability of rising power system. In power systems, several intellectual optimization methods were exploited to position the UPQC. However, those optimization models fail to offer more reliability and feedback signal. Hence, this paper presents a power quality improvement model, which is based on a hybrid algorithm that links genetic algorithm (GA) and DragonFly algorithm (DA). In the current research work, the optimal solution is determined based on the crossover operation of GA in dragonfly algorithm (DA), and hence, the adopted model is named as Genetically Modified DA algorithm. Moreover, the proposed model discovers the optimal location of UPQC device by focusing on the UPQC cost, power losses, and Voltage stability Index. The proposed model is carried out in IEEE 69, and IEEE 33 test bus systems. In addition, the performance of implemented model is distinguished over other conventional models such as artificial bee colony, firefly, grey wolf optimization, whale optimization algorithm, worst solution linked whale optimization algorithm update (WS-WU), GA and DA. The performance of the proposed model is effectively proved by performance and convergence analysis.																	1864-5909	1864-5917															10.1007/s12065-020-00364-1		FEB 2020											
J								Heuristic solutions to robust variants of the minimum-cost integer flow problem	JOURNAL OF HEURISTICS										Robust optimization; Network flow; Minimum-cost flow; Heuristic; Local search; Evolutionary computing	DISCRETE OPTIMIZATION; NETWORK	This paper deals with robust optimization applied to network flows. We consider two robust variants of the minimum-cost integer flow problem. Thereby, uncertainty in problem formulation is limited to arc costs and expressed by a finite set of explicitly given scenarios. It turns out that both problem variants are NP-hard. To solve the considered variants, we propose several heuristics based on local search or evolutionary computing. We also evaluate our heuristics experimentally on appropriate problem instances.																	1381-1231	1572-9397				AUG	2020	26	4					531	559		10.1007/s10732-020-09441-1		FEB 2020											
J								Pareto-based evolutionary multiobjective approaches and the generalized Nash equilibrium problem	JOURNAL OF HEURISTICS										Generalized Nash equilibrium problem; Multi-objective optimization; Evolutionary algorithms	SERVICE PROVISIONING PROBLEM; RELAXATION ALGORITHMS; OPTIMIZATION; CLOUD; POWER; ADAPTATION; NETWORKS; GAMES	Pareto-based evolutionary multiobjective approaches are methods that use the Pareto dominance concept to guide the search of evolutionary algorithms towards the Pareto frontier of a problem. To address the challenge of providing an entire set of optimal solutions they use specially designed mechanisms for preserving search diversity and maintaining the non-dominated solutions set. The limitation of the Pareto dominance relation in high-dimensional spaces has rendered these methods inefficient for many-objective optimization. In this paper we aim to exploit existing Pareto-based methods to compute the generalized Nash equilibrium for multi-player games by replacing the Pareto dominance relation with an equilibrium generative relation. The generalized Nash equilibrium extends the Nash equilibrium concept by considering constraints over players' strategies. Numerical experiments indicate that the selected methods can be employed for equilibria computation even for games with up to twenty players.																	1381-1231	1572-9397				AUG	2020	26	4					561	584		10.1007/s10732-020-09438-w		FEB 2020											
J								Improving Naive Bayes for Regression with Optimized Artificial Surrogate Data	APPLIED ARTIFICIAL INTELLIGENCE											PERFORMANCE	Can we evolve better training data for machine learning algorithms? To investigate this question we use population-based optimization algorithms to generate artificial surrogate training data for naive Bayes for regression. We demonstrate that the generalization performance of naive Bayes for regression models is enhanced by training them on the artificial data as opposed to the real data. These results are important for two reasons. Firstly, naive Bayes models are simple and interpretable but frequently underperform compared to more complex "black box" models, and therefore new methods of enhancing accuracy are called for. Secondly, the idea of using the real training data indirectly in the construction of the artificial training data, as opposed to directly for model training, is a novel twist on the usual machine learning paradigm.																	0883-9514	1087-6545				MAY 11	2020	34	6					484	514		10.1080/08839514.2020.1726615		FEB 2020											
J								Strategyproof and fair matching mechanism for ratio constraints	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS											PARETO-OPTIMAL MATCHINGS; SCHOOL CHOICE; PROOFNESS; MINIMUM; STABILITY; BOUNDS	We introduce a new type of distributional constraints called ratio constraints, which explicitly specify the required balance among schools in two-sided matching. Since ratio constraints do not belong to the known well-behaved class of constraints called M-convex set, developing a fair and strategyproof mechanism that can handle them is challenging. We develop a novel mechanism called quota reduction deferred acceptance (QRDA), which repeatedly applies the standard DA by sequentially reducing artificially introduced maximum quotas. As well as being fair and strategyproof, QRDA always yields a weakly better matching for students compared to a baseline mechanism called artificial cap deferred acceptance (ACDA), which uses predetermined artificial maximum quotas. Finally, we experimentally show that, in terms of student welfare and nonwastefulness, QRDA outperforms ACDA and another fair and strategyproof mechanism called Extended Seat Deferred Acceptance (ESDA), in which ratio constraints are transformed into minimum and maximum quotas.																	1387-2532	1573-7454				FEB 14	2020	34	1							23	10.1007/s10458-020-09448-9													
J								Taylor-AMS features and deep convolutional neural network for converting nonaudible murmur to normal speech	COMPUTATIONAL INTELLIGENCE										AMS features; Deep convolutional neural network; Optimization; Taylor series; Whisper speech recognition	RECOGNITION; ALGORITHM; WHISPER	Communication becomes effective when the speech signal arrives with the profound characteristics. This insisted the researchers to develop an automatic system of recognizing the speech signals from the murmurs. Some of the traditional automatic recognition systems are unfit for the silent environments imposing a need for an effective recognition system. Also, the traditional automatic recognition methods, like Neural Networks, render poor performance in the presence of the murmurs. Thus, this article proposes a method for automatic whisper recognition using the Deep Convolutional Neural Network (DCNN). The training of the DCNN is performed using the proposed Stochastic-Whale Optimization Algorithm (Stochastic-WOA), which is designed by the integration of Stochastic Gradient Descent algorithm with WOA. The input to the classifier is the features that include pitch chroma, spectral centroid, spectral skewness, and Taylor-Amplitude Modulation Spectrogram (Taylor-AMS), which is obtained by combining Taylor series and Amplitude Modulation Spectrogram (AMS) features, of the preprocessed input speech signal. The experimentation of the method is performed using the real database and the analysis proves that the proposed method acquired a maximal accuracy of 0.9723, minimal False Positive Rate of 0.0257, and maximal True Positive Rate of 0.9981, respectively.																	0824-7935	1467-8640				AUG	2020	36	3					940	963		10.1111/coin.12281		FEB 2020											
J								Hidden data states-based complex terminology extraction from textual web data model	APPLIED INTELLIGENCE										Information retrieval; Term recognition; Hidden Markov model; Markov chain; Linguistic knowledge; Complex term	TERM EXTRACTION; INFORMATION-RETRIEVAL; DOMAIN	In order to respect the standards of the "semantic web" which allows the data to be shared and reused between several applications, it became necessary to model web text documents with a vision based on the concepts and exploit available linguistic resources. It's evident that the extraction of semantic tokens ensures semantic modelling of web documents. Unfortunately, terminology extraction techniques from unstructured Web text remain unable to provide powerful results. Indeed, systems developed based on the classical techniques extract massively high amounts of candidate terms and leave the task of separation between relevant and irrelevant candidates for post-processing. In this paper, we introduce HMM-Extract a novel model for terminology retrieval based on Markov model. Our model integrates two modules that work in cascade: a module based on Hidden Markov Model (HMM) for complex term extraction and a module based on Markov Chain for filtering terms provided by the HMM. Thus, we try to focus on three main contributions: firstly, we provide a linguistic and statistical specification of relevant terms. Secondly, we show the possibility of using a HMM to extract relevant terms from unstructured textual documents. Finally, we prove the importance of integrating statistical knowledge in a Markov Chain and we show, experimentally, its contribution to the field of terminology extraction.																	0924-669X	1573-7497				JUN	2020	50	6					1813	1831		10.1007/s10489-019-01568-4		FEB 2020											
J								NetNPG: Nonoverlapping pattern matching with general gap constraints	APPLIED INTELLIGENCE										Pattern matching; Pattern mining; Gap constraint; Nonoverlapping condition; Nettree	SEQUENTIAL PATTERNS; EFFICIENT; WILDCARDS; ALGORITHM	Pattern matching (PM) with gap constraints (or flexible wildcards) is one of the essential tasks in repetitive sequential pattern mining (or sequence pattern mining), since it can compute the support of a pattern in a sequence. Nonoverlapping PM (or PM under nonoverlapping condition) which is a kind of PM with gap constraints methods allows the same position character in the sequence to be reused at different locations in the pattern, but is not allowed to be reused in the same position of the pattern. The researches on nonoverlapping are under non-negative gaps which are more restrictive on the order of each character occurring in the sequence. As we know that it is easy to obtain valuable patterns under the nonoverlapping condition in sequence pattern mining. This paper addresses a nonoverlapping PM problem with general gaps which means that the gap can be a negative value. We proposes an effective algorithm which employs Nettree structure to convert the problem into a general gap Nettree at first. In order to find the nonoverlapping occurrences, the algorithm employs a backtracking strategy to find the leftmost full path in each iteration. This paper also analyzes the time and space complexities of the proposed algorithm. Experimental results verify the proposed algorithm has better performance and demonstrate that the general gap is more flexible than the non-negative gap.																	0924-669X	1573-7497				JUN	2020	50	6					1832	1845		10.1007/s10489-019-01616-z		FEB 2020											
J								Analyzing Connections Between User Attributes, Images, and Text	COGNITIVE COMPUTATION										Personality; Gender; Natural language processing; Computer vision; Computational social science	LANGUAGE USE; PERSONALITY; RECOGNITION; TRAITS; POWER	This work explores the relationship between a person's demographic/psychological traits (e.g., gender and personality) and self-identity images and captions. We use a dataset of images and captions provided by N approximate to 1350 individuals, and we automatically extract features from both the images and captions. We identify several visual and textual properties that show reliable relationships with individual differences between participants. The automated techniques presented here allow us to draw interesting conclusions from our data that would be difficult to identify manually, and these techniques are extensible to other large datasets. Additionally, we consider the task of predicting gender and personality using both single modality features and multimodal features. We show that a multimodal predictive approach outperforms purely visual methods and purely textual methods. We believe that our work on the relationship between user characteristics and user data has relevance in online settings, where users upload billions of images each day.																	1866-9956	1866-9964															10.1007/s12559-019-09695-3		FEB 2020											
J								GP-SLAM: laser-based SLAM approach based on regionalized Gaussian process map reconstruction	AUTONOMOUS ROBOTS										Simultaneous localization and mapping; Laser-based; Gaussian process	REGRESSION; REGISTRATION; EXPLORATION	Existing laser-based 2D simultaneous localization and mapping (SLAM) methods exhibit limitations with regard to either efficiency or map representation. An ideal method should estimate the map of the environment and the state of the robot quickly and accurately while providing a compact and dense map representation. In this study, we develop a new laser-based SLAM algorithm by redesigning the two core elements common to all SLAM systems, namely the state estimation and map construction. Utilizing Gaussian process (GP) regression, we propose a new type of map representation based on the regionalized GP map reconstruction algorithm. With this new map representation, both the state estimation method and the map update method can be completed with the use of concise mathematics. For small- or medium-scale scenarios, our method, consisting of only state estimation and map construction, demonstrates outstanding performance relative to traditional occupancy-grid-map-based approaches in both accuracy and especially efficiency. For large-scale scenarios, we extend our approach to a graph-based version.																	0929-5593	1573-7527				JUL	2020	44	6					947	967		10.1007/s10514-020-09906-z		FEB 2020											
J								Inference of Manipulation Intent in Teleoperation for Robotic Assistance	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Object manipulation; Human intent; Teleoperation; Robotic assistant	NETWORKS; GRASP	In teleoperation, predicting an operator's intent and providing subsequent assistance have demonstrated great advantages in reducing an operator's workload and a task's difficulty as well as enhancing the task performance. Current research aims to tackle target-approaching intent, while our work focus on inferring manipulation (task) intent after the user grasps the object. We model how an object is grasped when being utilized in different manipulation tasks (intents) and then adopt this object grasping model in teleoperation for the intent inference. Our paper focuses on determining if direct interaction models can be used for indirect interaction. As the nature of one's grasping pose may satisfy multiple tasks (intents), we explore a form of classification modeling known as multi-label classification for multiple broad categories of tasks and objects. We also comprehensively compare classification techniques to determine the most suitable method for determining manipulation intent. With knowing the manipulation intent, future robot control algorithms can provide more helpful and appropriate assistance to facilitate task accomplishment.																	0921-0296	1573-0409				JUL	2020	99	1					29	43		10.1007/s10846-019-01125-8		FEB 2020											
J								Evolving semantic object segmentation methods automatically by genetic programming from images and image processing operators	SOFT COMPUTING										Semantic object segmentation; Genetic programming; Cooperative coevolution; Strongly typed representation	TEXTURE SEGMENTATION	Even though numerous segmentation methods exist, the requirement of prior knowledge or parameter tuning makes them restricted to limited image domains. Without predefining solution models, genetic programming (GP) is able to solve complex problems by evolving computer programs automatically. In this paper, three new GP-based methods are designed to evolve segmentation algorithms automatically from images and primitive image processing operators (e.g., filters and histogram equalization). Specifically, a strongly typed representation, the cooperative coevolution technique and a two-stage evolution are introduced in GP, respectively, to form three new methods that can evolve solutions to conduct image preprocessing, segmentation and postprocessing automatically. The new methods are termed as StronglyGP, CoevoGP and TwostageGP, and standard GP-based algorithm (StandardGP) is employed as a reference method. The proposed methods are tested on two complicated datasets (i.e., Weizmann and Pascal datasets), which contain high variations in both objects and backgrounds. The results show that StronglyGP and StandardGP can evolve effective segmentors for the given complex segmentation tasks, while CoevoGP and TwostageGP perform worse than StronglyGP and StandardGP, which may be caused by the overfitting problem in deriving postprocessing solutions. In addition, compared with StandardGP, StronglyGP achieves better segmentation performance with smaller solution sizes. Moreover, compared with four widely used segmentation methods, StronglyGP and StandardGP can produce satisfactory results consistently on both Weizmann and Pascal datasets.																	1432-7643	1433-7479				SEP	2020	24	17					12887	12900		10.1007/s00500-020-04713-1		FEB 2020											
J								A distributed parallel training method of deep belief networks	SOFT COMPUTING										Distributed computing; Model parallelism; Deep belief network; Restricted Boltzmann machine; Rough representation	NEURAL-NETWORKS	Nowadays, it has become well known that efficient training of deep neural networks plays a vital role in various successful applications. To achieve this goal, it is impractical to use only one computer, especially when the scale of models is large and some efficient computing resources are available. In this paper, we present a distributed parallel computing framework for training deep belief networks (DBNs) by employing the great power of high-performance clusters (i.e., a system consists of many computers). Motivated by the greedy layer-wise learning algorithm of DBNs, the whole training process is divided layer by layer and distributed to different machines. At the same time, rough representations are exploited to parallelize the training process. By conducting experiments on several large-scale real datasets, the novel algorithms are shown to significantly accelerate the training speed of DBNs while achieving better or competitive prediction accuracy in comparison with the original algorithm.																	1432-7643	1433-7479				SEP	2020	24	17					13357	13368		10.1007/s00500-020-04754-6		FEB 2020											
J								IADF security: insider attack detection using fuzzy logic in wireless multimedia sensor networks	SOFT COMPUTING										IADF; Fuzzification; Network parameters; Physical parameters; Insider attack detection; WMSN	DATA AGGREGATION	Wireless multimedia sensor networks (WMSNs) consist of spatially distributed, autonomous sensor nodes that monitor physical or environmental conditions. WMSN networks, in general, are prone to insider attacks like node replication attacks, where a malicious node replicates itself as an insider node, which may lead to a situation where a non-authentic node becomes a rightful part of the network. Insider attacks are not detectable with classic cryptographical techniques. Many WMSN applications require a dedicated on-demand algorithm to detect the insider attacker in the system without any prior knowledge of the system. This paper proposes a new security scheme, called the insider attack detection using fuzzy logic (IADF). This security scheme uses the fuzzification of six parameters present in a wireless multimedia sensor node categorized as network and physical parameters. Physical parameters used are received signal strength, transceiver range and residual energy. Network parameters used include packet delivery ratio, forwarding delay and transmission rate. The fuzzy approach makes the IADF algorithm flexible and scalable. Simulation results show that the IADF algorithm can predict malicious nodes with a higher prediction rate and higher detection accuracy.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13893	13902		10.1007/s00500-020-04764-4		FEB 2020											
J								A self-verifying clustering approach to unsupervised matching of product titles	ARTIFICIAL INTELLIGENCE REVIEW										Product matching; Entity matching; Entity resolution; Clustering; Unsupervised learning; Data mining		The continuous growth of the e-commerce industry has rendered the problem of product retrieval particularly important. As more enterprises move their activities on the Web, the volume and the diversity of the product-related information increase quickly. These factors make it difficult for the users to identify and compare the features of their desired products. Recent studies proved that the standard similarity metrics cannot effectively identify identical products, since similar titles often refer to different products and vice-versa. Other studies employ external data sources to enrich the titles; these solutions are rather impractical, since the process of fetching external data is inefficient. In this paper we introduce UPM, an unsupervised algorithm for matching products by their titles that is independent of any external sources. UPM consists of three stages. During the first stage, the algorithm analyzes the titles and extracts combinations of words out of them. These combinations are evaluated in stage 2 according to several criteria, and the most appropriate of them are selected to form the initial clusters. The third phase is a post-processing verification stage that refines the initial clusters by correcting the erroneous matches. This stage is designed to operate in combination with all clustering approaches, especially when the data possess properties that prevent the co-existence of two data points within the same cluster. The experimental evaluation of UPM with multiple datasets demonstrates its superiority against the state-of-the-art clustering approaches and string similarity metrics, in terms of both efficiency and effectiveness.																	0269-2821	1573-7462				OCT	2020	53	7					4777	4820		10.1007/s10462-020-09807-8		FEB 2020											
J								A general framework for multi-granulation rough decision-making method under q-rung dual hesitant fuzzy environment	ARTIFICIAL INTELLIGENCE REVIEW										q-rung dual hesitant fuzzy set; Multi-granulation rough set; Decision rule; Medical diagnosis	SYMMETRIC MEAN OPERATORS; 2 UNIVERSES; SET	In the realistic decision-making (DM) process, the DM results were provided by multiple DM experts, which are more accurate than those based on one DM expert. Therefore, the multi-granulation rough set (MGRS) model is more accurate in DM problems. It is imperative to apply the idea of multi-granulation to the complex fuzzy uncertain information. By combining q-rung dual hesitant fuzzy sets (q-DHFSs) with multi-granulation rough sets (MGRSs) over two universes, we propose a q-rung dual hesitant fuzzy multi-granulation rough set (q-RDHFMGRS) over two universes, and prove some of their basic properties. Then, based on this model, we propose a new multi-attribute DM algorithm. Finally, we validate the practicability and validity of the algorithm through an example of medical diagnosis.																	0269-2821	1573-7462				OCT	2020	53	7					4903	4933		10.1007/s10462-020-09810-z		FEB 2020											
J								Semantic Knowledge Representation for Strategic Interactions in Dynamic Situations	FRONTIERS IN NEUROROBOTICS										cognitive maps; manipulation of objects; dynamical systems; semantic description; neural networks	TIME; SPACE	Evolved living beings can anticipate the consequences of their actions in complex multilevel dynamic situations. This ability relies on abstracting the meaning of an action. The underlying brain mechanisms of such semantic processing of information are poorly understood. Here we show how our novel concept, known as time compaction, provides a natural way of representing semantic knowledge of actions in time-changing situations. As a testbed, we model a fencing scenario with a subject deciding between attack and defense strategies. The semantic content of each action in terms of lethality, versatility, and imminence is then structured as a spatial (static) map representing a particular fencing (dynamic) situation. The model allows deploying a variety of cognitive strategies in a fast and reliable way. We validate the approach in virtual reality and by using a real humanoid robot.																	1662-5218					FEB 13	2020	14								4	10.3389/fnbot.2020.00004													
J								Neurorobotics Workshop for High School Students Promotes Competence and Confidence in Computational Neuroscience	FRONTIERS IN NEUROROBOTICS										neurorobots; neurorobotics; brain-based robots; computational neuroscience; education technology; workshop; active learning; high school	SCIENCE; MODEL; ACHIEVEMENT; GENERATION	Understanding the brain is a fascinating challenge, captivating the scientific community and the public alike. The lack of effective treatment for most brain disorders makes the training of the next generation of neuroscientists, engineers and physicians a key concern. Over the past decade there has been a growing effort to introduce neuroscience in primary and secondary schools, however, hands-on laboratories have been limited to anatomical or electrophysiological activities. Modern neuroscience research labs are increasingly using computational tools to model circuits of the brain to understand information processing. Here we introduce the use of neurorobots - robots controlled by computer models of biological brains - as an introduction to computational neuroscience in the classroom. Neurorobotics has enormous potential as an education technology because it combines multiple activities with clear educational benefits including neuroscience, active learning, and robotics. We describe a 1-week introductory neurorobot workshop that teaches high school students how to use neurorobots to investigate key concepts in neuroscience, including spiking neural networks, synaptic plasticity, and adaptive action selection. Our do-it-yourself (DIY) neurorobot uses wheels, a camera, a speaker, and a distance sensor to interact with its environment, and can be built from generic parts costing about $170 in under 4 h. Our Neurorobot App visualizes the neurorobot's visual input and brain activity in real-time, and enables students to design new brains and deliver dopamine-like reward signals to reinforce chosen behaviors. We ran the neurorobot workshop at two high schools (n = 295 students total) and found significant improvement in students' understanding of key neuroscience concepts and in students' confidence in neuroscience, as assessed by a pre/post workshop survey. Here we provide DIY hardware assembly instructions, discuss our open-source Neurorobot App and demonstrate how to teach the Neurorobot Workshop. By doing this we hope to accelerate research in educational neurorobotics and promote the use of neurorobots to teach computational neuroscience in high school.																	1662-5218					FEB 13	2020	14								6	10.3389/fnbot.2020.00006													
J								An advanced active set L-BFGS algorithm for training weight-constrained neural networks	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Constrained optimization; L-BFGS; Modified secant equation		In this work, a new advanced active set limited memory BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm is proposed for efficiently training weight-constrained neural networks, called AA-L-BFGS. The proposed algorithm possesses the significant property of approximating the curvature of the error function with high-order accuracy by utilizing the theoretically advanced secant condition proposed by Livieris and Pintelas (Appl Math Comput 221:491-502, 2013). Moreover, the global convergence of the proposed algorithm is established provided that the line search satisfies the modified Armijo condition. The presented numerical experiments illustrate the efficiency of the proposed AA-L-BFGS, providing empirical evidence that it significantly accelerates the convergence of the training process.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6669	6684		10.1007/s00521-019-04689-6		FEB 2020											
J								Image denoising via structure-constrained low-rank approximation	NEURAL COMPUTING & APPLICATIONS										Image denoising; Sparse representation; Low-rank approximation; Wiener filtering; Deep learning	THRESHOLDING ALGORITHM; SPARSE; REPRESENTATIONS; FUSION	Low-rank approximation-based methods have recently achieved impressive results in image restoration. Generally, the low-rank constraint integrated with the nonlocal self-similarity prior is enforced for image recovery. However, it is still unsatisfactory to recover complex image structures due to the lack of joint modeling based on local and global information, especially when the signal-to-noise ratio is low. In this paper, we propose a novel structure-constrained low-rank approximation method using complementary local and global information, as, respectively, modeled by kernel Wiener filtering and low-rank regularization. The proposed method solves the ill-posed inverse problem associated with image denoising by the alternating direction method of multipliers. Experimental results demonstrate that the proposed method not only removes noise effectively, but also is highly competitive against the state-of-the-art methods both qualitatively and quantitatively.																	0941-0643	1433-3058				AUG	2020	32	16					12575	12590		10.1007/s00521-020-04717-w		FEB 2020											
J								Purities prediction in a manufacturing froth flotation plant: the deep learning techniques	NEURAL COMPUTING & APPLICATIONS										Froth flotation; Deep learning; Long short-term memory; Concentrate purity	PARAMETERS	Accurate and timely investigation to concentrate grade and recovery is a premise of realizing automation control in a froth flotation process. This study seeks to use deep learning technologies modeling a manufacturing flotation process, forecasting the concentrate purities for iron and the waste silica. Considering the size and temporality of engineering data, we adopted a long short-term memory to form the core part of the deep learning model. To perform this process, 23 variables reflecting a flotation plant were monitored and collected hourly over a half year time span, then wrangled, split, and restructured for deep learning model use. A deep learning model encompassing a stacked long short-term memory architecture was designed, trained, and tested with prepared data. The model's performance on test data demonstrates the capability of our proposed model to predict real-time concentrate purities for iron and silica. Compared with a traditional machine model typified by a random forest model in this study, the proposed deep learning model is significantly more competent to model a manufacturing froth flotation process. Expected to lay a foundation for realizing automation control of the flotation process, this study should encourage deep learning in mineral processing engineering.																	0941-0643	1433-3058				SEP	2020	32	17					13639	13649		10.1007/s00521-020-04773-2		FEB 2020											
J								Solving combined economic emission dispatch model via hybrid differential evaluation and crow search algorithm	EVOLUTIONARY INTELLIGENCE										CEED issue; Fuel cost; Emission of pollutions; Hybrid CSA and DE; CPU time	PARTICLE SWARM OPTIMIZATION; FLOWER POLLINATION ALGORITHM; EMISSION/ECONOMIC DISPATCH; EVOLUTION	One of the major aspects regarding the operation and planning of a power system was to reduce the emission of pollutions and fuel costs in thermal power plants. This issue was resolved as an optimization crisis, where the reduction of emission of pollutions and fuel cost was done and it is known as the combined economic emission dispatch (CEED) problem. Various techniques were introduced for improving the performance of the power plants with respect to algorithm reliability, solution accuracy, global optimality, and convergence speed for resolving the CEED issues. Therefore, this work establishes a CEED approach for the smart grid system and resolves it by exploiting the hybridized concepts of the crow search algorithm (CSA) and differential evolution (DE). The hybridized model of the two well-known schemes is achieved by updating the solutions of both the schemes and merging them with the random searching model. Thus, the new approach is named as hybrid DE and CSA model. The CEED approach is subjected to reduce its cost and therefore, sufficient trade-off between the emission and economic costs could be sustained. The presented hybrid scheme is simulated on 3 diverse bus systems and its performance is evaluated over other state-of-the-art models in terms of CPU time and generation strategy.																	1864-5909	1864-5917															10.1007/s12065-020-00357-0		FEB 2020											
J								A comparative study of cellular automata-based digital image scrambling techniques	EVOLVING SYSTEMS										Image scrambling; Cellular automata; Gray difference degree; Image encryption; Number of pixels change rate	ALGORITHM	Cellular automata (CA) are an important class of dynamic systems, discrete in both time and space units. A cellular automaton evolves by the local interaction of its discrete space units or cells at discrete time steps. This local interaction is governed by simple rules that compute the next state of each cell. Many of these rules evolve CA to generate chaotic or complex patterns and, as such, these CA rules find application in a wide variety of areas including digital image scrambling (DIS). The dynamic behavior of any given CA is largely influenced by the non-quiescent state ratios present in the initial CA configuration. In this paper, we first implement and analyze different CA-based DIS techniques using same parameters, wherever possible, and same dataset of test images for a justified comparison of their performance in terms of gray difference degree (GDD). Next, the effect of different non-quiescent state ratios in the initial CA configuration, and varying image sizes on GDD using these CA-based DIS techniques is analyzed. Robustness of all the DIS techniques is evaluated using correlation coefficient analysis and number of pixels change rate.																	1868-6478	1868-6486															10.1007/s12530-020-09326-5		FEB 2020											
J								Recurrent Interval Type-2 Fuzzy Wavelet Neural Network with Stable Learning Algorithm: Application to Model-Based Predictive Control	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Identification; Sliding mode; Model predictive controller; Fractional-order chaotic systems; Synchronization	PROJECTIVE SYNCHRONIZATION; SYSTEMS; IDENTIFICATION; CHAOS	Fuzzy neural networks, with suitable learning strategy, have been demonstrated as an effective tool for online data modeling. However, it is a challenging task to construct a model to ensure its quality and stability for non-stationary dynamic systems with some uncertainties. To solve this problem, this paper presents a novel identification model based on recurrent interval type-2 fuzzy wavelet neural network (RIT2FWNN) with new learning algorithm. The model benefits from both advantages of recurrent and wavelet neural networks such as use of temporal data and fast convergence properties. The proposed antecedent and consequent parameters update rules are derived using sliding-mode-control-theory. To evaluate the proposed fuzzy model, it is utilized to design a nonlinear model-based predictive controller and is applied for the synchronization of fractional-order time-delay chaotic systems. Using Lyapunov stability analysis, it is shown that all update rules of the parameters are uniformly ultimately bounded. The adaptation laws obtained in this method are very simple and have closed forms. Some stability conditions are derived to prove learning dynamics and asymptotic stability of the network by using an appropriate Lyapunov function. The efficacy and performance of the proposed method is verified by simulation examples.																	1562-2479	2199-3211				MAR	2020	22	2			SI		351	367		10.1007/s40815-019-00766-z		FEB 2020											
J								Multi constraints applied energy efficient routing technique based on ant colony optimization used for disaster resilient location detection in mobile ad-hoc network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile ad-hoc networks (MANETs); Routing; Computing; energy consumption; Quality of service (QoS); Ant colony optimization		A mobile ad-hoc network (MANET) is a group of advanced mobile devices which are capable of self-organization. Due to the diverse nature of mobile devices and wireless connectivity, MANET faces several issues like topology management, energy management due to battery power limits, data communication issues etc. The utilization rate of battery powered energy and QoS properties are significant in MANET. In order to address these issues, we propose a new ant colony inspired technique for energy efficient routing in MANET. The proposed technique is a multi-objective constraints applied energy efficient routing technique based on ant colony optimization in mobile adhoc networks (MCER-ACO). The proposed MCER-ACO technique selects the next hop node centered on the constraints, residual energy of mobile node, no of packets in path and dynamic movement of topology. By applying ant colony technique on objectives and constraints, probability of choosing next hop node as forwarding node is determined. From the performance evaluation carried out in this research work, it is proved that the proposed MCER-ACO approach is providing optimal energy efficient routing while comparing with few other existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-01767-9		FEB 2020											
J								Loop-Type Sequent Calculi for Temporal Logic	JOURNAL OF AUTOMATED REASONING										Temporal logics; Sequent calculi; Derivation loops	CUT-FREE; COMPLETENESS	Various types of calculi (Hilbert, Gentzen sequent, resolution calculi, tableaux) for propositional linear temporal logic (PLTL) have been invented. In this paper, a sound and complete loop-type sequent calculus G(L)T for PLTL with the temporal operators "next" and "henceforth always" (PLTLn,a) is considered at first. We prove that all rules of G(L)T are invertible and that the structural rules of weakening and contraction, as well as the rule of cut, are admissible in G(L)T. We describe a decision procedure for PLTLn,a based on the introduced calculus G(L)T. Afterwards, we introduce a sound and complete sequent calculus G(L)T(U) for PLTL with the temporal operators "next" and "until".																	0168-7433	1573-0670				DEC	2020	64	8					1663	1684		10.1007/s10817-020-09544-1		FEB 2020											
J								New heuristic algorithms for the Dubins traveling salesman problem	JOURNAL OF HEURISTICS										Dubins traveling salesman problem; Curvature-constrained traveling salesman problem; Dubins vehicle; Heuristic methods	SALESPERSON PROBLEMS; PATHS; CURVATURE; UAVS	The problem of finding a shortest curvature-constrained closed path through a set of targets in the plane is known as Dubins traveling salesman problem (DTSP). Applications of the DTSP include motion planning for kinematically constrained mobile robots and unmanned fixed-wing aerial vehicles. The difficulty of the DTSP is to simultaneously find an order of the targets and suitable headings (orientation angles) of the vehicle when passing the targets. Since the DTSP is known to be NP-hard there is a need for heuristic algorithms providing good quality solutions in reasonable time. Inspired by standard methods for the TSP we present a collection of such heuristics adapted to the DTSP. The algorithms are based on a technique that optimizes the headings of the targets of an open or closed subtour with given order. This is done by discretizing the headings, constructing an auxiliary network and computing a shortest path in the network. The first algorithm for the DTSP uses the order of the targets obtained from the solution of the Euclidean TSP. A second class of algorithms extends an open subtour by successively adding a new target and closes the tour if all targets have been added. A third class of algorithms starts with a closed subtour consisting of few targets and successively inserts a new target into the tour. The individual algorithms differ by the number of headings to be optimized in each iteration. Extensive simulation results show that the proposed methods are competitive with state-of-the-art methods for the DTSP concerning performance and superior concerning running time, which makes them applicable also to large-scale scenarios.																	1381-1231	1572-9397				AUG	2020	26	4					503	530		10.1007/s10732-020-09440-2		FEB 2020											
J								An automatic constructive matheuristic for the shift minimization personnel task scheduling problem	JOURNAL OF HEURISTICS										Task scheduling; Decomposition; SMPTSP; Matheuristic	ALGORITHMS	The shift minimization personnel task scheduling problem is an NP-complete optimization problem that concerns the assignment of tasks to multi-skilled employees with a view to minimize the total number of assigned employees. Recent literature indicates that hybrid methods which combine exact and heuristic techniques such as matheuristics are efficient as regards to generating high quality solutions. The present work employs a constructive matheuristic (CMH): a decomposition-based method where sub-problems are solved to optimality using exact techniques. The optimal solutions of sub-problems are subsequently utilized to construct a feasible solution for the entire problem. Based on the study, a time-based CMH has been developed which, for the first time, solves all the difficult instances introduced by Smet et al. (Omega 46:64-73, 2014) to optimality. In addition, an automated CMH algorithm that utilizes instance-specific problem features has also been developed that produces high quality solutions over all current benchmark instances.																	1381-1231	1572-9397															10.1007/s10732-020-09439-9		FEB 2020											
J								Anti-periodic Oscillations of Fuzzy Delayed Cellular Neural Networks with Impulse on Time Scales	NEURAL PROCESSING LETTERS										Fuzzy delayed cellular neural networks; Anti-periodic solution; Exponential stability; Time scales; Impulse	GLOBAL EXPONENTIAL STABILITY; PERIODIC-SOLUTION; DISTRIBUTED DELAYS; VARYING DELAYS; ASYMPTOTIC STABILITY; DYNAMICAL BEHAVIORS; NONLINEAR DYNAMICS; DISCRETE; EXISTENCE; EQUATIONS	In this manuscript, fuzzy delayed cellular neural networks with impulse are studied. Applying time scale calculus knowledge, mathematical inequalities and constructing Lyapunov function, we establish a sufficient criterion that guarantees the existence and exponential stability of anti-periodic solutions for fuzzy delayed cellular neural networks with impulse. In addition, an example with its numerical simulations is given to illustrate our theoretical predictions.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2379	2402		10.1007/s11063-020-10203-0		FEB 2020											
J								Improving quality-of-service in fog computing through efficient resource allocation	COMPUTATIONAL INTELLIGENCE										CloudSim; fog computing; light-weight computing device; resource allocation; virtual machine	MANAGEMENT; ENERGY	In today's world, large group migration of applications to the fog computing is registered in the information technology world. The main issue in fog computing is providing enhanced quality of service (QoS). QoS management consists of various method used for allocating fog-user applications in the virtual environment and selecting suitable method for allocating virtual resources to physical resource. The resources allocation in effective manner in the fog environment is also a major problem in fog computing; it occurs when the infrastructure is build using light-weight computing devices. In this article, the allocation of task and placement of virtual machine problems is explained in the single fog computing environment. The experiment is done and the result shows that the proposed framework improves QoS in fog environment.																	0824-7935	1467-8640															10.1111/coin.12285		FEB 2020											
J								Double feature selection algorithm based on low-rank sparse non-negative matrix factorization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Non-negative matrix factorization; Low-rank sparse representation; Self-representation; Unsupervised feature selection	UNSUPERVISED FEATURE-SELECTION; GRAPH; REPRESENTATION	Recently, many feature selection algorithms based on non-negative matrix factorization have been proposed. However, many of these algorithms only consider unilateral information about global or local geometric structure normally. To this end, this paper proposes a new feature selection algorithm called double feature selection algorithm based on low-rank sparse non-negative matrix factorization (NMF-LRSR). Firstly, to reduce the dimensions effectively, NMF-LRSR uses non-negative matrix factorization as the framework to further reduce the dimension of the feature selection which is originally a dimension reduction problem. Secondly, the low-rank sparse representation with the self-representation is used to construct the graph, so both the global and intrinsic geometric structure information of the data could be taken into account in the process of feature selection, which makes full use of the information and makes the feature selection more accurate. In addition, the double feature selection theory is used to this paper, which makes the result of feature selection more accurate. NMF-LRSR is tested on the baseline and the other six algorithms in the literature and evaluated them on 11 publicly available benchmark datasets. Experimental results show that NMF-LRSR is more effective than the other six feature selection algorithms.																	1868-8071	1868-808X				AUG	2020	11	8					1891	1908		10.1007/s13042-020-01079-6		FEB 2020											
J								A Lightweight Defeasible Description Logic in Depth Quantification in Rational Reasoning and Beyond	KUNSTLICHE INTELLIGENZ											CLOSURE	In this thesis we study KLM-style rational reasoning in defeasible Description Logics. We illustrate that many recent approaches to derive consequences under Rational Closure (and its stronger variants, lexicographic and relevant closure) suffer the fatal drawback of neglecting defeasible information in quantified concepts. We propose novel model-theoretic semantics that are able to derive the missing entailments in two differently strong flavours. Our solution introduces a preference relation to distinguish sets of models in terms of their typicality (amount of defeasible information derivable for quantified concepts). The semantics defined through the most typical (most preferred) sets of models are proven superior to previous approaches in that their entailments properly extend previously derivable consequences, in particular, allowing to derive defeasible consequences for quantified concepts. The dissertation concludes with an algorithmic characterisation of this uniform maximisation of typicality, which accompanies our investigation of the computational complexity for deriving consequences under these new semantics.																	0933-1875	1610-1987															10.1007/s13218-020-00644-z		FEB 2020											
J								Feature selection based on hybridization of genetic algorithm and competitive swarm optimizer	SOFT COMPUTING										Feature selection; Competitive swarm optimization; Genetic algorithm; Classification; High dimensionality	KRILL HERD ALGORITHM; COMBINATORIAL	Feature selection is one of the hottest machine learning topics in recent years. The main purposes of it are to simplify the original model, improve the readability of the model, and prevent over-fitting by searching for a suitable subset of features. There are many methods for this problem, including evolutionary algorithms and particle swarm optimization. Among them, the competitive swarm optimizer is a new optimization algorithm proposed in recent years, which is based on particle swarm optimization algorithm, and has achieved good results in high-dimensional feature selection problems, but it also has the problems of high computation time cost and easily being premature. Aiming at these problems, this paper proposes to add the crossover operator and mutation operator in the genetic algorithm to the competitive swarm optimization, so as to improve the generation speed of new individuals in the algorithm and prevent premature population. After testing on UC Irvine Machine Learning Repository, the new algorithm not only improves the computational efficiency, but also avoids the problem that the competitive swarm optimization algorithm is easy to fall into the local optimum, which greatly improves the calculation effect.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11663	11672		10.1007/s00500-019-04628-6		FEB 2020											
J								Driver Fatigue Detection Using Viola Jones and Principal Component Analysis	APPLIED ARTIFICIAL INTELLIGENCE											SYSTEM	In this paper, we have proposed a low-cost solution for driver fatigue detection based on micro-sleep patterns. Contrary to conventional methods, we acquired images by placing a camera on the extreme left side of the driver and proposed two algorithms that facilitate accurate face and eye detections, even when the driver is not facing the camera or driver's eyes are closed. The classification to find whether eye is closed or open is done on the right eye only using SVM and Adaboost. Based on eye states, micro-sleep patterns are determined and an alarm is triggered to warn the driver, when needed. In our dataset, we considered multiple subjects from both genders, having different appearances and under different lightning conditions. The proposed scheme gives 99.9% and 98.7% accurate results for face and eye detection, respectively. For all the subjects, the average accuracy of SVM and Adaboost is 96.5% and 95.4%, respectively.																	0883-9514	1087-6545				MAY 11	2020	34	6					456	483		10.1080/08839514.2020.1723875		FEB 2020											
J								Individual strategic profiles in tacit coordination games	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Decision-making; tacit coordination games; focal points; level-k	FOCAL POINTS; PURE; THINKING; MODELS; TEAM	In tacit coordination games people manage to converge on prominent solutions, which are known as focal points. There is still no accepted explanation of how players manage to converge on the same solution. It could be that the limited explanatory power arises from the fact that existing theories rely on pure strategies to describe behaviour. The aim of the current study is to construct a cognitive model that more accurately describes human behaviour in tacit coordination games. To this end we constructed individual strategic profiles that take into account the subjective preferences of individual players regarding the prominent selection rules. Subsequently, the individual profiles were clustered to gain insights regarding different types of coordinators. By using machine learning and statistical methods we were able to demonstrate, for the first time, the relationship between different types of strategic profiles and coordination ability. The results of this study demonstrate the importance of constructing a descriptive behavioural model that may potentially improve prediction of human decision-making in the context of human-machine interaction.																	0952-813X	1362-3079															10.1080/0952813X.2020.1721572		FEB 2020											
J								A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach	APPLIED INTELLIGENCE										Quantum machine learning; Quantum graph theoretic approach; Quantum information processing; Quantum feature selection; Quantum amplitude estimation	GRAVITATIONAL SEARCH ALGORITHM; COEFFICIENT	Quantum machine learning bridges the gap between abstract developments in quantum computing and the applied research on machine learning. It generally exposes the synthesis of important machine learning algorithms in a quantum framework. Dimensionality reduction of a dataset with a suitable feature selection strategy is one of the most important tasks in knowledge discovery and data mining. The efficient feature selection strategy helps to improve the overall accuracy of a large dataset in terms of machine learning operations. In this paper, a quantum feature selection algorithm using a graph-theoretic approach has been proposed. The proposed algorithm has used the concept of correlation coefficient based graph-theoretic classical approach initially and then applied the quantum Oracle with CNOT operation to verify whether the dataset is suitable for dimensionality reduction or not. If it is suitable, then our algorithm can efficiently estimate their high correlation values by using quantum parallel amplitude estimation and amplitude amplification techniques. This paper also shows that our proposed algorithm substantially outperforms than some popular classical feature selection algorithms for supervised classification in terms of query complexity of O, where N is the size of the feature vectors whose values are > THmin(minimum threshold), k is the number of iterations and where is the error for estimating those feature vectors. Compared with the classical counterpart, i.e. the performance of our quantum algorithm quadratically improves than others.																	0924-669X	1573-7497				JUN	2020	50	6					1775	1793		10.1007/s10489-019-01604-3		FEB 2020											
J								Reduced-order observer-based robust leader-following control of heterogeneous discrete-time multi-agent systems with system uncertainties	APPLIED INTELLIGENCE										Robust leader-following consensus; Heterogeneous multi-agent systems; Linear transformation; Reduced-order observer	COOPERATIVE OUTPUT REGULATION; CONSENSUS; TRACKING; DESIGN	In this paper, the leader-following control of heterogeneous discrete-time multi-agent systems (HD_MASs) in the presence of system uncertainties under directed topology is addressed. It aims to achieve reference tracking, disturbance rejection and robust control while the references and disturbances are generated by an autonomous exosystem. In practice, these agents are often different types of devices, thus they have different internal dynamics. Moreover, it is difficult to measure all states of each aircraft due to high cost or technical limitation. In this case, a novel leader-following output consensus problem is formulated and solved in this paper. Firstly, an appropriate linear transformation is proposed to divide the state information of each agent into measurable and unmeasurable parts. Then the reduced-order observer is designed only for unmeasurable parts. Based on the designed observer, the distributed feedback controller is proposed such that the outputs of all followers reach the same trajectory with the leader. In light of the internal model principle and discrete-time algebraic Riccati equation, the robust leader-following consensus of HD_MASs is achieved. Furthermore, this paper extends the results to continuous-time multi-agent systems. Finally, several simulation experiments are presented to verify the effectiveness of the theoretical results.																	0924-669X	1573-7497				JUN	2020	50	6					1794	1812		10.1007/s10489-019-01553-x		FEB 2020											
J								Multiple-criteria decision making method based on the scaled prioritized operators with unbalanced linguistic information	ARTIFICIAL INTELLIGENCE REVIEW										Scaled prioritized operator; Unbalanced linguistic terms set; Multi-criteria decision making	GEOMETRIC AGGREGATION OPERATORS; MODEL; SEMANTICS; DEAL	The unbalanced linguistic terms set (ULTS) is a special linguistic term set which can describe the vagueness assessment that is non-uniform and non-symmetrical distributed. So, it is effective to describe the uncertainty information existed in some special real decision making problems by ULTS. As a special prioritized operator, the scaled prioritized (SP) operator has the advantage of taking the priority among different criteria into account by detailed priority labels in known case and unknown case. In this paper, we combine the merits of SP operators and ULTS for dealing with some special multi-criteria decision making (MCDM) problems where there is a priority relationship between criteria under ULTS evaluation information. We present the unbalanced 2-tuple linguistic scaled prioritized averaging operator and the unbalanced 2-tuple linguistic scaled prioritized geometric averaging operator, which can handle the issues of the detailed priority relationship among different categories of MCDM problems in knowable case. Further, we propose the unbalanced 2-tuple linguistic scaled prioritized weighted averaging operator and the unbalanced 2-tuple linguistic scaled prioritized geometric weighted averaging operator, which can deal with the case when the detailed priority relationship among different categories of different criteria is unknowable. Then, we discussed several characteristics of the proposed operators, such as boundedness, monotonicity, and idempotency. Besides, we presented an approach for the MCDM problems according to the proposed operators. In the last, we provide an example to explain the calculating steps and effectiveness of these methods.																	0269-2821	1573-7462				OCT	2020	53	7					4967	4991		10.1007/s10462-020-09812-x		FEB 2020											
J								Feed-forward versus recurrent architecture and local versus cellular automata distributed representation in reservoir computing for sequence memory learning	ARTIFICIAL INTELLIGENCE REVIEW										ReCA; Reservoir computing; Cellular automata; Recurrent architecture; Feed-forward architecture; Distributed representation; Local representation; 5-Bit memory task	COMPUTATION; MACHINE; CHAOS; EDGE	Reservoir computing based on cellular automata (ReCA) constructs a novel bridge between automata computational theory and recurrent neural networks. ReCA has been trained to solve 5-bit memory tasks. Several methods are proposed to implement the reservoir where the distributed representation of cellular automata (CA) in recurrent architecture could solve the 5-bit tasks with minimum complexity and minimum number of training examples. CA distributed representation in recurrent architecture outperforms the local representation in recurrent architecture (stack reservoir), then echo state networks and feed-forward architecture using local or distributed representation. Extracted features from the reservoir, using the natural diffusion of CA states in the reservoir offers the state-of-the-art results in terms of feature vector length and the required training examples. Another extension is obtained by combining the reservoir CA states using XOR, Binary or Gray operator to produce a single feature vector to reduce the feature space. This method gives promising results, however using the natural diffusion of CA states still outperform. ReCA can be considered to operate around the lower bound of complexity; due to using the elementary CA in the reservoir.																	0269-2821	1573-7462				OCT	2020	53	7					5083	5112		10.1007/s10462-020-09815-8		FEB 2020											
J								Facial feedback for reinforcement learning: a case study and offline analysis using the TAMER framework	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Reinforcement learning; Facial expressions; Human agent interaction; Interactive reinforcement learning	ROBOT; BEHAVIOR	Interactive reinforcement learning provides a way for agents to learn to solve tasks from evaluative feedback provided by a human user. Previous research showed that humans give copious feedback early in training but very sparsely thereafter. In this article, we investigate the potential of agent learning from trainers' facial expressions via interpreting them as evaluative feedback. To do so, we implemented TAMER which is a popular interactive reinforcement learning method in a reinforcement-learning benchmark problem-Infinite Mario, and conducted the first large-scale study of TAMER involving 561 participants. With designed CNN-RNN model, our analysis shows that telling trainers to use facial expressions and competition can improve the accuracies for estimating positive and negative feedback using facial expressions. In addition, our results with a simulation experiment show that learning solely from predicted feedback based on facial expressions is possible and using strong/effective prediction models or a regression method, facial responses would significantly improve the performance of agents. Furthermore, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface.																	1387-2532	1573-7454				FEB 12	2020	34	1							22	10.1007/s10458-020-09447-w													
J								A cascade network for the classification of rice grain based on single rice kernel	COMPLEX & INTELLIGENT SYSTEMS										Image; Rice grain; Morphological; Colour; Texture; Neural network	CEREAL-GRAINS; MACHINE VISION; NEURAL-NETWORK; SYSTEM; COLOR; WHEAT; IDENTIFICATION; CULTIVARS; FEATURES; MORPHOLOGY	This paper describes the classification of four different varieties of rice grain based on four sets of features, namely morphology, colour, texture and wavelet. The classification is carried out on single rice kernel using image pre-processing steps followed by a cascade network classifier. The performance of the classifiers based on the above feature sets is also compared. It is found that morphological feature is more suitable for the classification of rice kernels, as compared to other features. The number of input features is reduced by a feature selection process using statistical analysis system (SAS) software. The classification accuracy based on selected features is compared with that of original features using different classifiers. It is found that the selected features are able to provide classification accuracy very close to the original features. The performance of the proposed cascade classifier is also tested against standard datasets from the University of California, Irvine (UCI), and the results are compared with other classifiers. The results show that the proposed classifier provides better classification accuracy as compared to other classifiers.																	2199-4536	2198-6053				JUL	2020	6	2					321	334		10.1007/s40747-020-00132-9		FEB 2020											
J								Evaluation of green and sustainable building project based on extension matter-element theory in smart city application	COMPUTATIONAL INTELLIGENCE										correlation function; entropy method; extension matter-element theory; green building; project evaluation	SYSTEM; LEED	The postoccupancy performance and operation management level of green building largely determines the overall sustainability level of green buildings. It is necessary to evaluate the degree of success of green buildings in the operation stage to ensure the implementation effect of the whole life cycle. However, the current green building evaluation system is incomplete, and the research on the degree of success evaluation of green buildings is very lacking. It is necessary to establish a scientific and effective evaluation index system to evaluate the degree of success of green buildings. This article applies the extension matter-element theory and entropy method to evaluate the degree of success of green building projects, taking a green building project in Nanchang as an example, and seven experts were invited to score and evaluate the project's degree of success by calculating the relevance degree of each evaluation index. The results show that this green building's degree of success emerged as at level II, that is, the "generally successful" level, and each first-level indicator, respectively, lay at level III, level II, level II, and level II. The case study proves that the evaluation method of green building determined in this article is scientific and reliable.																	0824-7935	1467-8640															10.1111/coin.12286		FEB 2020											
J								A hybrid EVSA approach in clustered search space with ad-hoc partitioning for multi-robot searching	EVOLUTIONARY INTELLIGENCE										Multi-robot system; Target searching; Exploration; Frontier-cluster; Ad-hoc partitioning	EXPLORATION; SWARM	This paper examines the problem of multi-robot target searching in an unknown environment. Since no information is available about the targets, so the search is similar to the exploration problem. In this paper, a new method is proposed to improve the efficiency of exploration. The objective of the proposed approach is to minimize the exploration time by reducing the redundant coverage and computational overhead. For exploration, the concept of frontiers is being used. The following hypothesis formulated in order to improve the exploration: (1) Introduction of an ad-hoc partitioning method to handle redundant coverage. (2) Reduction of the search space by clustering (grouping) the frontier cells to minimize the computational overhead. (3) Introduction of methods for robots' next position assignment problem, namely, nearest frontier-cluster center method when a single robot is searching in the sub-region. A hybrid of Egyptian vulture and simulated annealing based approach when more than one robots are searching within a sub-region. Performance of the proposed approach is evaluated through simulation in two different workspaces with a team size of 2 and 4 robots. Four different performance measures namely Redundant coverage, Object localization time, Exploration time and Exploration percentage are considered to evaluate the performance of the proposed method. Results show that proposed hybrid-EVSA method completes exploration much faster in both the workspaces with the team size of 2 and 4 robots as compared to other state of art approaches due to low computational overhead and reduced redundant coverage.																	1864-5909	1864-5917				DEC	2020	13	4					551	570		10.1007/s12065-020-00356-1		FEB 2020											
J								Fault detection in smart grids with time-varying distributed generation using wavelet energy and evolving neural networks	EVOLVING SYSTEMS										Evolving neural network; Fault detection; Smart grid; Distributed generation; Wavelet transform	DISTRIBUTION-SYSTEMS; DATA STREAMS; IMPEDANCE; FUZZY; LOCATION; IDENTIFICATION	Online monitoring systems have been developed for real-time detection of high impedance faults in power distribution networks. Sources of distributed generation are usually ignored in the analyses. Distributed generation imposes great challenges to monitoring systems. This paper proposes a wavelet transform-based feature-extraction method combined with evolving neural networks to detect and locate high impedance faults in time-varying distributed generation systems. Empirically validated IEEE models, simulated in the ATPDraw and Matlab environments, were used to generate data streams containing faulty and normal occurrences. The energy of detail coefficients obtained from different wavelet families such as Symlet, Daubechies, and Biorthogonal are evaluated as feature extraction method. The proposed evolving neural network approach is particularly supplied with a recursive algorithm for learning from online data stream. Online learning allows the neural models to capture novelties and, therefore, deal with nonstationary behavior. This is a unique characteristic of this type of neural network, which differentiate it from other types of neural models. Comparative results considering feed-forward, radial-basis, and recurrent neural networks as well as the proposed hybrid wavelet-evolving neural network approach are shown. The proposed approach has provided encouraging results in terms of accuracy and robustness to changing environment using the energy of detail coefficients of a Symlet-2 wavelet. Robustness to the effect of distributed generation and to transient events is achieved through the ability of the neural model to update parameters, number of hidden neurons, and connection weights recursively. New conditions could be captured on the fly, during the online operation of the system.																	1868-6478	1868-6486				JUN	2020	11	2			SI		165	180		10.1007/s12530-020-09328-3		FEB 2020											
J								On utilizing an enhanced object partitioning scheme to optimize self-organizing lists-on-lists	EVOLVING SYSTEMS										Learning automata; "Adaptive" data structures; Hierarchical singly-linked lists; Object migration automaton	PURSUIT LEARNING SCHEMES; AUTOMATA; ALGORITHMS; UPDATE	With the advent of "Big Data" as a field, in and of itself, there are at least three fundamentally new questions that have emerged, namely the Artificially Intelligence (AI)-based algorithms required, the hardware to process the data, and the methods to store and access the data efficiently. This paper (The work of the second author was partially supported by NSERC, the Natural Sciences and Engineering Council of Canada. We are very grateful for the feedback from the anonymous Referees of the original submission. Their input significantly improved the quality of this final version.) presents some novel schemes for the last of the three areas. There have been thousands of papers written regarding the algorithms themselves, and the hardware vendors are scrambling for the market share. However, the question of how to store, manage and access data, which has been central to the field of Computer Science, is even more pertinent in these days when megabytes of data are being generated every second. This paper considers the problem of minimizing the cost of retrieval using the most fundamental data structure, i.e., a Singly-Linked List (SLL). We consider a SLL in which the elements are accessed by a Non-stationary Environment (NSE) exhibiting the so-called "Locality of Reference". We propose a solution to the problem by designing an "Adaptive" Data Structure (ADS), which is created by utilizing a composite of hierarchical data "sub"-structures to constitute the overall data structure (In this paper, the primitive data structure is the SLL. However, these concepts can be extended to more complicated data structures.). In this paper, we design hierarchical Lists-on-Lists (LOLs) by assembling a SLL into a hierarchical scheme that results in SLLs on SLLs (SLLs-on-SLLs) comprising of an outer-list and sublist contexts. The goal is that elements that are more likely to be accessed together are grouped within the same sub-context, while the sublists themselves are moved "en masse" towards the head of the list-context to minimize the overall access cost. This move is carried out by employing the "de-facto" list re-organization schemes, i.e., the Move-To-Front (MTF) and Transposition (TR) rules. To achieve the clustering of elements within the sublists, we invoke the Object Migration Automaton (OMA) family of reinforcement schemes from the theory of Learning Automata (LA). They capture the probabilistic dependence of the elements in the data structure, receiving query accesses from the Environment. We show that SLLs-on-SLLs augmented with the Enhanced OMA (EOMA) minimizes the retrieval cost for elements in NSEs, and are superior to the stand-alone MTF and TR schemes, and also superior to the OMA-augmented SLLs-on-SLLs operating in such Environments.																	1868-6478	1868-6486															10.1007/s12530-020-09327-4		FEB 2020											
J								An intelligent hybrid approach for task scheduling in cluster computing environments as an infrastructure for biomedical applications	EXPERT SYSTEMS										ant colony optimization (ACO); biomedical data processing; cellular learning automata (CLA); cluster computing environment; metaheuristics; processing infrastructure; task graph scheduling	ALGORITHMS	Nowadays, increase in time complexity of applications and decrease in hardware costs are two major contributing drivers for the utilization of high-performance architectures such as cluster computing systems. Actually, cluster computing environments, in the contemporary sophisticated data centres, provide the main infrastructure to process various data, where the biomedical one is not an exception. Indeed, optimized task scheduling is key to achieve high performance in such computing environments. The most distractive assumption about the problem of task scheduling, made by the state-of-the-art approaches, is to assume the problem as a whole and try to enhance the overall performance, while the problem is actually consisted of two disparate-in-nature subproblems, that is, sequencing subproblem and assigning one, each of which needs some special considerations. In this paper, an efficient hybrid approach named ACO-CLA is proposed to solve task scheduling problem in the mesh-topology cluster computing environments. In the proposed approach, an enhanced ant colony optimization (ACO) is developed to solve the sequence subproblem, whereas a cellular learning automata (CLA) machine tackles the assigning subproblem. The utilization of background knowledge about the problem (i.e., tasks' priorities) has made the proposed approach very robust and efficient. A randomly generated data set consisting of 125 different random task graphs with various shape parameters, like the ones frequently encountered in the biomedicine, has been utilized for the evaluation of the proposed approach. The conducted comparison study clearly shows the efficiency and superiority of the proposed approach versus traditional counterparts in terms of the performance. From our first metric, that is, the NSL (normalized schedule length) point of view, the proposed ACO-CLA is 2.48% and 5.55% better than the ETF (earliest time first), which is the second-best approach, and the average performance of all other competing methods. On the other hand, from our second metric, that is, the speedup perspective, the proposed ACO-CLA is 2.66% and 5.15% better than the ETF (the second-best approach) and the average performance of all the other competitors.																	0266-4720	1468-0394														e12536	10.1111/exsy.12536		FEB 2020											
J								Option implied moments obtained through fuzzy regression	FUZZY OPTIMIZATION AND DECISION MAKING										Forecasting; Fuzzy regression; Skewness; Kurtosis; Italian market	NONLINEAR-REGRESSION; PORTFOLIO SELECTION; VOLATILITY; MODEL; BLACK	The aim of this paper is to investigate the potential of fuzzy regression methods for computing more reliable estimates of higher-order moments of the risk-neutral distribution. We improve upon the formula of Bakshi et al. (RFS 16(1):101-143, 2003), which is used for the computation of market volatility and skewness indices (such as the VIX and the SKEW indices traded on the Chicago Board Options Exchange), through the use of fuzzy regression methods. In particular, we use the possibilistic regression method of Tanaka, Uejima and Asai, the least squares fuzzy regression method of Savic and Pedrycz and the hybrid method of Ishibuchi and Nii. We compare the fuzzy moments with those obtained by the standard methodology, based on the Bakshi et al. (2003) formula, which relies on an ex-ante choice of the option prices to be used and cubic spline interpolation. We evaluate the quality of the obtained moments by assessing their forecasting power on future realized moments. We compare the competing forecasts by using both the Model Confidence Set and Mincer-Zarnowitz regressions. We find that the forecasts for skewness and kurtosis obtained using fuzzy regression methods are closer to the subsequently realized moments than those provided by the standard methodology. In particular, the lower bound of the fuzzy moments obtained using the Savic and Pedrycz method is the best ones. The results are important for investors and policy makers who can rely on fuzzy regression methods to get a more reliable forecast for skewness and kurtosis.																	1568-4539	1573-2908				JUN	2020	19	2					211	238		10.1007/s10700-020-09316-x		FEB 2020											
J								Pothole detection using location-aware convolutional neural networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Computer vision; Deep learning; Pavement monitoring; Pothole detection; Location-aware network		Poor road conditions, such as potholes, are a nuisance to society, which would annoy passengers, damage vehicles, and even cause accidents. Thus, detecting potholes is an important step toward pavement maintenance and rehabilitation to improve road conditions. Potholes have different shapes, scales, shadows, and illumination effects, and highly complicated backgrounds can be involved. Therefore, detection of potholes in road images is still a challenging task. In this study, we focus on pothole detection in 2D vision and present a new method to detect potholes based on location-aware convolutional neural networks, which focuses on the discriminative regions in the road instead of the global context. It consists of two main subnetworks: the first localization subnetwork employs a high recall network model to find as many candidate regions as possible, and the second part-based subnetwork performs classification on the candidates on which the network is expected to focus. The experiments using the public pothole dataset show that the proposed method could achieve high precision (95.2%), recall (92.0%) simultaneously, and outperform the most existing methods. The results also demonstrate that accurate part localization considerably increases classification performance while maintains high computational efficiency. The source code is available at .																	1868-8071	1868-808X				APR	2020	11	4					899	911		10.1007/s13042-020-01078-7		FEB 2020											
J								Intent-based networking with proactive load distribution in data center using IBN manager and Smart Path manager	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intent based networking; Software defined network; Proactive networking; Load distribution; Resource utilization; Fat-tree topology	PERFORMANCE; ENERGY	Network establishment and management in the data center need a considerable investment of time and money. To increase the cost productivity of networks, maximizing the utilization of network resources is mandatory. Many network inconsistencies may occur due to the utilization of maximum link capacity. Nowadays, Fat-Tree topology is a popular network architecture in data center networks therefore many researchers are designing routing algorithms for fat-tree topology by adopting load balancing methods. The inefficient way of obtaining network traffic statistics from network devices is a major problem to achieve load balancing traditionally. Software-Defined Network (SDN) platform provides researchers, an environment to develop a user-defined routing algorithm that can be flexible as well as cost-efficient. In this paper, an SDN based system has been proposed for Fat-Tree topology that provides a solution that enables networks to make effective resource utilization and minimize the maximum link capacity utilization of network using intent-based networking (IBN). The proposed system provides a permanent unique path from source to destination that allows traffic to flow uniformly and enables all the links to be offloaded by providing them with equal traffic share proactively. By taking a proactive approach which is inherence of an IBN, the proposed system provides the secure channel to provision the host in network with load balancing capability in the running network by restricting illegal in-premise access to the network.																	1868-5137	1868-5145															10.1007/s12652-020-01753-1		FEB 2020											
J								The adoption of scale space hierarchical cluster analysis algorithm in the classification of rock-climbing teaching evaluation system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Scale space; Hierarchical clustering; Rock climbing techniques; Teaching and training environmental analysis; Similarity	FRAMEWORK	In order to construct the basic frame of the evaluation system of the training effect of rock climbing technique to carry out the teaching practice of rock climbing technique under the guidance of scientific theory, in this research, the teaching and training environment of rock climbing technology is analyzed using the cluster analysis method based on scale spatial hierarchy analysis. Firstly, the students' physical qualities, physical knowledge, sports skills, and sports experience are selected as the first level evaluation index of rock-climbing technique teaching and training, and the index evaluation system is constructed. Then the proportional space is divided into regions. According to the similarity between samples, the distance between classified categories and the consistency within the same category are defined, and the calculation is carried out to simplify the calculation process. Finally, the effectiveness of the proposed algorithm in rock climbing teaching and training is verified by empirical analysis. The results show that the four first-level indicators selected have obvious positive effects on climbing techniques. During the climb, the subject needs to stretch as much as possible in order to make the ideal movement and climbing posture according to the climbing route. In addition, stretching plays a large part in warm-up and relaxation before and after climbing. The study has a good guiding significance for the development of climbing.																	1868-5137	1868-5145															10.1007/s12652-020-01778-6		FEB 2020											
J								New sensor fault detection and isolation strategy-based interval-valued data	JOURNAL OF CHEMOMETRICS										data-driven process monitoring; fault detection and isolation; generalized likelihood ratio; interval-valued data; principal component analysis; reconstruction	PRINCIPAL COMPONENT ANALYSIS; PLS-BASED GLRT; RECONSTRUCTION; IDENTIFICATION; DIAGNOSIS; SELECTION; NUMBER; FDI; PCA	In this paper, a new data-driven sensor fault detection and isolation (FDI) technique for interval-valued data is developed. The developed approach merges the benefits of generalized likelihood ratio (GLR) with interval-valued data and principal component analysis (PCA). This paper has three main contributions. The first contribution is to develop a criterion based on the variance of interval-valued reconstruction error to select the number of principal components to be kept in the PCA model. Secondly, interval-valued residuals are generated, and a new fault detection chart-based GLR is developed. Lastly, an enhanced interval reconstruction approach for fault isolation is developed. The proposed strategy is applied for distillation column process monitoring and air quality monitoring network.																	0886-9383	1099-128X				MAY	2020	34	5							e3222	10.1002/cem.3222		FEB 2020											
J								Generalized Visual Information Analysis Via Tensorial Algebra	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Commutative ring; Grassmannian manifold; Image classification; Tensor singular value decomposition; Tensors	SINGULAR-VALUE DECOMPOSITION; SPATIAL FEATURE-EXTRACTION; REPRESENTATION	High-order data are modeled using matrices whose entries are numerical arrays of a fixed size. These arrays, called t-scalars, form a commutative ring under the convolution product. Matrices with elements in the ring of t-scalars are referred to as t-matrices. The t-matrices can be scaled, added and multiplied in the usual way. There are t-matrix generalizations of positive matrices, orthogonal matrices and Hermitian symmetric matrices. With the t-matrix model, it is possible to generalize many well-known matrix algorithms. In particular, the t-matrices are used to generalize the singular value decomposition (SVD), high-order SVD (HOSVD), principal component analysis (PCA), two-dimensional PCA (2DPCA) and Grassmannian component analysis (GCA). The generalized t-matrix algorithms, namely TSVD, THOSVD, TPCA, T2DPCA and TGCA, are applied to low-rank approximation, reconstruction and supervised classification of images. Experiments show that the t-matrix algorithms compare favorably with standard matrix algorithms.																	0924-9907	1573-7683				MAY	2020	62	4					560	584		10.1007/s10851-020-00946-9		FEB 2020											
J								Enhancing supervised bug localization with metadata and stack-trace	KNOWLEDGE AND INFORMATION SYSTEMS										Bug localization; Bug reports; Supervised topic modeling; Metadata; Stack-trace	FAULT LOCALIZATION; TOOL	Locating relevant source files for a given bug report is an important task in software development and maintenance. To make the locating process easier, information retrieval methods have been widely used to compute the content similarities between bug reports and source files. In addition to content similarities, various other sources of information such as the metadata and the stack-trace in the bug report can be used to enhance the localization accuracy. In this paper, we propose a supervised topic modeling approach for automatically locating the relevant source files of a bug report. In our approach, we take into account the following five key observations. First, supervised modeling can effectively make use of the existing fixing histories. Second, certain words in bug reports tend to appear multiple times in their relevant source files. Third, longer source files tend to have more bugs. Fourth, metainformation brings additional guidance on the search space. Fifth, buggy source files could be already contained in the stack-trace. By integrating the above five observations, we experimentally show that the proposed method can achieve up to 67.1% improvement in terms of prediction accuracy over its best competitors and scales linearly with the size of the data.																	0219-1377	0219-3116				JUN	2020	62	6					2461	2484		10.1007/s10115-019-01426-2		FEB 2020											
J								Perception of cloth in assistive robotic manipulation tasks	NATURAL COMPUTING										Robotic assistance; Robotic cloth manipulation; Perception of cloth	DEFORMABLE OBJECTS; GEOMETRIC APPROACH; POSE ESTIMATION; CLASSIFICATION; RECOGNITION; CATEGORY; STRATEGY; LAUNDRY; MODELS	Assistive robots need to be able to perform a large number of tasks that imply some type of cloth manipulation. These tasks include domestic chores such as laundry handling or bed-making, among others, as well as dressing assistance to disabled users. Due to the deformable nature of fabrics, this manipulation requires a strong perceptual feedback. Common perceptual skills that enable robots to complete their cloth manipulation tasks are reviewed here, mainly relying on vision, but also resorting to touch and force. The use of such basic skills is then examined in the context of the different cloth manipulation tasks, be them garment-only applications in the line of performing domestic chores, or involving physical contact with a human as in dressing assistance.																	1567-7818	1572-9796				JUN	2020	19	2			SI		409	431		10.1007/s11047-020-09784-5		FEB 2020											
J								LGSim: local task-invariant and global task-specific similarity for few-shot classification	NEURAL COMPUTING & APPLICATIONS										Few-shot learning; Local similarity; Global similarity; Meta-learning	NEURAL-NETWORKS	Few-shot learning is one of the most challenging problems in computer vision due to the difficulty of sample collection in many real-world applications. It aims at classifying a sample when the number of training samples for each identity is limited. Most of the existing few-shot learning models learn a distance metric with pairwise or triplet constraints. In this paper, we make initial attempts on learning local and global similarities simultaneously to improve the few-shot classification performance in terms of accuracy. In particular, our system differs in two aspects. Firstly, we develop a neural network to learn the pairwise local relationship between each pair of samples in the union set that is composed of support set and query set, which fully utilize the supervision. Secondly, we design a global similarity function from the manifold perspective. The latent assumption is that if the neighbors of one sample are similar to those of another sample, the global similarity between them will be high. Otherwise, the global similarity of the two samples will become very low even if the local similarity between them is high. Meanwhile, we propose a new loss according to the pairwise local loss and task-specific global loss, encouraging the model toward better generalization. Extensive experiments on three popular benchmarks (Omniglot, miniImageNet and tieredImageNet) demonstrate that our simple, yet effective approach can achieve competitive accuracy compared to the state-of-the-art methods.																	0941-0643	1433-3058				AUG	2020	32	16					13065	13076		10.1007/s00521-020-04750-9		FEB 2020											
J								Analysing the intermeshed patterns of road transportation and macroeconomic indicators through neural and clustering techniques	PATTERN ANALYSIS AND APPLICATIONS										Artificial intelligence; Exploratory projection pursuit; Clustering; Forecast; Economy; Road transportation	FREIGHT TRANSPORT; MODELS; PREDICTION; SYSTEMS; IMPACT	As is widely acknowledged, the transportation of goods by road can, in one way or another, be linked to a range of macroeconomic indicators. A hybrid artificial intelligence system is proposed in this paper to analyse the interaction between transportation patterns and the economy. The temporal patterns of road transportation and macroeconomic trends are studied, by combining the use of both (supervised and unsupervised) neural networks and clustering techniques. The proposed system is validated, by establishing links between road transportation data from Spain and macroeconomic trends over 6 years (2011-2017). The results reveal an interesting inner structure of the data, through data visualizations of intermeshed relations between road transportation patterns and macroeconomic indicators. The same data structure was also visible in the output of the clustering techniques. Furthermore, a number of high-quality predictions were advanced by processing the road transportation data as time series, and forecasting the future values of the main series. These results demonstrated the validity of the proposed linkage between road transportation data and macroeconomic indicators.																	1433-7541	1433-755X				AUG	2020	23	3					1059	1070		10.1007/s10044-020-00872-x		FEB 2020											
J								Learning discriminative hashing codes for cross-modal retrieval based on multi-view features	PATTERN ANALYSIS AND APPLICATIONS										Cross-modal retrieval; Hashing learning; Subspace learning; Kernelization; Multi-view features		Hashing techniques have been applied broadly in retrieval tasks due to their low storage requirements and high speed of processing. Many hashing methods based on a single view have been extensively studied for information retrieval. However, the representation capacity of a single view is insufficient and some discriminative information is not captured, which results in limited improvement. In this paper, we employ multiple views to represent images and texts for enriching the feature information. Our framework exploits the complementary information among multiple views to better learn the discriminative compact hash codes. A discrete hashing learning framework that jointly performs classifier learning and subspace learning is proposed to complete multiple search tasks simultaneously. Our framework includes two stages, namely a kernelization process and a quantization process. Kernelization aims to find a common subspace where multi-view features can be fused. The quantization stage is designed to learn discriminative unified hashing codes. Extensive experiments are performed on single-label datasets (WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE), and the experimental results indicate the superiority of our method compared with the state-of-the-art methods.																	1433-7541	1433-755X				AUG	2020	23	3					1421	1438		10.1007/s10044-020-00870-z		FEB 2020											
J								A metaheuristic segmentation framework for detection of retinal disorders from fundus images using a hybrid ant colony optimization	SOFT COMPUTING										Retinal image segmentation; Fundus images; DRIVE; STARE; Ant colony optimization; Neural network classifier; Classification accuracy	VESSEL SEGMENTATION; BLOOD-VESSEL; OPTIC DISC; FILTER	Imaging modalities play a major role in early detection and diagnosis of various medical conditions related to the patient. Retinal image segmentation has been taken up for investigation in this research paper to efficiently detect the presence of eye disorder which could be indicators of major onset of conditions like hypertension, cataracts, diabetic retinopathy, age-related macular disorders, etc. A machine learning method for classification of given pixels in the search space into regions containing blood vessels and those that do not contain blood vessels is implemented using a three-stage neural classifier in this paper. Prior to classification, an optimization algorithm namely ant colony optimization derived from nature-inspired phenomena is used to provide an optimal feature vector set to set high standards for the neural network based classification approach. The novelty and merits of the paper lie in back tracing of the segmentation process in which optimization is done first on the preprocessed features followed by classification for segmented output on the optimized features. This results in elimination of redundant feature vectors which tend to occupy much memory as well increase the computational overhead on the process. The entire implemented system is automated by the machine learning process and tested on 30 samples, 15 each on DRIVE and STARE databases. Classification rates of nearly 98% on an average scenario have been achieved for segmentation and 96.5% for abnormality detection. The performances have been compared against Bayesian set models and standalone ANN models.																	1432-7643	1433-7479				SEP	2020	24	17					13347	13356		10.1007/s00500-020-04753-7		FEB 2020											
J								Island artificial bee colony for global optimization	SOFT COMPUTING										Artificial bee colony; Island-based model; Structured population; Population diversity; Optimization	AUTOMATIC-GENERATION CONTROL; SHOP SCHEDULING PROBLEM; GENETIC ALGORITHM; MODEL; SEARCH; IMPACT	This paper proposes an efficient version of artificial bee colony (ABC) algorithm based on the island model concepts. The new version is called the island artificial bee colony (iABC) algorithm. It uses the structured population concept by applying the island model to improve the diversification capabilities of ABC. In the island model, the population is divided into a set of sub-populations called islands, each of which is manipulated separately by an independent variant of the ABC. After a predefined number of iterations, the islands exchange their solutions by migration. This process can help ABC in controlling the diversity of the population during the search process and thus improve the performance. The proposed iABC is evaluated using global optimization functions established by the IEEE-CEC 2015 which include 15 test functions with various dimensions and complexities (i.e., 10, 30, and 50). In order to evaluate the performance of iABC, various parameter settings are utilized to test the effectiveness of their convergence properties. Furthermore, the performance of iABC is compared against 19 comparative methods that used the same IEEE-CEC 2015 test functions. The results show that iABC produced better results when compared with ABC in all IEEE-CEC 2015 test functions, while the results of iABC better than those of the other island-based algorithm on almost all test functions. Furthermore, iABC is able to obtain three new results for three test functions better than all the comparative methods. Using Friedman test and Holm's procedure, iABC is ranked third, seventh, and ninth out of 19 comparative methods for the test functions with 10, 30, 50 dimensionality, respectively.																	1432-7643	1433-7479				SEP	2020	24	17					13461	13487		10.1007/s00500-020-04760-8		FEB 2020											
J								Data mining powered by the gene ontology	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										association rule mining; clustering; data mining; gene ontology; text mining	ASSOCIATION RULES; EXPRESSION DATA; ALGORITHM; TOOL	The gene ontology (GO) is a widely used resource for describing molecular functions, biological processes, and cellular components of gene products. Since its inception in 2006, the GO has been used to describe millions of gene products resulting in a massive data store of over 6 million annotations. The staggering amount of data that has resulted from annotating gene products with GO terms has led the way and opened new avenues for a wide variety of large-scale computational analyses. Specifically, a variety of data mining techniques such as association rule mining, clustering etc. have been applied successfully to a range of biological applications. This article provides a review of four data mining applications/techniques for GO data mining gene expression data, association rule mining, clustering, and text mining and highlights future directions in each of these areas. This article is categorized under: Algorithmic Development > Association Rules Algorithmic Development > Biological Data Mining Ensemble Methods > Text Mining																	1942-4787	1942-4795				MAY	2020	10	3							e1359	10.1002/widm.1359		FEB 2020											
J								A study on domatic number of cycle related graphs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diameter; Diametrically uniform; Tadpole graph; Lollipop graph; Barbell graph; Dominat set; Cognitive WSN		A domatic partition of G is the partition of vertices V(G) into disjoint dominating sets. The maximum size of disjoint dominating sets is called the domatic number of G. In this paper, comparative results are investigated on domatic partition of few graphs of cycle related graphs such as complete graph, tadpole graph, lollipop graph and barbell graph for cognitive wireless sensor networks. Then the middle graph and central graph of these graphs are studied and domatic number of the defined graphs are determined to find the nodes in disjoint sets to disribute the tasks uniformly rather than burden the nodes in domatic set. Furthermore, diameter and domination number of these graphs are observed.																	1868-5137	1868-5145															10.1007/s12652-020-01740-6		FEB 2020											
J								TI-SC: top-k influential nodes selection based on community detection and scoring criteria in social networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social network; Viral marketing; Influence maximization; Seed node; Community detection	INFLUENCE MAXIMIZATION; SPREADERS; ALGORITHM	Influence maximization is a classic optimization problem to find a subset of seed nodes in a social network that has a maximum influence with respect to a propagation model. This problem suffers from the overlap of seed nodes and the lack of optimal selection of seed nodes. Kempe et al. have shown that this problem is an NP-hard problem, and the objective function is submodular. Therefore, some heuristic and greedy algorithms have been proposed to find a near-optimal solution. However, the greedy algorithm may not satisfy the accuracy of a given solution and high time-consuming problem. To overcome these problems, the TI-SC algorithm is proposed for the problem of influence maximization. The TI-SC algorithm selects the influential nodes by examining the relationships between the core nodes and the scoring ability of other nodes. After selecting each seed node, the scores are updated to reduce the overlap in selecting the seed nodes. This algorithm has efficient performance in high Rich-Club networks. The Rich-Club phenomenon causes overlapping of the influence spread among the seed nodes in most of the other methods so that the TI-SC algorithm reduces this overlapping. Furthermore, the discovered communities with low expansion are not considered in the seed node selection phase, and this is useful for reducing computational overhead. Experimental results on both synthetic and real datasets show that the proposed TI-SC algorithm significantly outperforms the state-of-the-art algorithms in terms of efficiency in both small and large-scale datasets.																	1868-5137	1868-5145															10.1007/s12652-020-01760-2		FEB 2020											
J								Visualization of spatial matching features during deep person re-identification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Person re-identification; Network visualization; Feature map	NEURAL-NETWORKS	Person re-identification (Re-ID) based on deep learning has made great progress and achieved state-of-the-art performance in recent years. However, the end-to-end properties of deep neural networks allow us to directly feedback the output results based on its input, making the inner working mechanism of the deep person Re-ID model and its decision reasons lack of transparency and explainability. This further impedes improvements to pedestrian recognition performance. As feature visualization has been proven to be an effective method for characterizing the middle layer of a neural network, we propose a novel gradient-based visualization method to interpret the internal features learned by deep person Re-ID. Based on the idea of transfer learning, this model regards the pretrained ResNet-50 on the ImageNet dataset as a basic network for deep person Re-ID. First, the network is fine-tuned on the person Re-ID dataset to achieve pedestrian classification, and then, the gradient-based visualization of the trained network is performed to highlight important regions contributing to image similarity. Experiments conducted on the Market-1501 dataset verify that our model can not only enable the network to identify key features of an individual across different images, but also provide visual interpretation for the pedestrian classification results to improve the reliability of person Re-ID and foster trust from users regarding its decisions.																	1868-5137	1868-5145															10.1007/s12652-020-01754-0		FEB 2020											
J								A hybrid-feedback recommender system for employment websites	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender system; Job seeker; JSS-Tree; Employment website		Recommender systems have been widely used in many fields, but have been particularly common in the generation of user feedback. For Taiwanese employment websites, recommender systems match job seekers with employers but often fail to satisfy both the job seekers' preferences and the prospective employers' requirements. In the process, these websites often waste users' time. To address this problem, this research proposes a hybrid-feedback recommender system specifically for job seekers in the context of Taiwan's online employment scene. To identify the correlations between job searches and active job-seeking users, this research harness both implicit and explicit techniques that are based on user portfolios and a new data-mining technique called JSS-Tree. With the system, job seekers can spend less time and get better results than is currently possible on Taiwanese employment websites. Furthermore, through JSS-Tree, the recommender system generates substantially diverse recommendation results for job seekers, who thus have more choices than would normally be available and who can avoid wasting time while searching for desirable jobs on employment websites.																	1868-5137	1868-5145															10.1007/s12652-020-01772-y		FEB 2020											
J								A load balancing and optimization strategy (LBOS) using reinforcement learning in fog computing environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Load balancing; Resource allocation; Real time fog computing; Healthcare system; Machine learning; Reinforcement learning; Artificial intelligence; Genetic algorithm; Mobile HEALTH dataset	RESOURCE-ALLOCATION; ALGORITHMS; ENERGY	Fog computing (FC) can be considered as a computing paradigm which performs Internet of Things (IoT) applications at the edge of the network. Recently, there is a great growth of data requests and FC which lead to enhance data accessibility and adaptability. However, FC has been exposed to many challenges as load balancing (LB) and adaptation to failure. Many LB strategies have been proposed in cloud computing, but they are still not applied effectively in fog. LB is an important issue to achieve high resource utilization, avoid bottlenecks, avoid overload and low load, and reduce response time. In this paper, a LB and optimization strategy (LBOS) using dynamic resource allocation method based on Reinforcement learning and genetic algorithm is proposed. LBOS monitors the traffic in the network continuously, collects the information about each server load, handles the incoming requests, and distributes them between the available servers equally using dynamic resource allocation method. Hence, it enhances the performance even when it's the peak time. Accordingly, LBOS is simple and efficient in real-time systems in fog computing such as in the case of healthcare system. LBOS is concerned with designing an IoT-Fog based healthcare system. The proposed IoT-Fog system consists of three layers, namely: (1) IoT layer, (2) fog layer, and (3) cloud layer. Finally, the experiments are carried out and the results show that the proposed solution improves the quality-of-service in the cloud/fog computing environment in terms of the allocation cost and reduce the response time. Comparing the LBOS with the state-of-the-art algorithms, it achieved the best load balancing Level (85.71%). Hence, LBOS is an efficient way to establish the resource utilization and ensure the continuous service.																	1868-5137	1868-5145															10.1007/s12652-020-01768-8		FEB 2020											
J								Rule-based aggregation driven by similar images for visual saliency detection	APPLIED INTELLIGENCE										Saliency detection; Similar images; Rough-set-based rules; Binary saliency estimation	OBJECT DETECTION; MODEL; CNN	The visual saliency detection consists in determining the relevant visual information in a scene to segment it from the background. This paper proposes visual saliency detection in a rule-based approach using the image similarity cue to improve saliency detection performance. Our system induces rules for saliency detection, and for a given input image, determines the subset of rules to be used from a set of candidate rules. The proposed approach consists of two main stages: training and testing. Firstly, during the training stage, our system learns an ensemble of rough-set-based rules by combining knowledge extracted from outputs of four state-of-the-art saliency models. Secondly, our system determines the most suitable subset of induced rules for binary detection of pixels of a salient object in an image. The decision of the best subset of rules is based on the image similarity cue. The binary determination of saliency in the output image, exempts us from performing a post-processing stage as is needed in most saliency approaches. The proposed method is evaluated quantitatively on three challenging databases designed for the saliency detection task. The results obtained from the performed experiments indicate that the proposed method outperforms the state-of-the-art approaches used for comparison.																	0924-669X	1573-7497				JUN	2020	50	6					1745	1762		10.1007/s10489-019-01582-6		FEB 2020											
J								Exploiting complexity in pen- and touch-based signature biometrics	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION											VERIFICATION; RECOGNITION	Biometric signature verification has been traditionally performed in pen-based office-like scenarios using devices specifically designed for acquiring handwriting. However, the high deployment of devices such as smartphones and tablets has given rise to new and thriving scenarios for signature biometrics where handwriting can be performed using not only a pen stylus but also the finger via touch interaction. Some preliminary studies have highlighted the challenge of this new scenario and the necessity of further research on the topic. The main contribution of this work is to propose a new on-line signature verification architecture adapted to the signature complexity in order to tackle this new and challenging scenario. Additionally, an exhaustive comparative analysis of both pen- and touch-based scenarios using our proposed methodology is carried out along with a review of the most relevant and recent studies in the field. Significant improvements of biometric verification performance and practical insights are extracted for the application of signature verification in real scenarios.																	1433-2833	1433-2825				JUN	2020	23	2					129	141		10.1007/s10032-020-00351-3		FEB 2020											
J								A new multi-view learning machine with incomplete data	PATTERN ANALYSIS AND APPLICATIONS										Multi-view learning; Incomplete data; Low-rank assumption	LINEAR CLASSIFIER DESIGN; LABEL	Multi-view learning with incomplete views (MVL-IV) is a reliable algorithm to process incomplete datasets which consist of instances with missing views or features. In MVL-IV, it exploits the connections among multiple views and suggests that different views are generated from a shared subspace such that it can recover the missing views or features well while MVL-IV neglects two facts. One is that different views should always be generated from different subspaces. The other is that the information of view-based classifiers is useful to the design of MVL-IV. Thus, on the base of MVL-IV, we consider these two facts and develop a new multi-view learning with incomplete data (NMVL-IV). Related experiments on clustering, regression, classification, bipartite ranking, and image retrieval have validated that the proposed NMVL-IV can recover the incomplete data much better.																	1433-7541	1433-755X				AUG	2020	23	3					1085	1116		10.1007/s10044-020-00863-y		FEB 2020											
J								Densely connected network for impulse noise removal	PATTERN ANALYSIS AND APPLICATIONS										Impulse noise removal; Deep learning; CNN; Densely connected network	SPARSE REPRESENTATION; MEDIAN FILTERS; REGULARIZATION	Recently, a new convolutional neural network (CNN) architecture, dubbed as densely connected convolutional network (DenseNet), has shown excellent results on image classification tasks. The idea of DenseNet is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion, then the network will be more accurate and easier to train. In this study, we extend DenseNet to deal with the problem of impulse noise reduction. It aims to explore the densely connected network for impulse noise removal (DNINR), which utilizes CNN to learn pixel-distribution features from noisy images. Compared with the traditional median filter-based and variational regularization methods that utilize the spatial neighbor information around the pixels and optimize in an iterative manner, it is more efficient to capture multi-scale contextual information and directly tackles the original image. Additionally, DNINR turns to capture the pixel-level distribution information by means of wide and transformed network learning. In terms of edge preservation and noise suppression, the proposed DNINR consistently achieved significantly superior performance, which is better than current state-of-the-art methods, particularly at extremely high noise levels.																	1433-7541	1433-755X				AUG	2020	23	3					1263	1275		10.1007/s10044-020-00871-y		FEB 2020											
J								Developing socially inspired robotics through the application of human analogy: capabilities and social practice	AI & SOCIETY										Sociable robots; Socially inspired robotics; Trusting robots; Capability approach		Socially inspired robotics involves drawing on the observation and study of human social interactions to apply them to the design of sociable robots. As there is increasing expectation that robots may participate in social care and provide some relief for the increasing shortage of human care workers, social interaction with robots becomes of increasing importance. This paper demonstrates the potential of socially inspired robotics through the exploration of a case study of the interaction of a partially sighted social worker with a support worker. This is framed within the capability approach in which the interaction of a human and a sociable robot is understood as resulting in a collaborative capability which is grounded the relationship between the human and the robot rather than the autonomous capabilities of the robot. The implications of applying the case study as an analogy for human-robot interaction are expressed through a discussion of capabilities and social practice and policy. The study is attenuated by a discussion of the technical limits of robots and the extensive complexity of the social context in which it is envisaged sociable robots may be employed.																	0951-5666	1435-5655															10.1007/s00146-020-00948-6		FEB 2020											
J								Optimal processing of nearest-neighbor user queries in crowdsourcing based on the whale optimization algorithm	SOFT COMPUTING										Optimal query; Crowdsource; Whale optimization algorithm; KNN and range query	NETWORKS; PROTOCOL; WISDOM	Generally, human and machine-based query operations can be modified with the use of crowdsourcing. Location-based queries are classified into range and k-nearest neighbor (KNN) queries. Space and point of interest (POI) information can be obtained from both range and KNN queries. In this paper, we expose the trust stage computation of range and KNN query answers with the help of the whale optimization algorithm (WOA). The system chooses either parallel or serial processing, and the experiments are carried out using real-time crowdsourcing. The effectiveness of the proposed concept is evaluated through various consequences such as gang dimension, POI information, space information, and range and KNN query consequences. Each of these effects produces an optimal and reliable result. Finally, the computation time and communication overhead performance of serial and parallel processing are analyzed by examining consequences and production of optimal outcomes.																	1432-7643	1433-7479				SEP	2020	24	17					13037	13050		10.1007/s00500-020-04722-0		FEB 2020											
J								Switched-mode Luo converter with power factor correction and fast regulation under transient conditions	SOFT COMPUTING										Continuous conduction mode; Diode bridge rectifier; Particle swarm optimization; Power quality; Total harmonic distortion; State space average	PFC; IMPLEMENTATION	New advancements in light-emitting diode (LED) have attracted many to use LEDs in commercial and industrial applications. In the recent years, new family of DC-to-DC converters, namely super-lift Luo converters, has evolved in the power electronics terrain which have high gain than the conventional boost converter. In this paper, AC-DC converter with super-lift Luo converter as PFC topology for high-brightness light-emitting diode applications is proposed. When subjected to load and line variations, power quality parameters of PFC AC-DC converter will have a change. The indices continue to change till the output voltage gets stabilized. For fast regulation under transient conditions, particle swarm optimization tuned proportional-integral (PI) controller is used. MATLAB/Simulink environment is used to carry out extensive simulation on the work proposed. The laboratory model is constructed and tested to verify simulation results. Total harmonic distortion at the input side of an AC-DC converter is validated as per the standards of IEC 61000-3-2 of class c equipment's.																	1432-7643	1433-7479				SEP	2020	24	17					13319	13329		10.1007/s00500-020-04747-5		FEB 2020											
J								Research on the performance of multi-population genetic algorithms with different complex network structures	SOFT COMPUTING										Complex network; Network structure; Multi-population; Genetic algorithm; Flexible job shop scheduling problem	EVOLUTIONARY ALGORITHMS; OPTIMIZATION; COOPERATION; MODELS	Genetic algorithm is a frequently used evolutionary algorithm that cannot avoid premature convergence. Multi-population is usually used to overcome this disadvantage, obtaining multi-population genetic algorithm (MGA). If sub-populations and communications among them are considered as nodes and edges, respectively, an MGA can be represented as a complex network. After reviewing previous researches, we find that the network structures used to design MGAs are limited and some parameters (SPS, sub-population size, and SPN, sub-population number) under a certain total individual number (TIN) are always ignored. Using seven network structures (BAnet, BDnet, CTnet, ERnet, HAnet, LCnet, and SWnet) to design MGAs that are used to solve some flexible job shop scheduling problems, how the network structures and parameters affect the performances of MGAs is addressed. The simulation results indicate that: (i) the MGA with ERnet rather than the famous BAnet often performs well although their performances are problem-dependent; (ii) the Hamming distance index proposed here can properly capture the phenomenon that the smaller the average path length, the higher the propagation rate; and (iii) under a certain TIN, their performances first increase and then decrease gradually as SPN increases, and their performances first increase rapidly and then remain almost unchanged as SPS increases.																	1432-7643	1433-7479				SEP	2020	24	17					13441	13459		10.1007/s00500-020-04759-1		FEB 2020											
J								Comparative study of machine learning approaches for classification and prediction of selective caspase-3 antagonist for Zika virus drugs	NEURAL COMPUTING & APPLICATIONS										Caspase-3; QSAR; Machine learning; Antagonist	INHIBITORS; OUTBREAK	Zika virus (ZIKV) infection is an enervating and fast-growing disease. The increasing incidences of birth defects (microcephaly) in newborns due to ZIKV represent a public health problem. The viral infection is characterized by an increase in cell death of human neural progenitors and astrocytes, which can be inhibited by suppressing infection-induced caspase-3 activity. The aim of the present work is to develop classification models for the prediction of highly active and low active caspase-3 antagonists and to seek the important structural features related to the high anti-ZIKV property. Here, machine learning (ML) is applied in quantitative structure-activity relationship (QSAR) study. QSAR study is performed on the dataset by means of ML approaches, i.e., multiple linear regression (MLR), linear discriminant analysis (LDA), least square support vector machine (LS-SVM), deep neural net (DNN), k-nearest neighbor (KNN), naive Bayes (NB) and random forest (RF). MLR, LDA are used for feature selection process and DNN, LS-SVM, KNN, NB, RF classifier for classification. The obtained results confirmed the discriminative capacity of the calculated descriptors. A good correlation is found by regression analysis between the observed and predicted activities as evident by their R-2 (0.895) and Rpred 2 (0.716) for the molecular descriptor dataset, R-2 (0.892) and Rpred 2 (0.736) for fingerprint dataset. The classification model obtained using RF (85.71%, 97.57%) and DNN (85.71%, 91.07%) classifier gave better accuracy than other approaches in fingerprint dataset and molecular descriptor dataset, respectively. This work provides an effective method to screen caspase-3 antagonists that will help out further in drug design for Zika virus.																	0941-0643	1433-3058				AUG	2020	32	15					11311	11328		10.1007/s00521-019-04626-7		FEB 2020											
J								DC programming and DCA for parametric-margin nu-support vector machine	APPLIED INTELLIGENCE										Support vector machine; Non-convex optimization; Generalized Newton's method; DC programming; DCA	MINIMUM NORM SOLUTION; ALGORITHM; NUMBER	As a development of nu-support vector machine (nu-SVM), parametric-margin nu-support vector machine (Par-nu-SVM) can be useful in many cases, especially heteroscedastic noise classification problems. The present article proposes a novel and fast method to solve the primal problem of Par-nu-SVM (named as DC-Par-nu-SVM), while Par-nu-SVM maximizes the parametric-margin by solving a dual quadratic programming problem. In fact, the primal non-convex problem is converted into an unconstrained problem to express the objective function as the difference of convex functions (DC). The DC-Algorithm (DCA) based on generalized Newton's method is proposed to solve the unconstrained problem cited. Numerical experiments performed on several artificial, real-life, UCI and NDC data sets showed the superiority of the DC-Par-nu-SVM in terms of both accuracy and learning speed.																	0924-669X	1573-7497				JUN	2020	50	6					1763	1774		10.1007/s10489-019-01618-x		FEB 2020											
J								Theoretical study of GDM-SA-SVR algorithm on RAFM steel	ARTIFICIAL INTELLIGENCE REVIEW										Simulated annealing; Support vector regression; RAFM steel; Optimization	FEATURE-SELECTION; BEHAVIOR	With the development of society and the exhaustion of fossil energy, we need to identify new alternative energy sources. Nuclear energy is an ideal choice, but the key to the successful application of nuclear technology is determined primarily by the behavior of nuclear materials in reactors. Therefore, we studied the radiation performance of the fusion material RAFM steel. We used the GDM algorithm to upgrade the annealing stabilization process of simulated annealing algorithm. The yield stress performance of RAFM steel was successfully predicted by the hybrid model, which combined simulated annealing with the support vector machine for the first time. The prediction process was as follows: first, we used the improved annealing algorithm to optimize the SVR model after training on a training dataset. Next, we established the yield stress prediction model of RAFM steel. By testing the model and conducting sensitivity analysis on the model, we can conclude that, compared with other similar models such as the ANN, linear regression, generalized regression neural network, and random forest, the predictive attribute variables cover all of the variables in the training set. Moreover, the generalization performance of the model on the test set is superior to that of other similar prediction models. Thus, this paper introduces a new method for the study of RAFM steel.																	0269-2821	1573-7462				AUG	2020	53	6					4601	4623		10.1007/s10462-020-09803-y		FEB 2020											
J								Harmonizing two approaches to fuzzy random variables	FUZZY OPTIMIZATION AND DECISION MAKING										Borel measurability; Fuzzy random variable; Perfect distribution; Probability distribution	DECISION-PROBLEMS; METRICS; OPTION	We prove a measurability result which implies that the measurable events concerning the values of a fuzzy random variable, in two related mathematical approaches wherein the codomains of the variables are different spaces, are the same (provided both approaches apply). Further results on the perfectness of probability distributions of fuzzy random variables are presented.																	1568-4539	1573-2908				JUN	2020	19	2					177	189		10.1007/s10700-020-09317-w		FEB 2020											
J								GPU fast restoration of non-uniform illumination images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image restoration; Graphic processing unit; Non-uniform illumination; Parallel implementation		This paper presents a GPU based parallel implementation for the non-uniform illumination image restoration method, which uses a retinex based algorithm to decompose the original image into brightness and reflectance components, and adjusts the brightness value through an adaptive gamma correction and nonparametric mapping to achieve the restoration. Specifically, we parallelize the improved retinex algorithm on GPU to extract the brightness value of each pixel. After that, the probability of different brightness range is counted through each block to the entire image to reduce the competition of memory access. Finally, we use two different parallel reduce methods to calculate the probability density and cumulative density of brightness value and generate the mapping curve. The experiment conducted on three different GPUs and two CPUs with different resolution images shows that our method can process a 1024 x 2048 image in 1.024 ms on RTX2080Ti, indicates a great potential for real-time application.																	1861-8200	1861-8219															10.1007/s11554-020-00950-7		FEB 2020											
J								Automatic Semantic Segmentation with DeepLab Dilated Learning Network for Change Detection in Remote Sensing Images	NEURAL PROCESSING LETTERS										Deep neural network; Change detection; Synthetic aperture radar images; Convolutional neural network; Multi-temporal detection techniques	CLASSIFICATION; REPRESENTATION; MODEL	Automatic change detection is an interesting research area in remote sensing (RS) technology aims to detect the changes in synthetic aperture radar (SAR) and multi-temporal hyperspectral images acquired at different time intervals. This method identifies the differences between the images and accomplishes the classification result into changed and unchanged areas. However, the existing algorithms are degraded due to noises present in the RS images. The main aim of the proposed method is the automatic semantic segmentation based change detection that produces a final change between the two input images. This paper proposes a feature learning method named deep lab dilated convolutional neural network (DL-DCNN) for the detection of changes from the images. The proposed approach consists of three stages: (i) pre-processing, (ii) semantic segmentation based change detection and (iii) accuracy assessment. Initially, preprocessing is performed to correct the errors and to obtain detailed information from the scene. Then, map the changes between the two images with the help of a trained network. The DCNN network performs fine-tuning and determines the relationship between two images as changed and unchanged pixel areas. The experimental analysis conducted on various datasets and compared with several existing algorithms. The experimental analysis is performed in terms of F-score, percentage correct classification, kappa coefficient, and overall error rate measures to show a better performance measure than the other state-of-art approaches.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2355	2377		10.1007/s11063-019-10174-x		FEB 2020											
J								A criterion for automatic image deconvolution with L-0-norm regularization	JOURNAL OF CHEMOMETRICS										deconvolution; image; optimization; regularization parameter; sparsity	ILL-POSED PROBLEMS; PARAMETER SELECTION	Automatic penalty adjustment in sparse deconvolution with penalized least squares is required for improved reliability and broader applicability. In sparse deconvolution with an L-0-norm penalty, the latent signal is by nature discontinuous, and the magnitudes of the residuals and sparsity regularization terms are of different order of magnitude. This makes approaches such as generalized cross validation or L-curve unsuitable in practice. The criterion proposed in this paper is based on the representation of the sum of the normalized residuals and regularization terms (SNT) as a function of the penalty parameter. We observed that the minimum of the SNT corresponds to the optimal value of the penalty parameter. This approach was tested in the context of super-resolution fluorescence microscopy imaging. Both simulated and real live-cell images characterized by different complexities and emitter densities were analyzed to assess the performance of the developed optimization strategy and to demonstrate its usefulness over manual tuning.																	0886-9383	1099-128X				JUN	2020	34	6							e3227	10.1002/cem.3227		FEB 2020											
J								A Framework to Overcome Hesitancy of Decision-Makers in E-Government Web Site Evaluation	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										E-government; Website evaluation; Hesitant fuzzy set; Step-wise weight assessment ratio analysis; Hesitant fuzzy weighted geometric Heronian mean; Iran	FUZZY-SETS	Considering the undeniable role of websites in the success of interactions between citizens and governments, evaluating e-government web sites is a worthwhile topic. This research has developed a multi-attribute-decision-making technique rely on Step-Wise Weight Assessment Ratio Analysis and hesitant fuzzy weighted geometric Heronian mean decision-making methods, for evaluating e-government websites. Proposed framework overcome the limitations of previous investigations such as the possibility of aggregation of expert opinions using group decision-making and modeling the hesitancy of the experts (due to the incompleteness of their knowledge or their doubts in evaluating different aspects of the model) through hesitant fuzzy sets. The designed framework was used in Iran to evaluate e-government websites. In this regard, after a careful and systematic review of previous researches, a list of dimensions and evaluation criteria was presented in the form of a comprehensive model. Then, an expert-based process was proposed to localize the model according to the conditions of the case study. The results of the research show that the final rankings of the decision-making method used in the hesitant environment rarely changed for different operators. So as the experts have confirmed, the model used in this study has acceptable stability.																	1562-2479	2199-3211				MAR	2020	22	2			SI		583	603		10.1007/s40815-019-00790-z		FEB 2020											
J								Multicriteria Group Decision-Making for Supplier Selection Based on Intuitionistic Cubic Fuzzy Aggregation Operators	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Intuitionistic cubic fuzzy set; ICFWA operator; ICFOWA operator; ICFWG operator; ICFOWG operator; ICFHA operator; ICFHG operator; Multicriteria decision-making	VIKOR METHOD; METHODOLOGY	This article is an advanced approach to intuitionistic fuzzy set through application of cubic set theory. For instance, we establish the idea of the intuitionistic cubic fuzzy set (ICFS) theory and define several operations for ICFS ; also establish a series of weighted aggregation operators under intuitionistic cubic fuzzy information, so-called intuitionistic cubic fuzzy weighted averaging (ICFWA) operator, intuitionistic cubic fuzzy order weighted averaging (ICFOWA) operator, intuitionistic cubic fuzzy weighted geometric (ICFWG) operator, intuitionistic cubic fuzzy order weighted geometric (ICFOWG) operator, intuitionistic cubic fuzzy hybrid averaging (ICFHA) operator, and intuitionistic cubic fuzzy hybrid geometric (ICFHG) operator; and further study their fundamental properties and showed the relationship among these aggregation operators. In order to demonstrate the feasibility and practicality of the mentioned new technique, we develop multicriteria group decision-making algorithm under intuitionistic cubic fuzzy environment. Further, the proposed method applied to supply chain management and for implementation, consider numerical application of supply chain management. Also the selected supplier by ICFD aggregation operators is verified by VIKOR method. Comparing the proposed techniques with other pre-existing aggregation operators, we concluded that the proposed technique is better, reliable, and effective.																	1562-2479	2199-3211				APR	2020	22	3					810	823		10.1007/s40815-019-00768-x		FEB 2020											
J								PragmaticOIE: a pragmatic open information extraction for Portuguese language	KNOWLEDGE AND INFORMATION SYSTEMS										Open information extraction; Relation extraction; Inference; Context; Intention; Pragmatics		Information extraction (IE) involves the extraction of useful facts from texts. IE approaches have been categorized into two types: Traditional IE and Open IE. Traditional IE recognizes a predefined set of relationships between the arguments, and it has typically been applied to specific domains. Open IE extracts relationship descriptors expressing any semantic relationship between a pair of arguments in different domains. Although a sentence can have a different meaning, given the context and intention used, a single semantic analysis does not guarantee useful extractions. Extractions depend on the context and the intention inherited in a sentence that goes beyond the semantic meaning. Thus, a pragmatic analysis enhances the set of extractions by considering the contextual and intentional aspects. As a consequence, new facts can be extracted from this set of sentences. The combination of inference, context, and intention enables the extraction of implicit facts from texts achieving a first pragmatic level. This novel approach increases the number of facts, extracting relationships from a sentence analyzing inference, context, and intention. This is the first method to analyze a first pragmatic level from a sentence within a set of Portuguese text documents. Our method was performed over a set of Portuguese text documents and outperforms the most relevant related work comparing accuracy, number of extracted facts, and minimality measures.																	0219-1377	0219-3116				SEP	2020	62	9					3811	3836		10.1007/s10115-020-01442-7		FEB 2020											
J								Improved image representation and sparse representation for image classification	APPLIED INTELLIGENCE										Image representation; Image classification; Sparse representation	FACE-RECOGNITION; K-SVD; DICTIONARY; ROBUST	It seems that for multiple available images of the same object, the pixel values at the same image position are almost always different, which is especially obvious for the deformable object. This implies that it will be not easy to correctly classify the deformable object. In order to extract salient features of images and improve the performance of image classification, a novel image classification algorithm is proposed in this paper. The algorithm can effectively preserve the large-scale information and global features of the original image, reduce the difference in different images of the same object, and significantly improve the accuracy of image classification. Firstly, the virtual image is generated by the new image representation procedure. Secondly, the image classification algorithm is used to obtain the corresponding classification scores of the original image and the virtual image, respectively. Finally, the ultimate classification score is obtained by a simple and efficient score fusion scheme. A large number of experiments on three widely used image databases show that the proposed algorithm outperforms other state-of-the-art algorithms in classification accuracy. At the same time, the algorithm has the advantages of simple implementation and high computational efficiency.																	0924-669X	1573-7497				JUN	2020	50	6					1687	1698		10.1007/s10489-019-01612-3		FEB 2020											
J								Hierarchical linear and nonlinear adaptive learning model for system identification and prediction	APPLIED INTELLIGENCE										System identification; Hierarchical algorithm; Adaptive learning; Prediction; Parameter estimation	CONVERGENCE ANALYSIS; PARAMETER-ESTIMATION; ALGORITHM	In this paper, we propose a method to increase the model accuracy with linear and nonlinear sub-models. The linear sub-model applies the least square error (LSE) algorithm and the nonlinear sub-model uses neural networks (NN). The two sub-models are updated hierarchically using the Lyapunov function. The proposed method has two advantages: 1) The neural networks is a multi-parametric model. Using the proposed model, the weights of NN model can be summarized into the coefficients or parameters of auto-regressive eXogenous/auto-regressive moving average (ARX/ARMA) model structure, making it easier to establish control laws, 2) learning rate is updated to ensure the convergence of errors at each training epoch. One can improve the accuracy of model and the whole control system. We have demonstrated by the experimental studies that the proposed technique gives better results when compared to the existing studies.																	0924-669X	1573-7497				JUN	2020	50	6					1699	1710		10.1007/s10489-019-01615-0		FEB 2020											
J								Transferring the semantic constraints in human manipulation behaviors to robots	APPLIED INTELLIGENCE										Knowledge transfer; Semantic constraint; Object manipulation; Ontology; Collaborative reasoning; PR-OWL; Robot intelligence	KNOWLEDGE; OBJECTS	In this study, we aim to help robots manipulate objects with the guidance of semantic constraints (the grasp location, grasp type, approaching way, trajectory constraint, grasp force, and opening width) which are learnt from human manipulation behaviors. In order to transfer the complex and uncertain relationships between the attributes of object and task and semantic constraints in human behaviors to robots. We propose a representation method of human behaviors in machine-understandable semantics and a collaborative reasoning mechanism. With the suggested semantic constraints from human behaviors, the robot manipulation can be completed under "consciousness" and proper for both the object and task. The shareability of the object attributes, primitive actions and semantic constraints makes the proposed method be generalized to new objects and even new tasks. Additionally, we find that the object's parts can be grasped sequentially, according to the object's state and the action to be performed.																	0924-669X	1573-7497				JUN	2020	50	6					1711	1724		10.1007/s10489-019-01580-8		FEB 2020											
J								Customized ranking for products through online reviews: a method incorporating prospect theory with an improved VIKOR	APPLIED INTELLIGENCE										Product ranking; Online reviews; Prospect theory; Improved VIKOR; Sentiment degree	SENTIMENT ANALYSIS; DECISION-MAKING; LEXICON; CLASSIFICATION; FRAMEWORK; OPINIONS; QUALITY; FUSION; MODEL	Online product reviews are significant in modern e-business because they can influence consumers' purchase decisions. However, with the dramatic increase in the number of product categories and reviews, it is impossible for consumers to read all online reviews. In this paper, we design a novel method to help customers rank products using online reviews. Our method can be divided into three stages: generating a list of related alternative products based on specific filter conditions, collecting online reviews, and processing and measuring customer satisfaction. This study offers three significant improvements over previous approaches. First, we incorporate prospect theory that reflects the greater impact of negative reviews on customers' purchase decisions to measure customer satisfaction concerning each attribute more accurately. Second, we combine the collective attribute weights calculated by entropy weight method (EWM) and the individual attribute weights given by a customer to improve VIKOR method, which can adjust the proportion of the two types of weights according to the customer's knowledge of the product attributes. Third, for processing online reviews, we develop a new sentiment analysis algorithm that factors in the degree of consumer sentiment. This technique is different from the procedures used by existing studies for ranking products. To validate our method, we conduct a case study of automobile ranking and make some comparisons, which together demonstrate that the proposed method not only saves time and effort but also helps consumers select the products they really want.																	0924-669X	1573-7497				JUN	2020	50	6					1725	1744		10.1007/s10489-019-01577-3		FEB 2020											
J								A bio-inspired credit card fraud detection model based on user behavior analysis suitable for business management in electronic banking	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Behavior analysis; Credit card fraud detection; E-banking; Information fusion; Multi-level classification; Optimization algorithm		The widened uses of Internet credit cards in e-banking systems are currently prone to credit card fraud. Data imbalance also poses a significant difficulty in the method of fraud detection. The efficiency of the existing fraud detection systems is only in question because it detects fraudulent action after the suspect transaction has been completed. To address these difficulties, this article offers an improved two-level credit card fraud tracking model from imbalanced datasets based on the semantic fusion of k-means and the artificial bee colony (ABC) algorithm to improve identification precision and accelerate the convergence of detection. In the proposed model, ABC works as a kind of neighborhood search associated with a global search to be a second classification level to manage the failure of the k-means classifier to explore the actual clusters as it is sensitive to the initial condition. The proposed model filters the characteristics of the dataset using an integrated rule engine to evaluate whether the operation is real or false, depending on many parameters of client conduct (profile) such as geographical locations, usage frequency, and book balance. Experimental findings show that the suggested model can improve the precision of ranking against the danger of suspect operations and provide higher accuracy relative to traditional techniques.																	1868-5137	1868-5145															10.1007/s12652-020-01759-9		FEB 2020											
J								Forecasting stock market return with nonlinearity: a genetic programming approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Return forecasting; Nonlinear models; Genetic programming	TECHNICAL ANALYSIS; EQUITY PREMIUM; VOLATILITY; EFFICIENT; RISK; DYNAMICS; JUMP; PROFITABILITY; PERFORMANCE; HYPOTHESIS	The issue whether return in the stock market is predictable remains ambiguous. This paper attempts to establish new return forecasting models in order to contribute on addressing this issue. In contrast to existing literatures, we first reveal that the model forecasting accuracy can be improved through better model specification without adding any new variables. Instead of having a unified return forecasting model, we argue that stock markets in different countries shall have different forecasting models. Furthermore, we adopt an evolutionary procedure called Genetic programming (GP), to develop our new models with nonlinearity. Our newly-developed forecasting models are testified to be more accurate than traditional AR-family models. More importantly, the trading strategy we propose based on our forecasting models has been verified to be highly profitable in different types of stock markets in terms of stock index futures trading.																	1868-5137	1868-5145															10.1007/s12652-020-01762-0		FEB 2020											
J								Intelligent oriented middleware system based navigation detection time orient node location identification in mobile ad hoc network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster; Node; Location; Network; Stability; Manet	INTRUSION-DETECTION; CLUSTERING-ALGORITHM; EFFICIENT; QUALITY	Different measures have moved toward the issue of intrusion detection, and there is the number of elements being considered by various techniques. Intrusion detection is a technology to effectively avoid several risks of network intrusion. The intrusion detection issue has not been set out to accomplish higher execution. The algorithm is widely used in intrusion detection. But, the algorithm has some shortcomings, such as the random selection of k value, sensitive selection of initial cluster centers, and low accuracy in clustering high-dimensional data. To tackle this issue, a proficient approach is examined in this paper. Choosing the Cluster Head (CH) assumes the essential part in arrange performance and lifetime. However, considering just the energy parameter would not create significant outcomes where the states of every location of the system is known. By analyzing all these, the proposed Time Orient based Node Location Identification (TONLI) techniques handle the cluster head determination as indicated by numerous key measures. The strategy utilizes the energy parameter as well as considers node location-based conditions. For the choice of cluster head, the number of nodes in every location, the transmission recurrence, the CH reachable, the CH throughput and energy parameters are considered. Utilizing the variables as specified over, the strategy processes the node stability which expresses to the lifetime of nodes. Given processed node stability esteem a single node has been chosen for every location. Given node stability, the technique recognizes the intrusion efficiently and produces substantial outcomes.																	1868-5137	1868-5145															10.1007/s12652-020-01720-w		FEB 2020											
J								Lightweight job submission and file sharing schemes for a teaching ecosystem for parallel computing courses	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Teaching ecosystem; Parallel computing course; Job submission; File sharing; Lightweight	ACCESS-CONTROL; POLICIES; SYSTEM	Parallel computing has been widely used in various industries. Many colleges and universities have launched courses on parallel computing and can build teaching ecosystems to serve these courses. Job submission and file sharing are two essential functionalities. This paper builds a teaching ecosystem for parallel computing courses and proposes lightweight job submission and file sharing schemes for the ecosystem. The job submission scheme supports Pthread, OpenMP, and MPI (message passing interface) jobs. It has five working modes: demonstration, experiment, examination, research, and maintenance. In particular, the examination mode can be applied for online examinations. For the fair use of resources, the job submission scheme not only limits the submission times of each student and the run time of each job but also designs a fair job scheduling algorithm. The ecosystem adopts Samba to supply file sharing services. To ensure the safety of the shared files, this paper proposes a lightweight attribute-based access control mechanism. This mechanism uses Python logical expressions to describe access rules; thus, it is not only easy to use but also able to describe complicated rules. In addition, the ecosystem has a well-structured module design that facilitates functionality expansion and modification. Experiments verify the feasibility and validity of the ecosystem.																	1868-5137	1868-5145															10.1007/s12652-020-01695-8		FEB 2020											
J								Bottom-Up Scene Text Detection with Markov Clustering Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Neural networks; Markov clustering; Self-attention; Text detection and segmentation	LOCALIZATION; RECOGNITION	A novel detection framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. Different from the traditional top-down scene text detection approaches that inherit from the classic object detection, MCN detects scene text objects in a bottom-up manner. MCN predicts instance-level bounding boxes by firstly converting an image into a stochastic flow graph where Markov Clustering is performed based on the predicted stochastic flows. The stochastic flows encode the local correlation and semantic information of scene text objects. An object is modeled as strongly connected nodes by flows, which allows flexible and bottom-up detection for scale-varying and rotated text objects without prior knowledge of object size. The flow prediction is supported by the advanced Convolutional Neural Networks architectures and Position-aware spatial attention mechanism, which provides enhanced flow prediction by adaptively fusing spatial representations. The experimental evaluation on public benchmarks shows that our MCN method achieves the state-of-art performance on public benchmarks, especially in retrieving long and oriented texts.																	0920-5691	1573-1405				JUN	2020	128	6					1786	1809		10.1007/s11263-020-01298-y		FEB 2020											
J								Generic extended multigranular sets for mixed and incomplete information systems	SOFT COMPUTING										Rough sets; Multigranularity; Incomplete information systems; Mixed information systems	ROUGH SET; GRANULATION	Granular computing is a widely used computational paradigm nowadays. Particularly, within the rough set theory, granular computing plays a key role. In this paper, we propose a generic approach of rough sets, the granular extended multigranular sets (GEMS) for dealing with both mixed and incomplete information systems. Not only our proposal does use the traditional optimistic and pessimistic granulations with respect to single attributes, but also we introduce granulations with respect to attribute sets, as well as two new ways of granulating: the optimistic + pessimistic granulation and the pessimistic + optimistic granulation. In addition, we have developed a particular case of the proposed GEMS: the multigranular maximum similarity rough sets (MMSRS). We have proved some of the properties of the MMSRS, and we tested its effectiveness with respect to other existing granular rough sets models. The experimental results show the flexibility and the capabilities of the proposed model, while handling mixed and incomplete information systems.																	1432-7643	1433-7479				APR	2020	24	8			SI		6119	6137		10.1007/s00500-020-04748-4		FEB 2020											
J								An enhanced heuristic ant colony optimization for mobile robot path planning	SOFT COMPUTING										Ant colony optimization; Enhanced heuristic; Pheromone diffusion; Path planning	NAVIGATION; ALGORITHM; ACO	To realize a fast and efficient path planning for mobile robot in complex environment, an enhanced heuristic ant colony optimization (EH-ACO) algorithm is proposed. Four strategies are introduced to accelerate the ACO algorithm and optimize the final path. Firstly, the heuristic distance in the local visibility formula is improved by considering the heuristic distance from ant's neighbor points to target. Secondly, a new pheromone diffusion gradient formula is designed, which emphasizes that pheromones left the path would spread into a region and the pheromone density would present a gradient distribution in the region. Thirdly, backtracking strategy is introduced to enable ants to find new path when their search is blocked. Finally, path merging strategy is designed to further obtain an optimal path. Simulations are carried out to verify each individual strategy, and comparisons are made with the state-of-the-art algorithms. The results show our proposed EH-ACO algorithm outperforms other algorithms in both optimality and efficiency, especially when the map is large and complex.																	1432-7643	1433-7479				APR	2020	24	8			SI		6139	6150		10.1007/s00500-020-04749-3		FEB 2020											
J								A novel fractional-order neural network for model reduction of large-scale systems with fractional-order nonlinear structure	SOFT COMPUTING										Fractional order; Large-scale system; Neural network; Model reduction	SYNCHRONIZATION; SIMULATION	In this paper, a new method for reducing the order of nonlinear large-scale fractional-order systems is presented. The considered system has a nonlinear large-scale dynamic. The proposed method is developed by introducing a new fractional-based approach for neural network learning. According to the fractional-order modeling of the system, the structure of the neural network is selected as a recurrent neural network and new design and analysis are done on this network. In order to show the proposed method for model reduction has an acceptable error, a novel fractional-order stability analysis is used to derive the neural network weighting function. Moreover, it can be concluded that the proposed reducing method can preserve the main properties of the original system like a system's stability. Simulation examples are provided to show the effectiveness of the proposed method. Finally, the proposed method is compared with the existing methods and advantages of the proposed method are shown.																	1432-7643	1433-7479				SEP	2020	24	17					13489	13499		10.1007/s00500-020-04763-5		FEB 2020											
J								A scalarization method for fuzzy set optimization problems	FUZZY OPTIMIZATION AND DECISION MAKING										Fuzzy set optimization; Scalarization; Fuzzy max order; Order preserving property; Set optimization	DUALITY	In the present paper, we consider fuzzy optimization problems which involve fuzzy sets only in the objective mappings, and give two concepts of optimal solutions which are non-dominated solutions and weak non-dominated solutions based on orderings of fuzzy sets. First, by using level sets of fuzzy sets, the fuzzy optimization problems treated in this paper are reduced to set optimization problems, and relationships between (weak) non-dominated solutions of the fuzzy optimization problems and the reduced set optimization problems are derived. Next, the set optimization problems are reduced to scalar optimization problems which can be regarded as scalarization of the fuzzy optimization problems. Then, relationships between non-dominated solutions of the fuzzy optimization problems and optimal solutions of the reduced scalar optimization problems are derived.																	1568-4539	1573-2908				JUN	2020	19	2					135	152		10.1007/s10700-020-09313-0		FEB 2020											
J								An actor-critic reinforcement learning-based resource management in mobile edge computing systems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Reinforcement learning; Actor-critic algorithm; Eligibility traces; Mobile edge computing; Resource allocation	WIRELESS CELLULAR NETWORKS; COMPUTATIONAL RESOURCES; JOINT OPTIMIZATION; RADIO; ALLOCATION	Reinforcement learning (RL) as an effective tool has attracted great attention in wireless communication field nowadays. In this paper, we investigate the offloading decision and resource allocation problem in mobile edge computing (MEC) systems based on RL methods. Different from existing literature, our research focuses on improving mobile operators' revenue by maximizing the amount of the offloaded tasks while decreasing the energy expenditure and time-delays. Considering the dynamic characteristics of wireless environment, the above problem is modeled as a Markov decision process (MDP). Since the action space of the MDP is multidimensional continuous variables mixed with discrete variables, traditional RL algorithms are powerless. Therefore, an actor-critic (AC) with eligibility traces algorithm is proposed to resolve the problem. The actor part introduces the parameterized normal distribution to generate the probabilities of continuous stochastic actions, and the critic part employs a linear approximator to estimate the value of states, based on which the actor part updates policy parameters in the direction of performance improvement. Furthermore, an advantage function is designed to reduce the variance of the learning process. Simulation results indicate that the proposed algorithm can find the best strategy to maximize the amount of the tasks executed by the MEC server while decreasing the energy consumption and time-delays.																	1868-8071	1868-808X				AUG	2020	11	8					1875	1889		10.1007/s13042-020-01077-8		FEB 2020											
J								A Four-Valued Dynamic Epistemic Logic	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Many-valued logics; Epistemic logic; Paraconsistent logics; Public announcements; Multi-agent systems; Evidence	MODAL-LOGICS; SEMANTICS; KNOWLEDGE; TABLEAUX; BELIEF	Epistemic logic is usually employed to model two aspects of a situation: the factual and the epistemic aspects. Truth, however, is not always attainable, and in many cases we are forced to reason only with whatever information is available to us. In this paper, we will explore a four-valued epistemic logic designed to deal with these situations, where agents have only knowledge about the available information (or evidence), which can be incomplete or conflicting, but not explicitly about facts. This layer of available information or evidence, which is the object of the agents' knowledge, can be seen as a database. By adopting this sceptical posture in our semantics, we prepare the ground for logics where the notion of knowledge-or more appropriately, belief-is entirely based on evidence. The technical results include a set of reduction axioms for public announcements, correspondence proofs, and a complete tableau system. In summary, our contributions are twofold: on the one hand we present an intuition and possible application for many-valued modal logics, and on the other hand we develop a logic that models the dynamics of evidence in a simple and intuitively clear fashion.																	0925-8531	1572-9583				DEC	2020	29	4					451	489		10.1007/s10849-020-09313-8		FEB 2020											
J								Novel clustering-based pruning algorithms	PATTERN ANALYSIS AND APPLICATIONS										Ensemble pruning; Classifier ensemble; Clustering; Multistage organization	ENSEMBLES; DIVERSITY	One of the crucial problems of designing a classifier ensemble is the proper choice of the base classifier line-up. Basically, such an ensemble is formed on the basis of individual classifiers, which are trained in such a way to ensure their high diversity or they are chosen on the basis of pruning which reduces the number of predictive models in order to improve efficiency and predictive performance of the ensemble. This work is focusing on clustering-based ensemble pruning, which looks for the group of similar classifiers which are replaced by their representatives. We propose a novel pruning criterion based on well-known diversity measures and describe three algorithms using classifier clustering. The first method selects the model with the best predictive performance from each cluster to form the final ensemble, the second one employs the multistage organization, where instead of removing the classifiers from the ensemble each classifier cluster makes the decision independently, while the third proposition combines multistage organization and sampling with replacement. The proposed approaches were evaluated using 30 datasets with different characteristics. Experimentation results validated through statistical tests confirmed the usefulness of the proposed approaches.																	1433-7541	1433-755X				AUG	2020	23	3					1049	1058		10.1007/s10044-020-00867-8		FEB 2020											
J								Intrinsic RGB and multispectral images recovery by independent quadratic programming	PEERJ COMPUTER SCIENCE										Intrinsic images decomposition; Dichromatic model; Color and multispectral image processing; Quadratic Programming	SEPARATING REFLECTION COMPONENTS; DICHROMATIC MODEL; REMOVAL	This work introduces a method to estimate reflectance, shading, and specularity from a single image. Reflectance, shading, and specularity are intrinsic images derived from the dichromatic model. Estimation of these intrinsic images has many applications in computer vision such as shape recovery, specularity removal, segmentation, or classification. The proposed method allows for recovering the dichromatic model parameters thanks to two independent quadratic programming steps. Compared to the state of the art in this domain, our approach has the advantage to address a complex inverse problem into two parallelizable optimization steps that are easy to solve and do not require learning. The proposed method is an extension of a previous algorithm that is rewritten to be numerically more stable, has better quantitative and qualitative results, and applies to multispectral images. The proposed method is assessed qualitatively and quantitatively on standard RGB and multispectral datasets.																	2376-5992					FEB 10	2020									e256	10.7717/peerj-cs.256													
J								GSA-LA: gravitational search algorithm based on learning automata	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Learning automata; gravitational search algorithm; unconstrained optimisation	ECONOMIC OPTIMIZATION; GENETIC ALGORITHM; SYSTEMS; DESIGN	Regardless of the performance of gravitational search algorithm (GSA), it is nearly incapable of avoiding local optima in high-dimension problems. To improve the accuracy of GSA, it is necessary to fine tune its parameters. This study introduces a gravitational search algorithm based on learning automata (GSA-LA) for optimisation of continuous problems. Gravitational constant G(t) is a significant parameter that is used to adjust the accuracy of the search. In this work, learning capability is utilised to select G(t) based on spontaneous reactions. To measure the performance of the introduced algorithm, numerical analysis is conducted on several well-designed test functions, and the results are compared with the original GSA and other evolutionary-based algorithms. Simulation results demonstrate that the learning automata-based gravitational search algorithm is more efficient in finding optimum solutions and outperforms the existing algorithms.																	0952-813X	1362-3079															10.1080/0952813X.2020.1725650		FEB 2020											
J								A Methodology Combining Cosine Similarity with Classifier for Text Classification	APPLIED ARTIFICIAL INTELLIGENCE											FEATURE-SELECTION	Text Classification has received significant attention in recent years because of the proliferation of digital documents and is widely used in various applications such as filtering and recommendation. Consequently, many approaches, including those based on statistical theory, machine learning, and classifier performance improvement, have been proposed for improving text classification performance. Among these approaches, centroid-based classifier, multinomial naive bayesian (MNB), support vector machines (SVM), convolutional neural network (CNN) are commonly used. In this paper, we introduce a cosine similarity-based methodology for improving performance. The methodology combines cosine similarity (between a test document and fixed categories) with conventional classifiers such as MNB, SVM, and CNN to improve the accuracy of the classifiers, and then we call the conventional classifiers with cosine similarity as enhanced classifiers. We applied the enhanced classifiers to famous datasets - 20NG, R8, R52, Cade12, and WebKB - and evaluated the performance of the enhanced classifiers in terms of the confusion matrix's accuracy; we obtained outstanding results in that the enhanced classifiers show significant increases in accuracy. Moreover, through experiments, we identified which of two considered knowledge representation techniques (word count and term frequency-inverse document frequency (TFIDF)) is more suitable in terms of classifier performance.																	0883-9514	1087-6545				APR 15	2020	34	5					396	411		10.1080/08839514.2020.1723868		FEB 2020											
J								Dynamic decision support framework for production scheduling using a combined genetic algorithm and multiagent model	EXPERT SYSTEMS										dynamic decision support; genetic algorithm; multiagent system; prefabricated concrete components; production scheduling	JOB-SHOP; COOPERATIVE APPROACH; MINIMIZING MAKESPAN; SUPPLY CHAIN; PRECAST; SYSTEM; AGENT; OPTIMIZATION; NETWORK	Due to the dynamic nature, complexity, and interactivity of production scheduling in an actual business environment, suitable combined and hybrid methods are necessary. This paper takes prefabricated concrete components as an example and develops the dynamic decision support framework based on a genetic algorithm and multiagent system (MAS) to optimize and simulate the production scheduling. First, a multiobjective genetic algorithm is integrated into the MAS for preliminary optimization and a series of near-optimal solutions are obtained. Subsequently, considering the resource constraints and uncertainties, the MAS is used to simulate complex real-world production environments. Considering the different types of uncertainty factors, the paper proposes the corresponding dynamic scheduling method and uses MAS to generate the optimal production schedule. Finally, a practical prefabricated construction case is used to validate the proposed model. The results show that the model can effectively address the occurrence of uncertain events and can provide dynamic decision support for production scheduling.																	0266-4720	1468-0394														e12533	10.1111/exsy.12533		FEB 2020											
J								Detection of Thin Boundaries between Different Types of Anomalies in Outlier Detection Using Enhanced Neural Networks	APPLIED ARTIFICIAL INTELLIGENCE											NOVELTY DETECTION; CLASSIFICATION	Outlier detection has received special attention in various fields, mainly for those dealing with machine learning and artificial intelligence. As strong outliers, anomalies are divided into point, contextual and collective outliers. The most important challenges in outlier detection include the thin boundary between the remote points and natural area, the tendency of new data and noise to mimic the real data, unlabeled datasets and different definitions for outliers in different applications. Considering the stated challenges, we defined new types of anomalies called Collective Normal Anomaly and Collective Point Anomaly in order to improve a much better detection of the thin boundary between different types of anomalies. Basic domain-independent methods are introduced to detect these defined anomalies in both unsupervised and supervised datasets. The Multi-Layer Perceptron Neural Network is enhanced using the Genetic Algorithm to detect new defined anomalies with a higher precision so as to ensure a test error less than that be calculated for the conventional Multi-Layer Perceptron Neural Network. Experimental results on benchmark datasets indicated reduced error of anomaly detection process in comparison to baselines.																	0883-9514	1087-6545				APR 15	2020	34	5					345	377		10.1080/08839514.2020.1722933		FEB 2020											
J								Religion, Robots and Rectitude: Communicative Affordances for Spiritual Knowledge and Community	APPLIED ARTIFICIAL INTELLIGENCE											BUDDHIST; TECHNOLOGIES; ORGANIZATION; PERSPECTIVES; CULTURE; MEDIA	In light of growing concerns on AI growth and gloomy projections of attendant risks to human well-being and expertise, recent development of robotics designed to fulfill spiritual goals can help provide an alternative, possibly uplifting vision of global futures. To further understanding of the potential of robots as embodied communicators for virtuous knowledge and community, this paper discusses the affordances or possibilities of action of robots for spiritual communication by drawing upon the recent highly publicized case of Xian'Er the robot monk (XE). By discussing XE's communicative affordances including its searchability, multimediality, liveliness and extendibility, findings illustrate how robots can facilitate religious education, augment priestly authority and cultivate spiritual community. Contrary to abstract and dystopic visions of AI, findings here temper extreme pronouncements of societal disorder and points to prospects for pious and positive interplays between AI technology and society while also identifying various limitations for spiritual communication. In doing so, this paper unpacks the profound relations between religion, robots and rectitude, contributing interdisciplinary insights into an understudied area of AI development as faith leaders and adherents interact with new technological features and applications in their desire for transcendence.																	0883-9514	1087-6545				APR 15	2020	34	5					412	431		10.1080/08839514.2020.1723869		FEB 2020											
J								In defense of the Turing test	AI & SOCIETY										Turing test; Winograd schema; Practical certainty; Collaborative conversation		In 2014, widespread reports in the popular media that a chatbot named Eugene Goostman had passed the Turing test became further grist for those who argue that the diversionary tactics of chatbots like Goostman and others, such as those who participate in the Loebner competition, are enabled by the open-ended dialog of the Turing test. Some claim a new kind of test of machine intelligence is needed, and one community has advanced the Winograd schema competition to address this gap. We argue to the contrary that implicit in the Turing test is the cooperative challenge of using language to build a practical working understanding, necessitating a human interrogator to monitor and direct the conversation. We give examples which show that, because ambiguity in language is ubiquitous, open-ended conversation is not a flaw but rather the core challenge of the Turing test. We outline a statistical notion of practical working understanding that permits a reasonable amount of ambiguity, but nevertheless requires that ambiguity be resolved sufficiently for the agents to make progress.																	0951-5666	1435-5655															10.1007/s00146-020-00946-8		FEB 2020											
J								Real-time personalization and recommendation in Adaptive Learning Management System	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										E-learning; Adaptive learning; Real time content; Personalised learning; Skill test; Navies Bayes; Behaviour test	ATTENTION	Various e-learning environments have been developed to provide sufficient materials for learners and thereby guide them to gain knowledge in any domain. Even though there were several factors that determine the motivation of the student, the skill set possessed by them is definitely an important factor. Hence in our proposed work, the behavioural and educational skill of the learners is tested by a skill test and learning content is provided only based on their skill test evaluation reports. The entire process is done by real time personalization based Adaptive Learning Management System and Personalized Page Rank algorithm. Finally, the Navies' Bayes classifier was employed to classify the learners based on the performance over the skill test. Learners on their own can express both the optimistic and adverse skills that considerably impact the adaptive learning in e-learning environment. High skilled categorized learners are offered with advanced, intermediate learners with moderate and beginner or slow learners are provided with basic level of study content. The results are analysed based on their skills, individual's method of learning and time constraints.																	1868-5137	1868-5145															10.1007/s12652-020-01729-1		FEB 2020											
J								A differential privacy based probabilistic mechanism for mobility datasets releasing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobility datasets; Differential privacy; Count min sketch	LOCATION PRIVACY; SYSTEM	With the rapid popularization and development of the global positioning systems, location-based services (LBSs) are springing up to provide mobile internet users with door-to-door services. The users' privacy becomes one of the main concerns of such services, as location data reflects various sensitive information, such as home address, employment and even health conditions. Releasing the aggregated mobility datasets, i.e., the population of mobile users at different regions in the area, is one of the solutions in solving the privacy concerns that covers the individual users' information and accepted as a valid privacy preserving method in releasing mobility datasets. However, in a recent research, by exploiting the uniqueness and regularity of mobility data, individual trajectories can be recovered from the aggregated mobility datasets with accuracy about 73-91%. In this paper, we propose a novel differential privacy based probabilistic mechanism for mobility datasets releasing (DP-Mobi), in which the privacy preserved population distributions are generated and released to support LBSs. We employ a probabilistic structure count min sketch in the mechanism to count the number of users at different regions, and add noise drawn from Laplace distribution to perturb the sketches. Meanwhile, we prove the perturbed sketches satisfy differential privacy, so that the users are able to control the privacy level by tuning the parameters of Laplace distribution. Through evaluation, we show that comparing with another privacy preserving approach in resisting the attack model, our mechanism DP-Mobi achieves 8% more recovery error with the same utility loss.																	1868-5137	1868-5145															10.1007/s12652-020-01746-0		FEB 2020											
J								Verified Analysis of Random Binary Tree Structures	JOURNAL OF AUTOMATED REASONING										Binary search trees; Randomised data structures; Randomised algorithms; Quicksort; Isabelle; Interactive theorem proving		This work is a case study of the formal verification and complexity analysis of some famous probabilistic algorithms and data structures in the proof assistant Isabelle/HOL. In particular, we consider the expected number of comparisons in randomised quicksort, the relationship between randomised quicksort and average-case deterministic quicksort, the expected shape of an unbalanced random Binary Search Tree, the randomised binary search trees described by Martinez and Roura, and the expected shape of a randomised treap. The last three have, to our knowledge, not been analysed using a theorem prover before and the last one is of particular interest because it involves continuous distributions.																	0168-7433	1573-0670				JUN	2020	64	5			SI		879	910		10.1007/s10817-020-09545-0		FEB 2020											
J								A rough set model based on fuzzifying neighborhood systems	SOFT COMPUTING										Rough set; Fuzzy set; Neighborhood system; Approximation operator; Axiomatic approach; Reduction	APPROXIMATION OPERATORS; AXIOMATIC CHARACTERIZATIONS; REDUCTION	The notion of neighborhood systems is abstracted from the geometric notion of "near," and it is primitive in the theory of topological spaces. Now, the notion of neighborhood systems has been extensively applied in the study of rough set. The notion of fuzzifying neighborhood systems is a fuzzification of the notion of neighborhood systems, and it is initially in the theory of fuzzifying topological spaces. Said briefly, each element x and each subset A of a universe are associated with a number in the unit interval, interpreted as the degree of A being a neighborhood of x. In this paper, a model of rough sets derived from fuzzifying neighborhood systems is developed. It is shown that this model unifies many well-known rough sets such as binary relation-based rough sets, covering-based rough sets and neighborhood system-based rough sets into one framework. The new rough sets are studied from two approaches: the constructive and axiomatic approaches. Furthermore, when the fuzzifying neighborhood system is serial, reflexive, unary, transitive, symmetric and Euclidean, then the corresponding rough sets are discussed and characterized, respectively. At last, the reduction theory of this rough set model is established.																	1432-7643	1433-7479				APR	2020	24	8			SI		6085	6099		10.1007/s00500-020-04744-8		FEB 2020											
J								A hybrid control approach for the cracking outlet temperature system of ethylene cracking furnace	SOFT COMPUTING										Cracking outlet temperature; Advanced control; Non-minimal state space; Predictive control; Ethylene cracking furnace	PREDICTIVE FUNCTIONAL CONTROL; STATE-VARIABLE FEEDBACK; CONTROL STRATEGY; MODEL; DESIGN; ALGORITHM; MPC	The main objective of this paper is to show the design and application on the cracking outlet temperature (COT) system using a hybrid control approach including non-minimal state-space model predictive control with an adjustable factor (AFNMSSMPC) and humanoid intelligent multimodality control (HIMMC) in two 100 k tone/year SC-1 ethylene cracking furnaces. Compared with the conventional control structure, the advanced control structure is designed based on the existing control problem. The advanced controller is developed at the platform of APC-ISYS software made by Zhejiang Supcon Company and implemented in an industrial upper computer which sets over and is connected with a CS3000 distributed control system (DCS) through OPCServer. In order to reduce the fluctuation of the COT, AFNMSSMPC is chosen as feedback controller, which guarantees system robustness together with tracking ability, and meanwhile HIMMC is as feed-forward controller based on the change in chamber temperature (measured disturbance), which can eliminate the effect on cracking outlet temperature (COT) in advance. The application results on COT of two ethylene cracking furnaces labeled M and N furnaces in practice using AFNMSSMPC-HIMMC compared to the traditional proportional-integral-differential (PID) control are presented. The standard deviation of COT reduces by 54.03% and 60.87% for M and N furnaces, respectively. The results show that the designed COT system has the capability of good tracking and strong disturbance rejection.																	1432-7643	1433-7479				AUG	2020	24	16					12375	12390		10.1007/s00500-020-04679-0		FEB 2020											
J								Risk Factors Analysis and Classification on Heart Disease	SOFT COMPUTING										Canonical correlation analysis; Heart disease; Functional classification; Risk factors analysis; Group factors	CARDIOVASCULAR-DISEASE; FAILURE	In recent years, there has been a high prevalence rate of heart disease (HD) among 50-year-old people in China. It has become the first disease of old ages death. It is a very interesting and challenging work to have an effective early forecasting of the risk of HD according to the patients data. In this paper, we propose a novel method to analyze the factors with views of group features. Normalized mutual information based on entropies and information gain ratio are employed to select factors. Discriminant minimum class locality preserving canonical correlation analysis is presented to determine the effectiveness of the view of group factors. Moreover, a novel model is given to forecast the risks of New York Heart Association Functional Classification. To verify the effectiveness of the proposed method and model, we collected electronic health records of 1271 patients from 28 Chinese Level III-A hospitals in 2015. After the risk factors analysis, several results are concluded: (1) Patients with HD usually suffer from similar complications. For example, most patients with heart disease suffer from hypertension, diabetes and arrhythmia at the same time. (2) The risk forecasting has an accurate recognition rate. The risk value of the level of patients is impacted on the complications. (3) Hypertension, arrhythmia, chronic cardiac insufficiency and coronary disease are the highest concurrent diseases. There is a high reliability to have a decision of levels on the cardiac functional diseases according to the output of our proposed model.																	1432-7643	1433-7479				SEP	2020	24	17					13167	13178		10.1007/s00500-020-04731-z		FEB 2020											
J								A comprehensive survey on model compression and acceleration	ARTIFICIAL INTELLIGENCE REVIEW										Model compression and acceleration; Machine learning; Deep learning; CNN; RNN; Resource-constrained devices; Efficient neural networks	NEURAL-NETWORK; PROXIMAL NEWTON; QUANTIZATION; CLASSIFICATION; ALGORITHM	In recent years, machine learning (ML) and deep learning (DL) have shown remarkable improvement in computer vision, natural language processing, stock prediction, forecasting, and audio processing to name a few. The size of the trained DL model is large for these complex tasks, which makes it difficult to deploy on resource-constrained devices. For instance, size of the pre-trained VGG16 model trained on the ImageNet dataset is more than 500 MB. Resource-constrained devices such as mobile phones and internet of things devices have limited memory and less computation power. For real-time applications, the trained models should be deployed on resource-constrained devices. Popular convolutional neural network models have millions of parameters that leads to increase in the size of the trained model. Hence, it becomes essential to compress and accelerate these models before deploying on resource-constrained devices while making the least compromise with the model accuracy. It is a challenging task to retain the same accuracy after compressing the model. To address this challenge, in the last couple of years many researchers have suggested different techniques for model compression and acceleration. In this paper, we have presented a survey of various techniques suggested for compressing and accelerating the ML and DL models. We have also discussed the challenges of the existing techniques and have provided future research directions in the field.																	0269-2821	1573-7462				OCT	2020	53	7					5113	5155		10.1007/s10462-020-09816-7		FEB 2020											
J								Improved and scalable online learning of spatial concepts and language models with mapping	AUTONOMOUS ROBOTS										Online learning; Place categorization; Scalability; Semantic mapping; Lexical acquisition; Unsupervised Bayesian probabilistic model	ACQUISITION	We propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with high accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised Bayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our original algorithm had limited estimation accuracy owing to the influence of the early stages of learning, and increased computational complexity with added training data. Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that, in terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning. In addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes constant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language interactions between humans and robots.																	0929-5593	1573-7527				JUL	2020	44	6					927	946		10.1007/s10514-020-09905-0		FEB 2020											
J								Fast multi-language LSTM-based online handwriting recognition	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION											NEURAL-NETWORKS; TEXT	We describe an online handwriting system that is able to support 102 languages using a deep neural network architecture. This new system has completely replaced our previous segment-and-decode-based system and reduced the error rate by 20-40% relative for most languages. Further, we report new state-of-the-art results on IAM-OnDB for both the open and closed dataset setting. The system combines methods from sequence recognition with a new input encoding using Bezier curves. This leads to up to 10x faster recognition times compared to our previous system. Through a series of experiments, we determine the optimal configuration of our models and report the results of our setup on a number of additional public datasets.																	1433-2833	1433-2825				JUN	2020	23	2					89	102		10.1007/s10032-020-00350-4		FEB 2020											
J								The revelation of superintelligence	AI & SOCIETY										Superintelligence; Existential threat; Future studies; Thought experiment; revelation; Embodied mind; Ethics		The idea of superintelligence is a source of mainly philosophical and ethical considerations. Those considerations are rooted in the idea that an entity which is more intelligent than humans, may evolve in some point in the future. For obvious reasons, the superintelligence is considered as a kind of existential threat for humanity. In this essay, we discuss two ideas. One of them is the putative nature of future superintelligence which does not necessary need to be harmful for humanity. Our key idea states that the superintelligence does not need to assess its own survival as the highest value. As a kind of intelligence that is not biological, it is not clear what kind of attitude the superintelligent entity may evolve towards living organisms. Our second idea refers to the possible revelation of superintelligence. We assume that the self-revelation of such entity cannot be random. The metaphor of God as a superintelligence is introduced here as a helpful conceptual tool.																	0951-5666	1435-5655				SEP	2020	35	3					755	758		10.1007/s00146-020-00947-7		FEB 2020											
J								On weak consistency of interval additive reciprocal matrices	FUZZY OPTIMIZATION AND DECISION MAKING										Decision analysis; Analytic hierarchy process (AHP); Weak consistency; Axiomatic property; Interval additive reciprocal matrix	DECISION-MAKING; PREFERENCE RELATIONS	When one estimates the importance of alternatives under rational choice, it is natural to avoid self-contradiction from the viewpoint of psychology. Due to the vagueness encountered in a manner analogous to human thought, decision makers always exhibit limited rationality. The judgements could be expressed as interval-valued comparison matrices within the framework of analytic hierarchy process. In this study, for additive reciprocal matrices (ARMs), three axiomatic properties are proposed to characterize the additive consistency and the multiplicative consistency under fully rational behavior. For interval additive reciprocal matrices (IARMs), the concept of weak consistency is used to capture the limited rationality. By weakening some axiomatic properties of consistent ARMs, the reasonable properties of IARMs with weak consistency are presented. Two kinds of IARMs satisfying the properties of weak consistency are analyzed and some comparisons are offered. It is observed that the consistency of ARMs can be defined exactly and characterized by using the axiomatic properties. The properties of characterizing the consistency degree of IARMs should be captured by weakening the axiomatic ones of consistent ARMs. The proposed approach visualizes the development process starting from cardinal consistency of numeric-valued preference relations to weak consistency of interval-valued comparison matrices.																	1568-4539	1573-2908				JUN	2020	19	2					153	175		10.1007/s10700-020-09314-z		FEB 2020											
J								Derivative-Based Learning of Interval Type-2 Intuitionistic Fuzzy Logic Systems for Noisy Regression Problems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Interval type-2 intuitionistic fuzzy set; Decoupled extended Kalman filter; Intuitionistic fuzzy index; Gradient descent; Intuitionistic fuzzy set	PREDICTION	This study presents a comparative evaluation of interval type-2 intuitionistic fuzzy logic system using three derivative-based learning algorithms on noisy regression problems. The motivation for this study is to manage uncertainty in noisy regression problems for the first time using both membership and non-membership functions that are fuzzy. The proposed models are able to handle 'neither this nor that state' in the noisy regression data with the aim of enabling hesitation and handling more uncertainty in the data. The gradient descent-backpropagation (first-order derivative), decoupled extended Kalman filter (second-order derivative) and hybrid approach (where the decoupled extended Kalman filter is used to learn the consequent parameters and gradient descent is used to optimise the antecedent parameters) are applied for the adaptation of the model parameters. The experiments are conducted using two artificially generated and one real-world datasets, namely Mackey-Glass time series, Lorenz time series and US stock datasets. Experimental analyses show that the extended Kalman filter-based learning approaches of interval type-2 intuitionistic fuzzy logic exhibit superior prediction accuracies to gradient descent approach especially at high noise level. The decoupled extended Kalman filter model however converges faster but incurs more computational overhead in terms of the running time.																	1562-2479	2199-3211				APR	2020	22	3					1007	1019		10.1007/s40815-020-00806-z		FEB 2020											
J								Rotation invariant descriptors for galaxy morphological classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Rotation invariant; Moment; Galaxy morphologies; Classification; Image processing; Pattern recognition	PATTERN-RECOGNITION; NEURAL-NETWORKS; MACHINE; FEATURES	The detection of multi-oriented objects is a difficult pattern recognition problem. In this paper, we propose to evaluate the performance of different families of descriptors for the classification of galaxy morphologies. We investigate the performance of the Hu moments, Flusser moments, Zernike moments, Fourier-Mellin moments, and ring projection techniques based on 1D moment and the Fourier transform. We consider two main datasets for the performance evaluation. The first dataset is an artificial dataset based on representative templates from 11 types of galaxies, which are evaluated with different transformations (noise, smoothing), alone or combined. The evaluation is based on image retrieval performance to estimate the robustness of the rotation invariant descriptors with this type of images. The second dataset is composed of real images extracted from the Galaxy Zoo 2 project. The binary classification of elliptical and spiral galaxies is achieved with pre-processing steps including morphological filtering and a Laplacian pyramid. For the binary classification, we compare the different set of features with Support Vector Machines, Extreme Learning Machine, and different types of linear discriminant analysis techniques. The results support the conclusion that the proposed framework for the binary classification of elliptical and spiral galaxies provides an area under the receiver operating characteristic curve reaching 99.54%, proving the robustness of the approach for helping astronomers to study galaxies.																	1868-8071	1868-808X				AUG	2020	11	8					1839	1853		10.1007/s13042-020-01075-w		FEB 2020											
J								A compact firefly algorithm for matching biomedical ontologies	KNOWLEDGE AND INFORMATION SYSTEMS										Biomedical ontology matching; Compact firefly algorithm; Compact movement operator	MEMETIC ALGORITHM	Biomedical ontologies have gained particular relevance in the life science domain due to its prominent role in representing knowledge in this domain. However, the existing biomedical ontologies could define the same biomedical concept in different ways, which yields the biomedical ontology heterogeneous problem. To implement the inter-operability among the biomedical ontologies, it is critical to establish the semantic links between heterogenous biomedical concepts, so-called biomedical ontology matching. Since modeling the ontology matching problem is a complex and time-consuming task, swarm intelligent algorithm (SIA) becomes a state-of-the-art methodology for solving this problem. However, when addressing the biomedical ontology matching problem, the existing SIA-based matchers tend to be inefficient due to biomedical ontology's large-scale concepts and complex semantic relationships. In this work, we propose a compact firefly algorithm (CFA), where the explicit representation of the population is replaced by a probability distribution and two compact movement operators are presented to save the memory consumption and runtime of the population-based SIAs. We exploit the anatomy track, disease and phenotype track and biodiversity and ecology track from the ontology alignment evaluation initiative (OAEI) to test CFA-based matcher's performance. The experimental results show that CFA can improve the FA-based matcher's memory consumption and runtime by, respectively, 68.92% and 38.97% on average, and its results significantly outperform other SIA-based matchers and OAEI's participants.																	0219-1377	0219-3116				JUL	2020	62	7					2855	2871		10.1007/s10115-020-01443-6		FEB 2020											
J								Neural network for automatic farm control	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Smart home farm; the neural network in the agricultural farm; machine learning in the agriculture	BIG DATA; SMART; PREDICTION; PLATFORM	Prediction of metrological, botanical characteristics is extremely important for different directions in agriculture. The availability of these data allows us to adjust the process of growing crops, which has a huge impact on yield, speed of ripening and the presence of vitamins in the grown culture. Increasing yields due to changes in culture growing conditions without the use of gene mutations and herbicides are the most popular destination in the agriculture field. In this manuscript, a realisation of the neural network for the construct of an efficient autonomous farm was represented. The developed by farm creates the optimal conditions for growing a crop by controlling the following indicators: Illumination, PH of the ground, air temperature, the temperature of the ground, air humidity, CO2 concentration and humidity of the ground. Theoretical research and experimental research on the use of a neural network to predict vegetable growth were represented. The presented model can also be considered as a prototype device for testing various cultivated vegetables to identify the optimal characteristics for them growing.																	0952-813X	1362-3079															10.1080/0952813X.2020.1725653		FEB 2020											
J								Extreme Learning Regression for nu Regularization	APPLIED ARTIFICIAL INTELLIGENCE											MACHINE; NETWORKS	Extreme learning machine for regression (ELR), though efficient, is not preferred in time-limited applications, due to the model selection time being large. To overcome this problem, we reformulate ELR to take a new regularization parameter nu (nu-ELR) which is inspired by Scholkopf et al. The regularization in terms of nu is bounded between 0 and 1, and is easier to interpret compared to C. In this paper, we propose using the active set algorithm to solve the quadratic programming optimization problem of nu-ELR. Experimental results on real regression problems show that nu-ELR performs better than ELM, ELR, and nu-SVR, and is computationally efficient compared to other iterative learning models. Additionally, the model selection time of nu-ELR can be significantly shortened.																	0883-9514	1087-6545				APR 15	2020	34	5					378	395		10.1080/08839514.2020.1723863		FEB 2020											
J								Application of meta-heuristic methods to generation expansion planning: advanced formulations and case studies	ARTIFICIAL INTELLIGENCE REVIEW										Meta-heuristic methods; Power systems; Evolutionary algorithms; Generation expansion planning	DIFFERENTIAL EVOLUTION ALGORITHM; NSGA-II ALGORITHM; WIND FARMS	The generation expansion planning (GEP) is a very relevant optimization problem in power systems research. Several factors must be considered in the management of an optimal planning that aims at expanding the generation, such as satisfying a growing demand, dealing with costs of investment, operation and maintenance, dealing with pollutant emission, among others. Considering these aspects, this problem tends to be very complex and meta-heuristics methods have been employed in order to provide satisfactory solutions. This paper aims at reviewing GEP considering five different formulations of the problem and how meta-heuristics approaches can be applied to it. For this purpose, two case studies are presented and, in order to provide a deep analysis, these methods are compared according to their performance, the solution provided by them, and also comparing results obtained among different formulations, which leads to a promising methodology and some interesting results provided.																	0269-2821	1573-7462				OCT	2020	53	7					4737	4776		10.1007/s10462-020-09806-9		FEB 2020											
J								A competitive swarm optimizer with hybrid encoding for simultaneously optimizing the weights and structure of Extreme Learning Machines for classification problems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Competitive swarm optimizer; Extreme Learning Machines; Classification; Optimization	NEURAL-NETWORK; EVOLUTIONARY; PERFORMANCE	Extreme Learning Machine (ELM) is a learning algorithm proposed recently to train single hidden layer feed forward networks (SLFN). It has many attractive properties that include better generalization performance and very fast learning. ELM starts by assigning random values to the input weights and hidden biases and then in one step it determines the output weights using Moore-Penrose generalized inverse. Despite the aforementioned advantages, ELM performance might be affected by the random initialization of weights and biases or by the large generated network which might contain unnecessary number of neurons. In order to increase the generalization performance and to produce more compact networks, a hybrid model that combines ELM with competitive swarm optimizer (CSO) is proposed in this paper. The proposed model (CSONN-ELM) optimizes the weights and biases and dynamically determines the most appropriate number of neurons. To evaluate the effectiveness of the CSONN-ELM, it is experimented using 23 benchmark datasets, and compared to a set of static rules extracted from literature that are used to determine the number of neurons of SLFN. Moreover, it is compared to two dynamic methods that are used to enhance the performance of ELM, that are Optimally pruned ELM (OP-ELM) and metaheuristic based ELMs (Particle Swarm Optimization-ELM and Differential Evolution-ELM). The obtained results show that the proposed method enhances the generalization performance of ELM and overcomes the static and dynamic methods.																	1868-8071	1868-808X				AUG	2020	11	8					1801	1823		10.1007/s13042-020-01073-y		FEB 2020											
J								A data-driven approach for predicting printability in metal additive manufacturing processes	JOURNAL OF INTELLIGENT MANUFACTURING										Additive manufacturing; Machine learning; Powder bed fusion; Electron beam melting; Printability analysis	STRESS	Metal powder-bed fusion additive manufacturing technologies offer numerous benefits to the manufacturing industry. However, the current approach to printability analysis, determining which components are likely to build unsuccessfully, prior to manufacture, is based on ad-hoc rules and engineering experience. Consequently, to allow full exploitation of the benefits of additive manufacturing, there is a demand for a fully systematic approach to the problem. In this paper we focus on the impact of geometry in printability analysis. For the first time, we detail a machine learning framework for determining the geometric limits of printability in additive manufacturing processes. This framework consists of three main components. First, we detail how to construct strenuous test artefacts capable of pushing an additive manufacturing process to its limits. Secondly, we explain how to measure the printability of an additively manufactured test artefact. Finally, we construct a predictive model capable of estimating the printability of a given artefact before it is additively manufactured. We test all steps of our framework, and show that our predictive model approaches an estimate of the maximum performance obtainable due to inherent stochasticity in the underlying additive manufacturing process.																	0956-5515	1572-8145				OCT	2020	31	7			SI		1769	1781		10.1007/s10845-020-01541-w		FEB 2020											
J								Fuzzy Bezier splines with application to fuzzy functional integral equations	SOFT COMPUTING										Fuzzy Bezier splines; Iterative method; Fuzzy integral equations with modified argument	NUMERICAL-SOLUTION; ACCURATE SOLUTIONS; APPROXIMATION; INTERPOLATION; ERROR; BERNSTEIN; NUMBERS	In this paper, we present the description of fuzzy Bezier splines and as an application we propose an iterative numerical method for approximating the solution of fuzzy functional integral equations of Fredholm type. The convergence of the method is proved by providing an error estimate and it is tested on some numerical examples. The numerical stability regarding the choice of the first iteration is investigated.																	1432-7643	1433-7479				APR	2020	24	8			SI		6069	6084		10.1007/s00500-020-04740-y		FEB 2020											
J								Formal modelling of OWL ontologies-based requirements for the development of safe and secure smart city systems	SOFT COMPUTING										Smart city; Event-B formal method; Requirements; OntoGraf; ACE	PRIVACY; CHALLENGES	Formal methods are mathematical techniques used for developing reliable and verified systems. Event-B formal method is proved to be very useful to construct models of systems that are corrected by construction. Developing safe, secure, and reliable smart systems is essential for effective smart city solutions. The integration of safety and security mechanisms is an important aspect to achieve trust in smart cities' services and applications. In this paper, we present prototype for the development of smart systems using OWL ontologies and Event-B formal models. We focus on the proposed approach that uses OWL ontologies to generate Event-B formal models for secure and safe development of systems. In recent years, ontologies-driven approaches have been applied during different phases to requirements engineering (RE), such as elicitation, analysis, specification, and validation. Many empirical studies have demonstrated benefits of the application of ontologies to handle ambiguity, inconsistency and incompleteness of requirements. We derive benefit from OWL ontologies to produce textual requirements that are consistent, complete, and unambiguous for formal modelling and to manage traceability between requirements and models. The approach uses Protege-OWL editor, OWL verbaliser, Rodin platform, and OntoGraf tool. Protege-OWL editor enables to build and view ontologies in Web Ontology Language (OWL). OWL verbaliser is used to generate controlled English requirements called Attempto Controlled English (ACE) from OWL ontologies. ACE representation is used as input requirements and transformed into Event-B formal models. Rodin platform is used for specification, refinement and proof. OntoGraf is a tool in Protege that is used to visualise ontologies, and we make use of OntoGraf in this paper to assist in deciding refinement strategy and managing traceability between requirements and models.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11095	11108		10.1007/s00500-020-04688-z		FEB 2020											
J								A novel ODV crossover operator-based genetic algorithms for traveling salesman problem	SOFT COMPUTING										Genetic algorithm; Population seeding; Crossover; Ordered distance vector; Permutation coded	RECOMBINATION; OPTIMIZATION; POPULATION	The genetic algorithm is a popular meta-heuristic optimization technique whose performance depends on the quality of the initial population and the crossover operator used to manipulate the individuals to obtain the final optimal solution. It is evident that when similar principle is followed for population seeding and crossover operators, it can enhance the speed of convergence and the quality of final individuals. The recent and popular population seeding technique for combinatorial genetic algorithm is ordered distance vector-based population seeding which works best with respect to convergence rate and diversity. However, the technique could not achieve the zero error rate convergence for the large-sized test instances. Thus, in this paper, an ordered distance vector-based crossover operator is proposed that exclusively exploits the advantages of individuals' generated using the same initialization methods to attain the complete convergence, particularly for most of the large-sized test instances considered. One of the famous combinatorial problems of traveling salesman problem obtained from standard library is chosen as the testbed. From the experimental results, the proposed genetic algorithm model outshines the other existing and popular working genetic algorithm models in the literature. [GRAPHICS] .																	1432-7643	1433-7479				SEP	2020	24	17					12855	12885		10.1007/s00500-020-04712-2		FEB 2020											
J								Multi-agent informed path planning using the probability hypothesis density	AUTONOMOUS ROBOTS										Path planning; Target tracking; Probability hypothesis density (PHD); Multi-agent	RANDOM FINITE SETS; MULTITARGET; ALGORITHM; FILTERS	An Informed Path Planning algorithm for multiple agents is presented. It can be used to efficiently utilize available agents when surveying large areas, when total coverage is unattainable. Internally the algorithm has a Probability Hypothesis Density (PHD) representation, inspired by modern multi-target tracking methods, to represent unseen objects. Using the PHD, the expected number of observed objects is optimized. In a sequential manner, each agent maximizes the number of observed new targets, taking into account the probability of undetected objects due to previous agents' actions and the probability of detection, which yields a scalable algorithm. Algorithm properties are evaluated in simulations, and shown to outperform a greedy base line method. The algorithm is also evaluated by applying it to a sea ice tracking problem, using two datasets collected in the Arctic, with reasonable results. An implementation is provided under an Open Source license.																	0929-5593	1573-7527				JUL	2020	44	6					913	925		10.1007/s10514-020-09904-1		FEB 2020											
J								Intertemporal Hesitant Fuzzy Soft Sets: Application to Group Decision Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Hesitant fuzzy set; Soft set; Intertemporal choice; Group decision making	THEORETIC APPROACH; CROSS-ENTROPY; CHOICE; MODEL	A novel hybrid soft set model called intertemporal hesitant fuzzy soft set is proposed, and then, it is showed how it can deal with group decision making problems. This model can be used to incorporate hesitant information that varies across time, when the alternatives are described by their degree of agreement with various characteristics. According to the period of time (whether it is indefinitely long or with a known termination date), the proposed model is, respectively, called long-term and short-term intertemporal hesitant fuzzy soft set. Corresponding group decision making approaches are provided, where the application of Quasi-Hyperbolic discounting permits to deal with the time-inconsistent experts' preferences. Both the effect of scores of hesitant fuzzy elements and the effect of uncertainty contained in hesitant information are considered in the proposed decision approaches.																	1562-2479	2199-3211				MAR	2020	22	2			SI		619	635		10.1007/s40815-020-00798-w		FEB 2020											
J								T-S Fuzzy Adaptive Control Based on Small Gain Approach for an Uncertain Robot Manipulators	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										T-S fuzzy logic system; Adaptive control; Input-to-state stability (ISS); Robot manipulators; Small gain theorem	FEEDBACK NONLINEAR-SYSTEMS; TRACKING CONTROL; NEURAL-NETWORKS; DESIGN; APPROXIMATION; STABILIZATION; MODEL	In this paper, a T-S (Takagi-Sugeno) adaptive tracking algorithm control based on small gain theorem is proposed for an uncertain robot system with n-link manipulators. A nonzero time-varying parameter is introduced in the common T-S fuzzy logic system, the T-S type fuzzy logic system with updated parameters laws is build, then the new and original universal approximation with parameter is introduced. The approximation accuracy can be updated on-line by the parameters, which is not limited by the number of fuzzy rules. With the novel property of universal approximation, the proposed adaptive control can be synthetized to overcome the limitations such as on-line learning computation burden in conventional fuzzy logic systems. The originality T-S fuzzy logic system is used to compensate the unknown model of robot manipulators and the adaptive tracking control algorithm is designed with the new property of universal approximation. Based on the analysis of small gain theorem and ISS theory (input-to-state stability), all signals in closed-loop system can be guaranteed to be bounded, and the system can be extended from semi-global stability to global stability by employing the proposed adaptive control scheme. Finally, simulation results are shown to demonstrate the effectiveness of the adaptive control scheme.																	1562-2479	2199-3211				APR	2020	22	3					930	942		10.1007/s40815-019-00793-w		FEB 2020											
J								Distributionally Robust Chance Constrained Optimization Model for the Minimum Cost Consensus	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Distributionally robust optimization; Group decision-making; Consensus; CVaR	GROUP DECISION-MAKING; REACHING PROCESS; FEEDBACK MECHANISM	As a solution method that not only considers the probability distribution information of data, but also ensures that the results are not too conservative, more and more researches have been made on the distributionally robust optimization method. Based on the minimum cost consensus model, this paper proposes a new minimum cost consensus model with distributionally robust chance constraints (DRO-MCC). Firstly, Conditional Value-at-Risk (CVaR) is used to approximate the chance constraints in the cost model. Secondly, when the information of the first and second moments of random variables affecting the unit adjustment cost are known, the min-max problem is obtained based on the moment method and dual theory, and a tractable semidefinite programming problem can be easily processed through further transformation. Finally, in order to evaluate the robustness of the proposed model, the results of different parameters are compared, and the DRO-MCC is compared with the robust optimization model (RO-MCC) and the minimum cost consensus model (MCC). The example proves that MCC is too optimistic and RO-MCC is too conservative. In contrast, DRO-MCC overcomes the conservatism of robust optimization and considers the probability information of data, so the result is more ideal.																	1562-2479	2199-3211				SEP	2020	22	6			SI		2041	2054		10.1007/s40815-019-00791-y		FEB 2020											
J								Design and development of network simulator module for distributed mobility management protocol	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										NS2; Simulation; IP mobility; Distributed mobility management; PMIPv6	FAST HANDOVER; PERFORMANCE ENHANCEMENT; IPV6; MECHANISM	Distributed Mobility Management (DMM) is presented recently by IETF to overcome the limitations of the conventional Centralized Mobility Management (CMM) protocols. It is developed based on the network-based CMM protocol; Proxy Mobile IPv6 (PMIPv6). DMM tackles the issue of relying on a single entity by decoupling the control and data planes and distributes the functionalities of the centralized entity in CMM protocols. To study and examine the performance of DMM protocol, different evaluation approaches can be utilized. Although the test-bed implementation is more realistic approach, its drawbacks, such as high cost, complexity and unscalable, make the utilization of test-bed extremely difficult. Alternatively, simulation is an inexpensive and effective approach of testing various complex scenarios with scalability features. In this context, this paper presents DMM module for Network Simulator-2 (NS-2) implementing DMM entities, functionalities and operation. The paper also evaluates the performance of DMM protocol comparing to CMM protocol in different scenarios. The analytical evaluation of handover latency and session recover time is employed to ensure validity and reliability of DMM module. The results show that the developed module is valid and reliable where the theoretical results are approximately similar to the results obtained from simulation.																	1868-5137	1868-5145															10.1007/s12652-020-01764-y		FEB 2020											
J								From AAL to ambient assisted rehabilitation: a research pilot protocol based on smart objects and biofeedback	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart objects; Computer assisted rehabilitation; Ambient assisted rehabilitation; Pediatric rehabilitation; EPIQ approach	CHILDREN; PROGRAM; YOUTH	The progressive miniaturization of electronic devices and their exponential increase in processing, storage and transmission capabilities, represent key factors of the current digital transformation, also sustaining the great development of Ambient Assisted Living (AAL) and the Internet of Things. Although most of the investigations in the recent years focused on remote monitoring and diagnostics, rehabilitation too could be positively affected by the widespread integrated use of these devices. Smart Objects in particular may be among the enablers to new quantitative approaches. In this paper, we present a proof-of-concept and some preliminary results of an innovative pediatric rehabilitation protocol based on Smart Objects and biofeedback, which we administered to a sample of children with unilateral cerebral palsy. The novelty of the approach mainly consists in placing the sensing device into a common toy (a ball in our protocol) and using the information measured by the device to administer multimedia-enriched type of exercises, more engaging if compared to the usual rehabilitation activities used in clinical settings. We also introduce a couple of performance indexes, which could be helpful for a quantitative continuous evaluation of movements during the exercises. Even if the number of children involved and sessions performed are not suitable to assess any change in the subjects' abilities, nor to derive solid statistical inferences, the novel approach resulted very engaging and enjoyable by all the children participating in the study. Moreover, given the almost non-existent literature on the use of Smart Objects in pediatric rehabilitation, the few qualitative/quantitative results here reported may promote the scientific and clinical discussion regarding AAL solutions in a "Computer Assisted Rehabilitation" perspective, towards what can be defined "Pediatric Rehabilitation 2.0".																	1868-5137	1868-5145															10.1007/s12652-020-01744-2		FEB 2020											
J								Visual Detection of Small Unmanned Aircraft System: Modeling the Limits of Human Pilots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Small unmanned aircraft system; Unmanned aerial vehicle; Visual detection; See-and-avoid; Monte Carlo simulation		Every month, the Federal Aviation Administration (FAA) receives over 100 reports of small Unmanned Aircraft Systems (sUAS) operating in airspace where they do not belong and the industry has not deployed any specific, ubiquitous solution to preclude this potential collision hazard for pilots of manned aircraft [1]. The purpose of this research is to determine the key physical attributes and construct a new mathematical model to determine the probability of visual detection and avoidance of sUAS. Using the Monte Carlo simulation method, this study provided a means for addressing the effects of uncertainty in the uncontrollable inputs. As a result, it produced a set of probability curves for various operating scenarios and depicted the likelihood of visually detecting a small, unmanned aircraft in time to avoid colliding with it. This study suggested the probability of detecting a sUAS in time to avoid a collision, in all cases modeled during the study, is far less than 50%. The probability was well under 10% for sUAS aircraft similar to the products used by many recreational and hobby operators. This study indicated the see-and-avoid is not a reliable technique for collision prevention by manned-aircraft pilots with small, unmanned aircraft and call for regulators and the industry's deployment of alternative methods.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		933	947		10.1007/s10846-020-01152-w		FEB 2020											
J								Development of Smart Plate Number Recognition System for Fast Cars with Web Application	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING												Traffic law violation has been recognized as a major cause for road accidents in most parts of the world with majority occurring in developing countries. Even with the presence of rules and regulations stipulated against this, violators are still on the increase. This is due to the fact that the rules are not properly enforced by appropriate authorities in those parts of the world. Therefore, a system needs to be designed to assist law enforcement agencies to impose these rules to improve road safety and reduce road accidents. This work uses a Vehicle Plate Number Recognition (VNPR) system which is a real-time embedded system to automatically recognize license plate numbers. It provides an alternative means to VPNR using an open-source library known as openCV. The main aim of the system is to use image processing to identify vehicles violating traffic by their plate numbers. It consists of an IR sensor for detecting the vehicle. During testing, a minimum time was set for the sensor to detect the object which was recorded by the microprocessor. Once it was less than the set time, the camera was triggered to capture the plate number and store the image on the Raspberry Pi. The image captured is processed by the Raspberry Pi to extract the numbers on the image. The numbers on the capture imaged were viewed on a web page via an IP address. The system if implemented can be used to improve road safety and control traffic of emerging smart cities. It will also be used to apply appropriate sanctions for traffic law violators.																	1687-9724	1687-9732				FEB 7	2020	2020								8535861	10.1155/2020/8535861													
J								Online transferable representation with heterogeneous sources	APPLIED INTELLIGENCE										Online learning; Autoencoders; Heterogeneous streaming data; Transfer learning		Learning from streaming data has gained a lot of attention and interest in the past decades. These improvements have shown promising results when the models are trained and test on a single streaming source. However, the trained model often fail to produce the reliable results due to the difficulty of data shift and knowledge transfer with heterogeneous streaming domains. In this paper, we propose an architecture that is based on autoencoders. Specifically, we use online feature learning based on denoising autoencoder to learn more robust representations from streaming data. In order to tackle with data shift between source and target streaming data, we develop an ensemble weighted strategy, which can effectively handle the concept drifts of streaming data. Moreover, we develop the transfer mechanism, which is capable of transferring label information across heterogeneous domains. Finally, we combine online learning, data shift adaption and knowledge transfer with heterogeneous domains into a single process, which makes our proposed architecture powerful in learning and predicting for multistream classification problem. Experiments on heterogeneous datasets validate that the proposed algorithm can quickly and accurately classify instances on a stream together with a small number of labeled examples. Compared with a few related methods, our algorithm achieves some state-of-the-art results.																	0924-669X	1573-7497				JUN	2020	50	6					1674	1686		10.1007/s10489-019-01620-3		FEB 2020											
J								Finite-time stability of uncertain fractional difference equations	FUZZY OPTIMIZATION AND DECISION MAKING										Fractional difference equations; Uncertainty theory; Finite-time stability	GRONWALL INEQUALITY; DELAY SYSTEMS	Uncertain fractional difference equations may preferably describe the behavior of the systems with the memory effect and discrete feature in the uncertain environment. So it is of great significance to investigate their stability. In this paper, the concept of finite-time stability almost surely for uncertain fractional difference equations is introduced. A finite-time stability theorem is then stated by Mittag-Leffler function and proved by a generalized Gronwall inequality on a finite time. Some examples are finally presented to illustrate the validity of our results.																	1568-4539	1573-2908				JUN	2020	19	2					239	249		10.1007/s10700-020-09318-9		FEB 2020											
J								Genetic programming in the steelmaking industry	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Review; Steelmaking industry; Practical applications; Economic effects; Future applications	ARTIFICIAL NEURAL-NETWORK; ELECTRIC-ARC FURNACE; BLAST-FURNACE; MECHANICAL-PROPERTIES; HOT METAL; SINTER REDUCIBILITY; STEEL; PREDICTION; STRENGTH; SURFACE	Genetic programming is a powerful, robust and versatile tool that is suitable for predicting and forecasting, especially in the steelmaking industry, where the diversity of serial production processes and equipment strongly influence final product properties, quality and price. The article reviews a wide spectrum of implementation attempts of genetic programing in the steelmaking industry, including real practical applications where direct economic effects can be easily established. The article also presents remaining challenges.																	1389-2576	1573-7632															10.1007/s10710-020-09382-5		FEB 2020											
J								GPU acceleration of NL-means, BM3D and VBM3D	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image denoising; Video denoising; OpenCL; GPU; NL-means; BM3D; VBM3D	NONLOCAL MEANS; IMAGE; ALGORITHM; FILTER	Denoising is an essential part of any image- or video-processing pipeline. Unfortunately, due to time-processing constraints, many pipelines do not consider the use of modern denoisers. These algorithms have only CPU implementations or suboptimal GPU implementations. We propose a new efficient GPU implementation of NL-means and BM3D, and, to our knowledge, the first GPU implementation of the video-denoising algorithm VBM3D. The performance of these implementations enable their use in real-time scenarios.																	1861-8200	1861-8219															10.1007/s11554-020-00945-4		FEB 2020											
J								McDPC: multi-center density peak clustering	NEURAL COMPUTING & APPLICATIONS										Density peak clustering; Multi-center cluster; Image segmentation	IMAGE SEGMENTATION	Density peak clustering (DPC) is a recently developed density-based clustering algorithm that achieves competitive performance in a non-iterative manner. DPC is capable of effectively handling clusters with single density peak (single center), i.e., based on DPC's hypothesis, one and only one data point is chosen as the center of any cluster. However, DPC may fail to identify clusters with multiple density peaks (multi-centers) and may not be able to identify natural clusters whose centers have relatively lower local density. To address these limitations, we propose a novel clustering algorithm based on a hierarchical approach, named multi-center density peak clustering (McDPC). Firstly, based on a widely adopted hypothesis that the potential cluster centers are relatively far away from each other. McDPC obtains centers of the initial micro-clusters (named representative data points) whose minimum distance to the other higher-density data points are relatively larger. Secondly, the representative data points are autonomously categorized into different density levels. Finally, McDPC deals with micro-clusters at each level and if necessary, merges the micro-clusters at a specific level into one cluster to identify multi-center clusters. To evaluate the effectiveness of our proposed McDPC algorithm, we conduct experiments on both synthetic and real-world datasets and benchmark the performance of McDPC against other state-of-the-art clustering algorithms. We also apply McDPC to perform image segmentation and facial recognition to further demonstrate its capability in dealing with real-world applications. The experimental results show that our method achieves promising performance.																	0941-0643	1433-3058				SEP	2020	32	17					13465	13478		10.1007/s00521-020-04754-5		FEB 2020											
J								Uni- and bidirectional pedestrian flows through zigzag corridor in a tourism area: a field study	ADAPTIVE BEHAVIOR										Observational experiment; crowd movement; tourism area; zigzag corridor	UNDERSTANDING CROWD DYNAMICS; EVACUATION; MODEL; SIMULATION; BEHAVIORS; SAFETY	Nowadays, the famous tourist attractions are becoming more and more popular for people from all over the world. Thus, to ensure the safety of tourists is a tough task in such crowded area. The study of pedestrian's characteristics in crowd movement is essential for safety management. In this article, both uni- and bidirectional observational experiments were conducted to quantitatively analyze the movement properties of pedestrians in a zigzag corridor which is located in a tourism area named Yuyuan business district in Shanghai. Several phenomena have been found during the tourists' movement process: congestion at boundary, competing and bypassing behavior, and flow gap. As indicated by the transit time of pedestrians in both uni- and bidirectional scenarios, pedestrians in bidirectional pattern (>10 s) spend more time on going through the corridor than that in unidirectional one (<10 s). Besides, the fundamental diagrams in both uni- and bidirectional scenarios are significantly different from data in a controlled experiment, and obvious differences are observed within the density regime from 1.5 to 2.5 ped/m(2) between the uni- and bidirectional scenarios. In addition, spatial distributions of density and velocity demonstrate that pedestrians would like to cluster at the boundary of straight corridors in both uni- and bidirectional scenarios. The results could enrich the database of fundamental diagrams, and then be used for model calibration by taking the actual situation into consideration in similar scenarios.																	1059-7123	1741-2633														UNSP 1059712320902236	10.1177/1059712320902236		FEB 2020											
J								Violence Detection in Videos by Combining 3D Convolutional Neural Networks and Support Vector Machines	APPLIED ARTIFICIAL INTELLIGENCE											AUDIO	Video-surveillance has always been a vital tool to enforce safety in both public and private environments. Even though (smart) cameras are nowadays relatively widespread and cheap, such monitoring systems lack effectiveness in most scenarios. In addition, there is no guarantee about a human operator who monitors rare events in live video footages, forcing the use of such systems after unwanted events already took their undisturbed course, as a mere tool for investigations. Having an intelligent software to perform the task would allow to unlock the full potential of video-surveillance systems. To this end, in this paper we propose a solution based on a 3D Convolutional Neural Network that can effectively detect fights, aggressive motions and violence scenes in live video streams. Compared to state-of-the-art techniques, our method showed very promising performance on three challenging benchmark datasets: Hockey Fight, Crowd Violence and Movie Violence.																	0883-9514	1087-6545				MAR 20	2020	34	4					329	344		10.1080/08839514.2020.1723876		FEB 2020											
J								Synchronisation of 6D hyper-chaotic system with unknown parameters in the presence of disturbance and parametric uncertainty with unknown bounds	CONNECTION SCIENCE										6D hyper-chaotic system; fractional order system; adaptive-sliding mode control	COMMUNICATION	In this paper, adaptive-sliding mode control method is proposed for synchronisation of two 6D hyper-chaotic systems in the presence of external disturbance and parametric uncertainty and unknown parameters in the slave system. In the first section of this paper, two 6D integer order hyper-chaotic systems in the presence of external disturbance signal, parametric uncertainty and unknown parameters in the slave system are studied. In the second part of this paper, after identifying chaos in fractional order dynamic of the mentioned system, synchronisation of two 6D fractional derivative hyper-chaotic systems in the presence of external disturbance signal, parametric uncertainty and unknown parameters in the slave system is investigated, in which fractional order Riemann-Liouville derivative is used; a new fractional order sliding surface is defined for the hyper-chaotic system to determine the proper active control. Proper adaptive control laws are used to estimate the uncertainty bound, unknown disturbance signal and system parameters. Stability of the closed-loop control system is proved using Lyapunov theory in both modes. Simulation results in MATLAB show the desired application of the proposed controllers in the presence of disturbance and parametric uncertainty.																	0954-0091	1360-0494															10.1080/09540091.2020.1723491		FEB 2020											
J								Graph Laplacian for image anomaly detection	MACHINE VISION AND APPLICATIONS										Anomaly detection; Graph Fourier transform; Graph-based image processing; Principal component analysis; Hyperspectral images; PET	TARGET DETECTION; RX-ALGORITHM; CLASSIFICATION; SEGMENTATION; REPRESENTATION; PERFORMANCE; MODELS; NOISE	Reed-Xiaoli detector (RXD) is recognized as the benchmark algorithm for image anomaly detection; however, it presents known limitations, namely the dependence over the image following a multivariate Gaussian model, the estimation and inversion of a high-dimensional covariance matrix, and the inability to effectively include spatial awareness in its evaluation. In this work, a novel graph-based solution to the image anomaly detection problem is proposed; leveraging the graph Fourier transform, we are able to overcome some of RXD's limitations while reducing computational cost at the same time. Tests over both hyperspectral and medical images, using both synthetic and real anomalies, prove the proposed technique is able to obtain significant gains over performance by other algorithms in the state of the art.																	0932-8092	1432-1769				FEB 7	2020	31	1							11	10.1007/s00138-020-01059-4													
J								A computing method-based rearrangement of network protocols to improvise quality factors of FACTS devices and sensors using Newton-Raphson technique	COMPUTATIONAL INTELLIGENCE										FACTS devices; flower pollination algorithm; Newton-Raphson method	POWER; SYSTEM; SSSC	The load flow analysis project was carried out using the Newton-Raphson's iteration technique and a multiobjective method was suggested to minimize power loss, increase bus voltage, reduce operating costs, and controlling the flexible AC transmission system (FACTS) controllers. The key focus is to improvise the load sustainability subjected to controlling of system safety, integrity, and stability margins within specified limits by acquiring optimum place, installation expenses for FACTS controllers. It is important to analyze the benefits and architect the FACTS devices for the power steady state analysis. For effective modeling, the five bus standard is analyzed without the FACTS end devices and with the FACTS controllers. Transient voltage is critical which requires accurate and quick response to avoid the voltage collapse and instability issues. The Newton-Raphson's method of load flow analysis is an iterative method which approximates the set of nonlinear simultaneous load flow equations to a set of linear simultaneous load flow equations using Taylor's series expansion and the terms are limited to first order approximation. The variations in voltage are within 5% for a well designated power system. If it exceeds the specified limit then the performance of equipment will be poor and the life of equipment will reduce. Hence the voltage control is very important to improvise the quality factor of the FACTS controllers and devices in power system. The voltage variations in a bus or node are related to reactive power. If the reactive power is injected to a bus is less than reactive power drawn from the FACTS devices, the voltage instability becomes infinite issue causes damage to the controllers and devices. In a load flow problem, two quantities are specified for each bus and the remaining quantities are obtained by the load flow equation analysis using Newton-Raphson method. This method has been tested for IEEE 30 bus system and then the values are compared and analyzed with MATLAB.																	0824-7935	1467-8640															10.1111/coin.12284		FEB 2020											
J								How to cheat the page limit	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										conference organization; fairness; LaTeX; reviewing process; TeXnical Desk Reject Phase		Every conference imposing a limit on the length of submissions must deal with the problem of page limit cheating: authors tweaking the parameters of the game such that they can squeeze more content into their paper. We claim that this problem is endemic, although we lack the data to formally prove this. Instead, this paper provides a far from exhaustive summary of ways to cheat the page limit, a case study involving the papers accepted for the Research and Applied Data Science tracks at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2019, and a discussion of ways for program chairs to tackle this problem. Of the 130 accepted papers in these two ECMLPKDD 2019 tracks, 68 satisfied the page limit; 62 (47.7%) turned out to spill over the page limit, by up to as much as 50%. To misappropriate a phrase from Darrell Huff's "How to Lie with Statistics," we intend for this paper not to be a manual for swindlers; instead, nefarious paper authors already know these tricks, and honest program chairs must learn them in self-defense. This article is categorized under: Commercial, Legal, and Ethical Issues > Fairness in Data Mining																	1942-4787	1942-4795				MAY	2020	10	3							e1361	10.1002/widm.1361		FEB 2020											
J								Backlit images enhancement using global tone mappings and image fusion	IET IMAGE PROCESSING										image colour analysis; image fusion; image sensors; image enhancement; photography; backlit images enhancement; global tone mappings; photography subject; bright regions; single tone mapping function; dark regions; image fusion algorithm	COLOR; IMPLEMENTATION; RETINEX	The authors present a method for the enhancement of backlit images, i.e. images in which the main source of light is behind the photography subject. These images contain, simultaneously, very dark and very bright regions. In this situation, a single tone mapping function is unable to enhance the whole image. They propose the use of several such tone mappings, some of them enhancing the dark regions while others enhancing the bright regions, and then the combination of all these results using an image fusion algorithm. Qualitative and quantitative results confirm the validity of the proposed method.																	1751-9659	1751-9667				FEB 7	2020	14	2					211	219		10.1049/iet-ipr.2019.0814													
J								Hyperspectral image restoration by subspace representation with low-rank constraint and spatial-spectral total variation	IET IMAGE PROCESSING										image restoration; hyperspectral imaging; image representation; image classification; geophysical image processing; spatial information; HSI restoration; artificial rank constraint; structured sparse noise; intrinsic structure; spectral space; spatial-spectral total variation regularisation; spatial smoothness; spectral smoothness; hyperspectral image restoration; spectral vectors; subspace low-rank representation; low-rank structure; rank-minimum representation; SLRR framework; visual quality	SPARSE REPRESENTATION; NOISE-REDUCTION; REMOVAL; MODEL	Hyperspectral images (HSIs) restoration is an important preprocessing step. The spectral vectors in HSI can be separated into different classification based on the land-covers, which means the spectral space can be regarded as the union of several low-rank subspaces. Subspace low-rank representation (SLRR) is powerful in exploring the inner low-rank structure and has been applied for HSI restoration. However, the traditional SLRR only seek for the rank-minimum representation under a given dictionary, which may treat the structured sparse noise as inherent low-rank components. In addition, the SLRR framework cannot make full use of the spatial information. In this study, a framework named subspace representation with low-rank constraint and spatial-spectral total variation is proposed for HSI restoration. In which, an artificial rank constraint is introduced to control the rank of the representation result, which can improve the removal of the structured sparse noise and exploit the intrinsic structure of spectral space more effectively. Meanwhile, the spatial-spectral total variation regularisation is applied to enhance the spatial and spectral smoothness. Several experiments conducted in simulated and real HSI datasets demonstrate that the proposed method can achieve a state-of-the-art performance both in visual quality and quantitative assessments.																	1751-9659	1751-9667				FEB 7	2020	14	2					220	230		10.1049/iet-ipr.2019.0803													
J								Research on fingerprint classification based on twin support vector machine	IET IMAGE PROCESSING										particle swarm optimisation; image classification; fingerprint identification; Gabor filters; trees (mathematics); feature extraction; pattern classification; support vector machines; image texture; optimised model; classification model; fingerprint images; TWSVM; twin support vector machine; fingerprint recognition; fingerprint classification method; multiclass model; quantum particle swarm optimisation algorithm; fingerprints		Fingerprint classification is one of the core steps of fingerprint recognition and directly relates to the accuracy of recognition. For this reason, a fingerprint classification method based on Twin Support Vector Machine (TWSVM) is studied. First, the Gabor filter is used to extract texture features from fingerprint images. Second, a multi-class model based on TWSVM is constructed by using the 'one-versus-all' strategy and the binary tree method, respectively. The quantum particle swarm optimisation algorithm is used to optimise the parameters in the model. Then the fingerprints are divided into five categories using the optimised model. Finally, the classification model is evaluated using fingerprint images from the NIST-4 database. The experimental results show that applying the TWSVM to fingerprint classification can get good classification results.																	1751-9659	1751-9667				FEB 7	2020	14	2					231	235		10.1049/iet-ipr.2018.5977													
J								Real-time multi-trajectory matching for dynamic hand gesture recognition	IET IMAGE PROCESSING										gesture recognition; image matching; pattern clustering; convolutional neural nets; nearest neighbour methods; convolutional neural network; multitrajectory recognition; multitrajectory gestures; dynamic hand gesture recognition; multiple fingertip detection; concavity kernel accumulation algorithm; CKA; region proposal generator; real-time multitrajectory matching; global nearest neighbour point matching algorithm; long short-term memory; GPU	LOCALIZATION; EXTRACTION	Focus on the field of dynamic gesture recognition, there is a problem of the action, is difficult to recognise when multiple fingertips move in a small range (rotation, grabbing), the authors proposed a method to get the recognition results with high robustness in real-time. Firstly, they proposed the concavity kernel accumulation algorithm (CKA) to cluster corners in an image. Secondly, they deem CKA as a region proposal generator and combined it with convolutional neural network to detect fingertips. Thirdly, they proposed the global nearest neighbour point matching algorithm to match fingertips from two frames. Finally, the long short-term memory is used in multi-trajectory recognition to get the results of gesture recognition. Experiments show that their method could recognise multi-trajectory gestures accurately, furthermore, it can run in real time (20 FPS) without graphics processing unit (GPU).																	1751-9659	1751-9667				FEB 7	2020	14	2					236	244		10.1049/iet-ipr.2019.1068													
J								Background subtraction in dynamic scenes using the dynamic principal component analysis	IET IMAGE PROCESSING										estimation theory; image colour analysis; object detection; principal component analysis; image motion analysis; image sequences; video signal processing; Gaussian processes; corresponding detection thresholds; background subtraction; depth-based methods; dynamic scenes; dynamic principal component analysis; foreground detection method; dynamic effects; successive frames; robust pixel-based background model; value colour space; kernel density estimation; background time-lagged data matrix	OBJECT DETECTION; SEGMENTATION; FEATURES; MIXTURE; MODEL	This study presents a foreground detection method capable of robustly estimating the background under the presence of dynamic effects. The key contribution of this study is the use of the dynamic principal component analysis to model the serial correlation between successive frames and construct a robust pixel-based background model. The frames are normalised in hue, saturation and value colour space to reduce the effect of illumination changes. To restrict the background model, kernel density estimation is used to identify the distribution of the background time-lagged data matrix and then confidence interval limits are used to determine the corresponding detection thresholds. The foreground is detected using background subtraction. This method is tested on several common sequences such as CDnet 2014, ETSI 2014 and MULTIVISION 2013. The authors also hold comparisons based on quantitative metrics with several state-of-the-art methods. Experimental results show that their method outperforms some state-of-the-art methods and has comparable performance with some depth-based methods.																	1751-9659	1751-9667				FEB 7	2020	14	2					245	255		10.1049/iet-ipr.2018.6095													
J								Training approach using the shallow model and hard triplet mining for person re-identification	IET IMAGE PROCESSING										data mining; sensor fusion; learning (artificial intelligence); matrix algebra; target tracking; image matching; neural nets; cameras; shallow model; hard triplet mining; multitarget tracking; nonoverlapping camera network; person re-identification problem; backbone model; fusion model; mAP performance; relationship matrices; rank-1 performance; DukeMTMC-reID dataset; Market1501 dataset	TRACKING; NETWORK	Multi-target tracking in a non-overlapping camera network is an active research field, and one of the important problems in it is the person re-identification problem. In this study, the authors propose an approach to improve the performance of the backbone model in the person re-identification. Their approach focuses on training a fusion model with a shallow model and making hard triplets with relationship matrices quickly and efficiently. The proposed approach is simple, but it improves the performance of the backbone. In addition, the hard triplet mining in their process is much faster than the conventional approach. Experimental evaluation shows that the proposed approach can improve the performances of the backbone model. The proposed approach improves rank-1 and mean average precision (mAP) performance by more than 12.54 and 15.44%, respectively, over the backbone models in the Market1501 and DukeMTMC-reID dataset. The approach also achieves competitive performances compared with state-of-the-art approaches.																	1751-9659	1751-9667				FEB 7	2020	14	2					256	266		10.1049/iet-ipr.2019.0334													
J								Heat diffusion embedded level set evolution for infrared image segmentation	IET IMAGE PROCESSING										image segmentation; minimisation; infrared imaging; data term; energy function; bias field model; local intensity clustering property; intensity inhomogeneity; appropriate seeded pixels; combinatorial optimal algorithm; graph model; level set evolution implementation; infrared image datasets; standard level set methods; infrared image segmentation; local region-based models; local window scale; heat diffusion process	ACTIVE CONTOURS; INTENSITY INHOMOGENEITY; GRAPH CUTS; INFORMATION; FRONTS; MODEL	In this study, the authors present a novel level set method for infrared image segmentation. Local region-based models can fit intensity inhomogeneity partly but they are sensitive to local window scale. To deal with it, they embed an heat diffusion process in conventional level set evolution and convert heat to a part of data term in level set energy function. Besides, bias field model can extract the local intensity clustering property of the image. Therefore, the proposed method can deal with the interference of intensity inhomogeneity and complex background if appropriate seeded pixels are selected. Finally, the energy functional is minimised by a combinatorial optimal algorithm in a graph model to get a global optimal solution and accelerate the level set evolution implementation. The experiments show that the proposed method is robust to parameter setting, noise, and initial contour position. The comparisons on a large quantity of infrared image datasets with standard level set methods also demonstrate the efficiency of the proposed method.																	1751-9659	1751-9667				FEB 7	2020	14	2					267	278		10.1049/iet-ipr.2018.6629													
J								Assessment of Sentinel-2A multispectral image for benthic habitat composition mapping	IET IMAGE PROCESSING										bathymetry; image segmentation; remote sensing; image sensors; geophysical image processing; image classification; oceanographic techniques; geophysical signal processing; ALOS AVNIR-2; aerial image; custom-made unmanned aerial vehicle; individual benthic class; benthic habitat class spatial distribution; image segmentation; classification tree analysis; nonconstructed class covers carbonate sand; Sentinel-2A; benthic habitat spatial distribution mapping; multispectral image; benthic habitat composition mapping	CORAL-REEFS; SUN GLINT; VEGETATION; BIOMASS; COLOR; COVER	Sentinel-2A accuracy for benthic habitat composition mapping was tested and compared to ALOS AVNIR-2. Aerial image acquired using custom-made unmanned aerial vehicle was used to train and validate the model. The mapping was conducted regardless of the benthic class and at individual benthic class. Benthic habitat class spatial distribution was obtained using the combination of image segmentation and classification tree analysis. The aerial image was interpreted based on the percentage of the constructed and non-constructed classes. The constructed class includes coral reefs, dead coral, seagrass, and macroalgae, while non-constructed class covers carbonate sand, rock, and rubble. Sentinel-2A produced higher accuracy (92%) than ALOS AVNIR-2 (78%) for benthic habitat spatial distribution mapping. However, in the empirical modelling of benthic habitat composition, ALOS AVNIR-2 (SE 23-24%) produced slightly better accuracy than Sentinel-2A (SE 23-27%). Several factors affected the low accuracy, which include the sub-pixel mixing of benthic habitat and constructed class, the delay between dates of acquisition, and radiometric quality of the images. Since the fundamental relationship between reflectance value and the percentage of the constructed class has been justified and consistent, given more experiments it has the potential to predict benthic habitat composition with higher accuracy in the future.																	1751-9659	1751-9667				FEB 7	2020	14	2					279	288		10.1049/iet-ipr.2018.6044													
J								Effective hybrid genetic algorithm for removing salt and pepper noise	IET IMAGE PROCESSING										image enhancement; genetic algorithms; image denoising; pepper noise; called effective HGA; EHGA; image denoising methods; rapid convergence; mutation operators; peak signal-to-noise ratio; structural similarity index metric; image enhancement factor; noise density; effective hybrid genetic algorithm	MEDIAN FILTER; IMAGES	This study presents a new approach for recovering an image perturbed by salt and pepper noise (SPN) using a hybrid genetic algorithm (HGA) at all densities, called effective HGA (EHGA). The main contribution of the proposed algorithm is combining the genetic algorithm with image denoising methods that are integrated into the population to achieve rapid convergence. The idea is to evolve a group of individuals into a number of iterations using crossover and mutation operators. This approach evolves a set of images rather than a set of parameters from the filters. Experimental results of simulation on different images using peak signal-to-noise ratio, structural similarity index metric, image enhancement factor and Universal Quality Index show that the proposed algorithm outperforms other methods in removing the SPN qualitatively and quantitatively if the noise density is moderate and high. EHGA also preserves important features such as texture and corners of the image.																	1751-9659	1751-9667				FEB 7	2020	14	2					289	296		10.1049/iet-ipr.2019.0566													
J								Blur parameter locus curve and its applications	IET IMAGE PROCESSING										image denoising; image restoration; optical transfer function; image processing; blur parameter locus curve; point spread function; characteristic function; optical system; BPLC; system representation; PSF; measurement function; blur kernels; system characteristics; point source	DEFOCUS MAP ESTIMATION; SINGLE IMAGE; DEPTH	Conventionally, the point spread function (PSF) is understood as a characteristic function of any optical system. It captures the information about the amount of blur present along all the directions for a point in the scene. However, the dependence of blur on the PSF is in the form of convolution for any object other than a point source present in the scene and hence their relationship is less explicit. The authors propose a blur parameter locus curve (BPLC) as a system representation which has a one to one relationship with blur. BPLC simply is a chart of blur amounts in all directions of a given PSF with respect to the selected measurement function. They further characterise the PSF by decomposing the variation of BPLC across all directions based on the study performed for different possible forms of the blur kernels. Such decomposition provides powerful tools for various analysis. As PSF can be anisotropic, the computation of BPLC becomes an essential intermediate step to obtain the scale map as at the same scale, blur is different in different directions. Furthermore, they demonstrate the use of BPLC to obtain other system characteristics function such as PSF.																	1751-9659	1751-9667				FEB 7	2020	14	2					297	309		10.1049/iet-ipr.2019.0577													
J								Improving 3D reconstruction accuracy in wavelet transform profilometry by reducing shadow effects	IET IMAGE PROCESSING										shape measurement; wavelet transforms; mean square error methods; surface topography measurement; image colour analysis; image reconstruction; shadow regions; coloured fringe patterns; shadow outline information; background-plane fringe pattern; reconstructed objects; shadow removal; Bioucas phase-unwrapping method; Ghiglia phase-unwrapping method; shadow case; 3D reconstruction accuracy; shadow effects; wavelet transform profilometry; three-dimensional reconstruction method; structured light technique; fringe pattern projection; high-performance 3D reconstruction method; image capture process; accurate 3D reconstructions; phase data; object reconstruction; robust phase-unwrapping algorithms; diverse intensities; mean squared error; MSE	FRINGE PROJECTION PROFILOMETRY; PHASE UNWRAPPING METHOD; SHAPE MEASUREMENT; FOURIER; HILBERT	Wavelet transform profilometry is a three-dimensional (3D) reconstruction method based on the structured light technique of fringe pattern projection, widely used because it is a non-invasive, high-performance 3D reconstruction method. The presence of shadows created by the object in the image capture process is an obstacle in obtaining accurate 3D reconstructions, as they add noise to the phase data, leading to artefacts in object reconstruction, even when using robust phase-unwrapping algorithms. Since shadows present diverse intensities and shapes, detecting and eliminating their effects are challenging tasks. This work presents a novel method to detect shadow regions and reduce their effects in 3D reconstruction. The proposed method uses coloured fringe patterns to detect the shadows and mathematical morphology to condition the outlines of the shadow regions. The shadow outline information is used to interpolate the background-plane fringe pattern onto the captured scene, where the shadows are detected. The mean squared error (MSE) of the reconstructed objects is reduced to 25% of the MSE without shadow removal, on an average, when using the Bioucas phase-unwrapping method. When using the Ghiglia phase-unwrapping method, the MSE reduction is to 8.3%, on an average, of the MSE in the shadow case.																	1751-9659	1751-9667				FEB 7	2020	14	2					310	317		10.1049/iet-ipr.2019.0854													
J								Ensemble of deep convolutional neural networks based multi-modality images for Alzheimer's disease diagnosis	IET IMAGE PROCESSING										brain; image classification; feature extraction; medical image processing; decision trees; learning (artificial intelligence); positron emission tomography; diseases; neurophysiology; biomedical MRI; convolutional neural nets; Alzheimer's disease diagnosis; common progressive neurodegenerative diseases; structural magnetic resonance imaging; anatomical structure; fluorodeoxy-glucose positron emission tomography; multimodality images; image processing stage; deep convolutional neural networks; Alzheimer's Disease Neuroimaging Initiative database; Alzheimer's disease classification; Adaboost ensemble classifier; single decision tree classifier; classification accuracy; brain	MILD COGNITIVE IMPAIRMENT; FEATURE REPRESENTATION; CLASSIFICATION; PET; IDENTIFICATION; FUSION	Alzheimer's disease (AD) is one of the most common progressive neurodegenerative diseases. Structural magnetic resonance imaging (MRI) would provide abundant information on the anatomical structure of human organs. Fluorodeoxy-glucose positron emission tomography (PET) obtains the metabolic activity of the brain. Previous studies have demonstrated that multi-modality images could contribute to improve diagnosis of AD. However, these methods need to extract the handcrafted features that demand domain specific knowledge and image processing stage is time consuming. In order to tackle these problems, in this study, the authors propose a novel framework that ensembles three state-of-the-art deep convolutional neural networks (DCNNs) with multi-modality images for AD classification. In detail, they extract some slices from each subject of each modality, and every DCNN generates a probabilistic score for the input slices. Furthermore, a 'dropout' mechanism is introduced to discard low discrimination slices of the category probabilities. Then average reserved slices of each subject are acquired as a new feature. Finally, they train the Adaboost ensemble classifier based on single decision tree classifier with the MRI and PET probabilistic scores of each DCNN. Evaluations on Alzheimer's Disease Neuroimaging Initiative database show that the proposed algorithm has better performance compared to existing method, the algorithm proposed in this study significantly improved the classification accuracy.																	1751-9659	1751-9667				FEB 7	2020	14	2					318	326		10.1049/iet-ipr.2019.0617													
J								Reversible contrast enhancement for medical images with background segmentation	IET IMAGE PROCESSING										biomedical MRI; diagnostic radiography; medical image processing; image enhancement; image segmentation; contrast-changed image; storage space; archiving system; visual distortions; image segmentation; CE process; selected regions; 30 chest radiograph images; 20 magnetic resonance images; reversible CE; CE effects; preserving image quality; reversible contrast enhancement; medical images; background segmentation; unclear content; interested regions	DATA HIDING METHOD; HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT; CHEST RADIOGRAPHS; WATERMARKING	Contrast enhancement (CE) of medical images is helpful to bring out the unclear content in the interested regions. Recently, reversible CE has been proposed so that the original version of a contrast-changed image can be exactly recovered. This property can be used to save storage space or facilitate the archiving system. To enhance the regions of interest (ROI) without introducing visual distortions, the technique of image segmentation (e.g. using Otsu's method) has been used to obtain the background before conducting the CE process. To segment the ROI more accurately, an interactive algorithm called GrabCut is employed in the proposed scheme. In addition, a new preprocessing strategy is adopted to preserve the image quality through the CE process. Consequently, the content in the selected regions can be better brought out while the reversibility of the CE process is achieved. The experimental results on 30 chest radiograph images and 20 magnetic resonance images have demonstrated the efficacy of the proposed scheme for reversible CE. The evaluation results are provided to show the better performances of the proposed method in achieving CE effects and preserving image quality.																	1751-9659	1751-9667				FEB 7	2020	14	2					327	336		10.1049/iet-ipr.2019.0423													
J								Investigation of filtering of rain streaks affected video sequences under various quantisation parameter in HEVC encoder using an enhanced V-BM4D algorithm	IET IMAGE PROCESSING										image matching; video coding; image denoising; image sequences; image filtering; image enhancement; HEVC encoder; enhanced V-BM4D algorithm; high-efficiency video coding encoder; high definition video sequences; real-time video application; temporal information; natural degradation parameters; structural noise pattern; enhanced block matching denoising block; enhanced V-BM4D-based in-loop filtering; HD video sequences; highly directional oriented rain streak pattern; video block enhancement; high definition video sequences; quantisation parameter; HEVC reference software HM16; 0; peak signal to noise ratio value	REMOVAL; PREDICTION	This study proposes an enhanced video block matching four-dimensional (V-BM4D) denoising algorithm for highly correlated rain streaks pattern video sequence in high-efficiency video coding (HEVC) encoder. Removal of structural rain streaks from high definition (HD) video sequences are challenging issues in real-time video application. Temporal information of original video sequences is distorted among the successive video frames due to the presence of natural degradation parameters such as fog, smog and rain streaks. Rain streaks are highly correlated structural noise pattern, which affects the nature of video frame compared to other natural degradations. Existing HEVC encoder is employed with in-loop filtering to protect the temporal information of original video sequences from rain streaks pattern. However, in-loop filtering is unable to remove the highly directional oriented rain pattern from video frames. To retain temporal information among the successive video frames, enhanced block matching denoising block is adopted in HEVC coder. The proposed enhanced VBM4D- based in-loop filtering eliminates the various level of rain streaks from the HD video sequences by temporal information prediction in 4D platform. Experimental results demonstrate that the proposed denoising algorithm yields better peak signal to noise ratio value and provides better bit rate saving for various HEVC configuration.																	1751-9659	1751-9667				FEB 7	2020	14	2					337	347		10.1049/iet-ipr.2018.6005													
J								Identification of wool and mohair fibres with texture feature extraction and deep learning	IET IMAGE PROCESSING										production engineering computing; wool; textile industry; image texture; feature extraction; image classification; natural fibres; learning (artificial intelligence); microscopic images; separate wool fibre; mohair fibre; texture analysis based identification method; deep learning; animal fibres; animal-based fibres	LOCAL BINARY PATTERNS; IMAGE-BASED METHOD; ANIMAL FIBERS; NEURAL-NETWORKS; CASHMERE; CLASSIFICATION; RECOGNITION; PROJECTION; WAVELET; FILTER	Wool and mohair fibres are both animal-based fibres and having circular scales on their microscopic images from the longitudinal view. Although they look very similar in their microscopic view, they show different physical/chemical properties which determine their usage area. Thus, in textile industry, they need to be separated carefully from each other. The separation of wool/mohair fibres is an important issue and can be performed with human eye by using the microscopic images, that is not time/cost effective and not objective. The novelty of the presented study is to design an objective, easy, rapid, time and cost-effective method in order to separate wool fibre from mohair fibre by using a texture analysis based identification method. For this purpose, microscopic images of both wool and mohair fibres were preprocessed as the texture images. Local binary pattern-based feature extraction process and deep learning were separately used to get determinative information from the fibres. In order to identify the samples, the classification based method was completed. Experimental results indicated that an accurate texture analysis for this kind of animal fibres is possible to identify wool and mohair fibres by using deep learning and machine learning with 99.8% and 90.25% accuracy rates, respectively.																	1751-9659	1751-9667				FEB 7	2020	14	2					348	353		10.1049/iet-ipr.2019.0907													
J								Highly efficient neoteric histogram-entropy-based rapid and automatic thresholding method for moving vehicles and pedestrians detection	IET IMAGE PROCESSING										video surveillance; object detection; image segmentation; entropy; probability; automatic thresholding method; vehicles; pedestrians detection; important key step; accurate threshold value; complex image; coarse image; general histogram-entropy-based thresholding method; HEBT method; fast threshold value evaluation; efficient automatic threshold value evaluation; 1D bimodal histogram; optimal threshold values; three-frame differencing segmentation technique; change detection; state-of-the-art automatic thresholding methods; experimental segmented image results; fast image segmentation; effective image segmentation	GRAY-LEVEL; MAXIMUM-ENTROPY; 2D HISTOGRAM; IMAGE; MODEL	Thresholding for segmentation is an important key step and necessary process in various applications. Estimating an accurate threshold value for a complex and coarse image is computationally expensive and lacks accuracy and stability. This study is aimed at developing a general histogram-entropy-based thresholding method, referred as our HEBT method, for fast and efficient automatic threshold value evaluation. In the proposed method, the probability density function and Shannon entropy derived from 1D bimodal histogram have been used to find the optimal threshold values automatically. The proposed method implemented with a three-frame differencing segmentation technique has been tested on real-time datasets - change detection 2012, change detection 2014, and Wallflower - to identify pedestrians and vehicles. The performance of our HEBT method has been compared with six state-of-the-art automatic thresholding methods. The experimental segmented image results confirmed that our HEBT method is more adaptable and better suited for real-time systems with severe challenging conditions of great variations. Further, the new HEBT method achieved the best segmentation results with highest values of several performance parameters, i.e. recall, precision, similarity, and f-measure. Interestingly, the computation time is the lowest for the proposed method than the state-of-the-art methods, promising its application for a fast and effective image segmentation.																	1751-9659	1751-9667				FEB 7	2020	14	2					354	365		10.1049/iet-ipr.2018.5555													
J								Non-redundant frame identification and keyframe selection in DWT-PCA domain for authentication of video	IET IMAGE PROCESSING										discrete wavelet transforms; singular value decomposition; entropy; principal component analysis; ant colony optimisation; video watermarking; DWT-principal component analysis domain; SSIM-AMD-based NRFI algorithm; entropy-AMD-based KFS algorithm; watermark image block; principal component analysis technique; video processing; DWT-singular value decomposition; video authentication; ant colony optimisation; KFS; NRFI; structural similarity index metric-absolute difference metric; SSIM-AMD based nonredundant frame identification; entropy-AMD-based keyframe selection; chaotic map	WATERMARKING SCHEME; DIGITAL IMAGES; WAVELET; ALGORITHM	This study is intended to protect video data and watermark from unauthorised access. The proposed methodology accentuates two new algorithms, namely structural similarity index metric-absolute difference metric (SSIM-AMD) based non-redundant frame identification (NRFI) and entropy-AMD based keyframe selection (KFS) to reduce the challenges posed by traditional discrete wavelet transform-singular value decomposition. Traditional techniques embed the entire watermark to all existing frames in the video, which is cumbersome and time-consuming. In this methodology, NRFI algorithm is applied to segregate the redundant and non-redundant frames to specific database. The KFS algorithm is used to identify suitable keyframes. DWT is applied into keyframes, which decomposes the frames into subbands. The middle band is selected for embedding. The principal component of watermark image block is embedded into identified keyframes in the video. The chaotic map is adapted to reorder the watermark block for improving the authentication level of the watermarking. The ant colony optimization (ACO) technique is adapted to select the suitable scaling factor for watermarking process. The principal component analysis technique is employed for avoiding false-positive attacks. Experimental results show the proposed methodology can withstand image processing, video processing, false-positive attacks and produces good results in terms of perceptual quality and robustness.																	1751-9659	1751-9667				FEB 7	2020	14	2					366	375		10.1049/iet-ipr.2019.0341													
J								Fast and robust video stabilisation with preserved intentional camera motion and smear removal for infrared video	IET IMAGE PROCESSING										image matching; video signal processing; image motion analysis; motion estimation; image sequences; feature extraction; cameras; video surveillance; infrared domain; complete visual sequences; pitch dark night conditions; thermal camera; output video; visual quality; unintentional camera motion; unwanted motions; smear removal; intentional motion; robust features; stabilised output videos; robust video stabilisation; infrared video; border military surveillance; intentional camera motion		Border military surveillance is one of the demanding and challenging tasks for any nation. Thermal (infrared) camera, which works on the infrared domain, provides complete visual sequences even in pitch dark night conditions. When the video is recorded from a thermal camera mounted on a vehicle, the output video is unstabilised with poor visual quality due to unintentional camera motion. These unwanted motions also introduce smear. Furthermore, there may be situations when a camera is moved intentionally to capture target. This study proposes a fast and robust algorithm for auto stabilisation of videos with smear removal while keeping the intentional motion of camera. This algorithm is developed under the framework of speeded up robust features matching. The proposed algorithm is capable of correcting both motions, i.e. translation as well as rotational. Quality improvement of up to 21 dB is achieved in the stabilised output videos.																	1751-9659	1751-9667				FEB 7	2020	14	2					376	383		10.1049/iet-ipr.2019.0764													
J								No-reference image quality assessment via structural information fluctuation	IET IMAGE PROCESSING										feature extraction; visual perception; image processing; regression analysis; support vector machines; structural features; IQA model; natural image; SCI; reference image quality assessment; structural information fluctuation; meaningful research topic; high-quality image; image structural information; human visual system; IQA method; human visual perception; screen content image; structural information analysis; GFM; grey-scale fluctuation direction map		Image quality assessment (IQA) is a meaningful research topic to meet the increasing demand of high-quality image. The degradation of image quality will cause changes in image structural information. Meanwhile, human visual system is sensitive to changes in structural information. This finding motivates us to utilise structural information for proposing IQA method which is consistent with human visual perception. Recently, IQA methods are mainly focused on individual image type, e.g. natural image or screen content image (SCI), thus, the authors proposed a novel no-reference IQA method which can be suitable for both natural image and SCI. The proposed method is based on structural information analysis. For each image, they first obtain the grey-scale fluctuation maps (GFMs) in four detection directions. After that, the grey-scale fluctuation direction map (GFD) of certain image can be acquired via its GFMs. Based on the GFMs and GFD, the structural features of each image are extracted, and then collected and transformed to feature vectors. Subsequently, the IQA model is trained by support vector regression. The experimental results on the public databases demonstrate the proposed method can predict image quality accurately for both natural image and SCI, and the performance is competitive with prevalent methods.																	1751-9659	1751-9667				FEB 7	2020	14	2					384	396		10.1049/iet-ipr.2019.0750													
J								New video encryption schemes based on chaotic maps	IET IMAGE PROCESSING										video coding; cryptography; video streaming; data compression; new video encryption schemes; chaotic maps; transferred video streams; unauthorised access; malicious users; encryption algorithms; secure video transmission; video frames; substitution step; permutation step; diffusion requirements; secure encryption scheme; video file; authors; MPEG-2 standard; compression technique; block size; shuffling process; smaller blocks; processing time; peak-signal-to-noise ratio; encrypted frame; encryption time; different tradeoffs; secret encryption key		Two new encryption algorithms for secure video transmission are proposed in this paper. The two algorithms employ different types of chaotic maps to generate the keystream for encrypting the video frames. Both algorithms involve a substitution step and a permutation step to achieve confusion and diffusion requirements. For efficient transmission, the video file is compressed before being encrypted. In the basic implementation of both algorithms, MPEG-2 standard is used for compression. However, the algorithms are shown to be compliant with other compression techniques. In the permutation step, the effect of the block size used in the shuffling process is examined. Smaller blocks result in increasing the processing time, while reducing both the correlation between adjacent pixels and the peak-signal-to noise ratio of an encrypted frame. The use of a Feistel structure is investigated to enhance security and its negative impact on the encryption time is demonstrated. The experimental results of the two proposed schemes confirm that they represent different tradeoffs between security and computational efficiency. Both schemes are sensitive to slight variations in the encryption key as apparent from the obtained differential measures. The conducted comparative study shows the competitiveness of the proposed schemes to existing schemes in literature.																	1751-9659	1751-9667				FEB 7	2020	14	2					397	406		10.1049/iet-ipr.2018.5250													
J								Scalable Bayesian preference learning for crowds	MACHINE LEARNING												We propose a scalable Bayesian preference learning method for jointly predicting the preferences of individuals as well as the consensus of a crowd from pairwise labels. Peoples' opinions often differ greatly, making it difficult to predict their preferences from small amounts of personal data. Individual biases also make it harder to infer the consensus of a crowd when there are few labels per item. We address these challenges by combining matrix factorisation with Gaussian processes, using a Bayesian approach to account for uncertainty arising from noisy and sparse data. Our method exploits input features, such as text embeddings and user metadata, to predict preferences for new items and users that are not in the training set. As previous solutions based on Gaussian processes do not scale to large numbers of users, items or pairwise labels, we propose a stochastic variational inference approach that limits computational and memory costs. Our experiments on a recommendation task show that our method is competitive with previous approaches despite our scalable inference approximation. We demonstrate the method's scalability on a natural language processing task with thousands of users and items, and show improvements over the state of the art on this task. We make our software publicly available for future work (https://github.com/UKPLab/tacl2018-preference-convincing/tree/crowdGPPL).																	0885-6125	1573-0565				APR	2020	109	4					689	718		10.1007/s10994-019-05867-2		FEB 2020											
J								Evolutionary music: applying evolutionary computation to the art of creating music	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Music composition; Evolutionary computation; Computational creativity; Review	GENETIC ALGORITHM; GENERATIVE MUSIC; OPTIMIZATION; MELOMICS; SYSTEM; LAW	We present a review of the application of genetic programming (GP) and other variations of evolutionary computation (EC) to the creative art of music composition. Throughout the development of EC methods, since the early 1990s, a small number of researchers have considered aesthetic problems such as the act of composing music alongside other more traditional problem domains. Over the years, interest in these aesthetic or artistic domains has grown significantly. We review the implementation of GP and EC for music composition in terms of the compositional task undertaken, the algorithm used, the representation of the individuals and the fitness measure employed. In these aesthetic studies we note that there are more variations or generalisations in the algorithmic implementation in comparison to traditional GP experiments; even if GP is not explicitly stated, many studies use representations that are distinctly GP-like. We determine that there is no single compositional challenge and no single best evolutionary method with which to approach the act of music composition. We consider autonomous composition as a computationally creative act and investigate the suitability of EC methods to the search for creativity. We conclude that the exploratory nature of evolutionary methods are highly appropriate for a wide variety of compositional tasks and propose that the development and study of GP and EC methods on creative tasks such as music composition should be encouraged.																	1389-2576	1573-7632															10.1007/s10710-020-09380-7		FEB 2020											
J								Inventive approach of path planning mechanism for mobile anchors in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										RSS; Trilateration; Localization; Path planning; Dual Beacon; Wireless sensor networks	LOCALIZATION	Recently, Wireless Sensor Network has become a fast growing research area in the field of communication. Sensor Networks find applications in various areas of communication like acoustic detection, medical monitoring, detection of forest fire, and surveillance of military movements. These applications are essential in order to update the captured data with the locally transmitted information. Quite fittingly, anchor node has been evolved to enable locating the exact position of sensor nodes in the absence of GPS in each node. However, mobile anchor node fills the requirement by covering the sensing area at lesser cost. Localization is an essential issue in wireless sensor networks because many applications require the sensor nodes to know their locations with a high degree of precision. Various localization methods based on mobile anchor nodes have been proposed for assisting the sensor nodes to determine their locations. However, none of these methods attempt to optimize the trajectory of the mobile anchor node. Generally, with regard to localization algorithm, only single mobile anchor is customized for identifying the location and to receive the data by sensing. In this paper, a mechanism with two mobile anchors is proposed by using specified exact trajectory known as Z-curve. In this method, two mobile anchors are designed to move along both the directions for localizing the sensor node and transmit/broadcast the data sensed. Hence the use of two mobile anchors attains optimal localization in comparatively shorter duration. The simulation results derived from this study proves better performance of the proposed path planning method than the existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-01752-2		FEB 2020											
J								An analytical framework for distributed and centralized mobility management protocols	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed Mobility Management; Proxy Mobile IPv6; Modeling and analysis; Performance evaluation	LOCATION UPDATE; COST-ANALYSIS; PERFORMANCE; IPV6	Proxy Mobile IPv6 (PMIPv6) maintains the mobility management of mobile users without involving them in the signaling of mobility process. The main limitations of PMIPv6 are the high latency and packet loss. Consequently, IETF has addressed these limitations by standardizing Fast Handover for Proxy Mobile IPv6 (PFMIPv6) protocols. The whole processes of PMIPv6 and PFMIPv6 protocols, including mobility management and connectivity needs, are based on a centralized and static mobility anchor. Therefore, the centralized anchor usually suffers from enormous burdens and hence degradation in performance, scalability, and reliability of the network. Lately, Distributed Mobility Management (DMM) solution is introduced based on PMPv6 to tackle the issue of relying on a single entity. Analyzing and investigating the performance of these centralized and distributed solutions depends on traffic characteristics and user mobility model. Accordingly, we propose through these two factors an analytical framework to evaluate the handover performance of PMIPv6, PFMIPv6 and DMM in vehicular environment. Our analysis and experimental validation are very significant to determine the impacts of different network parameters on the handover performance of these protocols to facilitate decision making on which analytical framework must be adopted in a network. Analytical results demonstrate that there is a trade-off between network parameters and handover performance metrics. PFMIPv6 is the most suited protocol for low to high mobility scenarios in term of handover performance.																	1868-5137	1868-5145															10.1007/s12652-020-01749-x		FEB 2020											
J								CLOES: cross-layer optimal energy scheduling mechanism in a smart distributed multi-microgrid system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimal energy management; Energy trading; Sequential interactions; Multi-microgrids	MANAGEMENT-SYSTEM; GENERATION; NETWORK; SUPPORT	Optimal scheduling of multi-microgrids is one of the important tasks in multi-microgrid operations as it is an effective way to enhance operational and economic performance. However, with the presence of varied distributed dispatchable and non-dispatchable generation and loads in different microgrids make the optimal scheduling very challenging. This paper presents a cross-layer optimal energy scheduling (CLOES) mechanism in a multimicrogrid system, covering different elements of a community in a smart city such as industry, commercial/office, single residence and multi-dwelling unit with their varied nature of distributed generations and loads. The cross-layer sequential coordinated operations are performed between two layer i.e. lower and upper layer. The lower layer consists of different microgrids, whereas the upper layer comprises distribution system operator. The cross-layer sequential interactions between upper and lower layer lead to an optimal energy scheduling for each microgrid which is essential for the reliability of a multi-microgrid system. The importance of internal and external trading prices is described in a unique way for energy trading in a multi-microgrid system. The simulation result and discussions show the effectiveness of the proposed CLOES in terms of cost reduction in a multi-microgrid system compared to the independent external trading of each individual microgrid with the utility grid.																	1868-5137	1868-5145															10.1007/s12652-020-01745-1		FEB 2020											
J								Dynamic scheduling and congestion control for minimizing delay in multihop wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless Communication; Advanced Network Architecture; Mobile Computing; Delay analysis; Throughput; Congestion control		In the real world the QOS attribute delay is a research dispute that has raised much distress being pragmatic in the multi-hop wireless networks with huge amount of data to be transmitted. The objective is to enhance the throughput and accomplish lesser end-to-end delay at the same time. The prevailing work tries to condense the delay by regulating the packet flow that might control the throughput ratio, as well as congestion is also not considered in the case of multiple flows that share single route path for the transmission of data. In the proposed system, optimal data flow scheduling is conceded out with the function of reducing the end to end delay with maximized throughput ratio by modifiable packet flow ratio. This is performed by offering the novel algorithm stated to as dynamic optimized scheduling and congestion control in wireless network that can defer the optimal performance. This novel approach recovers the throughput and rapidity for wireless networks by altering scheduling scheme with virtual adaptation model. In this scheduling every slot is separated into mini slots to reduce the complexity. To control the congestion again the mini slots are separated as micro slots. Virtual rate adjustment is performed in a diverse manner for multiple flow links that exist within the network environment on the origin of their level of emergency, so as to present individual priority for the separate flow links. This technique succeeds in rendering optimal delay compared to the available research techniques in terms of enhanced performance ratio.																	1868-5137	1868-5145															10.1007/s12652-020-01742-4		FEB 2020											
J								An improved memory adaptive up-growth to mine high utility itemsets from large transaction databases	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Frequent pattern growth; Utility pattern tree; High utility itemset mining; Memory management; Utility pattern growth	EFFICIENT ALGORITHMS; FREQUENT	High utility itemset (HUI) mining identifies an interesting pattern from transactional databases by taking into consider the utility of each in the transaction for instance, margins or profits. Many candidates are generated for HUIs which reduces the mining performance. Frequent pattern-growth (FP-Growth) algorithm was widely used to discover the frequent itemsets using FP-Tree. But, UP-Tree was constructed to store high utility item set. The mining of UP-Tree by FP-Growth extracts high utility itemset and generates too many candidates. So, UP-Growth and UP-Growth(+) was proposed to shorten the candidate itemsets. In UP-Growth, two tactics such as discarding local unpromising items (DLU) and decreasing local node (DLN) were used in FP-Growth. In UP-Growth(+), two strategies such as DLU and its estimated node utilities (DNU) and DLN tools for the nodes of internal UP-Tree by evaluated the descendant nodes (DNN) were incorporated in FP-Growth. However, the UP-Growth and UP-Growth(+) has the problem of poor spatial and temporal localities. Initially, the UP-Tree is created in available main memory then extended to secondary memory when the transaction is large. The storage of data structure in secondary memory and accessibility is not clearly derived in existing algorithms. In this paper, the spatial locality issues of UP-Growth and UP-Growth(+) is solved by rearranging nodes of UP-Tree in a depth first manner and temporal locality issues of UP-Growth and UP-Growth(+) is solved by page blocking technique which reorganizes the execution part of UP-Growth and UP-Growth(+). The computational part is rearranged. In addition to, the memory management strategy is introduced to minimize the space requirement of high utility itemsets. Thus the proposed Improved Adaptive UP-Growth (IAUP-Growth) and Improved Adaptive UP-Growth(+) (IAUP-Growth(+)) overcomes the spatial and temporal locality problem and effectively reduces the memory usage.																	1868-5137	1868-5145															10.1007/s12652-020-01706-8		FEB 2020											
J								Robot acquisition, control and interfacing using multimodal feedback	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Voice recognition; Robot; Multimodal feedback; Human-robot interface; Semantic robotic interface; Welding; Tensile strength		The novel method of robot system is designed to control the robot manipulator by human voice. The sensor signals control the manipulator in a robotic manipulator arm. The purpose of design of this concept is to access the robotic system in hazardous environments. Several experiments are conducted and studied using robot speech with multimodal feedback of different operators as end users. The work was designed and developed to weld the products using speech recognition. Voice data collected and program developed to recognize and store the voice data of different users and interface this data to the robot. The tensile strength was examined using manual and speech recognized robot welding. Trajectory planning algorithms of the robot is generated efficient welding where the end effectors carrying the welding torch trace a path.																	1868-5137	1868-5145															10.1007/s12652-020-01738-0		FEB 2020											
J								Approximate Bayesian reinforcement learning based on estimation of plant	AUTONOMOUS ROBOTS										Model-based Bayesian reinforcement learning; Model uncertainty; Plant estimation; Interpolating model; Robustness of proper policy		This study proposes an approximate parametric model-based Bayesian reinforcement learning approach for robots, based on online Bayesian estimation and online planning for an estimated model. The proposed approach is designed to learn a robotic task with a few real-world samples and to be robust against model uncertainty, within feasible computational resources. The proposed approach employs two-stage modeling, which is composed of (1) a parametric differential equation model with a few parameters based on prior knowledge such as equations of motion, and (2) a parametric model that interpolates a finite number of transition probability models for online estimation and planning. The proposed approach modifies the online Bayesian estimation to be robust against approximation errors of the parametric model to a real plant. The policy planned for the interpolating model is proven to have a form of theoretical robustness. Numerical simulation and hardware experiments of a planar peg-in-hole task demonstrate the effectiveness of the proposed approach.																	0929-5593	1573-7527				MAY	2020	44	5					845	857		10.1007/s10514-020-09901-4		FEB 2020											
J								XCSP3 and its ecosystem	CONSTRAINTS										Format; Modeling	CONSTRAINT LANGUAGE; ALGORITHMS	In this paper, we present a summary of XCSP3, together with its ecosystem. XCSP3 is a format used to build integrated representations of combinatorial constrained problems. Interestingly, XCSP3 preserves the structure of models, by handling arrays of variables and groups/blocks of constraints, which makes it rather unique in the literature. Furthermore, the ecosystem of XCSP3 is well supplied: it includes companion tools (parsers and checkers), a website with a search engine for selecting and downloading instances, and competitions of solvers. The Java-based modeling API, called JvCSP(3), is the last developed piece of this complete production chain.																	1383-7133	1572-9354				APR	2020	25	1-2					47	69		10.1007/s10601-019-09307-9		FEB 2020											
J								Learning to Predict Perceptual Distributions of Haptic Adjectives	FRONTIERS IN NEUROROBOTICS										haptic intelligence; perception; ordinal regression; tactile sensing; predicting probability distributions; haptic adjectives	DIMENSIONS; TOUCH	When humans touch an object with their fingertips, they can immediately describe its tactile properties using haptic adjectives, such as hardness and roughness; however, human perception is subjective and noisy, with significant variation across individuals and interactions. Recent research has worked to provide robots with similar haptic intelligence but was focused on identifying binary haptic adjectives, ignoring both attribute intensity and perceptual variability. Combining ordinal haptic adjective labels gathered from human subjects for a set of 60 objects with features automatically extracted from raw multi-modal tactile data collected by a robot repeatedly touching the same objects, we designed a machine-learning method that incorporates partial knowledge of the distribution of object labels into training; then, from a single interaction, it predicts a probability distribution over the set of ordinal labels. In addition to analyzing the collected labels (10 basic haptic adjectives) and demonstrating the quality of our method's predictions, we hold out specific features to determine the influence of individual sensor modalities on the predictive performance for each adjective. Our results demonstrate the feasibility of modeling both the intensity and the variation of haptic perception, two crucial yet previously neglected components of human haptic perception.																	1662-5218					FEB 6	2020	13								116	10.3389/fnbot.2019.00116													
J								Incremental attribute reduction with rough set for dynamic datasets with simultaneously increasing samples and attributes	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Attribute reduction; Dynamic datasets; Discernibility relation; Incremental mechanism; Rough set	FEATURE-SELECTION; ALGORITHM	Attribute reduction with rough set is a popular data analysis methodology for data dimensionality reduction. For dynamic datasets, the existing research has mainly focused on incremental attribute reduction with increasing samples (rows) or attributes (columns), but there is hardly any further research on attribute reduction for dynamic datasets with simultaneously increasing samples and attributes. This paper presents a novel incremental algorithm for attribute reduction with rough set. Firstly, the definition of discernibility relation is proposed based on the improved discernibility matrix. Then, the incremental mechanisms of samples and attributes are studied in terms of discernibility relation under a unified framework. On the basis of two incremental mechanisms, a unified incremental mechanism is introduced for dynamic datasets with simultaneously increasing samples and attributes, and the incremental algorithm is developed according to the unified incremental mechanism. The proposed algorithm has the solid mathematical foundation, which is also suitable for datasets with massive samples and attributes. Finally, compared experimentally with other algorithms, the efficiency of the developed incremental algorithm is demonstrated in terms of running time.																	1868-8071	1868-808X				JUN	2020	11	6					1339	1355		10.1007/s13042-020-01065-y		FEB 2020											
J								Swarm intelligence based centralized clustering: a novel solution	JOURNAL OF INTELLIGENT MANUFACTURING										Swarm intelligence (SI)-based centralized clustering solutions; Wireless sensor networks (WSNs); Manufacturing optimization; Scalability; Data delivery rate; Energy consumption	WIRELESS SENSOR NETWORKS; ROUTING PROTOCOLS	Recent evolutions of MEMS technology, digital electronics, and wireless communication technologies have made smart environments possible; especially through the apparition or incorporation of sensors. Despite being small-sized, the sensors have paved the way for data collection in the environments where they are applied; including luminosity, gas presence, water content, humidity, pressure, and temperature. This research aims to pragmatically optimize Wireless Sensor Network Localization and Network Coverage issues using nature-inspired algorithms. The specific objective is to establish an optimal nature-inspired algorithm and comparing it with other algorithms regarding the capacity to achieve manufacturing optimization in large WSNs, especially in relation to the parameters of high scalability, data delivery rate, and low-energy consumption. Also, the study seeks to determine the extent to which swarm intelligence (SI)-based centralized clustering solutions (optimal nature-inspired algorithms), compared to other approaches, might optimize the WSN features of localization and network coverage. To determine the solutions' performance, the study involved three scenarios. In scenario 1, the network operational time and the stability of SI-based algorithms were investigated for large WSNs that had the minimum heterogeneity. Imperative to note is that the WSNs on focus had different numbers of nodes, which included 500, 300, and 100. In scenario 2, the motivation was to investigate the SI-based WSN protocols in relation to the parameters of packet delivery, energy conception, and network lifetime for large WSNs. In scenario 3, the performance of SI-based solutions over large WSNs was compared to that which had been reported previously for other algorithms; with the target parameters of comparison being attributes such as packet delivery, energy conception, and network lifetime. From the findings, this study established that SI-based centralized clustering solutions are not only more recent but also exhibit superior performance compared to other algorithms; with the parameters of the amount of data delivered to the BS, energy consumption and scalability on the focus.																	0956-5515	1572-8145															10.1007/s10845-020-01542-9		FEB 2020											
J								Flooding region growing: a new parallel image segmentation model based on membrane computing	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image segmentation; Flooding region growing; Membrane computing; Parallelism; CUDA	NEURAL P SYSTEMS; ALGORITHM	Region-growing (RG) algorithm is one of the most common image segmentation methods used for different image processing and machine vision applications. However, this algorithm has two main problems: (1) high computational complexity and the difficulty of its parallel implementation caused by sequential process of adding pixels to regions; (2) low performance of RG in region with weak edges, due to the use of location and the number of seed points. In this paper, a new model of RG algorithm based on tissue-like P system is proposed to resolve these limitations. In this model, each pixel is modeled by a membrane, and in one step, the similarity of each membrane with its neighbors is computed. Then, all membranes are used as seed points to grow simultaneously in a parallel and flood-like manner. To realize the parallel implementation of the proposed model, Graphic Processing Unit (GPU) and CUDA programming language are used. The evaluation of execution time indicates that the proposed model has better performance than the conventional RG algorithm, its speed-up is about 12.5x. Qualitative and quantitative evaluations of segmentation performance also demonstrate that the proposed method not only does not damage the overall segmentation accuracy, but also it has better results on images with complicated background compared to the state-of-the-art methods.																	1861-8200	1861-8219															10.1007/s11554-020-00949-0		FEB 2020											
J								PCA-based drift and shift quantification framework for multidimensional data	KNOWLEDGE AND INFORMATION SYSTEMS										Principal component analysis; Drift detection; Hellinger distance	LIKELIHOOD ESTIMATOR; HETEROSCEDASTICITY	Concept drift is a serious problem confronting machine learning systems in a dynamic and ever-changing world. In order to manage concept drift it may be useful to first quantify it by measuring the distance between distributions that generate data before and after a drift. There is a paucity of methods to do so in the case of multidimensional numeric data. This paper provides an in-depth analysis of the PCA-based change detection approach, identifies shortcomings of existing methods and shows how this approach can be used to measure a drift, not merely detect it.																	0219-1377	0219-3116				JUL	2020	62	7					2835	2854		10.1007/s10115-020-01438-3		FEB 2020											
J								Cross-layer congestion control of wireless sensor networks based on fuzzy sliding mode control	NEURAL COMPUTING & APPLICATIONS										Wireless sensor networks (WSNs); Internet of Things (IoT); Cross-layer congestion control; Fuzzy sliding mode control; NS-2; 35	CONTROL PROTOCOLS	Wireless sensor networks (WSNs) act as a building block of Internet of Things and have been used in various applications to sense environment and transmit data to the Internet. However, WSNs are very vulnerable to congestion problem, resulting in higher packet loss ratio, longer delay and lower throughput. To address this issue, this paper presents a fuzzy sliding mode congestion control algorithm (FSMC) for WSNs. Firstly, by applying the signal-to-noise ratio of wireless channel to TCP model, a new cross-layer congestion control model between transmission layer and MAC layer is proposed. Then, by combining fuzzy control with sliding mode control (SMC), a fuzzy sliding mode controller (FSMC) is designed, which adaptively regulates the queue length of buffer in congested nodes and significantly reduces the impact of external uncertain disturbance. Finally, numerous simulations are implemented in MATLAB/Simulink and NS-2.35 by comparing with traditional control strategies such as fuzzy, PID and SMC, which show that the proposed FSMC effectively adapts to the change of queue length and has good performance, such as rapid convergence, lower average delay, less packet loss ratio and higher throughput.																	0941-0643	1433-3058				SEP	2020	32	17					13505	13520		10.1007/s00521-020-04758-1		FEB 2020											
J								Bi-ideal approximation spaces and their applications	SOFT COMPUTING										Rough sets; Lower and upper approximations; Ideals	ROUGH SETS	The original model of rough sets was advanced by Pawlak, which was mainly involved with the approximation of things using an equivalence relation on the universal set of his approximation space. In this paper, two kinds of approximation operators via ideals which represent extensions of Pawlak's approximation operator have been presented. In both kinds, the definitions of upper and lower approximations based on ideals have been given. Moreover, a new type of approximation spaces via two ideals which is called bi-ideal approximation spaces was introduced for the first time. This type of approximations was analyzed by two different methods, their properties are investigated, and the relationship between these methods is proposed. The importance of these methods was its dependent on ideals which were topological tools, and the two ideals represent two opinions instead of one opinion. At the end of the paper, an applied example had been introduced in the chemistry field by applying the current methods to illustrate the definitions in a friendly way.																	1432-7643	1433-7479				SEP	2020	24	17					12989	13001		10.1007/s00500-020-04720-2		FEB 2020											
J								Developed multi-objective grey wolf optimizer with fuzzy logic decision-making tool for direction overcurrent relays coordination	SOFT COMPUTING										Direction overcurrent relays; Optimal coordination; Multi-objective grey wolf optimizer; Fuzzy logic decision-making	ROUTING METHOD; ALGORITHM; NETWORKS; INTERNET; ROBUST	This paper proposes a new methodology for solving the coordination problem of DOCRs based on multi-objective grey wolf optimizer and fuzzy logic decision-making. In addition to the conventional objective function, a new objective function which aims to minimize the discrimination time between primary and backup relays is proposed. Moreover, the conventional objective function related to minimizing the total operating time of primary and backup relays is considered. The feasibility and performance of the proposed methodology for solving the coordination problem of DOCRs are investigated using two different systems (8-bus system and IEEE-30 bus system). The proposed methodology is compared with other reported methods. The results prove the viability and effectiveness of the proposed methodology to solve the DOCR coordination problem without any miscoordination between primary and backup relays.																	1432-7643	1433-7479				SEP	2020	24	17					13305	13317		10.1007/s00500-020-04745-7		FEB 2020											
J								A knowledge-driven layered inverse reinforcement learning approach for recognizing human intents	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Inverse reinforcement learning; knowledge representation; advisor; object affordance	REPRESENTATION; AFFORDANCES	There is a rising trend in exploring the capability of inverse reinforcement learning (IRL) in high dimensional demonstrations. Our aim is to recognise human intents from video data within an IRL framework. For this, we present a two-layered maximum likelihood IRL model. The usefulness of knowledge representation (KR) schemes and availability of advisors at different layers is exploited through this model. Two main aspects are addressed: a. the importance of having abstract high-level information to the IRL framework in terms of semantic object affordance and b. deductively exploring the utility of a state at different temporal abstractions. The effectiveness of the proposed model has been evaluated with the help of standard Cornell Activity Dataset (CAD-120).																	0952-813X	1362-3079															10.1080/0952813X.2020.1718773		FEB 2020											
J								Performance analysis of localisation strategy for island model genetic algorithm in population diversity preservation	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Genetic algorithms; Island model genetic algorithm; localisation strategy; computationally expensive optimisation	ENSEMBLE	The genetic algorithm (GA) is one of the most common solutions to solve many optimisation problems. Its distributed version, Island Model GA (IMGA), was introduced to overcome more complex and scalable cases. However, there is a recurrent problem in IMGA called premature convergence as a consequence of selection in the migration. This process is a mechanism of migrating individuals from one into another island to keep population diversity. The primary cause is the structural similarity of a migrated individual because of the genetic operator configurations are identical. Localised IMGA (LIMGA) tries to implement different island characteristics to avoid premature convergence. The main motivation of this paper is to investigate the performance of LIMGA capability in maintaining population diversity. In detail, the contributions of this research are (1) to prove LIMGA concept in handling general optimisation problem, (2) to analyse the performance LIMGA in diversity preservation, and (3) compare LIMGA performance with the current solvers. By harmonising three different GA cores, LIMGA could overcome computationally expensive functions with a great result and acceptable execution time. Moreover, because of its success in maintaining the diversity, Localised Island Model Genetic Algorithm (LIMGA) could lead to the among other current solvers for this case.																	0952-813X	1362-3079															10.1080/0952813X.2020.1721570		FEB 2020											
J								A Two-Level Function Evaluation Management Model for Multi-Population Methods in Dynamic Environments: Hierarchical Learning Automata Approach	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Dynamic optimisation problems; differential evolution; moving peaks benchmark; evolutionary computation; hierarchical learning automata; function evaluation management	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; HYBRID APPROACH; CUCKOO SEARCH; ALGORITHMS; OPTIMA; PSO	The fitness evaluation (FE) management has been successfully applied to improve the performance of multi-population methods for dynamic optimisation problems (DOPs). In this work, we extend one of its variants to address DOPs which was recently proposed by the authors. The aim of our proposal is to increase the efficiency of the FE management. To this end, we propose a technique based on hierarchical learning automata that manages FEs at two level: at first level the algorithm decides which population should be executed, and at the second level it specifies the operation that should be performed by the selected population. A detailed experimental analysis shows the effectiveness of our proposal.																	0952-813X	1362-3079															10.1080/0952813X.2020.1721568		FEB 2020											
J								A mobile robot mapping model inspired from the place cells functionality of hippocampus based on dimension reduction technique	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Mapping; dimension Reduction; hippocampus; clustering	SIMULTANEOUS LOCALIZATION; GABOR; MAP	In this paper, a new mobile robot mapping algorithm inspired from the functionality of hippocampus cells is presented. Place cells in hippocampus can store a map of the environment. This model fuses odometry and vision data based on dimensionality reduction technique, hierarchically. These two types of data are first fused and then considered as inputs to the place cell model. Place cells do the clustering of places. The proposed Place cell model has two types of inputs: Grid cells input and input from the lateral entorhinal cortex (LEC). The LEC is modelled based on the dimension reduction technique. Therefore, the data that causes locations different to be inserted into the place cell from this layer. Another contribution is proposing a new unsupervised dimension reduction method based on k-means. The method can find perpendicular independent dimensions. Also, the distance of cluster centres found in these dimensions is maximised. The method was compared with LDA and PCA in standard functions. Although LDA is a supervised method, the result showed that the proposed unsupervised method outperformed. To evaluate the place cells model, sequences of images collected by a mobile robot was used and similar results to real place cells achieved.																	0952-813X	1362-3079															10.1080/0952813X.2020.1721569		FEB 2020											
J								Cross-spectral registration of natural images with SIPCFE	MACHINE VISION AND APPLICATIONS										Cross-spectral registration; Phase congruency; Near infrared; Feature-based image registration	MUTUAL-INFORMATION; MAXIMIZATION; DESCRIPTOR	Image registration is a viable task in the field of computer vision with many applications. When images are captured under different spectrum conditions, a challenge is imposed on the task of registration. Researchers carefully handcraft a local module insensitive to illumination changes across cross-spectral image pairs to tackle this challenge. We, in this paper, develop an optimized feature-based approach Single Instance Phase Congruency Feature Extractor (SIPCFE) to tackle the problem of natural cross-spectral image registration. SIPCFE uses the phase information of an image pair to quickly identify and describe reliable keypoints that are insensitive to illumination. It then employs a sequence of outlier removal processes to find the matching feature points accurately and the Direct Linear Transformation to estimate the geometric transformation to align the image pair. We extensively study the proposed approach for every module in the system to give more insights into the challenges. We benchmark our proposed method and other state-of-the-art feature-based methods developed for cross-spectral imagery on three datasets with various settings and image contents. The comprehensive analysis of cross-spectral registration results of natural images demonstrates that SIPCFE achieves up to 47.24%, 14.29%, and 12.45% accuracy improvement on the first, second, and third dataset, respectively, over the second best registration method in the benchmark.																	0932-8092	1432-1769				FEB 6	2020	31	1							10	10.1007/s00138-020-01057-6													
J								Finite-Time and Fixed-Time Non-chattering Control for Inertial Neural Networks with Discontinuous Activations and Proportional Delay	NEURAL PROCESSING LETTERS										Discontinuous activation functions; Finite-time and fixed-time synchronization; Inertial neural networks; Non-chattering control; Proportional delay	SYNCHRONIZATION ANALYSIS; GLOBAL CONVERGENCE; DYNAMICAL NETWORKS; STABILITY; STABILIZATION; DIFFERENTIATION; DESIGN	Based on the framework of Filippov solutions, this paper considers synchronization of inertial neural networks (INNs) with discontinuous activation functions and proportional delay. By designing several non-chattering controllers, both finite-time and fixed-time synchronization are studied. The designed controllers are simple to be implemented and can overcome the effects of both nonidentical uncertainties of Filippov solutions and the proportional delay without inducing any chattering. By designing new Lyapunov functionals and utilizing 1-norm methods, several sufficient conditions are obtained to ensure that the INNs achieve drive-response synchronization in finite time and fixed time, respectively. Moreover, the settling time is estimated for the two types of synchronization. Simulations are provided to illustrate the effectiveness of theoretical analysis.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2337	2353		10.1007/s11063-020-10199-7		FEB 2020											
J								Content semantic image analysis and storage method based on intelligent computing of machine learning annotation	NEURAL COMPUTING & APPLICATIONS										Intelligent computing; Machine learning; Image annotation; Visual dictionary	RETRIEVAL; FUSION	With the popularity of computers and the rapid development of various application platforms, the explosive growth of data poses a huge challenge to data analysis and storage. For large-scale image analysis applications, the time delay in storing read data becomes an important issue that constrains this application. The semantic information asymmetry between image application and storage is the root cause of this problem. In view of content semantic analysis, in recent years, intelligent computing has become the main research direction. Among them, machine learning has become a research hot spot because of its offline learning and online generation characteristics. For the semantics of image content, machine learning can complete tasks such as content semantic association, classification, annotation and hash mapping, and provide algorithm support for applying image semantics and improving semantic analysis ability in large-scale environment. Image annotation is an important topic in the semantic analysis of image content. Annotation can establish a classification relationship between image content and semantics. In order to solve the problem of extracting a large amount of data in large-scale image analysis, a content semantic image content analysis and storage scheme based on intelligent computer learning image annotation is proposed. Combined with DSTH work, the program introduces deep learning, visual lexicon and map metadata. Hash semantic metadata supplemental metadata is obtained through deep learning, and semantic metadata is constructed and managed in a hierarchical structure. In addition, according to the characteristics of the graph structure, by improving the PageRank algorithm, the SemRank node ranking algorithm based on Hamming distance is proposed. Experimental results demonstrate the effectiveness and reliability of the algorithm.																	0941-0643	1433-3058				APR	2020	32	7			SI		1813	1822		10.1007/s00521-020-04739-4		FEB 2020											
J								A data-driven operational integrated driving behavioral model on highways	NEURAL COMPUTING & APPLICATIONS										Integrated driving; Neural network; Social force; Lane changing	CAR-FOLLOWING MODEL; FORCE MODEL; MICROSCOPIC SIMULATION; FEATURES; VEHICLE	Car-following (CF) and lane-changing (LC) behavior models have been widely studied separately as the core models of traffic simulation. However, in practice, CF and LC are inseparable and thus integrated driving (ID) models containing CF and LC behaviors emerge. Here, we proposed a new work to introduce the social force (SF) model to the operational ID behavioral model on the highway. First, a data-driven-based operational ID behavioral model is proposed in the hierarchical social force behavioral model framework. Then, the inputs/output of the SF-ID behavioral model is determined. SF-ID model is built by the feed-forward neural networks (FNN), and the network structure and other parameters are calibrated and verified by field data. Results of the test on CF and LC situations show that our proposed FNN SF-ID model has a good capability in reproducing/predicting the operational ID behaviors on the highway. In addition, we also analyzed the structural features of the FNN SF-ID models, and refine the original models by removing insignificant inputs. The comparison results showed that the refined model-FNN SF-ID (R)-performed better than the original models.																	0941-0643	1433-3058				AUG	2020	32	16					13017	13033		10.1007/s00521-020-04746-5		FEB 2020											
J								Deep neural network-based clustering technique for secure IIoT	NEURAL COMPUTING & APPLICATIONS										Power demand; IIoT; System's security capacity; Clustering; Deep learning	SECRECY CAPACITY; POWER ALLOCATION; COMMUNICATION; EFFICIENT; HML	The advent of Industrial Internet of Things (IIoT) has determined the proliferation of smart devices connected to the Internet and injected a vast amount of data into it, which may undergo many computational stages at several clusters. On the one hand, the benefits brought by these technologies are well known; however, in the envisaged scenario, the exposure of data, services and infrastructures to malicious attacks has definitely grown. Even a single breach on any of the links of the data-service-infrastructure chain may seriously compromise the security of the end-user application. Therefore, the logical and smart clustering while satisfying security and reliability is a key issue for IIoT networks. A novel clustering method proposed based on power demand assures security of data information in IIoT-based applications. First, security capacity of the system is calculated from mutual information of primary channel and eavesdropping channel. Then, under the maximum transmit power constraint, an optimal transmit power is found based on deep learning technique, which maximizes security capacity of the system. Finally, the network is clustered according to the calculated power demand. Experimental results accredit the proposed method has higher security and reliability, as well as lower network time overhead and power consumption.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16109	16117		10.1007/s00521-020-04763-4		FEB 2020											
J								Enhanced robustness of convolutional networks with a push-pull inhibition layer	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Image corruption; Network robustness; Neuron response inhibition; push-pull layer	ORIENTATION SELECTIVITY; RECEPTIVE-FIELDS; SIMPLE CELL; SUPPRESSION; BROAD; MODEL	Convolutional neural networks (CNNs) lack robustness to test image corruptions that are not seen during training. In this paper, we propose a new layer for CNNs that increases their robustness to several types of corruptions of the input images. We call it a 'push-pull' layer and compute its response as the combination of two half-wave rectified convolutions, with kernels of different size and opposite polarity. Its implementation is based on a biologically motivated model of certain neurons in the visual system that exhibit response suppression, known as push-pull inhibition. We validate our method by replacing the first convolutional layer of the LeNet, ResNet and DenseNet architectures with our push-pull layer. We train the networks on original training images from the MNIST and CIFAR data sets and test them on images with several corruptions, of different types and severities, that are unseen by the training process. We experiment with various configurations of the ResNet and DenseNet models on a benchmark test set with typical image corruptions constructed on the CIFAR test images. We demonstrate that our push-pull layer contributes to a considerable improvement in robustness of classification of corrupted images, while maintaining state-of-the-art performance on the original image classification task. We released the code and trained models at the url .																	0941-0643	1433-3058															10.1007/s00521-020-04751-8		FEB 2020											
J								Transfer learning privileged information fuels CAD diagnosis of breast cancer	MACHINE VISION AND APPLICATIONS										Learning using privileged information (LUPI); Breast cancer; Ultrasound (US); Breast mammography; Deep learning (DL); Feature representation	CONVOLUTIONAL NEURAL-NETWORK; PARTICLE SWARM OPTIMIZATION; ULTRASOUND IMAGES; MAMMOGRAPHY; PLUS; CLASSIFIER; ALGORITHM; SVM; SELECTION; ENSEMBLE	The efficiency in breast cancer from imaging-based computer-aided diagnosis (CAD) has been revealed in recent years. As a fact, the methods grounded on a single modality constantly lack behind multimodal CAD imaging. However, owing to the restrictions of imaging devices, expressly in rural hospitals, single-modal imaging becomes a favorite in clinical practice for diagnosis. A fresh learning model trending nowadays known as learning using privileged information (LUPI) adopts additional privileged information (PI) modality to help during the training stage, but PI does not contribute in the testing stage. Meanwhile, the link exists between PI and training samples; the same is then reassigned to the learned model. We propose a LUPI-based CAD framework for breast cancer using privileged information in this work. The work offers both a classifier- or feature-level LUPI, in which the information is shifted from the additional PI modality to the diagnosis modality. A thorough comparison has been made among six classifier-level algorithms and six feature-level LUPI algorithms. The experimental results on both the acquired primary datasets show that all classifier-level and deep learning-based feature-level LUPI algorithms can enhance the performance of a single-modal imaging-based CAD for breast cancer by relocating PI.																	0932-8092	1432-1769				FEB 5	2020	31	1-2							9	10.1007/s00138-020-01058-5													
J								The effect of downsampling-upsampling strategy on foreground detection algorithms	ARTIFICIAL INTELLIGENCE REVIEW										Foreground detection; Video size downsampling; Interpolation techniques	GAUSSIAN MIXTURE MODEL; BACKGROUND SUBTRACTION; VISUAL SURVEILLANCE; IMAGE; FEATURES; TRACKING; MOTION	In video surveillance systems which incorporate stationary cameras, the first phase of movement object detection is crucial for the correct modelling of the behavior of these objects, as well as being the most complex in terms of execution time. There are many algorithms that provide a reliable and adequate segmentation mask, obtaining real-time ratios for reduced image sizes. However, due to the increased performance of camera hardware, the application of previous methods to sequences with higher resolutions (from 640 x 480 to 1920 x 1080) is not carried out in real time, compromising their use in real video surveillance systems. In this paper we propose a methodology to reduce the computational requirements of the algorithms, consisting of a reduction of the input frame and, subsequently, an interpolation of the segmentation mask of each method to recover the original frame size. In addition, the viability of this meta-model is analyzed together with the different selected algorithms, evaluating the quality of the resulting segmentation and its gain in terms of computation time.																	0269-2821	1573-7462				OCT	2020	53	7					4935	4965		10.1007/s10462-020-09811-y		FEB 2020											
J								Multi-objective genetic programming for manifold learning: balancing quality and dimensionality	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Manifold learning; Genetic programming; Dimensionality reduction; Feature construction	FEATURE CONSTRUCTION; FEATURE-SELECTION; CLASSIFICATION; CLASSIFIERS	Manifold learning techniques have become increasingly valuable as data continues to grow in size. By discovering a lower-dimensional representation (embedding) of the structure of a dataset, manifold learning algorithms can substantially reduce the dimensionality of a dataset while preserving as much information as possible. However, state-of-the-art manifold learning algorithms are opaque in how they perform this transformation. Understanding the way in which the embedding relates to the original high-dimensional space is critical in exploratory data analysis. We previously proposed a Genetic Programming method that performed manifold learning by evolving mappings that are transparent and interpretable. This method required the dimensionality of the embedding to be known a priori, which makes it hard to use when little is known about a dataset. In this paper, we substantially extend our previous work, by introducing a multi-objective approach that automatically balances the competing objectives of manifold quality and dimensionality. Our proposed approach is competitive with a range of baseline and state-of-the-art manifold learning methods, while also providing a range (front) of solutions that give different trade-offs between quality and dimensionality. Furthermore, the learned models are shown to often be simple and efficient, utilising only a small number of features in an interpretable manner.																	1389-2576	1573-7632				SEP	2020	21	3			SI		399	431		10.1007/s10710-020-09375-4		FEB 2020											
J								ACIAR: application-centric information-aware routing technique for IOT platform assisted by wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Decision making; IoT; Information dissemination; Route discovery; WSN	INTERNET; ALGORITHM; ENERGY; WSN; OPTIMIZATION	Wireless sensor network (WSN) is integrated with internet of things (IoT) for providing feasible solution towards data handling and information access, in a ubiquitous manner. The operations of the IoT platform are backboned by the sensor nodes to meet the end-user application demands. Contrarily, the intrinsic behavior and resource constraint nature of the nodes degrade the performance of the IoT application and services. To suppress the issues in WSN communication due to energy and data handling, application-centric information-aware routing (ACIAR) technique is introduced in this article. This routing technique focuses on route discovery and information handling with the aid of iterative decision process and weighted neighbor selection to grant seamless support for IoT data requirements. The process of weighted neighbor selection is adaptable depending on the requirement and run-time of the application ensuring unanimous data support from the sensor network using ACIAR. The performance of the proposed routing technique is estimated using the metrics throughput, delay, data loss, transmission and residual energy.																	1868-5137	1868-5145															10.1007/s12652-020-01748-y		FEB 2020											
J								Transfer-robot task scheduling in flexible job shop	JOURNAL OF INTELLIGENT MANUFACTURING										Robot task scheduling; Flexible job shop; Simultaneous scheduling; Constraint programming	GENETIC ALGORITHM; TABU SEARCH; TIME-WINDOW; MACHINES; TAXONOMY; ALLOCATION; SYSTEM	This paper studies a simultaneous scheduling of production and material transfer in a flexible job shop environment. The simultaneous scheduling approach has been recently adopted by a robotic mobile fulfillment system, wherein transbots pick up jobs and deliver to pick-stations for processing, which requires a simultaneous scheduling of jobs, transbots, and stations. Two different constraint programming formulations are proposed for the first time for a flexible job shop scheduling problem with transbots, significantly outperforming all other benchmark approaches in the literature and proving optimality of the well-known benchmark instances.																	0956-5515	1572-8145				OCT	2020	31	7			SI		1783	1793		10.1007/s10845-020-01537-6		FEB 2020											
J								Analysis of production cycle-time distribution with a big-data approach	JOURNAL OF INTELLIGENT MANUFACTURING										DP-RBFN; RBFN; Cycle time (CT); Computer components manufacturing; CT forecasting; Parallel training		In production planning, one of the most crucial issues involves cycle time forecasting and distribution. Particularly, the parameter aids in realizing high delivery reliability. In the production planning process that involves computer component manufacturing, an estimation of the tasks' cycle time offers an important basis for dispatching control, material purchase, and due date assignment. In this study, a big-data approach was proposed and examined to determine how it could be used to predict cycle time distribution. Also, the research context involved computer components manufacturing systems. Indeed, the motivation was to determine how the proposed mechanism could improve delivery reliability in manufacturing systems.Regarding the implementation and design of the CT forecasting system, with the proposed DP-RBFN framework being a model to be implemented in computer components manufacturing, components of the system constituted three major parts. The first part, being the basic platform, played the role of Hadoop series software installation. This installation had its role lie in enabling the parallel computing of big data. Another part of the framework design and implementation involved data preprocessing. In this case, the role of the data preprocessing procedure lay in the extraction, transformation, and loading of data to CTF. The third part that followed the basic platform design and data preprocessing procedure involved CT forecasting. Results demonstrated that the proposed model performs superiorly than the contrast or other comparative methods on both the computer components manufacturing system dataset and benchmark datasets. From the findings, the proposed framework (DP-RBFN) exhibited superior performance compared to previous performance outcomes that had been reported relative to the use of the RBFN algorithm. These findings held for both MAD and SD-relative to the selected datasets.																	0956-5515	1572-8145															10.1007/s10845-020-01544-7		FEB 2020											
J								Enactive hermeneutics and smart medical technologies	AI & SOCIETY										Mind; Machine learning; Preconscious cognitive processes; Memory; AI; Enactive hermeneutics; Material hermeneutics		Embodied cognition is an interpretative-or hermeneutical-cognition inherent in motor-sensory perception intrinsically informed by biological and sociocultural memory, a cognition embedded in the organism as well as the socio-cultural environment interacting with it (Ward et al. TOPOI 36:365-375, 2017), of which technologies are a part. Yet, smart machines are advancing on human abilities to perceive and interpret concerning the accuracy, quantity, and quality of the data processed. Machines process and categorize images, perform classification tasks, they calculate and perform pattern analysis, all machine learning processes are task-specific. Machine learning processes resemble human interpretative processes; however, these are two very different ways of "dealing with something," although both can be said to be hermeneutical, one enactive, the other material; the first is a meaning-generating interpretative process, the second a statistics-based information output. The information extracted from all the data fed into the computer is the end-product of machine learning, whereas human interpretative enactments are continuous and necessary for any application of the information output.																	0951-5666	1435-5655															10.1007/s00146-020-00944-w		FEB 2020											
J								A computational algorithm based on biogeography-based optimization method for computing power system security constrains with multi FACTS devices	COMPUTATIONAL INTELLIGENCE										biogeography based optimization; FACTS device; unified power flow controller; classical BBO; IEEE-30 bus test system	CONGESTION MANAGEMENT; OPTIMAL LOCATION; FLOW CONTROLLER; HEALTH-CARE; IDENTIFICATION; IOT	A major role is played by the analysis of power system security in heightening system security and in system collapse condition avoidance. This article presents a cutting edge mechanism which is devised applying transmission line loadings as well as variance in bus voltage magnitude. The use of flexible alternating current transmission systems devices improves the objectives of generation fuel charges in addition to the severity index proposed which were investigated considering the contingency circumstances of generator(s) or/and transmission channel(s). To boost system security in spite of contingency circumstances in the existence of unified power flow controller or UPFC, it would be most appropriate to pinpoint a most advantageous position to install aforementioned device. We propose a model of UPFC where power insertion is done by using voltage source. Also a procedure to incorporate the same and a strategy to find optimum position has been proposed which uses line overload sensitivity indices. This work mainly focused on establishment of available transfer capability on the heavily congested line. The proposed congestion management scheme alleviates the heavy stress in transmission line and provides an ample corridor for the power to flow. Biogeography-based optimization or BBO in short, is a technique which is a growing recognized optimization method which has been lucratively engaged in solving intricate optimization problem in dissimilar fields. The BBO provides better results than the metaheuristic counter parts such as Genetic Algorithm and Particle Swarm Optimization. The effectiveness of proposed BBO has been tested on standard IEEE 30 bus system and the results are compared with classic methods and other metaheuristic methods. This is established through the MATLAB package. Improved bus voltage profile was also attained and it can be inferred from the outcome that the prospective approach can drastically enhance security of power system when comparing with other optimization methods.																	0824-7935	1467-8640															10.1111/coin.12282		FEB 2020											
J								Much faster cross-validation in PLSR-modelling by avoiding redundant calculations	JOURNAL OF CHEMOMETRICS										classification; kernel matrix; regression; subspace modelling; validation	CONJUGATE GRADIENTS; REGRESSION	A novel formulation of the wide kernel algorithm for partial least squares regression (PLSR) is proposed. We show how the elimination of redundant calculations in the traditional applications of PLSR helps in speeding up any choice of cross-validation strategy by utilizing precalculated lookup matrices. The proposed lookup approach is combined with some additional computational shortcuts resulting in highly effective and numerically accurate cross-validation results. The computational advantages of the proposed method are demonstrated by comparisons to the classical NIPALS and the bidiag2 algorithms for calculating cross-validated PLSR models. Problems including both one and several responses, double/nested cross-validated, and one-vs-all classification are among the considered applications.																	0886-9383	1099-128X				MAR	2020	34	3			SI				e3201	10.1002/cem.3201		FEB 2020											
J								Ranking and multicriteria decision making in optimization of raspberry convective drying processes	JOURNAL OF CHEMOMETRICS										bioactive compounds; deep ranking analysis by power eigenvectors; freeze-drying; physico-mechanical properties; sum of ranking differences	ANTIOXIDANT ACTIVITY; BIOACTIVE COMPOUNDS; DEGRADATION KINETICS; AIRBORNE ULTRASOUND; OSMOTIC DEHYDRATION; RED RASPBERRY; ANTHOCYANIN; COLOR; BLACKBERRY; STRAWBERRY	Serbia is one of the leading producers and exporters of raspberry in the world, and considering the short shelf life of raspberry, the processing, storage, and transport are some of the main issues to be addressed. A comparative experiment was conducted in order to find the suitable process parameters for convective drying that may be considered as the alternatives to freeze-drying, which is a widely used preservation method for raspberry even though it is a costly and energy-consuming method. Twelve convective drying regimens were applied with a combination of three influencing factors: air temperature (60 degrees C, 70 degrees C, and 80 degrees C), air rate (0.5 and 1.5 m center dot s(-1)), and stage of raspberry (fresh and frozen). The final product, a dried raspberry, was assessed for chemical, physical, and mechanical properties and rehydration capacity. Deep ranking analysis by power eigenvectors (DRAPE) and sum of ranking differences (SRD) were used to uncover the differences and similarities between the applied drying methods. SRD showed that convective drying of fresh raspberries proved to be more similar to freeze-dried raspberries than convective drying of frozen ones. Fresh samples dried at 60 degrees C air temperature and 1.5 m center dot s(-1) air flow proved to be the most similar to the reference freeze-drying method. This convective regimen gives samples with the lowest color change, shrinkage, and shape deformation. With the mechanical and chemical properties of these samples being observed, statistical Duncan's test show that there is no significant difference (P < .05) in terms of hardness, shear force resistance, total phenolic, and total flavonoid preservation, compared with freeze-dried samples. DRAPE gave similar results, but it added the variable importance in ranking as well, and total phenol reduction was defined as the most important variable. These results can help practitioners to develop cheaper and simpler drying methods that would replace the freeze-drying but keep the same quality of the dried products.																	0886-9383	1099-128X														e3224	10.1002/cem.3224		FEB 2020											
J								Nonlinear system modeling using self-organizing fuzzy neural networks for industrial applications	APPLIED INTELLIGENCE										Nonlinear system modeling; Self-organizing fuzzy neural network; Neuronal activity; Singular value decomposition; Adaptive learning algorithm	PARTICLE SWARM OPTIMIZATION; PREDICTION; IDENTIFICATION; ALGORITHM; NITROGEN	In this paper, a novel self-organizing fuzzy neural network with an adaptive learning algorithm (SOFNN-ALA) for nonlinear system modeling and identification in industrial processes is proposed. To efficiently enhance the generalization capability, the proposed SOFNN-ALA is designed by using both structure identification and parameter estimation simultaneously in the learning process. In the structure identification phase, the rule neuron with the highest neuronal activity will be split into two new rule neurons. Meanwhile, the redundant rule neurons with small singular values will be removed to simplify the network structure. In the parameter estimation phase, an adaptive learning algorithm (ALA), which is designed based on the widely used Levenberg-Marquardt (LM) optimization algorithm, is adopted to optimize the network parameters. The ALA-based learning algorithm can not only speed up the convergence speed but also enhance the modeling performance. Moreover, we carefully analyze the convergence of the proposed SOFNN-ALA to guarantee its successful practical application. Finally, the effectiveness and efficiency of the proposed SOFNN-ALA is validated by several examples. The experimental results demonstrate that the proposed SOFNN-ALA exhibits a better comprehensive performance than some other state-of-the-art SOFNNs for nonlinear system modeling in industrial applications. The source code can be downloaded from https://github.com/hyitzhb/SOFNN-ALA.git.																	0924-669X	1573-7497				MAY	2020	50	5					1657	1672		10.1007/s10489-020-01645-z		FEB 2020											
J								Attribute Normalization Approaches to Group Decision-making and Application to Software Reliability Assessment	COGNITIVE COMPUTATION										Normalization; Multi-attribute decision-making; Group decision-making; Normalized projection; Software reliability assessment	THE-ART SURVEY; AGGREGATION OPERATORS; MATERIAL SELECTION; FEMORAL COMPONENT; FUZZY TOPSIS; WEIGHTS; DESIGN; EXTENSION; MODEL; OPTIMIZATION	A group decision-making (GDM) process is a social cognition process, which is a sub-topic of cognitive computation. The normalization of attribute values plays an important role in multi-attribute decision-making (MADM) and GDM problems. However, this research finds that the existing normalization methods are not always reasonable for GDM problems. To solve the problem of attribute normalization in GDM systems, some new normalization models are developed in this paper. An integrative study contributes to cognitive MADM and GDM systems. In existing normalization models, there are some bounds, such as Max(uj),Min(uj), n-ary sumation (uj),and n-ary sumation (uj)2. They are limited to a single attribute vector u(j). The bound of new normalization method proposed in this work is related to one or more attribute vectors, in which the attribute values are graded in the same measure system. These related attribute vectors may be distributed to all decision matrices graded by this decision system. That is, the new bound in developed normalization model is an uniform bound, which is related to a decision system. For example, this uniform bound can be written as one of Max(.),Min(.), n-ary sumation (.), n-ary sumation (.)2\documentclass[12pt]{minimal} (.)<^>{2}}$\end{document}. Some illustrative examples are provided. A practical application to the evaluation of software reliability is introduced in order to illustrate the feasibility and practicability of methods introduced in this paper. Some experimental and computational comparisons are provided. The results show that new normalization methods are feasibility and practicability, and they are superior to the classical normalization methods. This work has provided some new normalization models. These new methods can adapt to all decision problems, including MADM and GDM problems. Some important limitations and future research are introduced.																	1866-9956	1866-9964															10.1007/s12559-019-09707-2		FEB 2020											
J								Discrepancy-Based Theory and Algorithms for Forecasting Non-Stationary Time Series	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Time series; Forecasting; Non-stationary; Non-mixing; Generalization bounds; Discrepancy; Expected sequential covering numbers; Sequential Rademacher complexity	CONVERGENCE; PREDICTION; BOUNDS	We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. Our learning bounds guide the design of new algorithms for non-stationary time series forecasting for which we report several favorable experimental results.																	1012-2443	1573-7470				APR	2020	88	4					367	399		10.1007/s10472-019-09683-1		FEB 2020											
J								An informative path planning framework for UAV-based terrain monitoring	AUTONOMOUS ROBOTS										Informative path planning; Aerial robotics; Environmental monitoring; Remote sensing	GAUSSIAN-PROCESSES; ALGORITHMS; OPTIMIZATION; EXPLORATION	Unmanned aerial vehicles represent a new frontier in a wide range of monitoring and research applications. To fully leverage their potential, a key challenge is planning missions for efficient data acquisition in complex environments. To address this issue, this article introduces a general informative path planning framework for monitoring scenarios using an aerial robot, focusing on problems in which the value of sensor information is unevenly distributed in a target area and unknown a priori. The approach is capable of learning and focusing on regions of interest via adaptation to map either discrete or continuous variables on the terrain using variable-resolution data received from probabilistic sensors. During a mission, the terrain maps built online are used to plan information-rich trajectories in continuous 3-D space by optimizing initial solutions obtained by a coarse grid search. Extensive simulations show that our approach is more efficient than existing methods. We also demonstrate its real-time application on a photorealistic mapping scenario using a publicly available dataset and a proof of concept for an agricultural monitoring task.																	0929-5593	1573-7527				JUL	2020	44	6					889	911		10.1007/s10514-020-09903-2		FEB 2020											
J								Data mining model for food safety incidents based on structural analysis and semantic similarity	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining model; Food safety incidents; Semantic analysis	WILLINGNESS-TO-PAY; INTERNET	Food safety is of vital interest for public health and the stability of society. In this paper, we analyzed the characteristics of food safety incidents (FSIs), including spatial distribution, food categories, risk factors, and supply chain links, reported by mainstream media in China. Based on our analysis, we constructed a semantic template for text data related to FSIs. Furthermore, we introduced a multi-layer, multi-level semantic structure of rank (MMSS-Rank) algorithm to measure the similarity between collected food safety data and the semantic template. We then calculated the overall scores (i.e., text layer weight, semantic template weight, and keyword density matrix) and selected an appropriate threshold to determine the accuracy of the FSI data. Results showed that, compared with traditional methods, MMSS-Rank is an efficient and robust method for identifying large-scale FSI data with higher accuracy and recall rate.																	1868-5137	1868-5145															10.1007/s12652-020-01750-4		FEB 2020											
J								MIP neighborhood search heuristics for a service network design problem with design-balanced requirements	JOURNAL OF HEURISTICS										Service network design; Capacity scaling; MIP; Neighborhood search	TRANSPORTATION; MANAGEMENT; ALGORITHM	Service network design problems are used to address a variety of services in transportation and logistics planning. In the present paper, we consider the service network design problem with design-balanced requirements. This problem is particularly relevant to operations for consolidation transportation systems and determines the transportation network configuration and the characteristics of the corresponding services. We present a solution approach that combines a capacity scaling procedure for finding an initial feasible solution and a MIP neighborhood search for improving the solutions. Computational experiments on benchmark instances show that the proposed heuristic finds high-quality solutions in a short computation time.																	1381-1231	1572-9397				AUG	2020	26	4					475	502		10.1007/s10732-020-09437-x		FEB 2020											
J								Path Planning under Constraints and Path Following Control of Autonomous Underwater Vehicle with Dynamical Uncertainties and Wave Disturbances	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										AUV; Path planning; Path following; MPC; SMC; PSO	MODEL-PREDICTIVE CONTROL; LINE-OF-SIGHT; TRACKING CONTROL; UNDERACTUATED SHIPS; TRAJECTORY TRACKING; DESIGN; STABILITY	The path planning and following control problems of autonomous underwater vehicle (AUV) in three-dimension (3D) are studied in this paper. In order to realize obstacle avoidance and path optimization, a path planning method based on particle swarm optimization (PSO) and cubic spline interpolation is developed. The curvature of the path obtained by this method is continuous, which can not only avoid obstacles but also meet the constraint of AUV's minimum radius of rotation. In the design of kinematics controller, an optimal guidance scheme based on model predictive control (MPC) is proposed, which takes into account the wave disturbances. Adaptive dynamical sliding mode control (ADSMC) technology is used to design dynamic controller, which can effectively overcome the influence of model uncertainties. In order to ensure the stability of the system, the stability condition of MPC is designed, and the stability of the closed-loop system is analyzed by applying cascade system theory. The control strategy proposed in this paper is compared with the line-of-sight (LOS) guidance through simulation experiment. The simulation results demonstrate that the proposed control strategy can not only improve the quality of path following, but also reduce the disturbance of waves, and thus is more conducive to energy saving.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		891	908		10.1007/s10846-019-01146-3		FEB 2020											
J								Adaptive path finding algorithm in dynamic environment for warehouse robot	NEURAL COMPUTING & APPLICATIONS										Path finding; Dynamic obstacle avoidance; Warehouse robot		Warehouse robots have been widely used by manufacturers and online retailer to automate good delivery process. One of the fundamental components when designing a warehouse robot is path finding algorithm. In the past, many path finding algorithms had been proposed to identify the optimal path and improve the efficiency in different conditions. For example, A* path finding algorithm is developed to obtain the shortest path, while D* obtains a complete coverage path from source to destination. Although these algorithms improved the efficiency in path finding, dynamic obstacle that may exist in warehouse environment was not considered. This paper presents AD* algorithm, a path finding algorithm that works in dynamic environment for warehouse robot. AD* algorithm is able to detect not only static obstacle but also dynamic obstacles while operating in warehouse environment. In dynamic obstacle path prediction, image of the warehouse environment is processed to identify and track obstacles in the path. The image is pre-processed using perspective transformation, dilation and erosion. Once obstacle has been identified using background subtraction, the server will track and predict future path of the dynamic object to avoid the obstacle.																	0941-0643	1433-3058				SEP	2020	32	17					13155	13171		10.1007/s00521-020-04764-3		FEB 2020											
J								Analysing wear behaviour of Al-CaCO3 composites using ANN and Sugeno-type fuzzy inference systems	NEURAL COMPUTING & APPLICATIONS										Al-CaCO3 composite; Stir casting; Wear; ANN; Sugeno-type FIS	SURFACE CHARACTERIZATION; MECHANICAL-PROPERTIES; PREDICTION; TENSILE	Design of experiment for the development of stir cast calcium carbonate-reinforced aluminium composite is a search for optimum combination of material and process control parameters for best physical and mechanical properties. A soft-computing model can accurately learn the complex interactions between process parameters to provide great insights in the development of this composite. This paper demonstrates and analyses the potential of artificial neural network (ANN) and Sugeno-type fuzzy inference systems (FIS) for wear behaviour prediction of calcium carbonate-reinforced aluminium composites. The models were trained with data collected from the experiment. The data consist of filler particle size of 150 mu m with weights fractions varied from 0 to 25 wt%, in step of 5. Wear test data at different time of contacts (30, 60, 90, 120 and 150 s) and variable loads of 2.27 N, 4.54 N and 6.80 N were collected, resulting to 120 length vectors. Comparing the experimental results of wear test with those predicted using the ANN and Sugeno-type FIS, the integration of calcium carbonate particulate enhanced the wear characteristics of Al matrix up to 200%. On the use of back-propagation neural network with 4-3-1 architecture for wear prediction, the Levenberg-Marquardt training algorithm performs better. For Sugeno-type FIS, the Gaussian membership function resulted to the best prediction of wear rate. When ANN and Sugeno-type FIS performance on the test set were analysed based on some statistical parameters, the later returned an R-2 value of 0.9775 as against ANN's value of 0.3684. The predicted wear rate using ANFIS with Gaussian membership functions was in good agreement with the experimental values.																	0941-0643	1433-3058				SEP	2020	32	17					13453	13464		10.1007/s00521-020-04753-6		FEB 2020											
J								Robust Exponential Stability for Discrete-Time Quaternion-Valued Neural Networks with Time Delays and Parameter Uncertainties	NEURAL PROCESSING LETTERS										Quaternion-valued neural networks; Discrete-time; Robust exponential stability; Interval parameter uncertainty	FAULT-DIAGNOSIS; SYNCHRONIZATION; EQUATIONS	In this paper, the robust exponential stability for discrete-time quaternion-valued neural networks with time delays and parameter uncertainties is investigated. By means of Lyapunov theorem, linear matrix inequality and contraction mapping theorem, new sufficient conditions are derived to ensure the existence, uniqueness and robust exponential stability of the equilibrium point of the proposed quaternion-valued neural networks. Compared with the existed literatures, the obtained results are less conservative. Finally, simulations are presented to illustrate the effectiveness of the theoretical results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2317	2335		10.1007/s11063-020-10196-w		FEB 2020											
J								Content-Aware Summarization of Broadcast Sports Videos: An Audio-Visual Feature Extraction Approach	NEURAL PROCESSING LETTERS										Highlight generation; Sports analysis; Multimodal sports video analysis	EVENT DETECTION; FRAMEWORK	A large number of videos available on the internet belong to the category of sports. Generally, a sports video has a long duration and consists of only a few exciting moments. Sports enthusiasts keep themselves updated on the current happenings, in less time, by means of a summarized version of the sports video known as highlights. For the past few years, sports video summarization is regaining attention among the research community. Automatic generation of highlights form a sports video is a challenging task as different sports games have different rules and situations. In this paper, we propose a method for automatically generating highlights from broadcast sports videos. The proposed method generates highlights by extracting audio and visual features from a sports video. Our method automatically learns the scorebox template from a broadcast sports video using SIFT features, and then locates and extracts the template from a video stream. The extracted template is further analyzed to find out all the possible text regions. Afterward, the information is extracted from all the text regions by means of deep neural network. Based on user preferences, the most relevant information is extracted and converted to a keyframe representation which helps to generate highlights. Extensive experiments were performed to evaluate the effectiveness of the proposed method. Results of the experiments reveal the effectiveness and superiority of the proposed method.																	1370-4621	1573-773X															10.1007/s11063-020-10200-3		FEB 2020											
J								A self-adaptive virus optimization algorithm for continuous optimization problems	SOFT COMPUTING										Continuous optimization; Virus optimization algorithm; Self-adaptation; Metaheuristic	EVOLUTION; ADAPTATION; SIMULATION	Given the outstanding effectiveness and efficiency performance in different fields such as image processing and energy dispatching, the virus optimization algorithm (VOA), a newly developed metaheuristic for general optimization purposes, has been further improved. Similar to other metaheuristic methods, VOA performance to some degree relies on proper parameter settings, which may require large numbers of experiments to determine. Therefore, this study proposes a self-adaptive version of VOA (SaVOA) to decrease the number of controllable parameters in the algorithm and thus reduce the time needed to determine proper parameter values by any sort of experimental design process. Having an SaVOA ensures the ease access of the algorithm for different types of continuous domain problems, whereas previous different optimization problems may have needed different parameter settings. To perform the comparison, SaVOA is tested by optimizing the same set of benchmark functions used when proposing the original VOA. Computational results indicate some major advances were achieved by the SaVOA in addition to competitive results obtained. Most importantly, SaVOA proved its superiority on functions where the original VOA was not powerful enough to perform well, such as Rosenbrock, Schwefel, Drop Wave, Levy, and Easom's functions. In terms of implementation, the number of controllable parameters in SaVOA was greatly reduced to only one-the stopping criterion. This promises a significant improvement in the utility of SaVOA for any type of continuous domain optimization problem.																	1432-7643	1433-7479				SEP	2020	24	17					13147	13166		10.1007/s00500-020-04730-0		FEB 2020											
J								Self-adaptive weight vector adjustment strategy for decomposition-based multi-objective differential evolution algorithm	SOFT COMPUTING										Evolutionary computations; Multi-objective optimization; Decomposition; Many-objective optimization; Weight vector	PARTICLE SWARM OPTIMIZATION; REFERENCE-POINT; MOEA/D; CONVERGENCE	In multi-objective and many-objective optimization, weight vectors are particularly crucial to the performance of decomposition-based optimization algorithms. The uniform weight vectors are not suitable for complex Pareto fronts (PFs), so it is necessary to improve the distribution of weight vectors. Besides, the balance between convergence and diversity is a difficult issue as well in multi-objective optimization, and it becomes increasingly important with the augment of the number of objectives. To address these issues, a self-adaptive weight vector adjustment strategy for decomposition-based multi-objective differential evolution algorithm (AWDMODE) is proposed. In order to ensure that the guidance of weight vectors becomes accurate and effective, the adaptive adjustment strategy is introduced. This strategy distinguishes the shapes and adjusts weight vectors dynamically, which can ensure that the guidance of weight vectors becomes accurate and effective. In addition, a self-learning strategy is adopted to produce more non-dominated solutions and balance the convergence and diversity. The experimental results indicate that AWDMODE outperforms the compared algorithms on WFG suites test instances, and shows a great potential when handling the problems whose PFs are scaled with different ranges in each objective.																	1432-7643	1433-7479				SEP	2020	24	17					13179	13195		10.1007/s00500-020-04732-y		FEB 2020											
J								Clustering of the body shape of the adult male by using principal component analysis and genetic algorithm-BP neural network	SOFT COMPUTING										Principal component analysis; GA-BP neural network; Male; Body shape; Prediction and cluster		In order to improve the efficiency and accuracy of human body shape prediction, principal component analysis method (PCA) is proposed to reduce the dimension of related variables and eliminate the multicollinearity among variables. Then, the transformed variables are input into genetic algorithm and BP neural network, and a new method of human body shape prediction is designed. To avoid the problems that slow convergence speed and easy falling into local minima of BP neural network, the genetic algorithm is used to optimize the weights and thresholds of BP neural network. Moreover, to prove the superiority of PCA-GA-BP model, the prediction results are compared with those of other algorithms. Body sizes of 18-25-year-old, 26-44-year-old and 45-59-year-old males were selected as experimental data to analyze these models. The prediction results of GA-BP, PCA-BP, BP, SVM and K-means were compared with PCA-GA-BP neural network. The results show that the prediction effect of PCA-GA-BP neural network is significantly better than that of GA-BP, PCA-BP, BP, SVM and K-means prediction models, which can accurately predict and cluster the human body shape. The model has better prediction and classification and simpler structure.																	1432-7643	1433-7479				SEP	2020	24	17					13219	13237		10.1007/s00500-020-04735-9		FEB 2020											
J								Optimization of planning cost of radial distribution networks at different loads with the optimal placement of distribution STATCOM using differential evolution algorithm	SOFT COMPUTING										DSTATCOM; Reactive power compensation; Power loss; Voltage improvement; Net profit; Present worth factor (PWF)	PARTICLE SWARM OPTIMIZATION; OPTIMAL CAPACITOR PLACEMENT; DISTRIBUTION-SYSTEMS; OPTIMAL LOCATION; GENETIC ALGORITHM; VOLTAGE STABILITY; GENERATION; ALLOCATION; DSTATCOM; RECONFIGURATION	This paper presents an optimization of planning of distribution systems with the allocation of DSTATCOM based on maximization of a net cost profit/savings analysis approach by using the differential evolution algorithm. In the proposed approach, the optimal placement of DSTATCOM, and reactive power compensation at a certain location, and the improvement of voltage profile are obtained by three objective functions: (1) minimization of the size of DSTATCOM, (2) minimization of network power loss, and (3) maximization of net cost profit/savings by minimizing the total planning cost of DSTATCOM installation scheme. Present worth factor is adopted to evaluate the net cost profit/savings of the DSTATCOM installation scheme for a certain planning horizon. The appropriate mathematical modeling of DSTATCOM is used to incorporate it suitably in the forward-backward sweep load flow algorithm of radial distribution networks to provide the reactive power compensation. The recommended method is validated on the IEEE 30-bus, 33-bus and 69-bus distribution networks.																	1432-7643	1433-7479				SEP	2020	24	17					13269	13284		10.1007/s00500-020-04739-5		FEB 2020											
J								Fair-by-design matching	DATA MINING AND KNOWLEDGE DISCOVERY										Algorithmic bias; Fairness; Matching; Combinatorial optimization	ALGORITHM	Matching algorithms are used routinely to match donors to recipients for solid organs transplantation, for the assignment of medical residents to hospitals, record linkage in databases, scheduling jobs on machines, network switching, online advertising, and image recognition, among others. Although many optimal solutions may exist to a given matching problem, when the elements that shall or not be included in a solution correspond to individuals, it becomes of paramount importance that the solution is selected fairly. In this paper we study individual fairness in matching problems. Given that many maximum matchings may exist, each one satisfying a different set of individuals, the only way to guarantee fairness is through randomization. Hence we introduce the distributional maxmin fairness framework which provides, for any given input instance, the strongest guarantee possible simultaneously for all individuals in terms of satisfaction probability (the probability of being matched in the solution). Specifically, a probability distribution over feasible solutions is maxmin-fair if it is not possible to improve the satisfaction probability of any individual without decreasing it for some other individual which is no better off. Our main contribution is a polynomial-time algorithm building on techniques from minimum cuts, and edge-coloring algorithms for regular bipartite graphs, and transversal theory. In the special case of bipartite matching, our algorithm runs in expected time. An experimental evaluation of our fair-matching algorithm shows its ability to scale to graphs with tens of millions of vertices and hundreds of millions of edges, taking only a few minutes on a simple architecture. To the best of our knowledge, this yields the first large-scale implementation of the egalitarian mechanism of Bogomolnaia and Moulin (Econometrica 72(1):257-279, 2004). Our analysis confirms that our method provides stronger satisfaction probability guarantees than non-trivial baselines.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1291	1335		10.1007/s10618-020-00675-y		FEB 2020											
J								A chemometric study combined with spectroscopy for the quantification of secondary structure of flagellar-associated protein 174 (FAP174)	JOURNAL OF CHEMOMETRICS										chemometric methods; circular dichroism; flagellar associated protein 174; Fourier transform infrared spectroscopy; partial least square regression	TRANSFORM INFRARED-SPECTROSCOPY; CIRCULAR-DICHROISM SPECTRA; KINASE ANCHORING PROTEINS; MYC-BINDING PROTEIN; CHLAMYDOMONAS-REINHARDTII; STRUCTURE PREDICTION; PATTERN-RECOGNITION; INFORMATION-CONTENT; MEMBRANE-PROTEINS; C-MYC	The secondary structure analyses of proteins hold immense importance in the field of protein science because it plays a vital role in its hierarchical classification. It is the most important transitional step in the prediction of the three-dimensional structure of any protein. The aim of the current study is to quantitatively determine the secondary structure of an important ciliary protein, viz, an MYC-binding protein-1 (MYCBP-1) orthologue, viz, flagellar-associated protein 174 (FAP174) from the green alga, Chlamydomonas reinhardtii. FAP174 binds to A-kinase anchoring protein 240 (AKAP240) and has a crucial role to play in ciliary motility. Hence, the biochemical characterization of FAP174 becomes more vital in understanding its molecular function. The accurate secondary structure of FAP174 was investigated through circular dichroism (CD) and Fourier transform infrared spectroscopy (FTIR) combined with partial least square regression (PLSR) methods. The far-UV CD spectrum of FAP174 exhibited a positive band at 192 nm and negative bands at 208 and 221 nm, whereas quantification through BeStSel web server revealed 65% alpha-helix, approximately 2% of antiparallel beta-sheets, 6% of beta-turns, and 27% of unordered structures. These results were further confirmed by FTIR spectrum of FAP174 that revealed amide I and II bands at 1654 and 1547 cm(-1), respectively. And the PLSR calibration models led to the quantification of the secondary structure for FAP174 protein that fairly corroborated with the values obtained by quantifying CD spectrum, these being approximately 54% alpha-helix, 0% beta-sheets, approximately 12% beta-turns, and approximately 34% other structures. The recombinant FAP174 protein is therefore considerably alpha-helical and has negligible or no beta-sheets.																	0886-9383	1099-128X				MAY	2020	34	5							e3221	10.1002/cem.3221		FEB 2020											
J								Wafer map defect pattern classification based on convolutional neural network features and error-correcting output codes	JOURNAL OF INTELLIGENT MANUFACTURING										Wafer map; Defect pattern classification; Deep learning; Convolutional neural network; Error-correcting output codes; Support vector machine; Multi-class classification	RECOGNITION; SVM	Defect clusters on the wafer map can provide important clue to identify the process failures so that it is important to accurately classify the defect patterns into corresponding pattern types. In this research, we present an image-based wafer map defect pattern classification method. The presented method consists of two main steps: without any specific preprocessing, high-level features are extracted from convolutional neural network and then the extracted features are fed to combination of error-correcting output codes and support vector machines for wafer map defect pattern classification. To the best of our knowledge, no prior work has applied the presented method for wafer map defect pattern classification. Experimental results tested on 20,000 wafer maps show the superiority of presented method and the overall classification accuracy is up to 98.43%.																	0956-5515	1572-8145															10.1007/s10845-020-01540-x		FEB 2020											
