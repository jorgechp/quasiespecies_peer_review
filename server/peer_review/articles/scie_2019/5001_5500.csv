PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Expert-validated estimation of diagnostic uncertainty for deep neural networks in diabetic retinopathy detection	MEDICAL IMAGE ANALYSIS										Deep neural networks; Diabetic retinopathy; Uncertainty; Calibration	CLASSIFICATION; MEDICINE; CANCER	Deep learning-based systems can achieve a diagnostic performance comparable to physicians in a variety of medical use cases including the diagnosis of diabetic retinopathy. To be useful in clinical practice, it is necessary to have well calibrated measures of the uncertainty with which these systems report their decisions. However, deep neural networks (DNNs) are being often overconfident in their predictions, and are not amenable to a straightforward probabilistic treatment. Here, we describe an intuitive framework based on test-time data augmentation for quantifying the diagnostic uncertainty of a state-of-the-art DNN for diagnosing diabetic retinopathy. We show that the derived measure of uncertainty is well-calibrated and that experienced physicians likewise find cases with uncertain diagnosis difficult to evaluate. This paves the way for an integrated treatment of uncertainty in DNN-based diagnostic systems. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101724	10.1016/j.media.2020.101724													
J								Spatially regularized parametric map reconstruction for fast magnetic resonance fingerprinting	MEDICAL IMAGE ANALYSIS										Magnetic resonance fingerprinting; Convolutional neural network; Quantitative magnetic resonance imaging; Image reconstruction		Magnetic resonance fingerprinting (MRF) provides a unique concept for simultaneous and fast acquisition of multiple quantitative MR parameters. Despite acquisition efficiency, adoption of MRF into the clinics is hindered by its dictionary matching-based reconstruction, which is computationally demanding and lacks scalability. Here, we propose a convolutional neural network-based reconstruction, which enables both accurate and fast reconstruction of parametric maps, and is adaptable based on the needs of spatial regularization and the capacity for the reconstruction. We evaluated the method using MRF T1-FF, an MRF sequence for T1 relaxation time of water (T1(H2O)) and fat fraction (FF) mapping. We demonstrate the method's performance on a highly heterogeneous dataset consisting of 164 patients with various neuromuscular diseases imaged at thighs and legs. We empirically show the benefit of incorporating spatial regularization during the reconstruction and demonstrate that the method learns meaningful features from MR physics perspective. Further, we investigate the ability of the method to handle highly heterogeneous morphometric variations and its generalization to anatomical regions unseen during training. The obtained results outperform the state-of-the-art in deep learning-based MRF reconstruction. The method achieved normalized root mean squared errors of 0.048 +/- 0.011 for T1(H2O) maps and 0.027 +/- 0.004 for FF maps when compared to the dictionary matching in a test set of 50 patients. Coupled with fast MRF sequences, the proposed method has the potential of enabling multiparametric MR imaging in clinically feasible time. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				AUG	2020	64								101741	10.1016/j.media.2020.101741													
J								A novel approach to multiple anatomical shape analysis: Application to fetal ventriculomegaly	MEDICAL IMAGE ANALYSIS										Joint spectral embedding; Similarity fusion; Fetal ventriculomegaly; Cortical folding	AUTOMATIC QUANTIFICATION; FOLDING PATTERNS; BRAIN; MRI; FETUSES; ATLAS; SEGMENTATION; CONSTRUCTION; WIDTH	Fetal ventriculomegaly (VM) is a condition in which one or both lateral ventricles are enlarged, and is diagnosed as an atrial diameter larger than 10 mm. Evidence of altered cortical folding associated with VM has been shown in the literature. However, existing works use a single scalar value such as diagnosis or lateral ventricular volume to characterize VM and study its relationship with alterations in cortical folding, thus failing to reveal the spatially-heterogeneous associations. In this work, we propose a novel approach to identify fine-grained associations between cortical folding and ventricular enlargement by leveraging the vertex-wise correlations between their growth patterns in terms of area expansion and curvature. Our approach comprises three steps. In the first step, we define a joint graph Laplacian matrix using cortex-to-ventricle correlations. The joint Laplacian is built based on multiple cortical features. Next, we propose a spectral embedding of the cortex-to-ventricle graph into a common underlying space where its nodes are projected according to the joint ventricle-cortex growth patterns. In this low-dimensional joint ventricle-cortex space, associated growth patterns lie close to each other. In the final step, we perform hierarchical clustering in the joint embedded space to identify associated sub-regions between cortex and ventricle. Using a dataset of 25 healthy fetuses and 23 fetuses with isolated nonsevere VM within the age range of 26-29 gestational weeks, our approach reveals clinically relevant and heterogeneous regional associations. Cortical regions forming these associations are further validated using statistical analysis, revealing regions with altered folding that are significantly associated with ventricular dilation. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101750	10.1016/j.media.2020.101750													
J								Uncertainty-aware domain alignment for anatomical structure segmentation	MEDICAL IMAGE ANALYSIS										Uncertainty; Domain adaptation; Unsupervised segmentation; Deep learning	CHOROIDAL THICKNESS; OCT	Automatic and accurate segmentation of anatomical structures on medical images is crucial for detecting various potential diseases. However, the segmentation performance of established deep neural networks may degenerate on different modalities or devices owing to the significant difference across the domains, a problem known as domain shift. In this work, we propose an uncertainty-aware domain alignment framework to address the domain shift problem in the cross-domain Unsupervised Domain Adaptation (UDA) task. Specifically, we design an Uncertainty Estimation and Segmentation Module (UESM) to obtain the uncertainty map estimation. Then, a novel Uncertainty-aware Cross Entropy (UCE) loss is proposed to leverage the uncertainty information to boost the segmentation performance on highly uncertain regions. To further improve the performance in the UDA task, an Uncertainty-aware Self-Training (UST) strategy is developed to choose the optimal target samples by uncertainty guidance. In addition, the Uncertainty Feature Recalibration Module (UFRM) is applied to enforce the framework to minimize the cross-domain discrepancy. The proposed framework is evaluated on a private cross-device Optical Coherence Tomography (OCT) dataset and a public cross-modality cardiac dataset released by MMWHS 2017. Extensive experiments indicate that the proposed UESM is both efficient and effective for the uncertainty estimation in the UDA task, achieving state-of-the-art performance on both cross-modality and cross-device datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101732	10.1016/j.media.2020.101732													
J								Unsupervised lesion detection via image restoration with a normative prior	MEDICAL IMAGE ANALYSIS										Unsupervised learning; Lesion detection; Autoencoding models	NEURAL-NETWORKS; BRAIN; SEGMENTATION	Unsupervised lesion detection is a challenging problem that requires accurately estimating normative distributions of healthy anatomy and detecting lesions as outliers without training examples. Recently, this problem has received increased attention from the research community following the advances in unsupervised learning with deep learning. Such advances allow the estimation of high-dimensional distributions, such as normative distributions, with higher accuracy than previous methods. The main approach of the recently proposed methods is to learn a latent-variable model parameterized with networks to approximate the normative distribution using example images showing healthy anatomy, perform prior-projection, i.e. reconstruct the image with lesions using the latent-variable model, and determine lesions based on the differences between the reconstructed and original images. While being promising, the prior-projection step often leads to a large number of false positives. In this work, we approach unsupervised lesion detection as an image restoration problem and propose a probabilistic model that uses a network-based prior as the normative distribution and detect lesions pixel-wise using MAP estimation. The probabilistic model punishes large deviations between restored and original images, reducing false positives in pixel-wise detections. Experiments with gliomas and stroke lesions in brain MRI using publicly available datasets show that the proposed approach outperforms the state-of-the-art unsupervised methods by a substantial margin, +0.13 (AUC), for both glioma and stroke detection. Extensive model analysis confirms the effectiveness of MAP-based image restoration. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101713	10.1016/j.media.2020.101713													
J								Groupwise registration with global-local graph shrinkage in atlas construction	MEDICAL IMAGE ANALYSIS										Groupwise registration; Atlas construction; Graph shrinkage; Global-local	SEGMENTATION; IMAGE; MODEL	Graph-based groupwise registration methods are widely used in atlas construction. Given a group of images, a graph is built whose nodes represent the images, and whose edges represent a geodesic path between two nodes. The distribution of images on an image manifold is explored through edge traversal in a graph. The final atlas is a mean image at the population center of the distribution on the manifold. The procedure of warping all images to the mean image turns to dynamic graph shrinkage in which nodes become closer to each other. Most conventional groupwise registration frameworks construct and shrink a graph without considering the local distribution of images on the dataset manifold and the local structure variations between image pairs. Neglecting the local information fundamentally decrease the accuracy and efficiency when population atlases are built for organs with large inter-subject anatomical variabilities. To overcome the problem, this paper proposes a global-local graph shrinkage approach that can generate accurate atlas. A connected graph is constructed automatically based on global similarities across the images to explore the global distribution. A local image distribution obtained by image clustering is used to simplify the edges of the constructed graph. Subsequently, local image similarities refine the deformation estimated through global image similarity for each image warping along the graph edges. Through the image warping, the overall simplified graph shrinks gradually to yield the atlas with respecting both global and local features. The proposed method is evaluated on 61 synthetic and 20 clinical liver datasets, and the results are compared with those of six state-of-the-art groupwise registration methods. The experimental results show that the proposed method outperforms non-global-local method approaches in terms of accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101711	10.1016/j.media.2020.101711													
J								Deep learning with 4D spatio-temporal data representations for OCT-based force estimation	MEDICAL IMAGE ANALYSIS										4D Deep learning; 4D Data representations; Optical coherence tomography; Force estimation	SURGERY	Estimating the forces acting between instruments and tissue is a challenging problem for robot-assisted minimally-invasive surgery. Recently, numerous vision-based methods have been proposed to replace electro-mechanical approaches. Moreover, optical coherence tomography (OCT) and deep learning have been used for estimating forces based on deformation observed in volumetric image data. The method demonstrated the advantage of deep learning with 3D volumetric data over 2D depth images for force estimation. In this work, we extend the problem of deep learning-based force estimation to 4D spatio-temporal data with streams of 3D OCT volumes. For this purpose, we design and evaluate several methods extending spatio-temporal deep learning to 4D which is largely unexplored so far. Furthermore, we provide an in-depth analysis of multi-dimensional image data representations for force estimation, comparing our 4D approach to previous, lower-dimensional methods. Also, we analyze the effect of temporal information and we study the prediction of short-term future force values, which could facilitate safety features. For our 4D force estimation architectures, we find that efficient decoupling of spatial and temporal processing is advantageous. We show that using 4D spatio-temporal data outperforms all previously used data representations with a mean absolute error of 10.7 mN. We find that temporal information is valuable for force estimation and we demonstrate the feasibility of force prediction. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101730	10.1016/j.media.2020.101730													
J								Semi-supervised WCE image classification with adaptive aggregated attention	MEDICAL IMAGE ANALYSIS										WCE Image classification; Synergic network; Attention; Semi-supervised learning	POLYP DETECTION	Accurate abnormality classification in Wireless Capsule Endoscopy (WCE) images is crucial for early gastrointestinal (GI) tract cancer diagnosis and treatment, while it remains challenging due to the limited annotated dataset, the huge intra-class variances and the high degree of inter-class similarities. To tackle these dilemmas, we propose a novel semi-supervised learning method with Adaptive Aggregated Attention (AAA) module for automatic WCE image classification. Firstly, a novel deformation field based image preprocessing strategy is proposed to remove the black background and circular boundaries in WCE images. Then we propose a synergic network to learn discriminative image features, consisting of two branches: an abnormal regions estimator (the first branch) and an abnormal information distiller (the second branch). The first branch utilizes the proposed AAA module to capture global dependencies and incorporate context information to highlight the most meaningful regions, while the second branch mainly focuses on these calculated attention regions for accurate and robust abnormality classification. Finally, these two branches are jointly optimized by minimizing the proposed discriminative angular (DA) loss and Jensen-Shannon divergence (JS) loss with labeled data as well as unlabeled data. Comprehensive experiments have been conducted on the public CAD-CAP WCE dataset. The proposed method achieves 93.17% overall accuracy in a fourfold cross-validation, verifying its effectiveness for WCE image classification. The source code is available at https://github.com/Guo-Xiaoqing/SSL_WCE. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101733	10.1016/j.media.2020.101733													
J								Mimicking the radiologists' workflow: Estimating pediatric hand bone age with stacked deep neural networks	MEDICAL IMAGE ANALYSIS										Bone age assessment; Deep learning; Greulich and Pyle; Object detection; Pediatric radiographs; Radiologic workflow	SYSTEM	Pediatric endocrinologists regularly order radiographs of the left hand to estimate the degree of bone maturation in order to assess their patients for advanced or delayed growth, physical development, and to monitor consecutive therapeutic measures. The reading of such images is a labor-intensive task that requires a lot of experience and is normally performed by highly trained experts like pediatric radiologists. In this paper we build an automated system for pediatric bone age estimation that mimics and accelerates the workflow of the radiologist without breaking it. The complete system is based on two neural network based models: on the one hand a detector network, which identifies the ossification areas, on the other hand gender and region specific regression networks, which estimate the bone age from the detected areas. With a small annotated dataset an ossification area detection network can be trained, which is stable enough to work as part of a multi-stage approach. Furthermore, our system achieves competitive results on the RSNA Pediatric Bone Age Challenge test set with an average error of 4.56 months. In contrast to other approaches, especially purely encoder-based architectures, our two-stage approach provides self-explanatory results. By detecting and evaluating the individual ossification areas, thus simulating the workflow of the Tanner-Whitehouse procedure, the results are interpretable for a radiologist. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				AUG	2020	64								101743	10.1016/j.media.2020.101743													
J								Self-co-attention neural network for anatomy segmentation in whole breast ultrasound	MEDICAL IMAGE ANALYSIS										Breast anatomy segmentation; Self-co-attention mechanism; Non-local cue; Encoder-decoder architecture	CELL-COMPETITION ALGORITHM; DENSITY ANALYSIS; TUMOR-DETECTION; IMAGES; RISK; DIAGNOSIS; LESIONS	The automated whole breast ultrasound (AWBUS) is a new breast imaging technique that can depict the whole breast anatomy. To facilitate the reading of AWBUS images and support the breast density estimation, an automatic breast anatomy segmentation method for AWBUS images is proposed in this study. The problem at hand is quite challenging as it needs to address issues of low image quality, ill-defined boundary, large anatomical variation, etc. To address these issues, a new deep learning encoder-decoder segmentation method based on a self-co-attention mechanism is developed. The self-attention mechanism is comprised of spatial and channel attention module (SC) and embedded in the ResNeXt (i.e., Res-SC) block in the encoder path. A non-local context block (NCB) is further incorporated to augment the learning of high-level contextual cues. The decoder path of the proposed method is equipped with the weighted up-sampling block (WUB) to attain class-specific better up-sampling effect. Meanwhile, the co-attention mechanism is also developed to improve the segmentation coherence among two consecutive slices. Extensive experiments are conducted with comparison to several the state-of-the-art deep learning segmentation methods. The experimental results corroborate the effectiveness of the proposed method on the difficult breast anatomy segmentation problem on AWBUS images. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101753	10.1016/j.media.2020.101753													
J								Skin lesion segmentation via generative adversarial networks with dual discriminators	MEDICAL IMAGE ANALYSIS										Skin lesion segmentation; Generative adversarial network; Dense convolution U-Net; Dual discriminators	CLASSIFICATION; DIAGNOSIS	Skin lesion segmentation from dermoscopy images is a fundamental yet challenging task in the computer-aided skin diagnosis system due to the large variations in terms of their views and scales of lesion areas. We propose a novel and effective generative adversarial network (GAN) to meet these challenges. Specifically, this network architecture integrates two modules: a skip connection and dense convolution U-Net (UNet-SCDC) based segmentation module and a dual discrimination (DD) module. While the UNet-SCDC module uses dense dilated convolution blocks to generate a deep representation that preserves fine-grained information, the DD module makes use of two discriminators to jointly decide whether the input of the discriminators is real or fake. While one discriminator, with a traditional adversarial loss, focuses on the differences at the boundaries of the generated segmentation masks and the ground truths, the other examines the contextual environment of target object in the original image using a conditional discriminative loss. We integrate these two modules and train the proposed GAN in an end-to-end manner. The proposed GAN is evaluated on the public International Skin Imaging Collaboration (ISIC) Skin Lesion Challenge Datasets of 2017 and 2018. Extensive experimental results demonstrate that the proposed network achieves superior segmentation performance to state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101716	10.1016/j.media.2020.101716													
J								Dynamically constructed network with error correction for accurate ventricle volume estimation	MEDICAL IMAGE ANALYSIS										Dynamically constructed network; Residual correction; Ventricle volume estimation; Ejection fraction correlation	CARDIAC MRI; AUTOMATIC SEGMENTATION; LABEL FUSION; LEVEL SET; REGRESSION; HEART; MODEL	Automated ventricle volume estimation (AVVE) on cardiac magnetic resonance (CMR) images is very important for clinical cardiac disease diagnosis. However, current AVVE methods ignore the error correction for the estimated volume. This results in clinically intolerable ventricle volume estimation error and further leads to wrong ejection fraction (EF) assessment, which significantly limits the application potential of AVVE methods. The objective of this paper is to address this problem with AVVE and further make it more clinically applicable. We proposed a dynamically constructed network to achieve accurate AVVE. First, we introduced a novel dynamically constructed deep learning framework, that evolves a single model into a bi-model volume estimation network. In this way, the EF correlation can be built directly based on the bi-model network. Second, we proposed an error correction strategy using dynamically created residual nodes, which is based on stochastic configurations with an EF correlation constraint. Finally, we formulated the proposed method into an end-to-end joint optimization framework for accurate ventricle volume estimation with effective error correction. Experiments and comparisons on large-scale cardiac magnetic resonance datasets were carried out. Results show that the proposed method outperforms state-of-the-art methods, and has good potential for clinical application. Besides, the proposed method is the first work to achieve error correction for AVVE and also has the potential to be extended to other medical index estimation tasks. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101723	10.1016/j.media.2020.101723													
J								Automatic detection of landmarks for the analysis of a reduction of supracondylar fractures of the humerus	MEDICAL IMAGE ANALYSIS										Humerus landmark detection; Contralateral images; Computer-assisted orthopedic (CAOS); Geometrical approach	ANATOMICAL LANDMARKS; PROXIMAL HUMERUS; SHAFT FRACTURE; RECONSTRUCTION; OSTEOSYNTHESIS; IMAGES; CT; SEGMENTATION; LOCALIZATION; GEOMETRY	An accurate identification of bone features is required by modern orthopedics to improve patient recovery. The analysis of landmarks enables the planning of a fracture reduction surgery, designing prostheses or fixation devices, and showing deformities accurately. The recognition of these features was previously performed manually. However, this long and tedious process provided insufficient accuracy. In this paper, we propose a geometrically-based algorithm that automatically detects the most significant landmarks of a humerus. By employing contralateral images of the upper limb, a side-to-side study of the landmarks is also conducted to analyze the goodness of supracondylar fracture reductions. We conclude that a reduction can be classified by only considering the detected landmarks. In addition, our technique does not require a prior training, thus becoming a reliable alternative to treat this kind of fractures. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101729	10.1016/j.media.2020.101729													
J								Prediction of in-plane organ deformation during free-breathing radiotherapy via discriminative spatial transformer networks	MEDICAL IMAGE ANALYSIS										Motion prediction; Free-breathing; Liver; Lungs; Radiotherapy; Deep learning; LSTM	RESPIRATORY MOTION; TUMOR MOTION; LUNG MOTION; LIVER; MODEL; MRI; FEASIBILITY; ULTRASOUND; MANAGEMENT	External beam radiotherapy is a commonly used treatment option for patients with cancer in the thoracic and abdominal regions. However, respiratory motion constitutes a major limitation during the intervention. It may stray the pre-defined target and trajectories determined during planning from the actual anatomy. We propose a novel framework to predict the in-plane organ motion. We introduce a recurrent encoder-decoder architecture which leverages feature representations at multiple scales. It simultaneously learns to map dense deformations between consecutive images from a given input sequence and to extrapolate them through time. Subsequently, several cascade-arranged spatial transformers use the predicted deformation fields to generate a future image sequence. We propose the use of a composite loss function which minimizes the difference between ground-truth and predicted images while maintaining smooth deformations. Our model is trained end-to-end in an unsupervised manner, thus it does not require additional information beyond image data. Moreover, no pre-processing steps such as segmentation or registration are needed. We report results on 85 different cases (healthy subjects and patients) belonging to multiples datasets across different imaging modalities. Experiments were aimed at investigating the importance of the proposed multi-scale architecture design and the effect of increasing the number of predicted frames on the overall accuracy of the model. The proposed model was able to predict vessel positions in the next temporal image with a median accuracy of 0.45 (0.55) mm, 0.45 (0.74) mm and 0.28 (0.58) mm in MRI, US and CT datasets, respectively. The obtained results show the strong potential of the model by achieving accurate matching between the predicted and target images on several imaging modalities. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101754	10.1016/j.media.2020.101754													
J								MB-FSGAN: Joint segmentation and quantification of kidney tumor on CT by the multi-branch feature sharing generative adversarial network	MEDICAL IMAGE ANALYSIS										Segmentation; Quantification; Multi-scale; Feature commensal; Generative adversarial		The segmentation of the kidney tumor and the quantification of its tumor indices (i.e., the center point coordinates, diameter, circumference, and cross-sectional area of the tumor) are important steps in tumor therapy. These quantifies the tumor morphometrical details to monitor disease progression and accurately compare decisions regarding the kidney tumor treatment. However, manual segmentation and quantification is a challenging and time-consuming process in practice and exhibit a high degree of variability within and between operators. In this paper, MB-FSGAN (multi-branch feature sharing generative adversarial network) is proposed for simultaneous segmentation and quantification of kidney tumor on CT. MB-FSGAN consists of multi-scale feature extractor (MSFE), locator of the area of interest (LROI), and feature sharing generative adversarial network (FSGAN). MSFE makes strong semantic information on different scale feature maps, which is particularly effective in detecting small tumor targets. The LROI extracts the region of interest of the tumor, greatly reducing the time complexity of the network. FSGAN correctly segments and quantifies kidney tumors through joint learning and adversarial learning, which effectively exploited the commonalities and differences between the two related tasks. Experiments are performed on CT of 113 kidney tumor patients. For segmentation, MB-FSGAN achieves a pixel accuracy of 95.7%. For the quantification of five tumor indices, the R-2 coefficient of tumor circumference is 0.9465. The results show that the network has reliable performance and shows its effectiveness and potential as a clinical tool. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101721	10.1016/j.media.2020.101721													
J								Graph refinement based airway extraction using mean-field networks and graph neural networks	MEDICAL IMAGE ANALYSIS										Mean-field networks; Graph neural networks; Airways; Segmentation; CT	CT	Graph refinement, or the task of obtaining subgraphs of interest from over-complete graphs, can have many varied applications. In this work, we extract trees or collection of sub-trees from image data by, first deriving a graph-based representation of the volumetric data and then, posing the tree extraction as a graph refinement task. We present two methods to perform graph refinement. First, we use mean-field approximation (MFA) to approximate the posterior density over the subgraphs from which the optimal subgraph of interest can be estimated. Mean field networks (MFNs) are used for inference based on the interpretation that iterations of MFA can be seen as feed-forward operations in a neural network. This allows us to learn the model parameters using gradient descent. Second, we present a supervised learning approach using graph neural networks (GNNs) which can be seen as generalisations of MFNs. Subgraphs are obtained by training a GNN-based graph refinement model to directly predict edge probabilities. We discuss connections between the two classes of methods and compare them for the task of extracting airways from 3D, low-dose, chest CT data. We show that both the MFN and GNN models show significant improvement when compared to one baseline method, that is similar to a top performing method in the EXACT'09 Challenge, and a 3D U-Net based airway segmentation model, in detecting more branches with fewer false positives. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101751	10.1016/j.media.2020.101751													
J								A survey on medical image analysis in diabetic retinopathy	MEDICAL IMAGE ANALYSIS										Diabetic retinopathy; Deep learning; Image mining; Lesion detection	CONVOLUTIONAL NEURAL-NETWORKS; COLOR FUNDUS PHOTOGRAPHS; RETINAL BLOOD-VESSELS; AUTOMATIC DETECTION; MACULAR EDEMA; EXUDATE DETECTION; SEGMENTATION; DIAGNOSIS; MICROANEURYSMS; IDENTIFICATION	Diabetic Retinopathy (DR) represents a highly-prevalent complication of diabetes in which individuals suffer from damage to the blood vessels in the retina. The disease manifests itself through lesion presence, starting with microaneurysms, at the nonproliferative stage before being characterized by neovascularization in the proliferative stage. Retinal specialists strive to detect DR early so that the disease can be treated before substantial, irreversible vision loss occurs. The level of DR severity indicates the extent of treatment necessary - vision loss may be preventable by effective diabetes management in mild (early) stages, rather than subjecting the patient to invasive laser surgery. Using artificial intelligence (AI), highly accurate and efficient systems can be developed to help assist medical professionals in screening and diagnosing DR earlier and without the full resources that are available in specialty clinics. In particular, deep learning facilitates diagnosis earlier and with higher sensitivity and specificity. Such systems make decisions based on minimally handcrafted features and pave the way for personalized therapies. Thus, this survey provides a comprehensive description of the current technology used in each step of DR diagnosis. First, it begins with an introduction to the disease and the current technologies and resources available in this space. It proceeds to discuss the frameworks that different teams have used to detect and classify DR. Ultimately, we conclude that deep learning systems offer revolutionary potential to DR identification and prevention of vision loss. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101742	10.1016/j.media.2020.101742													
J								Prediction of inter-fractional radiotherapy dose plans with domain translation in spatiotemporal embeddings	MEDICAL IMAGE ANALYSIS										Head and neck cancer; Adaptive radio-therapy; Inter-fractional dose delivery; Spatio-temporal manifold learning; Unsupervised domain translation; Discriminant graph embeddings; Optimization	RADIATION-THERAPY	External beam radiation therapy fractions have become extremely complex and tedious procedures to plan, due to stringent requirements of delivering the highest radiation dose to the tumor while maximally avoiding organs at risk. However, due to anatomic and/or biological changes between fractions, dose re-optimization may be needed. Re-optimization is a time-consuming task which is typically triggered based on subjective visual assessment by an experienced physician. To address limitations in this process, we introduce a predictive framework which learns the evolution of tumor anatomy as well as inter-fractional dose delivery variations for head and neck cancers. First, joint low-dimensional discriminant embeddings maximizing the separation between responsive and non-responsive groups to external beam radiotherapy plans are constructed from deep neural networks in order to capture patient-specific dose modulations with respect to anatomical variations. Then, latent representations are fed to a domain-level adversarial network to translate observed anatomical changes into dosimetric variations, which aims to enforce local semantic consistency in the overall translation. Dose distribution trajectories are represented in a group-average piecewise-geodesic setting to handle anatomical variations during therapy, using a quadratic optimization to perform curve regression. At test time, an annotated baseline CT is projected onto the latent space and translated to dose domain, from which a spatiotemporal regression model is constructed using parallel transport trajectories defined from closest samples. This allows to predict dosimetry changes during the course of treatment. The model was trained on 337 cases and tested on 50 separate patients using sequential CT and associated dosimetry data, with the probabilistic framework yielding a Dice score of 92% and an overall dose difference of 1.2 Gy in organs at risk and tumor volume over the course of multi-day treatment course, with a 5% reduction in delivered fraction segments. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101728	10.1016/j.media.2020.101728													
J								Pseudo-healthy synthesis with pathology disentanglement and adversarial learning	MEDICAL IMAGE ANALYSIS										Pseudo-healthy synthesis; Generative adversarial networks; Pathology disentanglement	LESION SEGMENTATION; IMAGE SYNTHESIS	Pseudo-healthy synthesis is the task of creating a subject-specific `healthy' image from a pathological one. Such images can be helpful in tasks such as anomaly detection and understanding changes induced by pathology and disease. In this paper, we present a model that is encouraged to disentangle the information of pathology from what seems to be healthy. We disentangle what appears to be healthy and where disease is as a segmentation map, which are then recombined by a network to reconstruct the input disease image. We train our models adversarially using either paired or unpaired settings, where we pair disease images and maps when available. We quantitatively and subjectively, with a human study, evaluate the quality of pseudo-healthy images using several criteria. We show in a series of experiments, performed on ISLES, BraTS and Cam-CAN datasets, that our method is better than several baselines and methods from the literature. We also show that due to better training processes we could recover deformations, on surrounding tissue, caused by disease. Our implementation is publicly available at https://github.com/xiat0616/pseudo-healthy-synthesis. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101719	10.1016/j.media.2020.101719													
J								Texture preservation and speckle reduction in poor optical coherence tomography using the convolutional neural network	MEDICAL IMAGE ANALYSIS										Optical coherence tomography; Speckle; Image processing; Convolution neural network	NOISE-REDUCTION; IMAGE-ENHANCEMENT; SUPPRESSION; FILTER; SHRINKAGE; ALGORITHM; REMOVAL; RECONSTRUCTION; REGULARIZATION; RESTORATION	For a poor quality optical coherence tomography (OCT) image, quality enhancement is limited to speckle residue and edge blur as well as texture loss, especially at the background region near edges. To solve this problem, in this paper we propose a de-speckling method based on the convolutional neural network (CNN). In the proposed method, we use a deep nonlinear CNN mapping model in the serial architecture, here named as OCTNet. Our OCTNet in the proposed method can fully utilize the deep information on speckles and edges as well as fine textures of an original OCT image. And also we construct an available pertinent dataset by combining three existing methods to train the model. With the proposed method, we can accurately get the speckle noise from an original OCT image. We test our method on four experimental human retinal OCT images and also compare it with three state-of-the-art methods, including the adaptive complex diffusion (ACD) method and the curvelet shrinkage (Curvelet) method as well as the shearlet-based total variation (STV) method. The performance of these methods is quantitatively evaluated in terms of image distinguishability, contrast, smoothness and edge sharpness, and also qualitatively analyzed at aspects of speckle reduction, texture protection and edge preservation. The experimental results show that our OCTNet can reduce the speckle noise and protect the structural information as well as preserve the edge features effectively and simultaneously, even where the background region near edges. And also our OCTNet has full advantages on excellent generalization, adaptiveness, robust and batch performance. These advantages make our method be suitable to process a great mass of different images rapidly without any parameter fine-turning under a time-constrained real-time situation. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101727	10.1016/j.media.2020.101727													
J								Unified generative adversarial networks for multimodal segmentation from unpaired 3D medical images	MEDICAL IMAGE ANALYSIS										Unpaired medical image; Multimodal segmentation; Generative adversarial network; Multitask learning		To fully define the target objects of interest in clinical diagnosis, many deep convolution neural networks (CNNs) use multimodal paired registered images as inputs for segmentation tasks. However, these paired images are difficult to obtain in some cases. Furthermore, the CNNs trained on one specific modality may fail on others for images acquired with different imaging protocols and scanners. Therefore, developing a unified model that can segment the target objects from unpaired multiple modalities is significant for many clinical applications. In this work, we propose a 3D unified generative adversarial network, which unifies the any-to-any modality translation and multimodal segmentation in a single network. Since the anatomical structure is preserved during modality translation, the auxiliary translation task is used to extract the modality-invariant features and generate the additional training data implicitly. To fully utilize the segmentation-related features, we add a cross-task skip connection with feature recalibration from the translation decoder to the segmentation decoder. Experiments on abdominal organ segmentation and brain tumor segmentation indicate that our method outperforms the existing unified methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101731	10.1016/j.media.2020.101731													
J								High-dimensional embedding network derived prior for compressive sensing MRI reconstruction	MEDICAL IMAGE ANALYSIS										Magnetic resonance imaging; Compressed sensing; Multi-channel prior; Proximal gradient descent	RESONANCE IMAGE-RECONSTRUCTION	Although recent deep learning methodology has shown promising performance in fast imaging, the network needs to be retrained for specific sampling patterns and ratios. Therefore, how to explore the network as a general prior and leverage it into the observation constraint flexibly is urgent. In this work, we present a multi-channel enhanced Deep Mean-Shift Prior (MEDMSP) to address the highly under-sampled magnetic resonance imaging reconstruction problem. By extending the naive DMSP via integration of multi-model aggregation and multi-channel network learning, a high-dimensional embedding network derived prior is formed. Then, we apply the learned prior to single-channel image reconstruction via variable augmentation technique. The resulting model is tackled by proximal gradient descent and alternative iteration. Experimental results under various sampling trajectories and acceleration factors consistently demonstrated the superiority of the proposed prior. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101717	10.1016/j.media.2020.101717													
J								Rubik's Cube plus : A self-supervised feature learning framework for 3D medical image analysis	MEDICAL IMAGE ANALYSIS										Self-supervised learning; 3D Medical imaging data; Rubik's cube recovery	CLASSIFICATION; SEGMENTATION	Due to the development of deep learning, an increasing number of research works have been proposed to establish automated analysis systems for 3D volumetric medical data to improve the quality of patient care. However, it is challenging to obtain a large number of annotated 3D medical data needed to train a neural network well, as such manual annotation by physicians is time consuming and laborious. Self-supervised learning is one of the potential solutions to mitigate the strong requirement of data annotation by deeply exploiting raw data information. In this paper, we propose a novel self-supervised learning framework for volumetric medical data. Specifically, we propose a pretext task, i.e., Rubik's cube+, to pre-train 3D neural networks. The pretext task involves three operations, namely cube ordering, cube rotating and cube masking, forcing networks to learn translation and rotation invariant features from the original 3D medical data, and tolerate the noise of the data at the same time. Compared to the strategy of training from scratch, fine-tuning from the Rubik's cube+ pre-trained weights can remarkablely boost the accuracy of 3D neural networks on various tasks, such as cerebral hemorrhage classification and brain tumor segmentation, without the use of extra data. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				AUG	2020	64								101746	10.1016/j.media.2020.101746													
J								An improved cuckoo search algorithm with self-adaptive knowledge learning	NEURAL COMPUTING & APPLICATIONS										Cuckoo search algorithm; Individual knowledge; Population knowledge; Global optimization	PARTICLE SWARM OPTIMIZATION; ANT COLONY OPTIMIZATION; ARTIFICIAL BEE COLONY; KRILL HERD; DIFFERENTIAL EVOLUTION; DESIGN	Cuckoo search (CS) is a one of the most efficient evolutionary for global optimization and widely applied to solve diverse problems in the real world. Despite its efficiency and wide use, CS suffers from premature convergence and poor balance between exploitation and exploration. To cope with these issues, a new cuckoo search algorithm extension based on self-adaptive knowledge learning (I-PKL-CS) is proposed. In this study, learning model with individual history knowledge and population knowledge is introduced into the CS algorithm. Individuals constantly adjust their position by using historical knowledge and communicate with each other by using their own knowledge in the optimization process. In order to reduce complexity of the I-PKL-CS algorithm, the optimal learning model is selected to exploit the potential of individual knowledge learning and population knowledge learning by adopting threshold statistics learning strategy, which provides a good trade-off between the exploration and exploitation. The accuracy and performance of the proposed approach are evaluated by eighteen classic benchmark functions and CEC 2013 test suite. Statistical comparisons of the experimental results showed that the proposed I-PKL-CS algorithm made an appropriate trade-off between exploration and exploitation. Comparing the proposed I-PKL-CS with various CS algorithms, variants of differential evolution, and improved particle swarm optimization algorithms, the results demonstrate that I-PKL-CS is a competitive new type of algorithm.																	0941-0643	1433-3058				AUG	2020	32	16					11967	11997		10.1007/s00521-019-04178-w													
J								Exploiting flower constancy in flower pollination algorithm: improved biotic flower pollination algorithm and its experimental evaluation	NEURAL COMPUTING & APPLICATIONS										Metaheuristics; Optimization; Nature-inspired algorithms; Flower pollination algorithm	OPTIMIZATION; SEARCH; METAHEURISTICS	Recent growth of metaheuristic search strategies has brought a huge progress in the domain of computational optimization. The breakthrough started since the well-known Particle Swarm Optimization algorithm had been introduced and examined. Optimization technique presented in this contribution mimics the process of flower pollination. It is build on the foundation of the first technique of this kind-known as Flower Pollination Algorithm (FPA). In this paper, its simplified and improved version, obtained after extensive performance testing, is presented. It is based on only one natural phenomena-called flower constancy-the natural mechanism allowing pollen carrying insects to remember the positions of the best pollen sources. Modified FPA, named as Biotic Flower Pollination Algorithm (BFPA) and relying solely on biotic pollinators, outperforms original FPA, which itself proved to be very effective approach. The paper first presents a short description of original FPA and the changes leading to Biotic Flower Pollination Algorithm. It also discusses performance of the modified algorithm on a full set of CEC17 benchmark functions. Furthermore, in that aspect, the comparison between BFPA and other optimization algorithms is also given. Finally, brief exemplary application of modified algorithm in the field of probabilistic modeling, related to physics and engineering, is also presented.																	0941-0643	1433-3058				AUG	2020	32	16					11999	12010		10.1007/s00521-019-04179-9													
J								A non-revisiting quantum-behaved particle swarm optimization based multilevel thresholding for image segmentation	NEURAL COMPUTING & APPLICATIONS										Image segmentation; Quantum-behaved particle swarm optimization; Non-revisiting strategy; Automatic stopping mechanism	MOTH-FLAME OPTIMIZATION; GREY WOLF OPTIMIZER; ALGORITHM; CRITERION; ENTROPY; KAPURS	Multilevel thresholding for image segmentation is one of the crucial techniques in image processing. Even though numerous methods have been proposed in literature, it is still a challenge for the existing methods to produce steady satisfactory thresholds at manageable computational cost in segmenting images with various unknown properties. In this paper, a non-revisiting quantum-behaved particle swarm optimization (NrQPSO) algorithm is proposed to find the optimal multilevel thresholds for gray-level images. The proposed NrQPSO uses the non-revisiting scheme to avoid the re-evaluation of the evaluated solution candidates. To reduce the unnecessary computation cost, the NrQPSO provides an automatic stopping mechanism which is capable of gauging the progress of exploration and stops the algorithm rationally in a natural manner. For further improving the computation efficiency, the NrQPSO employs a meticulous solution search method to overcome the drawback of the existing QPSO algorithms using the original search methods. Performance of the NrQPSO is tested on the Berkeley segmentation data set. The experimental results have demonstrated that the NrQPSO can outperform the other state-of-the-art population-based thresholding methods in terms of efficiency, effectiveness and robustness; thus, the NrQPSO can be applied in real-time massive image processing.																	0941-0643	1433-3058				AUG	2020	32	16					12011	12031		10.1007/s00521-019-04210-z													
J								Solving multiple travelling officers problem with population-based optimization algorithms	NEURAL COMPUTING & APPLICATIONS										Travelling officer problem; Parking officers management; Cuckoo search; Genetic algorithm; Particle swarm optimization	VARIABLE NEIGHBORHOOD SEARCH; SALESMAN PROBLEM; GENETIC ALGORITHM; CUCKOO SEARCH	The travelling officer problem (TOP) is a graph-based orienteering problem for modelling the patrolling routines of a parking officer monitoring an area. Recently, a spatiotemporal probabilistic model was built for TOP to estimate the leaving probability of parking cars, and relevant algorithms were applied to search for the optimal path for a parking officer to maximize the collection of parking fines from cars in violation. However, there are often multiple parking officers on duty during business hours in the central business district, which provides us with the opportunities to introduce cooperation among officers for efficient car-parking management. The multiple travelling officers problem (MTOP) is a more complex problem than the TOP because multiple officers are involved simultaneously in paths construction. In this study, the MTOP is formulated and new components are established for solving the problem. One essential component called the leader-based random-keys encoding scheme (LERK) is developed for the representation of possible solutions. Then, cuckoo search (CS), genetic algorithm (GA) and particle swarm optimization (PSO) are implemented using the proposed components and compared with other state-of-the-art GA and PSO using other solution encoding schemes to solve MTOP. In addition, two greedy selection algorithms are adopted as baselines. The performance of the algorithms is evaluated with real parking sensors data and different metrics. The experimental results show that the performance of CS and GA using LERK is considerably improved in comparison with that of other implemented algorithms.																	0941-0643	1433-3058				AUG	2020	32	16					12033	12059		10.1007/s00521-019-04237-2													
J								Network characteristics for neighborhood field algorithms	NEURAL COMPUTING & APPLICATIONS										Evolutionary algorithm; Neighborhood field; Cooperation behavior; Scale-free network; Cluster coefficient	DIFFERENTIAL-EVOLUTION; GLOBAL OPTIMIZATION; PROPERTY	Evolutionary algorithms (EAs) have been successfully applied to solve numerous optimization problems. Neighborhood field optimization algorithm (NFO) has been proposed to integrate the neighborhood field in EAs, which utilizes local cooperation behaviors to explore new solutions. In this paper, certain new NFO variants are proposed based on the cooperation of descendent neighbors. The competitive and cooperative behaviors of NFO variants provide a remarkable ability to accelerate information exchanges and achieve global search. Experimental results show that NFO variants perform better than basic and other state-of-the-art EAs under different benchmark functions. For NFO and other EAs, it is difficult to quantify benefits of local cooperation in the optimization process. For this purpose, the cooperation behaviors are analyzed in a new network approach in this paper. In the proposed NFO variants, population graph shows a scale-free network with power-law distribution. Network characteristics, i.e., degree distribution, cluster coefficient and average degree, are used to quantify the cooperation behaviors. Experimental results show that network characteristics can effectively indicate the optimization performance of NFO variants in terms of convergence rate and population diversity. NFO variants with large cluster coefficients and significant heterogeneous characteristics can achieve a significant performance improvement on numerous problems.																	0941-0643	1433-3058				AUG	2020	32	16					12061	12078		10.1007/s00521-019-04255-0													
J								Optimum design of reinforced earth walls using evolutionary optimization algorithms	NEURAL COMPUTING & APPLICATIONS										Geosynthetic reinforced earth retaining wall; Mechanically stabilized earth; Evolutionary algorithms; Differential evolution; Evolutionary strategy; Biogeography-based optimization algorithm		This study addresses the optimum cost design of mechanically stabilized earth (MSE) using geosynthetics. The design process of MSEs is mathematically programmed based on an objective function depending on the length of reinforcements and vertical distance of reinforced layers. Design restrictions control the final design to be valid in terms of constraints. The aim is to explore the efficiency of evolutionary-based algorithms in dealing with MSE optimization problem along with automating the minimum cost design of MSE walls. To this end, three evolutionary algorithms, differential evolution (DE), evolution strategy, and biogeography-based optimization algorithm (BBO), are tackled to solve this problem. Comprehensive computational simulations confirm the impact of different effective parameters variation on the final design. Finally, the BBO algorithm performed the best, while DE recorded the most unsatisfactory results.																	0941-0643	1433-3058				AUG	2020	32	16					12079	12102		10.1007/s00521-019-04264-z													
J								PSO-based novel resource scheduling technique to improve QoS parameters in cloud computing	NEURAL COMPUTING & APPLICATIONS										Virtual machine; Makespan time; Cost optimization; Processing time; Particle swarm optimization	LOAD BALANCING ALGORITHM; PARTICLE SWARM OPTIMIZATION; COST; MECHANISM; WORKLOAD; MACHINE; IAAS	Cloud computing provides infinite resources and various services for the execution of variety of applications to end users, but still it has various challenges that need to be addressed. Objective of cloud users is to select the optimal resource that meets the demand of end users in reasonable cost and time, but sometimes users pay more for short time. Most of the proposed state-of-the-art algorithms try to optimize only one parameter at a time. Therefore, a novel compromise solution is needed to make the balance between conflicting objectives. The main goal of this research paper is to design and develop a task processing framework that has the decision-making capability to select the optimal resource at runtime to process the applications (diverse and complex nature) at virtual machines using modified particle swarm optimization (PSO) algorithm within a user-defined deadline. Proposed algorithm gives non-dominance set of optimal solutions and improves various influential parameters (time, cost, throughput, task acceptance ratio) by series of experiments over various synthetic datasets using Cloudsim tool. Computational results show that proposed algorithm well and substantially outperforms the baseline heuristic and meta-heuristic such as PSO, adaptive PSO, artificial bee colony, BAT algorithm, and improved min-min load-balancing algorithm.																	0941-0643	1433-3058				AUG	2020	32	16					12103	12126		10.1007/s00521-019-04266-x													
J								A non-convex economic load dispatch problem with valve loading effect using a hybrid grey wolf optimizer	NEURAL COMPUTING & APPLICATIONS										Economic load dispatch; Grey wolf optimizer; beta-Hill climbing optimizer; Power system; Optimization	BIOGEOGRAPHY-BASED OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; HARMONY SEARCH ALGORITHM; NATURAL-SELECTION METHODS; ARTIFICIAL BEE COLONY; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; DISTRIBUTED GENERATION; SQP METHOD; PSO-SQP	Economic load dispatch (ELD) is a crucial problem in the power system which is tackled by distributing the required generation power through a set of units to minimize the fuel cost required. This distribution is subject to two main constraints: (1) equality and inequality related to power balance and power output, respectively. In the optimization context, ELD is formulated as a non-convex, nonlinear, constrained optimization problem which cannot be easily solved using calculus-based techniques. Several optimization algorithms have been adapted. Due to the complexity nature of ELD search space, the theoretical concepts of these optimization algorithms have been modified or hybridized. In this paper, the grey wolf optimizer (GWO) which is a swarm intelligence is hybridized with beta-hill climbing optimizer (beta HC) which is a local search algorithm, to improve convergence properties. GWO is very powerful in a wide search, while beta HC is very powerful in deep search. By combining the wide and deep search ability in a single optimization framework, the balance between the exploration and exploitation is correctly managed. The proposed hybrid algorithm is named beta-GWO which is evaluated using five different test cases of ELD problems: 3 generating units with 850 MW; 13 generating units with 1800 MW; 13 generating units with 2520 MW; 40 generating units with 10,500 MW; and 80 generating units with 21,000 MW.beta-GWO is comparatively measured using 49 comparative methods. The results obtained by beta-GWO outperform others in most test cases. In conclusion, the proposed beta-GWO is proved to be a powerful method for ELD problem or for any other similar problems in the power system domain.																	0941-0643	1433-3058				AUG	2020	32	16					12127	12154		10.1007/s00521-019-04284-9													
J								Evolutionary model construction for electricity consumption prediction	NEURAL COMPUTING & APPLICATIONS										Channel selection; Feature extraction; Prediction; Neural network with random weights; Evolutionary algorithm	SHORT-TERM LOAD; EXTREME LEARNING-MACHINE; PARTICLE SWARM OPTIMIZATION; ENERGY-CONSUMPTION; FEATURE-SELECTION; NEURAL-NETWORK; DIFFERENTIAL EVOLUTION; PERFORMANCE EVALUATION; WAVELET TRANSFORM; ALGORITHM	Nowadays the ever-increasing energy consumption in buildings has caused supply shortages and adverse environmental impacts. The accurate prediction of energy consumption in smart buildings may help to monitor and control energy usage. As energy consumption is inevitably affected by exogenous factors such as temperature and wind speed, it is fundamentally important to select the informative channels of the factors, to extract the valuable features from the selected channels applied to the optimal-configured model, to improve prediction accuracy. However, existing work considers these parts in an almost disjoint way and lacks a model taking them into account, which may decrease prediction performance. Motivated by this challenge, an end-to-end prediction framework, called evolutionary model construction (EMC), is proposed to focus on performing these parts jointly. To implement EMC, a two-step evolutionary algorithm (EA) is designed, where one EA is firstly used to focus on exploiting the informative channels, while a new algorithm is proposed to concentrate on selecting the suitable feature extraction methods and respective time window sizes applied to the selected channels, and selecting the parameters in the predictor. The implementation of EMC chooses neural network with random weights as the predictor due to its highly recognized efficacy. We evaluate EMC in comparison with the existing approaches on a real-world electricity consumption dataset with various auxiliary factors. The superiority of EMC is further proved by analyzing and discussing the result according to the days in 1 week, time stamps in 1 day and month information on test samples.																	0941-0643	1433-3058				AUG	2020	32	16					12155	12172		10.1007/s00521-019-04310-w													
J								Modeling and optimization of a reliable blood supply chain network in crisis considering blood compatibility using MOGWO	NEURAL COMPUTING & APPLICATIONS										Blood supply chain; Disaster management; Reliability; Multi-objective optimization; Supply chain management; Meta-heuristic algorithms	VENDOR MANAGED INVENTORY; MULTIOBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHMS; QUANTITY MODEL; EOQ MODEL; DESIGN; EFFICIENT	Due to the prominent role of blood in human life, designing an efficient blood supply chain in case of an emergency situation is essential especially considering blood compatibility. This research proposes a multi-objective model for emergency blood supply chain management considering blood compatibility, routing, and location-allocation decisions. The blood supply chain network consists of donors, collection facilities, laboratories, blood centers, and hospitals. The mathematical model aims to minimize total supply chain cost and time while maximizing minimum reliability of established routes by making decisions regarding location-allocation, blood flow, inventory levels, and optimal routes. In order to solve the problem, a novel algorithm called Multi-Objective Grey Wolf Optimizer is used and compared to two classical algorithms Multi-Objective Particle Swarm Optimization and Non-dominated Sorting Genetic Algorithm-II. Performance of the algorithms is evaluated in various test problems using powerful measures. Also, the application of the proposed model is investigated in a case study in Iran's capital, Tehran. Based on the results, important managerial insights are derived and optimal locations for facilities, inventory levels, routes and blood flow between facilities are determined.																	0941-0643	1433-3058				AUG	2020	32	16					12173	12200		10.1007/s00521-019-04343-1													
J								Feature selection using binary grey wolf optimizer with elite-based crossover for Arabic text classification	NEURAL COMPUTING & APPLICATIONS										Text classification; Optimization; Grey wolf optimizer; Arabic; Swarm intelligence	SALP SWARM ALGORITHM; INSPIRED OPTIMIZER; SEARCH; REDUCTION	Text classification is one of the challenging computational tasks in machine learning community due to the increased amounts of natural language text documents available in the electronic forms. In this process, feature selection (FS) is an essential phase because thousands of possible feature sets may be considered in text classification. This paper proposes an enhanced binary grey wolf optimizer (GWO) within a wrapper FS approach to tackle Arabic text classification problems. The proposed binary GWO is utilized to play the role of a wrapper-based feature selection technique. The performance of the proposed method using different learning models, including decision trees,K-nearest neighbour, Naive Bayes, and SVM classifiers, are investigated. Three Arabic public datasets, namely Alwatan, Akhbar-Alkhaleej, and Al-jazeera-News, are utilized to evaluate the efficacy of different BGWO-based wrapper methods. Results and analysis show that SVM-based feature selection technique with the proposed binary GWO optimizer with elite-based crossover scheme has enhanced efficacy in dealing with Arabic text classification problems compared to other peers.																	0941-0643	1433-3058				AUG	2020	32	16					12201	12220		10.1007/s00521-019-04368-6													
J								Normal parameter reduction algorithm in soft set based on hybrid binary particle swarm and biogeography optimizer	NEURAL COMPUTING & APPLICATIONS										Classification; Markov chain model; Binary particle swarm optimization; Biogeography-based optimizer; Decision-making	KRILL HERD ALGORITHM; DECISION-MAKING; FRAMEWORK	Existing classification techniques that are proposed previously for eliminating data inconsistency could not achieve an efficient parameter reduction in soft set theory, which effects on the obtained decisions. Meanwhile, the computational cost made during combination generation process of soft sets could cause machine infinite state, which is known as nondeterministic polynomial time. The contributions of this study are mainly focused on minimizing choices costs through adjusting the original classifications by decision partition order and enhancing the probability of searching domain space using a developed Markov chain model. Furthermore, this study introduces an efficient soft set reduction-based binary particle swarm optimized by biogeography-based optimizer (SSR-BPSO-BBO) algorithm that generates an accurate decision for optimal and sub-optimal choices. The results show that the decision partition order technique is performing better in parameter reduction up to 50%, while other algorithms could not obtain high reduction rates in some scenarios. In terms of accuracy, the proposed SSR-BPSO-BBO algorithm outperforms the other optimization algorithms in achieving high accuracy percentage of a given soft dataset. On the other hand, the proposed Markov chain model could significantly represent the robustness of our parameter reduction technique in obtaining the optimal decision and minimizing the search domain.																	0941-0643	1433-3058				AUG	2020	32	16					12221	12239		10.1007/s00521-019-04423-2													
J								Video tampering localisation using features learned from authentic content	NEURAL COMPUTING & APPLICATIONS										CNN; Compression; Video tampering detection; Deep learning	FORGERY	Video tampering detection remains an open problem in the field of digital media forensics. As video manipulation techniques advance, it becomes easier for tamperers to create convincing forgeries that can fool human eyes. Deep learning methods have already shown great promise in discovering effective features from data, particularly in the image domain; however, they are exceptionally data hungry. Labelled datasets of varied, state-of-the-art, tampered video which are large enough to facilitate machine learning do not exist and, moreover, may never exist while the field of digital video manipulation is advancing at such an unprecedented pace. Therefore, it is vital to develop techniques which can be trained on authentic or synthesised video but used to localise the patterns of manipulation within tampered videos. In this paper, we developed a framework for tampering detection which derives features from authentic content and utilises them to localise key frames and tampered regions in three publicly available tampered video datasets. We used convolutional neural networks to estimate quantisation parameter, deblock setting and intra/inter mode of pixel patches from an H.264/AVC sequence. Extensive evaluation suggests that these features can be used to aid localisation of tampered regions within video.																	0941-0643	1433-3058				AUG	2020	32	16					12243	12257		10.1007/s00521-019-04272-z													
J								An explanation-based approach for experiment reproducibility in recommender systems	NEURAL COMPUTING & APPLICATIONS										Recommender systems; Explanations; Reproducibility; Fuzzy logic		The offline evaluation of recommender systems is typically based on accuracy metrics such as the Mean Absolute Error and the Root Mean Squared Error for error rating prediction and Precision and Recall for measuring the quality of the top-N recommendations. However, it is difficult to reproduce the results since there are various libraries that can be used for running experiments and also within the same library there are many different settings that if not taken into consideration when replicating the results might vary. In this paper, we show that within the use of the same library an explanation-based approach can be used to assist in the reproducibility of experiments. Our proposed approach has been experimentally evaluated using a wide range of recommendation algorithms ranging from collaborative filtering to complicated fuzzy recommendation approaches that can solve the filter bubble problem, a real dataset, and the results show that it is both practical and effective.																	0941-0643	1433-3058				AUG	2020	32	16					12259	12266		10.1007/s00521-019-04274-x													
J								Integrated CS optimization and OLS for recurrent neural network in modeling microwave thermal process	NEURAL COMPUTING & APPLICATIONS										Tunnel microwave thermal process; Recurrent neuron network; Cuckoo search optimization; Orthogonal least squares	NONLINEAR-SYSTEM IDENTIFICATION; CUCKOO SEARCH ALGORITHM; DESIGN; PREDICTION; REGRESSION	In this paper, we propose a novel hybrid algorithm to construct an improved recurrent neuron network (RNN) for modeling tunnel microwave thermal process. The new design involves a hierarchical learning process, in which the recurrent neurons of RNN are optimized by the cuckoo search (CS) algorithm, while effectiveness and efficiency of the model are guaranteed by using the orthogonal least squares (OLS) method, which is a fast approach for construction of neural networks in a stepwise forward procedure. The major contribution is to integrate seamlessly the OLS model selection and CS neuron optimization in an innovative way so that it can well track the underlying dynamic of this complicated thermal process with a very sparse model. By conducting a microwave rice drying experiment, a set of real-world datasets is used to drive the RNN model. Simulation results demonstrate the effectiveness of the proposed model compared with existing well-known approaches in terms of modeling accuracy and model compactness.																	0941-0643	1433-3058				AUG	2020	32	16					12267	12280		10.1007/s00521-019-04300-y													
J								Robust anomaly identification algorithm for noisy signals: spacecraft solar panels model	NEURAL COMPUTING & APPLICATIONS										Spacecraft electrical power subsystem; Spacecraft fault detection; Spacecraft fault identification; Noisy signals feature extraction	FAULT-DETECTION; DIAGNOSIS	Prony method is an efficient feature extraction technique when applied to signals at noiseless environment. However, space environment is very rich with different sources of noise. Noise has a massive effect on Prony method that is noticeably degrades its performance. The objective of this research is to introduce an efficient and robust technique for fault detection and identification at noisy environment to one of the most vital subsystems in spacecraft, which is electrical power subsystem (EPS). Occurrence of anomaly in spacecraft EPS is mainly related to some parameters that are voltage, current and delivered power. This will directly affect the overall spacecraft operation and may result in partial or total mission loss. It is crucial for spacecraft onboard computer to be provided with a diagnosis task that keeps track of EPS parameters and efficiently detect and identify anomalies at different noise levels. The proposed approach is based on noisy signal energy contents. The signal energy contents are large enough compared with noise energy contents. The algorithm uses the short-time energy for noisy signals to robustly identify photovoltaic (PV) anomalies according to their causes like short circuit, open circuit and shading. The proposed approach is proved to be able to discriminate among different types of PV anomalies, and the results ensure the robustness of the proposed algorithm. This is carried out with different noise levels, with minimal task execution time and with small memory footprint. The proposed algorithm is considered to be a generic approach for noisy signal identification.																	0941-0643	1433-3058				AUG	2020	32	16					12281	12294		10.1007/s00521-019-04407-2													
J								Indoor human activity recognition using high-dimensional sensors and deep neural networks	NEURAL COMPUTING & APPLICATIONS										Activity recognition; Deep neural networks; High-dimensional sensors; Sensor fusion	MICRO-DOPPLER; RADAR	Many smart home applications rely on indoor human activity recognition. This challenge is currently primarily tackled by employing video camera sensors. However, the use of such sensors is characterized by fundamental technical deficiencies in an indoor environment, often also resulting in a breach of privacy. In contrast, a radar sensor resolves most of these flaws and maintains privacy in particular. In this paper, we investigate a novel approach toward automatic indoor human activity recognition, feeding high-dimensional radar and video camera sensor data into several deep neural networks. Furthermore, we explore the efficacy of sensor fusion to provide a solution in less than ideal circumstances. We validate our approach on two newly constructed and published data sets that consist of 2347 and 1505 samples distributed over six different types of gestures and events, respectively. From our analysis, we can conclude that, when considering a radar sensor, it is optimal to make use of a three-dimensional convolutional neural network that takes as input sequential range-Doppler maps. This model achieves 12.22% and 2.97% error rate on the gestures and the events data set, respectively. A pretrained residual network is employed to deal with the video camera sensor data and obtains 1.67% and 3.00% error rate on the same data sets. We show that there exists a clear benefit in combining both sensors to enable activity recognition in the case of less than ideal circumstances.																	0941-0643	1433-3058				AUG	2020	32	16					12295	12309		10.1007/s00521-019-04408-1													
J								Single and ensemble classifiers for defect prediction in sheet metal forming under variability	NEURAL COMPUTING & APPLICATIONS										Machine learning; Ensemble learning; Defect prediction; Sheet metal forming	ARTIFICIAL NEURAL-NETWORKS; DIE-ROLL HEIGHT; FINITE-ELEMENT; GENETIC ALGORITHM; PARAMETER-IDENTIFICATION; SPRINGBACK PREDICTION; NUMERICAL-SIMULATION; OPTIMIZATION; BEHAVIOR; MODEL	This paper presents an approach, based on machine learning techniques, to predict the occurrence of defects in sheet metal forming processes, exposed to sources of scatter in the material properties and process parameters. An empirical analysis of performance of ML techniques is presented, considering both single learning and ensemble models. These are trained using data sets populated with numerical simulation results of two sheet metal forming processes: U-Channel and Square Cup. Data sets were built for three distinct steel sheets. A total of eleven input features, related to the mechanical properties, sheet thickness and process parameters, were considered; also, two types of defects (outputs) were analysed for each process. The sampling data were generated, assuming that the variability of each input feature is described by a normal distribution. For a given type of defect, most single classifiers show similar performances, regardless of the material. When comparing single learning and ensemble models, the latter can provide an efficient alternative. The fact that ensemble predictive models present relatively high performances, combined with the possibility of reconciling model bias and variance, offer a promising direction for its application in industrial environment.																	0941-0643	1433-3058				AUG	2020	32	16					12335	12349		10.1007/s00521-019-04651-6													
J								Tracking changes in user activity from unlabelled smart home sensor data using unsupervised learning methods	NEURAL COMPUTING & APPLICATIONS										Human activity recognition; Unlabelled sensor data; Data visualisation; Unsupervised learning	TECHNOLOGIES; HEALTH	This paper investigates the utility of unsupervised machine learning and data visualisation for tracking changes in user activity over time. This is done through analysing unlabelled data generated from passive and ambient smart home sensors, such as motion sensors, which are considered less intrusive than video cameras or wearables. The challenge in using unlabelled passive and ambient sensors data for activity recognition is to find practical methods that can provide meaningful information to support timely interventions based on changing user needs, without the overhead of having to label the data over long periods of time. The paper addresses this challenge to discover patterns in unlabelled sensor data using kernel density estimation (KDE) for pre-processing the data, together with t-distributed stochastic neighbour embedding and uniform manifold approximation and projection for visualising changes. The methodology is developed and tested on the Aruba CASAS smart home dataset and focusses on discovering and tracking changes in kitchen-based activities. The traditional approach of using sliding windows to segment the data requires a priori knowledge of the temporal characteristics of activities being identified. In this paper, we show how an adaptive approach for segmentation, KDE, is a suitable alternative for identifying temporal clusters of sensor events from unlabelled data that can represent an activity. The ability to visualise different recurring patterns of activity and changes to these over time is illustrated by mapping the data for separate days of the week. The paper then demonstrates how this can be used to track patterns over longer time-frames which could be used to help highlight differences in the user's day-to-day behaviour. By presenting the data in a format that can be visually reviewed for temporal changes in activity over varying periods of time from unlabelled sensor data, opens up the opportunity for carers to then initiate further enquiry if variations to previous patterns are noted. This is seen as an accessible first step to enable carers to initiate informed discussions with the service user to understand what may be causing these changes and suggest appropriate interventions if the change is found to be detrimental to their well-being.																	0941-0643	1433-3058				AUG	2020	32	16					12351	12362		10.1007/s00521-020-04737-6													
J								Deep neural network-based predictive modeling of road accidents	NEURAL COMPUTING & APPLICATIONS										Gene expression programming; Deep neural network; Back-propagation neural network; Random effect negative binomial model; Accident prediction model; Road safety	STATISTICAL-ANALYSIS; FREQUENCY; SEVERITY; HIGHWAYS	This work proposes to use deep neural networks (DNN) model for prediction of road accidents. DNN consists of two or more hidden layers with large number of nodes. Accident data of non-urban sections of eight highways were collected from official records, and dataset consists of a total of 2680 accidents. The data of 16 explanatory variables related to road geometry, traffic and road environment were collected from official records as well as through field studies. Out of a total of 222 data points of accident frequency, 148 were used for training and remaining 74 to test the models. To compare the performance of DNN-based modeling approach, gene expression programming (GEP) and random effect negative binomial (RENB) models were used. A correlation coefficient value of 0.945 (root mean square error = 5.908) was achieved by DNN in comparison with 0.914 (RMSE = 7.474) by GEP, and 0.891 (RMSE = 8.862) by RENB with the test dataset, indicating an improved performance by DNN in prediction of road accidents. In comparison with DNN, though lower value of correlation coefficient was achieved by GEP model, it quantified the effects of various variables on accident frequency and provided a ranked list of variables based upon their importance.																	0941-0643	1433-3058				AUG	2020	32	16					12417	12426		10.1007/s00521-019-04695-8													
J								U-COSFIRE filters for vessel tortuosity quantification with application to automated diagnosis of retinopathy of prematurity	NEURAL COMPUTING & APPLICATIONS										COSFIRE filtering; Retinopathy of prematurity; Tortuosity estimation; Vessel segmentation	RETINAL IMAGES; BLOOD-VESSELS; COMPUTATIONAL MODEL; MATCHED-FILTER; PLUS DISEASE; SEGMENTATION; DELINEATION; AGREEMENT; SELECTION; WIDTH	Retinopathy of prematurity (ROP) is a sight threatening disorder that primarily affects preterm infants. It is the major reason for lifelong vision impairment and childhood blindness. Digital fundus images of preterm infants obtained from a Retcam Ophthalmic Imaging Device are typically used for ROP screening. ROP is often accompanied by Plus disease that is characterized by high levels of arteriolar tortuosity and venous dilation. The recent diagnostic procedures view the prevalence of Plus disease as a factor of prognostic significance in determining its stage, progress and severity. Our aim is to develop a diagnostic method, which can distinguish images of retinas with ROP from healthy ones and that can be interpreted by medical experts. We investigate the quantification of retinal blood vessel tortuosity via a novel U-COSFIRE (Combination Of Shifted Filter Responses) filter and propose a computer-aided diagnosis tool for automated ROP detection. The proposed methodology involves segmentation of retinal blood vessels using a set of B-COSFIRE filters with different scales followed by the detection of tortuous vessels in the obtained vessel map by means of U-COSFIRE filters. We also compare our proposed technique with an angle-based diagnostic method that utilizes the magnitude and orientation responses of the multi-scale B-COSFIRE filters. We carried out experiments on a new data set of 289 infant retinal images (89 with ROP and 200 healthy) that we collected from the programme in India called KIDROP (Karnataka Internet Assisted Diagnosis of Retinopathy of Prematurity). We used 10 images (5 with ROP and 5 healthy) for learning the parameters of our methodology and the remaining 279 images (84 with ROP and 195 healthy) for performance evaluation. We achieved sensitivity and specificity equal to 0.98 and 0.97, respectively, computed on the 279 test images. The obtained results and its explainable character demonstrate the effectiveness of the proposed approach to assist medical experts.																	0941-0643	1433-3058				AUG	2020	32	16					12453	12468		10.1007/s00521-019-04697-6													
J								Iterative 3D feature enhancement network for pancreas segmentation from CT images	NEURAL COMPUTING & APPLICATIONS										Iterative 3D feature enhancement; Fully convolutional network; Residual learning; Pancreas segmentation	NEURAL-NETWORKS; MODEL	Automatic and accurate pancreas segmentation from 3D computed tomography volumes is a crucial prerequisite for computer-aided diagnosis, intraoperative planning and guidance. However, this is a challenging task because of the high inter-subject variability in the shape and location of the pancreas, as well as the existence of the surrounding organs. In order to address the above challenges, we propose a novel iterative 3D feature enhancement network to segment pancreas accurately. Specifically, the multi-level integrated features and the individual features at different levels can be progressively enhanced in an iterative manner by leveraging the complementary information encoded in different features. Therefore, the non-pancreas information at lower layers can be suppressed, and the fine details of pancreas at higher layers can be increased. In addition, because the pancreas region occupies only a small part of the scan, in order to prevent the final predictions from being biased toward the background class, we design the Dice similarity coefficients loss function in the training phase to mitigate this issue. Meanwhile, deep supervision with auxiliary classifier is incorporated in the intermediate layers at each iteration to guide the back-propagation of gradient flows and boost the discriminative capability at lower layers. Finally, in order to verify the effectiveness of the proposed method, we evaluated our approach on the publicly available NIH pancreas segmentation dataset. Extensive experiments illustrate that the proposed method achieves better performance than the state-of-the-art algorithms, and it can be easily applied to other volumetric image segmentation tasks.																	0941-0643	1433-3058				AUG	2020	32	16					12535	12546		10.1007/s00521-020-04710-3													
J								3D palmprint identification using blocked histogram and improved sparse representation-based classifier	NEURAL COMPUTING & APPLICATIONS										Local phase; Histogram; Surface type (ST); Sparse representation-based classifier (SRC)	FACE RECOGNITION; DIRECTION; PATTERN; 2D	The three-dimensional (3D) palmprint-based biometrics has been an emerging approach for human recognition. However, it mainly concentrates on one-to-one verification and is not efficient for one-to-many identification. Especially, when there is a large number of samples in the database, obtaining both the accuracy and speed of recognition has been the major challenge. To handle these problems, we propose an improved sparse representation-based 3D palmprint identification scheme. The novelty of this investigation focuses on: (1) The histogram of blocked surface type and local phase of palmprint are integrated as one feature to reduce the dimensionality of 3D information and the complexity of the subsequent computations; and (2) by adding intermediate terms to the calculation of sparse coefficient, an improved sparse representation-based classifier is proposed to refine the classification results and enhance the accuracy of identification. Experimental studies demonstrate the effectiveness of the proposed scheme. Compared with other methods, the proposed solution can reduce the computational complexity and elevate the accuracy, speed, and robustness of identification. Moreover, the proposed 3D feature occupies small data volume, which makes the scheme more suitable for large-scale specimen identification applications.																	0941-0643	1433-3058				AUG	2020	32	16					12547	12560		10.1007/s00521-020-04711-2													
J								A Randomized Block-Coordinate Adam online learning optimization algorithm	NEURAL COMPUTING & APPLICATIONS										Adam; Convergence; Online learning; Optimization algorithms; Randomized block-coordinate; Stochastic gradient descent	DESCENT; CONVERGENCE	In recent years, stochastic gradient descent (SGD) becomes one of the most important optimization algorithms in many fields, such as deep learning and reinforcement learning. However, the computation of full gradient in SGD is prohibitive when dealing with high-dimensional vectors. For this reason, we propose a randomized block-coordinate Adam (RBC-Adam) online learning optimization algorithm. At each round, RBC-Adam randomly chooses a variable from a subset of parameters to compute the gradient and updates the parameters along the negative gradient direction. Moreover, this paper analyzes the convergence of RBC-Adam and obtains the regret bound,O(T) whereTis a time horizon. The theoretical results are verified by simulated experiments on four public datasets. Moreover, the simulated experiment results show that the computational cost of RBC-Adam is lower than the variants of Adam.																	0941-0643	1433-3058				AUG	2020	32	16					12671	12684		10.1007/s00521-020-04718-9													
J								Adaptive neural fault-tolerant control for uncertain MIMO nonlinear systems with actuator faults and coupled interconnections	NEURAL COMPUTING & APPLICATIONS										Fault-tolerant control; Neural networks; MIMO nonlinear systems; Intermittent actuator fault	DYNAMIC SURFACE CONTROL; EXTREME LEARNING-MACHINE; FAILURE COMPENSATION; TRACKING CONTROL; FEEDBACK; NETWORKS; DESIGN	Handling both intermittent actuator faults and coupled interconnections in uncertain multiple-input-multiple-output nonlinear system is still a challenge in the control community. In this paper, to address this issue, an adaptive neural fault-tolerant control scheme is developed. Firstly, neural networks with random hidden nodes are used to approximate unknown functions, and an inequality is introduced to construct controllers such that the singularity problem of the controllers can be circumvented. Secondly, a projection algorithm is adopted to update online the estimated parameters in the controllers such that the boundedness of estimated parameters is ensured. In particular, the boundedness of estimate of unknown fault parameters with intermittent jumps can be definitely guaranteed. Due to the effects of intermittency jumps of unknown parameters on the system stability during operation, a modified Lyapunov function is developed to prove the system stability. It is proved that the system stability only depends on the jumping amplitude of Lyapunov function and the minimum fault time interval and is not affected by the total number of faults. Thirdly, a root mean square type of bound for the tracking error is established by using iterative calculation to illustrate that the system transient performance in the sense of the tracking error is adjustable by proper choice of design parameters. Finally, simulation studies are carried out to verify the effectiveness of the theoretical results.																	0941-0643	1433-3058				AUG	2020	32	16					12755	12770		10.1007/s00521-020-04723-y													
J								Subdata image encryption scheme based on compressive sensing and vector quantization	NEURAL COMPUTING & APPLICATIONS										Image encryption; Compressive sensing; Vector quantization; Chaotic; Tamper localization	SPATIOTEMPORAL CHAOS; MAP; RECONSTRUCTION; SYSTEM	An advanced image encryption scheme should equip the capability against malicious attacks, reduce the losses under attacks, and improve the compression rate tremendously due to the unsafe network environment and the limited bandwidth resources. Recently, compressive sensing (CS) has been introduced into image encryption schemes because of the merit of low sampling rate. However, these schemes still cannot address the above requirements well. In order to improve compression rate while providing higher security level, a novel subdata image cryptosystem is proposed by introducing vector quantization (VQ) into CS-based encryption scheme. The plaintext image is first divided into VQ index blocks and the error compensations that are sparse enough to be compressed by CS. Then, the index information and CS measurements are further scrambled and diffused by chaotic sequences to achieve enhanced security. It can be ensured that the primary index information is informative and occupies smaller proportion of cipher image such that it cannot be easily tampered if only a part of the image is attacked. In contrast, the secondary error information is a good supplement to the former and occupies larger proportion. Simulation results verify that our proposed scheme has overwhelming compression rate and security effect to resist malicious attacks when compared with the state-of-art schemes. In addition, even if the important information is damaged, the destroyed pixels can be located and the plaintext image can be reconstructed with VQ neighbor indexes.																	0941-0643	1433-3058				AUG	2020	32	16					12771	12787		10.1007/s00521-020-04724-x													
J								An enhanced learning algorithm with a particle filter-based gradient descent optimizer method	NEURAL COMPUTING & APPLICATIONS										Gradient descent; Optimizer; Particle filter; Neural network; Deep learning		This experiment integrates a particle filter concept with a gradient descent optimizer to reduce loss during iteration and obtains a particle filter-based gradient descent (PF-GD) optimizer that can determine the global minimum with excellent performance. Four functions are applied to test optimizer deployment to verify the PF-GD method. Additionally, the Modified National Institute of Standards and Technology (MNIST) database is used to test the PF-GD method by implementing a logistic regression learning algorithm. The experimental results obtained with the four functions illustrate that the PF-GD method performs much better than the conventional gradient descent optimizer, although it has some parameters that must be set before modeling. The results of implementing the MNIST dataset demonstrate that the cross-entropy of the PF-GD method exhibits a smaller decrease than that of the conventional gradient descent optimizer, resulting in higher accuracy of the PF-GD method. The PF-GD method provides the best accuracy for the training model, 97.00%, and the accuracy of evaluating the model with the test dataset is 90.37%, which is higher than the accuracy of 90.08% obtained with the conventional gradient descent optimizer.																	0941-0643	1433-3058				AUG	2020	32	16					12789	12800		10.1007/s00521-020-04726-9													
J								Virtual weather stations for meteorological data estimations	NEURAL COMPUTING & APPLICATIONS										Machine learning; Neural networks; Temperature; Relative humidity; Evapotranspiration	ARTIFICIAL NEURAL-NETWORK; MACHINE LEARNING-METHODS; GLOBAL SOLAR-RADIATION; SPATIAL INTERPOLATION; REFERENCE EVAPOTRANSPIRATION; WIND-SPEED; CLIMATIC DATA; MODEL; RAINFALL; REGRESSION	In this paper, the concept of Virtual Weather Stations (VWS) is introduced. A VWS is an integration of algorithms to download meteorological data, process and use them with the main objective of estimate data in nearby locations with no meteorological stations. To develop the VWS, the performances of different interpolation methods were evaluated to test the accuracy. Daily data from an automatic weather station network, such as precipitation (Precip), air temperature (Temp), air relative humidity, mean wind speed, total solar irradiation, and reference evapotranspiration were interpolated using artificial neural networks (ANNs) with the hardlim, sigmoid, hyperbolic tangent (tanh), softsign, and rectified linear unit (relu) activations functions were employed. To contrast the ANNs interpolations, alternatives methods such as inverse distance weighting, inverse-squared distance weighting, multilinear regression, and random forest regression were used. To validate the models, a randomly selected weather station was removed from the daily datasets, and the interpolated values were compared with the actual station records. Additionally, interpolations in the summer and winter months were performed to check the capability of the models during periods with more extreme phenomena. The results showed that the interpolation methods have anR(2)up to 0.98 for variables such as temperatures for the period of 1 year. Meanwhile, during the summer and winter, the models presented lower accuracy. From a practical perspective, the methods here described could be useful to produce meteorological data with the VWS to record temperatures and dose the irrigation in crops.																	0941-0643	1433-3058				AUG	2020	32	16					12801	12812		10.1007/s00521-020-04727-8													
J								Global adaptive matrix-projective synchronization of delayed fractional-order competitive neural network with different time scales	NEURAL COMPUTING & APPLICATIONS										Matrix-projective synchronization; Fractional-order competitive neural network; Adaptive control method; Time delay; Parameter identification	MITTAG-LEFFLER STABILITY; EXPONENTIAL STABILITY; RABINOVICH SYSTEM; MULTISTABILITY; CRITERIA	In this paper, by using the adaptive control method, the global matrix-projective synchronization of delayed fractional-order competitive neural network with different time scales is researched for the first time. Firstly, the fractional-order global matrix-projective synchronization is defined. Then, in order to achieve the matrix-projective synchronization, the sufficient condition is obtained under an adaptive controller and its effectiveness is proved by combining the fractional-order Barbalat theory with a suitable Lyapunov-Krasovskii functional as well as some fractional-order differential inequalities. And all unknown parameters are identified and estimated to the fixed constants successfully. Finally, as applications, a numerical example with simulations is employed to demonstrate the feasibility and efficiency of the new synchronization analysis.																	0941-0643	1433-3058				AUG	2020	32	16					12813	12826		10.1007/s00521-020-04728-7													
J								Scale-fusion framework for improving video-based person re-identification performance	NEURAL COMPUTING & APPLICATIONS										3D convolution; Short-term fast-varying motion information; Recurrent; Scale-fusion; Species invasion		Video-based person re-identification (re-id), which aims to match people through videos captured by non-overlapping camera views, has attracted lots of research interest recently. In this paper, we first propose a novel hybrid 2D and 3D convolution-based recurrent neural network (HCRN) for video-based person re-id task. Specifically, the 3D convolutional module can explore the local short-term fast-varying motion information, while the recurrent layer can leverage the global long-term spatial-temporal information. Based on HCRN, we design a scale-fusion framework to make full use of features of different scales to further improve the performance of video-based person re-id. More concretely, the scale-fusion framework preserves a complete subnetwork similar to HCRN for each scale to extract features and exchanges information between all subnetworks at several stages of the framework. Besides, we propose a training method called species invasion to further improve the performance of HCRN and scale-fusion framework by utilizing a large amount of unlabeled data. Experimental results on the publicly available PRID 2011, iLIDS-VID and MARS multi-shot pedestrian re-id datasets demonstrate the effectiveness of the proposed HCRN, scale-fusion framework and species invasion training method.																	0941-0643	1433-3058				AUG	2020	32	16					12841	12858		10.1007/s00521-020-04730-z													
J								Hardware implementation of real-time pedestrian detection system	NEURAL COMPUTING & APPLICATIONS										ADAS; Pedestrian detection; Embedded system; Real-time; Hardware architecture	DIAGNOSIS	Histogram of oriented gradients (HOG) descriptor and support vector machine (SVM) classifier have been considered as a very efficient and robust tools used in the development of pedestrian detection systems. However, the enormous calculations make it difficult to meet real-time requirement specifically for advanced driver assistance system applications. The latter requires higher frame rate and resolution. In the present work, efficient pipelined architecture founded on cell-based scanning and simultaneous linear SVM classification is highlighted. The proposed real-time pedestrian detection architecture performs without external memories, and only 3 x 3 pixels are processed using two line buffers. Indeed, intermediate results will contribute directly to build the cell histograms. Consequently, it allows to reduce the required memory and to speed up the HOG feature extraction procedure. The implementation results on a heterogeneous platform show that the proposed architecture achieves an efficient pedestrian detection for full-HD video recordings (1080 x 1920 pixels) at 60 fps with reduced hardware resource consumption. Using the same hardware resources at 444 MHz, the proposed system turns out to handle a HD video at 180 fps.																	0941-0643	1433-3058				AUG	2020	32	16					12859	12871		10.1007/s00521-020-04731-y													
J								Deformation modeling and classification using deep convolutional neural networks for computerized analysis of neuropsychological drawings	NEURAL COMPUTING & APPLICATIONS										Neuropsychological drawings; Deformation classification; Deep visual features; Bender-Gestalt test	GESTALT TEST; DISEASE; RECOGNITION; PRESSURE; ONLINE; TREMOR; TOOL	Drawing-based tests are cost-effective, noninvasive screening methods, popularly employed by psychologists for the early detection and diagnosis of various neuropsychological disorders. Computerized analysis of such drawings is a complex task due to the high degree of deformations present in the responses and reliance on extensive clinical manifestations for their inferences. Traditional rule-based approaches employed in visual analysis-based systems prove insufficient to model all possible clinical deformations. Meanwhile, procedural analysis-based techniques may contradict with the standard test conduction and evaluation protocols. Leveraging on the increasing popularity of convolutional neural networks (CNNs), we propose an effective technique for modeling and classifying dysfunction indicating deformations in drawings without modifying clinical standards. Contrary to conventional sketch recognition applications where CNNs are trained to diminish intra-shape class variations, we employ deformation-specific augmentation to enhance the presence of specific deviations that are defined by clinical practitioners. The performance of our proposed technique is evaluated using Lacks' scoring of the Bender-Gestalt test, as a case study. The results of our experimentation substantiate that our proposed approach can represent domain knowledge sufficiently without extensive heuristics and can effectively identify drawing-based biomarkers for various neuropsychological disorders.																	0941-0643	1433-3058				AUG	2020	32	16					12909	12933		10.1007/s00521-020-04735-8													
J								An enhanced KNN-based twin support vector machine with stable learning rules	NEURAL COMPUTING & APPLICATIONS										Twin support vector machine; K-nearest neighbor; Distance-weighted; Stable learning; Machine learning	CLASSIFICATION; ALGORITHM	Among the extensions of twin support vector machine (TSVM), some scholars have utilizedK-nearest neighbor (KNN) graph to enhance TSVM's classification accuracy. However, these KNN-based TSVM classifiers have two major issues such as high computational cost and overfitting. In order to address these issues, this paper presents an enhanced regularizedK-nearest neighbor-based twin support vector machine (RKNN-TSVM). It has three additional advantages: (1) Weight is given to each sample by considering the distance from its nearest neighbors. This further reduces the effect of noise and outliers on the output model. (2) An extra stabilizer term was added to each objective function. As a result, the learning rules of the proposed method are stable. (3) To reduce the computational cost of finding KNNs for all the samples, location difference of multiple distances-basedK-nearest neighbors algorithm (LDMDBA) was embedded into the learning process of the proposed method. The extensive experimental results on several synthetic and benchmark datasets show the effectiveness of our proposed RKNN-TSVM in both classification accuracy and computational time. Moreover, the largest speedup in the proposed method reaches to 14 times.																	0941-0643	1433-3058				AUG	2020	32	16					12949	12969		10.1007/s00521-020-04740-x													
J								Paper currency defect detection algorithm using quaternion uniform strength	NEURAL COMPUTING & APPLICATIONS										Paper currency image registration; Quaternion convolution; Quaternion uniform strength; Defect detection	BANKNOTE RECOGNITION; NEURAL-NETWORK; CLASSIFICATION	In this paper, we propose a novel paper currency defect detection algorithm using quaternion uniform strength. We first build paper currency image preprocessing integration framework which includes intensity balancing, paper currency location, and geometric correction. We then propose a global-local paper currency image registration algorithm by moving key areas within certain range which can eliminate the false difference effectively. Finally, the quaternion uniform strength is calculated by using quaternion convolution edge detector. The defect degree of paper currency is determined by using the quaternion uniform color difference. The proposed algorithm is tested using different datasets from five countries: CNY, USD, EUR, VND, and RUB. The experimental results demonstrate that the proposed algorithm yields better results than the existing state-of-the-art paper currency defect detection techniques. The demo of the proposed paper currency defect detection algorithm will be publicly available.																	0941-0643	1433-3058				AUG	2020	32	16					12999	13016		10.1007/s00521-020-04745-6													
J								Post hoc analysis of sport performance with differential evolution	NEURAL COMPUTING & APPLICATIONS										Data mining; Evolutionary algorithms; Sport science; Sport training; Trainers	INTELLIGENCE	Advising athletes how to improve their performance after a race is a very important aspect of sport training. It can also be called a post hoc analysis, which often includes a deep analysis of an athlete's performance, behavior and body characteristics after a race. These analyses help trainers to adapt their training plan according to the athlete's performance on the one hand, and to modify the strategy or tactic of the racing on the other. Until recently, rare solutions of automatic analysis, using modern artificial intelligence tools, were proposed. In this paper, recent solutions are reviewed and a novel solution is proposed, which relies on heart rate data for post hoc analysis. Here, the main focus is on individual sports, where the performed time determines the quality of results (e.g., running, cycling and triathlon). The proposed solution was tested on two case studies of running athletes.																	0941-0643	1433-3058				AUG	2020	32	15					10799	10808		10.1007/s00521-018-3395-3													
J								Verbal aggression detection on Twitter comments: convolutional neural network for short-text sentiment analysis	NEURAL COMPUTING & APPLICATIONS										Aggression detection; Sentiment analysis; Machine learning; Convolutional neural network	ALGORITHM	Cyberbullying and hate speeches are common issues in online etiquette. To tackle this highly concerned problem, we propose a text classification model based on convolutional neural networks for the de facto verbal aggression dataset built in our previous work and observe significant improvement, thanks to the proposed 2D TF-IDF features instead of pre-trained methods. Experiments are conducted to demonstrate that the proposed system outperforms our previous methods and other existing methods. A case study of word vectors is carried out to address the difficulty in using pre-trained word vectors for our short-text classification task, demonstrating the necessities of introducing 2D TF-IDF features. Furthermore, we also conduct visual analysis on the convolutional and pooling layers of the convolutional neural networks trained.																	0941-0643	1433-3058				AUG	2020	32	15					10809	10818		10.1007/s00521-018-3442-0													
J								Pattern-based image retrieval using GLCM	NEURAL COMPUTING & APPLICATIONS										Gray-level co-occurrence matrix (GLCM); Texture analysis; Pattern recognition; Distance; Direction parameter	FEATURES	Gray-level co-occurrence matrix (GLCM) is one of the oldest techniques used for texture analysis. It has two important parameters, i.e., distance and direction. In this paper, various combinations of distance and directional angles used for GLCM calculation are analyzed in order to recognize certain patterned images based on their textural features. In the proposed approach, the work is divided into two modules: determining the pattern of the image and pattern retrieval from the dataset. Patterns considered in this paper are horizontal striped, vertical striped, right and left diagonally striped, checkered and other images. For recognizing the pattern, the proposed approach has achieved a percentage accuracy of 96, 98, 96, 90, 96 and 94 for horizontal striped, vertical striped, right and left diagonally striped, checkered and other irregular patterns (not fully stripped), respectively. The proposed approach has a practical implementation in the fashion industry so to filter the search according to the pattern of the cloth.																	0941-0643	1433-3058				AUG	2020	32	15					10819	10832		10.1007/s00521-018-3611-1													
J								Portfolio optimization in fuzzy asset management with coherent risk measures derived from risk averse utility	NEURAL COMPUTING & APPLICATIONS										Coherent risk measure; Fuzzy random variable; Perception-based extension; Weighted average value-at-risk; Portfolio optimization	VALUE-AT-RISK; RANDOM-VARIABLES; MODEL	A portfolio optimization problem with fuzzy random variables is discussed using coherent risk measures, which are characterized by weighted average value-at-risks with risk spectra. By perception-based approach, coherent risk measures and weighted average value-at-risks are extended for fuzzy random variables. Coherent risk measures derived from risk averse utility functions are introduced to discuss the portfolio optimization with randomness and fuzziness. The randomness is estimated by probability, and the fuzziness is evaluated by lambda-mean functions and evaluation weights. By mathematical programming approaches, a solution is derived for the risk-minimizing portfolio optimization problem. Numerical examples are given to compare coherent risk measures. It is made clear that coherent risk measures derived from risk averse utility functions have excellent properties as risk criteria for these optimization problems. Not only pessimistic and necessity case but also optimistic and possibility case are calculated numerically to deal with uncertain information.																	0941-0643	1433-3058				AUG	2020	32	15					10847	10857		10.1007/s00521-018-3683-y													
J								Bagging trees with Siamese-twin neural network hashing versus unhashed features for unsupervised image retrieval	NEURAL COMPUTING & APPLICATIONS										Siamese-twin neural network; Bagging trees; Random projection; Binary hashing; Image retrieval		The goal of this paper is twofold. Firstly, a Siamese-twin random projection neural network (ST-RPNN) is proposed for unsupervised binary hashing of images and compared with state-of-the-art techniques. Secondly, a comparison between Hamming-distance-based retrieval and a proposed bagging trees retrieval (BT-retrieval) algorithm operating directly on the PCA features is made with respect to performance, storage and search time. The ST-RPNN is made of two identical random projection neural networks and is trained to produce similar binary codes for similar input image pairs and different binary codes otherwise. The learning process is divided into two steps: a fast sparse neurons selection algorithm followed by an unsupervised bagging trees algorithm to extract the compact required-length code. Moreover, a BT-retrieval algorithm is proposed in this paper as a fast retrieval tool that ranks the database with respect to a query without distance calculations. Furthermore, (BT-PCA) is a novel extension where the BT-retrieval is applied directly on the PCA features with a significantly lower time search than Hamming-distance-based approach. The proposed technique is compared with ten unsupervised image binary hashing techniques on the COREL1K dataset and the CIFAR10 dataset. The proposed technique obtained better precision-recall results than all compared techniques on the COREL1K dataset, and better than eight of them on the CIFAR10 dataset.																	0941-0643	1433-3058				AUG	2020	32	15					10859	10871		10.1007/s00521-018-3684-x													
J								Service composition and optimal selection in cloud manufacturing: landscape analysis and optimization by a hybrid imperialist competitive and local search algorithm	NEURAL COMPUTING & APPLICATIONS										Cloud manufacturing; Service composition and optimal selection; Mixed-integer programming; Landscape analysis; Imperialist competitive algorithm	PLATFORM; STRATEGY; DESIGN	Cloud manufacturing as an emerging service-oriented manufacturing paradigm integrates and manages geographically distributed manufacturing resources such that complex and highly customized manufacturing tasks can be performed cooperatively. The service composition and optimal selection (SCOS) problem, in which manufacturing cloud services are optimally selected for performing subtasks, is one of the key issues for implementing a cloud manufacturing system. In this paper, we propose a new mixed-integer programming model for solving the SCOS problem with sequential composition structure. Unlike the majority of previous research on the problem, in the proposed model, the transportation between distributed resources and its effects on quality of services are considered. Although a wide variety of metaheuristics have been tailored for solving the SCOS problem, no consistent and comprehensive conclusion has been reached so far on the superiority of a specific algorithm. Therefore, for the first time, solution space landscape of the problem was analyzed through several statistical criteria which demonstrated that the landscape is rugged and local optima are clustered in a small region of the search space. Therefore, to find good solutions, a metaheuristic algorithm needs to perform both proper exploitation and exploration of the search space. According to the landscape analysis, the basic imperialist competitive algorithm (ICA) was hybridized with a local search (LS) algorithm resulting in the hybrid ICA (HICA). To examine the performance of the proposed HICA, an example of online motorcycle production in the USA as well as four randomly generated large-scale instances, were solved through the LS, ICA, and HICA. Computational results showed that transportation consideration is indispensable for obtaining more realistic solutions in cloud manufacturing. The results also revealed that the HICA outperformed the LS and basic ICA in terms of the value of cost objective function, the stability of solutions and convergence speed. Hence, not only statistically but also analytically, it was proved that algorithms incorporating both exploitation and exploration are able to solve the SCOS problem more efficiently.																	0941-0643	1433-3058				AUG	2020	32	15					10873	10894		10.1007/s00521-018-3721-9													
J								A carbon emission optimization model with reduction method of type-2 zigzag uncertain variable	NEURAL COMPUTING & APPLICATIONS										Type-2 zigzag uncertain variables; Expected value-based reduction method; Solid transportation problem; Fuzzy satisficing method; Intuitionistic fuzzy programming technique	SOLID TRANSPORTATION PROBLEM	The main investigation of the paper is to propose a reduction method of type-2 zigzag uncertain variables. We have also proposed an innovative, dynamic solid transportation problem (STP) considering maximization of profit and minimization of emission. The model has been presented to show an awareness among the drivers or managers of firms to emit less amount of carbon particles. The transportation parameters of the model have been taken as type-2 zigzag uncertain variables. The proposed expected value-based reduction method has been used to convert the transportation parameters into deterministic form. A real-life study has been presented here to show the use of the proposed reduction method and proposed STP. LINGO 13.0 optimization software has been used to solve the multi-objective STP using fuzzy satisficing method and intuitionistic fuzzy programming technique. The transportation parameters of the model are also presented in type-2 triangular fuzzy variables to show a comparison with the type-2 zigzag uncertain variable. A sensitivity analysis is presented with respect to different credibility levels of profit, emission, source, demand and conveyance.																	0941-0643	1433-3058				AUG	2020	32	15					10895	10914		10.1007/s00521-018-3811-8													
J								Usability feature extraction using modified crow search algorithm: a novel approach	NEURAL COMPUTING & APPLICATIONS										Crow search algorithm; Modified crow search algorithm; Software quality; Feature extraction; HCI	ENABLING TECHNOLOGIES	For the purpose of usability feature extraction and prediction, an innovative metaheuristic algorithm is introduced. Generally, the term "usability" is defined by the several researchers with respect to the hierarchical-based software usability model and it has become one of the important methods in terms of software quality. In hierarchically based software, its usability factors, attributes, and its characteristics are combined. The paper presented an algorithm, i.e., modified crow search algorithm (MCSA) mainly for extraction of usability features from hierarchical model with the optimal solution under the search for useful features. MCSA is an extension of original crow search algorithm (CSA), which is a naturally inspired algorithm. The mechanism of this algorithm is based on the process of hiding food and prevents theft and hence introduced this CSA in the field of software engineering practices as an inspiration. The algorithm generates a particular number of selected features/attributes and is applied on software development life cycles models, finding out the best among them. The results of the presented algorithm are compared with the standard binary bat algorithm (BBA), original CSA, and modified whale optimization algorithm (MWOA). The outcomes conclude that the proposed MCSA performs well than the standard BBA and original CSA as the proposed algorithms generate fewer number of feature selection equal to 17 than 18 in BBA, 23 in CSA, and 19 in MWOA.																	0941-0643	1433-3058				AUG	2020	32	15					10915	10925		10.1007/s00521-018-3688-6													
J								A deep learning approach for Parkinson's disease diagnosis from EEG signals	NEURAL COMPUTING & APPLICATIONS										Computer-aided detection system; Convolutional neural network; Deep learning; Parkinson's disease	OBJECT RECOGNITION; IDENTIFICATION	An automated detection system for Parkinson's disease (PD) employing the convolutional neural network (CNN) is proposed in this study. PD is characterized by the gradual degradation of motor function in the brain. Since it is related to the brain abnormality, electroencephalogram (EEG) signals are usually considered for the early diagnosis. In this work, we have used the EEG signals of twenty PD andtwentynormal subjects in this study. Athirteen-layer CNN architecture which can overcome the need for the conventional feature representation stages is implemented. The developed model has achieved a promising performance of 88.25% accuracy, 84.71% sensitivity, and 91.77% specificity. The developed classification model is ready to be used on large population before installation of clinical usage.																	0941-0643	1433-3058				AUG	2020	32	15					10927	10933		10.1007/s00521-018-3689-5													
J								Automatic classification of pulmonary diseases using a structural co-occurrence matrix	NEURAL COMPUTING & APPLICATIONS										Structural co-occurrence matrix; Feature extraction; Machine learning classifiers; Medical image processing; Lung disease	SEGMENTATION; RECOGNITION; FEATURES	The World Health Organization (WHO) estimates that 300 million people have asthma and that this disease causes about 250 thousand deaths per year worldwide. The number of deaths from chronic obstructive pulmonary disease increased by 30% in 2015, and this disease will, according to WHO, be the third major cause of death worldwide in 2030. The identification of diseases using medical image processing techniques is in high demand to assist medical doctors to make more accurate diagnoses. However, although these techniques contribute in making medical diagnoses, most of them still need to have some parameters set and this can be a difficult and tedious process. In this paper, a new automatic approach to identify and classify lung diseases from a structural co-occurrence matrix (SCM) in chest computed tomography images is proposed. The most important novelty of this approach is that only the image is used as the input data and extract the structural information of the disease which, in this case, is related to the lower frequencies. In order to demonstrate the efficiency of the proposed technique, it was compared with other well-known state-of-art feature extractors. In addition, the SCM was evaluated with four filters (Gaussian, Fourier, Laplace and Sobel) using linear discriminant analysis, multi-layer perceptron, support vector machines and minimal learning machine classifiers. The results showed that the SCM, when using low frequencies, is able to adapt to different images and extract the most significant structural data, without the need of any additional parameters, yet maintaining the diagnostic precision.																	0941-0643	1433-3058				AUG	2020	32	15					10935	10945		10.1007/s00521-018-3736-2													
J								Computer-aided autism diagnosis via second-order difference plot area applied to EEG empirical mode decomposition	NEURAL COMPUTING & APPLICATIONS										Autism; EEG; EMD; IMF; SODP; Pvalue; 3D mapping	SPECTRUM; CLASSIFICATION; COHERENCE; DISORDER	Autism spectrum disorder (ASD) is a name for a group of neurodevelopmental conditions that are characterized by some degree of impairment in social interaction, verbal and non-verbal communication, and difficulty in symbolic capacity and repetitive behaviors. The only protocol followed currently for ASD diagnosis is the qualitative behavioral assessment by experts through internationally established descriptive scaling standards. The assessment can, therefore, be affected by the degree of the evaluator experience as well as by the level of the descriptive standard robustness. This paper presents an EEG-based quantitative approach intended for automatic discrimination between children with typical neurodevelopment and children with ASD. The suggested work relies on second-order difference plot (SODP) area as a discriminative feature: First, every EEG channel in a 64 electrode cap-for every volunteer-is decomposed into intrinsic mode functions (IMFs) by empirical mode decomposition (EMD). Next, the second-order difference plot for the first ten intrinsic mode functions-of every channel-is sketched. Third, the value of the elliptical area -for every plot-is calculated. The 95% confidence ellipse area is used as the discriminative feature. Fourth, pairedt-student test is applied to the vectors consisting of discriminative feature values for counterpart channels/IMFs (e.g., channel FPz/IMF7 in autistic and neurotypical) for all volunteers. Finally, principal component analysis (PCA) and neural network (NN) are applied to the SODP area feature matrix for two-class classification (ASD and neurotypical). Moreover, the 3D mapping of EEG SODP area values was implemented and analyzed. The obtained results show that the conductedt-student tests yield values of less than 0.05, and that the NN two-class classification based on SODP features leads to a 94.4% accuracy, which indicates significant differences between SODP area values of children with neurotypical development and those diagnosed with ASD. The obtained results have also been emphasized by the analysis of the findings of the performed 3D mapping.																	0941-0643	1433-3058				AUG	2020	32	15					10947	10956		10.1007/s00521-018-3738-0													
J								REHAB FUN: an assistive technology in neurological motor disorders rehabilitation of children with cerebral palsy	NEURAL COMPUTING & APPLICATIONS										Cerebral palsy; Assistive technology; Brain-computer interface	VIRTUAL-REALITY; INTERNET	Cerebral palsy is a childhood condition in which there is a motor disability (palsy) caused by a static, non-progressive lesion in the brain (cerebral), produced by decreased brain oxygenation during pregnancy at birth or soon after birth. The rehabilitation of patients with disabilities such as cerebral palsy is usually accomplished through exercises performed by a team of several specialists so that the patient can act independently or minimize the need for third parties. The virtual reality environment called REHAB FUN was developed as a purpose to aid in the treatment of patients with cerebral palsy from 3 to 8 years. The REHAB FUN is composed of 10 phases (activities) and a web area for analysis of the specialists to follow the evolution of the patients. Seven patients were participated using the virtual environment twice a week for 3 months (20 sessions). We measured attention levels, errors, and time. The REHAB FUN has been shown to be a motivating tool for patients, and through the data acquired, the specialists can direct the treatment in an individualized way so that there are greater effectiveness and efficiency in the treatment of motor rehabilitation.																	0941-0643	1433-3058				AUG	2020	32	15					10957	10970		10.1007/s00521-019-04059-2													
J								Drug recommendation with minimal side effects based on direct and temporal symptoms	NEURAL COMPUTING & APPLICATIONS										Drug side effects; Disease-drug recommendation; Medical data mining; Symptom-disease modeling	ENCEPHALITIS	Consuming drug is a common action to get relief from a disease. Drugs not only cure a disease, but also create few problems like side effects. Sometimes side effects may be tolerable such as headache which automatically cures after some time, but side effects can become a dangerous health problem like affecting kidney which is irreversible. Side effects are one of the top reasons for failure of a drug to enter the market. There are lots of drugs for a disease. A drug is said to be best, if it cures the disease completely and results in minimal side effects. In this paper we first identify the correct disease for a patient based on symptoms. The symptoms may be both direct physical symptoms and temporal symptoms (which are responsible for a disease only for a time period). Then, we address the problem of recommending the best drug which makes the side effects as minimum as possible. We propose some optimization that reduces time complexity for the recommendation model. To verify the efficiency of our model, we conducted experimental test on benchmark datasets and the results show our algorithm outperforms the existing works.																	0941-0643	1433-3058				AUG	2020	32	15					10971	10978		10.1007/s00521-018-3794-5													
J								Ear recognition system using adaptive approach Runge-Kutta (AARK) threshold segmentation with ANFIS classification	NEURAL COMPUTING & APPLICATIONS										Ear biometric; Runge-Kutta threshold segmentation; ANFIS classifier; Ring-projection technique; Discrete wavelet transform		In the field of biometrics, ear recognition is a niche idea of recognition for human authentication, which has several merits compared to the other biometric recognitions like face and finger print. The contour of an ear is distinctive for each person, which is the main reason for choosing this recognition technique. Only in very few studies, the ear recognition algorithms were presented. There still remains a large space for research in the field of ear biometrics. All recent papers have implemented ear recognition algorithms using 2-D ear images. The ear recognition algorithms should be efficient in order to provide accurate results, owing to issues like multiple poses and directional related. This paper proposes a novel method for segmentation based on adaptive approach Runge-Kutta (AARK) to recognize ear images. AARK threshold segmentation technique is used for finding the threshold value to determine the region to be segmented. The utilization of AARK's numerical methods in computing the threshold value for ear recognition process improves the result accuracy. Firstly, preprocessing has been carried out for the dataset. The following steps are carried out sequentially: ring projection, information normalization, morphological operation, AARK segmentation, feature extraction of DWT and finally ANFIS classifier are used. Among the various steps mentioned, ring projection converts the two dimensions into single dimensions. The self-adaptive discrete wavelet transform is used to extract features from the segmented region. Then the ANFIS classifier is used to recognize the ear region from the image by taking the features form the test image and the training images. The proposed method obtained 72% improvement in PSNR and accuracy is improved to 63.3%. Moreover, the speed and space occupation of the self-adaptive DWT technique and the conventional DWT technique are measured by implementing the methods in FPGA Spartan 6 device. Comparing with the implementation of conventional DWT, the area is reduced to 361 from 7021 while implementing the proposed self-adaptive DWT method.																	0941-0643	1433-3058				AUG	2020	32	15					10995	11006		10.1007/s00521-018-3805-6													
J								A proposal for Internet of Smart Home Things based on BCI system to aid patients with amyotrophic lateral sclerosis	NEURAL COMPUTING & APPLICATIONS										Brain-computer interfaces; Amyotrophic lateral sclerosis; Internet of Healthcare Things; Personal digital assistant	ENABLING TECHNOLOGIES; DIAGNOSIS	Smart devices are already present in all areas of knowledge: sometimes as the main role, and sometimes as the supporting role. Regardless of their function, smart devices communicate with other devices, monitor, and control everything in the physical world. In this way, the Internet of Things (IoT) has been gaining ramifications. The one that is growing most in the area of health, and that is becoming more and more popular, is IoHT. Several treatments have been employed around the world based around IoHT, including one for amyotrophic lateral sclerosis (ALS). ALS is one of four existing neuromotor diseases without a cure to date. With the progression of the disease, patients lose the strength of the body's voluntary muscles until total paralysis. Other consequences of the progression of the disease are the loss of autonomy in some aspects of life in addition to the loss of self-esteem. This work aims to develop a BCI system based on IoHT using a personal digital assistant brainwave sensor to assist patients with ALS in performing predefined tasks. To validate this tool as a complement to traditional medical treatment, ten volunteers conducted tests on the proposed solution. Results were very promising.																	0941-0643	1433-3058				AUG	2020	32	15					11007	11017		10.1007/s00521-018-3820-7													
J								Toward a novel method to support decision-making process in health and behavioral factors analysis for the composition of IT projects teams	NEURAL COMPUTING & APPLICATIONS										Decision support systems; Geographically distributed projects teams; Multicriteria; Psychological disorders; Verbal decision analysis	ALZHEIMERS-DISEASE; DISORDERS; DIAGNOSIS	Information technology (IT) is a competitive advantage to organizations. To achieve this, organizations require innovative IT projects. These, in turn, require the availability and performance of qualified, ethical and healthy professionals. When selecting professionals, the organization must draw attention to technical, behavior and health factors, since these factors influence professionals' productivity. Decision-makers need support models for professionals' selective processes when structuring teams, including multiple criteria. This study proposes a novel decision-making support model structured in methods of "verbal decision analysis (VDA)" and "measuring attractiveness by a category-based evaluation technique (MACBETH)" to solve a conflicted dilemma, seeking to assist this process. The model was tested to analyze the desired factors in professionals' selection process for the composition of geographically distributed teams responsible for developing and implementing IT projects. For the input values, the study used personal and professional qualitative factors, such as interpersonal skills, psychological health, physiological, cultural and intellectual indicators. When tested, the model solved the dilemma through alternatives categorization and prioritization. The test run showed inconsistency's absence, which would not happen if the methods (VDA and MACBETH) were run individually. The model was executed successfully and had low consumption time and accurate results. Psychological and physiological factors appear to be more defining to the decision making than intellectual level and technical qualifications. Big Data mining and exploration may be a professionals' attribute obtention way to support the model.																	0941-0643	1433-3058				AUG	2020	32	15					11019	11040		10.1007/s00521-018-3830-5													
J								Evaluation on diabetic plantar pressure data-set employing auto-segmentation technologies	NEURAL COMPUTING & APPLICATIONS										Plantar pressure images; Image preprocess; Edge detection; Threshold-based image segmenting; Image clustering; Region growing	IMAGES; CLASSIFICATION; PAIN	Plantar pressure imaging technologies play more important role in diabetics' shoe-last design. The plantar pressure imaging data-set was acquired through pressure sensors system and was preprocessed using typical image process technologies including Gauss filtering, gamma correction, and wavelet transform enhancement in this work. To decrease the computational complexity, edge detection operator with Sobel, Roberts, Prewitt, Log and Canny were applied. Finally, threshold-based method, gray, watershed, feature clustering and fuzzy cluster, region growing-based plantar pressure image segmentation were employed, respectively. Results illustrated that threshold performs the better effectiveness by using analytic hierarchy process evaluation method through a special indices definition. The proposed methods in the research will be potential application and guidance for comfort shoe design for diabetics.																	0941-0643	1433-3058				AUG	2020	32	15					11041	11054		10.1007/s00521-018-3838-x													
J								Adaptive computing-based biometric security for intelligent medical applications	NEURAL COMPUTING & APPLICATIONS										Medical applications; Adaptive computing; Interpulse intervals; Secret keys; Biometric security	HEART-RATE-VARIABILITY; BODY; AREA; CARE	For intelligent medical systems, clinical information security is one of critical requirements. Recently, random binary sequences (RBSs) based on interpulse intervals (IPIs) were applied as secret keys to secure medical information in medical applications. Most of the existing RBS generation methods acquire a uniform quantity of bits per IPI, thereby requiring more processing time. However, the functional capacity of humans influences their heart rate variability, thereby resulting in the extraction of adaptive entropic bits per IPI across individuals. Therefore, adaptive computing-based RBS generation method is proposed to extract a variable number of bits on the basis of the heart rate (HR) bands of individuals to provide a balance between processing time and security in WBSNs. The proposed method is evaluated by using ECG recordings of 126 subjects with dynamic scenarios. Our experimental results that 128-bit RBSs generated by applying the proposed method can be used as secret keys for entity identifiers or patient's data encryption for securing intelligent medical applications. In this study, the hamming distance metric is used to measure the uniqueness of the generated RBSs, and randomness of RBSs is computed by means of statistical tests, for different HR bands. Furthermore, the processing time of the proposed method for RBS generation shows improvement compared with the conventional techniques. The proposed approach is approximately three times faster for 55 <= HR < 80 and approximately two times faster for 80 <= HR < 105 and 80 <= HR < 105 than the existing IPI-based RBS generation techniques. Therefore, this study has got real-time significance for smart healthcare applications.																	0941-0643	1433-3058				AUG	2020	32	15					11055	11064		10.1007/s00521-018-3855-9													
J								Lung nodule malignancy classification in chest computed tomography images using transfer learning and convolutional neural networks	NEURAL COMPUTING & APPLICATIONS										Lung nodule malignancy classification; Convolutional neural networks; Transfer; learning; Computer-aided diagnoses	LEVEL CLASSIFICATION; CANCER; DIAGNOSIS	Lung cancer accounts for more than 1.5 million deaths worldwide, and it corresponded to 26% of all deaths due to cancer in 2017. However, lung computer-aided diagnosis systems developed to identify lung cancer at early stages are increasing survival rates. This study explores the performance of deep transfer learning from non-medical images on lung nodule malignancy classification tasks in order to improve such systems. Initially, the 1018 chest computed tomography (CT) examinations and medical annotations from the LIDC/IDRI were processed. Then, several convolutional neural networks (VGG16, VGG19, MobileNet, Xception, InceptionV3, ResNet50, InceptionResNetV2, DenseNet169, DenseNet201, NASNetMobile and NASNetLarge) were built, trained on the ImageNet dataset, converted into feature extractors and applied on the LIDC/IDRI nodule images. Following this, each set of deep features was submitted to 10-fold cross-validations with naive Bayes, multilayer perceptron, support vector machine (SVM), K-nearest neighbors KNN and random forest classifiers. Finally, the evaluation metrics accuracy (ACC), area under the curve (AUC), true positive rate (TPR), precision (PPV) and F1-score of each cross-validation average result were computed and compared. The results showed that the deep feature extractor based on the ResNet50 and the SVM RBF classifier, achieved an AUC metric of 93.1% (the highest value not only among the evaluated combinations, but also among the related works in the literature evaluated), a TPR of 85.38%, an ACC of 88.41%, a PPV of 73.48% and an F1-score of 78.83%. Based on these results, deep transfer learning proves to be a relevant strategy to extract representative features from lung nodule CT images.																	0941-0643	1433-3058				AUG	2020	32	15					11065	11082		10.1007/s00521-018-3895-1													
J								A novel deep learning-based multi-model ensemble method for the prediction of neuromuscular disorders	NEURAL COMPUTING & APPLICATIONS										Deep learning; Ensemble; Multi-model; Neuromuscular disorder; Bhattacharya coefficient	FEATURE-SELECTION; NEURAL-NETWORKS; DIAGNOSIS	Neuromuscular disorder is a complex progressive health problem which results in muscle weakness and fatigue. In recent years, with emergence and development of machine learning- and sequencing-driven technologies, the prediction of neuromuscular disorders could be made on the basis of gene expression for accurate diagnosis of disease. The intent is to correctly distinguish the patients affected from neuromuscular disorder from the healthy one with the help of various classification methods used in machine learning. In this paper, we proposed a novel feature selection method which applies deep learning method for grouping the outputs generated through various classifiers. The feature selection is performed on the basis of integrated Bhattacharya coefficient and genetic algorithm (GA) where fitness is computed on the basis of ensemble outputs of various classifiers which is performed using deep learning methods. The Bhattacharya coefficient computed the most effective gene subset; then, the most discriminative gene subset will be formulated using GA. The proposed integrated deep learning multi-model ensemble method was applied on two commercially available neuromuscular disorder datasets. The obtained results encouraged that the proposed integrated approach enhances the prediction accuracy of neuromuscular disorders as compared with different datasets and other classifier algorithms. The proposed deep learning-driven ensemble method provides more accurate and effective results for neuromuscular disorder prediction and classification with the help of distinguished classifiers.																	0941-0643	1433-3058				AUG	2020	32	15					11083	11095		10.1007/s00521-018-3896-0													
J								Revisiting computer networking protocols by wireless sniffing on brain signal/image portals	NEURAL COMPUTING & APPLICATIONS										Brain signal; image; Computer networking protocols; DNS; Image reconstruction; HTTP response; Nmap; Vulnerability; Wireshark		Brain signal/image processing is of significant attention not only the physiologist carrying out analysis and probe and the clinician investigating patients but further to the biomedical engineer who is vital for acquisition, processing, and interpreting the electroencephalogram signals by designing systems and algorithms for their control. The precious, abundant materials or information in the field of brain signal/image processing is distributed in the diverse scientific, technological and physiological periodicals, magazines, journals, international conference proceedings, and also in various portals/databases. Security threats or attacks may happen for a portal using data interruption, information interception, content modification and fabrication with new data. This study interprets the protocol layering information for the captured packets, image reconstruction after sniffing the packets, and the DNS/rDNS response times for a given portal/IP address using a Wireshark open source tool. Also, the security assessment results such as OS fingerprinting and port sweeping on the remote machines are performed using Nmap open source tool. Results are analyzed on specific brain signal/image processing portals around the globe located in USA, UK, and other countries.																	0941-0643	1433-3058				AUG	2020	32	15					11097	11109		10.1007/s00521-018-3919-x													
J								A new EEG software that supports emotion recognition by using an autonomous approach	NEURAL COMPUTING & APPLICATIONS										Emotion recognition; Software architecture; Support vector machine; Autonomous bat algorithm	EMPIRICAL MODE DECOMPOSITION; ALGORITHM; SEARCH	Human behavior is manly addressed by emotions. One of the most accepted models that represent emotions is known as the circumplex model. This model organizes emotions into points on a bidimensional plane: valence and arousal. Despite the importance of the emotion recognition, there are limited initiatives that seek to classify emotions easily in an uncontrolled environment. In this work, we present the architecture and the design of an extensible software which allows recognizing and classifying emotions by using a low-cost EEG. The proposed software implements an emotion classifier although a support vector machines (SVM) are boosted with an autonomous bio-inspired approach. The contribution was experimentally evaluated by taking a set of well-known validated EEG Databases for Emotion Recognition. Computational experiments show promising results. Using our proposal for EEG emotion classification, we reach an accuracy close to 95%. The results obtained confirm that our approach is able to overcome to a commonly used SVM classifier and that the proposed software can be useful in real environments.																	0941-0643	1433-3058				AUG	2020	32	15					11111	11127		10.1007/s00521-018-3925-z													
J								Energy demand classification by probabilistic neural network for medical diagnosis applications	NEURAL COMPUTING & APPLICATIONS										Energy demand; Prediction; Probabilistic neural network; Classification	ELECTRICITY DEMAND; PREDICTION; MODEL	Forecasting in the field of power management is essential in recent days, due to the high electrical consumption at household and medical diagnosis applications to classify the electricity usage. It is highly impossible to identify the more accurate calculations in electricity consumption due to many uncertainties. This paper helps to overcome these uncertainties into probabilities by utilizing probabilistic neural network (PNN). The most complicated, complex and non-defined problems are well tackled by PNN as it is universally accepted as the best alternative technique. The conventional way of programming is not done but it is trained on the basis of behavioral representation of the data using the previous history. Multiple applications have been benefited using this system. Generally, PNN is used to differentiate four kinds of data produced from various grids and simultaneously the data of the grid are classified. 95% of reliability and accuracy is obtained from calculations produced from PNN as per the data results. The design can be used for appropriate grid development and to classify electricity usage.																	0941-0643	1433-3058				AUG	2020	32	15					11129	11136		10.1007/s00521-018-03978-w													
J								Novel deep genetic ensemble of classifiers for arrhythmia detection using ECG signals	NEURAL COMPUTING & APPLICATIONS										ECG; Biomedical signal processing and analysis; Machine learning; Genetic algorithms; Ensemble learning; Deep learning	CONVOLUTIONAL NEURAL-NETWORK; HEARTBEAT CLASSIFICATION; AUTOMATED DETECTION; RECOGNITION; MODEL	The heart disease is one of the most serious health problems in today's world. Over 50 million persons have cardiovascular diseases around the world. Our proposed work based on 744 segments of ECG signal is obtained from the MIT-BIH Arrhythmia database (strongly imbalanced data) for one lead (modified lead II), from 29 people. In this work, we have used long-duration (10 s) ECG signal segments (13 times less classifications/analysis). The spectral power density was estimated based on Welch's method and discrete Fourier transform to strengthen the characteristic ECG signal features. Our main contribution is the design of a novel three-layer (48 + 4 + 1) deep genetic ensemble of classifiers (DGEC). Developed method is a hybrid which combines the advantages of: (1) ensemble learning, (2) deep learning, and (3) evolutionary computation. Novel system was developed by the fusion of three normalization types, four Hamming window widths, four classifiers types, stratified tenfold cross-validation, genetic feature (frequency components) selection, layered learning, genetic optimization of classifiers parameters, and new genetic layered training (expert votes selection) to connect classifiers. The developed DGEC system achieved a recognition sensitivity of 94.62% (40 errors/744 classifications), accuracy = 99.37%, specificity = 99.66% with classification time of single sample = 0.8736 (s) in detecting 17 arrhythmia ECG classes. The proposed model can be applied in cloud computing or implemented in mobile devices to evaluate the cardiac health immediately with highest precision.																	0941-0643	1433-3058				AUG	2020	32	15					11137	11161		10.1007/s00521-018-03980-2													
J								Automated detection of chronic kidney disease using higher-order features and elongated quinary patterns from B-mode ultrasound images	NEURAL COMPUTING & APPLICATIONS										Chronic kidney disease; Bispectrum; Cumulants; Elongated quinary pattern; Locality sensitive discriminant analysis; Ultrasound	DEEP; ENTROPY	Chronic kidney disease (CKD) is a continuing loss of kidney function, and early detection of this disease is fundamental to halting its progression to end-stage disease. Numerous methods have been proposed to detect CKD, mainly focusing on classification based upon peripheral clinical parameters and quantitative ultrasound parameters that must be manually calculated, or on shear wave elastography. No studies have been found that detect the presence or absence of CKD based solely from one B-mode ultrasound image. In this work, we propose an automated system to detect chronic kidney disease utilizing only the automatic extraction of features from a B-mode ultrasound image of the kidney, with a database of 405 images. Higher-order bispectrum and cumulants, and elongated quinary patterns, are extracted from each image to provide a final total of 24,480 features per image. These features were subjected to a locality sensitive discriminant analysis (LSDA) technique, which provides 30 LSDA coefficients. The coefficients were arranged according to theirtvalue and inserted into various classifiers, to yield the best diagnostic accuracy using the least number of features. The best performance was obtained using a support vector machine and a radial basis function, utilizing only five features, resulting in an accuracy of 99.75%, a sensitivity of 100%, and a specificity of 99.57%. Based upon these findings, it is evident that the technique accurately and automatically identifies subjects with and without CKD from B-mode ultrasound images.																	0941-0643	1433-3058				AUG	2020	32	15					11163	11172		10.1007/s00521-019-04025-y													
J								Robust twin support vector regression based on Huber loss function	NEURAL COMPUTING & APPLICATIONS										Huber M-estimator; Kernel methods; Regression; Support vector machine	MACHINE; CLASSIFICATION; ALGORITHM	Construction of robust regression learning models to fit data with noise is an important and challenging problem of data regression. One of the ways to tackle this problem is the selection of a proper loss function showing insensitivity to noise present in the data. Since Huber function has the property that inputs with large deviations of misfit are penalized linearly and small errors are squared, we present novel robust regularized twin support vector machines for data regression based on Huber and epsilon-insensitive Huber loss functions in this study. The proposed regression models result in solving a pair of strongly convex minimization problems in simple form in primal whose solutions are obtained by functional and Newton-Armijo iterative algorithms. The finite convergence of Newton-Armijo algorithm is proved. Numerical tests are performed on noisy synthetic and benchmark datasets, and their results are compared with few popular regression learning algorithms. The comparative study clearly shows the robustness of the proposed regression methods and further demonstrates their effectiveness and suitability.																	0941-0643	1433-3058				AUG	2020	32	15					11285	11309		10.1007/s00521-019-04625-8													
J								Interactive knowledge-enhanced attention network for answer selection	NEURAL COMPUTING & APPLICATIONS										Question-answer selection; Knowledge graph; Attention mechanism; Deep learning		Answer selection which aims to select the most appropriate answers from a set of candidate answers plays a crucial role in various applications such as question answering (QA) and information retrieval. Recently, remarkable progress has been achieved on matching sequence pairs by deep neural networks. However, most of them focus on learning semantic representations for the contexts of QA pairs while the background information and facts beyond the context are neglected. In this paper, we propose an interactive knowledge-enhanced attention network for answer selection (IKAAS), which interactively learns the sentence representations of query-answer pairs by simultaneously considering the external knowledge from knowledge graphs and textual information of QA pairs. In this way, we can exploit the semantic compositionality of the input sequences and capture more comprehensive knowledge-enriched intra-document features within the question and answer. Specifically, we first propose a context-aware attentive mechanism to learn the knowledge representations guided by the corresponding context. The relations between the question and answer are then captured by computing the question-answer alignment matrix. We further employ self-attention to capture the global features of the input sequences, which are then used to calculate the relevance score of the question and answer. Experimental results on four real-life datasets demonstrate thatIKAASoutperforms the compared methods. In addition, a series of analyses shows the robust superiority and the extensive applicability of the proposed method.																	0941-0643	1433-3058				AUG	2020	32	15					11343	11359		10.1007/s00521-019-04630-x													
J								Noise-suppressing zeroing neural network for online solving time-varying nonlinear optimization problem: a control-based approach	NEURAL COMPUTING & APPLICATIONS										Noise-suppressing zeroing neural network (NSZNN) model; Time-varying nonlinear optimization; Random noises; Global convergence; Exponential convergence; Online solution	CONJUGATE-GRADIENT METHOD	Time-varying nonlinear optimization problems with different noises often arise in the fields of scientific and engineering research. Noises are unavoidable in the practical workspace, but the most existing models for time-varying nonlinear optimization problems carry out with one assume that the computing process is free of noises. In this paper, from a control-theoretical framework, noise-suppressing zeroing neural dynamic (NSZND) model is developed, analyzed and investigated by feat of continuous-time zeroing neural network model, which behaves efficiently for hurdling online time-varying nonlinear optimization problems with the presence of different noises. Further, for speeding the rate of convergence, general noise-suppressing zeroing neural network (GNSZNN) model with different activation functions is discussed. Then, theoretical analyses show that the proposed noise-suppressing zeroing neural network model derived from NSZND model has the global convergence property in the presence of different kinds of noises. Besides, how GNSZNN model performs with different activation functions is also proved in detail. In addition, numerical results are provided to substantiate the feasibility and superiority of GNSZNN model for online time-varying nonlinear optimization problems with inherent tolerance to noises.																	0941-0643	1433-3058				AUG	2020	32	15					11505	11520		10.1007/s00521-019-04639-2													
J								Hierarchical stochastic graphlet embedding for graph-based pattern recognition	NEURAL COMPUTING & APPLICATIONS										Graph embedding; Hierarchical graph; Stochastic graphlets; Graph hashing; Graph classification	EDIT DISTANCE; VECTOR-SPACE; DATABASE; KERNELS	Despite being very successful within the pattern recognition and machine learning community, graph-based methods are often unusable because of the lack of mathematical operations defined in graph domain. Graph embedding, which maps graphs to a vectorial space, has been proposed as a way to tackle these difficulties enabling the use of standard machine learning techniques. However, it is well known that graph embedding functions usually suffer from the loss of structural information. In this paper, we consider the hierarchical structure of a graph as a way to mitigate this loss of information. The hierarchical structure is constructed by topologically clustering the graph nodes and considering each cluster as a node in the upper hierarchical level. Once this hierarchical structure is constructed, we consider several configurations to define the mapping into a vector space given a classical graph embedding, in particular, we propose to make use of the stochastic graphlet embedding (SGE). Broadly speaking, SGE produces a distribution of uniformly sampled low-to-high-order graphlets as a way to embed graphs into the vector space. In what follows, the coarse-to-fine structure of a graph hierarchy and the statistics fetched by the SGE complements each other and includes important structural information with varied contexts. Altogether, these two techniques substantially cope with the usual information loss involved in graph embedding techniques, obtaining a more robust graph representation. This fact has been corroborated through a detailed experimental evaluation on various benchmark graph datasets, where we outperform the state-of-the-art methods.																	0941-0643	1433-3058				AUG	2020	32	15					11579	11596		10.1007/s00521-019-04642-7													
J								Axiomatic fuzzy set theory-based fuzzy oblique decision tree with dynamic mining fuzzy rules	NEURAL COMPUTING & APPLICATIONS										Fuzzy oblique decision tree; Fuzzy rule extraction; AFS theory; Decision function	FRAMEWORK	This paper proposes a novel classification technology-fuzzy rule-based oblique decision tree (FRODT). The neighborhood rough sets-based FAST feature selection (NRS_FS_FAST) is first introduced to reduce attributes. In the axiomatic fuzzy set theory framework, the fuzzy rule extraction algorithm is then proposed to dynamically extract fuzzy rules. And these rules are regarded as the decision function during the tree construction. The FRODT is developed by expanding the unique non-leaf node in each layer of the tree, which results in a new tree structure with linguistic interpretation. Moreover, the genetic algorithm is implemented on sigma\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\sigma $$\end{document}to obtain the balanced results between classification accuracy and tree size. A series of comparative experiments are carried out with five classical classification algorithms (C4.5, BFT, LAD, SC and NBT), and recently proposed decision tree HHCART on 20 UCI data sets. Experiment results show that the FRODT exhibits better classification performance on accuracy and tree size than those of the rival algorithms.																	0941-0643	1433-3058				AUG	2020	32	15					11621	11636		10.1007/s00521-019-04649-0													
J								Face super-resolution via nonlinear adaptive representation	NEURAL COMPUTING & APPLICATIONS										Face super-resolution; Local-face; Contextual patch; Nonlinear adaptive representation; Residual learning	PATCH; HALLUCINATION	Face super-resolution is an example of super-resolution technique, where it takes one or multiple observed low-resolution images and then converts them to high-resolution image. Learning-based face super-resolution depends on prior information from training database. Most patch-based face super-resolution methods assume the homoscedasticity of the reconstruction error in an objective function and solve it with regularized least squares. In fact, the heteroscedasticity generally exists both in the observed data and in the reconstruction error. To access accurate prior information, we propose a nonlinear adaptive representation (NAR) scheme for hallucinating the individuality of facial images. First, we apply a weighted regularization process to both the reconstruction error and representation coefficients terms to eliminate the heteroscedasticity of the input data. Then, the contextual patches and residual high-frequency components are explored to enrich the prior information. Moreover, a nonlinear extension of the adaptive representation fully utilizes accurate prior information to achieve better reconstruction performance. Experiments on the CAS-PEAL-R1, Webface and LDHF databases show that NAR outperforms some state-of-the-art face super-resolution methods including some deep learning-based approaches.																	0941-0643	1433-3058				AUG	2020	32	15					11637	11649		10.1007/s00521-019-04652-5													
J								An unmanned aerial vehicle-aided node localization using an efficient multilayer perceptron neural network in wireless sensor networks	NEURAL COMPUTING & APPLICATIONS										Wireless sensor networks; Localizations; RSSI; UAV; DE; MLP	DIFFERENTIAL EVOLUTION; ALGORITHM	Localization of sensor node is decisive for many localization-based scenarios of wireless sensor networks (WSNs). Node localization using fixed terrestrial anchor nodes (ANs) equipped with global positioning system (GPS) modules suffers from high deployment cost and poor localization accuracy, because the terrestrial AN propagates signals to the unknown nodes (UNs) through unreliable ground-to-ground channel. However, the ANs deployed in unmanned aerial vehicles (UAVs) with a single GPS module communicate over reliable air-to-ground channel, where almost clear line-of-sight path exists. Thus, the localization accuracy and deployment cost are better with aerial anchors than terrestrial anchors. However, still the nonlinear distortions imposed in propagation channel limit the performance of classical RSSI and least square localization schemes. So, the neural network (NN) models can become good alternative for node localization under such nonlinear conditions as they can do complex nonlinear mapping between input and output. Since the multilayer perceptron (MLP) is a robust tool in the assembly of NNs, MLP-based localization scheme is proposed for UN localization in UAV-aided WSNs. The detailed simulation analysis provided in this paper prefers the MLP localization scheme as they exhibit improved localization accuracy and deployment cost.																	0941-0643	1433-3058				AUG	2020	32	15					11651	11663		10.1007/s00521-019-04653-4													
J								A new approach for intrusion detection system based on training multilayer perceptron by using enhanced Bat algorithm	NEURAL COMPUTING & APPLICATIONS										Intrusion detection system; Multilayer perceptron; Swarm intelligence; Bat algorithm; Global optimization problem; Mutation operator	PARTICLE SWARM OPTIMIZATION; FEEDFORWARD NEURAL-NETWORKS; FEATURE-SELECTION; ANOMALY DETECTION; GENETIC ALGORITHM; DECISION TREE; INTELLIGENCE; KDD99	The most pressing issue in network security is the establishment of an approach that is capable of detecting violations in computer systems and networks. There have been several efforts for improving it from various points of view. One example is the improvement of the classification of packets on the network, which is imperative in detecting abnormal traffic and hence any potential intrusion. Thus, this study proposes a new approach for intrusion detection that is implemented using an enhanced Bat algorithm (EBat) for training an artificial neural network. The goal of the current study is to increase the accuracy of the classification for malicious and un-malicious network traffic. The proposed study herein includes a comparison with nine other metaheuristic algorithms (conventional and new algorithms) that are used to evaluate the new approach alongside the related works. Firstly, the EBat algorithm was developed and used to select suitable weights and biases. Next, the neural network was employed using the found optimal weights and biases to realize the intrusion detection approach. Four types of intrusion detection evaluation datasets were used to compare the proposed approach against the other algorithms. The findings revealed that the proposed method outperformed the other nine classification algorithms and it is unparalleled for the network intrusion detection.																	0941-0643	1433-3058				AUG	2020	32	15					11665	11698		10.1007/s00521-019-04655-2													
J								A hybrid algorithm using particle swarm optimization for solving transportation problem	NEURAL COMPUTING & APPLICATIONS										Discrete optimization problem; Combinatorial optimization problem; Transportation problem; Particle swarm optimization; Optimal solution	STABILITY ANALYSIS; DESIGN	Particle swarm optimization (PSO) is a well-known population-based stochastic optimization algorithm intended by collective and communicative behavior of bird flocks looking for food. Being a very powerful tool for obtaining the global optimal solution, PSO has experienced a multitude of enhancements during the last three decades. The algorithm has been modified, hybridized and extended by various authors in terms of structural variations, parameters selection and tuning, convergence analysis and meta-heuristics. In this article, hybridized PSO has been proposed to solve balanced transportation problem, a discrete optimization problem, of any number of decision variables converging to the global optima. Two additional modules have been embedded within the PSO, in order to repair the negative and/or fractional values of the decision variables, and tested with variants of parameters present therein. The proposed algorithm generates an optimal solution even without considering the rigid conditions of the traditional techniques. The paper compares the performance of different variants of inertia weight, acceleration coefficients and also the population size with respect to the convergence to the optimal solution. The performance of the proposed algorithm is statistically validated using the pairedttest.																	0941-0643	1433-3058				AUG	2020	32	15					11699	11716		10.1007/s00521-019-04656-1													
J								ISA: a hybridization between iterated local search and simulated annealing for multiple-runway aircraft landing problem	NEURAL COMPUTING & APPLICATIONS										Aircraft landing problem; Iterated local search; Simulated annealing; Iterated simulated annealing; Scheduling optimization problems	OPTIMIZATION; ALGORITHMS	This paper presents an efficient method for aircraft landing problem (ALP) based on a mechanism that hybridizes the iterated local search (ILS) and simulated annealing (SA) algorithms. ALP is handled by scheduling each incoming aircraft to land on a runway in accordance with a predefined landing time frame. The main objective to address is to find a feasible aircraft scheduling solution within the range of target time. The proposed hybridization method complements the advantages of both ILS and SA in a single optimization framework, referred to as iterated simulated annealing (ISA). The optimization framework of ISA has two main loops: an inner loop and an outer loop. In the inner loop, SA is utilized through a cooling schedule and a relaxing acceptance strategy to allow ISA to escape the local optima. In the outer loop, the restart mechanism and perturbation operation of the standard ILS are used to empower ISA to broadly navigate different search space regions. Extensive evaluation experiments were conducted on thirteen small- and large-sized ALP instances to assess the effectiveness and solution quality of ISA. The obtained results confirm that the proposed ISA method considerably performs better than other state-of-the-art methods in which ISA is capable of reaching new best results in 4 out of 24 large-sized problem instances as well as obtaining the best results in all small-sized instances. Evaluation on large-sized instances confirms a high degree of performance. As a new line of research, ISA is an effective method for ALP which can be further investigated for other combinatorial optimization problems.																	0941-0643	1433-3058				AUG	2020	32	15					11745	11765		10.1007/s00521-019-04659-y													
J								An enhanced reference vectors-based multi-objective evolutionary algorithm with neighborhood-based adaptive adjustment	NEURAL COMPUTING & APPLICATIONS										Evolutionary computation; Multi-objective optimization; Many-objective optimization; Reference vector; Adaptive adjustment	MANY-OBJECTIVE OPTIMIZATION; DECOMPOSITION; DOMINANCE; MOEA/D	The decomposition-based evolutionary algorithms have shown great potential in multi-objective optimization and many-objective optimization. However, their performance strongly depends on the Pareto front shapes. This may result from the fixed reference vectors, which will waste computing resources when handling irregular Pareto fronts. Inspired by this issue, an enhanced reference vectors-based multi-objective evolutionary algorithm with neighborhood-based adaptive adjustment (MOEA-NAA) is proposed. Firstly, a few individuals of the population are used to search the solution space to accelerate the convergence speed until enough non-dominated solutions are found. Then, a multi-criteria environment selection mechanism is implemented to achieve the balance between convergence and diversity, which makes a fusion between dominance-based method and reference vector-based method. Finally, according to the neighborhood information, a small-scale reference vectors adaptive fine-tuning strategy is introduced to enhance the adaptability of different Pareto fronts. To validate the efficiency of MOEA-NAA, experiments are conducted to compare it with four state-of-the-art evolutionary algorithms. The simulation results have shown that the proposed algorithm outperforms the compared algorithms for overall performance.																	0941-0643	1433-3058				AUG	2020	32	15					11767	11789		10.1007/s00521-019-04660-5													
J								Concrete compressive strength using artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Compressive strength; Concrete; Non-destructive testing methods; Soft computing	LEARNING ALGORITHM; BEARING CAPACITY; PREDICTION; OPTIMIZATION; PERCEPTRON	The non-destructive testing of concrete structures with methods such as ultrasonic pulse velocity and Schmidt rebound hammer test is of utmost technical importance. Non-destructive testing methods do not require sampling, and they are simple, fast to perform, and efficient. However, these methods result in large dispersion of the values they estimate, with significant deviation from the actual (experimental) values of compressive strength. In this paper, the application of artificial neural networks (ANNs) for predicting the compressive strength of concrete in existing structures has been investigated. ANNs have been systematically used for predicting the compressive strength of concrete, utilizing both the ultrasonic pulse velocity and the Schmidt rebound hammer experimental results, which are available in the literature. The comparison of the ANN-derived results with the experimental findings, which are in very good agreement, demonstrates the ability of ANNs to estimate the compressive strength of concrete in a reliable and robust manner. Thus, the (quantitative) values of weights for the proposed neural network model are provided, so that the proposed model can be readily implemented in a spreadsheet and accessible to everyone interested in the procedure of simulation.																	0941-0643	1433-3058				AUG	2020	32	15					11807	11826		10.1007/s00521-019-04663-2													
J								Real-time 7-day forecast of pollen counts using a deep convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Pollen forecasting; Deep learning; Convolutional neural network; Allergy	DISPERSION; PATTERNS; POACEAE; CLIMATE; MODELS	Several studies have used regression analyses to forecast pollen concentrations, yet few have applied a deep neural network in their research. This study implements a deep convolutional neural network with the great potential to recognize patterns of pollen phenomena that enable the prediction of pollen concentrations. We train the model using data from 2009 to 2015 from multiple meteorological datasets, satellite data and processed data reflecting pollen flux as input for the model. The model forecasts pollen counts 1-7 days ahead for the entire year of 2016. Comparison of daily forecasts to observations, the algorithm obtains a relatively high index of agreement and Pearson correlation coefficient of up to 0.90 and 0.88, respectively. An evaluation of categorical statistics based on defined threshold levels shows satisfactory results. Critical success index of the model forecasts is as high as 0.887 for weed pollen, 0.646 for tree pollen, and 0.294 for grass pollen. Forecasts of grass pollen exhibit the largest decrease in accuracy because of the strong variance in annual pollen concentrations. Forecasts of weed pollen exhibit the greatest consistency, with a 7-day forecast correlation and index of agreement of 0.82 and 0.77, respectively, during the peak season. This correlates with the consistency of annual and seasonal trends of weed pollen within the study area. Compared to the conventional modeling approaches, convolutional neural network shows a promising ability to predict pollen for multiple days to allow individuals with allergies to take proper precautions during high pollen days.																	0941-0643	1433-3058				AUG	2020	32	15					11827	11836		10.1007/s00521-019-04665-0													
J								A Pareto-optimal evolutionary approach of image encryption using coupled map lattice and DNA	NEURAL COMPUTING & APPLICATIONS										Image encryption; Chaos; CML; DNA; MOGA; Pareto optimization	HYBRID GENETIC ALGORITHM; SEQUENCE OPERATION; CHAOS; SCHEME; OPTIMIZATION	Evolutionary algorithms are generally a suitable approach for optimization problems, having more than one conflicting objectives. For many complicated engineering optimization problems, multi-objective formulations are treated as realistic models. The paper presents and implements a Pareto-optimal image encryption algorithm that uses coupled map lattice (CML) chaos function and deoxyribonucleic acid (DNA) combination to encrypt an image. The discussed work uses multi-objective genetic algorithm (MOGA) to get the optimized results. The proposed two-step algorithm uses pseudo-random number generators, the chaotic method CML and DNA to create an initial population of DNA masks in its initial stage. The MOGA is applied in the second stage to obtain the best mask for encrypting the given plain image. The focus is on the generation of Pareto fronts by using the Pareto generation method of multi-objective optimization. The paper evaluates the performance of the implemented work using standard metrics like key sensitivity, secret key space, number of pixel change rate, unified average changed intensity, entropy, histogram and correlation coefficient. It also discusses the impact of using a genetic algorithm that uses more than one fitness function as the objective for encrypting images.																	0941-0643	1433-3058				AUG	2020	32	15					11859	11873		10.1007/s00521-019-04668-x													
J								Verification of dynamic signature using machine learning approach	NEURAL COMPUTING & APPLICATIONS										Biometric authentication; Signature features; False acceptance rate (FAR); False rejection rate (FRR); Classifiers	TREE	This paper presents a novel approach for dynamic signature authentication based on the machine learning approach. In the proposed method, average values of features are taken into consideration for the verification. Here, seven different types (xandycoordinates, time stamp, pen ups and downs, azimuth, altitude and pressure) of features are used. The obtained extracted feature is learned into different classifiers. Different classifiers have been taken into consideration like random tree, Naive Bayes, random forest, J48, etc. These features are extracted from well-known SVC2004 dataset.																	0941-0643	1433-3058				AUG	2020	32	15					11875	11895		10.1007/s00521-019-04669-w													
J								Multi-population adaptive genetic algorithm for selection of microarray biomarkers	NEURAL COMPUTING & APPLICATIONS										Biomarker; Gene selection; Classification; Genetic algorithm; Support vector machine	PARTICLE SWARM OPTIMIZATION; CANCER CLASSIFICATION; ENSEMBLE; FILTER; KNOWLEDGE; NETWORK; SEARCH	Due to the fast development of DNA microarray technology, researchers have measured large-scale gene expression data in a single trial. However, the classification of microarray data is a challenging task for cancer detection and prevention since gene expression datasets are often exceeding tens of thousands of genes with a small number of tissues. In order to determine a robust gene signature from microarray data, many researchers have explored several gene selection methods for the prediction of cancer recurrence. However, there is no agreement on which gene selection technique produces optimal subsets of genes and avoids over-fitting and curse of dimensionality issues. This inspires us to design a new technique for gene selection, called hybrid multi-population adaptive genetic algorithm that can overlook the irrelevant genes and classify cancer accurately. The proposed hybrid algorithm comprises two phases. In the first phase, an ensemble gene selection method is used to filter the noisy and redundant genes in high-dimensional datasets by combining multi-layer and F-score approaches. Then, a wrapper is designed by multi-population adaptive genetic algorithm with support vector machine and naive Bayes classifiers as an objective function to identify the high-risk differential genes. The performance of the proposed approach is evaluated on ten microarray datasets of numerous tumor types. Furthermore, the comparative experiments demonstrate that proposed method outperforms the several state-of-the-art wrapper and filter methods in terms of classification accuracy with an optimal number of genes.																	0941-0643	1433-3058				AUG	2020	32	15					11897	11918		10.1007/s00521-019-04671-2													
J								An intelligent data-driven model for Dean-Stark water saturation prediction in carbonate rocks	NEURAL COMPUTING & APPLICATIONS										Water saturation; Artificial intelligence; Functional network; Mathematical model	RESERVOIR; PARAMETERS; LOG	Carbonate rocks have a very complex pore system due to the presence of interparticle and intraparticle porosities. This makes the acquisition and analysis of the petrophysical data and the characterization of carbonate rocks a big challenge. In this study, a functional network (FN) tool is used to develop a model to predict water saturation using petrophysical well logs as input parameters and the Dean-Stark measured water saturation as an output parameter. The dataset is comprised of 150 well logs points with the available core data. The developed FN model was optimized by using several optimization algorithms such as differential evolution, particle swarm optimization (PSO), and covariance matrix adaptation evolution strategy. FN model optimized with PSO was found to be the most robust artificial intelligence tool to predict water saturation in carbonate rocks. The results showed that the proposed model can predict the water saturation with an accuracy of 97%. In addition to the development of optimized model, an explicit FN-based empirical correlation is also presented for a quick use. To validate the proposed correlation, three most commonly used water saturation models, namely Simandoux, Bardon and Pied, and Fertl and Hammack, were tested on the blind dataset. The results showed that FN model predicted the water saturation with an error of less than 5%, while the other saturation models predicted water saturation with an error up to 50%. This work clearly shows that machine learning techniques can determine water saturation with high accuracy.																	0941-0643	1433-3058				AUG	2020	32	15					11919	11935		10.1007/s00521-019-04674-z													
J								A survey of fracture detection techniques in bone X-ray images	ARTIFICIAL INTELLIGENCE REVIEW										X-rays; Radiology; Fractures; Deep learning; Convolutional networks	CONVOLUTIONAL NEURAL-NETWORKS; RADIOLOGY; ENSEMBLE; CLASSIFICATION; RELIABILITY; FEATURES; MODEL	Radiologists interprets X-ray samples by visually inspecting them to diagnose the presence of fractures in various bones. Interpretation of radiographs is a time-consuming and intense process involving manual examination of fractures. In addition, clinician's shortage in medically under-resourced areas, unavailability of expert radiologists in busy clinical settings or fatigue caused due to demanding workloads could lead to false detection rate and poor recovery of the fractures. A comprehensive study is imparted here covering fracture diagnosis with the aim to assist investigators in developing models that automatically detects fracture in human bones. The paper is presented in five folds. Firstly, we discuss data preparation stage. Second, we present various image-processing techniques used for fracture detection. Third, we analyze conventional and deep learning based techniques for diagnosing bone fractures. Fourth, we make comparative analysis of existing techniques. Fifth, we discuss different issues and challenges faced by researches while dealing with fracture detection.																	0269-2821	1573-7462				AUG	2020	53	6					4475	4517		10.1007/s10462-019-09799-0													
J								An implicit opinion analysis model based on feature-based implicit opinion patterns	ARTIFICIAL INTELLIGENCE REVIEW										Natural language processing; Opinion mining; Product feature; Sentiment analysis	PRODUCT FEATURE-EXTRACTION; SENTIMENT ANALYSIS; RECOMMENDATIONS; CONSUMERS	With the rapid growth of social networks, mining customer opinions based on online reviews is crucial to understand consumer needs. Due to the richness of language expressions, customer opinions are often expressed implicitly. However, previous studies usually focus on mining explicit opinions to understand consumer needs. In this paper, we propose a novel implicit opinion analysis model to perform implicit opinion analysis of Chinese customer reviews at both the feature and review levels. First, we extract an implicit-opinionated review/clause dataset from raw review dataset and introduce the concept of the feature-based implicit opinion pattern (FBIOP). Secondly, we develop a clustering algorithm to construct product feature categories. Based on the constructed feature categories, FBIOPs can be mined from the extracted implicit-opinionated clause dataset. Thirdly, the sentiment intensity and polarity of each FBIOP are calculated by using the Chi squared test and pointwise mutual information. Fourthly, according to the resulting FBIOP polarities, the polarities of implicit opinions can be determined at both the feature and review levels. Car forum reviews written in Chinese are collected and labeled as the experimental dataset. The results show that the proposed model outperforms the traditional support vector machine model and the cutting-edge convolutional neural network model.																	0269-2821	1573-7462				AUG	2020	53	6					4547	4574		10.1007/s10462-019-09801-9													
J								A ground truth contest between modularity maximization and modularity density maximization	ARTIFICIAL INTELLIGENCE REVIEW										Network clustering; Modularity maximization; Modularity density maximization; Algorithms; Heuristics; Ground truth analyses	COMMUNITY DETECTION; NETWORKS	Computational techniques for network clustering identification are critical to several application domains. Recently, Modularity Maximization and Modularity Density Maximization have become two of the main techniques that provide computational methods to identify network clusterings. Therefore, understanding their differences and common characteristics is fundamental to decide which one is best suited for a given application. Several heuristics and exact methods have been developed for both Modularity Maximization and Modularity Density Maximization problems. Unfortunately, no structured methodological comparison between the two techniques has been proposed yet. This paper reports a ground truth contest between both optimization problems. We do so aiming to compare their exact solutions and the results of heuristics inspired in these problems. In our analysis, we use branch-and-price exact methods which apply the best-known column generation procedures. The heuristic methods obtain the highest objective function scores and find solutions for networks with hundreds of thousands of nodes. Our experiments suggest that Modularity Density Maximization yields the best results over the tested networks. The experiments also show the behavior and importance of the quantitative factor of the Modularity Density Maximization objective function.																	0269-2821	1573-7462				AUG	2020	53	6					4575	4599		10.1007/s10462-019-09802-8													
J								Automatic Tuning of Rule-Based Evolutionary Machine Learning via Problem Structure Identification	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											CLASSIFIER; ADAPTATION; PREDICTION; MUTATION; ZCS	The success of any machine learning technique depends on the correct setting of its parameters and, when it comes to large-scale datasets, hand-tuning these parameters becomes impractical. However, very large-datasets can be pre-processed in order to distil information that could help in appropriately setting various systems parameters. In turn, this makes sophisticated machine learning methods easier to use to end-users. Thus, by modelling the performance of machine learning algorithms as a function of the structure inherent in very large datasets one could, in principle, detect "hotspots" in the parameters' space and thus, auto-tune machine learning algorithms for better dataset-specific performance. In this work we present a parameter setting mechanism for a rule-based evolutionary machine learning system that is capable of finding the adequate parameter value for a wide variety of synthetic classification problems with binary attributes and with/without added noise. Moreover, in the final validation stage our automated mechanism is able to reduce the computational time of preliminary experiments up to 71% for a challenging real-world bioinformatics dataset.																	1556-603X	1556-6048				AUG.	2020	15	3					28	46		10.1109/MCI.2020.2998232													
J								Improving Depression Level Estimation by Concurrently Learning Emotion Intensity	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											PHQ-9	Depression is considered a serious medical condition and a large number of people around the world are suffering from it. Within this context, a lot of studies have been proposed to estimate the degree of depression based on different features and modalities, specific to depression. Supported by medical studies that show how depression is a disorder of impaired emotion regulation, we propose a different approach, which relies on the rationale that the estimation of depression level can benefit from the concurrent learning of emotion intensity. To test this hypothesis, we design different attention-based multi-task architectures that concurrently regress/classify both depression level and emotion intensity using text data. Experiments based on two benchmark datasets, namely, the Distress Analysis Interview Corpus - a Wizard of Oz (DAIC-WOZ), and the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) show that substantial performance improvements can be achieved when compared to emotion-unaware single-task and multitask approaches.																	1556-603X	1556-6048				AUG.	2020	15	3					47	59		10.1109/MCI.2020.2998234													
J								Strength Adjustment and Assessment for MCTS-Based Programs [Research Frontier]	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											GAME; GO	This paper proposes an approach to strength adjustment and assessment for Monte-Carlo tree search based game-playing programs. We modify an existing softmax policy with a strength index to choose moves. The most important modification is a mechanism which filters low-quality moves by excluding those that have a lower simulation count than a pre-defined threshold ratio of the maximum simulation count. Through theoretical analysis, we show that the adjusted policy is guaranteed to choose moves exceeding a lower bound in strength by using a threshold ratio. Experimental results show that the strength index is highly correlated to the empirical strength. With an index value between ?2, we can cover a strength range of about 800 Elo ratings. The strength adjustment and assessment methods were also tested in real-world scenarios with human players, ranging from professionals (strongest) to kyu rank amateurs (weakest). For amateur levels, we tested our mechanism on two popular Go online platforms - Fox Weiqi and Tygem. The result shows that our method can adjust program strength to different ranks stably. In terms of strength assessment, we proposed a new dynamic strength adjustment method, then used it to evaluate human professionals, predicting reliably their playing strengths within 15 games. Lastly, we collected survey responses asking players about strength perception, entertainment, and general comments for different aspects of analysis. To our best knowledge, this result is state-ofthe- art in terms of the range of strengths in Elo rating while maintaining a controllable relationship between the strength and a strength index.																	1556-603X	1556-6048				AUG.	2020	15	3					60	73		10.1109/MCI.2020.2998315													
J								Adaptive Fuzzy Control With High-Order Barrier Lyapunov Functions for High-Order Uncertain Nonlinear Systems With Full-State Constraints	IEEE TRANSACTIONS ON CYBERNETICS										Nonlinear systems; Adaptive systems; Lyapunov methods; Fuzzy control; Sun; Backstepping; Adding a power integrator; barrier Lyapunov function (BLF); full-state constraints; high-order nonlinear system; practical output tracking	ITERATIVE LEARNING CONTROL; OUTPUT-FEEDBACK CONTROL; GLOBAL PRACTICAL TRACKING; STABILIZATION; INPUT; DESIGN	This paper focuses on the practical output tracking control for a category of high-order uncertain nonlinear systems with full-state constraints. A high-order tan-type barrier Lyapunov function (BLF) is constructed to handle the full-state constraints of the control systems. By the BLF and combining a backstepping design technique, an adding a power integrator, and a fuzzy control, the proposed approach can control high-order uncertain nonlinear system with full-state constraints. A novel controller is designed to ensure that the tracking errors approach to an arbitrarily small neighborhood of zero, and the constraints on system states are not violated. The numerical example demonstrates effectiveness of the proposed control method.																	2168-2267	2168-2275				AUG.	2020	50	8					3424	3432		10.1109/TCYB.2018.2890256													
J								Adaptive Reinforcement Learning Neural Network Control for Uncertain Nonlinear System With Input Saturation	IEEE TRANSACTIONS ON CYBERNETICS										Artificial neural networks; Reinforcement learning; Convergence; Adaptive systems; Optimal control; Discrete-time systems; Discrete-time systems; input saturation; multigradient recursive (MGR); neural networks (NNs); reinforcement learning	OUTPUT-FEEDBACK CONTROL; DISCRETE-TIME-SYSTEMS; NN CONTROL; IDENTIFICATION; DESIGN; MODEL	In this paper, an adaptive neural network (NN) control problem is investigated for discrete-time nonlinear systems with input saturation. Radial-basis-function (RBF) NNs, including critic NNs and action NNs, are employed to approximate the utility functions and system uncertainties, respectively. In the previous works, a gradient descent scheme is applied to update weight vectors, which may lead to local optimal problem. To circumvent this problem, a multigradient recursive (MGR) reinforcement learning scheme is proposed, which utilizes both the current gradient and the past gradients. As a consequence, the MGR scheme not only eliminates the local optimal problem but also guarantees faster convergence rate than the gradient descent scheme. Moreover, the constraint of actuator input saturation is considered. The closed-loop system stability is developed by using the Lyapunov stability theory, and it is proved that all the signals in the closed-loop system are semiglobal uniformly ultimately bounded (SGUUB). Finally, the effectiveness of the proposed approach is further validated via some simulation results.																	2168-2267	2168-2275				AUG.	2020	50	8					3433	3443		10.1109/TCYB.2019.2921057													
J								Interval Multiobjective Optimization With Memetic Algorithms	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Uncertainty; Search problems; Memetics; IP networks; Sun; Linear programming; Evolutionary algorithm (EA); interval; memetic algorithm (MA); multiobjective optimization	EVOLUTIONARY ALGORITHMS; TERMINATION CRITERION; SEARCH; MODEL	One of the most important and widely faced optimization problems in real applications is the interval multiobjective optimization problems (IMOPs). The state-of-the-art evolutionary algorithms (EAs) for IMOPs (IMOEAs) need a great deal of objective function evaluations to find a final Pareto front with good convergence and even distribution. Further, the final Pareto front is of great uncertainty. In this paper, we incorporate several local searches into an existing IMOEA, and propose a memetic algorithm (MA) to tackle IMOPs. At the start, the existing IMOEA is utilized to explore the entire decision space; then, the increment of the hypervolume is employed to develop an activation strategy for every local search procedure; finally, the local search procedure is conducted by constituting its initial population, whose center is an individual with a small uncertainty and a big contribution to the hypervolume, taking the contribution of an individual to the hypervolume as its fitness function, and performing the conventional genetic operators. The proposed MA is empirically evaluated on ten benchmark IMOPs as well as an uncertain solar desalination optimization problem and compared with three state-of-the-art algorithms with no local search procedure. The experimental results demonstrate the applicability and effectiveness of the proposed MA.																	2168-2267	2168-2275				AUG.	2020	50	8					3444	3457		10.1109/TCYB.2019.2908485													
J								Distributed Secure Cooperative Control Under Denial-of-Service Attacks From Multiple Adversaries	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Decentralized control; Eigenvalues and eigenfunctions; Multi-agent systems; Laplace equations; Denial-of-service attack; Communication networks; Asymptotic consensus; distributed denial-of-service (DoS) attack; distributed secure control; event-triggered; sample data	CONSENSUS CONTROL; MULTIAGENT	This paper develops a fully distributed framework to investigate the cooperative behavior of multiagent systems in the presence of distributed denial-of-service (DoS) attacks launched by multiple adversaries. In such an insecure network environment, two kinds of communication schemes, that is, sample-data and event-triggered communication schemes, are discussed. Then, a fully distributed control protocol with strong robustness and high scalability is well designed. This protocol guarantees asymptotic consensus against distributed DoS attacks. In this paper, "fully" emphasizes that the eigenvalue information of the Laplacian matrix is not required in the design of both the control protocol and event conditions. For the event-triggered case, two effective dynamical event-triggered schemes are proposed, which are independent of any global information. Such event-triggered schemes do not exhibit Zeno behavior even in the insecure environment. Finally, a simulation example is provided to verify the effectiveness of theoretical analysis.																	2168-2267	2168-2275				AUG.	2020	50	8					3458	3467		10.1109/TCYB.2019.2896160													
J								Event-Based Secure Consensus of Mutiagent Systems Against DoS Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Denial-of-service attack; Protocols; Power system stability; Stability analysis; Symmetric matrices; Control systems; Multi-agent systems; Event-triggered control (ETC); multiagent systems (MASs); secure consensus	LINEAR MULTIAGENT SYSTEMS; STABILIZING CONTROL; TRIGGERED CONTROL; TRACKING CONTROL; NETWORKS; FEEDBACK	This paper studies the problem of event-triggered secure consensus for multiagent systems subject to periodic energy-limited denial-of-service (DoS) attacks, where DoS attacks usually prevent agent-to-agent data transmission. The DoS attacks are assumed to occur periodically based on the time-sequence way and the period of DoS attacks and the uniform lower bound of the communication areas are predetected by some devices. Based on the above assumptions, an event-based protocol consisting of two different measurements corresponding to leader-followers and follower-follower is presented to schedule communications between agents, which can reduce the update frequency of the controller. Then, the stability of the resultant error system is analyzed to derive sufficient conditions of achieving secure consensus by employing the Lyapunov function and the inductive approach. Besides, positive low bounds on any two consecutive intervals of events generated by individual events are calculated to eliminate "Zeno behavior" under the developed triggering condition and event-triggered protocol. Simulation result is provided to verify the theoretical analysis.																	2168-2267	2168-2275				AUG.	2020	50	8					3468	3476		10.1109/TCYB.2019.2918402													
J								A Subregion Division-Based Evolutionary Algorithm With Effective Mating Selection for Many-Objective Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Sociology; Convergence; Evolutionary computation; Maintenance engineering; Pareto optimization; Convergence enhancement; many-objective optimization; mating selection; reference vector; region division	MULTIOBJECTIVE OPTIMIZATION; DIVERSITY	A variety of evolutionary algorithms have been proposed for many-objective optimization in recent years. However, the difficulties in balancing the convergence and diversity of the population and selecting promising parents for offspring reproduction remain. In this paper, we propose a subregion division-based evolutionary algorithm with an effective mating selection strategy, termed SdEA, for many-objective optimization. In SdEA, a subregion division approach is proposed to divide the objective space into different subregions for balancing the diversity and convergence of the population. Besides, an effective mating selection strategy is proposed to enhance the diversity of the mating pool solutions, aimed at enhancing the selection probability of solutions in the sparse subregions. The proposed SdEA is compared with five state-of-the-art many-objective evolutionary algorithms on 23 test problems from DTLZ, WFG, and MaF test suites. Experimental results on these problems demonstrate that the proposed algorithm is competitive in solving many-objective problems. Furthermore, the proposed mating selection strategy is embedded in several evolutionary algorithms and experimental results demonstrate its effectiveness on improving the performance of the embedded algorithms.																	2168-2267	2168-2275				AUG.	2020	50	8					3477	3490		10.1109/TCYB.2019.2906679													
J								Barrier Lyapunov Function-Based Adaptive Fuzzy FTC for Switched Systems and Its Applications to Resistance-Inductance-Capacitance Circuit System	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Switched systems; Circuit faults; Observers; RLC circuits; Nonlinear systems; Average dwell time; fault-tolerant control (FTC); output constraint; resistance-inductance-capacitance (RLC) circuit system; switched observer	OUTPUT-FEEDBACK CONTROL; FAULT-TOLERANT CONTROL; NONLINEAR-SYSTEMS; NEURAL-CONTROL; CONTROL DESIGN; ACTUATOR; STABILIZATION; PERFORMANCE; BACKLASH	In this article, the adaptive fault-tolerant control (FTC) problem is solved for a switched resistance-inductance-capacitance (RLC) circuit system. Due to the existence of faults which may lead to instability of subsystems, the innovation of this article is that the unstable subsystems are taken into account in the frame of output constraint and unmeasurable states. Obviously, there are not any unstable subsystems in unswitched systems. The unstable subsystems will involve many serious consequences and difficulties. Since the system states are unavailable, a switched state observer is designed. In addition, the fuzzy-logic systems (FLSs) are employed to approximate unknown internal dynamics in the controller design procedure. Then, the barrier Lyapunov function (BLF) is exploited to guarantee that the system output satisfy its constrained interval. Moreover, by using the average dwell-time method, all signals in the resulting systems are proofed to be bounded even when faults occur. Finally, the proposed strategy is carried out on the switched RLC circuit system to show the effectiveness and practicability.																	2168-2267	2168-2275				AUG.	2020	50	8					3491	3502		10.1109/TCYB.2019.2931770													
J								Scaled Consensus of Second-Order Nonlinear Multiagent Systems With Time-Varying Delays via Aperiodically Intermittent Control	IEEE TRANSACTIONS ON CYBERNETICS										Delays; Topology; Delay effects; Protocols; Symmetric matrices; Nonlinear dynamical systems; Linear matrix inequalities; Aperiodically intermittent control; multiagent systems (MASs); scaled consensus; strongly connected topology; time-varying delays	LEADER-FOLLOWING CONSENSUS; SUFFICIENT CONDITIONS; DYNAMICS; AGENTS; NONUNIFORM; NETWORKS	In this paper, the scaled consensus of multiagent systems (MASs) with second-order nonlinear dynamics and time-varying delays is investigated, where agents are supposed to aperiodically communicate with each other under a directed graph at some disconnected time intervals. Different from the existing works, we consider the case where time-varying delays exist in both nonlinear dynamics and communication networks. First, to address this problem, we propose a novel scaled consensus protocol. Second, using Lyapunov theory and graph theory, it is proved that under mild conditions, MASs with second-order nonlinear dynamics exponentially reach scaled consensus. Moreover, we extend our results to the case of leader-following scaled consensus. Finally, the simulation examples are included to verify the effectiveness of the theoretical results.																	2168-2267	2168-2275				AUG.	2020	50	8					3503	3516		10.1109/TCYB.2018.2883793													
J								Dual Shared-Specific Multiview Subspace Clustering	IEEE TRANSACTIONS ON CYBERNETICS										Correlation; Clustering algorithms; Redundancy; Clustering methods; Cybernetics; Benchmark testing; Sparse matrices; Complementary information; dual learning; multiview subspace clustering; view-specific property	CLASSIFICATION; SCALE	Multiview subspace clustering has received significant attention as the availability of diverse of multidomain and multiview real-world data has rapidly increased in the recent years. Boosting the performance of multiview clustering algorithms is challenged by two major factors. First, since original features from multiview data are highly redundant, reconstruction based on these attributes inevitably results in inferior performance. Second, since each view of such multiview data may contain unique knowledge as against the others, it remains a challenge to exploit complimentary information across multiple views while simultaneously investigating the uniqueness of each view. In this paper, we present a novel dual shared-specific multiview subspace clustering (DSS-MSC) approach that simultaneously learns the correlations between shared information across multiple views and also utilizes view-specific information to depict specific property for each independent view. Further, we formulate a dual learning framework to capture shared-specific information into the dimensional reduction and self-representation processes, which strengthens the ability of our approach to exploit shared information while preserving view-specific property effectively. The experimental results on several benchmark datasets have demonstrated the effectiveness of the proposed approach against other state-of-the-art techniques.																	2168-2267	2168-2275				AUG.	2020	50	8					3517	3530		10.1109/TCYB.2019.2918495													
J								Benchmark Problems and Performance Indicators for Search of Knee Points in Multiobjective Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Benchmark testing; Shape; Evolutionary computation; Pareto optimization; Couplings; Scalability; Benchmark problems; knee points; knee regions; multiobjective optimization; performance indicators	MANY-OBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHM; SELECTION; ARTICULATION; DIVERSITY; DESIGN	In multiobjective optimization, it is nontrivial for decision makers to articulate preferences without a priori knowledge, which is particularly true when the number of objectives becomes large. Depending on the shape of the Pareto front, optimal solutions such as knee points may be of interest. Although several multi- and many-objective optimization test suites have been proposed, little work has been reported focusing on designing multiobjective problems whose Pareto front contains complex knee regions. Likewise, few performance indicators dedicated to evaluate an algorithm's ability of accurately locating all knee points in high-dimensional objective space have been suggested. This paper proposes a set of multiobjective optimization test problems whose Pareto front consists of complex knee regions, aiming to assess the capability of evolutionary algorithms to accurately identify all knee points. Various features related to knee points have been taken into account in designing the test problems, including symmetry, differentiability, and degeneration. These features are also combined with other challenges in solving the optimization problems, such as multimodality, linkage between decision variables, nonuniformity, and scalability of the Pareto front. The proposed test problems are scalable to both decision and objective spaces. Accordingly, new performance indicators are suggested for evaluating the capability of optimization algorithms in locating the knee points. The proposed test problems, together with the performance indicators, offer a new means to develop and assess preference-based evolutionary algorithms for solving multi- and many-objective optimization problems.																	2168-2267	2168-2275				AUG.	2020	50	8					3531	3544		10.1109/TCYB.2019.2894664													
J								Observer-Based State Estimation of Discrete-Time Fuzzy Systems Based on a Joint Switching Mechanism for Adjacent Instants	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Observers; Cybernetics; Complexity theory; Fuzzy control; Fuzzy systems; intelligent control; joint switching mechanism; state estimation	NONQUADRATIC STABILIZATION CONDITIONS; NONLINEAR-SYSTEMS; DESIGN; STABILITY; MODELS	The problem of observer-based state estimation of discrete-time fuzzy systems is investigated by constructing a joint switching mechanism for adjacent instants. Thanks to the usage of both the proposed spatial partitioning method and a set of new free matrix-valued variables, abundant information about size differences of all the normalized fuzzy weighting functions for adjacent instants can be interactively integrated into the fuzzy observer design for the first time. Compared with the recent result from three important aspects (conservatism level, online computational burden, and offline computational burden), three positive results can be obtained: 1) it provides a chance for reducing the conservatism by a large margin; 2) the required online computational burden remains unchanged as referred ones; and 3) as a tradeoff, the required offline computational burden increases to some extent but is still affordable from the view of complexity analysis. Finally, two numerical simulations have been given to validate the effectiveness of our developed theoretical results.																	2168-2267	2168-2275				AUG.	2020	50	8					3545	3555		10.1109/TCYB.2019.2917929													
J								Hyperspectral Image Restoration Using Weighted Group Sparsity-Regularized Low-Rank Tensor Decomposition	IEEE TRANSACTIONS ON CYBERNETICS										Image restoration; TV; Matrix decomposition; Correlation; Hyperspectral imaging; Noise measurement; Augmented Lagrange multiplier (ALM) algorithm; group sparsity; hyperspectral image restoration; low-rank tensor decomposition	NOISE-REDUCTION; REPRESENTATION; REGRESSION; QUALITY	Mixed noise (such as Gaussian, impulse, stripe, and deadline noises) contamination is a common phenomenon in hyperspectral imagery (HSI), greatly degrading visual quality and affecting subsequent processing accuracy. By encoding sparse prior to the spatial or spectral difference images, total variation (TV) regularization is an efficient tool for removing the noises. However, the previous TV term cannot maintain the shared group sparsity pattern of the spatial difference images of different spectral bands. To address this issue, this article proposes a group sparsity regularization of the spatial difference images for HSI restoration. Instead of using l(1)- or l(2)-norm (sparsity) on the difference image itself, we introduce a weighted l(2,1)-norm to constrain the spatial difference image cube, efficiently exploring the shared group sparse pattern. Moreover, we employ the well-known low-rank Tucker decomposition to capture the global spatial-spectral correlation from three HSI dimensions. To summarize, a weighted group sparsity-regularized low-rank tensor decomposition (LRTDGS) method is presented for HSI restoration. An efficient augmented Lagrange multiplier algorithm is employed to solve the LRTDGS model. The superiority of this method for HSI restoration is demonstrated by a series of experimental results from both simulated and real data, as compared with the other state-of-the-art TV-regularized low-rank matrix/tensor decomposition methods.																	2168-2267	2168-2275				AUG.	2020	50	8					3556	3570		10.1109/TCYB.2019.2936042													
J								Distributed Secure Filtering for Discrete-Time Systems Under Round-Robin Protocol and Deception Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Security; Symmetric matrices; Discrete-time systems; Cybernetics; Filtering; Schedules; Deception attacks; discrete-time Wirtinger's inequality; Round-Robin protocol; security filter	NETWORKED CONTROL-SYSTEMS; CYBER-PHYSICAL SYSTEMS; INEQUALITY	This paper addresses the problem of designing a distributed filter for linear discrete-time networked control systems that suffer from randomly occurring deception attacks and bounded disturbances. A Round-Robin-type protocol is applied to schedule communication between filters due to the communication resource constraint. The discrete-time Wirtinger's inequality is used to derive a sufficient condition that ensures H-infinity performance of the closed-loop system. A corresponding filter that guarantees the security is designed. Finally, a numerical example of an inverted pendulum system is provided to demonstrate the feasibility of the proposed filter.																	2168-2267	2168-2275				AUG.	2020	50	8					3571	3580		10.1109/TCYB.2019.2897366													
J								Learning-Based Quantum Robust Control: Algorithm, Applications, and Experiments	IEEE TRANSACTIONS ON CYBERNETICS										Nonhomogeneous media; Robust control; Quantum computing; Task analysis; Chemistry; Machine learning algorithms; Uncertainty; Differential evolution; femtosecond laser; quantum control; quantum learning; quantum robust control	DIFFERENTIAL EVOLUTION; LYAPUNOV CONTROL; CONSENSUS; ENSEMBLE; SYSTEMS; DESIGN; COMMUNICATION; DYNAMICS	Robust control design for quantum systems has been recognized as a key task in quantum information technology, molecular chemistry, and atomic physics. In this paper, an improved differential evolution algorithm, referred to as multiple-samples and mixed-strategy DE (msMS_DE), is proposed to search robust fields for various quantum control problems. In msMS_DE, multiple samples are used for fitness evaluation and a mixed strategy is employed for the mutation operation. In particular, the msMS_DE algorithm is applied to the control problems of: 1) open inhomogeneous quantum ensembles and 2) the consensus goal of a quantum network with uncertainties. Numerical results are presented to demonstrate the excellent performance of the improved machine learning algorithm for these two classes of quantum robust control problems. Furthermore, msMS_DE is experimentally implemented on femtosecond (fs) laser control applications to optimize two-photon absorption and control fragmentation of the molecule CH2BrI. The experimental results demonstrate the excellent performance of msMS_DE in searching for effective fs laser pulses for various tasks.																	2168-2267	2168-2275				AUG.	2020	50	8					3581	3593		10.1109/TCYB.2019.2921424													
J								Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks	IEEE TRANSACTIONS ON CYBERNETICS										Training; Computational modeling; Acceleration; Neural networks; Matrix decomposition; Optimization; Training data; Filter pruning; image classification; neural networks		Deeper and wider convolutional neural networks (CNNs) achieve superior performance but bring expensive computation cost. Accelerating such overparameterized neural network has received increased attention. A typical pruning algorithm is a three-stage pipeline, i.e., training, pruning, and retraining. Prevailing approaches fix the pruned filters to zero during retraining and, thus, significantly reduce the optimization space. Besides, they directly prune a large number of filters at first, which would cause unrecoverable information loss. To solve these problems, we propose an asymptotic soft filter pruning (ASFP) method to accelerate the inference procedure of the deep neural networks. First, we update the pruned filters during the retraining stage. As a result, the optimization space of the pruned model would not be reduced but be the same as that of the original model. In this way, the model has enough capacity to learn from the training data. Second, we prune the network asymptotically. We prune few filters at first and asymptotically prune more filters during the training procedure. With asymptotic pruning, the information of the training set would be gradually concentrated in the remaining filters, so the subsequent training and pruning process would be stable. The experiments show the effectiveness of our ASFP on image classification benchmarks. Notably, on ILSVRC-2012, our ASFP reduces more than 40% FLOPs on ResNet-50 with only 0.14% top-5 accuracy degradation, which is higher than the soft filter pruning by 8%.																	2168-2267	2168-2275				AUG.	2020	50	8					3594	3604		10.1109/TCYB.2019.2933477													
J								Distributed State-Saturated Recursive Filtering Over Sensor Networks Under Round-Robin Protocol	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Upper bound; Network topology; Time-varying systems; Covariance matrices; Symmetric matrices; Linear matrix inequalities; Distributed recursive filtering; round-robin (RR) protocol; sensor networks; state saturations; time-varying systems	TIME-DELAY SYSTEMS; RANDOMLY OCCURRING NONLINEARITIES; SUBJECT	This article is concerned with the distributed recursive filtering issue for stochastic discrete time-varying systems subjected to both state saturations and round-robin (RR) protocols over sensor networks. The phenomenon of state saturation is considered to better describe practical engineering. The RR protocol is introduced to mitigate a network burden by determining which component of the sensor node has access to the network at each transmission instant. The purpose of the issue under consideration is to construct a distributed recursive filter such that a certain filtering error covariance's upper bound can be found and the corresponding filter parameters' explicit expression is given with both state saturations and RR protocols. By taking advantage of matrix difference equations, a filtering error covariance's upper bound can be presented and then be minimized by appropriately designing filter parameters. In particular, by using a matrix simplification technique, the sensor network topology's sparseness issue can be tackled. Finally, the feasibility for the addressed filtering scheme is demonstrated by an example.																	2168-2267	2168-2275				AUG.	2020	50	8					3605	3615		10.1109/TCYB.2019.2932460													
J								Resilient Control Design Based on a Sampled-Data Model for a Class of Networked Control Systems Under Denial-of-Service Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Denial-of-service attack; Communication networks; Process control; Closed loop systems; Stability criteria; Transmitters; Bessel-Legendre inequality; denial-of-service (DoS) attacks; looped functional; networked control systems (NCSs); resilient control	PREDICTIVE CONTROL; STABILITY ANALYSIS; POWER-SYSTEMS; STABILIZATION; SUBJECT	This article is concerned with designing resilient state feedback controllers for a class of networked control systems under denial-of-service (DoS) attacks. The sensor samples system states periodically. The DoS attacks usually prevent those sampled signals from being transmitted through a communication network. A logic processor embedded in the controller is introduced to not only receive sampled signals but also capture information on the duration time of each DoS attack. Note that the duration time of DoS attacks is usually both lower and upper bounded. Then the closed-loop system is modeled as an aperiodic sampled-data system closely related to both lower and upper bounds of duration time of DoS attacks. By introducing a novel looped functional, which caters for the N-order canonical Bessel-Legendre inequalities, some N-dependent stability criteria are presented for the resultant closed-loop system. It is worth pointing out that a number of identity formulas are uncovered, which enable us to apply the notable free-weighting matrix approach to derive less conservative stability criteria. A linear-matrix-inequality-based criterion is provided to design stabilizing state-feedback controllers against DoS attacks. A satellite control system is given to demonstrate the effectiveness of the proposed method.																	2168-2267	2168-2275				AUG.	2020	50	8					3616	3626		10.1109/TCYB.2019.2956137													
J								Going From RGB to RGBD Saliency: A Depth-Guided Transformation Model	IEEE TRANSACTIONS ON CYBERNETICS										Saliency detection; Image color analysis; Optimization; Shape; Feature extraction; Task analysis; Object detection; Depth cue; energy function optimization; refined depth shape prior (RDSP); RGBD images; saliency detection; transformation model	OBJECT DETECTION; OPTIMIZATION; RANKING; FUSION	Depth information has been demonstrated to be useful for saliency detection. However, the existing methods for RGBD saliency detection mainly focus on designing straightforward and comprehensive models, while ignoring the transferable ability of the existing RGB saliency detection models. In this article, we propose a novel depth-guided transformation model (DTM) going from RGB saliency to RGBD saliency. The proposed model includes three components, that is: 1) multilevel RGBD saliency initialization; 2) depth-guided saliency refinement; and 3) saliency optimization with depth constraints. The explicit depth feature is first utilized in the multilevel RGBD saliency model to initialize the RGBD saliency by combining the global compactness saliency cue and local geodesic saliency cue. The depth-guided saliency refinement is used to further highlight the salient objects and suppress the background regions by introducing the prior depth domain knowledge and prior refined depth shape. Benefiting from the consistency of the entire object in the depth map, we formulate an optimization model to attain more consistent and accurate saliency results via an energy function, which integrates the unary data term, color smooth term, and depth consistency term. Experiments on three public RGBD saliency detection benchmarks demonstrate the effectiveness and performance improvement of the proposed DTM from RGB to RGBD saliency.																	2168-2267	2168-2275				AUG.	2020	50	8					3627	3639		10.1109/TCYB.2019.2932005													
J								Multiview Hybrid Embedding: A Divide-and-Conquer Approach	IEEE TRANSACTIONS ON CYBERNETICS										Kernel; Interviews; Manifolds; Benchmark testing; Geometry; Optimization; Cybernetics; Cross-view classification; Divide-and-Conquer; multiview hybrid embedding (MvHE); multiview learning	DIMENSIONALITY REDUCTION; REGRESSION	We present a novel cross-view classification algorithm where the gallery and probe data come from different views. A popular approach to tackle this problem is the multiview subspace learning (MvSL) that aims to learn a latent subspace shared by multiview data. Despite promising results obtained on some applications, the performance of existing methods deteriorates dramatically when the multiview data is sampled from nonlinear manifolds or suffers from heavy outliers. To circumvent this drawback, motivated by the Divide-and-Conquer strategy, we propose multiview hybrid embedding (MvHE), a unique method of dividing the problem of cross-view classification into three subproblems and building one model for each subproblem. Specifically, the first model is designed to remove view discrepancy, whereas the second and third models attempt to discover the intrinsic nonlinear structure and to increase the discriminability in intraview and interview samples, respectively. The kernel extension is conducted to further boost the representation power of MvHE. Extensive experiments are conducted on four benchmark datasets. Our methods demonstrate the overwhelming advantages against the state-of-the-art MvSL-based cross-view classification approaches in terms of classification accuracy and robustness.																	2168-2267	2168-2275				AUG.	2020	50	8					3640	3653		10.1109/TCYB.2019.2894591													
J								Error Correction Regression Framework for Enhancing the Decoding Accuracies of Ear-EEG Brain-Computer Interfaces	IEEE TRANSACTIONS ON CYBERNETICS										Electroencephalography; Decoding; Visualization; Electrodes; Ear; Estimation; Brain modeling; Brain-computer interface (BCI); ear-electroencephalography (EEG); nonlinear regression; steady-state visual evoked potential (SSVEP)	EVOKED-POTENTIALS; MOTOR IMAGERY; CLASSIFICATION; SIGNALS; BCI; MEG	Ear-electroencephalography (EEG) is a promising tool for practical brain-computer interface (BCI) applications because it is more unobtrusive, comfortable, and mobile than a typical scalp-EEG system. However, an ear-EEG has a natural constraint of electrode location (e.g., limited in or around the ear) for acquiring informative brain signals sufficiently. Achieving reliable performance of ear-EEG in specific BCI paradigms that do not utilize brain signals on the temporal lobe around the ear is difficult. For example, steady-state visual evoked potentials (SSVEPs), which are mainly generated in the occipital area, have a significantly attenuated and distorted amplitude in ear-EEG. Therefore, preserving the high level of decoding accuracy is challenging and essential for SSVEP BCI based on ear-EEG. In this paper, we first investigate linear and nonlinear regression methods to increase the decoding accuracy of ear-EEG regarding SSVEP paradigm by utilizing the estimated target EEG signals on the occipital area. Then, we investigate an ensemble method to consider the prediction variability of the regression methods. Finally, we propose an error correction regression (ECR) framework to reduce the prediction errors by adding an additional nonlinear regression process (i.e., kernel ridge regression). We evaluate the ECR framework in terms of single session, session-to-session transfer, and subject-transfer decoding. We also validate the online decoding ability of the proposed framework with a short-time window size. The average accuracies are observed to be 91.11 +/- 9.14%, 90.52 +/- 8.67%, 86.96 +/- 12.13%, and 78.79 +/- 12.59%. This paper demonstrates that SSVEP BCI based on ear-EEG can achieve reliable performance with the proposed ECR framework.																	2168-2267	2168-2275				AUG.	2020	50	8					3654	3667		10.1109/TCYB.2019.2924237													
J								A Survey of Optimization Methods From a Machine Learning Perspective	IEEE TRANSACTIONS ON CYBERNETICS										Machine learning; Optimization methods; Stochastic processes; Machine learning algorithms; Linear programming; Task analysis; Approximate Bayesian inference; deep neural network (DNN); machine learning; optimization method; reinforcement learning (RL)	QUASI-NEWTON METHODS; DERIVATIVE-FREE OPTIMIZATION; ALGORITHM; NETWORK; CONVEX; SYSTEM	Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.																	2168-2267	2168-2275				AUG.	2020	50	8					3668	3681		10.1109/TCYB.2019.2950779													
J								Submanifold-Preserving Discriminant Analysis With an Auto-Optimized Graph	IEEE TRANSACTIONS ON CYBERNETICS										Manifolds; Data models; Principal component analysis; Robustness; Optimization; Clustering algorithms; Germanium; Auto-optimized graph; submanifold preserved learning; supervised and semisupervised dimensionality reduction (DR)	FACE RECOGNITION; REGULARIZATION; FRAMEWORK	Due to the multimodality of non-Gaussian data, traditional globality-preserved dimensionality reduction (DR) methods, such as linear discriminant analysis (LDA) and principal component analysis (PCA) are difficult to deal with. In this paper, we present a novel local DR framework via auto-optimized graph embedding to extract the intrinsic submanifold structure of multimodal data. Specifically, the proposed model seeks to learn an embedding space which can preserve the local neighborhood structure by constructing a k-nearest neighbors (kNNs) graph on data points. Different than previous works, our model employs the l(0)-norm constraint and binary constraint on the similarity matrix to impose that there only be a k nonzero value in each row of the similarity matrix, which can ensure the k-connectivity in graph. More important, as the high-dimensional data probably contains some noises and redundant features, calculating the similarity matrix in the original space by using a kernel function is inaccurate. As a result, a mechanism of an auto-optimized graph is derived in the proposed model. Concretely, we learn the embedding space and similarity matrix simultaneously. In other words, the selection of neighbors is automatically executed in the optimal subspace rather than in the original space when the algorithm reaches convergence, which can alleviate the affect of noises and improve the robustness of the proposed model. In addition, four supervised and semisupervised local DR methods are derived by the proposed framework which can extract the discriminative features while preserving the submanifold structure of data. Last but not least, since two variables need to be optimized simultaneously in the proposed methods, and the constraints on the similarity matrix are difficult to satisfy, which is an NP-hard problem. Consequently, an efficient iterative optimization algorithm is introduced to solve the proposed problems. Extensive experiments conducted on synthetic data and several real-world datasets have demonstrated the advantages of the proposed methods in robustness and recognition accuracy.																	2168-2267	2168-2275				AUG.	2020	50	8					3682	3695		10.1109/TCYB.2019.2910751													
J								Efficient Large-Scale Multiobjective Optimization Based on a Competitive Swarm Optimizer	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Clustering algorithms; Particle swarm optimization; Computer science; Sociology; Statistics; Trajectory; Competitive swarm optimizer (CSO); evolutionary multiobjective optimization; large-scale multiobjective optimization problem; particle swarm optimization (PSO)	EVOLUTIONARY ALGORITHM; MECHANISM	There exist many multiobjective optimization problems (MOPs) containing a large number of decision variables in real-world applications, which are known as large-scale MOPs. Due to the ineffectiveness of existing operators in finding optimal solutions in a huge decision space, some decision variable division-based algorithms have been tailored for improving the search efficiency in solving large-scale MOPs. However, these algorithms will encounter difficulties when solving problems with complicated landscapes, as the decision variable division is likely to be inaccurate and time consuming. In this paper, we propose a competitive swarm optimizer (CSO)-based efficient search for solving large-scale MOPs. The proposed algorithm adopts a new particle updating strategy that suggests a two-stage strategy to update position, which can highly improve the search efficiency. The experimental results on large-scale benchmark MOPs and an application example demonstrate the superiority of the proposed algorithm over several state-of-the-art multiobjective evolutionary algorithms, including problem transformation-based algorithm, decision variable clustering-based algorithm, particle swarm optimization algorithm, and estimation of distribution algorithm.																	2168-2267	2168-2275				AUG.	2020	50	8					3696	3708		10.1109/TCYB.2019.2906383													
J								Control Synthesis of Hidden Semi-Markov Uncertain Fuzzy Systems via Observations of Hidden Modes	IEEE TRANSACTIONS ON CYBERNETICS										sigma-error mean square stability; hidden semi-Markov chain; stochastic uncertainties; Takagi-Sugeno (T-S) fuzzy system	NONLINEAR-SYSTEMS; STABILIZATION; STABILITY	This paper investigates the stability analysis and fuzzy control problems for a class of discrete-time fuzzy systems with hidden semi-Markov stochastic uncertainties. The nonlinear plant is described via the Takagi-Sugeno (T-S) fuzzy model, and the parameter uncertainties are represented by a hidden semi-Markov chain. Owing to the semi-Markov kernel (SMK), the probability density functions (PDFs) of sojourn time for different modes in describing the stochastic uncertainties can address different types of distributions according to different target modes. A novel Lyapunov function that depends on the hidden mode and the observed mode with the elapsed time is proposed to analyze the stability and the H-infinity performance of the fuzzy system. Then, the sufficient criteria for the elapsed-time-dependent and observed-mode-dependent fuzzy controller are achieved by exploiting the observations of hidden modes, ensuring that the closed-loop system is sigma-error mean square stable with guaranteed H-infinity performance. A cart-pendulum system is used to demonstrate the effectiveness and applicability of the proposed theoretical results.																	2168-2267	2168-2275				AUG.	2020	50	8					3709	3718		10.1109/TCYB.2019.2921811													
J								Neural-Network-Based Consensus Control for Multiagent Systems With Input Constraints: The Event-Triggered Case	IEEE TRANSACTIONS ON CYBERNETICS										Artificial neural networks; Optimal control; Cost function; Multi-agent systems; Observers; Tuning; Topology; Consensus control; event-triggered protocols; input constraints; multiagent systems (MASs); neural networks (NNs)	DISCRETE-TIME-SYSTEMS; COOPERATIVE OPTIMAL-CONTROL; TOPOLOGY; SUBJECT	In this paper, the neural-network (NN)-based consensus control problem is investigated for a class of discrete-time nonlinear multiagent systems (MASs) with a leader subject to input constraints. Relative measurements related to local tracking errors are collected via some smart sensors. A local nonquadratic cost function is first introduced to evaluate the control performance with input constraints. Then, in view of the relative measurements, an NN-based observer under the event-triggered mechanism is designed to reconstruct the dynamics of the local tracking errors, where the adopted event-triggered condition has a time-dependent threshold and the weight of NNs is updated via a new adaptive tuning law catering to the employed event-triggered mechanism. Furthermore, an ideal control policy is developed for the addressed consensus control problem while minimizing the prescribed local nonquadratic cost function. Moreover, an actor-critic NN scheme with online learning is employed to realize the obtained control policy, where the critic NN is a three-layer structure with powerful approximation capability. Through extensive mathematical analysis, the consensus condition is established for the underlying MAS, and the boundedness of the estimated errors is proven for actor and critic NN weights. In addition, the effect from the adopted event-triggered mechanism on the local cost is thoroughly discussed, and the upper bound of the corresponding increment is derived in comparison with time-triggered cases. Finally, a simulation example is utilized to illustrate the usefulness of the proposed controller design scheme.																	2168-2267	2168-2275				AUG.	2020	50	8					3719	3730		10.1109/TCYB.2019.2927471													
J								Asynchronous Partially Mode-Dependent Filtering of Network-Based MSRSNSs With Quantized Measurement	IEEE TRANSACTIONS ON CYBERNETICS										Asynchronous filtering; l(2)-l(infinity) filter; Markovian switching systems; quantization effect	MARKOV JUMP SYSTEMS; NONLINEAR-SYSTEMS; FEEDBACK-CONTROL; NEURAL-NETWORKS; STABILITY; SUBJECT; DESIGN; FAULTS; DELAYS; RESET	This article addresses the issue of asynchronous partially mode-dependent filtering for networked Markov switching repeated scalar nonlinear systems (MSRSNSs) subject to quantized measurements (QMs). Especially, a novel partially mode-dependent filter (PMDF) is constructed, where the signal transmission of a filter mode occurred randomly and is modeled by a Bernoulli distributed sequence. The designed PMDF is different from state mode, which is governed by an asynchronous switching rule. By utilizing a diagonally dominant-type Lyapunov functional (DDTLF), sufficient conditions ensure that the existence of the PMDF and the l(2)-l(infinity) performance index are derived. Finally, an economic example is adopted to substantiate the applicability of the developed theoretical results.																	2168-2267	2168-2275				AUG.	2020	50	8					3731	3739		10.1109/TCYB.2019.2939830													
J								Reference Trajectory Reshaping Optimization and Control of Robotic Exoskeletons for Human-Robot Co-Manipulation	IEEE TRANSACTIONS ON CYBERNETICS										Robots; Trajectory; Exoskeletons; Task analysis; Impedance; Optimization; Damping; Optimized trajectory; physical human-robot interaction (pHRI); reference trajectory reshaping; robotic exoskeleton	ADAPTIVE NEURAL-CONTROL; NONLINEAR-SYSTEMS; IMPEDANCE CONTROL; TRACKING CONTROL; ADAPTATION; MODEL; STATE; LIMB; COORDINATION; MOVEMENT	For human-robot co-manipulation by robotic exoskeletons, the interaction forces provide a communication channel through which the human and the robot can coordinate their actions. In this article, an optimization approach for reshaping the physical interactive trajectory is presented in the co-manipulation tasks, which combines impedance control to enable the human to adjust both the desired and the actual trajectories of the robot. Different from previous studies, the proposed method significantly reshapes the desired trajectory during physical human-robot interaction (pHRI) based on force feedback, without requiring constant human guidance. The proposed scheme first formulates a quadratically constrained programming problem, which is then solved by neural dynamics optimization to obtain a smooth and minimal-energy trajectory similar to the natural human movement. Then, we propose an adaptive neural-network controller based on the barrier Lyapunov function (BLF), which enables the robot to handle the uncertain dynamics and the joint space constraints directly. To validate the proposed method, we perform experiments on the exoskeleton robot with human operators for co-manipulation tasks. The experimental results demonstrate that the proposed controller could complete the co-manipulation tasks effectively.																	2168-2267	2168-2275				AUG.	2020	50	8					3740	3751		10.1109/TCYB.2019.2933019													
J								Dynamic Intermittent Feedback Design for H-infinity Containment Control on a Directed Graph	IEEE TRANSACTIONS ON CYBERNETICS										Game algebraic Riccati equation (GARE); H-infinity intermittent containment control; multiagent systems (MASs)	LINEAR MULTIAGENT SYSTEMS; EVENT-TRIGGERED CONTROL; COOPERATIVE CONTROL; CONSENSUS; LEADER	This article develops a novel distributed intermittent control framework with the ultimate goal of reducing the communication burden in containment control of multiagent systems communicating via a directed graph. Agents are assumed to be under disturbance and communicate on a directed graph. Both static and dynamic intermittent protocols are proposed. Intermittent H-infinity containment control design is considered to attenuate the effect of the disturbance and the game algebraic Riccati equation (GARE) is employed to design the coupling and feedback gains for both static and dynamic intermittent feedback. A novel scheme is then used to unify continuous, static, and dynamic intermittent containment protocols. Finally, simulation results verify the efficacy of the proposed approach.																	2168-2267	2168-2275				AUG.	2020	50	8					3752	3765		10.1109/TCYB.2019.2933736													
J								Observer-Based Fuzzy Output-Feedback Control for Discrete-Time Strict-Feedback Nonlinear Systems With Stochastic Noises	IEEE TRANSACTIONS ON CYBERNETICS										Noise measurement; Fuzzy logic; Nonlinear systems; Backstepping; Stochastic processes; Observers; Stability criteria; Adaptive fuzzy control; discrete-time system; measurement noise; observer-based output-feedback control (OBOFC); state-dependent multiplicative noise; stochastic nonlinear systems	TRACKING CONTROL; ADAPTIVE-CONTROL; NEURAL-NETWORK; NN CONTROL; DESIGN; APPROXIMATION	This paper focuses on the observer-based output-feedback control (OBOFC) problem for a class of discrete-time strict-feedback nonlinear systems (DTSFNSs) with both multiplicative process noises and additive measurement noises. A state observer is first designed to estimate immeasurable system states, and then a novel observer-based backstepping control framework is proposed for DTSFNSs with known model information. To be specific, virtual control laws and the actual control law are derived using a variable substitution method that gets rid of the repeated accumulation of measurement noises in the recursive process. Furthermore, for technical derivation, the multiplicative noise is successively bounded by state estimation errors and controlled errors. Stability conditions are obtained to guarantee the exponential mean-square boundedness of the closed-loop system. Moreover, the nonlinear modeling uncertainties are taken into account to better reflect engineering practices. In virtue of the universal approximation property of fuzzy-logic systems, a fuzzy observer and the corresponding fuzzy output-feedback controller are simultaneously constructed to derive the stability criteria by using novel weight updated laws. Simulation studies are performed to test the validity of the proposed OBOFC scheme.																	2168-2267	2168-2275				AUG.	2020	50	8					3766	3777		10.1109/TCYB.2019.2902520													
J								Type-2 Fuzzy Hybrid Controller Network for Robotic Systems	IEEE TRANSACTIONS ON CYBERNETICS										Biological neural networks; Mobile robots; Fuzzy sets; Control systems; Mathematical model; Robot sensing systems; Adaptive control; brain emotional learning controller (BELC) network; robot dynamic control; Type-2 inference system	CMAC NEURAL-NETWORKS; SLIDING-MODE CONTROL; INTERVAL TYPE-2; LOGIC SYSTEMS; TRACKING CONTROL; MOBILE ROBOT; DESIGN; REDUCTION; SETS; ALGORITHMS	Dynamic control, including robotic control, faces both the theoretical challenge of obtaining accurate system models and the practical difficulty of defining uncertain system bounds. To facilitate such challenges, this paper proposes a control system consisting of a novel type of fuzzy neural network and a robust compensator controller. The new fuzzy neural network is implemented by integrating a number of key components embedded in a Type-2 fuzzy cerebellar model articulation controller (CMAC) and a brain emotional learning controller (BELC) network, thereby mimicking an ideal sliding mode controller. The system inputs are fed into the neural network through a Type-2 fuzzy inference system (T2FIS), with the results subsequently piped into sensory and emotional channels which jointly produce the final outputs of the network. That is, the proposed network estimates the nonlinear equations representing the ideal sliding mode controllers using a powerful compensator controller with the support of T2FIS and BELC, guaranteeing robust tracking of the dynamics of the controlled systems. The adaptive dynamic tuning laws of the network are developed by exploiting the popular brain emotional learning rule and the Lyapunov function. The proposed system was applied to a robot manipulator and a mobile robot, demonstrating its efficacy and potential; and a comparative study with alternatives indicates a significant improvement by the proposed system in performing the intelligent dynamic control.																	2168-2267	2168-2275				AUG.	2020	50	8					3778	3792		10.1109/TCYB.2019.2919128													
J								Robust Adaptive Fault-Tolerant Tracking Control for Nonaffine Stochastic Nonlinear Systems With Full-State Constraints	IEEE TRANSACTIONS ON CYBERNETICS										Nonlinear systems; Adaptive systems; Actuators; Stochastic processes; Fault tolerance; Fault tolerant systems; Adaptation models; Adaptive fuzzy control; fault-tolerant control; full-state constraints; nonaffine stochastic nonlinear system	BARRIER LYAPUNOV FUNCTIONS; LARGE-SCALE SYSTEMS; NEURAL-CONTROL; COMPENSATION CONTROL; FEEDBACK-CONTROL; DELAY SYSTEMS; STABILIZATION; NETWORKS; DESIGN	In this article, the adaptive fault-tolerant tracking control problem of nonaffine stochastic nonlinear systems with actuator failures and full-state constraints is studied. To surmount the design difficulty from nonaffine nonlinear term with multi-input and single-output (MISO) faulty modes, a novel nonlinear fault compensation function with adjustable parameter factor is first introduced to establish a standard adaptive fault-tolerant control (AFTC) strategy based on the mean-value theorem. Then, the remaining nonlinear function, including the partial loss of effectiveness, outage, and stuck cases, together with the constructed compound nonlinear function can be approximated by using the suitable fuzzy-logic system (FLS). Moreover, it is shown that all the states of nonaffine stochastic nonlinear systems are not violating the preset constraint bounds by employing the barrier Lyapunov functions (BLFs). Also, the given adaptive controller can guarantee all the closed-loop signals are uniformly ultimately bounded (UUB) in probability in the sense of fourth-moment within the appropriate compact sets. Finally, two simulation examples are given to demonstrate the validity of the proposed method.																	2168-2267	2168-2275				AUG.	2020	50	8					3793	3805		10.1109/TCYB.2019.2940296													
J								Synchronization of Memristive Complex-Valued Neural Networks With Time Delays via Pinning Control Method	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Delay effects; Biological neural networks; Asymptotic stability; Stability criteria; Numerical stability; Complex-valued neural networks (CVNNs); memristor; pinning control; synchronization; time delays	ROBUST SYNCHRONIZATION; STABILITY; SYSTEM	This article concentrates on the synchronization problem of memristive complex-valued neural networks (CVNNs) with time delays via the pinning control method. Different from general control schemes, the pinning control is beneficial to reduce the control cost by pinning the fractional nodes instead of all ones. By separating the complex-valued system into two equivalent real-valued systems and employing the Lyapunov functional as well as some inequality techniques, the asymptotic synchronization criterion is given to guarantee the realization of synchronization of memristive CVNNs. Meanwhile, sufficient conditions for exponential synchronization of the considered systems is also proposed. Finally, the validity of our proposed results is verified by a numerical example.																	2168-2267	2168-2275				AUG.	2020	50	8					3806	3815		10.1109/TCYB.2019.2946703													
J								Set Stabilization of Probabilistic Boolean Control Networks: A Sampled-Data Control Approach	IEEE TRANSACTIONS ON CYBERNETICS										Probabilistic logic; Switches; Feedback control; Stability criteria; Adaptive control; Probabilistic Boolean control network (PBCN); sampled-data (SD) state-feedback control; semitensor product (STP); set stabilization	STATE-FEEDBACK STABILIZATION; CONTROL DESIGN; SYNCHRONIZATION; STABILITY	This article investigates the set stabilization of probabilistic Boolean control networks (PBCNs) under sampled-data (SD) state-feedback control within finite and infinite time, respectively. First, the algorithms are, respectively, proposed to find the sampled point set and the largest sampled point control invariant set (SPCIS) of PBCNs by SD state-feedback control. Based on this, a necessary and sufficient criterion is proposed for the global set stabilization of PBCNs by SD state-feedback control within finite time. Moreover, the time-optimal SD state-feedback controller is designed. It is interesting that if the sampled period (SP) is changed, the time of global set stabilization of PBCNs may also change or even the PBCNs cannot achieve set stabilization. Second, a criterion for the global set stabilization of PBCNs by SD state-feedback control within infinite time is obtained. Furthermore, all possible SD state-feedback controllers are obtained by using all the complete families of reachable sets. Finally, three examples are presented to illustrate the effectiveness of the obtained results.																	2168-2267	2168-2275				AUG.	2020	50	8					3816	3823		10.1109/TCYB.2019.2940654													
J								Character-level text classification via convolutional neural network and gated recurrent unit	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Text categorization; Convolutional neural network; Gated recurrent unit; Highway network		Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Traditional deep leaning models for text categorization are generally time-consuming on large scale datasets due to slow convergence rate or heavily rely on the pre-trained word vectors. Motivated by fully convolutional networks in the field of image processing, we introduce fully convolutional layers to substantially reduce the number of parameters in the text classification model. A character-level model for short text classification, integrating convolutional neural network, bidirectional gated recurrent unit, highway network with the fully connected layers, is proposed to capture both the global and the local textual semantics at the fast convergence speed. Furthermore, In addition, error minimization extreme learning machine is incorporated into the proposed model to improve the classification accuracy further. Extensive experiments show that our approach achieves the state-of-the-art performance compared with the existing methods on the large scale text datasets.																	1868-8071	1868-808X				AUG	2020	11	8					1939	1949		10.1007/s13042-020-01084-9													
J								Avoidance Control with Relative Velocity Information for Lagrangian Dynamics	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Collision avoidance; Feedback control; Multi-agent system; Lagrangian dynamics; Relative velocity information	COLLISION-AVOIDANCE; SYSTEMS; COORDINATION	In this paper, a novel cooperative control strategy with relative velocity information is derived to guarantee collision-free trajectories for multi-agent systems with Lagrangian dynamics. An important feature of this method is that the avoidance control input of an agent depends not only on its proximity to other agents/obstacles but also on their relative motions. For instance, agents approaching at high speeds might be more critical than slow moving yet physically closer agents. The main advantage of using this additional velocity information is that the collision avoidance maneuvers of agents are smoother, and less conservative in the sense that the agents do not spread out as much while avoiding collisions with one another. A Lyapunov-based analysis is adopted to guarantee that the agents meet their desired objectives without colliding. Finally, simulation results on three different systems are provided to illustrate the effectiveness of the proposed control strategy.																	0921-0296	1573-0409				AUG	2020	99	2					229	244		10.1007/s10846-019-01122-x													
J								An Evolutionary Approach to Time-Optimal Control of Robotic Manipulators	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Time-optimal control; Time-optimal trajectory planning; Genetic algorithm; Robotic manipulator	ALGORITHM	Time-optimal control of robotic manipulators along specified paths is a well-known problem in robotics. It concerns the minimization of the trajectory-tracking time subject to a constrained path and actuator torque limits. Calculus of variations reveals that time-optimal control is of bang-bang type, meaning that at least one actuator is in saturation for every point on the path. Unfortunately, this rule is broken at singular points, where the enforcement of the maximal and/or minimal torque at the bounding actuator would cause the violation of the path constraint. At these particular points, and, sometimes, at critical ones too, the selection of the torques is cumbersome and may introduce jitters in the control references. In this paper, the time-optimal control is addressed in the phase plane with a genetic approach. Results of calculus of variations are ignored and bang-bang control is re-found for the most of the trajectory, while in the neighborhoods of singular points, torques are automatically selected in order to minimize the trajectory-tracking time. Compared to other techniques, the problem is solved directly, without intermediate steps requiring, for example, the explicit computation of the switching points and the management of torques at critical points. The algorithm is validated in simulation on a canonical 2R planar robot in order to ease the comparison with previous works.																	0921-0296	1573-0409				AUG	2020	99	2					245	260		10.1007/s10846-019-01116-9													
J								Learning from Demonstration Based on a Classification of Task Parameters and Trajectory Optimization	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Learning from demonstration; Task parameterized movement; Trajectory optimization; Robot trajectory		Learning from demonstration involves the extraction of important information from demonstrations and the reproduction of robot action sequences or trajectories with generalization capabilities. Task parameters represent certain dependencies observed in demonstrations used to constrain and define a robot action because of the infinite nature of the state-space environment. We present the methodology for learning from demonstration based on a classification of task parameters. The classified task parameters are used to construct a cost function, responsible for describing the demonstration data. For reproduction we propose a novel trajectory optimization that is able to generate a simplified version of the trajectory for different configurations of the task parameters. As the last step before reproduction on a real robotic arm we approximate this trajectory with a Dynamic movement primitive (DMP) - based system to retrieve a smooth trajectory. Results obtained for trajectories with three degrees of freedom (two translations and one rotation) show that the system is able to encode multiple task parameters from a low number of demonstrations and generate trajectories that are collision free.																	0921-0296	1573-0409				AUG	2020	99	2					261	275		10.1007/s10846-019-01101-2													
J								Part-of-Speech and Prosody-based Approaches for Robot Speech and Gesture Synchronization	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Human-computer interaction; Multimodal interaction; Humanoid robots; Prosody; Speech; Gesture modelling; Arm gesture synthesis; Speech and gesture synchronization; Text-to-gesture		Humanoid robots are already among us and they are beginning to assume more social and personal roles, like guiding and assisting people. Thus, they should interact in a human-friendly manner, using not only verbal cues but also synchronized non-verbal and para-verbal cues. However, available robots are not able to communicate in this multimodal way, being just able to perform predefined gesture sequences, handcrafted to accompany specific utterances. In the current paper, we propose a model based on three different approaches to extend humanoid robots communication behaviour with upper body gestures synchronized with the speech for novel utterances, exploiting part-of-speech grammatical information, prosody cues, and a combination of both. User studies confirm that our methods are able to produce natural, appropriate and good timed gesture sequences synchronized with speech, using both beat and emblematic gestures.																	0921-0296	1573-0409				AUG	2020	99	2					277	287		10.1007/s10846-019-01100-3													
J								Active Asteroid-SLAM Active Graph SLAM with Landing Site Discovery in a Deep Space Proximity Operations Scenario	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Autonomous relative navigation; Active graph SLAM; Active perception; Deep space; Active state estimation; Autonomous systems	SIMULTANEOUS LOCALIZATION; NAVIGATION; EXPLORATION; FRAMEWORK; SYSTEM	In this paper, we propose an active real-time capable 3D graph based simultaneous localization and mapping (Graph SLAM) approach, which actively estimates the state of an autonomous spacecraft relative to a simultaneously established map estimate. The graph is constructed in a tightly-coupled fashion, where an Extended Kalman Filter estimates the relative offset between two of its vertices. An additional relative measurement is derived by matching point clouds obtained by a light detection and ranging (LiDAR) system. In order to yield a significant speed-up, scan matching is implemented on the GPU. To reduce the uncertainty of either the state or the map estimate, we present an approach to actively control the system resting on an extended representation of uncertainty in the map. Furthermore, it adapts its behavior depending on the current uncertainty distribution in order to find a dynamic trade-off between exploitation (improve localization performance) and exploration (improve knowledge about the environment). Finally, we present a post-processing approach to discover landing sites in the map estimate without prior knowledge. The evaluation is conducted in a numerical simulation, where the spacecraft explores the real 3D model of Itokawa in its actual dynamic environment. Within that simulation, we use a shader-based GPU implementation for simulating LiDAR measurements. We evaluate the performance of the active SLAM approach and demonstrate that the use of the adaptive approach improves navigation and exploration performance at the same time.																	0921-0296	1573-0409				AUG	2020	99	2					303	333		10.1007/s10846-019-01103-0													
J								Hybridisation of Sequential Monte Carlo Simulation with Non-linear Bounded-error State Estimation Applied to Global Localisation of Mobile Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Interval analysis; Particle filtering; Kalman filtering; Bayesian filtering; Sequential Monte Carlo simulation; Bounded-error estimation; MSC 68 Computer Science; More	SET-INVERSION	Accurate self-localisation is a fundamental ability of any mobile robot. In Monte Carlo localisation, a probability distribution over a space of possible hypotheses accommodates the inherent uncertainty in the position estimate, whereas bounded-error localisation provides a region that is guaranteed to contain the robot. However, this guarantee is accompanied by a constant probability over the confined region and therefore the information yield may not be sufficient for certain practical applications. Four hybrid localisation algorithms are proposed, combining probabilistic filtering with non-linear bounded-error state estimation based on interval analysis. A forward-backward contractor and the Set Inverter via Interval Analysis are hybridised with a bootstrap filter and an unscented particle filter, respectively. The four algorithms are applied to global localisation of an underwater robot, using simulated distance measurements to distinguishable landmarks. As opposed to previous hybrid methods found in the literature, the bounded-error state estimate is not maintained throughout the whole estimation process. Instead, it is only computed once in the beginning, when solving the wake-up robot problem, and after kidnapping of the robot, which drastically reduces the computational cost when compared to the existing algorithms. It is shown that the novel algorithms can solve the wake-up robot problem as well as the kidnapped robot problem more accurately than the two conventional probabilistic filters.																	0921-0296	1573-0409				AUG	2020	99	2					335	357		10.1007/s10846-019-01118-7													
J								An informational approach for sensor and actuator fault diagnosis for autonomous mobile robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Fault diagnosis; Model-based fault detection and isolation; Data fusion; Trajectory tracking; Kullback-Leibler divergence; Fault tolerance; Information filter	KULLBACK-LEIBLER DIVERGENCE; SYSTEMS; DESIGN	In this paper, a model-based fault detection and isolation (FDI) method is proposed, with the objective to ensure a fault-tolerant autonomous mobile robot navigation. The proposed solution uses an informational framework, which is able to detect and isolate both sensor and actuator faults, including the case of multiple faults occurrence. An information filter with a prediction model based on encoders data is adopted. For the diagnosis layer, a bank of filters are used. Residuals are generated by computing the Kullback-Leibler Divergence between the probability distribution of the predicted estimation with updated estimation obtained from sensors measurements. In order to isolate encoder and actuator faults, a secondary information filter with a prediction model based on a closed-loop controller is added. An additional bank of filters is developed, and extra residuals based on the Kullback-Leibler Divergence are generated. In the proposed method, the two designed filters supervise each other, which improves fault diagnosis, by taking into account all available information of the system, from control objective to multi-sensor data fusion. Actuator and sensor faults are treated within the same frame during the fusion process, and multiple faults occurrence is considered. A real-time experimentation on a real differential mobile robot is performed and demonstrates the efficiency of the proposed method.																	0921-0296	1573-0409				AUG	2020	99	2					387	406		10.1007/s10846-019-01099-7													
J								Any Target Function Exists in a Neighborhood of Any Sufficiently Wide Random Network: A Geometrical Perspective	NEURAL COMPUTATION												It is known that any target function is realized in a sufficiently small neighborhood of any randomly connected deep network, provided the width (the number of neurons in a layer) is sufficiently large. There are sophisticated analytical theories and discussions concerning this striking fact, but rigorous theories are very complicated. We give an elementary geometrical proof by using a simple model for the purpose of elucidating its structure. We show that high-dimensional geometry plays a magical role. When we project a high-dimensional sphere of radius 1 to a low-dimensional subspace, the uniform distribution over the sphere shrinks to a gaussian distribution with negligibly small variances and covariances.																	0899-7667	1530-888X				AUG	2020	32	8					1431	1447		10.1162/neco_a_01295													
J								Inference of a Mesoscopic Population Model from Population Spike Trains	NEURAL COMPUTATION											NEURAL-NETWORKS; DYNAMICS	Understanding how rich dynamics emerge in neural populations requires models exhibiting a wide range of behaviors while remaining interpretable in terms of connectivity and single-neuron dynamics. However, it has been challenging to fit such mechanistic spiking networks at the single-neuron scale to empirical population data. To close this gap, we propose to fit such data at a mesoscale, using a mechanistic but low-dimensional and, hence, statistically tractable model. The mesoscopic representation is obtained by approximating a population of neurons as multiple homogeneous pools of neurons and modeling the dynamics of the aggregate population activity within each pool. We derive the likelihood of both single-neuron and connectivity parameters given this activity, which can then be used to optimize parameters by gradient ascent on the log likelihood or perform Bayesian inference using Markov chain Monte Carlo (MCMC) sampling. We illustrate this approach using a model of generalized integrate-and-fire neurons for which mesoscopic dynamics have been previously derived and show that both single-neuron and connectivity parameters can be recovered from simulated data. In particular, our inference method extracts posterior correlations between model parameters, which define parameter subsets able to reproduce the data. We compute the Bayesian posterior for combinations of parameters using MCMC sampling and investigate how the approximations inherent in a mesoscopic population model affect the accuracy of the inferred single-neuron parameters.																	0899-7667	1530-888X				AUG	2020	32	8					1448	1498		10.1162/neco_a_01292													
J								Stochastic Multichannel Ranking with Brain Dynamics Preferences	NEURAL COMPUTATION											DRIVER FATIGUE; EEG; ALGORITHM; TIME; CLASSIFICATION; PREDICTION; SYSTEM	A driver's cognitive state of mental fatigue significantly affects his or her driving performance and more important, public safety. Previous studies have leveraged reaction time (RT) as the metric for mental fatigue and aim at estimating the exact value of RT using electroencephalogram (EEG) signals within a regression model. However, due to the easily corrupted and also nonsmooth properties of RTs during data collection, methods focusing on predicting the exact value of a noisy measurement, RT generally suffer from poor generalization performance. Considering that human RT is the reflection of brain dynamics preference (BDP) rather than a single regression output of EEG signals, we propose a novel channel-reliability-aware ranking (CArank) model for the multichannel ranking problem. CArank learns from BDPs using EEG data robustly and aims at preserving the ordering corresponding to RTs. In particular, we introduce a transition matrix to characterize the reliability of each channel used in the EEG data, which helps in learning with BDPs only from informative EEG channels. To handle large-scale EEG signals, we propose a stochastic-generalized expectation maximum (SGEM) algorithm to update CArank in an online fashion. Comprehensive empirical analysis on EEG signals from 40 participants shows that our CArank achieves substantial improvements in reliability while simultaneously detecting noisy or less informative EEG channels.																	0899-7667	1530-888X				AUG	2020	32	8					1499	1530		10.1162/neco_a_01293													
J								A Discrete-Time Neurodynamic Approach to Sparsity-Constrained Nonnegative Matrix Factorization	NEURAL COMPUTATION											RECURRENT NEURAL-NETWORK; LEAST-SQUARES; OPTIMIZATION; CONVERGENCE	Sparsity is a desirable property in many nonnegative matrix factorization (NMF) applications. Although some level of sparseness of NMF solutions can be achieved by using regularization, the resulting sparsity depends highly on the regularization parameter to be valued in an ad hoc way. In this letter we formulate sparse NMF as a mixed-integer optimization problem with sparsity as binary constraints. A discrete-time projection neural network is developed for solving the formulated problem. Sufficient conditions for its stability and convergence are analytically characterized by using Lyapunov's method. Experimental results on sparse feature extraction are discussed to substantiate the superiority of this approach to extracting highly sparse features.																	0899-7667	1530-888X				AUG	2020	32	8					1531	1562		10.1162/neco_a_01294													
J								On a Scalable Entropic Breaching of the Overfitting Barrier for Small Data Problems in Machine Learning	NEURAL COMPUTATION												Overfitting and treatment of small data are among the most challenging problems in machine learning (ML), when a relatively small data statistics size T is not enough to provide a robust ML fit for a relatively large data feature dimension D. Deploying a massively parallel ML analysis of generic classification problems for different D and T, we demonstrate the existence of statistically significant linear overfitting barriers for common ML methods. The results reveal that for a robust classification of bioinformatics-motivated generic problems with the long short-term memory deep learning classifier (LSTM), one needs in the best case a statistics T that is at least 13.8 times larger than the feature dimension D. We show that this overfitting barrier can be breached at a 10(-12 )fraction of the computational cost by means of the entropy-optimal scalable probabilistic approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold dassification performance boost when compared to standard bioinformatics tools and a 7-fold boost when compared to the deep learning LSTM classifier.																	0899-7667	1530-888X				AUG	2020	32	8					1563	1579		10.1162/neco_a_01296													
J								Theory and Algorithms for Shapelet-Based Multiple-Instance Learning	NEURAL COMPUTATION											TIME-SERIES; CLASSIFICATION	We propose a new formulation of multiple-instance learning (MIL), in which a unit of data consists of a set of instances called a bag. The goal is to find a good classifier of bags based on the similarity with a "shapelet" (or pattern), where the similarity of a bag with a shapelet is the maximum similarity of instances in the bag. In previous work, some of the training instances have been chosen as shapelets with no theoretical justification. In our formulation, we use all possible, and thus infinitely many, shapelets, resulting in a richer class of classifiers. We show that the formulation is tractable, that is, it can be reduced through linear programming boosting (LPBoost) to difference of convex (DC) programs of finite (actually polynomial) size. Our theoretical result also gives justification to the heuristics of some previous work. The time complexity of the proposed algorithm highly depends on the size of the set of all instances in the training sample. To apply to the data containing a large number of instances, we also propose a heuristic option of the algorithm without the loss of the theoretical guarantee. Our empirical study demonstrates that our algorithm uniformly works for shapelet learning tasks on time-series classification and various MIL tasks with comparable accuracy to the existing methods. Moreover, we show that the proposed heuristics allow us to achieve the result in reasonable computational time.																	0899-7667	1530-888X				AUG	2020	32	8					1580	1613		10.1162/neco_a_01297													
J								Predicting the helpfulness score of online reviews using convolutional neural network	SOFT COMPUTING										E-commerce; Convolutional neural network; Machine learning; Helpfulness; Online generated reviews; Deep learning	PRODUCT REVIEWS; CONSUMER REVIEWS; USER REVIEWS; INFORMATION; SENTIMENT; COMMERCE; RATINGS; MODEL	The smart cities aim to provide an infrastructure to their citizens that reduces both their time and effort. An example of such an available infrastructure is electronic shopping. Electronic shopping has become the hotbeds of many customers as it is easier to judge the quality of the product based on the review information. The purpose of this study is to predict the best helpful online product review, out of the several thousand reviews available for the product using review representation learning. The prediction is done using a two-layered convolutional neural network model. The review texts are embedded into low-dimensional vectors using a pre-trained model. To learn the best features of the review text, three filters are used to learn tri-gram, four-gram, and five-gram features of the text. The proposed approach is found to be better than existing machine learning based models which used hand-crafted features. The very low value of mean squared error confirms the prediction accuracy of the proposed method. The proposed method can be easily applied to any kind of review as the features are calculated only from the review text and not from other domain knowledge. The proposed model helps in predicting the helpfulness score of new reviews as soon as it gets posted on the product review page.																	1432-7643	1433-7479				AUG	2020	24	15			SI		10989	11005		10.1007/s00500-019-03851-5													
J								A soft computing approach to violence detection in social media for smart cities	SOFT COMPUTING										Violence detection; Smart cities; Social media; Radon transform; Optical acceleration	AGGREGATION OPERATORS; CONTEXT	In recent years, social media has become an everyday tool for the distribution of videos in which signs of violence appear in different ways. Citizens of smart cities are demanding increasing efforts to authorities in order to maintain public safety, as well as to be efficient in an emergency response. The complexity of monitoring automatically the enormous amount of information generated through social networks results in the need for the development of systems that allow for the automatic detection of violent content in videos. This fact is becoming increasingly important in order to guarantee security for the citizens in any smart city. As a result, this work proposes the development of a system for detecting violence in videos by combining different descriptors that calculate the acceleration produced between two frames of a video. To do this, different techniques, such as the Radon transform or optical flow, are used. The trained system then performs the classification using support vector Machines. The results are promising, with accuracy rates between 85 and 97%, depending on the complexity of the databases used, which demonstrates the validity of our proposal.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11007	11017		10.1007/s00500-019-04310-x													
J								Predicting stock market trends using machine learning algorithms via public sentiment and political situation analysis	SOFT COMPUTING										Natural language processing; Predictive models; Stock markets; Sentiment analysis	NEWS; RETURNS; IMPACT	Stock market trends can be affected by external factors such as public sentiment and political events. The goal of this research is to find whether or not public sentiment and political situation on a given day can affect stock market trends of individual companies or the overall market. For this purpose, the sentiment and situation features are used in a machine learning model to find the effect of public sentiment and political situation on the prediction accuracy of algorithms for 7 days in future. Besides, interdependencies among companies and stock markets are also studied. For the sake of experimentation, stock market historical data are downloaded from Yahoo! Finance and public sentiments are obtained from Twitter. Important political events data of Pakistan are crawled from Wikipedia. The raw text data are then pre-processed, and the sentiment and situation features are generated to create the final data sets. Ten machine learning algorithms are applied to the final data sets to predict the stock market future trend. The experimental results show that the sentiment feature improves the prediction accuracy of machine learning algorithms by 0-3%, and political situation feature improves the prediction accuracy of algorithms by about 20%. Furthermore, the sentiment attribute is most effective on day 7, while the political situation attribute is most effective on day 5. SMO algorithm is found to show the best performance, while ASC and Bagging show poor performance. The interdependency results indicate that stock markets in the same industry show a medium positive correlation with each other.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11019	11043		10.1007/s00500-019-04347-y													
J								Bot prediction on social networks of Twitter in altmetrics using deep graph convolutional networks	SOFT COMPUTING										Social media; Twitter; Information spread; Smart cities; Bots; Prediction; Altmetrics; Deep learning	CITIES; MEDIA; ALGORITHM; INNOVATION; QUALITY; TWEETS	In the context of smart cities, it is crucial to filter out falsified information spread on social media channels through paid campaigns or bot-user accounts that significantly influence communication networks across the social communities and may affect smart decision-making by the citizens. In this paper, we focus on two major aspects of the Twitter social network associated with altmetrics: (a) to analyze the properties of bots on Twitter networks and (b) to distinguish between bots and human accounts. Firstly, we employed state-of-the-art social network analysis techniques that exploit Twitter's social network properties in novel altmetrics data. We found that 87% of tweets are affected by bots that are involved in the network's dominant communities. We also found that, to some extent, community size and the degree of distribution in Twitter's altmetrics network follow a power-law distribution. Furthermore, we applied a deep learning model, graph convolutional networks, to distinguish between organic (human) and bot Twitter accounts. The deployed model achieved the promising results, providing up to 71% classification accuracy over 200 epochs. Overall, the study concludes that bot presence in altmetrics-associated social media platforms can artificially inflate the number of social usage counts. As a result, special attention is required to eliminate such discrepancies when using altmetrics data for smart decision-making, such as research assessment either independently or complementary along with traditional bibliometric indices.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11109	11120		10.1007/s00500-020-04689-y													
J								Fuzzy and improved fuzzy-wavelet approach in modeling municipal residential water consumption estimation using climatic variables	SOFT COMPUTING										Climatic variable; Compress; Denoise; Fuzzy; Fuzzy wavelet	AGGREGATION OPERATORS; SYSTEM	This work highlights the importance of fuzzy-wavelet denoise and fuzzy-wavelet compress in modeling the municipal residential water consumption estimation. To begin, fuzzy logic is used with different rules, membership criteria and fuzzy set. Based on accuracy of the developed model, optimum number of rules and best membership function were selected. To improve the accuracy of the single fuzzy model, wavelets technique (denoise and compress approach) was coupled with fuzzy logic and results were compared to single fuzzy technique. To map the input and output functions, the present research work includes Mamdani fuzzy inference approach based on various climatic input variables like rainfall, maximum temperature, minimum temperature and relative humidity. The models were trained based on climatic data to a certain period, and corresponding estimated models were tested for the same period. Result highlights that models with denoise and compress approach have better accuracy compared to single fuzzy model.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11213	11222		10.1007/s00500-020-05053-w													
J								A hybrid technique for simultaneous network reconfiguration and optimal placement of distributed generation resources	SOFT COMPUTING										Distributed generation; Harmony search algorithm; Teaching-learning-based optimization; Network reconfiguration	HARMONY SEARCH ALGORITHM; LEARNING-BASED OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; DISTRIBUTION-SYSTEMS; LOSS MINIMIZATION; POWER; LOCATION; ALLOCATION; DESIGN; DGS	A new meta-heuristic method, comprehensive teaching learning harmony search optimization algorithm (CTLHSO), is developed in this paper for the simultaneous reconfiguration and optimal allocation of distributed generation resources in radial distribution systems. The proposed method is a hybridization of the teaching-learning-based optimization (TLBO) and the harmony search (HS) algorithms. Primarily, eleven mathematical benchmark functions are used to test the performance of the CTLHSO algorithm. The results are then compared with that of global best artificial bee colony (G-ABC), particle swarm artificial bee colony (PS-ABC), TLBO and improved TLBO (I-TLBO) with identical parameters and initial conditions. The results show that the CTLHSO performance is better than the G-ABC, PS-ABC, TLBO and I-TLBO. Subsequently, CTLHSO is implemented on the IEEE 33-bus and 69-bus radial distribution systems for network reconfiguration and optimal placement of distributed generation resources to minimize the power losses and improve the voltage profiles. Five case studies at three different load levels are carried out. The results obtained are found to be better than those obtained with HS algorithm, genetic algorithm, refined genetic algorithm and fireworks algorithm.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11315	11336		10.1007/s00500-019-04597-w													
J								A new algorithmic decision for categorical syllogisms via Carroll's diagrams	SOFT COMPUTING										Categorical syllogism; Validity; Deduction; Decision algorithm; Complexity		In this paper, we propose a new effective algorithm for the categorical syllogisms by using a calculus system Syllogistic Logic with Carroll Diagrams, which determines a formal approach to logical reasoning with diagrams, for representations of the fundamental Aristotelian categorical syllogisms. We show that this logical reasoning is closed under the syllogistic criterion of inference. Therefore, the calculus system is implemented to let the formalism which comprises synchronically bilateral and trilateral diagrammatical appearance and naive algorithmic nature. And also, there is no need specific knowledge or exclusive ability to understand this decision procedure as well as to use it in an algorithmic system. Consequently, the empirical contributions of this paper are to design a polynomial-time algorithm at the first time in the literature to conduce to researchers getting into the act in different areas of science used categorical syllogisms such as artificial intelligence, engineering, computer science and etc.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11337	11346		10.1007/s00500-019-04598-9													
J								Combining technique for order preference by similarity to ideal solution with relative preference relation for interval-valued fuzzy multi-criteria decision-making	SOFT COMPUTING										FMCDM; Fuzzy numbers; Interval-valued; Relative preference relation; TOPSIS	TOPSIS METHOD; MODEL	In the past, technique for order preference by similarity to ideal solution (TOPSIS) was one of multi-criteria decision-making (MCDM) methods and often extended into a fuzzy multi-criteria decision-making (FMCDM) one for encompassing uncertainty and vagueness messages. Obviously, TOPSIS extended under fuzzy environment is useful to solve FMCDM problems, but the extension only constructed on general fuzzy numbers (i.e., triangular or trapezoidal fuzzy ones), not interval-valued fuzzy ones. In real world, the recent decision-making process is getting complicated and thus more information now must be grasped than ever. For presenting varied and added data, general fuzzy numbers may not be adequate, whereas interval-valued fuzzy numbers are suitable. Based on above, TOPSIS should be extended under interval-valued fuzzy environment. In this paper, we associate TOPSIS with a relative preference relation under interval-valued fuzzy environment into interval-valued FMCDM for dealing with complicated decision-making problems to obtain more information. The proposed relative preference relation as similar as Wang's relative preference relation is also improved from Lee's fuzzy preference relation on general fuzzy numbers. An important difference is the proposed relative preference relation used on interval-valued fuzzy numbers, but Wang's relative preference relation is utilized on triangular fuzzy numbers. Through the combination of TOPSIS and relative preference relation under interval-valued fuzzy environment, interval-valued FMCDM can be feasibly and rationally finished.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11347	11364		10.1007/s00500-019-04599-8													
J								A new QPSO based hybrid algorithm for constrained optimization problems via tournamenting process	SOFT COMPUTING										Constrained optimization; PSO; Adaptive QPSO; Gaussian QPSO; Tournamenting; Hybrid algorithm; Engineering design problem	PARTICLE SWARM OPTIMIZATION; MIGRATING GENETIC ALGORITHM; SELECTION; GSA	The goal of this paper is to propose a new hybrid algorithm based on advanced quantum behaved particle swarm optimization (QPSO) technique and binary tournamenting for solving constrained optimization problems. In binary tournamenting, six different situations/options are considered and accordingly six variants of hybrid algorithms are proposed. Then to test the efficiency and performance of these algorithms and also to select the best algorithm among these, six benchmark optimization problems are selected and solved. Then the computational results are compared graphically as well as numerically. Finally, the best found algorithm is applied to solve three engineering design problems and the computational results are compared with the existing algorithms available in the literature.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11365	11379		10.1007/s00500-019-04601-3													
J								Data augmentation using MG-GAN for improved cancer classification on gene expression data	SOFT COMPUTING										Data augmentation; Generative adversarial network; Gene expression dataset; Cancer detection; Modified generator GAN; Multivariate noise; Gaussian distribution; Latent space; Saddle point	NETWORKS	Molecular biology studies on cancer, using gene expression datasets, have revealed that the datasets have a very small number of samples. Obtaining medical data is difficult and expensive due to privacy constraints. Accuracy of classifiers depends greatly on the quality and quantity of input data. The problem of small sample size or small data size has been addressed by augmentation. Owing to the sensitivity of synthetic data samples for the cancer data classification for gene expression data, this paper is motivated to investigate data augmentation using GAN. GAN is based on the principle of two blocks (generator and discriminator) working in a collaborative yet adversarial way. This paper proposes modified generator GAN (MG-GAN) where the generator is fed with original data and multivariate noise to generate data with Gaussian distribution. As the generated data lie within latent space, we reach saddle point faster. GAN has been widely used in data augmentation for image datasets. As per our understanding, this is the first attempt of using GAN for augmentation on gene expression dataset. The performance merit of proposed MG-GAN was compared with KNN and Basic GAN. As compared to KNN and GAN, MG-GAN improves classification accuracy by 18.8% and 11.9%, respectively. The loss value of the error function for MG-GAN is drastically reduced, from 0.6978 to 0.0082, ensuring sensitivity of the generated data. Improved classification accuracy and reduction in the loss value make our improved MG-GAN method better suited for critical applications with sensitive data.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11381	11391		10.1007/s00500-019-04602-2													
J								An improvement growing neural gas method for online anomaly detection of aerospace payloads	SOFT COMPUTING										Aerospace payloads; Gamma ray detector; Incremental learning; Online monitoring; Anomaly detection	NETWORK	The unfluctuating running of on-orbit spacecraft equipment has a decisive impact on the smooth implementation of space exploration mission. However, due to the adverse work conditions and complex running states, it is really a challenge for the online monitoring of aerospace equipment. In this paper, an improved growing neural gas method based on incremental learning is proposed, which is dedicated to solving the problem of online anomaly detection. The learning rate of the proposed method is adaptively adjusted according to the process of model training, ensuring the weights update quickly at the beginning of model construction and converge steady at the end of model training. The optimized insertion mechanisms of neurons ensure that the necessary new neurons are inserted at the right time and location dynamically, while the innovative deletion mechanisms of neurons ensure that the worthless neurons be deleted timely and at the same time guarantee the representation ability of model. The comparison results with the conventional methods on public datasets show that the proposed method achieves the better performance obviously, both in the aspects of detection accuracy and computational efficiency, respectively. At last, as a case study, the proposed method is used for online anomaly detection of a real aerospace device, i.e., a gamma ray detector, and the finalF(1)score of anomaly detection is as high as 98.78%. The results show that the proposed method can be applied to online detection of aerospace equipment health conditions effectively.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11393	11405		10.1007/s00500-019-04603-1													
J								Feature selection by using privacy-preserving of recommendation systems based on collaborative filtering and mutual trust in social networks	SOFT COMPUTING										Privacy-preserving; Error rate; Collaborative filtering; Recommendation systems; Clustering; Perturbation; Chaos	FUZZY C-MEANS; SCHEME; SWARM; CLASSIFICATION; FRAMEWORK; MODEL	Given the increasing growth of the Web and consequently the growth of e-commerce, the amount of data which users face are increasing day by day. Therefore, one of the key issues in today's world is the extraction of knowledge from a large database. The recommendation systems are able to extract useful information from large databases. The information extracted by the recommendation systems may breach the privacy-preserving of individuals and increase the error rate. Concerns will grow along with the increasing privacy breaches, which are done by recommendation systems. In recent years, researchers have provided a variety of techniques for privacy-preserving and reduced error rates in recommendation systems. But most of these methods have not offered good solutions for privacy-preserving issues and reducing error rates. The aim of the proposed method is to provide a solution for users' security concerns in common filtering systems with reduced error rates and more privacy preservation. In this article, we propose a privacy-preserving method for recommendation systems called PRS, which first uses an anonymous method to convert secondary data without user identification information. The existing trust data are measured in terms of resemblance and trust-weighted criterion and then converted from perturbation-based chaos to confidential data. Finally, these two algorithms have been used for clustering the data: fuzzy c-ordered means and particle swarm optimization. The results of experiments have been compared with state-of-the-art methods, which show the superiority of the proposed method in terms of classification error rates and privacy-preserving.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11425	11440		10.1007/s00500-019-04605-z													
J								New SVM kernel soft computing models for wind speed prediction in renewable energy applications	SOFT COMPUTING										Singular spectrum analysis (SSA); Variational mode decomposition (VMD); Support vector machine (SVM); Multi-step wind speed prediction; Renewable energy; Sustainable systems	ENTROPY	This paper proposes a hybrid multi-step wind speed prediction model based on combination of singular spectrum analysis (SSA), variational mode decomposition (VMD) and support vector machine (SVM) and was applied for sustainable renewable energy application. In the proposed SSA-VMD-SVM model, the SSA was applied to eliminate the noise and to approximate the signal with trend information; VMD was applied to decompose and to extract the features of input time series wind speed data into a number of sub-layers; and the SVM model with various kernel functions was adopted to predict the wind speed from each of the sub-layers, and the parameters of SVM were fine-tuned by differential evolutionary algorithm. To investigate the effectiveness of the proposed model, various prediction models are considered for comparative study, and it is demonstrated that the proposed model outperforms with better prediction accuracy.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11441	11458		10.1007/s00500-019-04608-w													
J								A comparative study of exact methods for the simple assembly line balancing problem	SOFT COMPUTING										Assembly line balancing; Combinatorial optimization; Heuristics; Branch and bound	WORKER ASSIGNMENT; BOUND ALGORITHM; REMEMBER ALGORITHM; BRANCH	Exact methods have shown advanced and promising performance in solving the simple assembly line balancing problem, known as NP-hard. This research investigates the impact of various structural parameters on the performance of exact methods, includingbranching methods,search direction,method to achieve upper bounds,utilized lower bounds,utilized dominance rulesandsearch strategy. In accordance with the structural parameter evaluation,utilized dominance rulesandsearch strategyhave shown the most important effect on the exact methods' performance. This research also improves and re-implements three well-known exact methods [i.e., SALOME, bounded dynamic programming (BDP) heuristic and branch, bound and remember (BBR) algorithm] using effective parameters. Computational study demonstrates that the utilization of high-performance structural parameters enhances the performance of exact methods by a significant margin. The re-implemented BBR method with proper parameters shows clear superiority over all the published exact methods and might be regarded as the state-of-the-art exact methodology.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11459	11475		10.1007/s00500-019-04609-9													
J								Enhancing human iris recognition performance in unconstrained environment using ensemble of convolutional and residual deep neural network models	SOFT COMPUTING										Contact lens detection; Iris recognition; Convolutional neural network (CNN); Residual network; Unconstrained environment	CLASSIFICATION; SEGMENTATION; DESIGN	Despite the prominent advancements in iris recognition, unconstrained image acquisition through heterogeneous sensors has been a major obstacle in applying it for large-scale applications. In recent years, deep convolutional networks have achieved remarkable performance in the field of computer vision and have been employed in iris applications. In this study, three distinct models based on the ensemble of convolutional and residual blocks are proposed to enrich heterogeneous (cross-sensor) iris recognition. In order to analyze their quantitative performances, extensive experiments are carried out on two publicly available iris databases, ND-iris-0405 dataset and ND-CrossSensor-Iris-2013 dataset. Further, the final model has been scrutinized based on the least error rate and then fused using score-level fusion with two preeminent feature extraction methods, i.e., scale-invariant feature transform and binarized statistical information features. The resultant model is examined for cross-sensor iris recognition and reported the top two error rates as 1.01% and 1.12%. It infers that the proposed approach constitutes vital discerning iris features and can recognize that the micro-patterns exist inside the iris region. Furthermore, a comparative study is carried out with the state of the art, where the proposed approach obtains significantly improved performance.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11477	11491		10.1007/s00500-019-04610-2													
J								Design of comprehensive evaluation index system for P2P credit risk of "three rural" borrowers	SOFT COMPUTING										"Three rural" P2P lending; Credit risk; Evaluation index system design; Two-stage feature selection	WRAPPER FEATURE-SELECTION; RANDOM FOREST; GENETIC ALGORITHM; HYBRID FILTER; INFORMATION; REGRESSION	In the emerging peer-to-peer (P2P) lending industry, risks such as credit risk and default risk will bring huge losses to online lending platforms and investors. Therefore, it is necessary to design a reasonable evaluation index system of credit risk to scientifically evaluate the risk level of borrowers. This paper studies the design of comprehensive evaluation index system for P2P credit risk of "three rural" (i.e., agriculture, rural areas and farmers) borrowers. Concretely, we construct the feature set for P2P credit risk of "three rural" borrowers. Based on the traditional index system, we add the static indexes specific to the agriculture-related borrowers and the dynamic indexes reflect the Internet as the preliminary indexes of the feature set and select the borrowers data of the "Pterosaur loan" platform as the research sample. Then, 35 borrower credit features are extracted as a feature set of credit risk. Then, we present a two-stage feature selection method based on filter and wrapper to select the main features from 35 initial borrower credit features. In the stage of filter, three filter methods are used to calculate the importance of the unbalanced features. In the stage of wrapper, a Lasso-logistic method is proposed to filter the feature subset through heuristic search algorithm. In the end, 21 main independent features are selected according to the classification accuracy, which constitute the evaluation index system of credit risk of "three rural" borrowers.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11493	11509		10.1007/s00500-019-04613-z													
J								Differential evolutionary algorithm with an evolutionary state estimation method and a two-level selection mechanism	SOFT COMPUTING										Differential evolutionary algorithm; Single-objective optimization; Evolutionary state estimation; Selection	MUTATION STRATEGIES; CONTROL PARAMETERS; OPTIMIZATION; ENSEMBLE	The efficiency and effectiveness of differential evolution (DE) greatly depend on the mutation operator due to the principle that different mutation operators are beneficial to different evolutionary states. However, it is not easy to automatically and effectively identify the evolutionary state. In this paper, we propose an evolutionary state estimation method (ESE) based on the correlation coefficient between the population's distributions in objective space (Delta f) and solution space (Delta x). To be specific,Delta fconsists of the distances between each individual and the current best individual based on their objective function values, while Delta xincludes the Euclidean distances between each individual and the current best individual based on their positions in the search space. Based on the correlation coefficient between Delta xand Delta f, the entire evolutionary process is classified into three kinds of state. At each generation, the evolutionary state is firstly determined according to the correlation coefficient, subsequently adaptively choosing a mutation operator from the corresponding candidate operator pool for each individual to generate its mutation vector. Moreover, a two-level selection mechanism (TLSM) is presented to get away from stagnation. The algorithm combines DE with ESE and TLSM (DEET for short) is proposed. Experimental results on twenty frequently used benchmark functions and the CEC2017 test problems show that DEET exhibits very competitive performance compared with other state-of-the-art DE variants.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11561	11581		10.1007/s00500-019-04621-z													
J								Solving an integrated cell formation and group layout problem using a simulated annealing enhanced by linear programming	SOFT COMPUTING										Cell formation; Group layout; Simulated annealing; Heuristic; Linear programming; Hybrid metaheuristic	ELECTROMAGNETISM-LIKE ALGORITHM; ALTERNATIVE PROCESS ROUTINGS; MANUFACTURING SYSTEM-DESIGN; MATHEMATICAL-MODEL; GENETIC ALGORITHM; MACHINE CELLS; SEQUENCE PAIR; PLACEMENT; OPTIMIZATION	This paper proposes a new approach to integrating the cell formation, group layout of rectangle-shaped machines and routing selection problems. The problem is formulated as a mixed-integer program with the objective of minimizing the handling costs. Due to the computational complexity of the problem, a hybrid simulated annealing (SA) is employed to solve the problem. The sequence-pair representation, originally proposed for block placement, is utilized for solution encoding. Two placement algorithms are developed to evaluate the objective function value of an encoded solution in the SA. The first placement algorithm is based on solving a linear program embedded with a constraint reduction algorithm. The second one is a fast heuristic that can evaluate an encoded solution in much less computational time. Benchmarks selected from the literature are solved to verify the performance of the SA and to accomplish comparisons. The computational results demonstrated the high performance of both designed hybrid SA algorithms. The comparison against the literature also indicated that the flexibility incorporated into the model results in better layouts, even if the model is solved only by a solver such as CPLEX.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11621	11639		10.1007/s00500-019-04626-8													
J								Dependency-aware software release planning through mining user preferences	SOFT COMPUTING										Fuzzy; Dependency; Mining; User preferences; Release planning	REQUIREMENTS PRIORITIZATION; EMPIRICAL-EVALUATION; SELECTION; OPTIMIZATION; SYSTEM; COST	Software vendors aim to find, for a release of the software, an optimal subset of features that gives the highest value while respecting the resource limitations. The value of a feature subset, however, is determined by the values of the individual features within that subset-which are specified by the preferences of users. But user preferences for some features may change in the presence or absence of others. As such, the values of certain software features may be influenced, either positively or negatively, by other features. Such influences are widely recognized and referred to in the literature asvalue-related dependenciesamong software features. Value-related dependencies impact the overall value of a software product and, therefore, need to be considered in software release planning. To achieve this, we have proposed identifying value-related dependencies by mining user preferences for software features. We integrate these dependencies into an integer programming model, that finds an optimal subset of the features for a release of a software product. We have demonstrated the practicality of our proposed approach by studying a real-world software project and simulations.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11673	11693		10.1007/s00500-019-04630-y													
J								Proposed soft computing models for moment capacity prediction of reinforced concrete columns	SOFT COMPUTING										Flexure failure; Moment capacity; Reinforced concrete columns; Soft computing; Structural capacity	TRANSVERSE REINFORCEMENT; DEFORMATION CAPACITY; SEISMIC BEHAVIOR; RC COLUMNS; STRENGTH; PERFORMANCE; FLEXURE; ANFIS	Computational intelligence (CI) is a powerful approach to determine the response values of complex systems. Despite their benefits, the way to reach the solution in these approaches is difficult and cannot be expressed in a clear and simple formulation. In recent years, some methods have been proposed to provide simple and efficient mathematical forms in such approaches. In this paper, five of these methods are investigated to estimate the amount of moment capacity in rectangular concrete columns based on the extracted equations of CI. To train, validate and also test the proposed equations, a set of experimental laboratory tests of RC columns were collected from PEER database, and then mathematical frameworks for calculating the target were extracted. The obtained results of the proposed structures are also compared with each other, and it was concluded that all methods with high accuracy were able to estimate the moment capacity, but equation-based neuro-fuzzy system had better results than other presented models. The proposed equations are very powerful tools for determining the final capacity of RC columns as a key element in concrete structures.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11715	11729		10.1007/s00500-019-04634-8													
J								Adaptive wavelet ELM-fuzzy inference system-based soft computing model for power estimation in sustainable CMOS VLSI circuits	SOFT COMPUTING										Very large-scale integration; Sustainable complementary metal oxide semiconductor; Soft computing; Extreme learning machine; Wavelet function; Fuzzy inference system	OPTIMIZATION; DESIGN	Rapid growth of very large-scale integration (VLSI) technologies has achieved integrating millions of transistors into a single chip. This integration into a single chip results in complex circuitry, and hence, it is required to have minimal cost and low complex power estimation approaches. Power estimation of VLSI circuits at an initial stage is most prominent because it increases the life and stability of the circuit. In this work, a modified version of extreme learning machine (ELM) neural network called as adaptive wavelet extreme learning machine neural network model (AWELM) is developed and integrated with a designed fuzzy inference system (FIS) for computing power in respect of standard International Symposium on Circuits and Systems 1989 (ISCAS 1989) benchmark circuits. The proposed method is devised to estimate the power accurately for the complementary metal oxide semiconductor VLSI circuits. The developed method does not require prior knowledge about the circuit architecture and its connections. The new AWELM-FIS technique developed in this paper estimates the power in the circuit based on the input and output information and various data of gates pertaining to VLSI circuit itself. The developed method is investigated for its validity and effectiveness by comparing it with the existing methods reported in earlier literature studies, and to train the new model, the results presented in the literature of ISCAS 1989 have been employed. Results prove the effectiveness of newly proposed AWELM-FIS approach over all other compared methods from the existing literatures.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11755	11768		10.1007/s00500-019-04636-6													
J								Development of harmonic aggregation operator with trapezoidal Pythagorean fuzzy numbers	SOFT COMPUTING										Pythagorean fuzzy numbers; Aggregation operator; Harmonic mean; Multi-criteria decision making	GROUP DECISION-MAKING; FUNDAMENTAL PROPERTIES; INFORMATION	Pythagorean fuzzy sets are one of the extensions of ordinary fuzzy sets and allow a larger domain to be utilized by decision makers with respect to other extensions. Pythagorean fuzzy sets have been often used as an effective tool for handling the vagueness of multi-criteria decision making problems. Aggregation operators are a useful tool in order to collect different information provided by different sources. The objective of this paper is to develop harmonic aggregation operators for trapezoidal Pythagorean fuzzy numbers. We developed trapezoidal Pythagorean fuzzy weighted harmonic mean operator, trapezoidal Pythagorean fuzzy ordered weighted harmonic mean operator, and trapezoidal Pythagorean fuzzy hybrid harmonic mean operator. We proved some theorems for the developed operators. Finally, we presented an illustrative example using the proposed aggregation operators in order to rank the alternatives.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11791	11803		10.1007/s00500-019-04638-4													
J								Security of MVD-based 3D video in 3D-HEVC using data hiding and encryption	JOURNAL OF REAL-TIME IMAGE PROCESSING										3D high-efficiency video coding (3D-HEVC); Multi-view video plus depth; Data hiding (MVD); Real time; Virtual view	SELECTIVE ENCRYPTION; HEVC; IMAGE; WATERMARKING; PROTECTION; STANDARD	To safely transmit secret data and protect three-dimensional (3D) videos, a novel jointly data hiding and encryption method for multi-view video plus depth based 3D video is proposed. Both data hiding and encryption are all format complaint for the 3D high-efficiency video coding (3D-HEVC), which obtains a real-time requirement. Since the depth map is not used for viewing but for rendering the virtual view, it is used to embed data by modulating the quantization parameter value of the largest encoding unit (LCU) block. According to the edge information of the depth map and the texture of the colour video, LCU blocks of the depth map are classified into four types. Different types of LCU blocks allow different embedding strength considering the quality of the rendered virtual view and the stable bitrate. Moreover, the colour video is encrypted using codeword substitution for 3D-HEVC format compliance without changing the bitrate. Experimental results demonstrate that the proposed method keeps the good quality of the virtual view after embedding data, protects the video contents efficiently, and has a limited influence on the bitrate.																	1861-8200	1861-8219				AUG	2020	17	4					773	785		10.1007/s11554-018-0817-5													
J								Parallelism exploration for 3D high-efficiency video coding depth modeling mode one	JOURNAL OF REAL-TIME IMAGE PROCESSING										Depth modeling modes; 3D-HEVC; Multicore; Parallel algorithm; GPU	COMPRESSION; EXTENSIONS	This article presents a parallelism exploration over the depth modeling mode 1 (DMM-1) encoding algorithm of the 3D high-efficiency video coding (3D-HEVC) standard and applied the proposed solutions in a multicore central processing unit (CPU) and two graphics processor unities (GPU). The article evaluates efficient parallel algorithms for DMM-1, which also take advantage of simplifications proposed in our previous works. We demonstrate that DMM-1 can obtain a scalable speedup when running in systems with several available cores even when simplifications are being applied. Experimental results for 1920 x 1088 resolution videos show that the proposed parallel algorithms achieved up to 2 frames per second (fps) in a four-cores (with eight threads) CPU and more than 30 fps in two different GPUs. Therefore, the speedup attained with GPU enables real-time 3D-video encoding applying the proposed parallelism strategies together with the DMM-1 proposed simplifications.																	1861-8200	1861-8219				AUG	2020	17	4					787	797		10.1007/s11554-018-0819-3													
J								No-reference real-time video transmission artifact detection for video signals	JOURNAL OF REAL-TIME IMAGE PROCESSING										Video transmission artifact detection; No-reference; PLAM; Real time	QUALITY; PERFORMANCE	Video signals are a very important part of multimedia applications. Due to limited network bandwidth, video signals are subjected to the compression process, which introduces different compression artifacts. During network transmission, additional artifacts are introduced in video signals due to random bit errors and packet loss (PL). Both mentioned artifact types degrade visual quality of the video signal and thus, it has to be continuously monitored to ensure the required quality of service (QoS) provided to end users. An important component of the video quality monitoring system deals with video transmission artifact detection. In this paper, a no-reference (NR) pixel-based video transmission artifact detection algorithm is proposed, called the packet loss area measure (PLAM) algorithm. When detecting video transmission artifacts, the PLAM algorithm takes into account spatial and temporal information of a video signal. The performance of the proposed PLAM algorithm has been compared to those of the three existing different PL detection algorithms on a broad set of significantly different video signals from two publicly available video databases. One of these databases, called the Referent Packet Loss (RPL) database, has been created within this research and is presented in this paper. The algorithm performance testing results show that PLAM achieves high performance and overcomes other tested algorithms. Furthermore, the results show that the PLAM algorithm is very robust when detecting video transmission artifacts in video signals of different contents, with distinct degradation levels and PL error-concealment methods used in decoder post-processing. Due to its low computational complexity, the PLAM algorithm is capable of processing Full HD and Ultra HD video signals with the frame rate up to 100 and 25 frames per second (fps), respectively, in real time, in the case when high-end CPU is used.																	1861-8200	1861-8219				AUG	2020	17	4					799	820		10.1007/s11554-018-0824-6													
J								Reconstruction of 3D human motion in real-time using particle swarm optimization with GPU-accelerated fitness function	JOURNAL OF REAL-TIME IMAGE PROCESSING										3D motion tracking; Particle swarm optimization; Parallelization of fitness function	BODY MOTION; VISION; TRACKING; CAPTURE	In this paper, a novel framework for acceleration of 3D model-based, markerless visual tracking in multi-camera videos is proposed. The objective function being the most computationally demanding part of model-based 3D motion reconstruction is calculated on a GPU. The proposed framework effectively utilizes the rendering power of OpenGL to render the 3D models in the predicted poses, whereas the CUDA threads are used to match such rendered models with the image observations and to perform particle swarm optimization-based tracking. We demonstrate effective parallelization of the particle swarm optimization on GPU. Execution of time-consuming parts of the algorithm on GPU using CUDA-OpenGL significantly accelerates the 3D motion reconstruction, making our method capable of tracking full-body movements with a maximum speed of 15 fps. Qualitative and quantitative experimental results on various four-camera benchmark datasets demonstrate the efficiency and accuracy of our method for real-time motion tracking.																	1861-8200	1861-8219				AUG	2020	17	4					821	838		10.1007/s11554-018-0825-5													
J								Fast and accurate line detection with GPU-based least median of squares	JOURNAL OF REAL-TIME IMAGE PROCESSING										Robust regression; Least median of squares (LMS); GPGPU; Line detection; Image processing; CUDA; Duality; Hough transform	HOUGH TRANSFORM; ALGORITHMS	We propose an accurate and efficient 2D line detection technique based on the standard Hough transform (SHT) and least median of squares (LMS). We prove our method to be very accurate and robust to noise and occlusions by comparing it with state-of-the-art line detection methods using both qualitative and quantitative experiments. LMS is known as being very robust but also as having high computation complexity. To make our method practical for real-time applications, we propose a parallel algorithm for LMS computation which is based on point-line duality. We also offer a very efficient implementation of this algorithm for GPU on CUDA architecture. Despite many years since LMS methods have first been described and the widespread use of GPU technology in computer vision and image-processing systems, we are unaware of previous work reporting the use of GPUs for LMS and line detection. We measure the computation time of our GPU-accelerated algorithm and prove it is suitable for real-time applications. Our accelerated LMS algorithm is up to 40 times faster than the fastest single-threaded CPU-based implementation of the state-of-the-art sequential algorithm.																	1861-8200	1861-8219				AUG	2020	17	4					839	851		10.1007/s11554-018-0827-3													
J								Exploiting architectural features of a computer vision platform towards reducing memory stalls	JOURNAL OF REAL-TIME IMAGE PROCESSING										Computer vision; Compiler optimization; Execution time; Memory bound stalls	QUALITY; SURVEILLANCE; PLACEMENT; FRAMEWORK; AFFINITY; LINUX	Computer vision applications are becoming more and more popular in embedded systems such as drones, robots, tablets, and mobile devices. These applications are both compute and memory intensive, with memory bound stalls (MBS) making a significant part of their execution time. For maximum reduction in memory stalls, compilers need to consider architectural details of a platform and utilize its hardware components efficiently. In this paper, we propose a compiler optimization for a vision-processing system through classification of memory references to reduce MBS. As the proposed optimization is based on the architectural features of a specific platform, i.e., Myriad 2, it can only be applied to other platforms having similar architectural features. The optimization consists of two steps: affinity analysis and affinity-aware instruction scheduling. We suggest two different approaches for affinity analysis, i.e., source code annotation and automated analysis. We use LLVM compiler infrastructure for implementation of the proposed optimization. Application of annotation-based approach on a memory-intensive program shows a reduction in stall cycles by 67.44%, leading to 25.61% improvement in execution time. We use 11 different image-processing benchmarks for evaluation of automated analysis approach. Experimental results show that classification of memory references reduces stall cycles, on average, by 69.83%. As all benchmarks are both compute and memory intensive, we achieve improvement in execution time by up to 30%, with a modest average of 5.79%.																	1861-8200	1861-8219				AUG	2020	17	4					853	870		10.1007/s11554-018-0830-8													
J								Fast restoration of geometric details of automobile castings scanned by RGB-D sensor	JOURNAL OF REAL-TIME IMAGE PROCESSING										RGB-D sensor; Automobile casting; GPU; Gauss-Newton solver; TSDF	3D RECONSTRUCTION; DEFECTS	The depth data of automobile castings obtained by RGB-D sensor are usually combined with noise, the classical regularization method can eliminate the noise efficiently. Yet the regularization step is too time-consuming to reconstruct the geometric details of automobile castings efficiently. Given this, we present a fast method called fast restoration of automobile castings (FRAC) to restore the geometric details of automobile castings in fast manner. First, the implicit surface data is extracted from globally aligned RGB-D images, the voxel data structure is extended to index and process the implicit surface in real time. Then, an inverse shading formula is constructed to compute TSDF (truncated signed distance field) values of casting surfaces quickly, and an objective function is designed to optimize the geometric details of casting surfaces in real time. Finally, a GPU-based Gauss-Newton solver is used to accelerate restoration of castings further. The defective casting models scanned by RGB-D sensor are quickly refined to a complete model with better accuracy. Experimental results show that with respect to the sampled automobile castings which include 359,470 points in average, the average optimization time reaches 0.66 s per frame, the average restoration time is about 6.48 s. Computing TSDF requires only about 34.8 MB GPU caches in average. FRAC is able to restore the geometric details of automobile castings in real time.																	1861-8200	1861-8219				AUG	2020	17	4					871	886		10.1007/s11554-018-0835-3													
J								Enhanced multicore-manycore interaction in high-performance video encoding	JOURNAL OF REAL-TIME IMAGE PROCESSING										Video coding; VP8; CPU-GPU interaction; Hybrid/heterogeneous architectures; Parallel processing; NVIDIA CUDA		This paper presents an efficient cooperative interaction between multicore (CPU) and manycore (GPU) resources in the design of a high-performance video encoder. The proposed technique, applied to the well-established and highly optimized VP8 encoding format, can achieve a significant speed-up with respect to the mostly optimized software encoder (up to x6), with minimum degradation of the visual quality and low processing latency. This result has been obtained through a highly optimized CPU-GPU interaction, the exploitation of specific GPU features, and a modified search algorithm specifically adapted to the GPU execution model. Several experimental results are reported and discussed, confirming the effectiveness of the proposed technique. The presented approach, though implemented for the VP8 standard, is of general interest, as it could be applied to any other video encoding scheme.																	1861-8200	1861-8219				AUG	2020	17	4					887	902		10.1007/s11554-018-0834-4													
J								Fast parallel blur detection on GPU	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image blur detection; Haar wavelet transform; GPU parallelism; Real-time algorithm; Embedded device	VISION	Blur detection, a task to determine whether an image is blurred or not, is very helpful in various applications of image processing and computer vision. In this paper, we propose a novel method to accelerate blur detection algorithms based on Haar wavelet transform. The method decouples data dependency to gain fast 3-level Haar wavelet transform. With the obtained independence, the blur detection steps can be performed in parallel using native GPU thread blocks. We evaluated our proposed method on embedded devices, desktop and server. Our experiments show that on desktop and server, the proposed method obtains a huge performance speedup. On embedded devices, our GPU-based 3-level Haar wavelet transform is up to 4.9 times better performance and 4.3 times better power efficiency than CPU-based blur detection algorithms.																	1861-8200	1861-8219				AUG	2020	17	4					903	913		10.1007/s11554-018-0837-1													
J								An intelligent video analytics model for abnormal event detection in online surveillance video	JOURNAL OF REAL-TIME IMAGE PROCESSING										IVAM; HOD; Anomaly detection; Video surveillance; Computer vision; Object detection	ANOMALY DETECTION	One of the primary tools for monitoring human movement and to prevent unwanted or unintended activities is surveillance camera. Nowadays, security management professional rely heavily on video surveillance to challenge crime and avert negative incidents that impact human society. Large numbers of surveillance camera installations have been increasing dramatically in the private and public sectors to monitor public activities. Video surveillance is one of the most effective methods to guarantee security. Fitting a surveillance camera simply transfers the captured video to security personnel. But the abnormal activities can be identified only by integrating an intelligent system to analyze the videos. Hence, this paper is motivated to design and implement an Intelligent Video Analytics Model (IVAM) also known as Human Object Detection (HOD) method for analyzing and detecting video-based abundant objects and abnormal human activities. IVAM can be deployed along with surveillance cameras in any public places like airport, hospital, shopping malls and railway station to automatically identify any abnormal event. IVAM is experimented with MATLAB software and the results are verified. The performance of IVAM is evaluated by comparing the obtained results with the existing approaches and it is proved that IVAM performs better compared to other contemporary methods in terms of accurately detecting the anomalies with less error rate and high classification accuracy.																	1861-8200	1861-8219				AUG	2020	17	4					915	930		10.1007/s11554-018-0840-6													
J								Highly parallel steered mixture-of-experts rendering at pixel-level for image and light field data	JOURNAL OF REAL-TIME IMAGE PROCESSING										Steered mixture-of-experts; Image compression; Light field rendering; Real-time rendering; GPU acceleration		A novel image approximation framework called steered mixture-of-experts (SMoE) was recently presented. SMoE has multiple applications in coding, scale-conversion, and general processing of image modalities. In particular, it has strong potential for coding and streaming higher dimensional image modalities that are necessary to leverage full translational and rotational freedom (6 degrees-of-freedom) in virtual reality for camera captured images. In this paper, we analyze the rendering performance of SMoE for 2D images and 4D light fields. Two different GPU implementations that parallelize the SMoE regression step at pixel-level are presented, including experimental evaluations based on rendering performance and quality. In this paper it is shown that on appropriate hardware, an OpenCL implementation can achieve 85 fps and 22 fps for, respectively, 1080p and 4K renderings of large models with more than 100,000 of Gaussian kernels.																	1861-8200	1861-8219				AUG	2020	17	4					931	947		10.1007/s11554-018-0843-3													
J								Fast 3D image reconstruction by cuboids and 3D Charlier's moments	JOURNAL OF REAL-TIME IMAGE PROCESSING										3D Charlier moments; 3D image reconstruction; Digital filters; 3D image cuboid representation	GEOMETRIC MOMENTS; FAST COMPUTATION; TCHEBICHEF MOMENTS; CLASSIFICATION; EFFICIENT; BINARY	In this article, we propose a novel approach to accelerate the processing of 3D images by the discrete orthogonal moments of Charlier. The proposed approach is based on two fundamental notions: The first is the acceleration of the computing time of Charlier discrete orthogonal polynomials and moments in the case of the 3D image using digital filters. The second is the description of the 3D image by a set of cuboids of fixed size instead of individual voxels by decomposing the image by cuboids of small sizes to ensure numerical stability. By applying this method, the 3D Charlier moments are calculated from the cuboids instead of the whole image, as the image processing will be locally in each cuboid. This method allows us to speed up the computation time of the moments and to avoid the problem of propagation of digital errors encountered as well when using of digital filters for 3D images of large sizes. The simulation results show the effectiveness of the proposed method in terms of the computation time of the 3D moments of Charlier and in terms of quality of 3D image.																	1861-8200	1861-8219				AUG	2020	17	4					949	965		10.1007/s11554-018-0846-0													
J								Generation of differential topographic images for surface inspection of long products	JOURNAL OF REAL-TIME IMAGE PROCESSING										Real-time imaging; 3D reconstruction; Differential image; Laser scanning		The current manufacturing industries need efficient quality control systems to ensure their products are free of defects. In most cases, surface inspection is carried out by automatic systems that process 2D images which lack measurable information such as the height or depth of the surface defects. An alternative technology for surface inspection is laser scanning. Using this technique, a 3D representation of a product can be generated and therefore, defects can be easily measured. This paper proposes a real-time algorithm to generate differential topographic images of the surface of a product using laser scanning. The images generated by the proposed method are a flattened representation of the surface of the product which compare it to a perfect-shaped product. In these images, the volumetric defects can be easily segmented and measured using computer vision techniques to fulfill the requirements of the international standards of quality. The proposed algorithm is tested on 500,000 profiles meeting the constraints of real time.																	1861-8200	1861-8219				AUG	2020	17	4					967	980		10.1007/s11554-018-0844-2													
J								Fast background removal of JPEG images based on HSV polygonal cuts for a foot scanner device	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image segmentation; JPEG compression; Embedded devices; HSV color space; Foot scanners	COLOR; SEGMENTATION; MODELS	Foot scanning devices aim to provide information on a patient's foot and to help in diagnosing issues to be corrected with orthoses. The Galaxy foot scanner developed by the Aetrex company aims to provide a computer-aided framework to help physicians in their diagnoses. As numerous embedded devices, used for image processing and 3D-reconstruction, it includes cameras which provide JPEG pictures of the object to reconstruct. In this framework, an important step is the segmentation of the image, to isolate the object of interest, but the JPEG compression introduces artifacts which can lower the performance of any segmentation procedure. In this paper, we suggest a model which takes the artifacts stemming from the JPEG compression into account. The pixels are first sorted into layers of pixels with similar valueVin the HSV color space, and the background is modeled by a polygon from an additional picture. Segmentation based on the knowledge of the background and the layer to be processed is then performed. Results obtained with the Galaxy foot scanner illustrate that this method provides good results for segmentation, while being sufficiently fast to be implemented for near real-time applications.																	1861-8200	1861-8219				AUG	2020	17	4					981	992		10.1007/s11554-019-00850-5													
J								GPU-accelerated 2D OTSU and 2D entropy-based thresholding	JOURNAL OF REAL-TIME IMAGE PROCESSING										2D OTSU thresholding; 2D entropy-based thresholding; GPU acceleration; Image binarization; 2D histogram generation	SEGMENTATION; IMAGES	Image thresholding methods are commonly used to distinguish foreground objects from a background. 2D thresholding methods consider both the value of a pixel and the mean of the pixel's neighbors, so they are less sensitive to noises than 1D thresholding methods. However, the time complexity increases from O(l(2)) to O(l(4)), where l is the number of gray levels. This paper proposes a parallel algorithm (O(l + l log l)) to accelerate both 2D OTSU and 2D entropy-based thresholding on GPU. By dividing the thresholding methods into seven cascaded parallelizable computational steps, our algorithm performs all the computations on GPU and requires no data transfer between GPU memory and main memory. The time complexity analysis explains the theoretical superiority over the state-of-the-art CPU sequential algorithm (O(l(2))). Experimental results show that our parallel thresholding runs 50 times faster than the sequential one without loss of accuracy.																	1861-8200	1861-8219				AUG	2020	17	4					993	1005		10.1007/s11554-018-00848-5													
J								Multimodal image feature detection with ROI-based optimization for image registration	JOURNAL OF REAL-TIME IMAGE PROCESSING										Multimodal; Image registration; Thermal Image; ROI; Hardware architecture; Hardware acceleration		Image registration plays an imperative part of multimodal video analysis system. In video surveillance applications, change in the environmental conditions makes the registration process hard. Use of multiple sensors makes the system more robust to environmental changes as compared to single sensor imaging system. Using multiple modalities such as infrared(IR)/thermal sensors and CMOS image sensors augment the sturdiness of the surveillance system. Here we propose hardware implementation of feature detection on Genesys 2 Kintex-7 FPGA for a multimodal surveillance system, which is robust in poor lighting conditions and affine changes. To reduce the processing time, a region of interest (ROI) is identified and feature extraction is performed in this region. Design optimization in hardware architecture resulted in achieving the real-time performance of image registration on HD 720p video.																	1861-8200	1861-8219				AUG	2020	17	4					1007	1013		10.1007/s11554-018-0847-z													
J								Knowledge-based automatic extraction of multi-structured light stripes	JOURNAL OF REAL-TIME IMAGE PROCESSING										Automatic extraction; Manual contouring; Multi-structured light stripes; Knowledge model		To achieve automatic processing of the multi-structured light stripe images, we need to extract many stripes automatically in a short time. However, due to the complexity and diversity of its work, the related research progress is very slow. Therefore, we first use the Radon transformation and grayscale transform enhancement to eliminate noise in images. Then, a new and adaptive feature model is created based on the knowledge of stripes distribution. The distribution of the stripes is matched by the model, and then the target stripe regions are picked up to realize the extraction of the stripes. In the actual verification, the qualified rate of the detection is basically over 80%, and the detection time is controlled at about 2 s. The automatic extraction of target stripes effectively avoids the tedious work that workers need to do it manually, and it is of great significance for the application of multi-structured light detection technology.																	1861-8200	1861-8219				AUG	2020	17	4					1015	1027		10.1007/s11554-019-00851-4													
J								Fast video encoding based on random forests	JOURNAL OF REAL-TIME IMAGE PROCESSING										Fast video coding; HEVC; Random forests in HEVC; Machine learning in HEVC	MODE DECISION; INTER-PREDICTION; HEVC; SELECTION; ALGORITHM	Machine learning approaches have been increasingly used to reduce the high computational complexity of high-efficiency video coding (HEVC), as this is a major limiting factor for real-time implementations, due to the decision process required to find optimal coding modes and partition sizes for the quad-tree data structures defined by the standard. This paper proposes a systematic approach to reduce the computational complexity of HEVC based on an ensemble of online and offline Random Forests classifiers. A reduced set of features for training the Random Forests classifier is proposed, based on the rankings obtained from information gain and a wrapper-based approach. The best model parameters are also obtained through a consistent and generalizable method. The proposed Random Forests classifier is used to model the coding unit and transform unit-splitting decision and the SKIP-mode prediction, as binary classification problems, taking advantage from the combination of online and offline approaches, which adapts better to the dynamic characteristics of video content. Experimental results show that, on average, the proposed approach reduces the computational complexity of HEVC by 62.64% for the random access (RA) profile and 54.57% for the low-delay (LD) main profile, with an increase in BD-Rate of 2.58% for RA and 2.97% for LD, respectively. These results outperform the previous works also using ensemble classifiers for the same purpose.																	1861-8200	1861-8219				AUG	2020	17	4					1029	1049		10.1007/s11554-019-00854-1													
J								Convolution neural network joint with mixture of extreme learning machines for feature extraction and classification of accident images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Feature extraction; Accident images' classification; Convolutional neural networks; Mixture of ELM; Ensemble learning	INJURY SEVERITY; SYSTEMS	This paper considers the accident images and develops a deep learning method for feature extraction together with a mixture of experts for classification. For the first task, the outputs of the last max-pooling layer of a Convolution Neural Network (CNN) are used to extract the hidden features automatically. For the second task, a mixture of advanced variations of Extreme Learning Machine (ELM) including basic ELM, constraint ELM (CELM), On-Line Sequential ELM (OSELM) and Kernel ELM (KELM), is developed. This ensemble classifier combines the advantages of different ELMs using a gating network and its accuracy is very high while the processing time is close to real-time. To show the efficiency, the different combinations of the traditional feature extraction and feature selection methods and the various classifiers are examined on two kinds of benchmarks including accident images' data set and some general data sets. It is shown that the proposed system detects the accidents with 99.31% precision, recall and F-measure. Besides, the precisions of accident-severity classification and involved-vehicle classification are 90.27% and 92.73%, respectively. This system is suitable for on-line processing on the accident images that will be captured by Unmanned Aerial Vehicles (UAV) or other surveillance systems.																	1861-8200	1861-8219				AUG	2020	17	4					1051	1066		10.1007/s11554-019-00852-3													
J								Self-tuning fast adaptive algorithm for impulsive noise suppression in color images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Impulsive noise reduction; Color image enhancement and restoration; Image quality; Adaptive algorithm; Switching filter	VECTOR MEDIAN FILTER; SWITCHING FILTER; REMOVAL; REDUCTION	In this paper, a self-tuning version of the newly introduced Fast Adaptive Switching Trimmed Arithmetic Mean Filter, which is a very efficient technique for impulsive noise suppression, is elaborated. Most of the methods presented in the rich literature have numerous parameters, whose proper settings are crucial for efficient noise suppression. Although researchers often provide recommended values for their algorithms' parameters, the actual choice remains in the hands of the user. Our goal is to free the operator from parameter selection dilemma and to propose an algorithm which includes required expert knowledge within itself. The only obligatory inputs of the proposed algorithm (from the user perspective) are the image itself and the size of the operating window.																	1861-8200	1861-8219				AUG	2020	17	4					1067	1087		10.1007/s11554-019-00853-2													
J								Intuitionistic fuzzy optimistic and pessimistic multi-period portfolio optimization models	SOFT COMPUTING										Multi-period portfolio optimization; Intuitionistic fuzzy; Possibility measure; Multi-objective optimization; Max-min approach	SELECTION MODEL; REBALANCING MODEL; VARIANCE	There are myriad works that deal with the fuzzy multi-period portfolio selection problem, but when we talk about multi-period portfolio selection in an intuitionistic fuzzy realm, to the best of our knowledge, there is no research work that deals with the same. So, to fill this research gap, we propose an intuitionistic fuzzy multi-period portfolio selection model with the objectives of maximization of the terminal wealth and minimization of the cumulative risk subject to several realistic constraints such as complete capital utilization, no short selling, fixed transaction costs for buying and selling, bounds on the desired returns of each period, cardinality constraint, and bounds on the minimal and the maximal proportion of the capital allocated to an asset. The membership and non-membership of the objectives are modeled using their extreme values. The proposed approach provides avenues for the inclusion and minimization of the hesitation degree into decision making, thereby resulting in a significantly better portfolio. Parameters theta Ware used to introduce the hesitation in the model, and, based on their values, the model is further categorized into optimistic and pessimistic intuitionistic fuzzy multi-period portfolio selection models for optimistic and pessimistic investors, respectively. The max-min approach is used to solve the proposed models. Furthermore, a numerical illustration is presented to exhibit the virtues of the proposed model.																	1432-7643	1433-7479				AUG	2020	24	16					11931	11956		10.1007/s00500-019-04639-3													
J								A novel clustering method built on random weight artificial neural networks and differential evolution	SOFT COMPUTING										Clustering; Randomized artificial neural network; Differential evolution; Inner distance; Outer distance	EXTREME LEARNING-MACHINE; OPTIMIZATION; ALGORITHM	Clustering is the process of partition of samples, which have not got any labels, into groups. The main aim of clustering was to achieve the lowest distance between samples in each cluster and to achieve the highest distance between the samples in a cluster with the samples in other clusters. In this paper, a novel clustering approach was proposed. This novel approach was built on the differential evolution, which is a meta-heuristic method that searches for the optimum solution, and the randomized artificial neural network, which is a kind of artificial neural network that has a single hidden layer. To evaluate and validate the proposed approach, 48 datasets were employed. Achieved results by the proposed approach were compared with the obtained results by k-means, hierarchical, k-centers clustering, and some other versions of the proposed approach, which are built on ANN and particle swarm optimization, and harmony search methods. It was found that the proposed approach is successful enough to be employed for clustering in terms of achieved inner and outer distance.																	1432-7643	1433-7479				AUG	2020	24	16					12067	12078		10.1007/s00500-019-04647-3													
J								Development and evaluation of the cascade correlation neural network and the random forest models for river stage and river flow prediction in Australia	SOFT COMPUTING										Australia; Cascade correlation neural networks; Prediction; Random forest; River flow	ARTIFICIAL-INTELLIGENCE TECHNIQUES; DISCHARGE RELATIONS; REFINED INDEX; PERFORMANCE; ALGORITHM; ACCURACY; RUNOFF; CURVE; OPTIMIZATION; UNCERTAINTY	Accurately predicting river flows over daily timescales is considered as an important task for sustainable management of freshwater ecosystems, agricultural applications, and water resources management. In this research paper, artificial intelligence (AI) techniques, namely the cascade correlation neural networks (CCNN) and the random forest (RF) models, were employed in daily river stage and river flow prediction for two river systems (i.e., Dulhunty River and Herbert River) in Australia. To develop the CCNN and RF models, a significant 3-day antecedent river stage and river flow time series were used. 80% of the whole data were used for model training and the remaining 20% for model testing. A total of ten different model structures with different input combinations were used to evaluate the optimal model in the training phase, and the results were analyzed using statistical metrics including the root mean square error (RMSE), Nash-Sutcliffe coefficient (NS), Willmott's index of agreement (WI), and Legate and McCabe's index (E-LM) in the testing phase. The inter-comparison of CCNN and RF models for both river systems showed that the CCNN model was able to generate a more accurate prediction of the river stage and river flow compared to the RF model. Due to hydro-geographic differences leading to a different underlying historical data characteristics, the optimal CCNN's performance for the Dulhunty River was found to be most accurate, in terms ofE(LM) = 0.779, WI = 0.964, andE(NS) = 0.862 versus 0.775, 0.968, and 0.885 for the Herbert River. Following the performance accuracies, the authors ascertained that the CCNN model can be taken as a preferred data intelligent tool for river stage and river flow prediction.																	1432-7643	1433-7479				AUG	2020	24	16					12079	12090		10.1007/s00500-019-04648-2													
J								Some new basic operations of probabilistic linguistic term sets and their application in multi-criteria decision making	SOFT COMPUTING										Probabilistic linguistic term set; Operations; Shapley value; Probabilistic linguistic E-VIKOR method; Improved probabilistic linguistic VIKOR method	CORRELATION-COEFFICIENTS; WEB SITES; INFORMATION; QUALITY; MODEL; METHODOLOGY; OPERATORS; SYSTEM	This paper is concerned with the operations and methods to tackle the probabilistic linguistic multi-criteria decision making (PL-MCDM) problems where criteria are interactive. To avoid the defects of the existing operations of the probabilistic linguistic term sets (PLTSs) and make the operations easier, we redefine a family of operations for PLTSs and investigate their properties in-depth. Then, based on the probabilistic linguistic group utility measure, the probabilistic linguistic individual regret measure and the probabilistic linguistic compromise measure proposed in this paper, the probabilistic linguistic E-VIKOR method is developed. To make up for the deficiency of the above method, the improved probabilistic linguistic VIKOR method which can not only consider the distances between the alternatives and the positive ideal solution but also consider the distances between the alternatives and the negative ideal solution is developed to solve the correlative PL-MCDM problems. And then a case about the video recommender system is conducted to demonstrate the applicability and effectiveness of the proposed methods. Finally, the improved probabilistic linguistic VIKOR method is compared with the probabilistic linguistic E-VIKOR method, the general VIKOR method and the extended TOPSIS method to show its merits.																	1432-7643	1433-7479				AUG	2020	24	16					12131	12148		10.1007/s00500-019-04651-7													
J								UAV tracking based on saliency detection	SOFT COMPUTING										Convolutional neural networks (CNNs); UAV tracking; Saliency detection	VISUAL TRACKING; OBJECT TRACKING	This paper presents a novel unmanned aerial vehicle tracking framework. First, hierarchical convolutional neural network features are used to track the object independently in a correlation filter tracking framework. Second, a stability criterion is proposed, which is based on the variance of tracking results of each layer. Next, tracking result is adaptively fused via the variance. Meanwhile, the criterion can be used to measure the quality of tracking results. A saliency detection method is utilized to generate candidate regions when tracking failure occurs. By virtue of this method, our tracking algorithm can robustly cope with appearance changes and prevent drifting issues. Experimental results show that our proposed tracking algorithm performs favorably against state-of-the-art methods on two benchmark datasets.																	1432-7643	1433-7479				AUG	2020	24	16					12149	12162		10.1007/s00500-019-04652-6													
J								Secure auditing and deduplication for encrypted cloud data supporting ownership modification	SOFT COMPUTING										Cloud storage; Integrity auditing; Deduplication; Ownership modification; Re-encryption	SHARED DATA; SERVICES; PRIVACY	Storing only one unique copy of the same cloud data and guaranteeing its integrity are two main goals for cloud storage auditing and deduplication schemes. In such schemes, data owners can firmly believe the data integrity by periodically auditing and the cloud server can save lots of storage space by exploiting the duplication techniques. However, when a data owner deletes or modifies his outsourced data, he should lose the ownership for the original data and should not be able to successfully retrieve this data any more. For all we know, existing cloud storage auditing and deduplication literatures fail to support the modifications of ownership, which actually occur quite often in actual cloud storage scenarios. In this paper, we propose the first deduplicated data integrity auditing scheme supporting the ownership modification. It guarantees the integrity of the outsourced data and supports the dynamic access control over the outsourced data. We employ a re-encryption algorithm and the secure identity-based broadcast encryption technology, which prevent data from being disclosed to the revoked owners, even if they previously had prior ownership of these data. The security and efficiency of our proposed scheme have been validated by detailed analysis and experiments.																	1432-7643	1433-7479				AUG	2020	24	16					12197	12214		10.1007/s00500-019-04661-5													
J								Interpolative Leishman-Stained transformation invariant deep pattern classification for white blood cells	SOFT COMPUTING										White blood cells; Interpolative; Leishman stained; Multi-directional; Transformation invariant deep classification; Deep convolutional neural network	SEGMENTATION; SMEAR	Blood analysis is regarded as the one of the most predominant examinations in medicine field to obtain patient physiological state. A significant process in the classification of white blood cells (WBC) is blood sample analysis. An automatic system that is potential of identifying WBC aids the physicians in early disease diagnosis. In contrast to previous methods, thus resulting in trade-off among computational time (CT) and performance efficiency, an interpolative Leishman-stained multi-directional transformation invariant deep classification (LSM-TIDC) for WBC is presented. LSM-TIDC method discovers possibilities of interpolation and Leishman-stained function, because they require no explicit segmentation, and yet they eliminated false regions for several input images. Next, with the preprocessed images, optimal and relevant features are extracted by applying multi-directional feature extraction. To identify and classify blood cells, a system is developed via the implementation of transformation invariant model for extraction of nucleus and subsequently performs classification through convolutional and pooling characteristics. The proposed method is evaluated by extensive experiments on benchmark database like blood cell images from Kaggle. Experimental results confirm that LSM-TIDC method significantly captures optimal and relevant features and improves the classification accuracy without compromising CT and computational overhead.																	1432-7643	1433-7479				AUG	2020	24	16					12215	12225		10.1007/s00500-019-04662-4													
J								Dynamic ensemble selection based on hesitant fuzzy multiple criteria decision making	SOFT COMPUTING										Multiple classifier systems; Dynamic ensemble selection; Competence measure; Hesitant fuzzy sets; Multiple criteria decision making	OF-THE-ART; CLASSIFIER SELECTION; INTRUSION DETECTION; COMPETENCE; ALGORITHMS; FRAMEWORK	Among several extensions of fuzzy sets, hesitant fuzzy sets (HFSs) are interesting and practical. This paper proposes an application of HFSs in multiple classifier systems (MCSs). The MCSs have been proven as an effective and robust strategy for classification problems. These systems combine different classifiers and generally are composed of three steps: generation, selection (optional) and integration. This paper focuses on the selection step and proposes a novel dynamic ensemble selection method. In particular, the proposed method employs some selection criteria to determine the range of competency of the classifiers, and then, a HMCDM (hesitant fuzzy multiple criteria decision making) method is utilized to select the appropriate classifiers. Experimental results show that the proposed framework improves classification accuracy when compared against current state-of-the-art dynamic ensemble selection techniques. The Quade nonparametric statistical test confirms the capability of our proposed method.																	1432-7643	1433-7479				AUG	2020	24	16					12241	12253		10.1007/s00500-020-04668-3													
J								Evaluation of retail industry performance ability through integrated intuitionistic fuzzy TOPSIS and data envelopment analysis approach	SOFT COMPUTING										Retail industry (RI); Multi-criteria decision-making (MCDM); Intuitionistic fuzzy TOPSIS (IF-TOPSIS); Data envelopment analysis (DEA); Decision-making units (DMUs)	PORTFOLIO SELECTION; DEA APPROACH; EFFICIENCY; PRODUCTIVITY; SUPERMARKETS; SYSTEMS	This research proposes a performance evaluation model for the retail industry (RI) through an integrated intuitionistic fuzzy Technique for Order of Preference by Similarity to Ideal Solution (IF-TOPSIS) and data envelopment analysis (DEA). In the literature, DEA is one of the most commonly applied methods for RI evaluation to measure the relative efficiency of peer decision-making units (DMUs). This research proposes a decision-making algorithm with a hierarchical structure to find performance efficiencies. The objective of this research is to develop the performance evaluation process for RI in Turkey by utilizing both qualitative and quantitative criteria. Firstly, IF-TOPSIS is applied to handle more complex problems in which the decision-maker has some uncertainty and hesitation in assigning qualitative preference values for the considered objects. Secondly, the alternatives based on both qualitative and quantitative data are formulated by DEAs to classify RI performance evaluation. The results imply that using IF-TOPSIS value as the only output into DEA classification is more accurate than just using the entire quantitative variables. The results show that the combination of the two methods is suitable for any number of DMUs.																	1432-7643	1433-7479				AUG	2020	24	16					12255	12266		10.1007/s00500-020-04669-2													
J								Efficient hybrid multi-level matching with diverse set of features for image retrieval	SOFT COMPUTING										Multimedia; Global; Local features; Similarity; Multi-level matching; Color-related feature; CEDD; LBP; SURF	CLASSIFICATION; SCALE; ROTATION; SCHEME	Content-based image retrieval has become popular in the retrieval of images from large image database using reduced human intervention. Researchers are still in need to develop effective systems for dealing many of the wide-scope scientific and medical applications. Past research works have faced a problem on differentiating different images by means of using the single features alone. In this paper, a multi-level matching scheme is introduced for retrieval of image based on a hybrid feature similarity integrating local and global features. Both global- and local-level features included in multi-level scheme are used for image representation. From an image, the color information is extracted globally using a new color, edge directivity descriptor and color-based features. Further, the interest of points from each image is detected using local descriptors called local binary pattern and speeded-up robust features. Using two image databases, the improved retrieval accuracy obtained with the combination of global and local features is analyzed. Experimental outcomes have revealed the effectiveness of proposed system on achieving 91% and 92% precision rates over two datasets compared to other existing methods.																	1432-7643	1433-7479				AUG	2020	24	16					12267	12288		10.1007/s00500-020-04671-8													
J								Performance-driven ensemble ICA chemical process monitoring based on fault-relevant models	SOFT COMPUTING										Independent component analysis; Chemical process monitoring; Ensemble learning; Bayesian inference; Fault-relevant model	INDEPENDENT COMPONENT ANALYSIS; DIMENSION REDUCTION; VARIABLE SELECTION; PCA; VECTOR	Independent component analysis has been widely used in non-Gaussian chemical process monitoring. However, using the normal operation data or adopting a certain independent component selection criteria alone to construct the monitoring model cannot achieve satisfactory monitoring performance. To address the problem effectively, a fault-relevant model selection combined with ensemble learning and the Bayesian inference method is developed in this study. First, numerous models are constructed on the basis of the randomly selected ICs. Second, the models with the highest fault detection rates for each fault are selected. Then, a screening algorithm based on the difference in detection results is adopted to reduce the redundancy of the selected models and thus improve the monitoring performance. Finally, Bayesian inference is adopted to combine the testing results of the retained models. Case studies containing numerical simulations and the Tennessee Eastman benchmark process illustrate the validity of the proposed approach.																	1432-7643	1433-7479				AUG	2020	24	16					12289	12302		10.1007/s00500-020-04673-6													
J								Intelligent video analysis for enhanced pedestrian detection by hybrid metaheuristic approach	SOFT COMPUTING										Pedestrian detection; HOG filter; Video analytics; Metaheuristic	TRACKING; PEOPLE	Intelligent video analytics for pedestrian detection plays a vital role for enhanced and effective surveillance system. Since smart city projects are gaining momentum in most of the countries nowadays, enhanced pedestrian detection plays a vital role in the field of security and surveillance. Various classification models were in existence for detecting the pedestrians which suffers from variety of challenges like illumination, pedestrian outfits, gestures, occlusion, lighting, etc., that affects the accuracy of detection. A strong feature vector describing the pedestrian is developed to enhance the accuracy of detection. In this paper, a novel hybrid metaheuristic pedestrian detection (HMPD) approach is proposed to enhance the accuracy of the classifier. HMPD extracts the working principles of support vector machine and genetic algorithm. The proposed model is trained using a set of human and non-human images. The accuracy of the proposed model is tested with benchmarking video data available at VISOR repository. The result clearly shows that HMPD approach produces the maximum accuracy than any traditional approaches. HMPD approach can further be applied in other domains for enhanced security and surveillance.																	1432-7643	1433-7479				AUG	2020	24	16					12303	12311		10.1007/s00500-020-04674-5													
J								Multiple criteria decision making based on weighted Archimedean power partitioned Bonferroni aggregation operators of generalised orthopair membership grades	SOFT COMPUTING										Multiple criteria decision making; Aggregation operator; Generalised orthopair fuzzy set; Bonferroni mean; Geometric Bonferroni mean; ArchimedeanT-norm andT-conorm	FUZZY INFORMATION; MEAN OPERATORS; SUPPLIER SELECTION; OPERATIONS; ALGORITHM; TOPSIS; SET	In this paper, a multiple criteria decision making (MCDM) method based on weighted Archimedean power partitioned Bonferroni aggregation operators of generalised orthopair membership grades (GOMGs) is proposed. Bonferroni mean operator, geometric Bonferroni mean operator, power average operator, partitioned average operator, and ArchimedeanT-norm andT-conorm operations are introduced into generalised orthopair fuzzy sets to develop the Bonferroni aggregation operators. Their formal definitions are provided, and generalised and specific expressions are constructed. On the basis of the specific operators, a method for solving the MCDM problems based on GOMGs is designed. The working process, characteristics, and feasibility of the method are, respectively, demonstrated via a numerical example, a qualitative comparison at the aspect of characteristics, and a quantitative comparison using the example as benchmark. The demonstration results show that the proposed method is feasible that has desirable generality and flexibility in the aggregation of criterion values and concurrently has the capabilities to deal with the heterogeneous interrelationships of criteria, reduce the negative influence of biased criterion values, and capture the risk attitudes of decision makers.																	1432-7643	1433-7479				AUG	2020	24	16					12329	12355		10.1007/s00500-020-04676-3													
J								Note on "Fuzzy multi-granulation decision-theoretic rough sets based on fuzzy preference relation"	SOFT COMPUTING										Fuzzy preference relation; Upward fuzzy preference relation; Ordinal decision system		In this note, we point out that the transfer function for computing the fuzzy preference degree in Mandal and Ranadive (Soft Comput, 2018.) for the construction of upward/downward fuzzy relations is not additive consistent. Appropriate counterexample is given. Further, their modified versions are presented.																	1432-7643	1433-7479				AUG	2020	24	16					12357	12360		10.1007/s00500-020-04677-2													
J								Wireless sensor network intrusion detection system based on MK-ELM	SOFT COMPUTING										Wireless sensor networks; Intrusion detection; Kernel extreme learning machine; Multi-kernel learning		Advances in digital electronics, wireless communications, and electro-mechanical systems technology have revolutionized the society and economy across the globe by enabling the development of low-cost, low-power, and multi-functional sensor nodes, from which the sensor networks are realized by leveraging the features of sensing, data processing, and communication present in these nodes. Though the energy of the wireless sensor network (WSN) nodes is limited, the detection of existing intrusion detection systems in WSN is weakly accurate further. To reduce the energy consumption of nodes in WSNs during detection processing, we propose a hierarchical intrusion detection model that clusters the nodes in a WSN according to their functions. Even more, to improve the detection accuracy of abnormal behavior of the WSN intrusion detection system and reduce the false alarm rate, it is considered in this research the usage of the classification algorithm of kernel extreme learning machine, following to Mercer Property to synthesize multi-kernel functions. We realize the optimal linear combination by testing and applying the multi-kernel function and build a multi-kernel extreme learning machine to WSN intrusion detection systems. Simulation results show that the system not only guarantees a high detection accuracy but also dramatically reduces the detection time, being well suited for resource-constrained WSNs.																	1432-7643	1433-7479				AUG	2020	24	16					12361	12374		10.1007/s00500-020-04678-1													
J								A novel parallel image encryption algorithm based on hybrid chaotic maps with OpenCL implementation	SOFT COMPUTING										Hybrid chaotic maps; Image encryption; Parallel computing; OpenCL; GPU	BLOCK CIPHER; STANDARD	Since chaotic maps have the excellent properties of unpredictability, ergodicity and sensitivity to their parameters and initial values, they are quite suitable for generating chaotic sequences for securing communication systems and are also especially useful for securing images, and a lot of chaotic map-based image encryption algorithms have been proposed. But some existing image encryption algorithms were proved that their security, encryption efficiency or computational speeds are not quite satisfactory for practical applications. Some of them using only one type of chaotic system may suffer from low key space, and some others using two or more types of chaotic system may suffer from high computational overheads. In this paper, based on the classic 1D logistic map, a 2D one-coupling logistic dynamics system and OpenCL, a novel parallel image encryption algorithm HCMO is proposed. Our algorithm consists of a confusion phase and a diffusion phase using four sub-key matrices based on the hybrid logistic dynamics systems, the linear transformation and the enlarging operation. In the confusion phase, the image's pixel positions are first scrambled by performing row-wise and column-wise permutation operations using two sub-key matrices; then, in its diffusion phase, both the bit XOR operation and the bit cyclic shifting are applied onto the scrambled intermediate image matrix using the other two sub-key matrices. In order to reduce the whole encrypting execution time, we speed up our HCMO on an OpenCL's heterogeneous and parallel characteristics. Compared to the implementation of Vihari's algorithm and some other chaotic map-based algorithms referred in this paper with the OpenCL-based implementation on the CPU and on the GPU, respectively, our algorithm's simulation demonstrates remarkable improvement in the operational speedup, and the experimental result analyses have also shown that HCMO has a higher-level security than some other referred algorithms.																	1432-7643	1433-7479				AUG	2020	24	16					12413	12427		10.1007/s00500-020-04683-4													
J								Multidisciplinary design optimization of an aircraft by using knowledge-based systems	SOFT COMPUTING										Multidisciplinary design optimization; Preference function; Fuzzy logic; Genetic algorithm; UAV	MULTIOBJECTIVE OPTIMIZATION; CONCEPTUAL DESIGN; ALGORITHM	A new strategy for solving multidisciplinary design optimization problems is presented in this paper. The main idea of this approach is based on the use of designer experiences and attention to his/her preferences during design optimization which is implemented using a concept called the fuzzy preference function. Two important advantages of this approach are: (1) using the experiences of expert people during optimization and (2) transforming a constrained multiobjective design optimization problem into an unconstrained single-objective design optimization problem. The multidisciplinary design optimization of an unmanned aerial vehicle (UAV) is considered to show the performance of the proposed methodology. The optimization problem in this case study is a constrained two-objective problem (minimization of takeoff weight and drag of the cruise phase), and the genetic algorithm (GA) is utilized as the optimizer. Performance, weight, aerodynamics, center of gravity, trim and dynamic stability are the considered modules in the multidisciplinary analysis that are modeled using empirical and semiempirical equations. The optimization results show that the proposed strategy has been able to offer an optimal design where has higher performance relative to other methods from the point of view of objective functions, low computational cost and simplicity of implementation.																	1432-7643	1433-7479				AUG	2020	24	16					12429	12448		10.1007/s00500-020-04684-3													
J								Fuzzy conformable fractional differential equations: novel extended approach and new numerical solutions	SOFT COMPUTING										Fuzzy conformable derivative and integral; Fuzzy conformable fractional differential equation; Characterization theorem; Reproducing kernel Hilbert space method	PARTIAL INTEGRODIFFERENTIAL EQUATIONS; HILBERT-SPACE METHOD; REPRODUCING KERNEL; ALGORITHM; SUBJECT; SYSTEMS	The aim of this article is to propose a new definition of fuzzy fractional derivative, so-called fuzzy conformable. To this end, we discussed fuzzy conformable fractional integral softly. Meanwhile, uniqueness, existence, and other properties of solutions of certain fuzzy conformable fractional differential equations under strongly generalized differentiability are also utilized. Furthermore, all needed requirements for characterizing solutions by equivalent systems of crisp conformable fractional differential equations are debated. In this orientation, modern trend and new computational algorithm in terms of analytic and approximate conformable solutions are proposed. Finally, the reproducing kernel Hilbert space method in the conformable emotion is constructed side by side with numerical results, tabulated data, and graphical representations.																	1432-7643	1433-7479				AUG	2020	24	16					12501	12522		10.1007/s00500-020-04687-0													
J								Optimal spectral and energy efficiency trade-off for massive MIMO technology: analysis on modified lion and grey wolf optimization	SOFT COMPUTING										Massive MIMO system; Spectral efficiency; Energy efficiency; RE metric; Improved meta-heuristic; Convergence analysis	SYSTEMS; WIRELESS	As the technology makes progress towards the era of fifth generation (5G) communication networks, energy efficiency (EE) becomes an significant design criterion, because it guarantees sustainable evolution. In this regard, the massive multiple-input multiple-output (MIMO) technology, where the base stations are outfitted with enormous count of antennas so as to reach multiple orders of spectral and energy efficiency gains, will be a fundamental technology enabler for 5G. This paper plans to implement a massive MIMO model considering the spectral efficiency (SE) and EE. Here, the main goal is to generate the optimal solutions for beam-forming vectors and power allocation. The optimal solution is formed in such a way that both the SE and EE are maximum through resource efficiency metric model. The beam-forming vectors and power allocations are generated by two modified meta-heuristic algorithm to frame a valuable analysis. The first algorithm uses the modified grey wolf optimization (GWO) termed as improved random vector-based GWO (IRV-GWO), and the second algorithm uses the modified lion algorithm (LA) termed as improved random vector-based LA (IRV-LA). Both the algorithms have the ability to solve the complex optimization problems under different applications with respect to better convergence rate, which in turn performs well for pertaining better trade-off between the SE and EE in massive MIMO technology.																	1432-7643	1433-7479				AUG	2020	24	16					12523	12539		10.1007/s00500-020-04690-5													
J								An enhanced soft computing-based formulation for secure data aggregation and efficient data processing in large-scale wireless sensor network	SOFT COMPUTING										Security model using enhanced fully homomorphic encryption (OSM-EFHE); Data aggregation; Message authentication code (MAC); DGHV key generation scheme; Fuzzy logic; Soft computing	FULLY HOMOMORPHIC ENCRYPTION	Rapid growth in wireless technologies and communication, wireless sensor network (WSN) skills, data gathering and management models has paved the sensor technology a great impact on all factors of human life. In WSN, maximum consumption of constrained resources is considered to be the major challenge. Additionally, secure data aggregation has made the research domain more interesting. For consuming the limited sensor node resources optimally, data aggregation model plays a vital role. It reduces the redundant and unwanted data transmission and enhances the accuracy of data, thereby reducing the energy consumption rate and consumption overhead. Hence, for balancing the energy efficient data processing with secure data aggregation in large-scale WSN, optimized security model using enhanced fully homomorphic encryption (OSM-EFHE) has been developed in this work. First, the network is divided into clusters and cluster head which acts as an aggregator is selected based on the fuzzy if-then rule which helps in consumption of energy. Second, it provides data confidentiality and maintains subjective aggregation functions through fully homomorphic encryption (FHE). In this work, Van Dijk, Gentry, Halevi and Vaikunathan key generation plan with public key compression is used which condenses the public key dimension which is one of the major computations overhead for FHE. Finally, data integrity operation has also been induced with message authentication code. When comparing with the existing approaches, simulation results make a clear note of average delay of the network as 1.2 ms and a higher throughput of 4500 bps approximately. Thus, the overall transmission of data has been increased by means of employing OSM-EFHE model.																	1432-7643	1433-7479				AUG	2020	24	16					12541	12552		10.1007/s00500-020-04694-1													
J								An optimized collaborative intrusion detection system for wireless sensor networks	SOFT COMPUTING										Cyber security; Wireless sensor networks (WSNs); Optimized collaborative intrusion detection system (OCIDS); Improved artificial bee colony (IABC) algorithm; Resources consumption; Weighted support vector machine (WSVM)	SUPPORT VECTOR MACHINE; ATTACK	In wireless sensor networks (WSNs), sensor nodes regularly monitor environment and transmit measured values for specific phenomena to a central point called base station (BS). Recently, many intrusion detection systems (IDSs) are proposed for WSNs as they are vulnerable to multiple types of attacks. Unfortunately, most of these systems cause computational overhead and consume the limited resources of sensor nodes. Since sensor nodes are limited in resources (memory, microprocessor, battery, etc.), designing a real-time IDS for WSNs should be considered. In this article, an optimized collaborative intrusion detection system (OCIDS) is proposed for WSNs. It uses an improved artificial bee colony optimization algorithm to optimize the hierarchical IDS applied to WSNs with respect to both the accuracy of intrusion detection and also the consumption of limited resources. Besides that, the proposed system optimizes the weighted support vector machine algorithm to improve the detection accuracy and reduce false alarm rate. Since in hierarchical WSNs each of sensor nodes, cluster heads, and BS has different views of the network, collaboration among them is considered in the proposed OCIDS system to provide more precise intrusion detection. To prove the efficiency and robustness of the proposed system, we analyzed and evaluated the impact of different attack scenarios on the system performance and compared its performance to other systems. Comparing its performance to others in the presence of both normal and intrusion traffic using NSL-KDD dataset proves that it exhibits the highest detection rate and lowest false alarm rate. The proposed system outperforms the other systems by achieving 97.9% average detection rate with a small standard deviation 0.9%, and the average false alarm rate reaches 1.8% with a small standard deviation 1% which shows an obvious advantage than other detection systems.																	1432-7643	1433-7479				AUG	2020	24	16					12553	12567		10.1007/s00500-020-04695-0													
J								A rough-GA based optimal feature selection in attribute profiles for classification of hyperspectral imagery	SOFT COMPUTING										Hyperspectral images; Mathematical morphology; Attribute profiles; Feature selection; Genetic algorithms; Rough sets; Support vector machines	SPECTRAL-SPATIAL CLASSIFICATION; BAND SELECTION; CONNECTED OPERATORS; FEATURE-EXTRACTION; SETS; FILTERS	Morphological attribute profiles are robust in capturing the spectral-spatial information of hyperspectral imagery. To incorporate maximum spatial information, generation of a profile using multiple attributes with large number of threshold values is a well-known approach. Although the profile contains very rich spatial information, at the same time its dimensionality increases. This raises two critical problems for hyperspectral image classification: (i) curse of dimensionality and (ii) computational complexity. To mitigate such problems, the only supervised feature selection technique that exists in the literature is computationally demanding. In this article, a fast supervised feature selection technique by exploiting rough set theory and genetic algorithms is proposed. Our technique computes the relevance and significance of each feature in the profile using rough set theory. Then, based on the relevance and significance values a novel fitness function of genetic algorithms is designed to select an optimal subset of features from the constructed profile. To show the effectiveness of the proposed technique, it is compared with the existing state-of-the-art technique by considering three real hyperspectral data sets.																	1432-7643	1433-7479				AUG	2020	24	16					12569	12585		10.1007/s00500-020-04697-y													
J								RWFOA: a random walk-based fruit fly optimization algorithm	SOFT COMPUTING										Fruit fly optimization algorithm; Random work; Probability graph; Roulette scheme; Swarm intelligence algorithm	COLONY; MODEL	As one kind of the classic swarm intelligence (SI) algorithms, fruit fly optimization algorithm has been widely used in many aspects, such as multiple objective optimization and service computing. However, due to the limitation that introduced by the initial population location, its global optimization ability is limited. Therefore, we propose a random walk-based fruit fly optimization algorithm, namely RWFOA, to enhance its global optimization ability. RWFOA employs the random walk mechanism to dynamically adjust the position of the fruit fly population, which can reduce the impact of the initial population location and thus enhance the global optimization ability. We conduct a comprehensive experimental evaluation of RWFOA by comparing it with three representative algorithms, i.e., the OFOA, CFOA and IFFO, over 29 widely used benchmark functions published in CEC 2015. Experimental results demonstrate that RWFOA can find better solutions in most of the selected benchmark functions.																	1432-7643	1433-7479				AUG	2020	24	16					12681	12690		10.1007/s00500-020-04830-x													
J								A multi-label text classification method via dynamic semantic representation model and deep neural network	APPLIED INTELLIGENCE										Text classification; Word embedding; Clustering; Sparse representation; Neural network	KRILL HERD ALGORITHM; FEATURE-SELECTION; CROWD EVACUATION; MACHINE; HYBRIDIZATION	The increment of new words and text categories requires more accurate and robust classification methods. In this paper, we propose a novel multi-label text classification method that combines dynamic semantic representation model and deep neural network (DSRM-DNN). DSRM-DNN first utilizes word embedding model and clustering algorithm to select semantic words. Then the selected words are designated as the elements of DSRM-DNN and quantified by the weighted combination of word attributes. Finally, we construct a text classifier by combining deep belief network and back-propagation neural network. During the classification process, the low-frequency words and new words are re-expressed by the existing semantic words under sparse constraint. We evaluate the performance of DSRM-DNN on RCV1-v2, Reuters-21578, EUR-Lex, and Bookmarks. Experimental results show that our method outperforms the state-of-the-art methods.																	0924-669X	1573-7497				AUG	2020	50	8					2339	2351		10.1007/s10489-020-01680-w													
J								Multi-branch cross attention model for prediction of KRAS mutation in rectal cancer with t2-weighted MRI	APPLIED INTELLIGENCE										Rectal cancer; KRAS mutation; Deep learning; Attention mechanism; T2 MRI	FEATURES	The accurate identification of KRAS mutation status on medical images is critical for doctors to specify treatment options for patients with rectal cancer. Deep learning methods have recently been successfully introduced to medical diagnosis and treatment problems, although substantial challenges remain in the computer-aided diagnosis (CAD) due to the lack of large training datasets. In this paper, we propose a multi-branch cross attention model (MBCAM) to separate KRAS mutation cases from wild type cases using limited T2-weighted MRI data. Our model is built on multiple different branches generated based on our existing MRI data, which can take full advantage of the information contained in small data sets. The cross attention block (CA block) is proposed to fuse formerly independent branches to ensure that the model can learn as many common features as possible for preventing the overfitting of the model due to the limited dataset. The inter-branch loss is proposed to constrain the learning range of the model, confirming that the model can learn more general features from multi-branch data. We tested our method on the collected dataset and compared it to four previous works and five popular deep learning models using transfer learning. Our result shows that the MBCAM achieved an accuracy of 88.92% for the prediction of KRAS mutations with an AUC of 95.75%. These results are a significant improvement over those existing methods (p < 0.05).																	0924-669X	1573-7497				AUG	2020	50	8					2352	2369		10.1007/s10489-020-01658-8													
J								Virtual machine placement based on multi-objective reinforcement learning	APPLIED INTELLIGENCE										Virtual machine placement; Reinforcement learning; Energy saving; Resource utilization; Multi-objective optimization; Cloud computing	ANT COLONY SYSTEM; ALGORITHM; OPTIMIZATION	Multi-objective virtual machine (VM) placement is a powerful tool, which can achieve different goals in data centers. It is an NP-hard problem, and various works have been proposed to solve it. However, almost all of them ignore the selection of weights. The selection of weights is difficult, but it is essential for multi-objective optimization. The inappropriate weights will cause the obtained solution set deviating from the Pareto optimal set. Fortunately, we find that this problem can be easily solved by using the Chebyshev scalarization function in multi-objective reinforcement learning (RL). In this paper, we propose a VM placement algorithm based on multi-objective RL (VMPMORL). VMPMORL is designed based on the Chebyshev scalarization function. We aim to find a Pareto approximate set to minimize energy consumption and resource wastage simultaneously. Compared with other multi-objective RL algorithms in the field of VM placement, VMPMORL not only uses the concept of the Pareto set but also solves the weight selection problem. Finally, VMPMORL is compared with some state-of-the-art algorithms in recent years. The results show that VMPMORL can achieve better performance than the approaches above.																	0924-669X	1573-7497				AUG	2020	50	8					2370	2383		10.1007/s10489-020-01633-3													
J								Targeted aspects oriented topic modeling for short texts	APPLIED INTELLIGENCE										Topic modeling; Text mining; Short text clustering; Focused analysis	WORD EMBEDDINGS	Topic modeling has demonstrated its value in short text topic discovery. For this task, a common way adopted by many topic models is to perform a full analysis to find all the possible topics. However, these topic models overlook the importance of deeper topics, leading to confusing topics discovered. In practice, people always tend to find more focused topics on some special aspects (or events), rather than a set of coarse topics. Therefore, in this paper, we propose a novel method, Targeted Aspects Oriented Topic Modeling (TATM), to discover more focused topics on specific aspects in short texts. Specifically, each short text is assigned to only one targeted aspect derived from an enhanced Dirichlet Multinomial Mixture process (E-DMM). This process helps group similar words as many as possible, which achieves topic homogeneity. In addition, TATM discovers the topics for each targeted aspect from as many angles as possible by performing target-level modeling, which achieves topic completeness. Thus, TATM can make a balance between the two conflicting properties without employing any additional information or pre-trained knowledge. The extensive experiments conducted on five real-world datasets demonstrate that our proposed model can effectively discover more focused and complete topics, and it outperforms the state-of-the-art baselines.																	0924-669X	1573-7497				AUG	2020	50	8					2384	2399		10.1007/s10489-020-01672-w													
J								Reinforcement learning with convolutional reservoir computing	APPLIED INTELLIGENCE										Reinforcement learning; Reservoir computing; Evolution strategy; Untrained convolutional neural network		Recently, reinforcement learning models have achieved great success, mastering complex tasks such as Go and other games with higher scores than human players. Many of these models store considerable data on the tasks and achieve high performance by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using the stored data. In this study, we propose a novel practical approach called reinforcement learning with a convolutional reservoir computing (RCRC) model. The RCRC model uses a fixed random-weight CNN and a reservoir computing model to extract visual and time-series features. Using these extracted features, it decides actions with an evolution strategy method. Thereby, the RCRC model has several desirable features: (1) there is no need to train the feature extractor, (2) there is no need to store training data, (3) it can take a wide range of actions, and (4) there is only a single task-dependent weight matrix to be trained. Furthermore, we show the RCRC model can solve multiple reinforcement learning tasks with a completely identical feature extractor.																	0924-669X	1573-7497				AUG	2020	50	8					2400	2410		10.1007/s10489-020-01679-3													
J								Phase enhancement model based on supervised convolutional neural network for coherent DOA estimation	APPLIED INTELLIGENCE										Phase enhancement model; Supervised convolutional neural network; Coherent DOA estimation; Multi-path effect; VHF radar	OF-ARRIVAL ESTIMATION; LOW-ANGLE ESTIMATION; BACKPROPAGATION	When the elevation of targets is smaller than beamwidth, the coherent multi-path signals will significantly degrade the direction of arrival (DOA) estimation accuracy of existing methods for a very-high-frequency (VHF) radar system. Through detailed theoretical analysis, we demonstrate that the phase distortion is the key factor of degrading the accuracy of DOA estimation. Hence, a novel phase enhancement model based on supervised convolutional neural network (CNN) for coherent DOA estimation is proposed to mitigate the phase distortion and improve estimation accuracy. The results of simulation experiments and real data have demonstrated the superiority of proposed method in DOA estimation accuracy and resolution compared to classic physics-driven methods. Moreover, the proposed scheme is suitable for the coherent DOA estimation compared with existing data-driven methods.																	0924-669X	1573-7497				AUG	2020	50	8					2411	2422		10.1007/s10489-020-01678-4													
J								Minimum interpretation by autoencoder-based serial and enhanced mutual information production	APPLIED INTELLIGENCE										AutoEncoder; Minimum interpretation; Enhanced information; Generalization; Interpretation; Mutual information	FEATURE-SELECTION; FEATURES; REPRESENTATIONS; MAXIMIZATION	The present paper aims to propose an information-theoretic method for interpreting the inference mechanism of neural networks. The new method aims to interpret the inference mechanism minimally by disentangling complex information into simpler and easily interpretable information. This disentanglement of complex information can be realized by maximizing mutual information between input patterns and the corresponding neurons. However, because the use of mutual information has faced difficulty in computation, we use the well-known autoencoder to increase mutual information by re-interpreting the sparsity constraint, which is considered a device to increase mutual information. The computational procedures to increase mutual information are decomposed into the serial operation of equal use of neurons and specific responses to input patterns. The specific responses are realized by enhancing the results by the equal use of neurons. The method was applied to three data sets: the glass, office equipment, and pulsar data sets. With all three data sets, we could observe that, when the number of neurons was forced to increase, mutual information could be increased. Then, collective weights, or average collectively treated weights, showed that the method could extract the simple and linear relations between inputs and targets, making it possible to interpret the inference mechanism minimally.																	0924-669X	1573-7497				AUG	2020	50	8					2423	2448		10.1007/s10489-019-01619-w													
J								Joint user mention behavior modeling for mentionee recommendation	APPLIED INTELLIGENCE										Mentionee recommendation; User mention behavior; Joint Latent-class model	EMOTION	As an emerging online interaction service in Twitter-like social media systems,mentionserves to significantly improve both user interaction experience and information propagation. In recent years, the problem of mentionee recommendation, i.e., recommending mentionees (mentioned users) when mentioners (mentioning users) mention others, has received considerable attention. However, the extreme sparsity of mentioner-mentionee matrix creates a severe challenge. While an increasing line of work has exploited diverse effects such as the textual content and spatio-temporal context influences to cope with this challenge, there lacks a comprehensive study of the joint effect of all these influencing factors. In light of this, we propose a joint latent-class probabilistic model, named Joint Topic-Area Model (JTAM), to tackle the mentionee recommendation problem by simultaneously learning and modeling users' semantic interests, the spatio-temporal mentioning patterns of mentioners, the geographical distribution of mentionees, and their joint effects on users' mention behaviors in a unified way. Moreover, to facilitate online query performance, we design an efficient query answering approach that enables fast top-kmentionee recommendation. To evaluate the performance of our method, we conduct extensive experiments on a large real-world dataset. The results demonstrate the superiority of our method in recommending mentionees in terms of both effectiveness and efficiency compared with other state-of-the-art methods.																	0924-669X	1573-7497				AUG	2020	50	8					2449	2464		10.1007/s10489-020-01635-1													
J								Oversampling technique based on fuzzy representativeness difference for classifying imbalanced data	APPLIED INTELLIGENCE										Class imbalanced problem; Oversampling technique; Affinity propagation; Chromosome theory of inheritance; Fuzzy representativeness difference	SAMPLING APPROACH; ALGORITHMS; CLASSIFICATION; PREDICTION; SMOTE	Class imbalance problem poses a difficulty to learning algorithms in pattern classification. Oversampling techniques is one of the most widely used techniques to solve these problems, but the majority of them use the sample size ratio as an imbalanced standard. This paper proposes a fuzzy representativeness difference-based oversampling technique, using affinity propagation and the chromosome theory of inheritance (FRDOAC). The fuzzy representativeness difference (FRD) is adopted as a new imbalance metric, which focuses on the importance of samples rather than the number. FRDOAC firstly finds the representative samples of each class according to affinity propagation. Secondly, fuzzy representativeness of every sample is calculated by the Mahalanobis distance. Finally, synthetic positive samples are generated by the chromosome theory of inheritance until the fuzzy representativeness difference of two classes is small. A thorough experimental study on 16 benchmark datasets was performed and the results show that our method is better than other advanced imbalanced classification algorithms in terms of various evaluation metrics.																	0924-669X	1573-7497				AUG	2020	50	8					2465	2487		10.1007/s10489-020-01644-0													
J								Deep reinforcement learning for imbalanced classification	APPLIED INTELLIGENCE										Imbalanced classification; Deep reinforcement learning; Reward function; Classification policy		Data in real-world application often exhibit skewed class distribution which poses an intense challenge for machine learning. Conventional classification algorithms are not effective in case of imbalanced data distribution, and may fail when the data distribution is highly imbalanced. To address this issue, we propose a general imbalanced classification model based on deep reinforcement learning, in which we formulate the classification problem as a sequential decision-making process and solve it by a deep Q-learning network. In our model, the agent performs a classification action on one sample in each time step, and the environment evaluates the classification action and returns a reward to the agent. The reward from the minority class sample is larger, so the agent is more sensitive to the minority class. The agent finally finds an optimal classification policy in imbalanced data under the guidance of the specific reward function and beneficial simulated environment. Experiments have shown that our proposed model outperforms other imbalanced classification algorithms, and identifies more minority samples with better classification performance.																	0924-669X	1573-7497				AUG	2020	50	8					2488	2502		10.1007/s10489-020-01637-z													
J								A novel community detection method based on whale optimization algorithm with evolutionary population	APPLIED INTELLIGENCE										Complex network; Community detection; Whale optimization algorithm; Evolutionary population	PARTICLE SWARM OPTIMIZATION; COLONY OPTIMIZATION; GENETIC ALGORITHM; LABEL PROPAGATION; NODE IMPORTANCE; FACTORIZATION	Community detection is the process of detecting communities in complex networks. Communities are important structures that can help us further study the properties of complex networks. In recent years, swarm intelligence algorithms have been applied to community detection and have achieved remarkable results. However, these existing algorithms have limited search ability and easily fall into the problem of local optima. In this paper, we propose a new community detection approach based on an improved whale optimization algorithm (WOA). The WOA is applied to a discrete symbol space in solving the community detection problem, therefore topology structure-based search strategies, adjustment and mergence policies, and evolutionary population method are designed to improve the efficiency and effectiveness of the method. Then, a whale optimization algorithm with evolutionary population for community detection (EP-WOCD) is proposed. Extensive experiments are conducted to compare the EP-WOCD with other state-of-the-art algorithms on both artificial and real-world social networks. Experimental results show that the EP-WOCD is effective and stable.																	0924-669X	1573-7497				AUG	2020	50	8					2503	2522		10.1007/s10489-020-01659-7													
J								Structured block diagonal representation for subspace clustering	APPLIED INTELLIGENCE										Subspace clustering; Structured sparse subspace clustering; Block diagonal representation; Structured block diagonal representation	ALGORITHM; SEGMENTATION; MODELS	The aim of the subspace clustering is to segment the high-dimensional data into the corresponding subspace. The structured sparse subspace clustering and the block diagonal representation clustering are quite advanced spectral-type subspace clustering algorithms when handling to the linear subspaces. In this paper, the respective advantages of these two algorithms are fully exploited, and the structured block diagonal representation (SBDR) subspace clustering is proposed. In many classical spectral-type subspace clustering algorithms, the affinity matrix which obeys the block diagonal property can not necessarily bring satisfying clustering results. However, thek-block diagonal regularizer of the SBDR algorithm directly pursues the block diagonal matrix, and this regularizer is obviously more effective. On the other hand, the general procedure of the spectral-type subspace clustering algorithm is to get the affinity matrix firstly and next perform the spectral clustering. The SBDR algorithm considers the intrinsic relationship of the two seemingly separate steps, the subspace structure matrix obtained by the spectral clustering is used iteratively to facilitate a better initialization for the representation matrix. The experimental results on the synthetic dataset and the real dataset have demonstrated the superior performance of the proposed algorithm over other prevalent subspace clustering algorithms.																	0924-669X	1573-7497				AUG	2020	50	8					2523	2536		10.1007/s10489-020-01629-z													
J								D-ANP: a multiple criteria decision making method for supplier selection	APPLIED INTELLIGENCE										D numbers; Dempster-Shafer theory; Analytic Network Process (ANP); MCDM; Supplier selection	PERSONALIZED INDIVIDUAL SEMANTICS; ANALYTIC NETWORK PROCESS; D NUMBERS; DIVERGENCE MEASURE; PREFERENCE RELATIONS; CONSENSUS MODEL; HESITANT FUZZY; FAILURE MODE; PRIORITIZATION; INFORMATION	Supplier selection can be regarded as a classic multiple criteria decision making (MCDM) problem. To a great extent, experts' evaluations play a decisive role in the decision-making process. There will inevitable exist a variety of indefinite factors, which result from imprecision, uncertainty, and fuzziness due to the subjective judgment of human beings. As an effective tool to express uncertain information, the theory of D numbers performs better in comparison to other existing methods. In addition to that, analytic network process (ANP) method is applied more broadly for its advantages of flexibility, rationality and creditability than analytic hierarchy process (AHP) method. In this study, the D-ANP methodology is proposed to apply in the field of supplier selection, which is the extension of the traditional ANP method using D numbers. The validity of the presented methodology is illustrated by an application for supplier selection.																	0924-669X	1573-7497				AUG	2020	50	8					2537	2554		10.1007/s10489-020-01639-x													
J								Attribute susceptibility and entropy based data anonymization to improve users community privacy and utility in publishing data	APPLIED INTELLIGENCE										User attributes; Anonymization; Susceptibility; Entropy; Community privacy; Utility; Social network	ONLINE SOCIAL NETWORKS; PRESERVING PRIVACY; ALGORITHM; FRAMEWORK; ANONYMITY	User attributes affect community (i.e., a group of people with some common properties/attributes) privacy in users' data publishing because some attributes may expose multiple users' identities and their associated sensitive information during published data analysis. User attributes such as gender, age, and race, may allow an adversary to form users' communities based on their values, and launch sensitive information inference attack subsequently. As a result, explicit disclosure of private information of a specific users' community can occur from the privacy preserved published data. Each item of user attributes impacts users' community privacy differently, and some types of attributes are highly susceptible. More susceptible types of attributes enable multiple users' unique identifications and sensitive information inferences more easily, and their presence in published data increases users' community privacy risks. Most of the existing privacy models ignore the impact of susceptible attributes on user's community privacy and they mainly focus on preserving the individual privacy in the released data. This paper presents a novel data anonymization algorithm that significantly improves users' community privacy without sacrificing the guarantees on anonymous data utility in publishing data. The proposed algorithm quantifies the susceptibility of each attribute present in user's dataset to effectively preserve users' community privacy. Data generalization is performed adaptively by considering both user attributes' susceptibility and entropy simultaneously. The proposed algorithm controls over-generalization of the data to enhance anonymous data utility for the legitimate information consumers. Due to the widespread applications of social networks (SNs), we focused on the SN users' community privacy preserved and utility enhanced anonymous data publishing. The simulation results obtained from extensive experiments, and comparisons with the existing algorithms show the effectiveness of the proposed algorithm and verify the aforementioned claims.																	0924-669X	1573-7497				AUG	2020	50	8					2555	2574		10.1007/s10489-020-01656-w													
J								SmartIX: A database indexing agent based on reinforcement learning	APPLIED INTELLIGENCE										Artificial intelligence; Reinforcement learning; Database; Indexing		Configuring databases for efficient querying is a complex task, often carried out by a database administrator. Solving the problem of building indexes that truly optimize database access requires a substantial amount of database and domain knowledge, the lack of which often results in wasted space and memory for irrelevant indexes, possibly jeopardizing database performance for querying and certainly degrading performance for updating. In this paper, we develop theSmartIXarchitecture to solve the problem of automatically indexing a database by using reinforcement learning to optimize queries by indexing data throughout the lifetime of a database. We train and evaluateSmartIXperformance using TPC-H, a standard, and scalable database benchmark. Our empirical evaluation shows thatSmartIXconverges to indexing configurations with superior performance compared to standard baselines we define and other reinforcement learning methods used in related work.																	0924-669X	1573-7497				AUG	2020	50	8					2575	2588		10.1007/s10489-020-01674-8													
J								Rule-centred genetic programming (RCGP): an imperialist competitive approach	APPLIED INTELLIGENCE										Genetic programming; Rule-centred genetic programming; Evolutionary rules; Imperialist competitive algorithm; Symbolic regression problems	DESIGN	Automatic programming is one of the challenging fields of AI to generate solutions for high-level programming problems. There are variant methodologies attempting to introduce an efficient technique which address problems of this domain. In this paper, a novel Rule-Centred Genetic Programming(RCGP)is proposed.RCGPbenefits from a series of evolutionary rules to help the algorithm choose intelligent alterations in the chromosome of individuals during the evolution yet preserves its stochastic evolutionary nature. Further, a modified search strategy based on Imperialist Competitive Algorithm(ICA)is employed inRCGPthat shows to be significantly effective to deal with various problems which differ in degree of complexity. The proposed method features competitive convergence both in the case of speed and accuracy as well as a simpler mechanism than the several existingGPmethods.RCGPis tested on nine benchmark problems which are synthesis and real world. The obtained results indicate thatRCGPoutperforms recentGPmethods and is capable of hybridizing with other types of evolutionary algorithms. The method shows to be competent enough to enhance the quality of automatic programming solutions in both aspects of accuracy and efficiency compared to existing methods.																	0924-669X	1573-7497				AUG	2020	50	8					2589	2609		10.1007/s10489-019-01601-6													
J								k-PbC: an improved cluster center initialization for categorical data clustering	APPLIED INTELLIGENCE										Data mining; Distance-based clustering; Pattern mining; Maximal frequent itemsets; Cluster center initialization; Categorical data	EFFICIENT ALGORITHM	The performance of a partitional clustering algorithm is influenced by the initial random choice of cluster centers. Different runs of the clustering algorithm on the same data set often yield different results. This paper addresses that challenge by proposing an algorithm namedk-PbC, which takes advantage of non-random initialization from the view of pattern mining to improve clustering quality. Specifically,k-PbC first performs a maximal frequent itemset mining approach to find a set of initial clusters. It then uses a kernel-based method to form cluster centers and an information-theoretic based dissimilarity measure to estimate the distance between cluster centers and data objects. An extensive experimental study was performed on various real categorical data sets to draw a comparison betweenk-PbC and state-of-the-art categorical clustering algorithms in terms of clustering quality. Comparative results have revealed that the proposed initialization method can enhance clustering results andk-PbC outperforms compared algorithms for both internal and external validation metrics. k-PbC algorithm for categorical data clustering																	0924-669X	1573-7497				AUG	2020	50	8					2610	2632		10.1007/s10489-020-01677-5													
J								Active Online Learning for Social Media Analysis to Support Crisis Management	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Prototypes; Social networking (online); Crisis management; Uncertainty; Measurement; Labeling; Knowledge engineering; Online learning; multiple prototype classification; active learning; social media; crisis management	SENTIMENT ANALYSIS	People use social media (SM) to describe and discuss different situations they are involved in, like crises. It is therefore worthwhile to exploit SM contents to support crisis management, in particular by revealing useful and unknown information about the crises in real-time. Hence, we propose a novel active online multiple-prototype classifier, called AOMPC. It identifies relevant data related to a crisis. AOMPC is an online learning algorithm that operates on data streams and which is equipped with active learning mechanisms to actively query the label of ambiguous unlabeled data. The number of queries is controlled by a fixed budget strategy. Typically, AOMPC accommodates partly labeled data streams. AOMPC was evaluated using two types of data: (1) synthetic data and (2) SM data from Twitter related to two crises, Colorado Floods and Australia Bushfires. To provide a thorough evaluation, a whole set of known metrics was used to study the quality of the results. Moreover, a sensitivity analysis was conducted to show the effect of AOMPC's parameters on the accuracy of the results. A comparative study of AOMPC against other available online learning algorithms was performed. The experiments showed very good behavior of AOMPC for dealing with evolving, partly-labeled data streams.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1445	1458		10.1109/TKDE.2019.2906173													
J								Answering Top-k Graph Similarity Queries in Graph Databases	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Top-K; similarity search; graph database	EDIT DISTANCE; SEARCH; ALGORITHMS	Searching similar graphs in graph databases for a query graph has attracted extensive attention recently. Existing works on graph similarity queries are threshold based approaches which return graphs with distances to the query smaller than a given threshold. However, in many applications the number of answer graphs for the same threshold can vary significantly for different queries. In this paper, we study the problem of finding top-k most similar graphs for a query under the distance measure based on maximum common subgraph (MCS). Since computing MCS is NP-hard, we devise a novel framework to prune unqualified graphs based on the lower bounds of graph distance, and accordingly derive four lower bounds with different tightness and computational cost for pruning. To further reduce the number of MCS computations, we also propose an improved framework based on both lower and upper bounds, and derive three new upper bounds. To support efficient pruning, we design three indexes with different tradeoffs between pruning power and construction cost. To accelerate the index construction, we explore bound relaxation techniques, based on which approximate indexes can be efficiently built. We conducted extensive performance studies on real-life graph datasets to validate the effectiveness and efficiency of our approaches.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1459	1474		10.1109/TKDE.2019.2906608													
J								Approximate Nearest Neighbor Search on High Dimensional Data - Experiments, Analyses, and Improvement	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Similarity search; approximate nearest neighbor search; high-dimensional space; metric space; dense vector	SMALL WORLD; ALGORITHMS; QUANTIZATION; SPACE; CODES; HASH	Nearest neighbor search is a fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Because exact searching results are not efficient for a high-dimensional space, a lot of efforts have turned to approximate nearest neighbor search. Although many algorithms have been continuously proposed in the literature each year, there is no comprehensive evaluation and analysis of their performance. In this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 19 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1475	1488		10.1109/TKDE.2019.2909204													
J								Cleaning Data with Forbidden Itemsets	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Itemsets; Maintenance engineering; Cleaning; Data integrity; Data mining; Heuristic algorithms; Data cleaning; error detection; itemset mining		Methods for cleaning dirty data typically employ additional information about the data such as user-provided constraints specifying when data is dirty, e.g., domain restrictions, illegal value combinations, or logical rules. However, real-world scenarios usually only have dirty data available, without known constraints. In such settings, constraints are automatically discovered on dirty data and discovered constraints are used to detect and repair errors. Typical repairing processes stop there. Yet, when constraint discovery algorithms are re-run on the repaired data (assumed to be clean), new constraints and thus errors are often found. The repairing process then introduces new constraint violations. We present a different type of repairing method, which prevents introducing new constraint violations, according to a discovery algorithm. Summarily, our repairs guarantee that all errors identified by constraints discovered on the dirty data are fixed; and the constraint discovery process cannot identify new constraint violations. We do this for a new kind of constraints, called forbidden itemsets (FBIs), capturing unlikely value co-occurrences. We show that FBIs detect errors with high precision. Evaluation on real-world data shows that our repair method obtains high-quality repairs without introducing new FBIs. Optional user interaction is readily integrated, with users deciding how much effort to invest.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1489	1501		10.1109/TKDE.2019.2905548													
J								Explainable Outfit Recommendation with Joint Outfit Matching and Comment Generation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Visualization; Feature extraction; Task analysis; Clothing; Computer science; Recurrent neural networks; Recommender systems; Outfit recommendation; explainable recommendation		Most previous work on outfit recommendation focuses on designing visual features to enhance recommendations. Existing work neglects user comments of fashion items, which have been proven to be effective in generating explanations along with better recommendation results. We propose a novel neural network framework, neural outfit recommendation (NOR), that simultaneously provides outfit recommendations and generates abstractive comments. Neural outfit recommendation (NOR) consists of two parts: outfit matching and comment generation. For outfit matching, we propose a convolutional neural network with a mutual attention mechanism to extract visual features. The visual features are then decoded into a rating score for the matching prediction. For abstractive comment generation, we propose a gated recurrent neural network with a cross-modality attention mechanism to transform visual features into a concise sentence. The two parts are jointly trained based on a multi-task learning framework in an end-to-end back-propagation paradigm. Extensive experiments conducted on an existing dataset and a collected real-world dataset show NOR achieves significant improvements over state-of-the-art baselines for outfit recommendation. Meanwhile, our generated comments achieve impressive ROUGE and BLEU scores in comparison to human-written comments. The generated comments can be regarded as explanations for the recommendation results. We release the dataset and code to facilitate future research.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1502	1516		10.1109/TKDE.2019.2906190													
J								Generative Adversarial Active Learning for Unsupervised Outlier Detection	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Anomaly detection; Generators; Computational modeling; Data models; Training; Generative adversarial networks; Gallium nitride; Outlier detection; generate potential outliers; curse of dimensionality; generative adversarial active learning; mode collapsing problem; multiple-objective generative adversarial active learning	ANOMALY DETECTION; CLASSIFICATION; DENSITY	Outlier detection is an important topic in machine learning and has been used in a wide range of applications. In this paper, we approach outlier detection as a binary-classification issue by sampling potential outliers from a uniform reference distribution. However, due to the sparsity of data in high-dimensional space, a limited number of potential outliers may fail to provide sufficient information to assist the classifier in describing a boundary that can separate outliers from normal data effectively. To address this, we propose a novel Single-Objective Generative Adversarial Active Learning (SO-GAAL) method for outlier detection, which can directly generate informative potential outliers based on the mini-max game between a generator and a discriminator. Moreover, to prevent the generator from falling into the mode collapsing problem, the stop node of training should be determined when SO-GAAL is able to provide sufficient information. But without any prior information, it is extremely difficult for SO-GAAL. Therefore, we expand the network structure of SO-GAAL from a single generator to multiple generators with different objectives (MO-GAAL), which can generate a reasonable reference distribution for the whole dataset. We empirically compare the proposed approach with several state-of-the-art outlier detection methods on both synthetic and real-world datasets. The results show that MO-GAAL outperforms its competitors in the majority of cases, especially for datasets with various cluster types or high irrelevant variable ratio. The experiment codes are available at: https://github.com/leibinghe/GAAL-based-outlier-detection.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1517	1528		10.1109/TKDE.2019.2905606													
J								Interactive Bike Lane Planning Using Sharing Bikes' Trajectories	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Trajectory; Roads; Planning; Government; Indexes; Silicon; Parallel processing; Data mining; distributed computing; urban computing	ROAD; ROUTES	Cycling as a green transportation mode has been promoted by many governments all over the world. As a result, constructing effective bike lanes has become a crucial task to promote the cycling life style, as well-planned bike lanes can reduce traffic congestions and safety risks. Unfortunately, existing trajectory mining approaches for bike lane planning do not consider one or more key realistic government constraints: 1) budget limitations, 2) construction convenience, and 3) bike lane utilization. In this paper, we propose a data-driven approach to develop bike lane construction plans based on the large-scale real world bike trajectory data collected from Mobike, a station-less bike sharing system. We enforce these constraints to formulate our problem and introduce a flexible objective function to tune the benefit between coverage of users and the length of their trajectories. We prove the NP-hardness of the problem and propose greedy-based heuristics to address it. To improve the efficiency of the bike lane planning system for the urban planner, we propose a novel trajectory indexing structure and deploy the system based on a parallel computing framework (Storm) to improve the system's efficiency. Finally, extensive experiments and case studies are provided to demonstrate the system efficiency and effectiveness.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1529	1542		10.1109/TKDE.2019.2907091													
J								Model-Based Synthetic Sampling for Imbalanced Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Data models; Machine learning; Training; Sampling methods; Manufacturing; Kernel; Data mining; Imbalanced data; over-sampling; synthetic sampling; model-based approach	MACHINE; SMOTE; CLASSIFICATION; DIAGNOSIS	Imbalanced data is characterized by the severe difference in observation frequency between classes and has received a lot of attention in data mining research. The prediction performances usually deteriorate as classifiers learn from imbalanced data, as most classifiers assume the class distribution is balanced or the costs for different types of classification errors are equal. Although several methods have been devised to deal with imbalance problems, it is still difficult to generalize those methods to achieve stable improvement in most cases. In this study, we propose a novel framework called model-based synthetic sampling (MBS) to cope with imbalance problems, in which we integrate modeling and sampling techniques to generate synthetic data. The key idea behind the proposed method is to use regression models to capture the relationship between features and to consider data diversity in the process of data generation. We conduct experiments on 13 datasets and compare the proposed method with 10 methods. The experimental results indicate that the proposed method is not only comparative but also stable. We also provide detailed investigations and visualizations of the proposed method to empirically demonstrate why it could generate good data samples.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1543	1556		10.1109/TKDE.2019.2905559													
J								Multi-Party High-Dimensional Data Publishing Under Differential Privacy	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Differential privacy; multiple parties; data publishing; high-dimensional data	RELEASE	In this paper, we study the problem of publishing high-dimensional data in a distributed multi-party environment under differential privacy. In particular, with the assistance of a semi-trusted curator, the parties (i.e., local data owners) collectively generate a synthetic integrated dataset while satisfying r-differential privacy. To solve this problem, we present a differentially private sequential update of Bayesian network (DP-SUBN) approach. In DP-SUBN, the parties and the curator collaboratively identify the Bayesian network N that best fits the integrated dataset in a sequential manner, from which a synthetic dataset can then be generated. The fundamental advantage of adopting the sequential update manner is that the parties can treat the intermediate results provided by previous parties as their prior knowledge to direct how to team N. The core of DP-SUBN is the construction of the search frontier, which can be seen as a priori knowledge to guide the parties to update N. By exploiting the correlations of attribute pairs, we propose exact and heuristic methods to construct the search frontier. In particular, to privately quantify the correlations of attribute pairs without introducing too much noise, we first put forward a non-overlapping covering design (NOCD) method, and then devise a dynamic programming method for determining the optimal parameters used in NOCD. Through privacy analysis, we show that DP-SUBN satisfies epsilon-differential privacy. Extensive experiments on real datasets demonstrate that DP-SUBN offers desirable data utility with low communication cost.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1557	1571		10.1109/TKDE.2019.2906610													
J								Privacy-Preserving User Profile Matching in Social Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Encryption; Servers; Social networking (online); Databases; Privacy; User profile matching; data privacy protection; ElGamal encryption; Paillier encryption; homomorphic encryption	SET INTERSECTION; EFFICIENT PROTOCOLS; COMPUTATION; CARDINALITY; SECURITY	In this paper, we consider a scenario where a user queries a user profile database, maintained by a social networking service provider, to identify users whose profiles match the profile specified by the querying user. A typical example of this application is online dating. Most recently, an online dating website, Ashley Madison, was hacked, which resulted in a disclosure of a large number of dating user profiles. This data breach has urged researchers to explore practical privacy protection for user profiles in a social network. In this paper, we propose a privacy-preserving solution for profile matching in social networks by using multiple servers. Our solution is built on homomorphic encryption and allows a user to find out matching users with the help of multiple servers without revealing to anyone the query and the queried user profiles in clear. Our solution achieves user profile privacy and user query privacy as long as at least one of the multiple servers is honest. Our experiments demonstrate that our solution is practical.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1572	1585		10.1109/TKDE.2019.2912748													
J								Reliable Accuracy Estimates from k-Fold Cross Validation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Dependency relationship; k-fold cross validation; reliable estimate; replication; variance		It is popular to evaluate the performance of classification algorithms by k-fold cross validation. A reliable accuracy estimate will have a relatively small variance, and several studies therefore suggested to repeatedly perform k-fold cross validation. Most of them did not consider the correlation among the replications of k-fold cross validation, and hence the variance could be underestimated. The purpose of this study is to explore whether k-fold cross validation should be repeatedly performed for obtaining reliable accuracy estimates. The dependency relationships between the predictions of the same instance in two replications of k-fold cross validation are first analyzed for k-nearest neighbors with k = 1. Then, statistical methods are proposed to test the strength of the dependency level between the accuracy estimates resulting from two replications of k-fold cross validation. The experimental results on 20 data sets show that the accuracy estimates obtained from various replications of k-fold cross validation are generally highly correlated, and the correlation will be higher as the number of folds increases. The k-fold cross validation with a large number of folds and a small number of replications should be adopted for performance evaluation of classification algorithms.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1586	1594		10.1109/TKDE.2019.2912815													
J								ROAM: A Fundamental Routing Query on Road Networks with Efficiency	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Road networks; query performance; indexing; graph algorithms	HIERARCHIES	Novel road-network applications often recommend a moving object (e.g., a vehicle) about interesting services or tasks on its way to a destination. A taxi-sharing system, for instance, suggests a new passenger to a taxi while it is serving another one. The traveling cost is then shared among these passengers. A fundamental query is: given two nodes s and t, and an area A on road network graph g, is there a "good" route (e.g., short enough path) P from s tot that crosses A in g? In a taxi-sharing system, s and t can be a taxi's current and destined locations, and A contains all the places to which a person waiting for a taxi is willing to walk. Answering this Route and Area Matching (ROAM) Query allows the application involved to recommend appropriate services to users efficiently. In this paper, we examine efficient ROAM query algorithms. Particularly, we develop solutions for finding a rho-route, which is an s-t. path that passes A, with a length of at most (1 + rho) times the shortest distance between s and t. The existence of a rho-route implies that a service or task located at A can be found for a given moving object m, and that m only deviates slightly from its current route. We present comprehensive studies on index-free and index-based algorithms for answering ROAM queries. Comprehensive experiments show that our algorithm runs up to 30 times faster than baseline algorithms.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1595	1609		10.1109/TKDE.2019.2906188													
J								Robust Multi-Label Learning with PRO Loss	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Multi label learning; learning criterion; partial labels; PRO Loss; ProSVM		Multi-label learning methods assign multiple labels to one object. In practice, in addition to differentiating relevant labels from irrelevant ones, it is often desired to rank relevant labels for an object, whereas the ranking of irrelevant labels is not important. Thus, we require an algorithm to do classification and ranking of relevant labels simultaneously. Such a requirement, however, cannot be met because most existing methods were designed to optimize existing criteria, yet there is no criterion which encodes the aforementioned requirement. In this paper, we present a new criterion, PRO Loss, concerning the prediction of all labels as well as the ranking of only relevant labels. We then propose ProSVM which optimizes PRO Loss efficiently using alternating direction method of multipliers. We further improve its efficiency with an upper approximation that reduces the number of constraints from O(T-2) to O(T), where T is the number of labels. We then notice that in real applications, it is difficult to get full supervised information for multi-label data. To make the proposed algorithm more robust to supervised information, we adapt ProSVM to deal with the multi-label learning with partial labels problem. Experiments show that our proposal is not only superior on PRO Loss, but also highly competitive on existing evaluation criteria.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1610	1624		10.1109/TKDE.2019.2908898													
J								T-PCCE: Twitter Personality based Communicative Communities Extraction System for Big Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Twitter; Data mining; Big Data; Detection algorithms; Feature extraction; Facebook; Classification; cloud computing; communicative community detection; MapReduce; personality mining; social media analytics		The identification of social media communities has recently been of major concern, since users participating in such communities can contribute to viral marketing campaigns. In this work, we focus on users' communication considering personality as a key characteristic for identifying communicative networks i.e., networks with high information flows. We describe the Twitter Personality based Communicative Communities Extraction (T-PCCE) system that identifies the most communicative communities in a Twitter network graph considering users' personality. We then expand existing approaches in users' personality extraction by aggregating data that represent several aspects of user behavior using machine learning techniques. We use an existing modularity based community detection algorithm and we extend it by inserting a post-processing step that eliminates graph edges based on users' personality. The effectiveness of our approach is demonstrated by sampling the Twitter graph and comparing the communication strength of the extracted communities with and without considering the personality factor. We define several metrics to count the strength of communication within each community. Our algorithmic framework and the subsequent implementation employ the cloud infrastructure and use the MapReduce Programming Environment. Our results show that the T-PCCE system creates the most communicative communities.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1625	1638		10.1109/TKDE.2019.2906197													
J								Translation-Based Sequential Recommendation for Complex Users on Sparse Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Recommender systems; Task analysis; Data models; Bayes methods; Training; Recurrent neural networks; Sequential recommendation; sequential behavior; recommender systems		Sequential recommendation is one of the main tasks in recommender systems, where the next action (e.g., purchase, visit, and click) of the user is predicted based on his/her past sequence of actions. Translating Embeddings is a knowledge graph completion approach which was recently adapted to a translation-based sequential recommendation (TransRec) method. We observe a flaw of TransRec when handling complex translations, which hinders it from generating accurate suggestions. In view of this, we propose a translation-based recommender for complex users (CTransRec), which utilizes category-specific projection and temporal dynamic relaxation. Using our proposed Margin-based Pairwise Bayesian Personalized Ranking and Time-Aware Negative Sampling, CTransRec outperforms state-of-the-art methods for sequential recommendation on extremely sparse data. The superiority of CTransRec, which is confirmed by our extensive experiments on both public data and real data obtained from the industry, comes from not only the additional information used in training but also the fact that CTransRec makes good use of this additional information to model the complex translations.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1639	1651		10.1109/TKDE.2019.2906180													
J								Voice of Charity: Prospecting the Donation Recurrence & Donor Retention in Crowdfunding	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Predictive models; Task analysis; Deep learning; Optimization; Data models; Analytical models; Collaboration; Crowdfunding; donor retention; survival analysis; ranking constraints; deep learning	RECOMMENDATION; PERSPECTIVE; RISK	Online donation-based crowdfunding has brought new life to charity by soliciting small monetary contributions from crowd donors to help others in trouble or with dreams. However, a crucial issue for crowdfunding platforms as well as traditional charities is the problem of high donor attrition, i.e., many donors donate only once or very few times within a rather short lifecycle and then leave. Thus, it is an urgent task to analyze the factors of and then further predict the donors behaviors. Especially, we focus on two types of behavioral events, e.g., donation recurrence (whether one donor will make donations at some time slices in the future) and donor retention (whether she will remain on the crowdfunding platform until a future time). However, this problem has not been well explored due to many domain and technical challenges, such as the heterogeneous influence, the relevance of the two types of events, and the censoring phenomenon of retention records. In this paper, we present a focused study on donation recurrence and donor retention with the help of large-scale behavioral data collected from crowdfunding. Specifically, we propose a Joint Deep Survival model, i.e., JDS, which can integrate heterogeneous features, e.g., donor motives, projects recently donated to, social contacts, to jointly model the donation recurrence and donor retention since these two types of behavioral events are highly relevant. In addition, we model the censoring phenomenon and dependence relations of different behaviors from the survival analysis view by designing multiple innovative constraints and incorporating them into the objective functions. Finally, we conduct extensive analysis and validation experiments with large-scale data collected from Kiva.org. The experimental results clearly demonstrate the effectiveness of our proposed models for analyzing and predicting the donation recurrence and donor retention in crowdfunding.																	1041-4347	1558-2191				AUG. 1	2020	32	8					1652	1665		10.1109/TKDE.2019.2906199													
J								A Continuation Method for Graph Matching Based Feature Correspondence	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Feature correspondence; graph matching; continuous method; continuation method; combinatorial optimization	OPTIMIZATION	Feature correspondence lays the foundation for many computer vision and image processing tasks, which can be well formulated and solved by graph matching. Because of the high complexity, approximate methods are necessary for graph matching, and the continuous relaxation provides an efficient approximate scheme. But there are still many problems to be settled, such as the highly nonconvex objective function, the ignorance of the combinatorial nature of graph matching in the optimization process, and few attention to the outlier problem. Focusing on these problems, this paper introduces a continuation method directly targeting at the combinatorial optimization problem associated with graph matching. Specifically, first a regularization function incorporating the original objective function and the discrete constraints is proposed. Then a continuation method based on Gaussian smoothing is applied to it, in which the closed forms of relevant functions with respect to the outlier distribution are deduced. Experiments on both synthetic data and real world images validate the effectiveness of the proposed method.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1809	1822		10.1109/TPAMI.2019.2903483													
J								A Curriculum Domain Adaptation Approach to the Semantic Segmentation of Urban Scenes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Semantics; Image segmentation; Task analysis; Adaptation models; Neural networks; Training; Buildings; Domain adaptation; semantic segmentation; curriculum learning; curriculum domain adaptation; deep learning; self-driving	OBJECT CLASSES; KERNEL	During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is one of the core tasks in many applications such as autonomous driving and augmented reality. However, to train CNNs requires a considerable amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic imagery with computer-generated annotations. Despite this, the domain mismatch between real images and the synthetic data hinders the models' performance. Hence, we propose a curriculum-style learning approach to minimizing the domain gap in urban scene semantic segmentation. The curriculum domain adaptation solves easy tasks first to infer necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train a segmentation network, while regularizing its predictions in the target domain to follow those inferred properties. In experiments, our method outperforms the baselines on two datasets and three backbone networks. We also report extensive ablation studies about our approach.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1823	1841		10.1109/TPAMI.2019.2903401													
J								A Simple and Fast Algorithm for L1-Norm Kernel PCA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Principal component analysis; Kernel; Matrix decomposition; Convergence; Anomaly detection; Loading; Sparse matrices; Principal component analysis; L1-norm; kernel; outlier detection	ROBUST PCA	We present an algorithm for L1-norm kernel PCA and provide a convergence analysis for it. While an optimal solution of L2-norm kernel PCA can be obtained through matrix decomposition, finding that of L1-norm kernel PCA is not trivial due to its non-convexity and non-smoothness. We provide a novel reformulation through which an equivalent, geometrically interpretable problem is obtained. Based on the geometric interpretation of the reformulated problem, we present a "fixed-point" type algorithm that iteratively computes a binary weight for each observation. As the algorithm requires only inner products of data vectors, it is computationally efficient and the kernel trick is applicable. In the convergence analysis, we show that the algorithm converges to a local optimal solution in a finite number of steps. Moreover, we provide a rate of convergence analysis, which has been never done for any L1-norm PCA algorithm, proving that the sequence of objective values converges at a linear rate. In numerical experiments, we show that the algorithm is robust in the presence of entry-wise perturbations and computationally scalable, especially in a large-scale setting. Lastly, we introduce an application to outlier detection where the model based on the proposed algorithm outperforms the benchmark algorithms.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1842	1855		10.1109/TPAMI.2019.2903505													
J								Back to the Future: Radial Basis Function Network Revisited	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Supervised learning; radial basis function networks; k-means	APPROXIMATION; ALGORITHM	Radial Basis Function (RBF) networks are a classical family of algorithms for supervised learning. The most popular approach for training RBF networks has relied on kernel methods using regularization based on a norm in a Reproducing Kernel Hilbert Space (RKHS), which is a principled and empirically successful framework. In this paper we aim to revisit some of the older approaches to training the RBF networks from a more modern perspective. Specifically, we analyze two common regularization procedures, one based on the square norm of the coefficients in the network and another one using centers obtained by k-means clustering. We show that both of these RBF methods can be recast as certain data-dependent kernels. We provide a theoretical analysis of these methods as well as a number of experimental results, pointing out very competitive experimental performance as well as certain advantages over the standard kernel methods in terms of both flexibility (incorporating of unlabeled data) and computational complexity. Finally, our results shed light on some impressive recent successes of using soft k-means features for image recognition and other tasks.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1856	1867		10.1109/TPAMI.2019.2906594													
J								Contactless Biometric Identification Using 3D Finger Knuckle Patterns	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Two dimensional displays; Thumb; Fingerprint recognition; Databases; Feature extraction; Biometrics; finger knuckle identification; 3d finger dorsal matching; contactless hand identification	INDIVIDUALITY; RECOGNITION; SURFACE; SHAPE	Study on finger knuckle patterns has attracted increasing attention for the automated biometric identification. However, finger knuckle pattern is essentially a 3D biometric identifier and the usage or availability of only 2D finger knuckle databases in the literature is the key limitation to avail full potential from this biometric identifier. This paper therefore introduces (first) contactless 3D finger knuckle database in public domain, which is acquired from 130 different subjects in two-session imaging using photometric stereo approach. This paper investigates on the 3D information from the finger knuckle patterns and introduces a new feature descriptor to extract discriminative 3D features for more accurate 3D finger knuckle matching. An individuality model for the proposed feature descriptor is also presented. Comparative experimental results using the state-of-the-art feature extraction methods on this challenging 3D finger knuckle database validate the effectiveness of our approach. Although our feature descriptor is designed for 3D finger knuckle patterns, it is also attractive for other hand-based biometric identifiers with similar patterns such as the palmprint and fingerprint. This observation is validated from the outperforming results, using the state-of-the-art pixel-wise 3D palmprint and 3D fingerprint feature descriptors, on other publicly available datasets.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1868	1883		10.1109/TPAMI.2019.2904232													
J								Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Feature extraction; Image edge detection; Streaming media; Semantics; Clutter; Image restoration; Deep learning; Defocus blur detection; multi-stream bottom-top-bottom network; cascaded DBD map residual learning	MAP ESTIMATION; IMAGE	Defocus blur detection (DBD) is aimed to estimate the probability of each pixel being in-focus or out-of-focus. This process has been paid considerable attention due to its remarkable potential applications. Accurate differentiation of homogeneous regions and detection of low-contrast focal regions, as well as suppression of background clutter, are challenges associated with DBD. To address these issues, we propose a multi-stream bottom-top-bottom fully convolutional network (BTBNet), which is the first attempt to develop an end-to-end deep network to solve the DBD problems. First, we develop a fully convolutional BTBNet to gradually integrate nearby feature levels of bottom to top and top to bottom. Then, considering that the degree of defocus blur is sensitive to scales, we propose multi-stream BTBNets that handle input images with different scales to improve the performance of DBD. Finally, a cascaded DBD map residual learning architecture is designed to gradually restore finer structures from the small scale to the large scale. To promote further study and evaluation of the DBD models, we construct a new database of 1100 challenging images and their pixel-wise defocus blur annotations. Experimental results on the existing and our new datasets demonstrate that the proposed method achieves significantly better performance than other state-of-the-art algorithms.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1884	1897		10.1109/TPAMI.2019.2906588													
J								Generalized Feedback Loop for Joint Hand-Object Pose Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Pose estimation; Solid modeling; Optimization; Feedback loop; Training data; Data models; 3D hand pose estimation; 3D object pose estimation; feedback loop; hand-object manipulation	MOTION	We propose an approach to estimating the 3D pose of a hand, possibly handling an object, given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. This approach can be generalized to a hand interacting with an object. Therefore, we jointly estimate the 3D pose of the hand and the 3D pose of the object. Our approach performs en-par with state-of-the-art methods for 3D hand pose estimation, and outperforms state-of-the-art methods for joint hand-object pose estimation when using depth images only. Also, our approach is efficient as our implementation runs in real-time on a single GPU.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1898	1912		10.1109/TPAMI.2019.2907951													
J								Inferring Salient Objects from Human Fixations	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Visualization; Object detection; Task analysis; Predictive models; Deep learning; Biological system modeling; Measurement; Image saliency; salient object detection; fixation prediction; deep learning	VISUAL-ATTENTION; MODEL; OPTIMIZATION; PREDICT; SCENE	Previous research in visual saliency has been focused on two major types of models namely fixation prediction and salient object detection. The relationship between the two, however, has been less explored. In this work, we propose to employ the former model type to identify salient objects. We build a novel Attentive Saliency Network (ASNet)(1) 1. Available at: https://github.com/wenguanwang/ASNet. that learns to detect salient objects from fixations. The fixation map, derived at the upper network layers, mimics human visual attention mechanisms and captures a high-level understanding of the scene from a global view. Salient object detection is then viewed as fine-grained object-level saliency segmentation and is progressively optimized with the guidance of the fixation map in a top-down manner. ASNet is based on a hierarchy of convLSTMs that offers an efficient recurrent mechanism to sequentially refine the saliency features over multiple steps. Several loss functions, derived from existing saliency evaluation metrics, are incorporated to further boost the performance. Extensive experiments on several challenging datasets show that our ASNet outperforms existing methods and is capable of generating accurate segmentation maps with the help of the computed fixation prior. Our work offers a deeper insight into the mechanisms of attention and narrows the gap between salient object detection and fixation prediction.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1913	1927		10.1109/TPAMI.2019.2905607													
J								Learning of Gaussian Processes in Distributed and Communication Limited Systems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Kernel; Training; Machine learning; Gaussian processes; Task analysis; Optimization; Distortion; Distributed learning; Gaussian processes; communication constraints; vector quantization		It is of fundamental importance to find algorithms obtaining optimal performance for learning of statistical models in distributed and communication limited systems. Aiming at characterizing the optimal strategies, we consider learning of Gaussian Processes (GP) in distributed systems as a pivotal example. We first address a very basic problem: how many bits are required to estimate the inner-products of some Gaussian vectors across distributed machines? Using information theoretic bounds, we obtain an optimal solution for the problem which is based on vector quantization. Two suboptimal and more practical schemes are also presented as substitutes for the vector quantization scheme. In particular, it is shown that the performance of one of the practical schemes which is called per-symbol quantization is very close to the optimal one. Schemes provided for the inner-product calculations are incorporated into our proposed distributed learning methods for GPs. Experimental results show that with spending few bits per symbol in our communication scheme, our proposed methods outperform previous zero rate distributed GP learning schemes such as Bayesian Committee Model (BCM) and Product of experts (PoE).																	0162-8828	1939-3539				AUG. 1	2020	42	8					1928	1941		10.1109/TPAMI.2019.2906207													
J								Learning to Index for Nearest Neighbor Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Indexing; Artificial neural networks; Vector quantization; Hash functions; Binary codes; Approximate nearest neighbor; asymmetric distance computation; cluster ranking and pruning; hash-based indexing; product quantization; residual vector quantization	OPTIMIZED PRODUCT QUANTIZATION; HASH; CODES	In this study, we present a novel ranking model based on learning neighborhood relationships embedded in the index space. Given a query point, conventional approximate nearest neighbor search calculates the distances to the cluster centroids, before ranking the clusters from near to far based on the distances. The data indexed in the top-ranked clusters are retrieved and treated as the nearest neighbor candidates for the query. However, the loss of quantization between the data and cluster centroids will inevitably harm the search accuracy. To address this problem, the proposed model ranks clusters based on their nearest neighbor probabilities rather than the query-centroid distances. The nearest neighbor probabilities are estimated by employing neural networks to characterize the neighborhood relationships, i.e., the density function of nearest neighbors with respect to the query. The proposed probability-based ranking can replace the conventional distance-based ranking for finding candidate clusters, and the predicted probability can be used to determine the data quantity to be retrieved from the candidate cluster. Our experimental results demonstrated that the proposed ranking model could boost the search performance effectively in billion-scale datasets.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1942	1956		10.1109/TPAMI.2019.2907086													
J								Motion-Guided Cascaded Refinement Network for Video Object Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Motion segmentation; Active contours; Optical imaging; Image segmentation; Optical propagation; Task analysis; Object segmentation; Video object segmentation; motion segmentation; convolutional neural networks; spatial-temporal embedding	ACTIVE CONTOURS; EVOLUTION; TRACKING	In this work, we propose a motion-guided cascaded refinement network for video object segmentation. By assuming the foreground objects show different motion patterns from the background, for each video frame we apply an active contour model on optical flow to coarsely segment the foreground. The proposed Cascaded Refinement Network (CRN) then takes as guidance the coarse segmentation to generate an accurate segmentation in full resolution. In this way, the motion information and the deep CNNs can complement each other well to accurately segment the foreground objects from video frames. To deal with multi-instance cases, we extend our method with a spatial-temporal instance embedding model that further segments the foreground regions into instances and propagates instance labels. We further introduce a single-channel residual attention module in CRN to incorporate the coarse segmentation map as attention, which makes the network effective and efficient in both training and testing. We perform experiments on popular benchmarks and the results show that our method achieves state-of-the-art performance with high time efficiency.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1957	1967		10.1109/TPAMI.2019.2906175													
J								On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Mathematical model; Convolution; Convolutional codes; Iterative algorithms; Dictionaries; Analytical models; Multi-layer convolutional sparse coding; network unfolding; recurrent neural networks; iterative shrinkage algorithms	LINEAR INVERSE PROBLEMS; THRESHOLDING ALGORITHM; SPARSE REPRESENTATION; DICTIONARIES; SHRINKAGE; RECOVERY	Parsimonious representations are ubiquitous in modeling and processing information. Motivated by the recent Multi-Layer Convolutional Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis Pursuit problem to a multi-layer setting, introducing similar sparse enforcing penalties at different representation layers in a symbiotic relation between synthesis and analysis sparse priors. We explore different iterative methods to solve this new problem in practice, and we propose a new Multi-Layer Iterative Soft Thresholding Algorithm (ML-ISTA), as well as a fast version (ML-FISTA). We show that these nested first order algorithms converge, in the sense that the function value of near-fixed points can get arbitrarily close to the solution of the original problem. We further show how these algorithms effectively implement particular recurrent convolutional neural networks (CNNs) that generalize feed-forward ones without introducing any parameters. We present and analyze different architectures resulting from unfolding the iterations of the proposed pursuit algorithms, including a new Learned ML-ISTA, providing a principled way to construct deep recurrent CNNs. Unlike other similar constructions, these architectures unfold a global pursuit holistically for the entire network. We demonstrate the emerging constructions in a supervised learning setting, consistently improving the performance of classical CNNs while maintaining the number of parameters constant.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1968	1980		10.1109/TPAMI.2019.2904255													
J								Recognizing Material Properties from Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Visualization; Image recognition; Material properties; Shape; Face; Visual perception; Semantics; Visual material attributes; human material perception; material recognition	REPRESENTATION; OBJECTS	Humans implicitly rely on the properties of materials to guide our interactions. Grasping smooth materials, for example, requires more care than rough ones. We may even visually infer non-visual properties (e.g., softness is a physical material property). We refer to visually-recognizable material properties as visual material attributes. Recognizing these attributes in images can provide valuable information for scene understanding and material recognition. Unlike typical object and scene attributes, however, visual material attributes are local (i.e., "fuzziness" does not have a shape). Given full supervision, we may accurately recognize such attributes from purely local information (small image patches). Obtaining consistent full supervision at scale, however, is challenging. To solve this problem, we probe the human visual perception of materials. By asking simple yes/no questions comparing pairs of image patches, we obtain the weak supervision required to build a set of classifiers for attributes that, while unnamed, function similarly to the attributes with which we describe materials. Furthermore, we integrate this method in the end-to-end learning of a CNN that simultaneously recognizes materials and their visual attributes. Experiments show that visual material attributes serve as both a useful representation for known material categories and as a basis for transfer learning.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1981	1995		10.1109/TPAMI.2019.2907850													
J								ROAM: A Rich Object Appearance Model with Application to Rotoscoping	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Adaptation models; Tools; Shape; Task analysis; Pipelines; Labeling; Deformable models; Rotoscoping; trimaps; video segmentation; video alpha-matting; probabilistic graphical models; dynamic programming	VIDEO; TRACKING; IMAGE	Rotoscoping, the detailed delineation of scene elements through a video shot, is a painstaking task of tremendous importance in professional post-production pipelines. While pixel-wise segmentation techniques can help for this task, professional rotoscoping tools rely on parametric curves that offer the artists a much better interactive control on the definition, editing and manipulation of the segments of interest. Sticking to this prevalent rotoscoping paradigm, we propose a novel framework to capture and track the visual aspect of an arbitrary object in a scene, given an initial closed outline of this object. This model combines a collection of local foreground/background appearance models spread along the outline, a global appearance model of the enclosed object and a set of distinctive foreground landmarks. The structure of this rich appearance model allows simple initialization, efficient iterative optimization with exact minimization at each step, and on-line adaptation in videos. We further extend this model by so-called trimaps which serve as an input to alpha-matting algorithms to allow truly seamless compositing. To this end, we leverage local classifiers attached to the roto-curves to define a confidence measure that is well-suited to define trimaps with adaptive band-widths. The resulting trimaps are parametric, temporally consistent and remain fully editable by the artist. We demonstrate qualitatively and quantitatively the merit of this framework through comparisons with tools based on either dynamic segmentation with a closed curve or pixel-wise binary labelling.																	0162-8828	1939-3539				AUG. 1	2020	42	8					1996	2010		10.1109/TPAMI.2019.2904963													
J								Tensor Graphical Model: Non-Convex Optimization and Statistical Inference	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Graphical models; Estimation; Convergence; Testing; Sparse matrices; Covariance matrices; Asymptotic normality; hypothesis testing; optimality; rate of convergence	COVARIANCE ESTIMATION; CONFIDENCE-INTERVALS; VARIABLE SELECTION; ADAPTIVE LASSO; CONVERGENCE; LIKELIHOOD; TESTS; RATES; CONNECTIVITY; REGIONS	We consider the estimation and inference of graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. A critical challenge in the estimation and inference of this model is the fact that its penalized maximum likelihood estimation involves minimizing a non-convex objective function. To address it, this paper makes two contributions: (i) In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with an optimal statistical rate of convergence. (ii) We propose a de-biased statistical inference procedure for testing hypotheses on the true support of the sparse precision matrices, and employ it for testing a growing number of hypothesis with false discovery rate (FDR) control. The asymptotic normality of our test statistic and the consistency of FDR control procedure are established. Our theoretical results are backed up by thorough numerical studies and our real applications on neuroimaging studies of Autism spectrum disorder and users' advertising click analysis bring new scientific findings and business insights. The proposed methods are encoded into a publicly available R package Tlasso.																	0162-8828	1939-3539				AUG. 1	2020	42	8					2024	2037		10.1109/TPAMI.2019.2907679													
J								Towards Efficient U-Nets: A Coupled and Quantized Approach	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Stacked U-nets; dense connectivity; network quantization; efficient Al; human pose estimation; face alignment		In this paper, we propose to couple stacked U-Nets for efficient visual landmark localization. The key idea is to globally reuse features of the same semantic meanings across the stacked U-Nets. The feature reuse makes each U-Net light-weighted. Specially, we propose an order-K coupling design to trim off long-distance shortcuts, together with an iterative refinement and memory sharing mechanism. To further improve the efficiency, we quantize the parameters, intermediate features, and gradients of the coupled U-Nets to low bit-width numbers. We validate our approach in two tasks: human pose estimation and facial landmark localization. The results show that our approach achieves state-of-the-art localization accuracy but using similar to 70% fewer parameters, similar to 30% less inference time, similar to 98% less model size, and saving similar to 75% training memory compared with benchmark localizers.																	0162-8828	1939-3539				AUG. 1	2020	42	8					2038	2050		10.1109/TPAMI.2019.2907634													
J								Unsupervised Learning of a Hierarchical Spiking Neural Network for Optical Flow Estimation: From Events to Global Motion Perception	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Neurons; Visualization; Biomedical optical imaging; Optical sensors; Biological system modeling; Biological information theory; Vision sensors; Event-based vision; feature extraction; motion detection; neural nets; neuromorphic computing; unsupervised learning	TIMING-DEPENDENT PLASTICITY; CIRCUIT; STDP; FLY	The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.																	0162-8828	1939-3539				AUG. 1	2020	42	8					2051	2064		10.1109/TPAMI.2019.2903179													
J								GBKOM: A generic framework for BK-based ontology matching	JOURNAL OF WEB SEMANTICS										Ontology matching; Ontology alignment; Background knowledge; Indirect matching; External resource; Anchoring; Derivation; Background knowledge selection	MODEL	BK-based matching exploits external background knowledge resources (BK) to fill the semantic gap between the ontologies to align. Existing BK-based matchers implement the indirect matching approach in their internal architecture, which makes any adaptation or reuse of the code difficult. Indeed, to improve a particular step in the BK-based matching process, it is necessary to code the whole process from scratch which requires a lot of time and effort. To overcome this issue, we propose a flexible framework called Generic BK-based Matcher (GBKOM). GBKOM is an extension that can be added to any existing matcher. It is a configurable framework that implements the BK-based matching process, with a rich set of parameters making it customizable and suitable for performing experimental evaluations. GBKOM has participated, with YAM++ as a direct matcher, in the OAEI 2017 and OAEI 2017.5 campaigns, where it has been successful on the biomedical benchmarks, and top-ranked in several tasks. Furthermore, we have performed experiments with two other direct matchers (i.e., LogMap and LogMapLite) to show that the effectiveness of GBKOM is independent of the direct matcher used. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				AUG	2020	63								100563	10.1016/j.websem.2020.100563													
J								The enslaved ontology: Peoples of the historic slave trade	JOURNAL OF WEB SEMANTICS										Digital humanities; Modular ontology; Data integration; Ontology design patterns; History of the slave trade	DESIGN PATTERNS	We present the Enslaved Ontology (V1.0) which was developed for integrating data about the historic slave trade from diverse sources in a use case driven by historians. Ontology development followed modular ontology design principles as derived from ontology design pattern application best practices and the eXtreme Design Methodology. Ontology content focuses on data about historic persons and the event records from which this data can be taken. It also incorporates provenance modeling and some temporal and spatial aspects. The ontology is available as serialized in the Web Ontology Language OWL, and carries modularization annotations using the Ontology Pattern Language (OPLa). It is available under the Creative Commons CC BY 4.0 license. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				AUG	2020	63								100567	10.1016/j.websem.2020.100567													
J								Chinese semantic document classification based on strategies of semantic similarity computation and correlation analysis	JOURNAL OF WEB SEMANTICS										Semantic document classification; Semantic similarity; Semantic embedding; Correlation analysis; Artificial intelligence	ONTOLOGY-DRIVEN	Document classification has become an indispensable technology to realize intelligent information services. This technique is often applied to the tasks such as document organization, analysis, and archiving or implemented as a submodule to support high-level applications. It has been shown that semantic analysis can improve the performance of document classification. Although this has been incorporated in previous automatic document classification methods, with an increase in the number of documents stored online, the use of semantic information for document classification has attracted greater attention as it can greatly reduce human effort. In this present paper, we propose two semantic document classification strategies for two types of semantic problems: (1) a novel semantic similarity computation (SSC) method to solve the polysemy problem and (2) a strong correlation analysis method (SCM) to solve the synonym problem. Experimental results indicate that compared with traditional machine learning, n-gram, and contextualized word embedding methods, the efficient semantic similarity and correlation analysis allow eliminating word ambiguity and extracting useful features to improve the accuracy of semantic document classification for texts in Chinese. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				AUG	2020	63								100578	10.1016/j.websem.2020.100578													
J								Examining the determinants of the count of customer reviews in peer-to-peer home-sharing platforms using clustering and count regression techniques	DECISION SUPPORT SYSTEMS										Sharing economy; Airbnb; Poisson; Quasi-Poisson; Negative Binomial; Text analytics	VALUE CO-CREATION; AIRBNB HOST; HOTEL; ATTRIBUTES; TRUST; PERSPECTIVES; REPUTATION; SUPERHOST; BEHAVIOR; QUALITY	The sharing economy has experienced massive growth in the short-term shared-home rental industry. However, few studies have investigated the determinants of the number of customer reviews received by these shared-homes. To fill this gap, we were motivated to propose an analytical framework that identified these determinants, both explicit and implicit. We applied Poisson, Quasi-Poisson, and Negative Binomial regressions with a dataset consisting of Airbnb properties from ten different cities worldwide, while successful bookings were proxied by the count of customer reviews posted by guests. We performed a cluster analysis based on the properties to generate homogeneous "cluster cities" and performed the regressions separately for each cluster. Among host-generated features, superhost, host duration, bedrooms, and amenities became significant. Among user-generated features, overall review scores and negative sentiments were significant. We also found that the "superhost" badge moderated the effects of host-generated content on the count of customer reviews. Consequently, guests paid a higher "price per night" for "superhost" properties, while they overlooked crucial attributes such as "website features." Through these novel "cluster-specific" recommendations, our study extends the existing theories and contributes to the literature of decision analytics and tourism management. Finally, we performed a sensitivity analysis to check for the timeliness and robustness of these determinants.																	0167-9236	1873-5797				AUG	2020	135								113324	10.1016/j.dss.2020.113324													
J								Predicting student dropout in subscription-based online learning environments: The beneficial impact of the logit leaf model	DECISION SUPPORT SYSTEMS										Learning analytics; Proactive student management; Subscription-based online learning; Student dropout; Logit leaf model; Machine learning	CUSTOMER CHURN PREDICTION; LOGISTIC-REGRESSION; CLASSIFICATION; PERFORMANCE; RETENTION; DECISION	Online learning has been adopted rapidly by educational institutions and organizations. Despite its many advantages, including 24/7 access, high flexibility, rich content, and low cost, online learning suffers from high dropout rates that hamper pedagogical and economic goal outcomes. Enhanced student dropout prediction tools would help providers proactively detect students at risk of leaving and identify factors that they might address to help students continue their learning experience. Therefore, this study seeks to improve student dropout predictions, with three main contributions. First, it benchmarks a recently proposed logit leaf model (LLM) algorithm against eight other algorithms, using a real-life data set of 10,554 students of a global subscription-based online learning provider. The LLM outperforms all other methods in finding a balance between predictive performance and comprehensibility. Second, a new multilevel informative visualization of the LLM adds novel benefits, relative to a standard LLM visualization. Third, this research specifies the impacts of student demographics; classroom characteristics; and academic, cognitive, and behavioral engagement variables on student dropout. In reviewing LLM segments, these results show that different insights emerge for various student segments with different learning patterns. This notable result can be used to personalize student retention campaigns.																	0167-9236	1873-5797				AUG	2020	135								113325	10.1016/j.dss.2020.113325													
J								Network projection-based edge classification framework for signed networks	DECISION SUPPORT SYSTEMS										Signed network; Edge classification; Spanning subgraph projection	BALANCE	Many real-world networks have signed relationships between the nodes. Identification of these relationships is an important aspect of decision making. The existing signed relationships in a network may impact the relationships between the other nodes, hence learning from the existing signed relationships in a network can be used for decision making in various mining tasks. These signed networks are getting attention in recent years due to their relevance to many applications such as categorization, recommendation, and relationship discovery in various domains for decision support such as biological, social network analysis, communication and making knowledge graphs. In this work, we focus on edge classification (sign/label prediction for edges) in unweighted and undirected signed networks where the task is to predict the label of the unlabeled edges. Edge classification is a challenging problem as in real-world signed networks, edges are scarcely labeled. In our work, we are using labeled edges to predict the sign of unlabeled edges (classification) with the help of structural information. In this work, we have proposed a novel framework named NPECF for the classification of unlabeled edges. The proposed framework is novel in its way of utilizing the existing information in the signed network to predict the label of unlabeled edges. The utilization of the unlabeled edges in NPECF using three spanning subgraph projections of the given network minimizes the information loss. The experiments have been performed on four realworld datasets from different domains to demonstrate the effectiveness of the proposed framework.																	0167-9236	1873-5797				AUG	2020	135								113321	10.1016/j.dss.2020.113321													
J								A generic framework for sentiment analysis: Leveraging opinion-bearing data to inform decision making	DECISION SUPPORT SYSTEMS										Sentiment analysis; Machine learning; Natural language processing; Decision Support Systems	MODEL	The increased exposure of the average citizen and customer to polarised content from various sources has been of significant consequence for companies and governmental organisations. Such content has, for example, served as a catalyst for violent uprisings and shifts in stock market prices. The collection and study of opinion have therefore become a necessity in many industries. Due to the vast nature of such data, manual approaches to this problem are no longer feasible. Several computational approaches have been proposed within the field of sentiment analysis, which successfully address many aspects of this problem, such as the classification of data into one of several sentiment categories. The research in the field is lacking, however, with respect to the integration and application of these techniques in practice, as well as their incorporation into the decision-making process of affected entities. In this paper, a generic framework for sentiment analysis is proposed, with a focus on facilitating the model development process for a user in a manner such that good performance may be achieved irrespective of the problem domain, as well as facilitating a flexible, exploratory analysis of model results in combination with existing structured attributes in order to gain actionable insights. The objective of the framework is to aid organisations in successfully leveraging unstructured, opinion-bearing data in combination with structured data sources to inform decision making.																	0167-9236	1873-5797				AUG	2020	135								113304	10.1016/j.dss.2020.113304													
J								A decision support framework and prototype for aircraft dispatch assessment	DECISION SUPPORT SYSTEMS										Decision support; Aircraft maintenance; Dispatch assessment; Information retrieval; Data integration; E-maintenance	DESIGN SCIENCE; E-MAINTENANCE; INFORMATION; SYSTEM	When an aircraft experiences an unexpected issue during flight operations, a technician determines whether the aircraft can safely perform the next flight. This operational decision process - known as dispatch assessment - has to happen within limited available time between aircraft arrival and departure. Currently, technicians face two main problems during the assessment: lack of access to decision support information and a time-consuming process for finding relevant information in extensive maintenance manuals. These issues often lead to delays and additional costs and are indicative of three larger challenges in the decision support domain: 1) a paucity of decision support models and applications for operational processes in maintenance; 2) relatively few efforts in applying and evaluating artifacts in experimental and real-life operational settings; and 3) a lack of systematic development, application and evaluation of digitization and automation efforts of complex decision processes in maintenance. This paper applies a design science research approach to address these challenges and introduces two novel artifacts: a decision support framework for real-time decision making in aircraft dispatch, and a web-based prototype tool accessible through mobile solutions. The practical relevance of the framework and prototype is validated through two representative application and evaluation studies, one in an experimental setting and one in an operational environment. Results show significant time savings and strong qualitative indications towards a higher incentive to use documentation and reducing human risk factors that lead to maintenance error.																	0167-9236	1873-5797				AUG	2020	135								113338	10.1016/j.dss.2020.113338													
J								Can digital consumption boost physical consumption? The effect of online music streaming on record sales	DECISION SUPPORT SYSTEMS										Access-based consumption; Ownership-based consumption; Music streaming services; Physical album sales; Digital downloads	WORD-OF-MOUTH; IMPACT; INDUSTRY; CHANNEL; MARKETS; DEMAND; PIRACY; TESTS	Despite a consensus that sales of physical albums have declined with the increasing prevalence of digital music, this seemingly unequivocal observation may not be true. Physical formats still occupy a fraction of the market, with a noticeable increase in the sales of physical albums in a few countries where digital media is predominant. This study empirically examines the impact of two online music channels (streaming and downloading) on physical album sales in the South Korean music market. By utilizing a unique dataset representing nationwide music sales, our empirical findings suggest that the growth in digital music services can be positively associated with an increase in physical album sales. In particular, this finding is more salient for male, solo, and lesser-known artistes. This empirical evidence implies that burgeoning music streaming services (access-based digital consumption) may lead to another ownership-based physical consumption. Our findings provide useful managerial implications for practitioners in countries where streaming services are growing.																	0167-9236	1873-5797				AUG	2020	135								113337	10.1016/j.dss.2020.113337													
J								How do agribusinesses thrive through complexity? The pivotal role of e-commerce capability and business agility	DECISION SUPPORT SYSTEMS										E-commerce capability; Business agility; IT-enabled organizational capabilities perspective; Agribusinesses; Complexity; Business value of IT	INFORMATION-TECHNOLOGY CAPABILITY; RESOURCE-BASED VIEW; BIG DATA ANALYTICS; FIRM PERFORMANCE; ORGANIZATIONAL AGILITY; ENVIRONMENTAL STRATEGY; INFRASTRUCTURE; INNOVATION; ADOPTION; IMPACT	The recent COVID-19 pandemic has clearly shown how agricultural foods and e-commerce initiatives are critical for many organizations, regions, and countries worldwide. Despite this vital importance, prior IS research on the business value of IT has not paid enough attention to the potential specificities of the agribusinesses. This study examines the impact of e-commerce capability on business agility in agribusinesses. Using a sample of Chinese agriculture firms, we find that: 1) The e-commerce capability of agribusinesses enables two types of business agility: market capitalizing agility and operational adjustment agility, and 2) while environmental complexity positively moderates the effects of e-commerce capability on the market capitalizing agility and operational adjustment agility, environmental dynamism does not. This study contributes to the IS research on the business value of IT by providing an eloquent theoretical explanation and empirical evidence on how e-commerce capability help agricultural firms to thrive through complexity by enabling market capitalizing agility (strategic focus) and operational adjustment agility (operational focus).																	0167-9236	1873-5797				AUG	2020	135								113342	10.1016/j.dss.2020.113342													
J								Effectiveness of privacy assurance mechanisms in users' privacy protection on social networking sites from the perspective of protection motivation theory	DECISION SUPPORT SYSTEMS										Effectiveness of privacy assurance mechanisms; Privacy customization; Social networking sites; Protection motivation; Privacy concern; Self-disclosure	SELF-DISCLOSURE; ONLINE PRIVACY; FEAR APPEALS; INFORMATION DISCLOSURE; PERSONALITY-TRAITS; CONSUMER TRUST; E-COMMERCE; BEHAVIOR; FACEBOOK; PERCEPTIONS	Along with recent advancement of web technologies, social networking sites (SNSs) affect peoples lifestyles by enabling them to perform so many activities which were not easy to do before. However, protection of users' disclosed information becomes an important challenge not only for SNS users but also for governing organizations. This study focuses on SNS users' protection from privacy breaches that are made by or facilitated by SNS vendors and explores the effectiveness of privacy assurance mechanisms on SNS users' privacy preserving actions. This study proposes a conceptual model that explains SNS users' privacy protection behavior based on protection motivation theory. An empirical data was collected to validate the proposed research model. The results suggest that users' coping appraisals of the overall SNS assurance mechanisms, along with their threat appraisals, positively relate to their protection motivation. Further, the results confirm that privacy concern mediates the relationship between threat appraisals and coping appraisals. Finally, findings suggest that protective motivation discourages one's engagement in risky behavior (disclosing self on SNS), in addition to undertaking safeguards (customizing privacy settings). Meanwhile, we find that individuals' use of safeguards increases personal engagement in risky behaviors (self-disclosure).																	0167-9236	1873-5797				AUG	2020	135								113323	10.1016/j.dss.2020.113323													
J								The effect of interactive analytical dashboard features on situation awareness and task performance	DECISION SUPPORT SYSTEMS										Interactive analytical dashboards; What-if analysis; Situation awareness; Out-of-the-loop syndrome; Eye-tracking; Operational decision support systems	BUSINESS INTELLIGENCE; SUPPORT; CONSEQUENCES; AUTOMATION; SYSTEMS	In recent years, new types of interactive analytical dashboard features have emerged for operational decision support systems (DSS). Analytical components of such features solve optimization problems hidden from the human eye, whereas interactive components involve the individual in the optimization process via graphical user interfaces (GUIs). Despite their expected value for organizations, little is known about the effectiveness of interactive analytical dashboards in operational DSS or their influences on human cognitive abilities. This paper contributes to the closing of this gap by exploring and empirically testing the effects of interactive analytical dashboard features on situation awareness (SA) and task performance in operational DSS. Using the theoretical lens of SA, we develop hypotheses about the effects of a what-if analysis as an interactive analytical dashboard feature on operational decision-makers' SA and task performance. The resulting research model is studied with a laboratory experiment, including eye-tracking data of 83 participants. Our findings show that although a what-if analysis leads to higher task performance, it may also reduce SA, nourishing a potential out-of-the-loop problem. Thus, designers and users of interactive analytical dashboards have to carefully mitigate these effects in the implementation and application of operational DSS. In this article, we translate our findings into implications for designing dashboards within operational DSS to help practitioners in their efforts to address the danger of the out-of-the-loop syndrome.																	0167-9236	1873-5797				AUG	2020	135								113322	10.1016/j.dss.2020.113322													
J								More than a feeling: Investigating the contagious effect of facial emotional expressions on investment decisions in reward-based crowdfunding	DECISION SUPPORT SYSTEMS										Reward-based crowdfunding; Funding behavior; Emotional contagion; Facial emotional expressions; Data-driven analysis	SUCCESS; SELECTION; MIMICRY; PASSION; IMPACT	Reward-based crowdfunding provides a unique instrument to acquire capital from small investors for a specific project. To successfully campaign for the desired funds, however, entrepreneurs also need to understand the subconscious factors, which influence the funding behavior of potential investors. In pursuit of that goal, this study examines how facial emotional expressions, which are shown in the pictures of project presentations, affect the funding decision. Building upon the emotional contagion theory and evaluating the emotions displayed on the pictures of 18,696 project webpages with a machine-learning algorithm, we show that facial expressions of happiness and sadness positively affect (i.e., encourage) the funding decision. Other than in venture capital markets, an emotional approach of potential investors might hence be a viable strategy in reward-based crowdfunding. As persuasion tool, emotions should nevertheless be used judiciously since we found high intensities of facial emotional expressions to negatively affect the funding decision.																	0167-9236	1873-5797				AUG	2020	135								113326	10.1016/j.dss.2020.113326													
J								Modeling cyber-physical systems - aGliderAgent3.0 perspective	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Software agents; Decision support system; JADE; Android; Anomaly detection; Cyber-physical systems	COMBINING INFORMATION	Eight years ago we have designed and implemented an initial prototype of an agent-based glider pilot support system (theGliderAgent). Our aim was to validate correctness of the initial assumption that an agent-based system, combined with sensor data, can help pilots in various situations that occur during a flight. For instance, theGliderAgentwas capable of detecting certain dangers, warn the pilot and, autonomously, send notification(s) to the ground station. Due to the continuous and rapid development of mobile technologies and sensors, our initial prototype has evolved. First, we moved the system, from an emulated environment, to real devices. Second, a semantic-rule-based decision making system was integrated with theGliderAgent, to analyze feeds from sensors (altitude, temperature, blood pressure, etc.) and, based on received information, to trigger appropriate behaviors. The result of our work provided a foundations for development of a general-purpose, cyber-physical system framework, which will be described in this paper. Furthermore, the developed system illustrates an interesting approach to integration of heterogeneous IoT devices (potentially also IoT platforms).																	0925-9902	1573-7675				AUG	2020	55	1					67	93		10.1007/s10844-019-00588-3													
J								A state-of-the-art survey of Digital Twin: techniques, engineering product lifecycle management and business innovation perspectives	JOURNAL OF INTELLIGENT MANUFACTURING										Digital Twin; Cyber-physical system; Business model; Product lifecycle management; Review	CYBER-PHYSICAL SYSTEMS; BIG DATA; INDUSTRY; DESIGN; SIMULATION; OPTIMIZATION; MACHINE; SERVICE; ARCHITECTURE; TECHNOLOGY	With the rapid advancement of cyber-physical systems, Digital Twin (DT) is gaining ever-increasing attention owing to its great capabilities to realize Industry 4.0. Enterprises from different fields are taking advantage of its ability to simulate real-time working conditions and perform intelligent decision-making, where a cost-effective solution can be readily delivered to meet individual stakeholder demands. As a hot topic, many approaches have been designed and implemented to date. However, most approaches today lack a comprehensive review to examine DT benefits by considering both engineering product lifecycle management and business innovation as a whole. To fill this gap, this work conducts a state-of-the art survey of DT by selecting 123 representative items together with 22 supplementary works to address those two perspectives, while considering technical aspects as a fundamental. The systematic review further identifies eight future perspectives for DT, including modular DT, modeling consistency and accuracy, incorporation of Big Data analytics in DT models, DT simulation improvements, VR integration into DT, expansion of DT domains, efficient mapping of cyber-physical data and cloud/edge computing integration. This work sets out to be a guide to the status of DT development and application in today's academic and industrial environment.																	0956-5515	1572-8145				AUG	2020	31	6					1313	1337		10.1007/s10845-019-01512-w													
J								A failure mode and risk assessment method based on cloud model	JOURNAL OF INTELLIGENT MANUFACTURING										Cloud model; Best-worst method; Risk assessment; Meta-action; Failure modes	FUZZY; FMEA; SYSTEMS; SELECTION	Failure mode and effects analysis (FMEA) is a predictive reliability analysis technique, which is widely used to improve the reliability and safety of products in products design, manufacture and service phases. However, traditional FMEA has many shortcomings in practical application, resulting in poor accuracy of analysis results. In this paper, based on meta-action failure modes, a risk assessment and ranking method based on cloud model is proposed. First, the domain expert's assessment of failure modes' attributes is transformed into a cloud model. Then, the best-worst method (BWM) and cloud model are combined to calculate the cloud weight of each attribute, and the weight of each expert to risk factors of each failure mode is evaluated by cloud distance. Finally, the comprehensive cloud expression of each failure mode is synthesized and compared. The proposed evaluation method not only has the advantages of cloud model in dealing with fuzziness and randomness, but also integrates the advantages of BWM, and fully takes into account the differences of experts in assigning weights to different failure modes' attributes. Finally, the effectiveness of the proposed method is verified by comparing the risk assessment results of the CNC machine tool's rotation-meta-action failure modes with different risk assessment methods.																	0956-5515	1572-8145				AUG	2020	31	6					1339	1352		10.1007/s10845-019-01513-9													
J								A six-layer architecture for the digital twin: a manufacturing case study implementation	JOURNAL OF INTELLIGENT MANUFACTURING										Industry 4; 0; Cyber physical systems (CPS); Internet of things (IoT); Digital twin; OPC; Tecnomatix	CYBER-PHYSICAL SYSTEMS; DESIGN	Industry 4.0, cyber-physical production systems (CPPS) and the Internet of Things (IoT) are current focusses in automation and data exchange in manufacturing, arising from the rapid increase in capabilities in information and communication technologies and the ubiquitous internet. A key enabler for the advances promised by CPPSs is the concept of adigital twin, which is the virtual representation of a real-world entity, or thephysical twin. An important step towards the success of Industry 4.0 is the establishment of practical reference architectures. This paper presents an architecture for such a digital twin, which enables the exchange of data and information between a remote emulation or simulation and the physical twin. The architecture comprises different layers, including a local data layer, an IoT Gateway layer, cloud-based databases and a layer containing emulations and simulations. The architecture can be implemented in new and legacy production facilities, with a minimal disruption of current installations. This architecture provides a service-based and real-time enabled infrastructure for vertical and horizontal integration. To evaluate the architecture, it was implemented for a small, but typical, physical manufacturing system component.																	0956-5515	1572-8145				AUG	2020	31	6					1383	1402		10.1007/s10845-019-01516-6													
J								On the end-of-life state oriented multi-objective disassembly line balancing problem	JOURNAL OF INTELLIGENT MANUFACTURING										End-of-life states; Disassembly line balancing problem; Multi-objective optimization; Hybrid migrating birds optimization	MIGRATING BIRDS OPTIMIZATION; SEARCH ALGORITHM; GENETIC ALGORITHM; FORMATION FLIGHT; MODEL; PROFIT; APPROXIMATION; FORMULATIONS; UNCERTAINTY; DESIGN	The biggest difference between a disassembly line and an assembly line is that there are many uncertainties in structure and quality of the disassembled products in a disassembly line. The disassembly line balancing problem, considering the effect of end-of-life states caused by the uncertainty of the structure or the quality of the disassembled products, is addressed in this paper. A multi-objective mathematical model for the addressed problem is built with three optimization goals: minimizing the number of workstations, minimizing the idle index and minimizing the number of resources. Then a multi-objective hybrid migrating birds optimization algorithm is proposed, which uses a greedy random search operation based on embedding mechanism to generate neighborhood individuals. To avoid the problem of easily being trapped into a local optimum by a basic migrating birds optimization algorithm, a reset mechanism based on simulated annealing operation is set up to accept other solutions with a certain probability, so that the algorithm can escape out of a local optimum. By solving disassembly examples of different scales in the literature and comparing with the existing algorithms, the effectiveness and superiority of the proposed multi-objective hybrid migrating birds optimization algorithm is validated. Finally, the proposed model and algorithm are applied to solving two disassembly instances, and the solving results are compared with the single-objective optimal solution solved by LINGO 11.0 solver and the basic migrating birds optimization algorithm to further identify the performance of the proposed algorithm.																	0956-5515	1572-8145				AUG	2020	31	6					1403	1428		10.1007/s10845-019-01519-3													
J								Assembly consistency improvement of straightness error of the linear axis based on the consistency degree and GA-MSVM-I-KM	JOURNAL OF INTELLIGENT MANUFACTURING										Assembly consistency improvement; Straightness error; Linear axis; Batch assembly; Consistency degree; GA-MSVM-I-KM	MACHINE-TOOL; THERMAL ERROR; QUALITY IMPROVEMENT; GEOMETRIC ERRORS; COMPENSATION; OPTIMIZATION; DEFORMATION; PERFORMANCE; WEAR	Fluctuation on the assembly quality of the linear axis of machine tools (LA-MT) at the same batch is urgent problem need to be solved in assembly of machine tools. In this paper, a new concept of assembly consistency degree was introduced for defining the fluctuation degree of assembly quality. Based on assembly consistency degree, a hybrid machine learning method, genetic algorithm optimized multi-class support vector machine and improved Kuhn-Munkres (GA-MSVM-I-KM) was proposed for improving assembly consistency of LA-MT. The assembly of linear axis of a three-axis vertical machining center was regarded as an example, and the assembly consistency influence factors on straightness error of Y-axis (SE-YA) were analyzed through the Kruskal-Wallis statistical method. The main factors affected on the assembly consistency of SE-YA turned out to be the machining errors of bed and the assembly team technical levels. Based on this, the assembly consistency improvement model was established. Then, the prediction model of SE-YA based on assembly experiment data and genetic algorithm optimized multi-class support vector machine (GA-MSVM) was constructed, and I-KM method was applied for improving assembly consistency of SE-YA. The results show that the GA-MSVM-I-KM method can effectively enhance the assembly consistency of SE-YA, and the assembly consistency degree is reduced from 0.19 to 0.08.																	0956-5515	1572-8145				AUG	2020	31	6					1429	1441		10.1007/s10845-019-01520-w													
J								A memetic algorithm for multi-objective distributed production scheduling: minimizing the makespan and total energy consumption	JOURNAL OF INTELLIGENT MANUFACTURING										Distributed production scheduling with different factories and workshops; Energy efficiency; Memetic algorithm; Multi-objective optimization	GENETIC ALGORITHM; SUPPLY CHAIN; SHOP; SEARCH; QUALITY; OPTIMIZATION; MULTIPRODUCT; MODEL	The classical distributed production scheduling problem (DPSP) assumes that factories are identical, and each factory is composed of just some machines. Inspired by the fact that manufacturers these days typically work across different factories, and each of these factories normally has some workshops, we study an important extension of the DPSP with different factories and workshops (DPFW), where jobs can be processed and transferred between the factories, workshops and machines. To the best of our knowledge, this is the very first time distributed production scheduling with different factories and workshops is studied. We propose a novel memetic algorithm (MA) to solve this DPFW, aiming to minimize the makespan and total energy consumption. The proposed MA is incorporated with a well-designed chromosome encoding method and a balance-transfer initialization method to generate a good initial population. An effective local search operator is also presented to improve the MA's convergence speed and fully exploit its solution space. A total of 50 DPFW benchmark instances are used to evaluate the performance of our MA. Computational experiments carried out confirm that the MA is able to easily obtain better solutions for the majority of the tested problem instances compared to three other well-known algorithms, demonstrating its superior performance over these algorithms in terms of solution quality. Our proposed method and the results presented here may be helpful for production managers who work with distributed manufacturing systems in scheduling their production activities by considering different factories and workshops. With this DPFW, imbalanced resource loads and unexpected bottlenecks, which regularly arise in traditional DPSP models, can be easily avoided.																	0956-5515	1572-8145				AUG	2020	31	6					1443	1466		10.1007/s10845-019-01521-9													
J								Imbalanced fault diagnosis of rotating machinery via multi-domain feature extraction and cost-sensitive learning	JOURNAL OF INTELLIGENT MANUFACTURING										Rotating machinery; Fault diagnosis; Imbalanced classification; Feature extraction; Cost-sensitive learning	DATA-DRIVEN; NEURAL-NETWORK; CLASSIFICATION; BEARINGS; DESIGN; SMOTE	Fault diagnosis plays an essential role in rotating machinery manufacturing systems to reduce their maintenance costs. How to improve diagnosis accuracy remains an open issue. To this end, we develop a novel framework through combined use of multi-domain vibration feature extraction, feature selection and cost-sensitive learning method. First, we extract time-domain, frequency-domain, and time-frequency-domain features to make full use of vibration signals. Second, a feature selection technique is employed to obtain a feature subset with good generalization properties, by simultaneously measuring the relevance and redundancy of features. Third, a cost-sensitive learning method is designed for a classifier to effectively learn the discriminating boundaries, with an extremely imbalanced distribution of fault instances. For illustration, a real-world dataset of rotating machinery collected from an oil refinery in China is utilized. The extensive experiments have demonstrated that our multi-domain feature extraction and feature selection can significantly improve the diagnosis accuracy. Meanwhile, our cost-sensitive learning method consistently outperforms the traditional classifiers such as support vector machine (SVM), gradient boosting decision tree (GBDT), etc., and even better than the classification method calibrated by six popular imbalanced data resampling algorithms, such as the Synthetic Minority Over-sampling Technique (SMOTE) and the Adaptive Synthetic sampling method (ADASYN), in terms of decreasing missed alarms and reducing the average cost. Owing to its high evaluation scores and low average misclassification cost, cost-sensitive GBDT (CS-GBDT) is preferred for imbalanced fault diagnosis in practice.																	0956-5515	1572-8145				AUG	2020	31	6					1467	1481		10.1007/s10845-019-01522-8													
J								A hybrid information model based on long short-term memory network for tool condition monitoring	JOURNAL OF INTELLIGENT MANUFACTURING										Tool condition monitoring; Tool wear; Maintenance; Deep learning; Long short-term memory network; Process information	USEFUL LIFE PREDICTION; NEURAL-NETWORKS; WEAR; FUSION; LSTM	Excessive tool wear leads to the damage and eventual breakage of the tool, workpiece, and machining center. Therefore, it is crucial to monitor the condition of tools during processing so that appropriate actions can be taken to prevent catastrophic tool failure. This paper presents a hybrid information system based on a long short-term memory network (LSTM) for tool wear prediction. First, a stacked LSTM is used to extract the abstract and deep features contained within the multi-sensor time series. Subsequently, the temporal features extracted are combined with process information to form a new input vector. Finally, a nonlinear regression model is designed to predict tool wear based on the new input vector. The proposed method is validated on both NASA Ames milling data set and the 2010 PHM Data Challenge data set. Results show the outstanding performance of the hybrid information model in tool wear prediction, especially when the experiments are run under various operating conditions.																	0956-5515	1572-8145				AUG	2020	31	6					1497	1510		10.1007/s10845-019-01526-4													
J								An adversarial bidirectional serial-parallel LSTM-based QTD framework for product quality prediction	JOURNAL OF INTELLIGENT MANUFACTURING										Quality prediction; Manufacturing and assembly processes; Bidirectional long short-term memory; Temporal interactions; Parallel processes	MANUFACTURING SYSTEMS; SURFACE-ROUGHNESS; NEURAL-NETWORK; MODEL; REGRESSION; NORMALITY; MULTIPLE; DESIGN	In order to capture temporal interactions among processes in manufacturing and assembly processes, an end-to-end unified product quality prediction framework called QTD is proposed in this paper. It consists of three modules: quality embedding model pool, temporal-interactive model, and decoding model. Besides, to handle the information transfer and integration problems in the time direction of parallel processes, a novel bidirectional serial-parallel LSTM (Bi-SP-LSTM) is devised as an instantiated model of temporal-interactive model. Bi-SP-LSTM is an extension of bidirectional long short-term memory. Moreover, an unsupervised task and a loss function named adversarial focal loss have been designed to give the framework the ability to assess heteroscedastic uncertainty in classification task due to intrinsic uncertainty in data. Furthermore, experiments are devised based on a subset of a public dataset from Kaggle competition to demonstrate the validity of the proposed framework. Compared with other latest methods, the proposed framework is verified to be more accurate and robust. Taking Matthews correlation coefficient as an example, the adversarial Bi-SP-LSTM-based QTD framework is superior to the best existing methods with 95% confidence interval in most cases, and its mean MCC is 4.88% higher than the best existing method. The results suggest that the proposed framework has a broad application prospect for quality prediction in manufacturing and assembly processes.																	0956-5515	1572-8145				AUG	2020	31	6					1511	1529		10.1007/s10845-019-01530-8													
J								Machine learning applied in production planning and control: a state-of-the-art in the era of industry 4.0	JOURNAL OF INTELLIGENT MANUFACTURING										Machine learning; Industry 4; 0; Smart manufacturing; Production planning and control; State-of-the-art; Systematic literature review	BIG DATA ANALYTICS; SHORT-TERM-MEMORY; CONTROL-SYSTEM; LEAD-TIME; FAULT-DIAGNOSIS; DECISION-MAKING; PRODUCTION LINE; NEURAL-NETWORK; PREDICTION; FRAMEWORK	Because of their cross-functional nature in the company, enhancing Production Planning and Control (PPC) functions can lead to a global improvement of manufacturing systems. With the advent of the Industry 4.0 (I4.0), copious availability of data, high-computing power and large storage capacity have made of Machine Learning (ML) approaches an appealing solution to tackle manufacturing challenges. As such, this paper presents a state-of-the-art of ML-aided PPC (ML-PPC) done through a systematic literature review analyzing 93 recent research application articles. This study has two main objectives: contribute to the definition of a methodology to implement ML-PPC and propose a mapping to classify the scientific literature to identify further research perspectives. To achieve the first objective, ML techniques, tools, activities, and data sources which are required to implement a ML-PPC are reviewed. The second objective is developed through the analysis of the use cases and the addressed characteristics of the I4.0. Results suggest that 75% of the possible research domains in ML-PPC are barely explored or not addressed at all. This lack of research originates from two possible causes: firstly, scientific literature rarely considers customer, environmental, and human-in-the-loop aspects when linking ML to PPC. Secondly, recent applications seldom couple PPC to logistics as well as to design of products and processes. Finally, two key pitfalls are identified in the implementation of ML-PPC models: the complexity of using Internet of Things technologies to collect data and the difficulty of updating the ML model to adapt it to the manufacturing system changes.																	0956-5515	1572-8145				AUG	2020	31	6					1531	1558		10.1007/s10845-019-01531-7													
J								Knowledge recommendation for product development using integrated rough set-information entropy correction	JOURNAL OF INTELLIGENT MANUFACTURING										Product development; Knowledge recommendation; Knowledge reuse; Rough set; Information entropy	SYSTEM APPLICATION; ONTOLOGY; MANAGEMENT; FRAMEWORK; SUPPORT	New product development is knowledge intensive as it needs the work teams and design engineers located at various locations to constantly share, update, and re-use knowledge. As such, improving the efficiency of acquiring knowledge and coping with the challenge of frequently retrieving related knowledge have become a key factor to managing knowledge in new product development. This paper combines rough set theory and information entropy to establish a new knowledge recommender technique to address the issue of knowledge reuse for new product development. Our method enhances knowledge acquisition and reuse, as it provides a realistic framework for knowledge acquisition and reuse, encompassing the entire process from what the design and work teams need, to recommending what they should have. To validate the proposed approach, we perform experiments on a case study to demonstrate the benefit and performance.																	0956-5515	1572-8145				AUG	2020	31	6					1559	1578		10.1007/s10845-020-01534-9													
J								Online anomaly search in time series: significant online discords	KNOWLEDGE AND INFORMATION SYSTEMS										Anomaly detection; Online analysis; Time series; Discord; Nearest neighbor distance; Big data	ALGORITHMS	The aim of this work is to obtain a useful anomaly definition for online analysis of time series. The idea is to develop an anomaly concept which is sustainable for long-lived and frequent streamings. As a solution, we provide an adaptation of the discord concept, which has been successfully used for anomaly detection on time series. An online approach implies the frequent processing of a data streaming for timely providing anomaly alerts. This requires a modification since discord search is not exactly decomposable in its original definition. With a statistical approach, allowing to rate the significance of the discords of each analysis, it has been possible to obtain a solution where the number of false positives is minimized. The new online anomalies are called significant online discords (sods). As a novel feature,sodsearch determines the quantity of anomalies in the time series under investigation. The search forsods has been implemented and its properties validated with synthetic and real data. As a result, we found thatsods can be considered as a useful new tool for anomaly detection in fast streaming time series or Big Data contexts.																	0219-1377	0219-3116				AUG	2020	62	8					3083	3106		10.1007/s10115-020-01453-4													
J								Improved covering-based collaborative filtering for new users' personalized recommendations	KNOWLEDGE AND INFORMATION SYSTEMS										Covering-based rough sets; User-based collaborative filtering; Covering reduction; Personalized recommendations; New user cold-start	COLD-START PROBLEM; SYSTEMS	User-based collaborative filtering (UBCF) is widely used in recommender systems (RSs) as one of the most successful approaches, but traditional UBCF cannot provide recommendations with satisfactory accuracy and diversity simultaneously. Covering-based collaborative filtering (CBCF) is a useful approach that we have proposed in our previous work, which greatly improves the traditional UBCF and could provide satisfactory recommendations to an active user which often has sufficient rating information. However, different from an active user, a new user in RSs often has special characteristics (e.g., fewer ratings or ratings concentrating on popular items), and the previous CBCF approach cannot provide satisfactory recommendations for a new user. In this paper, aiming to provide personalized recommendations for a new user, through a detailed analysis of the characteristics of new users, we reconstruct a decision class to improve the previous CBCF and utilize the covering reduction algorithm in covering-based rough sets to remove redundant candidate neighbors for a new user. Furthermore, unlike the previous CBCF, our improved CBCF could provide personalized recommendations without needing special additional information. Experimental results suggest that for the sparse datasets that often occur in real RSs, the improved CBCF significantly outperforms those of existing work and can provide personalized recommendations for a new user with satisfactory accuracy and diversity simultaneously.																	0219-1377	0219-3116				AUG	2020	62	8					3133	3154		10.1007/s10115-020-01455-2													
J								CAMAR: a broad learning based context-aware recommender for mobile applications	KNOWLEDGE AND INFORMATION SYSTEMS										Mobile application; Recommendation; Tensor decomposition; Multi-view learning		The emergence of a large number of mobile apps brings challenges to locate appropriate apps for users, which makes mobile app recommendation an imperative task. In this paper, we first conduct detailed data analysis to show the characteristics of mobile apps which are different with conventional items (e.g., movies, books). Considering the specific property of mobile apps, we propose a broad learning approach for context-aware mobile app recommendation with tensor analysis (CAMAR). Specifically, we utilize a tensor-based framework to effectively integrate app category information and multi-view features on users and apps to facilitate the performance of app recommendation. The multi-dimensional structure is employed to capture the hidden relationships among the app categories and multi-view features. We develop an efficient factorization method which applies Tucker decomposition to jointly learn the full-order interactions among the app categories and features without physically building the tensor. Furthermore, we employ a groupl1 norm regularization to learn the group-wise feature importance of each view with respect to each app category. Experiments on two real-world datasets demonstrate the effectiveness of the proposed method.																	0219-1377	0219-3116				AUG	2020	62	8					3291	3319		10.1007/s10115-020-01440-9													
J								LOW-SENSITIVITY CONTROL WITH ROBUST STABILITY USING DOUBLE-FEEDBACK CONTROL FOR MIMO TIME-DELAY SYSTEMS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Multivariable system; Time-delay system; Low-sensitivity control; Sensitivity function; Robust stability	INFINITE GAIN MARGIN; DESIGN	In this paper, we consider the problem of a two-degree-of-freedom control system using double-feedback control with low sensitivity and robust stability for multipleinput/multiple-output time-delay systems having a varying number of unstable poles. A control system is desired to achieve low sensitivity such as reducing the effect of the uncertainty for the output. In general, it is difficult for a control system with robust stability to reduce low-sensitivity characteristics. Yamada shows a robust stability condition that can achieve low-sensitivity characteristics for a plant having a varying number of poles. Yu et al. expand the results of Yamada and propose a design method for two-degreeof-freedom control systems for single-input/single-output minimum-phase systems using double-feedback control with robust stability to reduce the effect of the uncertainty for the output to be much smaller than that of the conventional two-degree-of-freedom control system. However, a design method for a double-feedback control system for multipleinput/multiple-output time-delay systems having a varying number of unstable poles has not been considered. In this paper, we expand the results of Yu et al. and propose a design method for a two-degree-of-freedom control system using double-feedback control with low sensitivity and robust stability to reduce the effect of the uncertainty for the output to be smaller than that of a conventional two-degree-of-freedom control system for multiple-input/multiple-output time-delay systems having a varying number of unstable poles.																	1349-4198	1349-418X				AUG	2020	16	4					1133	1146		10.24507/ijicic.16.04.1133													
J								AUTOMATIC RECOMMENDATION OF DESIGN PATTERNS BASED ON PATTERNS' INTENT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Design pattern; Design pattern recommendation; Cosine similarity; Design pattern intent ontology; Object-oriented software design	SELECTION	The gang-of-four (GoF) patterns provide best practices and reusable solutions to recurrent problems in object-oriented software design. We propose an automatic approach for ranking and recommending GoF patterns. Design-pattern vectors, representing the GoF patterns in terms of the problem types they address, are constructed based on the design pattern intent ontology (DPIO) developed by Kampffmeyer. An input design problem is represented as an input-problem vector, constructed by matching terms extracted from its description with constraints and concepts characterizing problem types in the DPIO. Patterns are ranked and recommended based on similarity scores computed between the design-pattern vectors and the input-problem vector. The proposed method was evaluated on a collection of 36 design problems. With appropriate parameter setting, the actual answers to 69.44% and 83.33% of the problems were recommended within the top-3 and top-5 ranks, respectively. With additional term correspondences for improvement of term matching, the results were increased to 75.00% and 88.89% for the top-3 and top-5 ranks, respectively. Compared to text-based pattern ranking using a vector space model, our proposed method yielded significantly better performance when they were evaluated on the same problem collection.																	1349-4198	1349-418X				AUG	2020	16	4					1147	1163		10.24507/ijicic.16.04.1147													
J								A MODEL OF REFUNDABLE ORDER CANCELLATION PROBLEM WITH FULFILLMENT COST	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Order cancellation; Fulfillment cost; Revenue management; Online grocery retailing		This paper addresses a refundable order cancellation problem from the observation of online grocery retail business in China, characterized by the fulfillment cost consisting of both physical losses caused by deteriorations in order fulfillment process and expenses spent on operational activities associated with the cancelled orders, which is a considerable sunk cost. In consideration of this fulfillment cost, we formulate a revenue management model by determining the refund rate and the hassle cost to optimize refund policies with the goal of maximizing the retailer's profit. Then, we conduct model specification and yield several main results. Finally, numerical experiments are performed on randomly generated data and the implications of numerical results are discussed. We show both theoretically and experimentally that offering partial refund is always the superior policy rather than imposing hassles of cancelling on consumers. The proposed model can expand to be implemented in revenue management for extensive emerging on-demand instant service systems, and we provide managerial insights to practitioners at managing order cancellations with fulfillment costs on physical operations.																	1349-4198	1349-418X				AUG	2020	16	4					1165	1181		10.24507/ijicic.16.04.1165													
J								GROUP CENTRALITY ALGORITHMS BASED ON THE H-INDEX FOR IDENTIFYING INFLUENTIAL NODES IN LARGE-SCALE NETWORKS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Influence maximization; Large-scale networks; H-index; Group centrality	RANKING	Our target is to investigate influence maximization because it has numerous applications, such as using influence maximization to spread ideas in social networks, or inversely, avoiding influence maximization in order to control sources of infection in a population. Identifying influential nodes in complex networks is a fundamental task in influence maximization. The main challenges include scalability, dynamics, and topological diversity. By focusing on the scalability of spreaders with large-scale datasets, we extend the variant h-index to one of group centralities and investigate it as a new centrality, called the h-index group centrality, which fuses the h-index with the group. Furthermore, we propose five algorithms based on the h-index group centrality. These five algorithms satisfy the requirements of large-scale networks. Experimental results show that our algorithms outperform comparison algorithms and that our algorithms have great scalability on tested datasets. In the best case, the results of our method exceed those of the latest method by more than 1,000 times. Group centrality algorithms based on the h-index achieve significant improvement and have many potential applications.																	1349-4198	1349-418X				AUG	2020	16	4					1183	1201		10.24507/ijicic.16.04.1183													
J								TWO STRATEGY COOPERATIVE PARTICLE SWARM OPTIMIZATION ALGORITHM WITH INDEPENDENT PARAMETER ADJUSTMENT AND ITS APPLICATION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Particle swarm optimization; Parameter adjustment; Reconstruction; Differential mutation; Flexible job shop scheduling	ADAPTIVE INERTIA WEIGHT; GENETIC ALGORITHM; TABU SEARCH; SOLVE	Aiming at the premature convergence of particle swarm optimization (PSO) in solving complex multimodal problems, a two strategy cooperative particle swarm optimization algorithm (TSPSO) with independent parameter adjustment is proposed. In the proposed algorithm, the variance of population fitness, evolution ability and evolution rate of particles are defined firstly, and then the inertia weight and learning factor of each particle are adjusted adaptively, which effectively balances the exploitation and exploration ability of the algorithm. Secondly, according to the fitness value and evolution ability of particles in each generation, the population is divided dynamically. The inferior subgroup uses reconstruction strategy to generate new particles by learning from the particles in the superior subgroup, so as to speed up the convergence of the algorithm; the superior subgroup uses differential mutation to avoid the premature convergence of the algorithm, and maintain the diversity of the population. Finally, a large number of experiments are carried out on the CEC2013 standard test function set and flexible job shop scheduling problem, and the experimental results verify that TSPSO has high efficiency. The convergence analysis shows the effectiveness of the algorithm.																	1349-4198	1349-418X				AUG	2020	16	4					1203	1223		10.24507/ijicic.16.04.1203													
J								DEPTH-BASED REAR-OBSTACLE DETECTION APPROACH FOR DRIVING IN THE REVERSE GEAR	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Obstacle detection; Reverse driving; Stereo vision; Disparity map	STEREO; WORLD	A vehicle is mainly driven in the forward direction and occasionally driven in the reverse gear. Blind spots for vehicles refer to the areas behind the vehicle that cannot be seen by a driver while driving. These spots are danger zones while driving a vehicle in the reverse gear. Therefore, vehicle safety in these spots is critical. In this paper, a rear-obstacle detection approach was proposed for drivers through stereo vision techniques. A binocular camera system based on parallax maps was used for rear obstacle detection. A stixel representation was developed for an outdoor traffic scene in a columnar manner. Also, disparity maps were used to estimate the ground region in the representation, identify free space in the representation, and segment the obstacles present in the image based on their height. Moreover, a road-edge-tracking step was incorporated in the proposed approach for stabilizing the approach. The experimental results reveal that the proposed approach exhibits high performance for rear obstacle detection.																	1349-4198	1349-418X				AUG	2020	16	4					1225	1235		10.24507/ijicic.16.04.1225													
J								AUTOMATED INSPECTION OF CONTOUR FAULTS FOR CONVEX MIRRORS USING WAVELET DESCRIPTORS AND EWMA CONTROL SCHEME	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Automated industrial inspection; Convex car mirrors; Contour faults; Wavelet descriptors; EWMA model	PACKET TRANSFORM	Side and rear car mirrors are among the most important safety features on our vehicles. Ordinary appearance faults of car mirrors comprise surface fault type and contour fault type. Since the contour faults will cause structural damages on vehicle mirrors and reduce the ability to withstand outer stress and pressure, the degree of harm is even more than the surface faults for vehicle mirrors. To substitute examiners from traditional inspection tasks of car mirrors, this study exploits a hybrid method based on computer vision to inspect contour faults on convex car mirrors. The hybrid method consists of wavelet transform and small variation detection algorithm. Distances from boundary points of a mirror to the centroid are transformed to 1-D wavelet domain with low-pass filtering to enhance the contour faults on the binary mirror images. The distance deviations of the corresponding boundary points before and after applying the wavelet filtering process can be distinguished by the exponential weighted moving average model to identify locations of the contour faults. This approach only uses self-own information of testing images to determine whether there are any irregular contour changes without the need of standard patterns for matching. Experimental outcomes show that the proposed hybrid method reaches 7% incorrect alert rate and 86% fault detection rate for the front-view image inspection; 5% incorrect alert rate and 92% fault detection rate for the side-view image inspection and it outperforms the existing methods in contour faults inspection on convex car mirrors.																	1349-4198	1349-418X				AUG	2020	16	4					1237	1255		10.24507/ijicic.16.04.1237													
J								COST REDUCTION FUNCTION CONSIDERING STOCHASTIC RISKS IN THE PRODUCTION PROCESS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Learning curve theory; Phase-field; Throughput; Lead time; Potential; Production process	SYNCHRONIZATION ANALYSIS; LEAD-TIME; THROUGHPUT; SYSTEM	In this paper, there are some risks assumed in small and medium-sized production systems: progress of process against demand, uncertainty of logistics, variation of process workers, and difficulty of synchronization with external suppliers. The first thing we have proposed for these challenges is a mathematical model of the work process flow. It was a model that utilized the heat conduction diffusion equation that handled several flowing processes in physics. In addition, fluctuations in demand and uncertainty in logistics were the introduction of stochastic models and the use of finance theory as stochastic phenomena. An important management theme is how to reduce production costs in consideration of these risks as a company. We propose a cost reduction function by analyzing these problems.																	1349-4198	1349-418X				AUG	2020	16	4					1257	1278		10.24507/ijicic.16.04.1257													
J								IMPROVED STABILITY CRITERIA FOR TIME-DELAYED LUR'E SYSTEMS WITH MARKOVIAN SWITCHING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Lur'e systems; Linear matrix inequality; Markov switch system; Time-delay; Robust stability	DEPENDENT ABSOLUTE STABILITY; ROBUST STABILITY; VARYING DELAY; SYNCHRONIZATION; STABILIZATION	In this paper, we focus on the problem of the stochastic stability for the time-delayed Markov jump Lur'e system with time-invariant and time-varying nonlinearities. By using an improved relaxed integral inequality technologies, new delay-dependent stochastic stability criteria are proposed via Lyapunov-Krasovskii functional (LKF) approach. The stability conditions reduce the conservation of some previous ones. Some numerical examples are presented to show the effectiveness of the proposed approach.																	1349-4198	1349-418X				AUG	2020	16	4					1279	1296		10.24507/ijicic.16.04.1279													
J								TEXTURE IMAGES ANALYSIS USING FRACTAL EXTRACTED ATTRIBUTES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Fractal dimension; k-means clustering; DBSCAN; Elbow; Silhouette	DIMENSION; SEGMENTATION	The extraction of texture attributes for image patterns is one of the vital tasks in the field of pattern recognition. For this purpose, there is substantial demand for extracting new features from texture images, in which it helps analyze the behavior of the extracted features. Fractal dimension as a gauge of complexity is considered as a good indicator to measure the hidden amount of information and the distribution of the data in a given dataset. However, it can be used to aid in the grouping of these data according to their complexity. In this work, a set of five fractal features is introduced for texture image analysis by utilizing two known clustering algorithms, the k-means, and the Density Based Scan (DBSCAN) to cluster the extracted fractal features into a related group, in which clustering extends the management of extensive data and then easily retrieves or classifies any new texture image. Two statistical algorithms (Elbow and Silhouette) are used to acquire a reasonable value of k in the k-means algorithm. After implementing the proposed algorithms on these fractal features, the best value of k is equal to three based on Elbow, and Silhouette and the k-means performs better than the DBSCAN. The results show that using these methods helps to acquire a reasonable value of k.																	1349-4198	1349-418X				AUG	2020	16	4					1297	1312		10.24507/ijicic.16.04.1297													
J								A SYSTEMATIC EVALUATION METHOD FOR PRODUCT CONFIGURATIONS IN THE DYNAMIC WAREHOUSE FOCUSING ON THE ZONE-SPECIFICITY	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Logistics; Optimization; Dijkstra's algorithm; Picking-up robot; Dynamic warehouse operations; Workflow compositions		In the warehouse, workload reduction is an important factor, while a simple automation cannot solve completely because supply and demand are dynamically changing. Recently, sophisticated machines have been introduced; however, a partial replacement of human operations does not guarantee an effective workload reduction in the sense of the whole warehousing management. A standard criterion is necessary for fair comparisons and a systematic and generalized evaluation method is expected beyond partial and heuristic assessments in logistics. For example, a zone-specific configuration of products was believed to have a benefit in the warehouse management as an expert knowledge, while it is unclear whether it actually contributes to a cost reduction or prevention of human errors. In this study, we proposed a systematic evaluation method for picking costs in the shelf by using the Dijkstra's algorithm, which calculates the minimum trajectory length connecting among target locations to pick. Our computer experiments demonstrated not only the comparison of picking processes based on two types of Dijkstra's algorithm with/without the zone-specificity, but also the difference of performances between the uniform random configuration of products and hit-rate based configuration depending on zones. In the proposed criterion, the hit-rate based configuration reduced 21.9% cost as the maximum in the evaluation by the efficacy based on the cumulative trajectory length in the single picking-up session in the comparison with the random configuration. This generalized evaluation method will contribute to improving the quality management in logistics and enhancing a transparent competition of automated systems of different companies.																	1349-4198	1349-418X				AUG	2020	16	4					1313	1322		10.24507/ijicic.16.04.1313													
J								TRAVEL MODE CHOICE MODELING VIA INFERENCE DIAGRAM CONSIDERING TRAVELERS' TASTE OF RISK	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Travel mode choice; Bayesian statistics; Travel uncertainty; Taste of risk	DECISION; OPTIMIZATION; EXPLORATION; UNCERTAINTY; BEHAVIOR	Current research approaches have not adequately considered the potential uncertainties of different modes. This paper develops an inference diagram (ID) approach to model travel mode choice with the consideration of travel time/cost uncertainty. The proposed ID model captures probabilistic relations between a trip's uncertain attributes and certain attributes. Given a trip's certain attributes, the uncertain attributes (e.g., travel time) can be inferred based on the Bayesian theorem; and the expected utility (EU) theorem is utilized for choice decision making. Simulation-based optimization (SBO) is applied to estimating the parameters of the ID model. There are two major contributions in this paper: 1) the ID model considers uncertain attributes inference in the decision-making process of travel mode choice; 2) the ID model is capable of considering travelers' heterogeneous tastes of risk. A real-world travel survey data conducted in the Washington D.C. area is used for a case study. We construct ID models with different tastes of risk; a traditional Logit model is fitted for comparison. The results indicate the ID model is superior in classification accuracy compared with MNL models. With travelers' heterogeneous tastes of risk, the ID model is capable of providing interesting findings in travel behavior research.																	1349-4198	1349-418X				AUG	2020	16	4					1323	1336		10.24507/ijicic.16.04.1323													
J								ADAPTIVE DYNAMIC SURFACE SATURATED CONTROL OF FLEXIBLE HYPERSONIC VEHICLE	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Flexible hypersonic vehicle; Backstepping control; Tracking control; Input saturation; Finite time stable	TIME TRACKING CONTROL	This paper investigates the flight longitudinal tracking control problem of flexible hypersonic vehicle with the external disturbance, model parameter uncertainties and input saturation. First, velocity and altitude subsystems are established using decomposing the longitudinal dynamics of flexible hypersonic vehicle, where the flexible effects, external disturbance and model parameter uncertainties are regarded as unknown bounded lumped disturbance. Second, the robust saturated controllers are designed for velocity and altitude subsystems by using the back-stepping method, adaptive tracking differentiator and tangent function with approximate saturation function, respectively. Finally, Lyapunov stability theory and simulations are adopted to verify the effectiveness of the control strategy.																	1349-4198	1349-418X				AUG	2020	16	4					1337	1351		10.24507/ijicic.16.04.1337													
J								THE RAIN NOISE REDUCTION USING GUIDED FILTER TO IMPROVE PERFORMANCE OF VEHICLE COUNTING	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Vehicle counting; Rainfall; Noise reduction; Guided filter		The calculation of the number of vehicles on the road using digital image processing technology is expected to be faster and more accurate. Nevertheless, there are obstacles such as noise that arises due to rain because it can reduce the accuracy of the calculation of the number of vehicles. Therefore, we need a rain noise reduction process to improve the accuracy of vehicle calculations. The guided filter has the advantage of reducing noise, including snow noise and fog noise. The experiment was conducted by comparing the accuracy of the calculation of the number of vehicles in noise conditions after and before they were reduced with heavy rain, moderate rain, and drizzle data. The results show that accuracy is increasing for heavy and moderate rainfall data, while the drizzle data has decreased. Therefore, the guided filter is suitable for heavy and moderate rain conditions.																	1349-4198	1349-418X				AUG	2020	16	4					1353	1370		10.24507/ijicic.16.04.1353													
J								NETWORK EMBEDDING OF TOPIC-ATTENTION NETWORK BASED ON SET PAIR ANALYSIS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Topic-attention network; SPA; Transition probability; Random walk; Network embedding		Concerning the problem that heterogeneous network embedding only considers social relations in structure and ignores semantics, combining the social relationship between users and the preference of users for topics, a network embedding algorithm based on topic-attention network was proposed. Firstly, according to the characteristics of the topic-attention network and combining with the idea of the identical-discrepancy-contrary (determination and uncertainty) of set pair analysis (SPA) theory, the transition probability model was given. Then a random walk algorithm based on two types of nodes was proposed by using the transition probability model, which obtained relatively high, quality random walk sequences. Finally, the embedding vector space representation of the topic-attention network was obtained by modeling based on two types of nodes in the sequences. After theoretical analysis, experimental results on the Douban dataset show that the modularity of the proposed algorithm is 0.5871 when the number of the overlapping communities is 11, which is nearly 6.5% higher than that of metapath2vec algorithm. The random walk algorithm combined with the transition probability model is more comprehensive in analyzing the connection relationship between nodes in the network, and can capture more detailed information in the network.																	1349-4198	1349-418X				AUG	2020	16	4					1371	1384		10.24507/ijicic.16.04.1371													
J								INTEGRATED FAULT DIAGNOSIS AND FAULT-TOLERANT CONTROL OF NONLINEAR NETWORK CONTROL SYSTEMS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Laplace transform; Nonlinear networked control systems; Time delay; Learning observer; State feedback control	RECONSTRUCTION	A new integrated fault diagnosis and fault tolerant control algorithm of nonlinear networked control systems is proposed in this paper. A Laplace transform strategy is used to analyze the random time delay problem. Besides, a new fault diagnosis algorithm based on learning observer is designed. Then, based on the fault diagnosis information, a new state feedback fault-tolerant controller is designed, which can ensure that the output of the post-fault system can still track the given target. Finally, a single-link robot is taken as an example for the numerical simulation. The simulation results show the effectiveness of the proposed method.																	1349-4198	1349-418X				AUG	2020	16	4					1385	1398		10.24507/ijicic.16.04.1385													
J								QUANTIZATION OF NUCLEAR MASS DEFECT AND MASS DIFFERENCES IN ATOMS OF THE SAME ISOTOPE	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Mass defect; Quantization; Isotope; Atomic weight; Mass spectrometer		According to the fact of nuclear mass defect, and combined with the quantization features generally existing in microcosm, a quantization hypothesis of nuclear mass defect is proposed, which assumes that different atoms of the same isotope of an element could have different masses. The rationality of this hypothesis is verified by the change in nuclear state, the accuracy of the measurement of isotopic atomic mass, and the Einstein's Equation of Mass and Energy, and several experimental ideas are given. The high-precision atomic weight data is used to search for the quantum of the mass defect and obtain an alternative mass quantum. Also, this paper proposed a small neutron hypothesis about the mass defect, which assumes that the nuclear mass defect is the mass reduction of an object with actual mass; the nuclear mass defect is the result of small neutrons loss.																	1349-4198	1349-418X				AUG	2020	16	4					1399	1408		10.24507/ijicic.16.04.1399													
J								CONSTRAINTS DECOMPOSITION AND CLUSTERING BASED WEB SERVICES COMPOSITION WITH UNCERTAIN QOS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Services composition; Uncertain QoS; QoS constraint decomposition; QoS aggregation; Clustering		Global QoS (Quality of Service) constraint decomposition is an important strategy for service composition, and QoS aggregation calculation is the key to service optimization. A constraint strength aware global QoS constraint decomposition model is proposed by introducing a relaxation factor that can be adjusted adaptively according to fuzzy reasoning rules. The improved K-means algorithm is adopted to avoid the combined explosion problem in the process of QoS aggregation calculation. Simulation experiments show that the constraint strength aware global QoS constraint decomposition model can effectively improve the success rate of service composition when the constraint strength is high, and can significantly improve the effect of reducing solution space when the constraint strength is low. The accuracy and time cost of clustering based QoS aggregation are better than the existing methods.																	1349-4198	1349-418X				AUG	2020	16	4					1409	1423		10.24507/ijicic.16.04.1409													
J								NEW COOPERATIVE TARGET ALLOCATION METHOD FOR THE TYPICAL TASKS OF UAVS BASED ON A DISCRETE SHEEP OPTIMIZATION ALGORITHM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Multi-UAVs collaboration; Target allocation; Sheep optimization algorithm; Genetic algorithm	ASSIGNMENT	As the traditional heuristic algorithm cannot solve the problem of multi-UAVs coordinated global target allocation perfectly, it is difficult to find a reliable initial allocation scheme and the convergence rate is not ideal. In order to solve these problems, a multi-UAVs coordinated global target allocation method based on discrete sheep optimization algorithm is proposed. Firstly, with the typical multi-UAVs coordinated attack task as the research background, the UAVs' fuel consumption cost, damage cost and revenue cost are taken as optimization indicators. Meanwhile, the target allocation model is established by considering the flight distance, flight time, load size, and target execution order of the UAVs. Then the penalty function method is used to deal with partially restrained condition to build a fitness function and the discrete sheep optimization algorithm which is discretized and combined with genetic algorithms is used as well to solve the problem. Finally, the simulation results show that the discrete sheep optimization algorithm for multi-UAVs coordinated global target allocation has the advantages of faster convergence rate and better stability compared with the genetic algorithm, and the allocation scheme with lower integrate-cost can be obtained in different scenarios, which can better solve the problem of multi-UAVs coordinated global target allocation.																	1349-4198	1349-418X				AUG	2020	16	4					1425	1440		10.24507/ijicic.16.04.1425													
J								SMART COFFEE VENDING MACHINE BASED ON IOT CONCEPT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Arduino board; Coffee vending machine; Dashboard; Franchise system; IoT; Raspberry Pi module; Ubidots cloud service		In order to improve the effectiveness of franchise system, this paper describes how to utilize the Internet of Things (IoT) concept for upgrading a franchisor's traditional coffee vending machine to be a smart coffee vending machine (SCVM) with remote monitoring and control capabilities. The IoT-based SCVM provides powerful web dashboard to monitor machine operation statuses, track ingredient and component availability, and adjust hot drink recipes. The proposed improvement can therefore help both franchisor and franchisee not only to ensure that how well the machine is operating, and how well the machine is stocked but also to plan that when a supply worker needs to pay a visit to the machine for refilling ingredients based on sales data, and when a maintenance worker needs to pay a visit to the machine for providing services based on technical data. The proposed technique uses an Arduino board to replace an existing machine controller and installs a Raspberry Pi module to perform as a gateway for data transmission between the SCVM and the Ubidots cloud server via Internet. The Node-RED engine running on the Raspberry Pi is utilized as the IoT platform to collect and process the technical and sales data of the SCVM that dispenses three kinds of hot drinks. Based on user login system, the franchisor is allowed to remotely monitor the interested parameters of all SCVMs on the created dashboard for ensuring product availability as well as to remotely control the ratios of instant coffee/cocoa powder and hot water for ensuring product quality, whereas each franchisee is allowed to remotely monitor the operation status and daily sales detail for his approved machines only. Test results from data simulations confirm that the upgraded SCVM and created dashboard subpages function correctly in accordance with the franchisor's requirements.																	1349-4198	1349-418X				AUG	2020	16	4					1441	1448		10.24507/ijicic.16.04.1441													
J								EFFECT OF STRAIGHTENING DISTORTION CAUSED BY RESIDUAL STRESSES ON ULTIMATE COMPRESSIVE STRENGTHS OF U-RIB STIFFENED PLATES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Ultimate compressive strengths; Stiffened plates; U-rib; Residual stresses; Initial geometric imperfection; Straightening distortion; FHWA	WELDING DISTORTION	The residual stresses resulting from a welding process in which plates and stiffeners are joined have a detrimental effect on the ultimate strength of stiffened plates and they have to be carefully analyzed. Therefore, the ultimate compressive strengths of U-rib stiffened plates with various combinations of column and plate slenderness parameters were evaluated using nonlinear Finite Element Analysis (FEA). The effect of straightening the distortion caused by the application of residual stresses on the strength of these plates was investigated. It was found that the straightening process could increase the ultimate strength up to 15% even though the displacements caused by the residual stresses could increase up to three times the initial geometric imperfection. FEA results were also compared with the strength predictions from the Federal Highway Administration (FHWA). For the stiffened plates whose plate slenderness parameters were greater than 0.9, the FHWA predictions were not conservative in the region having relatively low column slenderness parameter values.																	1349-4198	1349-418X				AUG	2020	16	4					1449	1457		10.24507/ijicic.16.04.1449													
J								WALKING NAVIGATION SYSTEM FOR VISUALLY IMPAIRED PEOPLE BASED ON HIGH-ACCURACY POSITIONING USING QZSS AND RFID AND OBSTACLE AVOIDANCE USING HOLOLENS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Navigation system; Visually impaired people; Obstacle avoidance		It is extremely difficult for visually impaired people to move around in new places. Because traditional pedestrian navigation systems such as Google Map are designed using visual information such as map displays, they are difficult for visually impaired people to use. The purpose of this research is to develop a wearable system to assist a visually impaired people in navigation. We propose a positioning method combining Radio Frequency Identifier (RFID) and Quasi-Zenith Satellite System (QZSS) to obtain highly accurate positioning both outdoors and indoors. Furthermore, to guide visually impaired persons, it is necessary to sense and guide the direction in which the user is facing, and to detect and avoid obstacles. Therefore, we adopted HoloLens, which can generate a 3D mapping of the environment to estimate the user's direction and to generate a route to avoid obstacles. The system directions are based on the shortest route from the current position to the destination, while dynamically generating a route that avoids obstacles if they are detected. In this paper, the navigation system construction and approaches are described, and then the results of the demonstration experiments are reported. According to the results, positioning accuracy based on RFID and QZSS was achieved with an error of less than lm. Furthermore, we also confirmed that it is possible to generate an obstacle avoidance route using sequential spatial mapping using HoloLens.																	1349-4198	1349-418X				AUG	2020	16	4					1459	1467		10.24507/ijicic.16.04.1459													
J								ROBUST TRACKING OF CATTLE USING SUPER PIXELS AND LOCAL GRAPH CUT FOR MONITORING SYSTEMS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Tracking; ROI; Scribble; SP; LGC; Centroid; VGP (Virtual Grounding Point)	BEHAVIOR	This paper proposes a robust tracking method of Japanese black cattle. Development of a cattle monitoring system using non-contact and non-invasive methods to improve productivity is a strong demand from livestock farmers in aged society. As one of elemental technologies to realize it, we focus on tracking of cattle for detecting estrus behaviors using video camera. The conventional methods like inter-frame difference and background subtraction do not work well under supposed environment. So we propose a new updating method of ROI (Region of Interest) and Scribbles (for foreground and background) according to the movement of the centroid of the extracted cattle region. SP (Super Pixel) and LGC (Local Graph Cut) are adopted for robust cattle region extraction. The tracking without updating soon fails before cattle goes out of frame, but the tracking with the proposed updating has been successfully continued until cattle has gone out. Through the experimental results carried at Sumiyoshi Field attached to Miyazaki University, the effectiveness of the proposed method has been confirmed.																	1349-4198	1349-418X				AUG	2020	16	4					1469	1475		10.24507/ijicic.16.04.1469													
J								PLACEMENT OF SURFACE SENSORS FOR COOPERATIVE LOCALIZATION OF SUBMERGED AUTONOMOUS UNDERWATER VEHICLES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										AUVs; Cooperative localization; Optimal formation; Sensor placement	OPTIMIZATION	Cooperative localization using surface sensors is an effective method for localization of submerged Autonomous Underwater Vehicles (AUVs), especially for long duration and wide area operations. However, placement of surface sensors such that the corresponding sensor formation maximizes the observability of AUVs is a challenging task. In this paper, a new method for obtaining optimal formation of surface sensors has been presented for multiple AUVs cooperative localization system. Most of the real world constraints/situations such as characteristics of acoustic signals in water, multiple-AUVs cooperative localization and computation complexity with the increase in number of AUVs have been considered. Firstly, an evaluation function based on acoustic signal transmission characteristics and Fisher information matrix theory has been formulated for AUVs whose position is known. In order to obtain the optimal formation for a given system, the value of evaluation function must reach its maximum. A recursive optimization algorithm has been used for the solution of the evaluation function. Simulation examples are then presented to calculate optimal formation for single AUV and multi-AUVs systems. Finally, to verify the effectiveness of the proposed method, various simulations have been performed using Extended Kalman Filter (EKF) to compare the localization error between optimal formations and random formations. The simulation results indicate that the optimal formation is able to decrease the noise error and enhance the observability of the system.																	1349-4198	1349-418X				AUG	2020	16	4					1477	1484		10.24507/ijicic.16.04.1477													
J								Improved quality assessment of colour surfaces for additive manufacturing based on image entropy	PATTERN ANALYSIS AND APPLICATIONS										3D prints; Additive manufacturing; Quality assessment; Entropy	DEFECT DETECTION; 3D	A reliable automatic visual quality assessment of 3D-printed surfaces is one of the key issues related to computer and machine vision in the Industry 4.0 era. The colour-independent method based on image entropy proposed in the paper makes it possible to detect and identify some typical problems visible on the surfaces of objects obtained by additive manufacturing. Depending on the quality factor, some of such 3D printing failures may be corrected during the printing process or the operation can be aborted to save time and filament. Since the surface quality of 3D-printed objects may be related to some mechanical or physical properties of obtained objects, its fast and reliable evaluation may also be helpful during the quality monitoring procedures. The method presented in the paper utilizes the assumption of the increase of image entropy for irregularly distorted 3D-printed surfaces. Nevertheless, because of the local nature of distortions, the direct application of the global entropy does not lead to satisfactory results of automatic surface quality assessment. Therefore, the extended method, based on the combination of the local image entropy and its variance with additional colour adjustment, is proposed in the paper, leading to the proper classification of 78 samples used during the experimental verification of the proposed approach.																	1433-7541	1433-755X				AUG	2020	23	3					1035	1047		10.1007/s10044-020-00865-w													
J								Classifying imbalanced data using BalanceCascade-based kernelized extreme learning machine	PATTERN ANALYSIS AND APPLICATIONS										Imbalanced learning; Classification; Kernelized extreme learning machine; BalanceCascade ensemble; Voting methods	CLASSIFICATION; TRENDS; SMOTE	Imbalanced learning is one of the substantial challenging problems in the field of data mining. The datasets that have skewed class distribution pose hindrance to conventional learning methods. Conventional learning methods give the same importance to all the examples. This leads to the prediction inclined in favor of the majority classes. To solve this intrinsic deficiency, numerous strategies have been proposed such as weighted extreme learning machine (WELM) and boosting WELM (BWELM). This work designs a novel BalanceCascade-based kernelized extreme learning machine (BCKELM) to tackle the class imbalance problem more effectively. BalanceCascade includes the merits of random undersampling and the ensemble methods. The proposed method utilizes random undersampling to design balanced training subsets. The proposed ensemble generates the base learner in a sequential manner. In each iteration, the correctly classified examples belonging to the majority class are replaced by the other majority class examples to create a new balanced training subset, i.e., the base learners differ in the choice of the balanced training subset. The cardinality of the balanced training subsets depends on the imbalance ratio. This work utilizes a kernelized extreme learning machine (KELM) as the base learner to build the ensemble as it is stable and has good generalization performance. The time complexity of BCKELM is considerably lower in contrast to BWELM, BalanceCascade, EasyEnsemble and hybrid artificial bee colony WELM. The exhaustive experimental evaluation on real-world benchmark datasets demonstrates the efficacy of the proposed method.																	1433-7541	1433-755X				AUG	2020	23	3					1157	1182		10.1007/s10044-019-00844-w													
J								Ensemble Adaptation Networks with low-cost unsupervised hyper-parameter search	PATTERN ANALYSIS AND APPLICATIONS										Unsupervised domain adaptation; Unsupervised hyper-parameter search; Domain-invariant feature learning	KERNEL	The development of deep learning makes the learning model have more parameters to be learned, and it means that sufficient samples are needed. On the other hand, it is extremely difficult to find tons of labels to support model training process. The existing methods can extend the model to a new domain by looking for domain-invariant features from different domains. In this paper, we propose a novel deep domain adaptation model. Firstly, we try to make a variety of statistics working on high-level feature layers at the same time to obtain better performance. What is more, inspired by the active learning, we propose 'uncertainty' metric to search for hyper-parameters under unsupervised setting. The 'uncertainty' uses entropy to describe the learning status of the current discriminator. The smaller the 'uncertainty', the more stable the discriminator predicts the data. Finally, the network parameters are obtained by fine-tuning a generic pre-trained deep network. As a conclusion, the performance of our algorithm has been further improved over other compared algorithms on standard benchmarks.																	1433-7541	1433-755X				AUG	2020	23	3					1215	1224		10.1007/s10044-019-00846-8													
J								Learning CNN features from DE features for EEG-based emotion recognition	PATTERN ANALYSIS AND APPLICATIONS										EEG classification; Emotion recognition; Convolutional neural network	VIGILANCE ESTIMATION; MACHINE	Recently, deep neural networks (DNNs) have shown the remarkable success of feature representations in computer vision, audio analysis, and natural language processing. Furthermore, DNNs have been used for electroencephalography (EEG) signal classification in recent studies on brain-computer interface. However, most works use one-dimensional EEG features to learn DNNs that ignores the local information within multichannel or multiple frequency bands in the EEG signals. In this paper, we propose a novel emotion recognition method using a convolutional neural network (CNN) while preventing the loss of local information. The proposed method consists of two parts. The first part generates topology-preserving differential entropy features while keeping the distance from the center electrode to other electrodes. The second part learns the proposed CNN to estimate three-class emotional states (positive, neutral, negative). We evaluate our work on SEED dataset, including 62-channel EEG signals recorded from 15 subjects. Our experimental results demonstrate that the proposed method achieved superior performance on SEED dataset with an average accuracy of 90.41% with the visualization of extracted features from the proposed CNN using t-SNE to show our representation outperforms the other representations based on standard features for EEG analysis. Besides, with the additional experiment on VIG dataset to estimate the vigilance of EEG dataset, we show the off-the-shelf availability of the proposed method.																	1433-7541	1433-755X				AUG	2020	23	3					1323	1335		10.1007/s10044-019-00860-w													
J								Segmentation of handwritten words using structured support vector machine	PATTERN ANALYSIS AND APPLICATIONS										S-SVM; Inter-word; Intra-word; SURF descriptors; SO	CORNER DETECTION; TEXT-LINE; CLASSIFICATION	Words and characters segmentation is a most indispensable and fundamental task for the handwritten script recognition. However, the complex language structures, deviation in pen breadth and slant in inscription make the feature extraction process very challenging. In this research, a binary quadratic process has been formulated for the word segmentation. It deliberates a co-relationship between the inter-word gap and intra-word gap. The structured support vector machine is used for the experiment. Experimental results of public datasets (i.e., ICDAR2009 and ICDAR2013) show state-of-the-art performance of the designed algorithm.																	1433-7541	1433-755X				AUG	2020	23	3					1355	1367		10.1007/s10044-019-00843-x													
J								Learning multi-scale features for foreground segmentation	PATTERN ANALYSIS AND APPLICATIONS										Foreground segmentation; Convolutional neural networks; Feature pooling module; Background subtraction; Video surveillance	BACKGROUND SUBTRACTION; NEURAL-NETWORKS	Foreground segmentation algorithms aim at segmenting moving objects from the background in a robust way under various challenging scenarios. Encoder-decoder-type deep neural networks that are used in this domain recently perform impressive segmentation results. In this work, we propose a variation of our formerly proposed method (Anonymous 2018) that can be trained end-to-end using only a few training examples. The proposed method extends the feature pooling module of FgSegNet by introducing fusion of features inside this module, which is capable of extracting multi-scale features within images, resulting in a robust feature pooling against camera motion, which can alleviate the need of multi-scale inputs to the network. Sample visualizations highlight the regions in the images on which the model is specially focused. It can be seen that these regions are also the most semantically relevant. Our method outperforms all existing state-of-the-art methods in CDnet2014 datasets by an average overall F-measure of 0.9847. We also evaluate the effectiveness of our method on SBI2015 and UCSD Background Subtraction datasets. The source code of the proposed method is made available at.																	1433-7541	1433-755X				AUG	2020	23	3					1369	1380		10.1007/s10044-019-00845-9													
J								Learning a representation with the block-diagonal structure for pattern classification	PATTERN ANALYSIS AND APPLICATIONS										Pattern classification; Low-rank and sparse representation; Block-diagonal structure	ROBUST FACE RECOGNITION; LOW-RANK; SPARSE-REPRESENTATION; DISCRIMINATIVE DICTIONARY	Sparse-representation-based classification (SRC) has been widely studied and developed for various practical signal classification applications. However, the performance of a SRC-based method is degraded when both the training and test data are corrupted. To counteract this problem, we propose an approach that learns representation with block-diagonal structure (RBDS) for robust image recognition. To be more specific, we first introduce a regularization term that captures the block-diagonal structure of the target representation matrix of the training data. The resulting problem is then solved by an optimizer. Last, based on the learned representation, a simple yet effective linear classifier is used for the classification task. The experimental results obtained on several benchmarking datasets demonstrate the efficacy of the proposed RBDS method. The source code of our proposed RBDS is accessible at .																	1433-7541	1433-755X				AUG	2020	23	3					1381	1390		10.1007/s10044-019-00858-4													
J								Robust object tracking with crow search optimized multi-cue particle filter	PATTERN ANALYSIS AND APPLICATIONS										Particle filter; CSA; Object tracking; Fusion model	VISUAL TRACKING; IMPOVERISHMENT; CONDENSATION; TARGET	Particle filter is used extensively for estimation of target nonlinear and non-Gaussian state. However, its performance suffers due to its inherent problem of sample degeneracy and impoverishment. In order to address this, we propose a novel resampling method based upon crow search optimization to overcome low performing particles detected as the outlier. Proposed outlier detection mechanism with transductive reliability achieves faster convergence of the proposed PF tracking framework. In addition, we present an adaptive fusion model to integrate multi-cue extracted for each evaluated particle. Automatic boosting and suppression of particles using the proposed fusion model not only enhance the performance of the resampling method but also achieve optimal state estimation. Performance of the proposed tracker has been evaluated over benchmark video sequences and compared with state-of-the-art solutions. Qualitative and quantitative results reveal that the proposed tracker not only outperforms existing solutions but also efficiently handles various tracking challenges. On average of the outcome, we achieve CLE of 10.99 andFmeasure of 0.683.																	1433-7541	1433-755X				AUG	2020	23	3					1439	1455		10.1007/s10044-019-00847-7													
J								PathQuery Pregel: high-performance graph query with bulk synchronous processing	PATTERN ANALYSIS AND APPLICATIONS										Distributed graph compute; Pregel; Graph query; Bulk synchronous parallel computing; Graph database		High-performance graph query systems are a scalable way to mine information in Knowledge Graphs, especially when the queries benefit from a high-level expressive query language. This paper presents techniques to algorithmically compile queries expressed in a high-level language (e.g., Datalog) into a directed acyclic graph query plan and details how these queries can be run on a Pregel graph vertex-centric compute system. Our solution, called PathQuery Pregel, creates plans for any conjunctive or disjunctive queries with aggregation and negation; we describe how the query execution extracts graph results optimally while avoiding many join operations where parallel map execution is permitted. We provide details of how we scaled this system out to execute large set of queries in parallel over the Google Knowledge Graph, a graph of 70B edges, or facts; we describe our production experience with PathQuery Pregel.																	1433-7541	1433-755X				AUG	2020	23	3					1493	1504		10.1007/s10044-019-00841-z													
J								Self-organizing neighborhood-based differential evolution for global optimization	SWARM AND EVOLUTIONARY COMPUTATION										Differential evolution; Neighborhood utilization technique; Self-organizing map; Neighborhood learning; Neighborhood adaption; Mutation operator	MUTATION; ALGORITHM	Combining neighborhood utilization technique (NUT) has shown a tremendous benefit to differential evolution (DE) due to that the acquired neighborhood information of population is of great help in guiding the search. However, in most NUT-based DE algorithms, on the one hand, the neighborhood relationships between individuals cannot be effectively and properly learned, and on the other hand, the search roles of different individuals have not yet been fully considered in the design of the NUT. Therefore, this study develops a novel NUT, termed self-organizing neighborhood (SON), with three features: 1) the neighborhood relationships between individuals are incrementally learned and extracted by self-organizing map with the cosine similarity; 2) the neighborhood sizes for different individuals are adaptively adjusted according to their distinct roles in the search; 3) the evolution direction constructed with the neighborhood of each individual is incorporated into the mutation process to guide the search. By combining SON with DE, a SON-based DE (SON-DE) framework is proposed for global optimization. Experimental results on 58 real-parameter functions and 17 real-world problems have demonstrated the superiority of SON-DE in comparison with several state-of-the-art DE algorithms and evolutionary algorithms (EAs).																	2210-6502	2210-6510				AUG	2020	56								100699	10.1016/j.swevo.2020.100699													
J								A test-suite of non-convex constrained optimization problems from the real-world and some baseline results	SWARM AND EVOLUTIONARY COMPUTATION										Real-world optimization problem; Metaheuristics; Constraint handling technique; Benchmark suite	OPTIMAL PULSEWIDTH MODULATION; DIFFERENTIAL EVOLUTION; STRUCTURAL OPTIMIZATION; GLOBAL OPTIMIZATION; DESIGN; ALGORITHM; INTEGER; STRATEGY	Real-world optimization problems have been comparatively difficult to solve due to the complex nature of the objective function with a substantial number of constraints. To deal with such problems, several metaheuristics as well as constraint handling approaches have been suggested. To validate the effectiveness and strength, performance of a newly designed approach should be benchmarked by using some complex real-world problems, instead of only the toy problems with synthetic objective functions, mostly arising from the area of numerical analysis. A list of standard real-life problems appears to be the need of the time for benchmarking new algorithms in an efficient and unbiased manner. In this study, a set of 57 real-world Constrained Optimization Problems (COPs) are described and presented as a benchmark suite to validate the COPs. These problems are shown to capture a wide range of difficulties and challenges that arise from the real life optimization scenarios. Three state-of-the-art constrained optimization methods are exhaustively tested on these problems to analyze their hardness. The experimental outcomes reveal that the selected problems are indeed challenging to these algorithms, which have been shown to solve many synthetic benchmark problems easily.																	2210-6502	2210-6510				AUG	2020	56								100693	10.1016/j.swevo.2020.100693													
J								Evolution strategies for continuous optimization: A survey of the state-of-the-art	SWARM AND EVOLUTIONARY COMPUTATION										Black-box optimization; Evolution strategies; Covariance matrix adaptation; Evolution path; Natural gradient	COVARIANCE-MATRIX ADAPTATION; STEP-SIZE ADAPTATION; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; SELF-ADAPTATION; CMA-ES; UNCONSTRAINED OPTIMIZATION; CONSTRAINED OPTIMIZATION; ALGORITHMS; SEARCH	Evolution strategies are a class of evolutionary algorithms for black-box optimization and achieve state-of-the-art performance on many benchmarks and real-world applications. Evolution strategies typically evolve a Gaussian distribution to approach the optimum. In this paper, we present a survey of recent advances in evolution strategies. We summarize the techniques, extensions, and practical considerations of evolution strategies for various optimization problems. We discuss some important open questions and promising topics that desire further research. Many of the discussed techniques and principles are applicable to other algorithms.																	2210-6502	2210-6510				AUG	2020	56								100694	10.1016/j.swevo.2020.100694													
J								Efficient parallel and fast convergence chaotic Jaya algorithms	SWARM AND EVOLUTIONARY COMPUTATION										Optimisation; Jaya algorithm; Chaotic map; Parallel algorithms; OpenMP	LEARNING-BASED OPTIMIZATION; DIFFERENTIAL EVOLUTION; FIREFLY ALGORITHM; HYBRID ALGORITHM; SEARCH; SYSTEM; MATRIX; MODEL; TOOL	The Jaya algorithm is a recent heuristic approach for solving optimisation problems. It involves a random search for the global optimum, based on the generation of new individuals using both the best and the worst individuals in the population, thus moving solutions towards the optimum while avoiding the worst current solution. In addition to its performance in terms of optimisation, a lack of control parameters is another significant advantage of this algorithm. However, the number of iterations needed to reach the optimal solution, or close to it, may be very high, and the computational cost can hamper compliance with time requirements. In this work, a chaotic two-dimensional (2D) map is used to accelerate convergence, and parallel algorithms are developed to alleviate the computational cost. Coarse- and fine-grained parallel algorithms are developed, the former based on multi-populations and the latter at the individual level, and in both cases these are accelerated by an improved (computational) use of the chaos map.																	2210-6502	2210-6510				AUG	2020	56								100698	10.1016/j.swevo.2020.100698													
J								A rotationally invariant semi-autonomous particle swarm optimizer with directional diversity	SWARM AND EVOLUTIONARY COMPUTATION										Particle swarm optimization; Global continuous optimization; Rotational invariance; Directional diversity; Rotation matrix	VELOCITY UPDATE RULES; NEURAL-NETWORK; ALGORITHM; CONVERGENCE; COLONY	The semi-autonomous particle swarm optimizer (SAPSO) [1] is a relatively recent algorithm for global continuous optimization based on gradient direction and diversity controlling approach, providing autonomy for the particles and the swarm for exploiting regions in the search space, and preserving exploration during the whole search process. In the first study, although SAPSO algorithm holds the property rotational invariance in which it normally brings a lack of directional diversity in PSO context, the algorithm has shown very good performance in comparison to other PSO-like algorithms. In this paper, an improved version of SAPSO, named rotationally invariant SAPSO (RI-SAPSO), is proposed, which still holds the same property, but now it incorporates a rotation matrix generated by an exponential map to maintain directional diversity. A mathematical proof to prove that the RI-SAPSO algorithm is rotationally invariant is given. RI-SAPSO was evaluated on test functions extracted from CEC 2017 benchmark problems with six other PSO-like algorithms, along with its previous version. The comparative study was strengthened with a non-parametric Friedman's hypothesis test for 1 x k comparisons and p-values were adjusted in the post-hoc procedure. Simulation results showed that the proposed RI-SAPSO, in most problems, was able to find much better solutions and statistical significances were also observed.																	2210-6502	2210-6510				AUG	2020	56								100700	10.1016/j.swevo.2020.100700													
J								A grey prediction-based evolutionary algorithm for dynamic multiobjective optimization	SWARM AND EVOLUTIONARY COMPUTATION										Dynamic multiobjective optimization; Memory-based strategy; Predictive strategy; Grey model	GENETIC ALGORITHMS; MEMORY; IMMIGRANTS; HYBRID	Dynamic multiobjective optimization problems (DMOPs) usually involve multiple conflicting objectives that change over time. A good evolutionary algorithm should be able to quickly track the moving Pareto optimal front (POF) and Pareto optimal set (POS) over time. To solve DMOPs, a predictive method is proposed herein based on grey prediction model, which is composed of three essential ingredients. The first one is that the population is divided into multiple clusters, which can help the population to preserve diversity throughout the evolutionary process. The second one is that the individuals used to detect environmental changes are taken from different clusters, which in turn help the proposed algorithm to detect environmental changes more promptly and accurately. The third one is to build the grey prediction model by using the centroid point of each cluster when detecting the environmental change, and then generate the initial population. Empirical results show that the proposed algorithm can deal with dynamic environments and track the varying POS and POF effectively and efficiently, and achieve better performances on most test problems than several selected state-of-the-art algorithms.																	2210-6502	2210-6510				AUG	2020	56								100695	10.1016/j.swevo.2020.100695													
J								Selective-candidate framework with similarity selection rule for evolutionary optimization	SWARM AND EVOLUTIONARY COMPUTATION										Evolution status; Similarity selection; Exploitation and exploration; Differential evolution (DE); Covariance matrix adaptation evolution strategy (CMA-ES); Particle swarm optimization (PSO); Global optimization	DIFFERENTIAL EVOLUTION; ALGORITHM; PERFORMANCE; PARAMETER; ADAPTATION; ENSEMBLE; MODEL	Achieving better exploitation and exploration capabilities (EEC) have always been an important yet challenging issue in the design of evolutionary optimization algorithm (EOA). The difficulties lie in obtaining a good balance in EEC, which is determined cooperatively by operations and parameters in an EOA. When deficiencies in exploitation or exploration are observed, most existing works consider a piecemeal approach, either by designing new operations or by altering the parameters. Unfortunately, when different situations are encountered, these proposals may fail to be the winner. To address these problems, this paper proposes an explicit EEC control method named selective-candidate framework with similarity selection rule (SCSS). M (M > 1) candidates are first generated from each current solution with independent operations and parameters to enrich the search. Then, a similarity selection rule is designed to determine the final candidate by considering the fitness ranking of the current solution and its Euclidian distance to each of these M candidates. Superior current solutions will prefer the closest candidates for efficient local exploitation while inferior ones will favor the farthest for exploration purpose. In this way, the rule could synthesize exploitation and exploration, making the evolution more effective. When applied to three classic, four state-of-the-art and four up-to-date EOAs from branches of differential evolution, evolution strategy and particle swarm optimization, significant enhancement in performance is achieved.																	2210-6502	2210-6510				AUG	2020	56								100696	10.1016/j.swevo.2020.100696													
J								Dependence space models to construct concept lattices	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Formal concept analysis; Dependence space; Closure operator; Congruence relation; Attribute granule	APPROXIMATE CONCEPT CONSTRUCTION; FORMAL CONCEPT ANALYSIS; ATTRIBUTE REDUCTION; ROUGH SETS	Concept lattice is a kind of efficient mathematical tools for data analysis and knowledge discovery. In this paper, dependence space models are constructed to obtain a concept lattice. For a formal context, a congruence relation is first defined based on a sufficiency operator on the power set of the attribute set. A dependence space is generated. By using the dependence space, we introduce a closure operator. Related properties of the operator are discussed. It is also proved that any closed element of the closure operator is the maximum element in the attribute granule generated by the element. Further, the closed element is just the intension of some formal concept. Similarly, we define a congruence relation on the basis of a sufficiency operator, and introduce a dependence space on the power set of the object set. A closure operator is proposed for the subset of objects. The corresponding closed element of the closure operator is the maximum element in each object granule. All maximum elements generate the set of all extensions of the concept lattice. An example is used to show the validity of the approach to obtain all intensions. Finally, we introduce a dominance congruence relation, which is used to produce a closure operator. The dependence of attributes is defined based on the closure operator. The related decision rules are discussed. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				AUG	2020	123						1	16		10.1016/j.ijar.2020.04.004													
J								Aggregation of experts' opinions and conditional consensus opinion by the Steiner point	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Epsilon contamination capacities; Steiner point; Full Bayesian updating; Multiple priors; Capacities; Consensus distribution	IMPRECISE; JUDGMENTS; FUSION	This paper considers the issue of aggregation of experts' opinions expressed through not necessarily independent and even conflicting probability distributions or multiple priors. The paper addresses the elicitation of the consensus distribution by introducing the Steiner point of the set of all common distributions among experts and derives its conditional probabilities on future events. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				AUG	2020	123						17	25		10.1016/j.ijar.2020.04.005													
J								Depth-bounded Belief functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Belief functions; Uncertain reasoning; Depth-bounded logics; Probability		This paper introduces and investigates Depth-bounded Belief functions, a logic-based representation of quantified uncertainty. Depth-bounded Belief functions are based on the framework of Depth-bounded Boolean logics [4], which provide a hierarchy of approximations to classical logic. Similarly, Depth-bounded Belief functions give rise to a hierarchy of increasingly tighter lower and upper bounds over classical measures of uncertainty. This has the rather welcome consequence that "higher logical abilities" lead to sharper uncertainty quantification. In particular, our main results identify the conditions under which Dempster-Shafer Belief functions and probability functions can be represented as a limit of a suitable sequence of Depth-bounded Belief functions. (C) 2020 The Author(s). Published by Elsevier Inc.																	0888-613X	1873-4731				AUG	2020	123						26	40		10.1016/j.ijar.2020.05.001													
J								Weighted partial order oriented three-way decisions under score-based common voting rules	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decisions; Weighted partial orders; Common voting rules	SETS	Recently, with the trisecting and acting models of human cognitive behaviors, three-way decisions are introduced to deal with uncertain decisions in many applications. To define and conduct the three-way decisions on the possible or necessary loser or co-loser and/or winner or co-winner, we first define the score-based common voting rules, where a score-based function is introduced to evaluate certain utility, including positional scoring rules, maximin, Bucklin, and so on. Then on one hand, it has been proved that under certain conditions three-way decisions on the possible loser or co-loser and/or winner or co-winner are NP-complete with positional scoring rules, maximin, and Bucklin, respectively. On the other hand, we have presented algorithms whose time complexity is polynomial to conduct three-way decisions on the necessary winner or co-winner and/or necessary loser or co-loser with positional scoring rules, maximin, and Bucklin in time O (nm(2)), O (nm(3)), and O (nm(2)), respectively, which has been validated experimentally. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				AUG	2020	123						41	54		10.1016/j.ijar.2020.05.006													
J								Non-linear failure rate: A Bayes study using Hamiltonian Monte Carlo simulation	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Non-linear failure rate; Bayesian estimators; Loss functions; Hamiltonian Monte Carlo; Cross-entropy method; Maximum likelihood estimators	WEIBULL DISTRIBUTION; MODEL	A generalization of the linear failure rate called non-linear failure rate is introduced, analyzed, and applied to real data sets for both censored and uncensored data. The Hamiltonian Monte Carlo and cross-entropy methods have been exploited to empower the traditional methods of statistical estimation. We have obtained the Bayes estimators of parameters and reliability characteristics using Hamiltonian Monte Carlo and these estimators are considered under both symmetric and asymmetric loss functions. Additionally, the maximum likelihood estimators of parameters are obtained by using the cross-entropy method to optimize the log-likelihood function. The superiority of the proposed model and estimation procedures are demonstrated on real data sets adopted from references. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				AUG	2020	123						55	76		10.1016/j.ijar.2020.04.007													
J								New two-sided confidence intervals for binomial inference derived using Walley's imprecise posterior likelihood as a test statistic	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Binomial inference; Belief functions; Possibility distributions; False confidence theorem	CURVES; MODELS	Starting with the Sterne's confidence regions, there have been a string of related attempts to improve on the two-sided Clopper-Pearson bounds for binomial inference. That work is brought to fruition using Walley's imprecise posterior likelihood as the basis for a new test statistic. The results are expressed as a consonant confidence structure, from which a new set of exact confidence intervals for binomial inference can be efficiently computed. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				AUG	2020	123						77	98		10.1016/j.ijar.2020.05.005													
J								A pseudo -dynamic search ant colony optimization algorithm with improved negative feedback mechanism	COGNITIVE SYSTEMS RESEARCH																													2214-4366	1389-0417				AUG	2020	62						1	9		10.1016/j.cogsys.2020.03.001													
J								Robust supervised sparse representation for face recognition	COGNITIVE SYSTEMS RESEARCH											NEURAL-NETWORKS; SIGNAL RECOVERY; REGRESSION; ALGORITHM; FRAMEWORK; SELECTION; FEATURES; MODEL																		2214-4366	1389-0417				AUG	2020	62						10	22		10.1016/j.cogsys.2020.02.001													
J								Facial expression recognition in the wild, by fusion of deep learnt and hand-crafted features	COGNITIVE SYSTEMS RESEARCH																													2214-4366	1389-0417				AUG	2020	62						23	34		10.1016/j.cogsys.2020.03.002													
J								Neuroevolution-based autonomous robot navigation: A comparative study	COGNITIVE SYSTEMS RESEARCH											ANT COLONY OPTIMIZATION; NEURAL-NETWORKS; ALGORITHM																		2214-4366	1389-0417				AUG	2020	62						35	43		10.1016/j.cogsys.2020.04.001													
J								The pattern theory of self in artificial general intelligence: A theoretical framework for modeling self in biologically inspired cognitive architectures	COGNITIVE SYSTEMS RESEARCH										Artificial general intelligence; Self; Biologically inspired cognitive architectures; Global workspace theory; Learning Intelligent Decision Agent (LIDA)	RESPONSES; LIDA	In an attempt to provide a unified account for a vast literature discussing a multiplicity of selves, Gallagher (2013) has proposed a pattern theory of self. Subsequent discussion on this account has led to a concern that the pattern theory, as originally presented, stands as a mere list of aspects that fails to explain how they are related in real-time. We suggest that one way to address these criticisms, and further develop the pattern theory of self is by exploring how it can be used to aid research on self in artificial general intelligence, especially in the context of biologically inspired cognitive architectures. We furthermore propose a conceptual implementation for actualizing such research in regards to the LIDA (Learning Intelligent Decision Agent) cognitive model. (C) 2019 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				AUG	2020	62						44	56		10.1016/j.cogsys.2019.09.018													
J								BICA for AGI q	COGNITIVE SYSTEMS RESEARCH											COMPUTERS; SYNTAX																		2214-4366	1389-0417				AUG	2020	62						57	67		10.1016/j.cogsys.2019.09.020													
J								On strengthening the logic of iterated belief revision: Proper ordinal interval operators	ARTIFICIAL INTELLIGENCE										AGM; Belief revision; Iterated belief change; Iterated revision		Darwiche and Pearl's seminal 1997 article outlined a number of baseline principles for a logic of iterated belief revision. These principles, the DP postulates, have been supplemented in a number of alternative ways. However, most of the suggestions for doing so have been radical enough to result in a dubious 'reductionist' principle that identifies belief states with orderings of worlds. The present paper offers a more modest strengthening of Darwiche and Pearl's proposal. While the DP postulates constrain the relation between a prior and a posterior conditional belief set, our new principles govern the relation between two posterior conditional belief sets obtained from a common prior by different revisions. We show that operators from the family that these principles characterise, which subsumes both lexicographic and restrained revision, can be represented as relating belief states that are associated with a 'proper ordinal interval' assignment, a structure more fine-grained than a simple ordering of worlds. We close the paper by noting that these operators satisfy iterated versions of a large number of AGM era postulates. (C) 2020 Published by Elsevier B.V.																	0004-3702	1872-7921				AUG	2020	285						CP3	U33	103289	10.1016/j.artint.2020.103289													
J								Verification of multi-agent systems with public actions against strategy logic	ARTIFICIAL INTELLIGENCE										Strategy logic; Multi-agent systems; Imperfect information; Verification; Formal methods	BELIEFS	Model checking multi-agent systems, in which agents are distributed and thus may have different observations of the world, against strategic behaviours is known to be a complex problem in a number of settings. There are traditionally two ways of ameliorating this complexity: imposing a hierarchy on the observations of the agents, or restricting agent actions so that they are observable by all agents. We study systems of the latter kind, since they are more suitable for modelling rational agents. In particular, we define multi-agent systems in which all actions are public and study the model checking problem of such systems against Strategy Logic with equality, a very rich strategic logic that can express relevant concepts such as Nash equilibria, Pareto optimality, and due to the novel addition of equality, also evolutionary stable strategies. The main result is that the corresponding model checking problem is decidable. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				AUG	2020	285								103302	10.1016/j.artint.2020.103302													
J								Complexity of abstract argumentation under a claim-centric view	ARTIFICIAL INTELLIGENCE										Abstract argumentation; Computational complexity; Fixed-parameter tractability	GRAPH; SEMANTICS	A Abstract argumentation frameworks have been introduced by Dung as part of an argumentation process, where arguments and conflicts are derived from a given knowledge base. It is solely this relation between arguments that is then used in order to identify acceptable sets of arguments. A final step concerns the acceptance status of particular statements by reviewing the actual contents of the acceptable arguments. Complexity analysis of abstract argumentation so far has neglected this final step and is concerned with argument names instead of their contents, i.e. their claims. As we outline in this paper, this is not only a slight deviation but can lead to different complexity results. We, therefore, give a comprehensive complexity analysis of abstract argumentation under a claim-centric view and analyse the four main decision problems under seven popular semantics. In addition, we also address the complexity of common sub-classes and introduce novel parameterisations - which exploit the nature of claims explicitly - along with fixed-parameter tractability results. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				AUG	2020	285								103290	10.1016/j.artint.2020.103290													
J								Compact and efficient encodings for planning in factored state and action spaces with learned Binarized Neural Network transition models	ARTIFICIAL INTELLIGENCE										Data-driven planning; Binarized Neural Networks; Weighted Partial Maximum Boolean; Satisfiability; Binary Linear Programming		In this paper, we leverage the efficiency of Binarized Neural Networks (BNNs) to learn complex state transition models of planning domains with discretized factored state and action spaces. In order to directly exploit this transition structure for planning, we present two novel compilations of the learned factored planning problem with BNNs based on reductions to Weighted Partial Maximum Boolean Satisfiability (FD-SAT-Plan+) as well as Binary Linear Programming (FD-BLP-Plan+). Theoretically, we show that our SAT-based Bi-Directional Neuron Activation Encoding is asymptotically the most compact encoding relative to the current literature and supports Unit Propagation (UP) - an important property that facilitates efficiency in SAT solvers. Experimentally, we validate the computational efficiency of our Bi-Directional Neuron Activation Encoding in comparison to an existing neuron activation encoding and demonstrate the ability to learn complex transition models with BNNs. We test the runtime efficiency of both FD-SAT-Plan+ and FD-BLP-Plan+ on the learned factored planning problem showing that FD-SAT-Plan+ scales better with increasing BNN size and complexity. Finally, we present a finite-time incremental constraint generation algorithm based on generalized landmark constraints to improve the planning accuracy of our encodings through simulated or real-world interaction. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				AUG	2020	285								103291	10.1016/j.artint.2020.103291													
J								Real-time 3D object proposal generation and classification using limited processing resources	ROBOTICS AND AUTONOMOUS SYSTEMS										Point cloud segmentation; 3D object detection; Optimization; CNN	URBAN ENVIRONMENTS	The task of detecting 3D objects is important in various robotic applications. The existing deep learning-based detection techniques have achieved impressive performances. However, these techniques are limited to being run on a graphics processing unit (GPU) in a real-time environment. To achieve real-time 3D object detection with limited computational resources, we propose an efficient detection method based on 3D proposal generation and classification. The proposal generation is based mainly on point segmentation, while proposal classification is performed by a lightweight convolution neural network (CNN). KITTI datasets are then used to validate our method. It takes only 0.082 s for our method to process one point block with one core of the central processing unit (CPU). In addition to efficiency, the experimental results also demonstrate the capability of the proposed method of producing a competitive performance in object recall and classification. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				AUG	2020	130								103557	10.1016/j.robot.2020.103557													
J								Earth-fixed trajectory and map online estimation: Building on GES sensor-based SLAM filters	ROBOTICS AND AUTONOMOUS SYSTEMS										SLAM; Procrustes problem; Perturbation theory; Mapping; Robotics	SIMULTANEOUS LOCALIZATION; CONSISTENCY; ROBUSTNESS; SEARCH; MOTION; UAV	This paper addresses the problem of obtaining an Earth-fixed trajectory and map (ETM), with the associated uncertainty, using the sensor-based map provided by a globally asymptotically/exponentially stable (GES) SLAM filter. The algorithm builds on an optimization problem with a closed-form solution, and its uncertainty description is derived resorting to perturbation theory. The combination of the algorithm proposed in this paper with sensor-based SLAM filtering results in a complete SLAM methodology, which is directly applied to the three main different formulations: range-and-bearing, range-only, and bearing-only. Simulation and experimental results for all these formulations are included in this work to illustrate the performance of the proposed algorithm under realistic conditions. The ETM algorithm proposed in this paper is truly sensor-agnostic, as it only requires a sensor-based map and imposes no constraints on how this map is acquired nor how egomotion is captured. However, in the experiments presented herein, all the sensor-based filters use a sensor to measure the angular velocity and, for the range-only and bearing-only formulations, a sensor to measure the linear velocity. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				AUG	2020	130								103552	10.1016/j.robot.2020.103552													
J								Kinematic and dynamic manipulability analysis for free-floating space robots with closed chain constraints	ROBOTICS AND AUTONOMOUS SYSTEMS										Cooperative manipulation; Free-floating; Multi-arm space robot; Closed chain system; Manipulability analysis	REDUNDANT; DESIGN; OPTIMIZATION	This paper presents the manipulability analysis of free-floating multi-arm space robots. Evaluation of manipulator capability is useful both in the design and in the operation phase. After capturing a target, closed kinematic chains are formed with multi-arm cooperative manipulating a common object. Owing to the dynamic coupling effect, the manipulability analysis of free-floating systems is more complex than that of ground-fixed closed chain systems. To analyze the cooperative manipulability, kinematic and dynamic formulations for the free-floating closed chain systems are firstly derived. The formulations describe the mapping of joint velocities and torques, respectively, to task velocities and forces, as well as joint torques to task accelerations and forces, by using the generalized Jacobian matrices. Next, the well-known concepts of manipulability ellipsoid, manipulability measure and task compatibility of the free-floating closed chain system are formally extended. Besides, a new approach called scaling factor method is used in the analysis of the task compatibility, which is more accurate compared with the manipulability ellipsoid method. Three applications of the performance indices are considered: (1) the feasibility analysis for a given task, (2) the trajectory planing giving a desired task path, and (3) configuration optimization with different task requirements. The proposed index is proved a very efficient tool that can be utilized in the cooperative manipulation tasks for free-floating space robotic systems. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				AUG	2020	130								103548	10.1016/j.robot.2020.103548													
J								WAGNN: A Weighted Aggregation Graph Neural Network for robot skill learning	ROBOTICS AND AUTONOMOUS SYSTEMS										Skill transfer learning; Serial structures; Robot skill learning; Graph Neural Network		Robotic skill learning suffers from the diversity and complexity of robotic tasks in continuous domains, making the learning of transferable skills one of the most challenging issues in this area, especially for the case where robots differ in terms of structure. Aiming at making the policy easier to be generalized or transferred, the graph neural networks (GNN) was previously employed to incorporate explicitly the robot structure into the policy network. In this paper, with the help of graph neural networks, we further investigate the problem of efficient learning transferable policies for robots with serial structure, which commonly appears in various robot bodies, such as robotic arms and the leg of centipede. Based on a kinematics analysis on the serial robotic structure, the policy network is improved by proposing a weighted information aggregation strategy. It is experimentally shown on different robotics structures that in a few-shot policy learning setting, the new aggregation strategy significantly improves the performance not only on the learning speed, but also on the control accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				AUG	2020	130								103555	10.1016/j.robot.2020.103555													
J								On combining variable ordering heuristics for constraint satisfaction problems	JOURNAL OF HEURISTICS										Constraint satisfaction problem; Variable ordering heuristic; Activity-based search; Dom; wdeg; Impact-based search		Variable ordering heuristics play a central role in solving constraint satisfaction problems. Combining two variable ordering heuristics may generate a more efficient heuristic, such as dom/deg. In this paper, we propose a novel method for combining two variable ordering heuristics, namely Pearson-Correlation-Coefficient-based Combination (PCCC). While the existing combination strategies always combine participant heuristics, PCCC checks whether the participant heuristics are suitable for combination before combining them in the context of search. If they should be combined, it combines the heuristic scores to select a variable to branch on, otherwise, it randomly selects one of the participant heuristics to make the decision. The experiments on various benchmark problems show that PCCC can be used to combine different pairs of heuristics, and it is more robust than the participant heuristics and some classical combining strategies.																	1381-1231	1572-9397				AUG	2020	26	4					453	474		10.1007/s10732-019-09434-9													
J								Timed-image based deep learning for action recognition in video sequences	PATTERN RECOGNITION										Data conditioning; Video analysis; Deep learning; Convolution frames; Hilbert space-filling curve; Action recognition; Violence detection		The paper addresses two issues relative to machine learning on 2D + X data volumes, where 2D refers to image observation and X denotes a variable that can be associated with time, depth, wavelength, etc. The first issue addressed is conditioning these structured volumes for compatibility with respect to convolutional neural networks operating on 2D image file formats. The second issue is associated with sensitive action detection in the "2D + Time" case (video clips and image time series). For the data conditioning issue, the paper first highlights that referring 2D spatial convolution to its 1D Hilbert based instance is highly accurate for information compressibility upon tight frames of convolutional networks. As a consequence of this compressibility, the paper proposes converting the 2D + X data volume into a single meta-image file format, prior to machine learning frameworks. This conversion is such that any 2D frame of the 2D + X data is reshaped as a 1D array indexed by a Hilbert space-filling curve and the third variable X of the initial file format becomes the second variable in the meta-image format. For the sensitive action recognition issue, the paper provides: (i) a 3 category video database involving non-violent, moderate and extreme violence actions; (ii) the conversion of this database into a timed meta-image database from the 2D + Time to 2D conditioning stage described above and (iii) outstanding 2-level and 3-level violence classification results from deep convolutional neural networks operating on meta-image databases. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107353	10.1016/j.patcog.2020.107353													
J								Biclustering with dominant sets	PATTERN RECOGNITION										Biclustering; Dominant set; Replicator dynamics; Prior knowledge	EXPRESSION DATA; ALGORITHMS	Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence. The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107318	10.1016/j.patcog.2020.107318													
J								Robust visual tracking by embedding combination and weighted-gradient optimization	PATTERN RECOGNITION										Visual tracking; Data imbalance; Embedding combination; Weighted-gradient loss	OBJECT TRACKING	Existing tracking-by-detection approaches build trackers on binary classifiers. Despite achieving state-of-the-art performance on tracking benchmarks, these trackers pay limited attention to data imbalance issue, e.g, positive and negative, easy and hard. In this paper, we demonstrate that separately learning feature embeddings corresponding to negative samples with different semantic characteristics is effective in reducing the background diversity to handle the imbalance between positive and negative samples, which facilitates background awareness of classifiers. Specifically, we propose a negative sample embedding combination network, which helps to learn several sub-embeddings and combine them to build a robust classifier. In addition, we propose a weighted-gradient loss to handle the imbalance between easy and hard samples. The gradient contribution of each sample to model training is dynamically weighted according to the gradient distribution, which prevents easy samples from overwhelming model training. Extensive experiments on benchmarks demonstrate that our tracker performs favorably against state-of-the-art algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107339	10.1016/j.patcog.2020.107339													
J								Semi-supervised network embedding with text information	PATTERN RECOGNITION										Network embedding; Structure preserving; Text representation; Stacked auto-encoders		Network embedding plays a pivotal role in network analysis, due to the capability of encoding each node to a low-dimensional dense feature vector. However, most existing network embedding approaches only focus on preserving structural information in the network. The text features and category attributes of nodes are ignored, which are important to network analysis. In this paper, we propose an innovative semi-supervised network embedding (SNE) model integrating structural information, text features and category attributes into embedding vectors simultaneously. Specifically, we design a structure preserving module and a text representation module to capture the global structural information and the text features separately. Meanwhile, a label indicator matrix and a supervised loss are proposed for preserving category information and mapping nodes in the same class closer. We utilize stacked auto-encoders to explore the highly nonlinear characteristics of the network. By optimizing the reconstruction loss and the designed supervised loss jointly in the proposed semi-supervised model, the embedding vectors are finally learned. Extensive experiments on real-world datasets demonstrate that our method is superior to the state-of-the-art baselines in a variety of tasks, including visualization, node classification and clustering. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107347	10.1016/j.patcog.2020.107347													
J								Visual domain adaptation based on modified A-distance and sparse filtering	PATTERN RECOGNITION										Domain adaptation; A-distance; Sparse filtering	EXTREME LEARNING-MACHINE	Domain adaptation studies how to build a robust model to solve pattern recognition problems when training in a source domain while testing in a related but different target domain. The existing methods focus on how to shorten the distance between the two domains, however, they have limited considerations on the preservation of data structures. In this paper, we propose a novel model for unsupervised domain adaptation. For the reduction of domain discrepancy, we propose modified A-distance, which is computationally fast and can be optimized using gradient information. Moreover, in order to capture the internal structures of target samples, within-domain normalization based sparse filtering is raised, which proved to be more powerful for domain adaptation. Extensive experiments compared to both shallow and deep methods demonstrate the effectiveness of our approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107254	10.1016/j.patcog.2020.107254													
J								Multi-label feature selection with shared common mode	PATTERN RECOGNITION										Feature selection; Multi-label learning; Coupled matrix factorization; Non-negative matrix factorization; Classification	MUTUAL INFORMATION; CLASSIFICATION	Multi-label feature selection plays an indispensable role in multi-label learning, which eliminates irrelevant and redundant features while retaining relevant features. Most of existing multi-label feature selection methods employ two strategies to construct feature selection models: extracting label correlations to guide feature selection process and maintaining the consistency between the feature matrix and the reduced low-dimensional feature matrix. However, the data information is described by two data matrices: the feature matrix and the label matrix. Previous methods devote attention to either of the two data matrices. To address this issue, we propose a novel feature selection method named Feature Selection considering Shared Common Mode between features and labels (SCMFS). First, we utilize Coupled Matrix Factorization (CMF) to extract the shared common mode between the feature matrix and the label matrix, considering the comprehensive data information in the two matrices. Additionally, Non-negative Matrix Factorization (NMF) is adopted to enhance the interpretability for feature selection. Extensive experiments are implemented on fifteen real-world benchmark data sets for multiple evaluation metrics, the experimental results demonstrate the classification superiority of the proposed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107344	10.1016/j.patcog.2020.107344													
J								SGM-Net: Skeleton-guided multimodal network for action recognition	PATTERN RECOGNITION										Action recognition; multi-modality; skeleton-guided		Single-modality human action recognition on RGB or skeleton has been extensively studied. Each of these two modalities has its own advantages as well as limitations, because they depict action from different perspectives. The feature of different modalities can complement each other for describing actions. Therefore, it is meaningful to fuse these two modalities using their complementarity for action recognition. However, existing multimodal methods fail to fully exploit the complementarity of RGB and skeleton modalities. In this paper, we propose a Skeleton-Guided Multimodal Network (SGM-Net) for human action recognition. The proposed method takes full use of the complementarity of these two modalities at semantic feature level. From the technical perspective, we introduce a guided block, the key component of SGM-Net. It enables skeleton feature to guide on RGB feature, so that the important RGB information strongly related to the action is enhanced. Moreover, in the guided block, two schemes of correlation operation are explored. We perform a series of ablation experiments to verify the effectiveness of the guided block. The experimental results show that our approach achieves state-of-the-art performance over the existing methods on NTU and Sub-JHMDB datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107356	10.1016/j.patcog.2020.107356													
J								Recognizing actions in images by fusing multiple body structure cues	PATTERN RECOGNITION										Image-based action recognition; Convolutional neural network; Body structure cues		Although Convolutional Neural Networks (CNNs) have made substantial improvements in many computer vision tasks, there remains room for improvements in image-based action recognition due to the limited capability to exploit the body structure information.In this work, we propose a unified deep model to explicitly explore body structure information and fuse multiple body structure cues for robust action recognition in images.In order to fully explore the body structure information, we design the Body Structure Exploration sub-network.It generates two novel body structure cues, Structural Body Parts and Limb Angle Descriptor, which capture structure information of human bodies from the global and local perspectives respectively. And then, we design the Action Classification sub-network to fuse the predictions from multiple body structure cues to obtain precise results. Moreover, we integrate the two sub-networks into a unified model by sharing the bottom convolutional layers, which improves the computational efficiency in both training and testing stages. We comprehensively evaluate our network on the challenging image-based human action datasets, Pascal VOC 2012 Action and Stanford40. Our approach achieves 93.5% and 93.8% mAP respectively, which outperforms all recent approaches in this field. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107341	10.1016/j.patcog.2020.107341													
J								CoCNN: RGB-D deep fusion for stereoscopic salient object detection	PATTERN RECOGNITION										Coupled CNN; Cascaded span network; Stereoscopic images; Salient object detection	VISUAL SALIENCY	Many convolutional neural network (CNN)-based approaches for stereoscopic salient object detection involve fusing either low-level or high-level features from the color and disparity channels. The former method generally produces incomplete objects, whereas the latter tends to blur object boundaries. In this paper, a coupled CNN (CoCNN) is proposed to fuse color and disparity features from low to high layers in a unified deep model. It consists of three parts: two parallel multilinear span networks, a cascaded span network and a conditional random field module. We first apply the multilinear span network to compute multiscale saliency predictions based on RGB and disparity individually. Each prediction, learned under separate supervision, utilizes the multilevel features extracted by the multilinear span network. Second, a proposed cascaded span network, under deep supervision, is designed as a coupling unit to fuse the two feature streams at each scale and integrate all fused features in a supervised manner to construct a saliency map. Finally, we formulate a constraint in the form of a conditional random field model to refine the saliency map based on the a priori assumption that objects with similar saliency values have similar colors and disparities. Experiments conducted on two commonly used datasets demonstrate that the proposed method outperforms previous state-of-the-art methods. (C) 2020 Published by Elsevier Ltd.																	0031-3203	1873-5142				AUG	2020	104								107329	10.1016/j.patcog.2020.107329													
J								Clustering quality metrics for subspace clustering	PATTERN RECOGNITION										Subspace clustering; Clustering validation; Union of subspaces	FACE RECOGNITION	We study the problem of clustering validation, i.e., clustering evaluation without knowledge of ground-truth labels, for the increasingly-popular framework known as subspace clustering. Existing clustering quality metrics (CQMs) rely heavily on a notion of distance between points, but common metrics fail to capture the geometry of subspace clustering. We propose a novel point-to-point pseudometric for points lying on a union of subspaces and show how this allows for the application of existing CQMs to the subspace clustering problem. We provide theoretical and empirical justification for the proposed point-to-point distance, and then demonstrate on a number of common benchmark datasets that our proposed methods generally outperform existing graph-based CQMs in terms of choosing the best clustering and the number of clusters. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107328	10.1016/j.patcog.2020.107328													
J								Anisotropic tubular minimal path model with fast marching front freezing scheme	PATTERN RECOGNITION										Minimal path model; Anisotropy enhancement; Riemannian metric; Path feature; Tubular structures	VESSEL SEGMENTATION; GLOBAL MINIMUM; RETINAL IMAGES; EXTRACTION; CURVES; 2D	In this work, we introduce an anisotropic minimal path model based on a new Riemannian tensor integrating the crossing-adaptive anisotropic radius-lifted tensor field and the front freezing indicator by appearance and path features. The non-local path feature only can be obtained during the geodesic distance computation process by the fast marching method. The predefined criterion derived from path feature is able to steer the front evolution by freezing the point causing high bending of the geodesic to solve the shortcut problem. We performed qualitative and quantitative experiments on synthetic and real images (including retinal vessels, rivers and roads) and compare with the minimal path models with classical anisotropic Riemannian metric and dynamic isotropic metric, which demonstrated the proposed method can detect desired targets from complex tubular tree structures. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107349	10.1016/j.patcog.2020.107349													
J								Point set registration with mixture framework and variational inference	PATTERN RECOGNITION										Point set registration; Image registration; Gaussian variational mixture model; Variational inference		We propose a new point set registration method based on mixture framework and variational inference. A three-phase registration strategy (TRS) is proposed to automatically process point set registration problem in different cases. A Gaussian variational mixture model (GVMM) with isotropic and anisotropic components under the variational inference framework is designed to weaken the effect of outliers. The Dirichlet distribution is applied to govern the mixture proportion of Gaussian components and then distinguishes missing points. We test the performance of our method in contour registration, Graffiti images, retinal images, remote sensing images and 3D human motion, and compare with six state-of-the-art methods. Our method shows favorable performances in most scenarios. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107345	10.1016/j.patcog.2020.107345													
J								Re-ranking image-text matching by adaptive metric fusion	PATTERN RECOGNITION										Image-text matching; Re-ranking method; Adaptive metric fusion		Image-text matching has drawn much attention recently with the rapid growth of multi-modal data. Many effective approaches have been proposed to solve this challenging problem, but limited effort has been devoted to re-ranking methods. Compared with the uni-modal re-ranking methods, modality heterogeneity is the major difficulty when designing a re-ranking method in the cross-modal field, which mainly lies in two aspects of different visual and textual feature spaces and different distributions in inverse directions. In this paper, we propose a heuristic re-ranking method called Adaptive Metric Fusion (AMF) for image-text matching. The method can obtain a better metric by adaptively fusing metrics based on two modules: 1) Cross-modal Reciprocal Encoding, which considers ranks in inverse directions to comprehensively evaluate a metric. The sentence retrieval and image retrieval have different distribution characteristics and galleries in different modalities, thus it is necessary to exploit them simultaneously for appropriate metric fusion. 2) Query Replacement Gap, which quantifies the gap between cross-modal and uni-modal similarities to alleviate the influence of different visual and textual feature spaces on the fused metric. The proposed re-ranking method can be implemented in an unsupervised way without requiring any human interaction or annotated data, and can be easily applied to any initial ranking result. Extensive experiments and analysis validate the effectiveness of our method on the large-scale MS-COCO and Flickr30K datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107351	10.1016/j.patcog.2020.107351													
J								Key protected classification for collaborative learning	PATTERN RECOGNITION										Privacy-preserving machine learning; collaborative learning; classification; generative adversarial networks		Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code will be made available at https://github.com/mbsariyildizikey-protected-classification. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107327	10.1016/j.patcog.2020.107327													
J								End-to-end training of CNN ensembles for person re-identification	PATTERN RECOGNITION										Deep networks; Ensemble learning; Person re-identification	NEURAL-NETWORKS	We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107319	10.1016/j.patcog.2020.107319													
J								Explaining away results in accurate and tolerant template matching	PATTERN RECOGNITION										Template matching; Feature detection; Image matching; Image registration; Correspondence problem; Multi-view vision	PREDICTIVE CODING MODEL; CROSS-CORRELATION; OBJECT DETECTION; HOUGH TRANSFORM; IMAGE; RECOGNITION; ALGORITHMS; NETWORKS; TRACKING	Recognising and locating image patches or sets of image features is an important task underlying much work in computer vision. Traditionally this has been accomplished using template matching. However, template matching is notoriously brittle in the face of changes in appearance caused by, for example, variations in viewpoint, partial occlusion, and non-rigid deformations. This article tests a method of template matching that is more tolerant to such changes in appearance and that can, therefore, more accurately identify image patches. In traditional template matching the comparison between a template and the image is independent of the other templates. In contrast, the method advocated here takes into account the evidence provided by the image for the template at each location and the full range of alternative explanations represented by the same template at other locations and by other templates. Specifically, the proposed method of template matching is performed using a form of probabilistic inference known as "explaining away". The algorithm used to implement explaining away has previously been used to simulate several neurobiological mechanisms, and been applied to image contour detection and pattern recognition tasks. Here it is applied for the first time to image patch matching, and is shown to produce superior results in comparison to the current state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107337	10.1016/j.patcog.2020.107337													
J								Topic modelling for routine discovery from egocentric photo-streams	PATTERN RECOGNITION										Routine; Egocentric vision; Lifestyle; Behaviour analysis; Topic modelling	ACTIVITY RECOGNITION; LIFE	Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individuals' lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine-dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0031-3203	1873-5142				AUG	2020	104								107330	10.1016/j.patcog.2020.107330													
J								Overview of deep-learning based methods for salient object detection in videos	PATTERN RECOGNITION										Deep-learning; Salient object detection; Video	MULTI-PATH; ATTENTION; OPTIMIZATION; NETWORK; FUSION	Video salient object detection is a challenging and important problem in computer vision domain. In recent years, deep-learning based methods have contributed to significant improvements in this domain. This paper provides an overview of recent developments in this domain and compares the corresponding methods up to date, including 1) Classification of the state-of-the-art methods and their frameworks; 2) summary of the benchmark datasets and commonly used evaluation metrics; 3) experimental comparison of the performances of the state-of-the-art methods; 4) suggestions of some promising future works for unsolved challenges. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107340	10.1016/j.patcog.2020.107340													
J								Exponential sparsity preserving projection with applications to image recognition	PATTERN RECOGNITION										Sparsity preserving projection; Dimensionality reduction; Small-sample-size problem; Matrix exponential; Image recognition	NONLINEAR DIMENSIONALITY REDUCTION; FACE; EIGENFACES; MANIFOLD; MATRIX; LPP	Sparsity preserving projection (SPP), as a widely used linear unsupervised dimensionality reduction (DR) method, is designed to preserve the sparse reconstructive relationship of the raw data. SPP constructs an affinity weight matrix by solving a sparse representation model which does not need any parameters. Moreover, the obtained projection may contain some discriminating information even if no prior knowledge is provided. Although SPP may be more conveniently used in practice due to these advantages, it still suffers from the so-called small-sample-size problem as may other DR methods do. To solve this problem, we propose an exponential sparsity preserving projection (ESPP) by using matrix exponential, and present two efficiently numerical methods for solving the corresponding large-scale matrix exponential eigenvalue problem. ESPP avoids the singularity of the coefficient matrices, and obtains more valuable information for the SPP. Image recognition experiments are conducted on several real-world image databases and the experimental results illustrate the outperformances of ESPP. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107357	10.1016/j.patcog.2020.107357													
J								Modality-specific and shared generative adversarial network for cross-modal retrieval	PATTERN RECOGNITION										Cross-modal retrieval; Generative adversarial networks (GAN); Modality-specific feature learning; Modality-shared feature learning		Cross-modal retrieval aims to realize accurate and flexible retrieval across different modalities of data, e.g., image and text, which has achieved significant progress in recent years, especially since generative adversarial networks (GAN) were used. However, there still exists much room for improvement. How to jointly extract and utilize both the modality-specific (complementarity) and modality-shared (correlation) features effectively has not been well studied. In this paper, we propose an approach named Modality-Specific and Shared Generative Adversarial Network (MS(2)GAN) for cross-modal retrieval. The network architecture consists of two sub-networks that aim to learn modality-specific features for each modality, followed by a common sub-network that aims to learn the modality-shared features for each modality. Network training is guided by the adversarial scheme between the generative and discriminative models. The generative model learns to predict the semantic labels of features, model the inter- and intra-modal similarity with label information, and ensure the difference between the modality-specific and modality-shared features, while the discriminative model learns to classify the modality of features. The learned modality-specific and shared feature representations are jointly used for retrieval. Experiments on three widely used benchmark multi-modal datasets demonstrate that MS(2)GAN can outperform state-of-the-art related works. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107335	10.1016/j.patcog.2020.107335													
J								Video semantic segmentation via feature propagation with holistic attention	PATTERN RECOGNITION										Real-time; Attention mechanism; Feature propagation; Video semantic segmentation		Since the frames of a video are inherently contiguous, information redundancy is ubiquitous. Unlike previous works densely process each frame of a video, in this paper we present a novel method to focus on efficient feature propagation across frames to tackle the challenging video semantic segmentation task. Firstly, we propose a Light, Efficient and Real-time network (denoted as LERNet) as a strong backbone network for per-frame processing. Then we mine rich features within a key frame and propagate the across-frame consistency information by calculating a temporal holistic attention with the following non-key frame. Each element of the attention matrix represents the global correlation between pixels of a non-key frame and the previous key frame. Concretely, we propose a brand-new attention module to capture the spatial consistency on low-level features along temporal dimension. Then we employ the attention weights as a spatial transition guidance for directly generating high-level features of the current non-key frame from the weighted corresponding key frame. Finally, we efficiently fuse the hierarchical features of the non-key frame and obtain the final segmentation result. Extensive experiments on two popular datasets, i.e. the CityScapes and the CamVid, demonstrate that the proposed approach achieves a remarkable balance between inference speed and accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107268	10.1016/j.patcog.2020.107268													
J								GAN-based person search via deep complementary classifier with center-constrained Triplet loss	PATTERN RECOGNITION										Person search; Re-Identification; Pedestrian detection		This paper addresses the person search task, which is a computer vision technology that finds the location of a pedestrian and retrieves it in a video taken by a single camera or multiple cameras. This task is much more challenging than the conventional settings for person re-identification or pedestrian detection since the search is susceptible to factors such as different resolutions, similar pedestrians, lighting, viewing angles and occlusion. Moreover, the person search task is a typical big data-small sample problem because each pedestrian only has a few images. It is difficult for the model to learn the discriminant features of pedestrians with a small quantity of pedestrian data. This paper proposes a framework for person search that uses the original training set without collecting extra data by implementing a generative adversarial network (GAN) to generate unlabeled samples. We propose a deep complementary classifier for pedestrian detection to leverage complementary object regions for pedestrian/non-pedestrian classification. In the re-identification section, we propose a center-constrained triplet loss that avoids the complicated triplet selection of the triplet loss and simultaneously pushes away all the distances of rather similar negative centers and the positive center. Experiments show that the GAN-generated data can effectively help to improve the discriminating ability of the CNN model. On the two large-scale datasets, CUHK-SYSU and PRW, we achieve a performance improvement over the baseline CNN. We additionally apply the proposed center-constrained triplet loss and complementary classifiers in the training model, and we achieve mAP improvements over the original method of +1.9% on CUHK-SYSU and +2.5% on PRW. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107350	10.1016/j.patcog.2020.107350													
J								Multi-focus image fusion based on non-negative sparse representation and patch-level consistency rectification	PATTERN RECOGNITION										Multi-focus image fusion; Non-negative sparse representation; Compact non-negative dictionary construction; Patch-level consistency rectification; High computational efficiency	PERFORMANCE; ALGORITHM; DECOMPOSITION	Most existing sparse representation-based (SR) fusion methods consider the local information of each image patch independently during fusion. Some spatial artifacts are easily introduced to the fused image. A sliding window technology is often employed by these methods to overcome this issue. However, this comes at the cost of high computational complexity. Alternatively, we come up with a novel multi-focus image fusion method that takes full consideration of the strong correlations among spatially adjacent image patches with NO need for a sliding window. To this end, a non-negative SR model with local consistency constraint (CNNSR) on the representation coefficients is first constructed to encode each image patch. Then a patch-level consistency rectification strategy is presented to merge the input image patches, by which the spatial artifacts in the fused images are greatly reduced. As well, a compact non-negative dictionary is constructed for the CNNSR model. Experimental results demonstrate that the proposed fusion method outperforms some state-of-the art methods. Moreover, the proposed method is computationally efficient, thereby facilitating real-world applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107325	10.1016/j.patcog.2020.107325													
J								Deep imitator: Handwriting calligraphy imitation via deep attention networks	PATTERN RECOGNITION										Calligraphy imitation; Attention; Mata-style matrix; Condition gated recurrent unit	INDEPENDENT WRITER IDENTIFICATION; VERIFICATION; GENERATION; FEATURES; MODEL	Calligraphy imitation (CI) from a handful of target handwriting samples is such a challenging task that most of the existing writing style analysis or handwriting generation methods do not exhibit satisfactory performance. In this paper, we propose a novel multi-module framework to address the problem of CI. Firstly, we utilized a deep convolution neural network (CNN) to extract personalized calligraphical features. Then we built a calligraphy-clustering attention module and a mata-style matrix (msM) to compute an embedding of calligraphy. The structure of conditional gated recurrent unit (cGRU) is then improved to predict the probabilistic density of pen tip movement displacement by dual condition inputs. Finally, we generated personalized handwriting stroke sequences through iterative sampling with Gaussian mixture model (GMM). Experiments on public online handwriting databases verify that the proposed method could achieve satisfactory performance; the generated samples achieved high similarities with original handwriting examples. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				AUG	2020	104								107080	10.1016/j.patcog.2019.107080													
J								Remote sensing image segmentation using geodesic-kernel functions and multi-feature spaces	PATTERN RECOGNITION										Remote sensing; Image segmentation; Riemannian manifold; Manifold projection; Kernel function	FUZZY; ALGORITHM	Image representation is the key factor influencing the accuracy of remote sensing image segmentation. Traditional algorithms rely on the pixel-wise characteristics exhibited in the feature space. They introduce spatial information by establishing the connections between neighboring pixels in the neighborhood system. But the spectral-spatial features cannot be well expressed. In this paper, a Riemannian manifold space is introduced to express the contextual information by jointly mapping the spectral features of a pixel and its neighboring ones on to it. To benefit from the expression ability and geometric properties of the Riemannian manifold, a data submanifold and a parameter submanifold are established to depict the characteristics of the detected image and all possible segmentation results. On the parameter submanifold, only points representing objects of the current segmentation are active. Then distance between a point on the data submanifold and an active point on the parameter submanifold is measured by a geodesic-kernel function. Consequently, four geodesic-kernel function-based manifold projection criteria are proposed to explore the complementation between features expressed in different feature spaces. Experiments on synthetic and real remote sensing images demonstrated that the proposed geodesic-kernel function-based manifold projection algorithm outperforms traditional ones and features expressed in different feature spaces did contain some complementary information. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107333	10.1016/j.patcog.2020.107333													
J								Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification	PATTERN RECOGNITION										Person re-identification; Sparse ranking; Joint graph regularization	RECOGNITION	The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				AUG	2020	104								107352	10.1016/j.patcog.2020.107352													
J								Moving horizon estimation meets multi-sensor information fusion: Development, opportunities and challenges	INFORMATION FUSION										Moving horizon estimation; State estimation; Filtering; Dynamical systems; Performance analysis; Multi-sensor information fusion	NETWORKED CONTROL-SYSTEMS; DISCRETE-TIME-SYSTEMS; CONSTRAINED STATE ESTIMATION; NONLINEAR-SYSTEMS; MULTIRATE SYSTEMS; LINEAR-SYSTEMS; ARRIVAL COST; ATTITUDE ESTIMATION; PREDICTIVE CONTROL; VARYING SYSTEMS	Since the proposal of moving horizon (MH) estimation in 1960s, the MH estimation approach has drawn ever-increasing research interests due mainly to its inherent capability of handling complex nonlinear systems and constrained systems. Recent years have witnessed considerable progress on the theoretical and practical research of MH estimation. In this work, a bibliographical review is provided on the moving horizon estimation problem and its applications. The basic idea of MH estimation is first introduced in detail. Then recent advances of MH estimation according to the underlying systems are summarized. Furthermore, some applications of MH estimation are presented. Finally, some research challenges of MH estimation problem are outlined for the further research.																	1566-2535	1872-6305				AUG	2020	60						1	10		10.1016/j.inffus.2020.01.009													
J								Recursive fusion estimation for stochastic discrete time-varying complex networks under stochastic communication protocol: The state-saturated case	INFORMATION FUSION										Discrete-time complex networks; Recursive state estimator; Stochastic communication protocol; State-saturated systems; Time-varying systems	RANDOMLY OCCURRING NONLINEARITIES; ARTIFICIAL NEURAL-NETWORKS; MARKOVIAN JUMP SYSTEMS; H-INFINITY CONTROL; STABILITY ANALYSIS; DELAYS; SYNCHRONIZATION; OPTIMIZATION; INFORMATION; DESIGN	In this paper, we investigate the recursive fusion estimation problem for time-varying state-saturated complex networks under stochastic communication protocol (SCP). To cater for physical limitations of network components, the phenomenon of state saturations is taken into account in the complex network model. The underlying communication mechanism is to ensure that just one sensor node is permitted to send its collected measurement at each time, and the SCP determines the permission to use the network channel for each sensor at each transmission time. A key issue of the addressed problem is to construct a time-varying state estimator such that an upper bound is guaranteed on the filtering error covariance subjected to both the state saturations and the SCP. By applying two sets of matrix difference equations, we first derive an upper bound according to the error covariance of the state estimation and then minimize such an upper bound by precisely calculating the estimator parameters. Then, the performance analysis of the obtained state estimator is given in terms of the boundedness. Finally, we provide a simulation example to illustrate the validity of the designed state estimator.																	1566-2535	1872-6305				AUG	2020	60						11	19		10.1016/j.inffus.2020.01.012													
J								Predicting and ranking box office revenue of movies based on big data	INFORMATION FUSION										Feature learning; Data fusion; Dynamic network embedding; Heterogeneous information network; Big data	PERFORMANCE; SUCCESS; MODEL	Predicting box office revenue (BOR) of movies before releasing on big screens successfully becomes an emerging need, as it informs investment decisions on the stock market, the design of promotion strategies by advertisement companies, movie scheduling by cinemas, etc. However, the task is very challenging as it is affected by a lot of complex factors. In this paper, we first provide a strategic investigation of these influential factors. Then, we put forward a novel framework to predict a movie's BOR by modeling these factors using big data. Specifically, the framework consists of a series of feature learning models and a prediction and ranking model. In particular, there are two models devised for learning features: (1) a novel dynamic heterogeneous network embedding model to simultaneously learn latent representations of actors, directors, and companies, capable of capturing their cooperation relationship collectively; (2) a deep neural network-based model designed to uncover high-level representations of movie quality from trailers. Based on the learned features, we train a mutually-enhanced prediction and ranking model to obtain the BOR prediction results. Finally, we apply the framework to the Chinese film market and conduct a comprehensive performance evaluation using real-world data. Experimental results demonstrate the superior performance of both extracted knowledge and the prediction results.																	1566-2535	1872-6305				AUG	2020	60						25	40		10.1016/j.inffus.2020.02.002													
J								Fusing wearable and remote sensing data streams by fast incremental learning with swarm decision table for human activity recognition	INFORMATION FUSION										Kinect depth sensor; Wearable sensor; Data mining; Classification model; Feature selection		Human activity recognition (HAR) by machine learning finds wide applications ranging from posture monitoring for healthcare and rehabilitation to suspicious or dangerous actions detection for security surveillance. Infrared cameras such as Microsoft Kinect and wearable sensors have been the two most adopted devices for collecting data for measuring the bodily movements. These two types of sensors generally are categorized as contactless sensing and contact sensing respectively. Due to hardware limitation, each of the two sensor types has their inherent limitations. One most common problem associating with contactless sensing like Kinect is the distance and indirect angle between the camera and the subject. For wearable sensor, it is limited in recognizing complex human activities. In this paper, a novel data fusion framework is proposed for combining data which are collected from both sensors with the aim of enhancing the HAR accuracy. Kinect is able to capture details of bodily movements from complex activities, but the accuracy is dependent heavily on the angle of view; wearable sensor is relatively primitive in gathering spatial data but reliable for detecting basic movements. Fusing the data from the two sensor types enables complimenting each other by their unique strengths. In particular, a new scheme using incremental learning with decision table coupled with swarm-based feature selection is proposed in our framework for achieving fast and accurate HAR by fusing data of two sensors. Our experiment results show that HAR accuracy could be improved from 23.51% to 68.35% in a case of almost 90 degrees slanted view of Kinect sensing while a wearing sensor is used at the same time. The swarm feature selection in general is shown to enhance the HAR performance compared to standard feature selection method. The experiment results reported here contribute to the possibilities of using hybridized sensors from the machine learning perspective.																	1566-2535	1872-6305				AUG	2020	60						41	64		10.1016/j.inffus.2020.02.001													
J								An overview on feedback mechanisms with minimum adjustment or cost in consensus reaching in group decision making: Research paradigms and challenges	INFORMATION FUSION										Group decision making; Consensus reaching process; Feedback mechanism; Minimum adjustment; Minimum cost; Optimization model	PERSONALIZED INDIVIDUAL SEMANTICS; FUZZY PREFERENCE RELATIONS; SOCIAL NETWORK; FUSION PROCESS; MODEL; CONSISTENCY; CONTEXT; ASSESSMENTS; FRAMEWORK; RULES	Consensus reaching process is a very powerful decision tool to eliminate the preference conflict in group decision making. In general, the consensus is achieved by the decision makers modifying their preferences (or opinions) toward a point of mutual consent, and the feedback mechanism aims to provide preference-modifications suggestions. In many situations, the preference-modifications mean cost and the resources for the consensus reaching process are limited. So, in the last decade, the feedback mechanism with minimum adjustment or cost (FMMA/C) has been developed and widely used in various group decision making contexts to improve consensus efficiency. In this review, we first analyze the origin and basic research paradigm of the FMMA/C. Then, we review the FMMA/C in two decision contexts: (1) the FMMA/C in classical group decision making problems, and (2) the FMMA/C in complex group decision making problems (e.g., social network, large-scale, and opinion dynamic group decision making problems). Finally, we identify research challenges and propose future research direction.																	1566-2535	1872-6305				AUG	2020	60						65	79		10.1016/j.inffus.2020.03.001													
J								Extended Kalman filtering subject to random transmission delays: Dealing with packet disorders	INFORMATION FUSION										Nonlinear systems; recursive filtering scheme; extended Kalman filter; random transmission delays; packet disorders	TIME-VARYING SYSTEMS; STATE ESTIMATION; COMPLEX NETWORKS; PARTIAL-NODES; NONLINEARITIES; CHANNELS	This paper studies the extended Kalman filtering problem for a class of nonlinear discrete-time systems with random transmission delays (RTDs) and RTD-induced packet disorders. The relationship between the RTDs and the resulting packet disorders is discussed. The bounded RTDs, which take place in the sensor-to-filter channel, are modeled as independent and identically distributed random variables obeying a certain probability distribution. A novel filter structure is proposed that utilizes an integer-valued function of the mathematical expectation of the RTDs so as to compensate the RTD-induced effects. Under the proposed extended Kalman filter, an upper bound for the filtering error covariance is derived by solving two Riccati-like difference equations, and subsequently minimized (in the sense of trace) by appropriately designing the filter gains. A numerical simulation is provided to verify the validity of the developed filter design scheme.																	1566-2535	1872-6305				AUG	2020	60						80	86		10.1016/j.inffus.2020.02.006													
J								Processes and methods of information fusion for ranking products based on online reviews: An overview	INFORMATION FUSION										Online reviews; Product ranking; Feature extraction; Sentiment analysis; Information fusion	WORD-OF-MOUTH; SENTIMENT ANALYSIS; FEATURE-EXTRACTION; CUSTOMER REVIEWS; CHINESE REVIEWS; BIG DATA; CLASSIFICATION; MODEL; INTELLIGENCE; ANALYTICS	Over the past few years, more and more consumers have come to read online reviews when they shop online. To support consumers' purchase decisions, many scholars focus on ranking products based on online reviews and propose various methods and techniques. Generally, the process of information fusion for ranking products based on online reviews consists of three stages: product feature extraction, sentiment analysis, and ranking products. In this paper, we review the existing studies on processes and methods of information fusion for each stage. Furthermore, we briefly review the existing research on information fusion based on online reviews in other fields. Finally, we summarize the main conclusions of this paper and point out the future research direction.																	1566-2535	1872-6305				AUG	2020	60						87	97		10.1016/j.inffus.2020.02.007													
J								Social network community analysis based large-scale group decision making approach with incomplete fuzzy preference relations	INFORMATION FUSION										Social network; Community analysis; Large-scale group decision making; Consensus; Fuzzy preference relation	CONSENSUS REACHING PROCESS; MODEL; INFORMATION; TRUST; PROPAGATION; CONSISTENCY; CONFIDENCE; CENTRALITY	Large-scale group decision making (LSGDM) is characterized by a large number of participators, multiple groups and a mass of decision data provided by the participators. With the development of social media and e-democracy technologies, the social relationships among group decision makers should be taken into consideration when we address LSGDM problems with incomplete fuzzy preference relations. In this paper, a social network community detection approach of social networks based on the fuzzy clustering method is proposed. Then, a method of repairing incomplete fuzzy preference relations based on the divided communities is proposed. Moreover, we have proposed a two-stage consensus reaching method to balance the number of iterations and modification range of original decision information, in which the group closeness centrality of community is applied to measure its importance. Also, the procedure for LSGDM with fuzzy preference relations based on social network community analysis is established. The feasibility and advantages of the proposed method for LSGDM based on social network community analysis are illustrated by an illustrative example of a flexible manufacturing system.																	1566-2535	1872-6305				AUG	2020	60						98	120		10.1016/j.inffus.2020.02.005													
J								Multi-classifier information fusion in risk analysis	INFORMATION FUSION										Structural health monitoring; Support vector machine; D-s evidence theory; Risk analysis; Global sensitivity analysis	SUPPORT VECTOR MACHINE; SHAFER EVIDENCE THEORY; SAFETY RISK; ARTIFICIAL-INTELLIGENCE; PROBABILISTIC APPROACH; FAULT-DIAGNOSIS; SHIELD TUNNEL; EXCAVATION; VIBRATION; PROJECTS	This paper develops a novel multi-classifier information fusion approach that integrates the probabilistic support vector machine (SVM) and the improved Dempster-Shafer (D-S) evidence theory to support risk analysis under uncertainty. Safety levels for various risk factors can be classified separately using the probabilistic SVM. Then, these multiple classification results will be fused at the decision level to achieve an overall risk evaluation by an improved D-S evidence theory with the integration of the Dempster' rule and the weighted average rule. The Monte Carlo simulation approach is employed to model the randomness and uncertainty underlying limited observations. A global sensitivity analysis is performed to identify the most significant factors contributing to the risk event. A realistic operational tunnel case in China is used to demonstrate the feasibility and effectiveness of the developed approach, aiming to assess the magnitude of the structural health risk. Results indicate the developed SVM-DS approach is capable of (1) Fusing multi-classifier information effectively from different SVM models with a high classification accuracy of 97.14%; (2) Performing a strong robustness to bias, which can achieve acceptable classification accuracy even under a 20% bias; and (3) Exhibiting a more outstanding classification performance (87.99% accuracy) than the single SVM model (63.84% accuracy) under a high bias (20%). Since the proposed reliable risk analysis method can efficiently fuse multi-sensory information with ubiquitous uncertainties, conflicts, and bias, it provides in-depth analysis for structural health status together with the most critical risk factors, and then proper remedial actions can be taken at an early stage.																	1566-2535	1872-6305				AUG	2020	60						121	136		10.1016/j.inffus.2020.02.003													
J								Semantic-k-NN algorithm: An enhanced version of traditional k-NN algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Semantic itemization; Bigram model; Big data analytics; Semantic-kNN; Machine learning	CLASSIFICATION	The k-NN algorithm is one of the most renowned ML algorithms widely used in the area of data classification research. With the emergence of big data, the performance and the efficiency of the traditional k-NN algorithm is fast becoming a critical issue. The traditional k-NN algorithm is inefficient to solve the high volume multi-categorical training datasets Traditional k-NN algorithm has a constraint in filtering the training dataset to yield training data that are most relevant to the intended or the targeted test dataset/file. It has to scan through all the training datasets categories to classify the intended/targeted data. As such, traditional k-NN is considered not intelligent and consequently is suffering poor accuracy performance with high computational complexity. A Semantic-kNN (Sic-NN) algorithm for ML is thus proposed in this paper to address the limitations in the traditional k-NN. The proposed Sk-NN deploys a process by leveraging on the semantic itemization and bigram model to filter the training dataset in accordance with the relevant information engaged in the test dataset. It is aimed for general security applications such as finding (the confidentiality level of the data when the algorithm is trained with multiple training categories during the data classification phase. Ultimately, Sk-NN is to elevate the ML performance in pattern extraction and labeling in the big data context. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113374	10.1016/j.eswa.2020.113374													
J								Geographic-aware collaborative filtering for web service recommendation	EXPERT SYSTEMS WITH APPLICATIONS										Recommendation; Location; Topic model; Implicit feedback; Matrix factorization	QOS PREDICTION; CONTEXT; SYSTEMS	The explosion of reusable Web services (e.g., open APIs, open data sources, and cloud/IoT services), has become a new opportunity for modern service-composition based applications development. However, this enormous growth of Web services increases the difficulty of selecting the best suitable Web services for a particular application. Hence, the design of an effective and efficient Web service recommendation, primarily based on user feedback, has become a challenge. In the mashup-API recommendation scenario, the most available feedback is the implicit invocation data, i.e., the binary data indicating whether or not a mashup has invoked an API. Various efforts are exploiting potential impact factors, such as the invocation context, to augment the implicit invocation data with the aim to improve service recommendation performance. One significant factor affecting the context of Web service invocations is geographical location, but it has been given less attention in the implicit-based service recommendation. In this paper, we propose a probabilistic matrix factorization based recommendation approach, which considers geographic location information in the derivation of the preference degree underlying a mashup-API interaction. The geographic information, which is integrated with functional descriptions, complements the mashup-API invocation data input for our matrix factorization model. We demonstrate the effectiveness of our approach by conducting extensive experiments on a real dataset crawled from ProgrammableWeb. The evaluation results show that augmenting the implicit data with geographical location information increases the precision of API recommendation for mashup services. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113347	10.1016/j.eswa.2020.113347													
J								Scalable auto-encoders for gravitational waves detection from time series data	EXPERT SYSTEMS WITH APPLICATIONS										Time series classification; Anomaly detection; Feature extraction; Deep neural networks; Machine learning; Big data analytics; Apache spark; Hadoop	CLASSIFICATION; ENSEMBLE; POWER	Gravitational waves represent a new opportunity to study and interpret phenomena from the universe. In order to efficiently detect and analyze them, advanced and automatic signal processing and machine learning techniques could help to support standard tools and techniques. Another challenge relates to the large volume of data collected by the detectors on a daily basis, which creates a gap between the amount of data generated and effectively analyzed. In this paper, we propose two approaches involving deep auto-encoder models to analyze time series collected from Gravitational Waves detectors and provide a classification label (noise or real signal). The purpose is to discard noisy time series accurately and identify time series that potentially contain a real phenomenon. Experiments carried out on three datasets show that the proposed approaches implemented using the Apache Spark framework, represent a valuable machine learning tool for astrophysical analysis, offering competitive accuracy and scalability performances with respect to state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113378	10.1016/j.eswa.2020.113378													
J								Classification methods for planar shapes	EXPERT SYSTEMS WITH APPLICATIONS										Supervised Learning; Landmarks; Pre-shapes; Kernel; Classifier; SVM		The goal of this paper is to propose supervised learning methods considering two-dimensional landmarks (planar shapes). We introduce a novel method based on a support vector machine (SVM) algorithm, which had to be adapted to complex vectors. We also propose other novel methods based on density estimation, kernel k-means, and hill-climbing. Combinations between the classifiers using the ensemble method are considered as well. Furthermore, we compared the proposed methods to the existing Bayes discriminant approach. We conducted simulation experiments to evaluate the performance of the proposed methods. The numerical results prove that for low concentrated data sets, the SVM algorithm outperforms the other methods. Moreover, four real-world data sets are considered: gorilla skulls, orangutan skulls, mouse vertebrae, and schizophrenia. These data sets present different configurations, such as several landmarks and variability. The proposed SVM method achieved the best performance in three data sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113320	10.1016/j.eswa.2020.113320													
J								Selective Opposition based Grey Wolf Optimization	EXPERT SYSTEMS WITH APPLICATIONS										Grey Wolf Optimizer; Opposition-based Learning; Spearman's coefficient; Selective opposition	METAHEURISTIC ALGORITHM; INSPIRED ALGORITHM; SEARCH	The use of metaheuristics is widespread for optimization in both scientific and industrial problems due to several reasons, including flexibility, simplicity, and robustness. Grey Wolf Optimizer (GWO) is one of the most recent and popular algorithms in this area. In this work, opposition-based learning (OBL) is combined with GWO to enhance its exploratory behavior while maintaining a fast convergence rate. Spearman's correlation coefficient is used to determine the omega (omega) wolves (wolves with the lowest social status in the pack) on which to perform opposition learning. Instead of opposing all the dimensions in the wolf, a few dimensions of the wolf are selected on which opposition is applied. This assists with avoiding unnecessary exploration and achieving a fast convergence without deteriorating the probability of finding optimum solutions. The proposed algorithm is tested on 23 optimization functions. An extensive comparative study demonstrates the superiority of the proposed method. The source code for this algorithm is available at "https://github.com/dhargupta-souvik/sogwo" (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113389	10.1016/j.eswa.2020.113389													
J								Light field reconstruction using hierarchical features fusion	EXPERT SYSTEMS WITH APPLICATIONS										Light field; Deep learning; Neural network; Image processing	DEPTH	Light field imagery has attracted increasing attention for its capacity of simultaneously capturing intensity values of light rays from multiple directions. Such imagery technique has become widely accessible with the emergence of consumer-grade devices, e.g. Lytro, and the Virtual Reality (VR) / Augmented Reality (AR) areas. Light field reconstruction is a critical topic to mitigate the trade-off problem between the spatial and angular resolutions. Learning-based methods have attained outstanding performance among the recently proposed methods, however, the state-of-the-art methods still suffer from heavy artifacts in the case of occlusion. This is likely to be a consequence of failure in capturing the semantic information from the limited spatial receptive field during training. It is crucial for light field reconstruction to learn semantic features and understand a wider context in both the angular and spatial dimensions. To address this issue, we introduce a novel end-to-end U-Net with SAS network (U-SAS-Net) to extract and fuse hierarchical features, both local and semantic, from a relatively large receptive field while establishing the relation of the correlated sub-aperture images. Experimental results on extensive light field datasets demonstrate that our method produces a state-of-the-art performance that exceeds the previous works by more than 0.6 dB PSNR with the fused hierarchical features, especially the semantic features for handling scenes with occlusion and the local features for recovering the rich details. Meanwhile, our method is at a substantially lower cost which takes 48% parameters and less than 10% computation of the previous state-of-the-art method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113394	10.1016/j.eswa.2020.113394													
J								GUI information-based interaction logging and visualization for asynchronous usability testing	EXPERT SYSTEMS WITH APPLICATIONS										Usability testing; Remote usability testing; Asynchronous remote usability testing; Mobile application		Asynchronous usability testing is efficient and promising usability testing methodology. However, evaluators are required to dedicate a considerable amount of effort for identifying usability problems in asynchronous testing. Evaluators cannot observe user interactions directly; only the collected data is available to them for identifying the usability problems. To reduce the amount of effort required, this paper presents a framework and tool for interaction visualization. This framework tracing user interactions based on meaningful GUI changes. The collected interaction log is visualized in a way that is similar to the visualization of traversed path used in web domains. Using our approach, the evaluator shall be able to understand user behavior more easily. In addition, we designed a black-box-based interaction logging for minimizing the implementation costs. Our tool especially helpful when testing has a large number of interaction data to analyze. To evaluate the proposed framework and tool, user interaction data were collected, and the results were evaluated by mobile application experts. The evaluation results demonstrate that the user interaction data traced by the proposed framework accurately reflect the user interactions and that the visualization results are valid. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				AUG 1	2020	151								113289	10.1016/j.eswa.2020.113289													
J								Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model	EXPERT SYSTEMS WITH APPLICATIONS										Clickbait; News; Classifier; Features; Social media		Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24%, 95.64%, 95.87% precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113350	10.1016/j.eswa.2020.113350													
J								A crossover operator for improving the efficiency of permutation-based genetic algorithms	EXPERT SYSTEMS WITH APPLICATIONS										Genetic algorithms; Chromosome representation; Crossover operators; Combinatorial optimisation problems; Traveling salesman problem	TRAVELING SALESMAN PROBLEM; GRAPH	Crossover is one of the most important operators in a genetic algorithm by which offspring production for the next generation is performed. There are a number of crossover operators for each type of chromosome representation of solutions that are closely related to different types of optimisation problems. Crossover operation in genetic algorithms, aimed at solving permutation-based combinatorial optimisation problems, is more computationally expensive compared to other cases. This is mainly caused by the fact that no duplicate numbers are allowed in a chromosome and therefore offspring legalisation is needed after each substring exchange. Under these conditions, the time required for performing crossover operation increases significantly with increasing chromosome size, which may deeply affect the efficiency of these genetic algorithms. In this paper, a genetic algorithm that uses path representation for chromosomes and benefits from an alternative form of the well-known partially mapped crossover is proposed. The results of numerical experiments performed on a set of benchmark problems clearly show that the use of this crossover operator can significantly increase the efficiency of permutation-based genetic algorithms and also help in producing good quality solutions. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113381	10.1016/j.eswa.2020.113381													
J								Automatic segmentation of whole-slide H&E stained breast histopathology images using a deep convolutional neural network architecture	EXPERT SYSTEMS WITH APPLICATIONS										Breast cancer; Segmentation; Deep learning; H&E staining; Whole-Slide Imaging	CANCER; DIAGNOSIS; WOMEN	The segmentation of malignant breast tissue from histological images represents a crucial task for the diagnosis of breast cancer (BC). This is a time-consuming process that could be alleviated with the help of computerized segmentation methods, leading to elevated precision and reproducibility results. However, this automated segmentation poses a challenge due to the large size of histological whole-slide images and the significant variability, heterogeneity and complexity of features in them. In this research, we propose a processing pipeline for the automatic segmentation of stained BC images presenting different types of histopathological patterns. To deal with the gigantic size of whole-slide images, the digital preparations were processed in a tile-wise manner: a large part of the image is split into patches. Then, the segmentation of each tile was accomplished by applying a deep convolutional neural network (DCNN) along with an encoder-decoder with separable atrous convolution architecture, which, once successfully validated, has revealed to be a promising method to segment pathological image patches. Next, in order to combine the local segmentation results (segmented tiles), while avoiding discontinuities and inconsistencies, an improved merging strategy based on an efficient fully connected Conditional Random Field (CRF) was applied. Experimental results on a collection of patches of breast cancer images demonstrate how the designed processing pipeline performs properly regardless the size, texture or any other colour-shape features typical of the malignant carcinomas considered in this study. The estimated segmentation accuracy and frequency weighted intersection over union (FWIoU) were 95.62%, 92.52%, respectively. Additionally, in order to facilitate the collaboration between pathologists and researchers to extract the specialist knowledge in form of training datasets that allows the training of new algorithms, a web-based platform which includes a slide-viewer and an annotation tool was developed. The automatic segmentation method proposed in this work was integrated into this platform and currently, it is being used as a decision support tool by pathologists. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113387	10.1016/j.eswa.2020.113387													
J								A comparison of clustering algorithms for automatic modulation classification	EXPERT SYSTEMS WITH APPLICATIONS										Automatic modulation classification; Constellation diagram; I/Q plane; Clustering algorithm; Centroid estimation		In this paper, the k-means, k-medoids, fuzzy c-means, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Ordering Points To Identify the Clustering Structure (OPTICS), and hierarchical clustering algorithms (with the addition of the elbow method) are examined for the purpose of Automatic Modulation Classification (AMC). This study compares these algorithms in terms of classification accuracy and execution time for either estimating the modulation order, determining centroid locations, or both. The best performing algorithms are combined to provide a simple AMC method which is then evaluated in an Additive White Gaussian Noise (AWGN) channel with M-Quadrature Amplitude Modulation (QAM) and M-Phase Shift Keying (PSK). Such an AMC method does not rely on any thresholds to be set by a human or machine learning algorithm, resulting in a highly flexible system. The proposed method can be configured to not give false positives, making it suitable for applications such as spectrum monitoring and regulatory enforcement. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				AUG 1	2020	151								113317	10.1016/j.eswa.2020.113317													
J								Multitaper-based method for automatic k-complex detection in human sleep EEG	EXPERT SYSTEMS WITH APPLICATIONS										Electroencephalogram; K-complex; Multitaper spectrogram; Sleep study; Spectral analysis	WAVE; METABOLISM; SPINDLES	In this paper, we propose a novel method for automatic k-complex (KC) detection in human sleep EEG, named MT-KCD. KCs are slow oscillations in the EEG signal characterized by a well-delineated, negative, sharp waves immediately followed by a positive component standing out from the background, with high-amplitude and total duration >= 0.5 s. Among the important aspects of the KC are its homeostatic and reactive functions in the brain, functioning as a sleep protection mechanism, and its practical use as a marker of N2 sleep stage during sleep studies. Given the importance of the KC, and the effort required from human experts to analyze EEG recordings visually, some recent research works have proposed automatic methods for KC detection. In comparison with existing methods, a key feature and novelty of MT-KCD is the use of multitaper spectral analysis to pre-process the EEG signal and automatically extract candidate KCs from it (characterized as 0-4 Hz power concentrations standing out from the background). After extraction, candidates are accepted/rejected depending on time domain characteristics (peak-to-peak amplitude >= 75 mu V, duration <= 2 s). The method overall time complexity is O(N logN). Regarding effectiveness, we have evaluated MT-KCD by using a public KC database (DREAMS) consisting of ten polysomnographic recordings of healthy patients (6 female and 4 male subjects with age range 20-47 years) partially annotated by two experts. Results have shown that MT-KCD improves detection metrics, especially F1 and F2 scores (harmonic averages of recall and precision), when compared to existing methods. Besides, improving F1 and F2 scores, MT-KCD also contributes to the automatic analysis of sleep EEG multitaper spectrograms, a technique recently proposed by researchers in the area of sleep studies as a complement to the traditional hypnogram (sleep stages diagram). (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113331	10.1016/j.eswa.2020.113331													
J								Discovering patterns of online popularity from time series	EXPERT SYSTEMS WITH APPLICATIONS										Multidimensional time series; Shape-based clustering; Online popularity; Social media		How is popularity gained online? Is being successful strictly related to rapidly becoming viral in an online platform, or is it possible to acquire popularity in a steady and disciplined fashion? What are other temporal characteristics that can unveil the popularity of online content? To answer these questions, we leverage a multifaceted temporal analysis of the evolution of popular online content. We present dipm-SC: a multidimensional shape-based time-series clustering algorithm with a heuristic to find the optimal number of clusters. First, we validate the accuracy of our algorithm on synthetic datasets generated from benchmark time series models. Second, we show that dipm-SC can uncover meaningful clusters of popularity behaviors in real-world GitHub and Twitter datasets. By clustering the multidimensional time-series of the popularity of contents coupled with other domain-specific dimensions, we discover two main patterns of popularity: bursty and steady temporal behaviors. Furthermore, we find that the way popularity is gained over time has no significant impact on the final cumulative popularity. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113337	10.1016/j.eswa.2020.113337													
J								entity2rec: Property-specific knowledge graph embeddings for item recommendation	EXPERT SYSTEMS WITH APPLICATIONS										Recommender systems; Knowledge graph embeddings; Knowledge graphs	SYSTEMS	Knowledge graphs have shown to be highly beneficial to recommender systems, providing an ideal data structure to generate hybrid recommendations using both content-based and collaborative filtering. Most knowledge-aware recommender systems are based on manually engineered features, typically relying on path counting and/or on random walks. Recently, knowledge graph embeddings have proven to be extremely effective at learning features for prediction tasks, reducing the complexity and time required to manually design effective features. In this work, we present entity2rec, which learns user-item relatedness for item recommendation through property-specific knowledge graph embeddings. A key element of entity2rec is the construction of property-specific subgraphs. Through an extensive evaluation on three datasets, we show that: (1) hybrid property-specific subgraphs consistently enhance the quality of recommendations with respect to collaborative and content-based subgraphs; (2) entity2rec generates accurate and non-obvious recommendations, compared to a set of state-of-the-art recommender systems, achieving high accuracy, serendipity and novelty. More in detail, entity2rec is particularly effective when the dataset is sparse and has a low popularity bias; (3) entity2rec is easily interpretable and can thus be configured for a particular recommendation problem. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				AUG 1	2020	151								113235	10.1016/j.eswa.2020.113235													
J								Hesitant fuzzy numbers with (alpha, k)-cuts in compact intervals and applications	EXPERT SYSTEMS WITH APPLICATIONS										Hesitant fuzzy number; (alpha, kappa)-cut; Hesitant fuzzy linear programming	LINEAR-PROGRAMMING PROBLEMS; GROUP DECISION-MAKING; INFORMATION AGGREGATION; SETS; OPTIMIZATION; RANKING; OPERATIONS; PREFERENCE; OPERATORS	In this paper, we propose a new definition of hesitant fuzzy numbers (HFNs) and study some essential properties of these numbers. We show (alpha, k)-cuts that were discussed in the recent literature for hesitant fuzzy sets (HFSs), on HFNs have resulted in compact intervals. In the following, we propose a new binary operation on these numbers. It has shown that the outcome of the proposed operation is a HFN. In addition, a new hesitant fuzzy relationship for comparing two HFNs is given. Finally, some applications of these numbers are presented in two examples. For this purpose, we propose a new approach to solve linear programming with hesitant fuzzy parameters. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113363	10.1016/j.eswa.2020.113363													
J								DNet: A lightweight and efficient model for aspect based sentiment analysis	EXPERT SYSTEMS WITH APPLICATIONS										Sentiment analysis; Convolutional neural network; BERT; Lightweight		Aspect based sentiment analysis (ABSA) is the task of identifying fine-grained opinion polarity towards a specific target in a sentence, which is empowering experts and intelligent systems with enriched interaction capabilities. Most of approaches to date usually capture semantic relations between target and context words based on RNNs (Recurrent Neural Networks) or pre-trained models (e.g. BERT). However, due to computational complexity and size constraints, these models are often hosted in the cloud. Enabling ABSA models to run on resource-constrained end-devices with quick response time is still challenging and not yet well studied. This paper presents distillation network (DNet), a lightweight and efficient sentiment analysis model based on gated convolutional neural networks for on-device inference. Through combining stacked gated convolution with attention mechanism, DNet can distill aspect-aware context information from unstructured text progressively, achieving high performance with less inference latency and reduced model size. Experiments on SemEval 2014 Task 4 and ACL14 Twitter datasets demonstrate that our approach achieves the state-of-the-art performance. Furthermore, compared with the BERT-based model, DNet reduces the model size by more than 50 times and improves the responsiveness by 24 times. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113393	10.1016/j.eswa.2020.113393													
J								An experimental evaluation of mixup regression forests	EXPERT SYSTEMS WITH APPLICATIONS										Mixup; Regression; Random forest; Rotation forest	CLASSIFIER ENSEMBLES; DIVERSITY; FUSION; SMOTE	Over the past few decades, the remarkable prediction capabilities of ensemble methods have been used within a wide range of applications. Maximization of base-model ensemble accuracy and diversity are the keys to the heightened performance of these methods. One way to achieve diversity for training the base models is to generate artificial/synthetic instances for their incorporation with the original instances. Recently, the mixup method was proposed for improving the classification power of deep neural networks (Zhang, Cisse, Dauphin, and Lopez-Paz, 2017). Mixup method generates artificial instances by combining pairs of instances and their labels, these new instances are used for training the neural networks promoting its regularization. In this paper, new regression tree ensembles trained with mixup, which we will refer to as Mixup Regression Forest, are presented and tested. The experimental study with 61 datasets showed that the mixup approach improved the results of both Random Forest and Rotation Forest. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113376	10.1016/j.eswa.2020.113376													
J								Subject-specific identification of three dimensional foot shape deviations using statistical shape analysis	EXPERT SYSTEMS WITH APPLICATIONS										3D foot scans; Statistical shape modelling; Personalized medicine; Outlier detection	HALLUX-VALGUS; ALGORITHMS; HEIGHT; GROWTH; MOTION; PAIN	The high prevalence of foot pain, and its relation to foot shape, indicates the need for an expert system to identify foot shape abnormalities. Yet, to date, no such expert system exists that examines the full 3D foot shape and produces an intuitive explanation of why a foot is abnormal. In this work, we present the first such expert system that satisfies those goals. The system is based on the concept of model-based outlier detection: a statistical shape model (SSM) is generated from 186 3D optical foot scans of healthy feet. This model acts as a knowledge base which is then parameterized by one's demographic characteristics (e.g., age, weight, height, shoe size) through a multivariate regression. This regression introduces model flexibility as it allows the model to be fine tuned to a specific individual. This fine tuned model is then used as a baseline to which the individual's 3D foot scan can be compared using standard statistical tests (e.g. t-tests). These statistical tests are performed at each vertex along the foot surface to identify the degree and location of shape outliers. Our expert system was validated on foot scans from patients with hallux valgus and abnormal foot arches. As expected, our results varied per patient, confirming that feet with the same clinical classification still show high shape variability. Additionally, the foot shape abnormalities identified by our system not only agreed with the expected location and severity of the tested foot deformities, but our analysis of the full 3D foot shape was able to completely characterize the extent of those abnormalities for the first time. These results show that the combination of statistical shape modelling, multivariate regression, and statistical testing is powerful enough to perform outlier detection for 3D foot shapes. The resulting insights provided by this system could prove useful in both shoe design and clinical diagnosis. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113372	10.1016/j.eswa.2020.113372													
J								Forecasting model selection using intermediate classification: Application to MonarchFx corporation	EXPERT SYSTEMS WITH APPLICATIONS										Expert system; Time series forecasting; Model selection; Classification; Supply chain management	TIME-SERIES; DEMAND; SYSTEM; ACCURACY; NETWORKS	Organizations rely on accurate demand forecasts to make production and ordering decisions in a variety of supply chain positions. Significant research in time series forecasting techniques and a variety of forecasting methods are available in the market. However, selecting the most accurate forecasting model for a given time series has become a complicated decision. Prior studies of forecasting methods have used either in-sample or out-of-sample performance as the basis for model selection procedures, but typically fail to incorporate both in their decision-making framework. In this research, we develop an expert system for time series forecasting model selection, using both relative in-sample performance and out-of-sample performance simultaneously to train classifiers. These classifiers are employed to automatically select the best performing forecasting model without the need for decision-maker intervention. The new model selection scheme bridges the gap between using in-sample and out-of-sample performance separately. The best performing model on the validation set is not necessarily selected by the expert system, since both in-sample and out-of-sample information are essential in the selection process. The performance of the proposed expert system is tested using the monthly dataset from the M3-Competition, and the results demonstrate an overall minimum of 20% improvement in the optimality gap comparing to the train/validation method. The new forecasting expert system is also applied to a real case study dataset obtained from MonarchFx (a distributed logistics solutions provider). This result demonstrates a robust predictive capability with lower mean squared errors, which allows organizations to achieve a higher level of accuracy in demand forecasts. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113371	10.1016/j.eswa.2020.113371													
J								Locality adaptive preserving projections for linear dimensionality reduction	EXPERT SYSTEMS WITH APPLICATIONS										Dimensionality reduction; Feature extraction; Intrinsic dimensionality; Local structure	FACE RECOGNITION; FRAMEWORK	Dimensionality reduction techniques aim to transform the high-dimensional data into a meaningful reduced representation and have been consistently playing a fundamental role in the study of intrinsic dimensionality estimation and the design of an intelligent expert system towards real-world applications. From the perspective of manifold learning, locality preserving projections is a classical and commonly used dimensionality reduction method and it essentially learns the low-dimensional embedding under the constraint of preserving the local geometry of data. However, since it determines the neighborhood relationships in the original feature space that probably contains noisy and irrelevant features, the derived similarity between the neighbors are unreliable and the corresponding local data manifold tends to be error-prone, which inevitably leads to degraded performance for subsequent data analyses. Hence, how to accurately identify the true neighbor relationships for each sample remains crucial to the robustness improvement. In this work, we propose a novel approach, termed locality adaptive preserving projections (LAPP), to adaptively determine the neighbors and their relationships in the optimal subspace rather than in the original space. Specifically, due to the absence of prior knowledge of local properties of the underlying manifold, LAPP adopts a coarse-to-fine strategy to iteratively update the projected low-dimensional subspace and optimize the identification of the local structure of the data. Moreover, an iterative algorithm with fast convergence is utilized to solve the transformation matrix for explicit out-of-sample extension. Besides, LAPP is easy to implement and its key idea can be potentially extended to other methods for neighbor-finding and similarity measurement. To evaluate the performance of LAPP, we conduct comparative experiments on numerous synthetic and real-world datasets. Experimental results show that seeking the local structure in the original feature space misleads the selection of neighbors and the calculation of similarity and that the proposed method helps alleviate the negative effect of noisy and irrelevant features, which demonstrates its effectiveness. Besides, this study has the potential to enlighten relevant studies to consider the problem of optimizing the neighborhood relationships. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113352	10.1016/j.eswa.2020.113352													
J								Efficient synthetical clustering validity indexes for hierarchical clustering	EXPERT SYSTEMS WITH APPLICATIONS										Clustering; Validity index; Hierarchical clustering; Optimal number of clusters	VALIDATION; ALGORITHMS; GRAPH	Clustering validation and identifying the optimal number of clusters are of great importance in expert and intelligent systems. However, the commonly used similarity measures for validating are not versatile to measure the complex data structure, in reality, some of which are not as effective as that of the used clustering algorithm which gives the clustering results. This paper studies the validity indexes for the hierarchical clustering algorithm and proposes a unified validity index framework. For the single-linkage agglomerative hierarchical clustering we propose two efficient synthetical clustering validity (SCV) indexes using the minimum spanning tree to calculate the intra-cluster compactness to overcome the deficiencies of the measurements in the existing validity indexes. For the general hierarchical clustering, a self-adaptive similarity measure strategy and two generalized synthetical clustering validity (GSCV) indexes, which are the extension of the proposed SCV indexes, are developed. The proposed SCV and GSCV indexes constitute a unified validity index framework, where SCV index is a special case of GSCV index, can avoid the incompatibility of the similarity measure between the clustering and validation. The experimental comparisons with the state-of-the-art validity indexes on artificial and real-world data sets demonstrate the efficiency of the proposed validity indexes in discovering the true number of clusters and dealing with various sorts of data sets, including imbalanced data sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113367	10.1016/j.eswa.2020.113367													
J								Enhanced attentive convolutional neural networks for sentence pair modeling	EXPERT SYSTEMS WITH APPLICATIONS										EACNN; Attentive convolution; Multi-grained similarity; Sentence pair modeling		Sentence Pair Modeling is a critical and challenging problem in natural language processing (NLP). It aims to seek the underlying semantic relationship between two sentences. Inspired by human learning, attention mechanisms are widely used in NLP. Previous attentive CNNs mainly focus on attentive pooling which only compute the matching scores between sentence pairs with the same filter size and lack substantive interactive context. In this paper, we propose Enhanced Attentive Convolutional Neural Networks (EACNNs) for modeling sentence pairs to make full use of the characteristics of convolution. Enhanced attention mechanisms help strengthen the interaction between sentences via adding alignment context into local context in convolution operation and combining multi-grained similarity features in different filter sizes. We exploit two attention schemes: (i) attention before representation to capture the interaction information of sentence pairs by attentive convolution (AC) and multi-window advanced attention (MWA), and (ii) attention after representation to emphasize different importance of each word by multi-view similarity measurement layer (MVS). All the enhanced attention mechanisms can make our EACNNs outperform existing attentive CNN models. Furthermore, the combinations of these attention have strong competition in complicated LSTMs and show great advantage in training efficiency. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113384	10.1016/j.eswa.2020.113384													
J								Item recommendation by predicting bipartite network embedding of user preference	EXPERT SYSTEMS WITH APPLICATIONS										Bipartite network embedding; Item recommendation; Kalman filtering; Time-aware Recommendation Method; User Preference	SYSTEMS	With the development of e-commerce, various methodologies have studied to improve recommendation performance. Recently, many deep learning based network embedding approaches are applied to the recommendation domain. However, these approaches still have several limitations, such as the problem of data sparseness and the changing in user preference over time, which cannot be considered. In this paper, we propose a novel method for item recommendation based on network embedding. First, we apply a bipartite network embedding to address the data sparsity problem. Bipartite network embedding is a vector representation method that reflects explicit (i.e., observed data) and implicit relations (i.e., unobserved data). Bipartite network embedding methodology can address the data sparsity problem by using implicit relationship information from applying the random walk approach. Second, we predict future bipartite network embedding of user preference by adopting a Kalman filter to consider the changes in user preferences. We have conducted experiments to evaluate the effectiveness and performance of the proposed recommendation method. Through experimentation, the proposed recommendation method is validated as outperforming than the existing approaches including existing network embedding methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113339	10.1016/j.eswa.2020.113339													
J								Modeling methodology for early warning of chronic heart failure based on real medical big data	EXPERT SYSTEMS WITH APPLICATIONS										Heart failure; Early warning; Social network; Risk factors; Medical big data	CHRONIC KIDNEY-DISEASE; RISK	Heart failure (HF) is among the most costly diseases to our society, and the prevalence keeps on increasing these days. Early detection of HF plays a vital role in saving lives through adjusting lifestyles and drug interventions that can slow down disease progression or prevent HF. There are many cardiovascular risk factors associated with HF, and they often coexist. In this paper, we assess the predictive value of pathological factors for early HF detection through a social network based approach. We use electronic health records (collected from the project HeartCarer) and compute the similarity of risk factors. The similarity values are used to construct an unweighted and a weighted medical social network. The constructed medical social network is further divided into a HF high-risk group and HF low-risk group using a group division algorithm. Patients in the high-risk group will be suggested for early screening. To evaluate the prediction value of our method, we perform four experiments based on real world data. The results demonstrate the high effectiveness of our method on heart failure risk assessment, with the best accuracy close to 90%. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 1	2020	151								113361	10.1016/j.eswa.2020.113361													
J								Multi-objective optimization of the resistance spot welding process using a hybrid approach	JOURNAL OF INTELLIGENT MANUFACTURING										Welding quality; Desirability approach; Grey entropy technique; Multi-objective optimization	MECHANICAL-PROPERTIES; DISSIMILAR JOINTS; PARAMETERS; PREDICTION; DIAMETER; STRENGTH	This study proposed an approach to optimize the process parameters using the entropy weight method combining regression analysis in the resistance spot welding process. Based on the central composite experimental design, tests were carried out with three levels of process parameters for spot-welded titanium alloy sheets. Multiple quality characteristics, namely nugget diameter, maximum displacement, tensile shear load, and failure energy, were converted into a comprehensive welding quality index. The weight for each quality index to obtain the comprehensive welding quality index was determined based on the grey entropy method. The welding heat input for each welding joints was calculated based on the dynamic power signal in the welding process. The mathematical model correlating process parameters and the comprehensive welding quality index was established on the basis of regression analysis. The relationship between the welding process parameters and welding heat was also quantified using a regression model. The effects of welding process parameters on welding quality and welding heat were also discussed. To optimize multi-performance characteristics, the desirability function was employed. The verification test results proved that the method proposed in this paper effectively optimized the welding parameters and kept the welding heat input as low as possible at the same time. Welding current is the most significant parameter affecting the welding quality followed by welding time. This can be owing to its direct influence on the amount of heat supplied to the welding zone during the welding process. The method proposed in this study can serve as a guidance and recommendation for resistance spot welding welders to guarantee welding quality and meet the needs of high production and effective energy saving.																	0956-5515	1572-8145															10.1007/s10845-020-01638-2		JUL 2020											
J								Fast and accurate cable detection using CNN	APPLIED INTELLIGENCE										Cable detection; Convolutional neural networks; Deep learning	POWER-LINE DETECTION; INSPECTION	In recent years, unmanned aerial vehicle (UAV) based vision inspections have been widely applied in electricity systems for both efficiency improvement and labor cost saving. Cable detection is essential for both navigation and flight safety of aerial vehicles. However, power cable detection is widely regarded as a challenging task since the targets are very weak and very easy to be confused with cluttered backgrounds. Traditional line and edge detectors are lack of robustness to scene variations. Recent deep learning based methods also can not support fast and stable power cable detection well for onboard applications . In this paper, a new convolutional neural network (CNN) based cable detection method is proposed. First of all, we encode cables by groups of evenly distributed key points, which reduce the complexities of detection tasks. By this approach, the proposed model detect grouped key points of cables from aerial images directly and the detailed pixels of cables can be restore with the curve equations which are implicitly behind those grouped key points. Subsequently, new methods of data labeling and augmentation, sample matching, post clustering, and performance evaluation for cable key points detection are presented. Finally, comprehensive experimental results demonstrate the efficiency and accuracy of our proposed cable detection method.																	0924-669X	1573-7497															10.1007/s10489-020-01746-9		JUL 2020											
J								Simulating Strong Practical Proof Systems with Extended Resolution	JOURNAL OF AUTOMATED REASONING										Propositional logic; SAT; SAT solving; Proof complexity; Resolution; DRAT; PR; Blocked clauses	HARD	Proof systems for propositional logic provide the basis for decision procedures that determine the satisfiability status of logical formulas. While the well-known proof system of extended resolution-introduced by Tseitin in the sixties-allows for the compact representation of proofs, modern SAT solvers (i.e., tools for deciding propositional logic) are based on different proof systems that capture practical solving techniques in an elegant way. The most popular of these proof systems is likely DRAT, which is considered the de-facto standard in SAT solving. Moreover, just recently, the proof system DPR has been proposed as a generalization of DRAT that allows for short proofs without the need of new variables. Since every extended-resolution proof can be regarded as a DRAT proof and since every DRAT proof is also a DPR proof, it was clear that both DRAT and DPR generalize extended resolution. In this paper, we show that-from the viewpoint of proof complexity-these two systems are no stronger than extended resolution. We do so by showing that (1) extended resolution polynomially simulates DRAT and (2) DRAT polynomially simulates DPR. We implemented our simulations as proof-transformation tools and evaluated them to observe their behavior in practice. Finally, as a side note, we show how Kullmann's proof system based on blocked clauses (another generalization of extended resolution) is related to the other systems.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1247	1267		10.1007/s10817-020-09554-z		JUL 2020											
J								Scalable time-constrained planning of multi-robot systems	AUTONOMOUS ROBOTS										Multi-robot systems; Cooperative control; Decentralized control; Abstractions; Metric interval temporal logic (MITL); Nonlinear model predictive control (NMPC); Robust control	MODEL-PREDICTIVE CONTROL; SCHEME	This paper presents a scalable procedure for time-constrained planning of a class of uncertain nonlinear multi-robot systems. In particular, we considerNrobotic agents operating in a workspace which contains regions of interest (RoI), in which atomic propositions for each robot are assigned. The main goal is to design decentralized and robust control laws so that each robot meets an individual high-level specification given as a metric interval temporal logic (MITL), while using only local information based on a limited sensing radius. Furthermore, the robots need to fulfill certain desired transient constraints such as collision avoidance between them. The controllers, which guarantee the transition between regions, consist of two terms: a nominal control input, which is computed online and is the solution of a decentralized finite-horizon optimal control problem (DFHOCP); and an additive state feedback law which is computed offline and guarantees that the real trajectories of the system will belong to a hyper-tube centered along the nominal trajectory. The controllers serve as actions for the individual weighted transition system (WTS) of each robot, and the time duration required for the transition between regions is modeled by a weight. The DFHOCP is solved at every sampling time by each robot and then necessary information is exchanged between neighboring robots. The proposed approach is scalable since it does not require a product computation among the WTS of the robots. The proposed framework is experimentally tested and the results show that the proposed framework is promising for solving real-life robotic as well as industrial applications.																	0929-5593	1573-7527				NOV	2020	44	8					1451	1467		10.1007/s10514-020-09937-6		JUL 2020											
J								A Fast Compression Framework Based on 3D Point Cloud Data for Telepresence	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										3D point cloud compression; motion estimation; signatures of histograms orientation; 3D point cloud matching; predicted frame and intra frame	UNIQUE SIGNATURES; HISTOGRAMS; SURFACE; SHOT	In this paper, a novel compression framework based on 3D point cloud data is proposed for telepresence, which consists of two parts. One is implemented to remove the spatial redundancy, i.e., a robust Bayesian framework is designed to track the human motion and the 3D point cloud data of the human body is acquired by using the tracking 2D box. The other part is applied to remove the temporal redundancy of the 3D point cloud data. The temporal redundancy between point clouds is removed by using the motion vector, i.e., the most similar cluster in the previous frame is found for the cluster in the current frame by comparing the cluster feature and the cluster in the current frame is replaced by the motion vector for compressing the current frame. The first, the B-SHOT (binary signatures of histograms orientation) descriptor is applied to represent the point feature for matching the corresponding point between two frames. The second, the K-mean algorithm is used to generate the cluster because there are a lot of unsuccessfully matched points in the current frame. The matching operation is exploited to find the corresponding clusters between the point cloud data of two frames. Finally, the cluster information in the current frame is replaced by the motion vector for compressing the current frame and the unsuccessfully matched clusters in the current and the motion vectors are transmitted into the remote end. In order to reduce calculation time of the B-SHOT descriptor, we introduce an octree structure into the B-SHOT descriptor. In particular, in order to improve the robustness of the matching operation, we design the cluster feature to estimate the similarity between two clusters. Experimental results have shown the better performance of the proposed method due to the lower calculation time and the higher compression ratio. The proposed method achieves the compression ratio of 8.42 and the delay time of 1 228 ms compared with the compression ratio of 5.99 and the delay time of 2 163 ms in the octree-based compression method under conditions of similar distortion rate.																	1476-8186	1751-8520															10.1007/s11633-020-1240-5		JUL 2020											
J								A BCI video game using neurofeedback improves the attention of children with autism	JOURNAL ON MULTIMODAL USER INTERFACES										Autism; Attention; Brain-computer interface; Neurofeedback; Video game		Major usability and technical challenges have created mistrust of the potential of brain computer interfaces used to control video games in challenging environments like healthcare. Despite several studies showing low cost commercial headsets can read the brainwave patterns of its users with great potential for long term adoption; there are limited studies showing its efficacy in concrete healthcare scenarios. In our past work, we developed FarmerKeeper, a BCI using users' attention to control a runner videogame to support neurofeedback therapies with great usability and user experience. In this paper, beyond usability, we describe the results of a 10-week deployment study with 26 children with severe autism using FarmerKeeper as a tool to support the neurofeedback therapies of children with autism. Pre- and post-assessment evaluation indicate all children with autism improve their attention, attentional control and sustained attention. Two children with autism no longer showed attention impairments in the post-assessment evaluation. We closed discussing directions for future work and the potential benefits of this new generation of BCI videogames in healthcare scenarios.																	1783-7677	1783-8738															10.1007/s12193-020-00339-7		JUL 2020											
J								Rotor fault diagnosis of frequency inverter fed or line-connected induction motors using mutual information	SOFT COMPUTING										Three-phase induction motors; Broken rotor bar; Similarity measure; Intelligent systems; Pattern recognition	BAR FAULT; START-UP; SIGNATURES; SIGNALS; HILBERT	Characteristics like robustness, adaptability to several load conditions, and low costs of operation and maintenance are some of the reasons three-phase induction motors are ubiquitous in industrial applications. Even so, these machines are subjected to electrical and mechanical faults which can result in malfunction. The detection of incipient faults is the focus of several recent studies because they provide information for timely decisions to avoid unplanned stops of industrial processes. A common problem of induction motors is the presence of broken rotor bars. Most of the methods of condition monitoring and fault diagnosis focus on the usage of only one type of power supply: line-connected or fed by frequency inverters. Given this situation, we present an alternative technique for the detection of broken rotor bars of three-phase induction motors regardless of the type of power supply. Our approach is based on similarity measures of the stator current signals of two motor phases in order to extract the relevant features of such signals, which are then classified using three intelligent systems: artificial neural network, support vector machines, andk-nearest neighbors. We performed 3310 experimental tests with motors operating in steady state, with sinusoidal and non-sinusoidal power supply, and variations of voltage unbalance levels, supply frequency, and load torque. Classification accuracy rates over 92% of success were obtained in these tests, which validate the proposed approach.																	1432-7643	1433-7479															10.1007/s00500-020-05224-9		JUL 2020											
J								A Structural Topic Modeling-Based Bibliometric Study of Sentiment Analysis Literature	COGNITIVE COMPUTATION										Sentiment analysis; Bibliometric; Structural topic modeling; Social network analysis	SOCIAL EMOTION CLASSIFICATION; NEWS IMPACT; EXTRACTION; TEXT; PREDICTION; RESOURCES; EVOLUTION; NETWORKS; SYSTEM	Sentiment analysis is an increasingly evolving field of research in computer science. With the considerable number of studies on innovative sentiment analysis available, it is worth the effort to present a review to understand the research on sentiment analysis comprehensively. This study aimed to investigate issues involved in sentiment analysis; for instance, (1) What types of research topics had been covered in sentiment analysis research? (2) How did the research topics evolve with time? (3) What were the topic distributions for major contributors? (4) How did major contributors collaborate in sentiment analysis research? Based on articles retrieved from the Web of Science, this study presented a bibliometric review of sentiment analysis with the basis of a structural topic modeling method to obtain an extensive overview of the research field. We also utilized methods such as regression analysis, geographic visualization, social network analysis, and the Mann-Kendal trend test. Sentiment analysis research had, overall, received a growing interest in academia. In addition, institutions and authors within the same countries/regions were liable to collaborate closely. Highly discussed topics weresentiment lexicons and knowledge bases,aspect-based sentiment analysis, andsocial network analysis. Several current and potential future directions, such asdeep learning for natural language processing,web services,recommender systems and personalization, andeducation and social issues, were revealed. The findings provided a thorough understanding of the trends and topics regarding sentiment analysis, which could help in efficiently monitoring future research works and projects. Through this study, we proposed a framework for conducting a comprehensive bibliometric analysis.																	1866-9956	1866-9964															10.1007/s12559-020-09745-1		JUL 2020											
J								Multiple Reliable Structured Patches for Object Tracking	COGNITIVE COMPUTATION										Object tracking; Patch model; Bounding box model; Reliable patches	VISUAL TRACKING	It is essential to build the effective appearance model for object tracking in computer vision. Most object trackers can be roughly divided into two categories according to the appearance model: the bounding box model and the patch model. The bounding box model cannot handle shape deformation and occlusion of the non-rigid moving object effectively. The patch model is prone to be disturbed by complex backgrounds. In this paper, we propose a robust multi-structured-patch appearance model to represent the target for object tracking. The proposed appearance model is aimed to exploit and identify reliable patches that can be tracked effectively through the whole tracking process. According to attention mechanism in biological vision system, a coarse-to-fine strategy is usually used to search the target. Therefore, the proposed appearance model is represented by robust patches in different sizes, in which the bigger patches search the rough region of the target and the smaller patches estimate the accurate location. Experimental results on OTB100 dataset show that the proposed method outperforms state-of-the-art trackers.																	1866-9956	1866-9964															10.1007/s12559-020-09741-5		JUL 2020											
J								Deep learning-based community detection in complex networks with network partitioning and reduction of trainable parameters	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Community detection; Complex networks; Deep learning; Autoencoder; Partitioning network; Parameters reduction and sharing	MODULARITY	Community detection in complex networks is an important multidisciplinary research area and is considered crucial for understanding the structure of complex networks. Unsupervised deep learning models (e.g. stack autoencoders) have been successfully proposed for the problem of community detection, which can extract network features and use them in splitting the network into communities. Despite their effectiveness, these methods are not very efficient, especially when large networks are involved. Furthermore, existing models usually handle the network as a single object, which means that massive trainable parameters are required during training (which in normal complex networks often reach millions of parameters, not to mention large networks), thereby increasing the complexity of the model. To overcome these problems, this paper proposes a deep autoencoder method for network community detection based on three techniques: network-data partitioning, reduction and sharing of trainable parameters, which contribute significantly to improve the efficiency of the method. A new partitioning strategy is imposed on complex networks at different levels. The paper also proposes a parallel design for the proposed method. Furthermore, a new similarity constraint function is proposed to improve and preserve the performance of community detection task. We performed extensive experiments for different partitioning levels of a network-dataset to evaluate the method with CPU and GPU devices. The results showed that the proposed method significantly improved training speed and efficiency while maintaining performance. The results also showed that the efficiency of the method increases as we move to a deeper level of partitioning.																	1868-5137	1868-5145															10.1007/s12652-020-02389-x		JUL 2020											
J								Scintillation and bit error rate calculation of Mathieu-Gauss beam in turbulence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mathieu-Gauss beam; Scintillation; Bit error rate; Atmospheric turbulence; Aperture averaging	APERTURED AVERAGED SCINTILLATION; HOLLOW BEAM; PROPAGATION; GENERATION; AXICON	We present on-axis, aperture averaged scintillation, and bit error rate performance of Mathieu-Gauss beams (MGB). We model atmosphere utilizing random phase screen. Our investigations show that scintillation indexes of both odd and even modes of MGBs are less than Gauss beam. In the comparison of odd and even modes, odd modes draw the attention with its less scintillation index. In case of aperture averaged scintillation, all types of MGBs are advantageous comparing with Gauss beam but this advantage vanishes with increase in receiver aperture opening. Mitigation in scintillation index brings us 10(-6)times lesser bit error rate (BER) than Gauss beam.																	1868-5137	1868-5145															10.1007/s12652-020-02430-z		JUL 2020											
J								Secure cubic dimension acoustic and routing in acoustic sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Acoustic Sensor Networks; Cubic dimension acoustic spaces; A positioning routing; Acoustic applications		There are expansive applications in sea science, seaward investigation, safeguard insurance, acoustic observation, acoustic sensor systems with explicit acoustic sensors, submerged boats, submerged vessels and acoustic surface stations. There is extremely vital to make acoustic sensor organize application, an effective correspondence between all gadgets and segments. Inferable from the mind boggling arrangement condition and the exceptional qualities of submerged acoustic diverts in cubic dimension acoustic, another proficient and predictable correspondence and systems administration conventions are important to plan the Acoustic Sensor Networks. In this paper, we center a cubic bunch arrangement way to deal with adjust to dynamic conditions with adaptable part and union tasks and utilize iterative key appropriation plan to infer mystery keys for the confirmation of hubs through the transfer hubs, it moves among the groups with restriction, position based steering, organizations in cubic dimension acoustic spaces to control the vitality misfortune.																	1868-5137	1868-5145															10.1007/s12652-020-02397-x		JUL 2020											
J								Computational method for monitoring pauses exercises in office workers through a vision model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Flask; TensorFlow; NumPy; Affine transformation; Web sockets	PATTERN	A sedentary routine at work can cause various muscular, skeletal or visual diseases, however these can be prevented with what is known as active pauses. This article is intended to illustrate how software can help reduce the risk of occupational disease due to the sedentary lifestyle of an office job, for this purpose a web application was developed under the SCRUM methodology, which makes use of the TensorFlow, Flask PoseNet model and python, for an active pause control application which is a proven practice of reducing the type of diseases already mentioned. With these tools it was possible to develop an algorithm capable of comparing two human figures; which serves to compare whether the user of the program is performing or not correctly performing the active pause exercise, with an average error squared on the order of 10(-32). Finally, The application can keep track of the figure and exercises performed by the user just by using the user personal webcam and the comparison algorithm developed, leaving behind the use of tools such as Kinect.																	1868-5137	1868-5145															10.1007/s12652-020-02391-3		JUL 2020											
J								Bayesian trust analysis of flooding attacks in distributed software defined networking nodes	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Communication; Software defined networks; Flooding attacks; Trust analysis		As the networking infrastructure is becoming complex, there is a parallel increase in the networking scenario to become more vulnerable to various types of sophisticated malicious attacks. This leads to the data and information loss. The most feasible way to reduce the vulnerable and malicious attacks is by evaluating trust of the interacting agents in a network. The primary objective of this paper is to analyze the flooding attacks caused due to the communication protocols like TCP and UDP in software defined spine leaf topology. The trust analysis is done using Bayesian networks for a reliable communication network. The probability and the causal effect of the denial of service in the proposed distributed networking scenario are reasoned out. The trust analysis will help to increase the confidence over a dedicated communication at the time of malicious attack over the distributed network. This analysis will enable the software defined control plane to make a crucial and needed decision over its communication with other nodes during compromised security attack through communication protocols.																	1868-5137	1868-5145															10.1007/s12652-020-02428-7		JUL 2020											
J								Computer aided diagnosis of brain tumor using novel classification techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Clustering; Bag of visual words; Morphological operation; Brain tumor	SEGMENTATION; ALGORITHM	Brain cancer treatment mainly depends on the accurate detection of the tumor type, location, size and borders. Magnetic resonance images (MRI) can be used to analyze the properties of the desired region such as tissues and tumors with automated and semi-automated approaches. So, the extraction of MRI brain tumor image is a challenging task in medical image processing. The major problems associated with MRI analysis by a physician are time consuming and the accuracy depends on the expertise of the physician.This limitation can be overcome by the computer aided diagnosis (CAD) technology. In this paper, a CAD system is designed to detect brain tumors with computer assistance using T1 and T2 weighted MR images. The designed system classifies the tumor into benign or malignant from MR Image using a novel automated method which increases the performance and reduces the complexity involved in the tumor diagnosis. The CAD system has four stages such as image acquisition, segmentation, feature extraction and classification. Segmentation is done with the help of K-means clustering, which enhances the medical image and the clustering quality to avoid local optima and to find global optima. The feature extraction is performed by gray level co-occurrence matrix (GLCM). The tumor classification is done using support vector machine (SVM) and bag of visual words (BOVW) classifiers. The test result of the SVM classifier gives accuracy 95.0%, sensitivity 91.79% and specificity 94.75%. Whereas, the BOVW classifier yields results of accuracy 96.0%, sensitivity 90.0% and specificity 100%. This result shows that, the designed system classifies the tumor into benign or malignant with a good level of accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02429-6		JUL 2020											
J								Cloud service recommendation system based on clustering trust measures in multi-cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud service; Service recommendation; Trust measure; Trust degree	BIG DATA; SELECTION	Due to technological advancement, cloud computing is an inevitable form of computing these days and is considered a boon to mid-scale industries. As the usage of cloud computing increases day-by-day, the service deployment improves every single day, which paves the way for security threats as well. Finding trustworthy service is a highly challenging problem, which may lead to time consumption or end with inappropriate services. Due to this problem, end user needs trust based appropriate service with minimum time consumption and the service should be reliable too. Hence, a cloud service recommendation system is the current need of the cloud environment. From a pool of available cloud services, the proposed system can recommend the time conserving reliable trustworthy services. This work attempts to keep this as the goal and presents a cloud service recommendation system using clustering based trust degree computation algorithm. Trust measures are deliberating to compute the trust degree (TD) for each dynamic service, which is computed for every time and the historical information is maintained as well. Since the trust agent clustered the services in automated fashion, to isolates the most trustworthy services from all the available clustered cloud services and efficiently allocates services to the end user using trustworthy service allocation algorithm. Process of service search and recommendation needs minimum time consumption. Registering service with trust agent (TA) provides most reliable trust worthy services. The performance of this recommendation system is evaluated in terms of precision, recall, F-measure and time consumption rates. The average F-measure rate of the proposed work is computed by varying the count of users from 200 to 300 and the average F-measure rate is 91.85% with minimal time consumption than the existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02368-2		JUL 2020											
J								An efficient projection-based method for high utility itemset mining using a novel pruning approach on the utility matrix	KNOWLEDGE AND INFORMATION SYSTEMS										Frequent itemset mining; High utility itemset mining; Itemset; Transaction-weighted utility; Pruning strategy	ALGORITHM; DISCOVERY; FRAMEWORK	High utility itemset mining is an important extension of frequent itemset mining which considers unit profits and quantities of items as external and internal utilities, respectively. Since the utility function has not downward closure property, an overestimated value of utility is obtained using an anti-monotonic upper bound of utility function to prune the search space and improve the efficiency of high utility itemset mining methods. Transaction-weighted utilization (TWU) of itemset was the first and one of the most important functions which has been used as the anti-monotonic upper bound of utility by various algorithms. A variety of high utility itemset mining methods have attempted to tighten the utility upper bound and have exploited appropriate pruning strategies to improve mining efficiency. Although TWU and its improved alternatives have attempted to increase the efficiency of high utility itemset mining methods by pruning their search spaces, they suffer from a significant number of generated candidates which are high-TWU but are not high utility itemsets. Calculating the actual utilities of low utility candidates needs to multiple scanning of the dataset and thus imposes a huge overhead to the mining methods, which can cause to lose the pruning benefits of the upper bounds. Proposing appropriate pruning strategies, exploiting efficient data structures, and using tight anti-monotonic upper bounds can overcome this problem and lead to significant performance improvement in high utility itemset mining methods. In this paper, a new projection-based method, called MAHI (matrix-aided high utility itemset mining), is introduced which uses a novel utility matrix-based pruning strategy, called MA-prune to improve the high utility itemset mining performance in terms of execution time. The experimental results show that MAHI is faster than former algorithms.																	0219-1377	0219-3116				NOV	2020	62	11					4141	4167		10.1007/s10115-020-01485-w		JUL 2020											
J								Hyper Autoencoders	NEURAL PROCESSING LETTERS										Autoencoder; Hypernetwork; Image processing; Deep learning	NETWORK	We introduce the hyper autoencoder architecture where a secondary, hypernetwork is used to generate the weights of the encoder and decoder layers of the primary, actual autoencoder. The hyper autoencoder uses a one-layer linear hypernetwork to predict all weights of an autoencoder by taking only one embedding vector as input. The hypernetwork is smaller and as such acts as a regularizer. Just like the vanilla autoencoder, the hyper autoencoder can be used for unsupervised or semi-supervised learning. In this study, we also present a semi-supervised model using a combination of convolutional neural networks and autoencoders with the hypernetwork. Our experiments on five image datasets, namely, MNIST, Fashion MNIST, LFW, STL-10 and CelebA, show that the hyper autoencoder performs well on both unsupervised and semi-supervised learning problems.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1395	1413		10.1007/s11063-020-10310-y		JUL 2020											
J								Flight Delay Prediction Using Deep Convolutional Neural Network Based on Fusion of Meteorological Data	NEURAL PROCESSING LETTERS										Flight delay prediction; DCNN; SE-DenseNet; Data fusion		Nowadays, the civil aviation industry has a high precision demand of flight delay prediction. To make full use of the characteristics of flight data and meteorological data, two flight delay prediction models using deep convolution neural network based on fusion of meteorological data are proposed in this paper. One is DCNN (Dual-channel Convolutional Neural Network), which refers to the ResNet network structure. The other is SE-DenseNet (Squeeze and Excitation-Densely Connected Convolutional Network), combining the advantages of DenseNet and SENet. Firstly, flight data and meteorological data are fused in the model. Then, both DCNN and SE-DenseNet models are used to extract feature automatically based on the fused flight data set. Finally, the softmax classifier is adopted to predict the flight delay level. For proposed DCNN model, both straight channel and convolution channel are designed to guarantee the lossless transmission of the feature matrix and enhance the patency of the deep network. For proposed SE-DenseNet model, a SE module is added after the convolution layer of each DenseNet block, which can not only enhance the transmission of deep information but also achieve feature recalibration in the feature extraction process. The research results indicate that after considering characteristics of meteorological information, the accuracy of the model can be improved 1% compared with only considering the flight information. The two deep convolutional neural networks proposed in this paper, DCNN and SE-DenseNet, can both effectively improve the prediction accuracies, reaching to 92.1% and 93.19%, respectively.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1461	1484		10.1007/s11063-020-10318-4		JUL 2020											
J								Logic Negation with Spiking Neural P Systems	NEURAL PROCESSING LETTERS										P systems; Logic negation; Membrane computing	NETWORKS	Nowadays, the success of neural networks as reasoning systems is doubtless. Nonetheless, one of the drawbacks of such reasoning systems is that they work as black-boxes and the acquired knowledge is not human readable. In this paper, we present a new step in order to close the gap between connectionist and logic based reasoning systems. We show that two of the most used inference rules for obtaining negative information in rule based reasoning systems, the so-calledClosed World AssumptionandNegation as Finite Failurecan be characterized by means ofspiking neural P systems, a formal model of the third generation of neural networks born in the framework of membrane computing.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1583	1599		10.1007/s11063-020-10324-6		JUL 2020											
J								Joint Reflectance Field Estimation and Sparse Representation for Face Image Illumination Preprocessing and Recognition	NEURAL PROCESSING LETTERS										Reflectance field estimation; Sparse representation; Illumination preprocessing; Pattern recognition	NORMALIZATION; MODELS	Illumination preprocessing is an important ingredient for handling lighting variation face recognition challenge. Nonetheless, existing methods are usually designed to be independent of the face recognition methods and the interaction between them is not yet well explored. In this paper, we formulate the face image illumination preprocessing and recognition into a unified sparse representation framework and propose a novel joint reflectance field estimation and sparse representation (JRSR) method for face recognition under extreme lighting conditions. The proposed method separates the identify factor and the interfered illumination of a query sample simultaneously by one nonconvex sparse optimizing model. We also present an efficient approximation algorithm to solve JRSR in this paper. Evaluation on several face databases and the experimental results of face recognition with illumination variation clearly demonstrate the advantages of our proposed JRSR algorithm in illumination preprocessing efficiency and recognition accuracy.																	1370-4621	1573-773X															10.1007/s11063-020-10316-6		JUL 2020											
J								Global synchronization of memristive hybrid neural networks via nonlinear coupling	NEURAL COMPUTING & APPLICATIONS										Memristive neural networks (MNNs); Synchronization; Linear convex combination; Nonlinear coupling; Quadratic function	TIME-VARYING DELAYS; EXPONENTIAL SYNCHRONIZATION; STABILITY ANALYSIS; INEQUALITY; CRITERIA; SYSTEMS; PERIODICITY; PASSIVITY; DISCRETE	This paper probes into the synchronization for memristor-based hybrid neural networks via nonlinear coupling. At first, a new condition is established to judge whether quadratic functions are negative or not on a closed interval regardless of their concavity or convexity. Then, by utilizing Legendre orthogonal polynomials, a recent extended integral inequality with free matrices is popularized to get tighter lower bound of some integral terms. Next, based on a novel Lyapunov functional, by applying our new integral inequality with free matrices, linear convex combination method and the new criterion, a new delay-dependent condition is gained to reach the global synchronization for the considered neural networks. At last, an example is presented to account for the validity of our results.																	0941-0643	1433-3058															10.1007/s00521-020-05166-1		JUL 2020											
J								Automatic room information retrieval and classification from floor plan using linear regression model	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Floor plan; Image retrieval; Regression model; a-shape; Document image analysis; Pattern recognition	SYMBOL RECOGNITION; SEGMENTATION	The automatic creation of a repository of the building's floor plan helps a lot to the architects to reuse them. The basic approach is to extract and recognize texts, symbols or graphics to retrieve the information of the floor plan from the images. This paper proposes a floor plan information retrieval algorithm. The proposed algorithm is based on shape extraction and room identification alpha-shape is used for finding an accurate shape. From the detected shapes, actual areas of rooms are calculated. Later, a regression model-based binary room classification model is proposed to classify them into room-type, i.e., bedroom, drawing room, kitchen, and non-room-type, i.e., parking porch, bathroom, study room and prayer room. The proposed model is tested on the CVC-FP dataset with an average room detection accuracy of 85.71% and room recognition accuracy of 88%.																	1433-2833	1433-2825				DEC	2020	23	4					253	266		10.1007/s10032-020-00357-x		JUL 2020											
J								Legal requirements on explainability in machine learning	ARTIFICIAL INTELLIGENCE AND LAW										Interpretability; Explainability; Machine learning; Law	AUTOMATED DECISION-MAKING; EXPLANATION; SELECTION; MODELS	Deep learning and other black-box models are becoming more and more popular today. Despite their high performance, they may not be accepted ethically or legally because of their lack of explainability. This paper presents the increasing number of legal requirements on machine learning model interpretability and explainability in the context of private and public decision making. It then explains how those legal requirements can be implemented into machine-learning models and concludes with a call for more inter-disciplinary research on explainability.																	0924-8463	1572-8382															10.1007/s10506-020-09270-4		JUL 2020											
J								A new chaotic complex map for robust video watermarking	ARTIFICIAL INTELLIGENCE REVIEW										Blind and robust video watermarking; Chaotic complex map; IWT; DWT; CT; SVD	SINGULAR-VALUE DECOMPOSITION; INTEGER WAVELET TRANSFORM; IMAGE WATERMARKING; FRAGILE WATERMARKING; CONTOURLET TRANSFORM; SCHEME; BLIND; AUTHENTICATION; DOMAIN; COMPONENT	In this paper, using a new two-dimensional complex map, a secure video watermarking system is presented. Standard analyzes have been performed to analyze a dynamical system to prove the existence of chaos in the proposed map and the results indicate a chaotic behavior in this complex chaotic map. In addition, an efficient algorithm based on IWT, DWT, and CT transforms with the participation of single value decomposition for the embedding and extraction process is introduced. The simulation results showed that the proposed algorithm has good visual quality based on criteria such as PSNR and SSIM. Geometric and non-geometric attacks were also performed on the video obtained by watermarking, and the results showed that the proposed algorithm in many attacks with a value of 1.00 for the NC criterion can be a very robust algorithm against attacks. A correlation-based process for detecting the rotational attack is also presented which makes the rotational geometric attack successfully pass. The comparison of simulation results with other similar algorithms shows that the proposed method performs better than any of these methods in terms of visual quality analysis and attack resistance and can be used as an efficient robust algorithm in applied processes.																	0269-2821	1573-7462															10.1007/s10462-020-09877-8		JUL 2020											
J								GSAPSO-MQC:medical image encryption based on genetic simulated annealing particle swarm optimization and modified quantum chaos system	EVOLUTIONARY INTELLIGENCE										Medical image encryption; Genetic simulated annealing particle swarm optimization; Modified quantum chaos system; Histogram analysis	ALGORITHM	Due to the large amount of image information data, high redundancy and high pixel correlation, the traditional medical image encryption algorithm is easy to be attacked by chosen plaintext. Therefore, a new medical image encryption algorithm combining genetic simulated annealing particle swarm optimization and modified quantum chaos system is proposed to obtain better security performance. Firstly, an improved quantum chaotic system is used to generate the key stream. Then the selection and cross operation of genetic algorithm are used to process the plaintext image. The optimal sequence generated by simulated annealing algorithm is used to scramble the image. Meanwhile, the particle swarm optimization (PSO) algorithm is introduced into the simulated annealing mechanism. The initial temperature is set according to the optimal fitness value of the initial population. Metropolis is used to optimize the generation of individual optimal position and global optimal position, and the inertial weight parameters of PSO algorithm are optimized to avoid particles falling into local optimal in the optimization process and improve the convergence speed of the algorithm. Through these three operations, the histogram of the scrambled image can be equalized to resist statistical attack. Experimental results and performance analysis show that the encryption system proposed in this paper can resist many typical attacks such as histogram analysis, correlation analysis, differential attack and violent attack, and has high security and encryption efficiency. Compared with other encryption methods, the encryption efficiency of our proposed method has improved by approximately 10%.																	1864-5909	1864-5917															10.1007/s12065-020-00440-6		JUL 2020											
J								Approximately Optimal Control of Discrete-Time Nonlinear Switched Systems Using Globalized Dual Heuristic Programming	NEURAL PROCESSING LETTERS										Globalized dual heuristic programming (GDHP); Optimal control; Switched systems; Neural networks	NEUTRAL SYSTEMS; STABILIZATION; DESIGNS	Based on the idea of data-driven control, a novel iterative adaptive dynamic programming (ADP) algorithm based on the globalized dual heuristic programming (GDHP) technique is used to solve the optimal control problem of discrete-time nonlinear switched systems. In order to solve the Hamilton-Jacobi-Bellman (HJB) equation of switched systems, the iterative ADP method is proposed and the strict convergence analysis is also provided. Three neural networks are constructed to implement the iterative ADP algorithm, where a novel model network is designed to identify the system dynamics, a critic network is used to approximate the cost function and its partial derivatives, and an action network is provided to obtain the approximate optimal control law. Two simulation examples are described to illustrate the effectiveness of the proposed method by comparing with the heuristic dynamic programming (HDP) and dual heuristic programming (DHP) methods.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1089	1108		10.1007/s11063-020-10278-9		JUL 2020											
J								Time Series Prediction Method Based on Variant LSTM Recurrent Neural Network	NEURAL PROCESSING LETTERS										Deep learning; Time series prediction; Recurrent neural network; Variant LSTM network		Time series prediction problems are a difficult type of predictive modeling problem. In this paper, we propose a time series prediction method based on a variant long short-term memory (LSTM) recurrent neural network. In the proposed method, we firstly improve the memory module of the LSTM recurrent neural network by merging its forget gate and input gate into one update gate, and using Sigmoid layer to control information update. Using improved LSTM recurrent neural network, we develop a time series prediction model. In the proposed model, the parameter migration method is used model update to ensure the model has good predictive ability after predicting multi-step sequences. Experimental results show, compared with several typical time series prediction models, the proposed method have better performance for long-sequence data prediction.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1485	1500		10.1007/s11063-020-10319-3		JUL 2020											
J								A Review of Dynamic Maps for 3D Human Motion Recognition Using ConvNets and Its Improvement	NEURAL PROCESSING LETTERS										Dynamic maps; 3D human motion recognition; ConvNets		RGB-D based action recognition is attracting more and more attention in both the research and industrial communities. However, due to the lack of training data, pre-training based methods are popular in this field. This paper presents a review of the concept of dynamic maps for RGB-D based human motion recognition using pretrained models in image domain. The dynamic maps recursively encode the spatial, temporal and structural information contained in the video sequence into dynamic motion images simultaneously. They enable the usage of Convolutional Neural Network and its pretained models on ImageNet for 3D human motion recognition. This simple, compact and effective representation achieves state-of-the-art results on various gesture/action/activities recognition datasets. Based on the review of previous methods using this concept upon different modalities (depth, skeleton or RGB-D data), a novel encoding scheme is developed and presented in this paper. The improved method generates effective flow-guided dynamic maps, and they could select the high motion window and distinguish the order among the frames with small motion. The improved flow-guided dynamic maps achieve state-of-the-art results on the large Chalearn LAP IsoGD and NTU RGB+D datasets.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1501	1515		10.1007/s11063-020-10320-w		JUL 2020											
J								ARBF: adaptive radial basis function interpolation algorithm for irregularly scattered point sets	SOFT COMPUTING										RBF interpolation; Scattered point sets; Shape factor; Point density	OPTIMAL SHAPE-PARAMETERS; ERROR ESTIMATE; OPTIMIZATION; COMPUTATION; EQUATIONS	Radial basis functions (RBFs) are isotropic, simple in form, dimensionally independent and mesh-free and are suitable for interpolation and fitting of scattered data. In a scattered point set, the calculation accuracy of multiquadric (MQ) RBF interpolation is strongly related to the selection of the shape factor. There is still no uniform method for determining the shape factor. Many scholars focus on determining the single optimal shape factor and seldom consider the change in the shape factor with the spatial point density in scattered point sets. In this paper, an adaptive radial basis function (ARBF) interpolation algorithm is proposed. The shape factors of MQ functions are determined adaptively by the local point densities of the points to be interpolated. To evaluate the computational performance of the ARBF interpolation algorithm, twelve groups of benchmark tests are conducted in this paper. We found that (1) the numerical error of ARBF interpolation is approximately 10% less than that of commonly used RBF interpolation with the shape factor recommended by Hardy. (2) The computational efficiency of ARBF interpolation is 1-2.5% lower than that of commonly used RBF interpolation with the shape factor recommended by Hardy.																	1432-7643	1433-7479															10.1007/s00500-020-05211-0		JUL 2020											
J								A new approach to evaluate linear programming problem in pentagonal neutrosophic environment	COMPLEX & INTELLIGENT SYSTEMS										Linear Programming Problem; PNN; PNLP; Ranking Function; CrLP	DECISION-MAKING; DEFUZZIFICATION; AGGREGATION; OPERATORS; NUMBER	In this paper, authors disclose a new concept of pentagonal neutrosophic (PN) approach to solve linear programming (LP) problem. To best of our insight, there is no approach for solving PNLP problem. For the first time, we take up the PNLP problem where the objectives, constraints are considered as pentagonal neutrosophic numbers (PNN). To deign our algorithm, we described the PN arithmetic operation laws and mathematical computation in PNN environment. This proposed method is based on ranking function and convert to its equivalent crisp LP (CrLP) problem. The obtained CrLP issue is presently being tackled by any LP method which is effectively accessible. To legitimize the proposed technique, some numerical tests are given to show the adequacy of the new model.																	2199-4536	2198-6053															10.1007/s40747-020-00181-0		JUL 2020											
J								A new rough set model based on multi-scale covering	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Covering rough set; Multi-scale; Information system; Optimal scale combination	OPTIMAL SCALE SELECTION; NEIGHBORHOOD OPERATORS; DECISIONS; SYSTEMS	Multi-scale rough set has become one of the hot research topics in rough set. In this article, from the point of view of granule, we first give the definition of the scale relationship between two coverings. Then some properties about neighborhood and approximation operators are explored based on multi-scale covering. Further, we investigate the multi-scale covering approximation space and thus construct a rough set model based on multi-scale covering. In the multi-scale covering decision approximation space, several optimal scale coverings are studied. Finally, we show the connection between multi-scale covering rough sets and multi-scale information systems, and generalize the multi-scale covering approximation space. Rough sets based on multi-scale covering are more extensive than multi-scale information systems. In addition, we design couple of algorithms and several experiments are conducted to verify the efficiency of the algorithms.																	1868-8071	1868-808X															10.1007/s13042-020-01169-5		JUL 2020											
J								Learning sequence-to-sequence affinity metric for near-online multi-object tracking	KNOWLEDGE AND INFORMATION SYSTEMS										Multi-object tracking; Sequence-to-sequence; MOT Challenge	MULTITARGET TRACKING	In this paper, we propose a sequence-to-sequence affinity metric for the data association of near-online multi-object tracking. The proposed metric learns the affinity between track sequence consisting of the already associated detections and hypothesis sequence consisting of detections in the near future. With the potential hypothesis sequences, we leverage the idea that if a track sequence has a high affinity for a hypothesis sequence, and the hypothesis sequence also shares a close affinity for a current detection, then the affinity between the track sequence and the detection is high. By using the short hypothesis sequence as a "bridge", the proposed sequence-to-sequence affinity metric enhances the conventional track sequence to detection affinity metric and improves its robustness to object occlusion and missing. Besides, in order to eliminate the negative effects of false alarms, we propose a false alarm model using both appearance and scale features of detection. The robustness of the proposed affinity metric allows us to use a simple greedy data association algorithm. Experimental results on the challenging MOT16 and MOT17 benchmarks demonstrate the effectiveness of our method.																	0219-1377	0219-3116				OCT	2020	62	10					3911	3930		10.1007/s10115-020-01488-7		JUL 2020											
J								Vector Field Guided RRT* Based on Motion Primitives for Quadrotor Kinodynamic Planning	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Kinodynamic planning; Motion primitives; Optimal control; Quadrotor; Vector field	TRAJECTORY GENERATION	The intelligent drone is the key device of the future Internet of Drone, and its safe and robust flight in complicated environments still faces challenges. In this paper, we present a sampling-based kinodynamic planning algorithm for quadrotors, which plans a dynamically feasible trajectory in a complicated environment. We have designed a method to constrain the sampling state by using the vector field to construct a cone in the sampling stage of RRT*, so that the generated trajectory is connected as smoothly as possible to other states in the reachable set. The motion primitives are then generated by solving an optimal control problem and an explicit solution of the optimal duration for the motion primitives is given to optimally connect any pair of states. In addition, we have tried a new method to determine the neighbor radius for the non-Euclidean metrics of this paper. Finally, the planned trajectory is applied to the simulated quadrotor, which verifies the dynamic feasibility of the trajectory. Simulation results show that compared with the existing kinodynamic RRT* under the same number of iterations, the proposed algorithm explores more states with a shorter execution time and generates a smoother trajectory.																	0921-0296	1573-0409															10.1007/s10846-020-01231-y		JUL 2020											
J								Possibility Degree and Power Aggregation Operators of Single-Valued Trapezoidal Neutrosophic Numbers and Applications to Multi-Criteria Group Decision-Making	COGNITIVE COMPUTATION										Single-valued trapezoidal neutrosophic number; Multi-criteria group decision-making; Possibility degree; Power aggregation operator	INTUITIONISTIC FUZZY NUMBERS; CORRELATION-COEFFICIENT; GEOMETRIC OPERATORS; SET	Single-valued trapezoidal neutrosophic numbers (SVTNNs) are very useful tools to describe complex cognitive information because of their advantage in maintaining the completeness and accuracy of information. This paper develops a method based on the single-valued trapezoidal neutrosophic power-weighted aggregation operators and possibility degree of SVTNNs for dealing with multi-criteria group decision-making (MCGDM) problems. First, the limitations of the existing operations for SVTNNs are discussed, and then an improved operation is defined. Moreover, the possibility degree of two SVTNNs with consideration of the influence of risk attitudes is proposed, and the comparison rules for SVTNNs are thereby established. Based on the new operation and possibility degree of SVTNNs, the single-valued trapezoidal neutrosophic power average and single-valued trapezoidal neutrosophic power geometric operators are proposed to aggregate the single-valued trapezoidal neutrosophic information. Furthermore, a single-valued trapezoidal neutrosophic MCGDM method is developed. Finally, an example of a company selecting the most suitable green supplier is provided to present a comparative analysis between the proposed approach and other related methods. This example can demonstrate the effectiveness and flexibility of the proposed methodology.																	1866-9956	1866-9964															10.1007/s12559-020-09736-2		JUL 2020											
J								Multi-task Compositional Network for Visual Relationship Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION										Visual relationship detection; Object detection; Predicate detection; Significance detection; Multi-task		Previous methods treat visual relationship detection as a combination of object detection and predicate detection. However, natural images likely contain hundreds of objects and thousands of object pairs. Relying only on object detection and predicate detection is insufficient for effective visual relationship detection because the significant relationships are easily overwhelmed by the dominant less-significant relationships. In this paper, we propose a novel subtask for visual relationship detection, the significance detection, as the complement of object detection and predicate detection. Significance detection refers to the task of identifying object pairs with significant relationships. Meanwhile, we propose a novel multi-task compositional network (MCN) that simultaneously performs object detection, predicate detection, and significance detection. MCN consists of three modules, an object detector, a relationship generator, and a relationship predictor. The object detector detects objects. The relationship generator provides useful relationships, and the relationship predictor produces significance scores and predicts predicates. Furthermore, MCN proposes a multimodal feature fusion strategy based on visual, spatial, and label features and a novel correlated loss function to deeply combine object detection, predicate detection, and significance detection. MCN is validated on two datasets: visual relationship detection dataset and visual genome dataset. The experimental results compared with state-of-the-art methods verify the competitiveness of MCN and the usefulness of significance detection in visual relationship detection.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2146	2165		10.1007/s11263-020-01353-8		JUL 2020											
J								The epistemic opacity of autonomous systems and the ethical consequences	AI & SOCIETY										Epistemic opacity; Ethics of AI; Embodiment; Autonomous systems		This paper takes stock of all the various factors that cause the design-time opacity of autonomous systems behaviour. The factors include embodiment effects, design-time knowledge gap, human factors, emergent behaviour and tacit knowledge. This situation is contrasted with the usual representation of moral dilemmas that assume perfect information. Since perfect information is not achievable, the traditional moral dilemma representations are not valid and the whole problem of ethical autonomous systems design proves to be way more empirical than previously understood.																	0951-5666	1435-5655															10.1007/s00146-020-01024-9		JUL 2020											
J								Artificial intelligence, culture and education	AI & SOCIETY										Decision-making systems; Computing machines; Artificial intelligence; Semiotics; Culture; Society; Education		Sequential transformative design of research (Hanson et al. in J Couns Psychol 52(2):224-235, 2015; Groleau et al. in J Mental Health 16(6):731-741, 2007; Robson and McCartan in Real world research: a resource for users of social research methods in applied settings, Wiley, Chichester, 2016) allows testing a group of theoretical assumptions about the connections of artificial intelligence with culture and education. In the course of research, semiotics ensures the description of self-organizing systems of cultural signs and symbols in terms of artificial intelligence as a special set of algorithms. This approach helps to consider the arguments proposed by Searle (Behav Brain Sci 3(3):417-457, 1980) against 'strong' artificial intelligence. Searle (Behav Brain Sci 3(3):417-457, 1980) believes that artificial or machine intelligence cannot fully emulate the processes of the human mind. Machine intelligence shows own inevitable weakness. This is non-autonomous tool for computations and data operating. In fact, this tool cannot provide insight into real cognitive conditions. After Lotman and Uspensky (On the semiotics mechanism of culture, Alexandra, Tallinn, 1993), authors expand the meaning of artificial intelligence. The authors identify a cultural type of 'strong' artificial intelligence or 'self-increase ofLogos' in terms by Lotman and Uspensky (On the semiotics mechanism of culture, Alexandra, Tallinn, 1993). The interpretation of human intelligence as imitation of machine intelligence makes possible such immersion of artificial intelligence in culture. The authors reveal a case of self-organizing autonomous generation, encoding, decoding, reception, storage, and transmission of social information in the field of physical training. From the empirical studies it is clear that the organization of collective activities without external control ensures the development of positive emotions and social orientations. Interest in autonomous behavior provides the formation of educational and cognitive motives. As a special set of algorithms, these motives are the most promising and favorable for personal development.																	0951-5666	1435-5655															10.1007/s00146-020-01026-7		JUL 2020											
J								Detection and pose estimation of auto-rickshaws from traffic images	MACHINE VISION AND APPLICATIONS										Vehicle detection; Pose classification; ACF; Faster R-CNN	VEHICLE DETECTION SYSTEM	In intelligent transport systems, detection and identification of vehicle types enact a substantial role. In this context, this paper addresses the detection and pose classification of a specific vehicle type: auto-rickshaws which have been heavily neglected by the publicly available vehicle datasets, but remains the most commonly used and cheap form of transportation in south Asian countries. Here, we introduce a dataset for auto-rickshaws which consists of instances of varying shape, orientation, size, scale, colour, viewpoint and many more. Further, we carry out a detailed analysis on the performance of state-of-the-art detection algorithms based on both hand-designed and deep features on the proposed dataset. The introduction of pose classification along with the detection eventually results in better understanding of road scenes involving auto-rickshaws. As a matter of fact, we came up with revisions for the currently employed detection algorithms to achieve a low miss rate on the validation sets. It is evident that the findings of this study are tangible and enormously consequential to the road scene understanding and intelligent transportation of developing countries where auto-rickshaws play a pivotal role in public transportation.																	0932-8092	1432-1769				JUL 30	2020	31	6							54	10.1007/s00138-020-01106-0													
J								Parallel Dual Networks for Visual Object Tracking	APPLIED INTELLIGENCE										Computer vision; Object tracking; Siamese network; Deep features; Handcraft features	CORRELATION FILTER TRACKER; TEMPORAL INFORMATION; SIAMESE; ROBUST	Visual Object Tracking plays an essential role in solving many basic problems in computer vision. In order to improve the tracking accuracy, the previous methods have prevented tracking failures from occurring by improving the ability to describe the target. However, few of them consider ways to relocate and track the target after a tracking failure. In this paper, we propose the use of a parallel dual network for visual object tracking. This is constructed from two networks and an adjustment module to enable judgement of tracking failures, as well as target relocation and tracking. Firstly, we employ the Siamese matching method and correlation filter method to build tracking network and inspecting network. Both networks track the target simultaneously to obtain two tracking results. Secondly, an adjustment module is constructed, which compares the overlap ratio of the two tracking results with a set threshold, then fuses them or selects the best one. Finally, the fusion or selection result is output and the tracker is updated. We perform comprehensive experiments on five benchmarks: VOT2016, UAV123, Temple Color-128, OTB-100 and OTB-50. The results demonstrate that, compared with other state-of-the-art algorithms, the proposed tracking method improves tracking precision while maintaining real-time performance.																	0924-669X	1573-7497															10.1007/s10489-020-01783-4		JUL 2020											
J								Effective Constructive Heuristic and Metaheuristic for the Distributed Assembly Blocking Flow-shop Scheduling Problem	APPLIED INTELLIGENCE										Distributed production; Assembly; Makespan; Constructive heuristic; Iterated local search	ITERATED LOCAL SEARCH; MINIMIZING MAKESPAN; GREEDY ALGORITHM; HYBRID FLOWSHOP; TOTAL FLOWTIME; PERMUTATION; OPTIMIZATION	Scheduling in distributed production system has become an active research field in recent years. This paper investigates the distributed assembly blocking flow-shop scheduling problem (DABFSP), which consists of two stages: production and assembly. The first stage is processing jobs in several identical factories. Each factory has a series of machines no intermediate buffers existing between adjacent ones. The second stage assembles the processed jobs into the final products through a single machine. The objective is to minimize the maximum completion time or makespan of all products. To address this problem, a constructive heuristic is proposed based on a new assignment rule of jobs and a product-based insertion procedure. Afterwards, an iterated local search (ILS) is presented, which integrates an integrated encoding scheme, a multi-type perturbation procedure containing four kinds of perturbed operators based on problem-specific knowledge and a critical-job-based variable neighborhood search. Finally, a comprehensive computational experiment and comparisons with the closely related and well performing methods in the literature are carried out. The experimental and comparison results show that the proposed constructive heuristic and ILS can solve the DABFSP effectively and efficiently.																	0924-669X	1573-7497															10.1007/s10489-020-01809-x		JUL 2020											
J								Light-YOLOv3: fast method for detecting green mangoes in complex scenes using picking robots	APPLIED INTELLIGENCE										Green mangoes; Picking robots; YOLOv3; Convolutional neural networks (CNNs)		When a robot picks green fruit under natural light, the color of the fruit is similar to the background; uneven lighting and fruit and leaf occlusion often affect the performance of the detection method. We take green mangoes as an experimental object. A lightweight green mangoes detection method based on YOLOv3 is proposed here. To improve the detection speed of the method, we first combine the color, texture, and shape features of green mango to design a lightweight network unit to replace the residual units in YOLOv3. Second, the improved Multiscale context aggregation (MSCA) module is used to concatenate multilayer features and make predictions, solving the problem of insufficient position information and semantic information on the prediction feature map in YOLOv3; this approach effectively improves the detection effect for the green mangoes. To address the overlap of green mangoes, soft non-maximum suppression (Soft-NMS) is used to replace non-maximum suppression (NMS), thereby reducing the missing of predicted boxes due to green mango overlaps. Finally, an auxiliary inspection green mango image enhancement algorithm (CLAHE-Mango) is proposed, is suitable for low-brightness detection environments and improves the accuracy of the green mango detection method. The experimental results show that the F1% of Light-YOLOv3 in the test set is 97.7%. To verify the performance of Light-YOLOv3 under the embedded platform, we embed one-stage methods into the Adreno 640 and Mali-G76 platforms. Compared with YOLOv3, the F1% of Light-YOLOv3 is increased by 4.5%, and the running speed is increased by 5 times, which can meet the real-time running requirements for picking robots. Through three sets of comparative experiments, we could determine that our method has the best detection results in terms of dense, backlit, direct light, night, long distance, and special angle scenes under complex lighting.																	0924-669X	1573-7497															10.1007/s10489-020-01818-w		JUL 2020											
J								Network traffic detection for peer-to-peer traffic matrices on bayesian network in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network traffic modeling; PVM fault localization feature; Bayesian network; Bayesian network peer-to-peer network traffic design; Co-existence mechanism		With the wide application of wireless sensor networks, network security has been a terrible problem when it provides many more services and applications. Rapid usage of internet and connectivity demands a network anomaly system combating cynical network attacks. Meanwhile, it is a common approach for acquiring, which can be used by network operators to carry out network management and configuration. Moreover, a great number of evaluations have been proposed to simulate and analyse the Wireless Sensor Network traffic, it is still a remarkable challenge since, and network traffic characterization has been tremendously changed, in particular, for a sensor computing network. Bayesian Based Network Traffic Prediction (BNTP) is proposed to solve the deep learning of statistical features of network traffic flow so that all the packets were sent to the receiver properly without any traffic density. Bayesian network-based peer-to-peer network traffic design is proposed to determine the spatial structure of traffic flow. PVM fault localization feature is proposed to remove the accuracy measure issues and performance problems. The co-existence mechanism is used to minimize the inference and overlap problem in wireless network devices. This paper avoids the conflicts in traffic analysis and statistical features of the network. The performance of the network is increased to 80% when compared to the existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02355-7		JUL 2020											
J								Power conserving resource allocation scheme with improved QoS to promote green cloud computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Green cloud computing; CML; GML; Power consumption	ENERGY	Though cloud computing has grabbed the attention of several industrialists and educationalists, the only concern of cloud computing is the uncontrollable rise of cloud data centres. The improper utilization of cloud resources paves way for inefficiency and environmental hazard as well. Understanding the seriousness of this issue, several researchers contribute to promote green cloud computing through different ways. The Green Cloud Computing is the act of executing approaches and the techniques to improve proficiency of the figuring assets so as to decrease the vitality utilization and natural effect of their usage. The power consumption of the data centre offers the features like web based checking, live virtual machine movement, and advancement of Virtual Machine Placement. This work focuses on effective resource allocation scheme for cloud users, which does not compromise on QoS by employing two layers such as Cloud Manager Layer (CMLs) and Green Manager Layer (GML). The CML is responsible for choosing the suitable resources out of all available resources and the GML picks the best one out of it. Due to this optimal selection of resource, the average service response time is reduced at the cost of minimized power consumption. When handling 500 service requests, the proposed work consumes about 4298 W and the comparative approaches consume more power.																	1868-5137	1868-5145															10.1007/s12652-020-02384-2		JUL 2020											
J								Designing an IoT based autonomous vehicle meant for detecting speed bumps and lanes on roads	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Autonomous vehicle; Speed bump; Road lane detection; IoT	UNMANNED AERIAL VEHICLES; INTERNET; THINGS	Due to the excessive demand of automated solutions, intelligent transportation is the currently popular research area. Internet of things (IoT) is a boon to the today's world, as communication with almost all the things around is made possible. IoT based autonomous vehicle (AV) is the most researchable topic, which is being approached by technology giants such as Google, Hyundai, Uber and so on. Inspired by this concept, this paper presents a technique to detect speed bumps and lanes on roads by utilizing IoT. This work considers that the speed bump and lane detection are the most fundamental, yet primary requirements of any AV. Both the goals are attained by employing ultrasonic and Wi-Fi sports camera sensor respectively. The lane detection is carried out by a computer vision based algorithm, which processes the video data. Furthermore, this work can be utilized on vehicle with drivers for the sake of alerting. The performance of the work is quite satisfactory in terms of accuracy, sensitivity, specificity and F-measure with reasonable time consumption.																	1868-5137	1868-5145															10.1007/s12652-020-02419-8		JUL 2020											
J								Application of back propagation artificial neural network in detection and analysis of diabetes mellitus	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diabetes mellitus; Pancreas; Vibration; Artificial neural network (ANN); Correlation coefficient		Diabetes Mellitus affects adults and children, causing changes in lifestyle. The diabetic affected person count has increased drastically worldwide over the last few years; about 425 million people have diabetes. By 2030, it is predicted that diabetic disorder will be the seventh leading cause of human death. Diabetes mellitus is measured invasively. This method has limitations such as patient's preparation, piercing of the skin, which causes infection and needs for skilled technicians. In order to avoid the limitations of invasive methods, vibrations from the pancreas are acquired using a smartphone accelerometer sensor and detecting the value of diabetes. The human body has a unique energy signature for every organ, which leads to vibrations with different frequencies. The frequency of the vibration signal from the pancreas is proportional to insulin secretion and dynamics. The signals obtained from the accelerometer sensor are trained and analyzed with the Levenberg-Marquardt algorithm for obtaining the relation between the excess insulin secretion and clinical value of the diabetic level of the person. The accelerometer signals and clinical values are modeled with Regression analysis for the diabetic and non-diabetic persons. The results show the correlation between the fluid dynamics of insulin and clinical value at about 95% in prediction.																	1868-5137	1868-5145															10.1007/s12652-020-02371-7		JUL 2020											
J								Brain image classification by the combination of different wavelet transforms and support vector machine classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MRI brain images; Feature selection; DWT; SWT; DMWT; SVM classifier	MRI; ENHANCEMENT	The human brain is the primary organ, and it is located in the centre of the nervous system in the human body. The abnormal cells in the brain are known as a brain tumor. The tumor in the brain does not spread to the other parts of the human body. Early diagnosis of brain tumor is required. In this work, an efficient technique is presented for magnetic resonance imaging (MRI) brain image classification using different wavelet transforms like discrete wavelet transform (DWT), stationary wavelet transform (SWT) and dual tree M-band wavelet transform (DMWT) for feature extraction and selection of coefficients and support vector machine classifier is used for classification. The normal and abnormal MRI brain image features are decomposed by DWT, SWT and DMWT. The coefficients of sub-bands are selected by rank features for the classification. Results show that DWT, SWT and DMWT produce 98% accuracy for the MRI brain classification system.																	1868-5137	1868-5145															10.1007/s12652-020-02299-y		JUL 2020											
J								Action recognition using Correlation of Temporal Difference Frame (CTDF)-an algorithmic approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Action recognition; Interest point; Human motion analysis		Presently in most of the real world applications like video surveillance systems, human activities are captured and retained as multimodal information for authorized permitted actions. However the degree of accuracy in recognition of such actions greatly depends on many factors, including occlusion, illumination factor, cluttered environment, and so on. In this work we propose the correlation of temporal difference frame (CTDF) algorithm which captures the local maxima's of every small movement and its neighboring information. Temporal difference obtained between frames, block size defined to obtain the surround information and finally, the comparison of one to all points between identified frames greatly increase the accuracy. The algorithm takes in the raw video input of the standard UT interaction and BIT interaction datasets. Features extracted using the proposed algorithm is passed through variants of SVM which gives state of art results, 95.83% accuracy for UT Interaction and an accuracy of 90.4% for BIT interaction dataset.																	1868-5137	1868-5145															10.1007/s12652-020-02378-0		JUL 2020											
J								Robust image watermarking using fractional Krawtchouk transform with optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										FrKT; Histogram shifting; Firefly Algorithm; Cuckoo Search Algorithm; Optimization	DWT-SVD; DOMAIN	Fractional Krawtchouk Transform is a generalization of the Krawtchouk transforms which has two fractional orders. By adjusting these fractional orders in the weighted two dimensional Krawtchouk polynomials, local image features can be located. This paper proposes a robust image watermarking method using FrKT with firefly and cuckoo search optimization algorithms. The frequency domain image is obtained by applying FrKT for the input image blocks. The optimal fractional parameters of the transform improve the imperceptibility of the secret data in the host images. The fractional parameter selection for the image transformation is performed by Firefly optimization algorithm. Also, the optimal location in each block to hide the secret data is identified by the cuckoo search algorithm. The histogram shifting technique is used to embed the secret data in the optimal locations due to its less computational complexity. The parameters like Peak Signal to Noise Ratio, Normalized Correlation Coefficient, Structural SIMilarity index, Bit Error Rate are used for comparison of the proposed method using the optimization algorithms. The experimental results of the proposed method FrKT with combination of both Firefly and cuckoo search optimization shows better quality of the watermark, robustness and imperceptibility against various attacks. It can be concluded that the proposed FrKT + CS + FA provides an average of 0.92 for most of the attacks that prove the robustness of the proposed scheme.																	1868-5137	1868-5145															10.1007/s12652-020-02379-z		JUL 2020											
J								Designing the routing protocol with secured IoT devices and QoS over Manet using trust-based performance evaluation method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Manet; QoS; MSRS; IoT; DS; DFS; Secure routing		The problem of routing in Manet are analyzed and different techniques are described towards QoS development of Manet. However, the modern IoT devices have been considered for the support of routing. Engaging IoT devices in Manet routing is considered and how the trust of IoT devices can measured is focused. Number of techniques discussed earlier with secure routing with IoT devices in Manet. They suffer to achieve higher performance in most QoS parameters. This paper proposes a real time secure route analysis (RSRA) approach for the secure routing in Manet. The method not only considers the strategy of intermediate nodes of route discovered, but also considers the presence of IoT devices and their trust. First, the method discovers the list of routes between any source and destination. For each mobile node, the trustworthy is verified by considering the location, mobility speed, energy, and number of transmission involved, their neighbor list and so on. The IoT devices are verified for their trustworthy based on their earlier support contributed to the network. The method measures mobile node secure route support (MSRS) for the mobile nodes where device support (DS) is measured for the IoT devices. Using these two measures, the method measure the data forwarding support (DFS) value by considering the number of IoT devices in the route. Based on the DFS measure, a single route has been selected which improves the QoS of Manet.																	1868-5137	1868-5145															10.1007/s12652-020-02358-4		JUL 2020											
J								An adaptive content based closer proximity pixel replacement algorithm for high density salt and pepper noise removal in images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Salt and pepper noise; Mode; Euclidean distance; Median	MEDIAN FILTERS; IMPULSE NOISE	An Adaptive Content based Closer Proximity Pixel Replacement algorithm for the removal of high density salt and pepper noise in images is proposed. The algorithm uses decision tree to identify and correct the pixels of the image is noisy or not. The algorithm finds Euclidean distance between the processed pixel and the number of non-noisy pixels inside the current processing kernel. The algorithm requires only two non-noisy pixels to be present in kernel for the algorithm to operate. The faulty pixels are replaced only by the median of pixels that occurs more frequently in the current processing kernel based on the Euclidean distance. The algorithm increases the window size by two when there are no non-noisy pixels in the current processing kernel. The proposed algorithm was compared with 16 standard and existing algorithms derived from recent literatures. Exhaustive experiments on standard database images suggest that the algorithm exhibit excellent noise suppression and good information preservation characteristics even at very high noise densities.																	1868-5137	1868-5145															10.1007/s12652-020-02376-2		JUL 2020											
J								De-ghosted HDR video acquisition for embedded systems Ghost-free HDR video of motion objects from stationary cameras	JOURNAL OF REAL-TIME IMAGE PROCESSING										HDR; HDR Acquisition; Embedded systems; Real-time HDR processing; HDR De-ghosting		This paper proposes a novel ghost-free High Dynamic Range (HDR) multi-exposure video acquisition suitable for real-time implementation in embedded systems. While the method is limited to stationary cameras, it achieves, with low requirements on resources, results comparable to state-of-the-art de-ghosting methods that are often very computationally expensive and almost impossible to implement in smart cameras and embedded systems. The paper describes the method itself and includes an evaluation of the performance on selected embedded platforms and a comparison of the results to the state of the art using HDR datasets.																	1861-8200	1861-8219															10.1007/s11554-020-01001-x		JUL 2020											
J								Reasoner = Logical Calculus plus Rule Engine	KUNSTLICHE INTELLIGENZ												We propose using rule languages to encode complex reasoning algorithms in a declarative way. This approach-which follows the classical slogan "Algorithm = Logic + Control"-promises to turn high-level specifications of logical calculi as systems of inference rules into declarative rule-based models that can be executed on state-of-the-art rule engines. Simple rule languages suffice for simple logics, and we review our results on using Datalog rules to reason in the description logic EL. For more expressive logics, a suitably expressive yet implementable rule language often seems to be missing. To fill this gap, we consider an extension of Datalog with sets, Datalog(S), that can be executed by modern existential-rule reasoners, and we use it to present a rule-based reasoning calculus for the expressive description logic ALC.																	0933-1875	1610-1987															10.1007/s13218-020-00667-6		JUL 2020											
J								Multi-scale generative adversarial network for improved evaluation of cell-cell interactions observed in organ-on-chip experiments	NEURAL COMPUTING & APPLICATIONS										Biomedical application; Deep learning; Generative adversarial network (GAN); Time-lapse microscopy; Video prediction	CANCER	Organs On a Chip (OOCs) represent a sophisticated approach for exploring biological mechanisms and developing therapeutic agents. In conjunction with high-quality time-lapse microscopy (TLM), OOCs allow for the visualization of reconstituted complex biological processes, such as multi-cell-type migration and cell-cell interactions. In this context, increasing the frame rate is desirable to reconstruct accurately cell-interaction dynamics. However, a trade-off between high resolution and carried information content is required to reduce the overall data volume. Moreover, high frame rates increase photobleaching and phototoxicity. As a possible solution for these problems, we report a new hybrid-imaging paradigm based on the integration of OOC/TLMs with a Multi-scale Generative Adversarial Network (GAN) predicting interleaved video frames with the aim to provide high-throughput videos. We tested the performance of the predictive capability of GAN on synthetic videos, as well as on real OOC experiments dealing with tumor-immune cell interactions. The proposed approach offers the possibility to acquire a reduced number of high-quality TLM images without any major loss of information on the phenomena under investigation.																	0941-0643	1433-3058															10.1007/s00521-020-05226-6		JUL 2020											
J								RemNet: remnant convolutional neural network for camera model identification	NEURAL COMPUTING & APPLICATIONS										Digital image forensics; Camera model identification; Convolutional neural networks; Remnant block	SENSOR PATTERN NOISE; FORENSICS	Camera model identification (CMI) has gained significant importance in image forensics as digitally altered images are becoming increasingly commonplace. In this paper, a novel convolutional neural network (CNN) architecture is proposed for CMI with emphasis given on the preprocessing task considered to be inevitable for removing the scene content that heavily obscures the camera model fingerprints. Unlike the conventional approaches where fixed filters are used for preprocessing, the proposed remnant blocks, when coupled with a classification block and trained end-to-end minimizing the classification loss, learn to suppress the unnecessary image contents dynamically. This helps the classification block extract more robust camera model-specific features for CMI from the remnant of the image. The whole network, called RemNet, consisting of a preprocessing block and a shallow classification block, when trained on 18 models from the Dresden database, shows 100% accuracy for 16 camera models with an overall accuracy of 97.59% on test images from unseen devices, outperforming the state-of-the-art deep CNNs used in CMI. Furthermore, the proposed remnant blocks, when cascaded with the existing deep CNNs, e.g., ResNet, DenseNet, boost their performances by a large margin. The proposed approach proves to be very robust in identifying the source camera models, even if the original images are post-processed. It also achieves an overall accuracy of 95.11% on the IEEE Signal Processing Cup 2018 dataset, which indicates its generalizability.																	0941-0643	1433-3058															10.1007/s00521-020-05220-y		JUL 2020											
J								Scale-aware feature pyramid architecture for marine object detection	NEURAL COMPUTING & APPLICATIONS										Marine object detection; Feature pyramid network; Non-maximum suppression; Underwater image		Marine object detection is an appealing but challengeable task in computer vision. Even though recent popular object detection algorithms perform well on common classes, they cannot acquire satisfied detection performance on marine objects because underwater images are affected by color cast and blur, and scales of the target in underwater images are usually small. These phenomena aggravate the difficulty of detection. Thus, it is urgent to design a proper structure to settle marine object detection issues. To this end, this paper proposes a novel scale-aware feature pyramid architecture named SA-FPN to extract abundant robust features on underwater images and improve the performance on marine object detection. Specifically, we design a special backbone subnetwork to improve the ability of feature extraction, which could provide richer fine-grained features for small object detection. What is more, this paper proposes a multi-scale feature pyramid to enrich the semantic features for prediction. Each feature map is enhanced by the higher level layer with context information through a top-down upsampling pathway. Through obtaining ample feature maps on underwater images, our algorithm could generate multiple bounding boxes for each target. To mitigate the reduplicative boxes and avoid miss suppression, we replace the non-maximum suppression method with soft non-maximum suppression. In this paper, we evaluate our algorithm on underwater image datasets and achieve 76.27% mAP. Meanwhile, we conduct experiments on PASCAL VOC datasets and smart unmanned vending machines datasets and get 79.13% mAP and 91.81% mAP, respectively. The experimental results reveal that our approach achieves best performance not only on marine object detection, but also on common classes.																	0941-0643	1433-3058															10.1007/s00521-020-05217-7		JUL 2020											
J								Evaluating the bond strength of FRP-to-concrete composite joints using metaheuristic-optimized least-squares support vector regression	NEURAL COMPUTING & APPLICATIONS										Fiber-reinforced polymer; Reinforced concrete; Bond strength; Lest squares support vector regression; Levy flight; Beetle antennae search algorithm	RC BEAMS; COMPRESSIVE STRENGTH; CROSS-VALIDATION; SEARCH; MODEL; PREDICTION; MACHINES; TUTORIAL; MODULUS	The reinforced concrete (RC) infrastructure can be retrofitted by adhesively bonding fiber-reinforced polymers (FRPs) to the tension face. In the FRP-to-concrete bonding system, the debonding of the FRP plate from the member is the most common failure type. Predicting the bond strength of FRP-to-concrete joints using traditional predictive models is far from being satisfactory because of the highly nonlinear relationships between the bond strength and a large number of influencing variables. To address this issue, this study proposes a metaheuristic-optimized least-squares support vector regression (LSSVR) model to predict the bond strength of FRP-to-concrete joints. The hyperparameters of the LSSVR model are tuned using a recently proposed beetle antennae search (BAS) algorithm. In addition, the Levy flight is incorporated in the BAS algorithm to improve its searching efficiency. The proposed model is then trained on a dataset collected from internationally published literature. To understand the importance of each input variable on the bond strength, the variable importance is calculated using the random forest algorithm. The results show that the proposed LBAS-LSSVR model has comparatively high prediction accuracy, as indicated by a high correlation coefficient (0.983) and low root mean square error (1.99 MPa) on the test set. Width of FRP is the most sensitive variable to the bond strength. The proposed model can be extended to solve other regression problems in structural engineering.																	0941-0643	1433-3058															10.1007/s00521-020-05191-0		JUL 2020											
J								Cognitive Traffic Anomaly Prediction from GPS Trajectories Using Visible Outlier Indexes and Meshed Spatiotemporal Neighborhoods	COGNITIVE COMPUTATION										Cognitive anomaly prediction; Trajectories; Abnormal levels; Visible outlier indexes; Spatiotemporal neighborhoods	FLOW PREDICTION	The advancement of cognitive computing for traffic status understanding, powered by machine learning and data analytics, enables prediction of traffic anomalies from continuously generated big GPS trajectory data. Existing methods generally use traffic indicators such as traffic flows and speeds to detect anomalies, but they may over-identify anomalies while missing the critical ones. For example, they use historical anomalies to train the prediction model, but past anomalies may not be a perfect indication of future anomalies since anomalies are often rare. In this paper, we propose a novel cognitive approach, a Visible Outlier Indexes and Meshed Spatiotemporal Neighborhoods (VOI-MSN) method, to predict traffic anomalies from GPS trajectories. In the VOI-MSN method, two cognitive techniques are provided. The first is VOI, which measures the abnormal scores using overall samples and can be intuitively understood by humans. The second is MSN, which learns the dynamic impact range (i.e., spatiotemporal neighborhood) from historical trajectory data and provides a complete and exact analysis of the local traffic situation. It emulates human cognitive processing to adaptively judge the impact range by experience. The effectiveness of the proposed method is demonstrated using a massive trajectory dataset with 2.5 billion location records for 27,266 taxis, and it achieves higher precision and recall in predicting traffic anomalies than the counterpart methods. The VOI-MSN method achieves high accuracy and recall for predicting traffic anomalies. It outperforms traffic indicator-based (speed and traffic flow) methods, the fixed-size spatial neighborhood method and the causal network method.																	1866-9956	1866-9964				SEP	2020	12	5					967	978		10.1007/s12559-020-09735-3		JUL 2020											
J								A Novel Functional Link Network Stacking Ensemble with Fractal Features for Multichannel Fall Detection	COGNITIVE COMPUTATION										Fall detection; Machine learning; Artificial neural network; RVFL neural network; Ensemble learning; Fractal features		Falls are a major health concern and result in high morbidity and mortality rates in older adults with high costs to health services. Automatic fall classification and detection systems can provide early detection of falls and timely medical aid. This paper proposes a novel Random Vector Functional Link (RVFL) stacking ensemble classifier with fractal features for classification of falls. The fractal Hurst exponent is used as a representative of fractal dimensionality for capturing irregularity of accelerometer signals for falls and other activities of daily life. The generalised Hurst exponents along with wavelet transform coefficients are leveraged as input feature space for a novel stacking ensemble of RVFLs composed with an RVFL neural network meta-learner. Novel fast selection criteria are presented for base classifiers founded on the proposed diversity indicator, obtained from the overall performance values during the training phase. The proposed features and the stacking ensemble provide the highest classification accuracy of 95.71% compared with other machine learning techniques, such as Random Forest (RF), Artificial Neural Network (ANN) and Support Vector Machine. The proposed ensemble classifier is 2.3x faster than a single Decision Tree and achieves the highest speedup in training time of 317.7x and 198.56x compared with a highly optimised ANN and RF ensemble, respectively. The significant improvements in training times of the order of 100x and high accuracy demonstrate that the proposed RVFL ensemble is a prime candidate for real-time, embedded wearable device-based fall detection systems.																	1866-9956	1866-9964				SEP	2020	12	5					1024	1042		10.1007/s12559-020-09749-x		JUL 2020											
J								Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-Based Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION												Low-shot sketch-based image retrieval is an emerging task in computer vision, allowing to retrieve natural images relevant to hand-drawn sketch queries that are rarely seen during the training phase. Related prior works either require aligned sketch-image pairs that are costly to obtain or inefficient memory fusion layer for mapping the visual information to a semantic space. In this paper, we address any-shot,i.e. zero-shot and few-shot, sketch-based image retrieval (SBIR) tasks, where we introduce the few-shot setting for SBIR. For solving these tasks, we propose a semantically aligned paired cycle-consistent generative adversarial network (SEM-PCYC) for any-shot SBIR, where each branch of the generative adversarial network maps the visual information from sketch and image to a common semantic space via adversarial training. Each of these branches maintains cycle consistency that only requires supervision at the category level, and avoids the need of aligned sketch-image pairs. A classification criteria on the generators' outputs ensures the visual to semantic space mapping to be class-specific. Furthermore, we propose to combine textual and hierarchical side information via an auto-encoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in any-shot SBIR performance over the state-of-the-art on the extended version of the challenging Sketchy, TU-Berlin and QuickDraw datasets.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2684	2703		10.1007/s11263-020-01350-x		JUL 2020											
J								Dm6A-TSVM: detection ofN(6)-methyladenosine (m(6)A) sites from RNA transcriptomes using the twin support vector machines	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										N-6-methyladenosine; Dm6A-TSVM; Twin support vector machines	N-6-METHYLADENOSINE; REVEALS	N-6-methyladenosine (m(6)A) is closely related to various life processes and diseases. The detection of genomic-level m(6)A sites plays an important role in explaining its biological mechanism. However, the current mainstream m(6)A sites detection method has limited precision. In this paper, a novel m(6)A sites detection method called "Dm6A-TSVM" is proposed. The feature vectors are firstly extracted from the mRNA sequences according to their nucleotide chemical property and position statistical distribution characteristics. Then the two kinds of features are combined, and the m(6)A sites detection model is constructed by the twin support vector machines method. Finally, based on the standard yeast dataset, the cross-validation experimental method is used to verify Dm6A-TSVM. The results demonstrate that the Dm6A-TSVM method is significantly better than the current mainstream m(6)A sites detection method, and its accuracy (ACC) value reaches 82.81%.																	1868-5137	1868-5145															10.1007/s12652-020-02409-w		JUL 2020											
J								Privacy-preserving location-based services for mobile users using directional service fetching algorithm in wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless networks; LSS; Mobility; MADSF; DPR; Routing; QoS; Content fetching	QUALITY; SENSOR	The wireless network has been utilized for several purposes in modern information world. Among them, the data collection and service access has been identified as key issue. As the services in wireless networks accessed through different intermediate nodes, it is necessary to consider various factors like route, traffic condition, throughput, latency and so on. Also, the location based service selection (LSS) has been identified as keen challenge in accessing any service. According to this, number of methods discussed earlier but no such method exists which achieve expected performance. To improve the performance, an efficient mobility aware directional service fetching algorithm (MADSF) is presented in this paper. The algorithm performs lookup about different services available around the user location. By fetching the trust are relation on verified route selection represents the directional approach based on the services identified, services present in the direction of user and estimates data pickup rate (DPR) for each service based on different properties of the route and service. Finally, user selected the current meet by the point single service selection and produces result to the user. The proposed MADSF-DPR algorithm improves service selection by the point of location aided service selection process and throughput in wireless networks.																	1868-5137	1868-5145															10.1007/s12652-020-02361-9		JUL 2020											
J								High-throughput field-programable gate array implementation of the advanced encryption standard algorithm for automotive security applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Advanced encryption standard (AES); Field-programable gate array (FPGA); Very large scale integration (VLSI); High-level synthesis (HLS); Vivado high-level synthesis; Register transfer language (RTL)	HIGH-LEVEL SYNTHESIS	Connected smart vehicles in automotive industries have increased, resulting in high vehicle-to-vehicle, vehicle-to-infrastructure, and vehicle-to-cloud connectivity. Increased data rates are required to achieve high bandwidth requirements to support such communication networks. Despite having numerous advantages, high connectivity between devices poses threats to vehicle and human security, rendering encryption critical before transmitting data across vehicular networks. Advanced encryption standard (AES) is commonly used for data encryption in automotive microcontrollers. Owing to modern digital design complexities, field-programmable gate arrays (FPGAs) are attracting attention for pre-silicon verification and software development. Owing to their parallel architectures, FPGAs are ideal for prototyping automotive designs running encryption algorithms, like AES at real-time data rates. Moreover, because they are reconfigurable, prototyping results of different implementation choices can be verified at an early stage, thereby helping architects and designers with forthcoming optimal designs. FPGAs also serve as platforms to develop software considerably before silicon arrives, thereby decreasing the time to market. Herein, we propose a high-throughput FPGA implementation of the AES algorithm for automotive microcontrollers using a 128-bit key created via Vivado high-level synthesis (HLS) tool. We use HLS design method based on application-specific bit widths to implement the design on FPGA. The generated design is implemented and verified using Xilinx Kintex 7 and Virtex 6 FPGA; despite identical resource utilization (Look up tables and Flip-Flops), the throughput results are superior to those obtained previously.																	1868-5137	1868-5145															10.1007/s12652-020-02403-2		JUL 2020											
J								Multimodal Machine Learning for Natural Language Processing: Disambiguating Prepositional Phrase Attachments with Images	NEURAL PROCESSING LETTERS										Multimodal machine learning; Deep neural networks; Natural language processing; Prepositional phrase attachment resolution		Although documents are increasingly multimodal, their automatic processing is often monomodal. In particular, natural language processing tasks are typically performed based on the textual modality only. This work extends the syntactic parsing task to the image modality in addition to text. In particular, we address the prepositional phrase attachment problem, a hard and semantic problem for syntactic parsers. Given an image and a caption, the proposed approach resolves syntactic attachment of prepositions in the parse tree according to both visual and lexical features. Visual features are derived from the nature and position of detected objects in the image that are aligned to textual phrases in the caption. A reranker uses this information to reorder syntactic trees produced by a shift-reduce syntactic parser. Trained on the Flickr-PP corpus which contains multimodal gold-standard attachments, this approach yields improvements over a text-only syntactic parser, in particular for the subset of prepositions that encode location, leading to an increase of up to 17 points of attachment accuracy.																	1370-4621	1573-773X															10.1007/s11063-020-10314-8		JUL 2020											
J								Positive influence maximization in signed social networks under independent cascade model	SOFT COMPUTING										Signed social network; Positive influence maximization; Independent cascade model; Negative influence	INFORMATION DIFFUSION	For existing methods for positive influence maximization in signed networks, two factors prevent them from getting high-quality results. First, very few researchers consider the critical effect of negative edges on influence propagation. Second, most of those methods use Monte Carlo simulation to estimate the influence propagating of each candidate seed set. Such time-consuming simulation process hinders the application of those methods in solving real-world problems. Motivated by these limitations, this study investigates the problem of positive influence maximization in competitive signed networks. First, an opposite influence propagating model is defined by a set of propagation rules, where negative links play a more critical role than the positive ones. Second, an influence propagation function is defined to estimate the positive influence propagating of a seed set. Using such influence propagation function, the process of simulation can be avoided, and the computation time can be reduced greatly. An algorithm is presented to select the seed nodes which can obtain the largest positive influence spreading in the signed network. The algorithm employs the greedy strategy to sequentially select the seed nodes according to their spreading increments, which are estimated by the influence propagation function. Experimental results on real-world social networks show that our algorithm consistently outperforms the state-of-the-art in terms of solution quality and is several orders of magnitude faster than other methods.																	1432-7643	1433-7479				OCT	2020	24	19					14287	14303		10.1007/s00500-020-05195-x		JUL 2020											
J								A novel statistical decimal pattern-based surface electromyogram signal classification method using tunableq-factor wavelet transform	SOFT COMPUTING										Statistical decimal pattern; Tunableqwavelet transform; sEMG identification; Signal processing; Pattern recognition	NEIGHBORHOOD COMPONENT ANALYSIS; EMG SIGNALS; FEATURE-EXTRACTION; DIAGNOSIS; PERFORMANCE; SELECTION; DISEASE; RELIEFF	Surface electromyogram sensors have been widely used to acquire hand gestures signals. Many machine learning and artificial intelligence methods have been presented for automated surface electromyogram signals classification. In this method, a novel surface electromyogram signals recognition method is presented using a novel 1D local descriptor. The proposed descriptor is called as statistical decimal pattern and it is utilized as feature extractor in this study and tunableq-factor wavelet transform is used as pooling in this method. By using tunableq-factor wavelet transform and the proposed statistical decimal pattern, a multileveled learning method is constructed. Ten levels are created by using tunableq-factor wavelet transform. Statistical decimal pattern extracts features from tunableq-factor wavelet transform sub-bands of the raw surface electromyogram signal. Then, the generated features are concatenated, and to select distinctive features, ReliefF and neighborhood component analysis are used together. In the classification phase,k-nearest neighbor classifier with city block distance is chosen. To test performance of the proposed tunableq-factor wavelet transform and the proposed statistical decimal pattern-based surface electromyogram classification method, a freely and publicly published dataset was used. In this dataset, 10 hand gestures were defined. Experimental results clearly shown that the proposed tunableqwavelet transform and statistical decimal pattern-based method achieved 98.0%, 99.79% accuracy rates on two datasets and it outcomes other state-of-the-art methods according to these results.																	1432-7643	1433-7479															10.1007/s00500-020-05205-y		JUL 2020											
J								Exploring CO(2)emissions according to planned energy investments and policies: the case of Turkey	SOFT COMPUTING										CO(2)emission; Adaptive grey prediction model; Forecasting; Turkey; Energy sources; GM (1; N)	ARTIFICIAL NEURAL-NETWORK; CO2 EMISSIONS; FUEL COMBUSTION; MODEL; CONSUMPTION; PREDICTION; CHINA	Controlling CO(2)emissions is the common task of all countries to cope with global warming and leave a clean environment. Therefore, in order to develop ecological plans and foresee the necessary measures, forecasting CO(2)emissions is an important issue for the governments. Turkey's CO(2)emissions have increased significantly due to economic and industrial growth since 2000. Due to Turkey's responsibility in the climate agreements, it needs to explore the CO(2)emissions resulting from future investments and policies. In this study, an adaptive grey prediction model was proposed to forecast energy-related CO(2)emission of Turkey for 2023 and 2030 based on planned energy investments and policies. The paper has addressed two scenarios that express the current usage level and the planned level where future investments are put into use in order to observe the change of CO(2)emissions. The obtained results show that the amount of CO(2)emissions in 2030 will decrease significantly thanks to nuclear energy investments according to the current scenario. However, in order to reduce the CO(2)emissions of Turkey to desired levels, there is still a need to tighten energy policies and continue emission-free energy investments.																	1432-7643	1433-7479															10.1007/s00500-020-05208-9		JUL 2020											
J								Inferring structure and parameters of dynamic system models simultaneously using swarm intelligence approaches	MEMETIC COMPUTING										Learning structure and parameters; Swarm Intelligence; Particle swarm optimization; Latin hypercube sampling; Genetic algorithm; Latin hypercube sampling multidimensional uniformity; Dynamic systems	DIFFERENTIAL-EQUATION MODELS; GENERAL-APPROACH; INFERENCE	Inferring dynamic system models from observed time course data is very challenging compared to static system identification tasks. Dynamic system models are complicated to infer due to the underlying large search space and high computational cost for simulation and verification. In this research we aim to infer both the structure and parameters of a dynamic system simultaneously by particle swarm optimization (PSO) improved by efficient stratified sampling approaches. More specifically, we enhance PSO with two modern stratified sampling techniques, i.e., Latin hyper cube sampling (LHS) and Latin hyper cube multi dimensional uniformity (LHSMDU). We propose and evaluate two novel swarm-inspired algorithms, LHS-PSO and LHSMDU-PSO, which can be used particularly to learn the model structure and parameters of complex dynamic systems efficiently. The performance of LHS-PSO and LHSMDU-PSO is further compared with the original PSO and genetic algorithm (GA). We chose real-world cancer biological model called Kinetochores to asses the learning performance of LHSMDU-PSO and LHS-PSO in comparison with GA and the original PSO. The experimental results show that LHSMDU-PSO can find promising models with reasonable parameters and structure, and it outperforms LHS-PSO, PSO, and GA in our experiments.																	1865-9284	1865-9292				SEP	2020	12	3					267	282		10.1007/s12293-020-00306-5		JUL 2020											
J								Multi-level knowledge-based approach for implicit aspect identification	APPLIED INTELLIGENCE										Implicit aspect extraction; Multi-level knowledge-based approach; Co-occurrences; IAC; NGD	SENTIMENT ANALYSIS; ASPECT EXTRACTION	Sentiment analysis or opinion mining is the area of research in Natural Language Processing (NLP) and text mining which deals with the systematic identification of subjective information from user generated text. At a more fine-grained level, aspect-based sentiment analysis focuses on the targets of users' opinions and determining sentiment orientation of these opinions. Among different tasks of aspect-based sentiment analysis, aspect extraction is the key task which includes extraction of both explicit and implicit aspects. Due to the complexity of implicit aspects, not much effort has been put forward to solve the problem while explicit aspects have been studied extensively in the recent past. Existing approaches for implicit aspect extraction have focused on specific type of aspects and have neglected the actual problem. Therefore, in this paper, we have proposed a multi-level approach which identifies implicit aspects using co-occurrence and similarity-based techniques. This research focuses on the extraction of clues for implicit targets of users' opinions and identification of true targets of users' opinions with the help of implicit aspect clues. The proposed approach is divided into two phases: first, several rules are crafted to identify clues for implicit aspects in a review sentence. Secondly, aspects are assigned on the basis of extracted clues using proposed multi-level approach. The proposed model can extract not only implicit aspect clues associated with opinion words but also allocate clues to opinion words where no association is identified. This helps to identify implicit aspects with or without co-occurrences of opinion words with explicit aspects. Experimental evaluation elaborates the importance of implicit aspect clues in the identification of targets of users' opinions. The proposed approach shows better results as compared with the state-of-the-art approaches for the identification of implicit targets of users' opinions on a dataset of different product reviews.																	0924-669X	1573-7497															10.1007/s10489-020-01817-x		JUL 2020											
J								Structuring of tactile sensory information for category formation in robotics palpation	AUTONOMOUS ROBOTS										Robotic palpation; Tactile sensing; Physical sensing; Sensory-motor coordination	IMPLEMENTATION	This paper proposes a framework to investigate the influence of physical interactions to sensory information, during robotic palpation. We embed a capacitive tactile sensor on a robotic arm to probe a soft phantom and detect and classify hard inclusions within it. A combination of PCA and K-Means clustering is used to: first, reduce the dimensionality of the spatiotemporal data obtained through the probing of each area in the phantom; second categorize the re-encoded data into a given number of categories. Results show that appropriate probing interactions can be useful in compensating for the quality of the data, or lack thereof. Finally, we test the proposed framework on a palpation scenario where a Support Vector Machine classifier is trained to discriminate amongst different types of hard inclusions. We show the proposed framework is capable of predicting the best-performing motion strategy, as well as the relative classification performance of the SVM classifier, solely based on unsupervised cluster analysis methods.																	0929-5593	1573-7527				NOV	2020	44	8					1377	1393		10.1007/s10514-020-09931-y		JUL 2020											
J								Conditional Preference Networks with User's Genuine Decisions	COMPUTATIONAL INTELLIGENCE										backtrack search; CP-net; Pareto optimality; preferences	CP-NETS; COMPLEXITY	User's choices involve habitual behavior and genuine decision. Habitual behavior is often expressed using preferences. In a multiattribute case, theConditional Preference Network(CP-net) is a graphical model to represent user's conditionalceteris paribus(all else being equal) preference statements. Indeed, the CP-net induces a strict partial order over the outcomes. By contrast, we argue that genuine decisions are environmentally influenced and introduce the notion of "comfort" to represent this type of choices. In this article, we propose an extension of the CP-net model that we call theCP-net with Comfort(CPC-net) to represent a user's comfort with preferences. Given that preference and comfort might be two conflicting objectives, we define the Pareto optimality of outcomes when achieving outcome optimization with respect to a given CPC-net. Then, we propose a backtrack search algorithm to find the Pareto optimal outcomes. On the other hand, two outcomes can stand in one of six possible relations with respect to a CPC-net. The exact relation can be obtained by performing dominance testing in the corresponding CP-net and comparing the numeric comforts.																	0824-7935	1467-8640				AUG	2020	36	3					1414	1442		10.1111/coin.12386		JUL 2020											
J								Particle swarm optimization based metamaterial inspired circularly polarized patch antenna for S band applications	EVOLUTIONARY INTELLIGENCE										Microstrip patch; Metamaterial; Particle Swarm Optimization; Circular polarization	REDUCTION; COMPACT; PATTERN	In this paper Particle Swarm Optimization technique (PSO) has been used to design a metamaterial inspired circularly polarised patch antenna with truncated corners and embedded square slots. The circular polarization has been achieved by trimming the corners of the square patch along one of its diagonal and square slots have been embedded along these truncated corners to improve the 3 dB axial ratio bandwidth of this proposed antenna. To achieve the maximum axial ratio bandwidth, the dimensions of this square slot have been optimized using PSO algorithm. Without any square slot, the axial ratio bandwidth of the patch antenna is 700 MHz. However, when square slots are embedded along the trimmed corners, with slot length w = 2.3 mm, axial ratio bandwidth of 1150 MHz is achieved. But, when PSO is applied to obtain the optimized dimension of this square slot so as to maximize this axial ratio bandwidth, the optimised w is determined to be 2.8 mm and the axial ratio bandwidth with this optimized slot length is determine to be 1600 MHz. Thus it is observed that by using the optimized value of w, obtained by using PSO technique, improvement in axial ratio bandwidth by 450 MHz is observed. To improve its gain, an array of splitted circular metamaterial rings is etched on the ground plane of this proposed patch antenna. The circularly polarised patch antenna with truncated corners resonates at 2.4 GHz with gain of 4.8 dBic and impedance bandwidth of 600 MHz. However, when square shaped slots of optimized length are embedded in the truncated corners and metamaterial substrate is loaded on the ground plane of this antenna, its gain reaches to 5.2 dBic. All simulations are done in an electronic simulator, HFSS.																	1864-5909	1864-5917															10.1007/s12065-020-00455-z		JUL 2020											
J								Single-shot augmentation detector for object detection	NEURAL COMPUTING & APPLICATIONS										Object detection; Feature fusion; Anchor mechanism; Neural network		Single-shot multibox detector (SSD), one of the top-performing object detection algorithms, has achieved both high accuracy and fast speed. However, its performance is limited by two factors: (1) anchors are generated uniformly over the image by predefined manners, and (2) multiscale features from the feature pyramid are used to detect objects independently. In this paper, we propose a single-shot augmentation detector, called SSADet, that significantly improves the detection accuracy of the original SSD with a slight decrease in speed. SSADet mainly consists of two modules, namely the anchor prediction module and the feature fusion module. These two modules aim to generate anchors with any scale and aspect ratio and fuse multiscale features from different layers, respectively. Specifically, we define an anchor generator whose parameter weights are predicted dynamically by a small neural network and then use the anchor generator to generate optimal anchors over the image in the anchor prediction module. In the feature fusion module, multiscale features from the feature pyramid are concatenated to generate a new feature pyramid through a set of downsampling and upsampling operations. The new feature pyramid takes the generated anchors as the input from the anchor prediction module to predict the final detection results. Extensive experiments are conducted to demonstrate the effectiveness of SSADet on the PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO detection datasets. The experimental results show that SSADet achieves state-of-the-art detection performance with high efficiency.																	0941-0643	1433-3058															10.1007/s00521-020-05202-0		JUL 2020											
J								Time-aware user profiling from personal service ecosystem	NEURAL COMPUTING & APPLICATIONS										User profile; Personal service ecosystem; Evolution analysis; User behavior analysis	RECOMMENDATION	The tremendous growth of services available on the Internet makes user profiling an increasing important topic in web personalization. While profiling a user, one of the main challenges is that user preferences and interests usually change over time. It would be much appreciated, if the user profile could describe these changes. Such user profile is defined as time-aware user profile (TUP) in this paper. The TUP can delineate both dynamic user preferences and interests and evolutions of them, which illustrates that it can represent users more clearly and accurately. A TUP consists of a set of time slot-specific user profiles, which is defined as sub-time-aware user profile (sTUP). And a TUP is special for a user; then, its sTUPs separately represent preferences and interests of this user for different time slots. Constructing a TUP needs the constructions of its sTUPs. While constructing a sTUP from the user behavior logs for a time slot, in this paper, we present a novel learning model called personal service ecosystem (PSE) to delineate user preferences and interests naturally. Different from PSE, sTUP is the representation model. For a time slot, the sTUP is constructed based on the PSE recovered from the user behavior logs of this time slot. In terms of a TUP, two main approaches to analyze the evolutions of sTUPs in this TUP are further discussed. They are the evolution analysis of various margins between every two neighboring sTUPs called EA-VM and the evolution analysis of different sTUPs on raw data defined as EA-RD. EA-VM mainly focuses on the fine-grained evolution between two neighboring time slots by analyzing these various margins. It can be further divided into the clustering method and the distribution analysis approach. EA-RD chiefly pays attentions to the overall evolution of such sTUPs. By a comprehensive survey on the raw data of sTUPs, we summarize six interesting overall evolution patterns, includingrefugee pattern,periodic pattern,stable pattern,fluctuant pattern,emergency patternandzombie pattern. Eventually, a systematic empirical study based on our collection data has been conducted, where smartphone users are taken as examples to illustrate our proposed models and approaches. And experimental results highlight the superiority of these approaches.																	0941-0643	1433-3058															10.1007/s00521-020-05215-9		JUL 2020											
J								EDense: a convolutional neural network with ELM-based dense connections	NEURAL COMPUTING & APPLICATIONS										Deep neural network; Convolutional neural network; Geospatial data learning; Extreme learning machine	EXTREME LEARNING-MACHINE; ENSEMBLE	The explosive growth of geospatial data is increasing requirements for automatic and efficient data learning abilities. Many deep learning methods have been widely applied for geospatial data understanding tasks, such as road networks and geospatial object detection. However, the demands for more accurate learning of high-level features require the use of deeper neural networks. To further improve the learning efficiency of deep neural networks, in this paper, we propose an improved convolutional neural network named EDense. First, we use its dense connectivity to integrate a CNN with an extreme learning machine. Then, we expand the kernels in the convolutional layers to increase the width of the network model. Furthermore, we propose one-feature EDense (OF-EDense), which is a simplified version of EDense, to fit conditions in which the number of parameters is strictly limited. Finally, the experimental results fully demonstrate the strong learning ability and high learning efficiency of EDense.																	0941-0643	1433-3058															10.1007/s00521-020-05181-2		JUL 2020											
J								Different classification methods of fundus image segmentation using quincunx wavelet decomposition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										BPDFHE; GLCM; Segmentation; Classifiers	HISTOGRAM EQUALIZATION	Retinal vessel in fundus image is segmented with a comprehensive method for classification. The method is processed in four phases namely, preprocessing, segmentation, features extraction and classification, this method can be used on different images sets. Retinal vessels are enhanced by brightness preserving dynamic fuzzy histogram equalization (BPDFHE), separating enhanced image is used to detect the retinal diseases. Then these enhanced images are segmented by using quincunx wavelet decomposition for extracting features like first order statistics and gray level co-occurrence matrix (GLCM). The feature vector encodes information to handle the normal and abnormal retinal image and those features are classified using different classifiers (Adaboost, DSVM, ELMASR, EPLS, KNN, NB, NBFFS, OCPLS, RBFN, RF, SOWA, SVM and SVNN) and the performance is evaluated in detail. Blood vessel segmentation with this method is effective for retinal image computational analyses such as early retinal disease detection. Experimental results on three public retinal data sets like DRIVE, STARE and MESSIDOR and real time images are taken from Agarwal's Eye Hospital, Tirunelveli, demonstrating an excellent performance in comparison with retinal vessel segmentation methods reported recently.																	1868-5137	1868-5145															10.1007/s12652-020-02340-0		JUL 2020											
J								FPGA based peripheral myopathy monitoring using MFCV at dynamic contractions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Peripheral myopathy; Muscle fibre conduction velocity (MFCV); Electromyography (EMG); Field programmable gate array	FIBER CONDUCTION-VELOCITY; SURFACE EMG PARAMETERS; MUSCLE	This article describes the design of a wearable, remote inserted framework for the Peripheral Myopathy (PM) estimation in customary dynamic movements. This unique circumstance, the electromyographic investigation can give data about the muscle and nerve condition by evaluating the connected MFCV. The framework works with synchronized and digitized information tests from four electromyographic channels, which are situated on both hands of the individual test, misusing the rules given by an inserted positional filtering calculation. This work displays a novel approach for the assessment of MFCV that relies upon the excellent anode relative estimation standard. The system uses dynamic bitstream change of Electromyographic signs and low computational retort for utilization of bit stream cross-correlation. The structure totally chips away at Cyclone-V altera field programmable gate array. The test outcomes on five subjects (Human Beings) show the limit of the proposed procedure for planning the physiological muscle fibre conduction velocity values, as nitty-gritty in a therapeutic composition. In explicit, differentiating the remedial characteristics, obtained in controlled conditions, with the system evacuated muscle fibre conduction velocity, the proportionate preliminary conditions: preeminent bungle is, overall, 0.3 m/s. The framework restores a likelihood of invalid constant measures underneath of 2% (assuming the worst possible scenario).																	1868-5137	1868-5145															10.1007/s12652-020-02363-7		JUL 2020											
J								Robust Adaptive Fault Reconfiguration for Micro-gas Turbine Based on Optimized T-S Fuzzy Model and Nonsingular TSMO	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Micro-gas turbine; Takagi-Sugeno fuzzy model; Adaptive nonsingular terminal sliding mode; Robust fault reconfiguration; Optimization algorithm	IMPERIAL COMPETITIVE ALGORITHM; DESIGN; SYSTEM; DIAGNOSIS; OBSERVER	This paper presents a novel robust fault reconfiguration scheme based on optimized Takagi-Sugeno (T-S) fuzzy model and nonsingular terminal sliding mode observer (NTSMO) with adaptive law for micro-gas turbine (MGS). An optimized T-S fuzzy model is introduced because it can approximate any nonlinear model with arbitrary precision, and an improved imperial competition algorithm (ICA) using adaptive reform probability is proposed to improve the accuracy of the model. A linear transformation method is introduced to decouple the fault and disturbance of the system. The nonsingular terminal sliding mode observer is designed to reconstruct actuator fault and disturbance with unknown upper bound of a change rate, in which an adaptive law is introduced to update the sliding mode gain in real-time to eliminate the influence of fault, disturbance and modeling uncertainty. Simulations in Matlab/Simulink show high reconfiguration accuracy and high-speed of the proposed method despite of the presence of fault, disturbance and modeling uncertainty.																	1562-2479	2199-3211				OCT	2020	22	7					2204	2222		10.1007/s40815-020-00917-7		JUL 2020											
J								Intelligent forecasting of time series based on evolving distributed Neuro-Fuzzy network	COMPUTATIONAL INTELLIGENCE										distributed Neuro-Fuzzy Takagi-Sugeno; evolving systems; hidden patterns; time series forecasting; unobservable components	MODEL; DECOMPOSITION	An evolving methodology based on Neuro-Fuzzy Takagi-Sugeno network (NF-TS) for distributed forecasting of univariate time series, is proposed. First, the unobservable components, or hidden patterns, are extracted from experimental data of the time series. Then, a distributed forecasting is performed separately for each component, considering an evolving NF-TS associated with each extracted pattern. The evolving NF-TS uses components data to adapt and adjust its structure, as the number of fuzzy rules increases or decreases according the behavior of the unobservable components. A recursive version of singular spectral analysis (SSA) technique is formulated, as one of the main contributions of this article, and it is applied to extract the components. The efficiency of proposed methodology is illustrated from results of comparison to others state-of-the-art techniques for forecasting of various univariate time series.																	0824-7935	1467-8640				AUG	2020	36	3					1394	1413		10.1111/coin.12383		JUL 2020											
J								Schemes of eStories for Children with Social Communication Difficulties	ADVANCES IN HUMAN-COMPUTER INTERACTION											DEVELOPMENTAL-DISABILITIES; ELECTRONIC BOOKS; YOUNG-CHILDREN; LIFE SKILLS; AUTISM; TECHNOLOGY; STORY; INTERVENTION; USABILITY; LITERACY	The current paper presents the development and the evaluation of an Arabic application (app) for electronic stories (eStories) that can be used as an assistive tool in the rehabilitation of children with social communication difficulties. The development process involved engineers and researchers and speech and language pathologists (SLPs) from a rehabilitation hospital, who formulated the design methodology of the Arabic app. This process is critical when designing tools for children with communication difficulties since they have a wide range of differences in their abilities. The evaluation of the app from the standpoint of the SLPs at a local school suggests that the app is easy to use, and the involvement of these SLPs helped in developing a better solution. The results of the usability study on twenty-five students showed positive evaluation with an average score that signifies that the app has above-average usability. This paper highlights the importance of the evaluation process in catering the app for successful user experience.																	1687-5893	1687-5907				JUL 28	2020	2020								9530218	10.1155/2020/9530218													
J								PROMETHEE II method based on variable precision fuzzy rough sets with fuzzy neighborhoods	ARTIFICIAL INTELLIGENCE REVIEW										Fuzzy covering; Fuzzy neighborhood; Covering-based variable precision fuzzy rough set; MADM; PROMETHEE II method	CRITERIA DECISION-MAKING; 3-WAY DECISIONS; TOPSIS METHOD; OPERATORS; EXTENSION; MODEL; NUMBERS	The model of covering-based fuzzy rough sets (CFRSs) can be regarded as a hybrid one by combining covering-based rough sets with fuzzy sets. In this paper, based on fuzzy neighborhoods, we propose two types of covering-based variable precision fuzzy rough sets (CVPFRSs) via fuzzy logical operators, i.e., type-I CVPFRSs and type-II CVPFRSs. Then, several basic properties of the two types of CVPFRSs are discussed. In addition, by virtue of the idea of PROMETHEE II methods, we construct a novel method to multi-attribute decision-making (MADM) in the context of medical diagnosis based on the proposed rough approximation operators. Finally, a test example for illustrating the proposed method is given. Meanwhile, a comparative analysis and an experimental evaluation are further discussed to interpret and evaluate the effectiveness and superiority of the proposed method. The proposed rough set model not only extends the theory of CFRSs, but also provides a new perspective for MADM with fuzzy evaluation information.																	0269-2821	1573-7462															10.1007/s10462-020-09878-7		JUL 2020											
J								Primate tool use and the socio-ecology of thinging: how non-humans think through tools	ADAPTIVE BEHAVIOR										Material engagement theory; cognition; primate evolution; agency; ecological psychology; animal tool use	BEARDED CAPUCHIN MONKEYS; CHIMPANZEES PAN-TROGLODYTES; PESTLE-POUNDING BEHAVIOR; NUT-CRACKING; CEBUS-APELLA; WILD CHIMPANZEES; CAUSAL COGNITION; NATIONAL-PARK; HAMMER TOOLS; AFFORDANCES	While ecological psychology and embodied approaches to cognition have gained traction within the literature on non-human primate tool use, a fear of making assumptions on behalf of animal minds means that their application has been conservative, often retaining the methodological individualism of the cognitivist approach. As a result, primate models for technical and cognitive evolution, rooted in the teleological functionalism of the Neo-Darwinist approach, reduce tool use to the unit of the individual, conflating technology with technique and physical cognition with problem-solving computations of energetic efficiency. This article attempts, through the application of material engagement theory, to explore non-human primate technology as a non-individualistic phenomenon in which technique is co-constructed through the ontogenetic development of skill within a dynamic system of structured action affordances and material interactions which constitute an emergent, species-specific mode of technical cognition.																	1059-7123	1741-2633														1059712320943623	10.1177/1059712320943623		JUL 2020											
J								A benchmark for unconstrained online handwritten Uyghur word recognition	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Online handwriting recognition; Uyghur alphabet; Database; Out-of-vocabulary words; Recurrent neural network; 1D Convolution	CHINESE; NETWORKS; DATABASE	Despite some interesting results from different research groups, a public database for Uyghur online handwriting recognition and a baseline study are not yet available for comparison purpose. In order to fill this void, we present a database of Uyghur online handwritten words and carry out the first benchmark experiments using it. This database contains 125,020 samples of 2030 words collected from 393 writers. According to Uyghur lexicon characteristics, two out-of-vocabulary datasets are especially provided for evaluation. We carry out some unconstrained handwritten word recognition experiments on the database using recurrent neural networks as base model. Recognition results are acquired using connectionist temporal classification without lexicon search and external language model. Concatenated and averaged bidirectional recurrent layers are compared for better generalization. Based on Uyghur unicode representation, we are interested in comparing the models using different alphabets, based both on character types and character forms. To improve generalization, we propose 1D convolutional model which implements 1D convolutional layers for sequence feature extraction. In our experiments, the proposed 1D convolutional model and its variations surpassed the base recurrent layered model on the out-of-vocabulary words by clear margin. 83.23% CAR (character accurate rate) was resulted when out-of-vocabulary samples are used for testing. The highest recognition rate is as high as 94.95% CAR when the test set shares the same lexicon to the training set. The experiments in this paper can be the baseline references for the future study using this database.																	1433-2833	1433-2825				SEP	2020	23	3					205	218		10.1007/s10032-020-00354-0		JUL 2020											
J								Fuzzy Approaches and Simulation-Based Reliability Modeling to Solve a Road-Rail Intermodal Routing Problem with Soft Delivery Time Windows When Demand and Capacity are Uncertain	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Road-rail intermodal routing; Demand uncertainty; Capacity uncertainty; Soft time windows; Simulation-based reliability; Fuzzy programming model; Fuzzy expected value; Fuzzy chance-constrained programming; Fuzzy ranking	LOOP SUPPLY CHAIN; MATHEMATICAL-PROGRAMMING TECHNIQUES; MULTIMODAL TRANSPORTATION PROBLEM; PROCESS SYSTEMS; NETWORK; DESIGN; OPTIMIZATION; FREIGHT; LOCATION	In this study, a freight routing problem considering both soft delivery time windows and demand and capacity uncertainty in a road-rail intermodal transportation system is investigated. According to fuzzy set theory, uncertain demands and capacities are formulated as trapezoidal fuzzy numbers. Soft delivery time windows under a fuzzy environment is established, in which fuzzy periods caused by early and late deliveries that lead to penalty are modeled based on maximum functions. To solve the routing problem yielding the above characteristics, this study designs a fuzzy mixed-integer nonlinear programming model whose objective is to minimize the total costs created in the road-rail intermodal transportation activities. After using the fuzzy expected value method to address the fuzzy objective, two fuzzy approaches, i.e., fuzzy chance-constrained programming method and fuzzy ranking method, are separately adopted to undertake the defuzzification of the fuzzy constraints. Improved linear formulations of the model are then produced to make it easier to solve. A simulation-based reliability modeling is developed to quantify the reliability of the optimization results given by different fuzzy approaches under different parameter settings in a simulation environment. Finally, an empirical case is presented to verify the feasibility of the proposed methods. The effects of demand and capacity fuzziness on the routing optimization are revealed, and an optimization procedure that helps decision-makers to select a more suitable fuzzy approach and determine the best parameter setting for a given case is demonstrated. Some insights that are helpful for organizing a reliable transportation are also drawn.																	1562-2479	2199-3211				OCT	2020	22	7					2119	2148		10.1007/s40815-020-00905-x		JUL 2020											
J								Mechanism of the effect of traditional Chinese medicine fumigation on blood lactic acid in exercise body	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chinese medicine; Fumigant; Exercise; Blood lactic acid	HERBAL MEDICINE; TANG	The production of blood lactic acid in exercise can make the body feel tired and affect the normal movement of the body. Therefore, from the aspects of energy metabolism, mechanical injury, inflammation, free radicals and lipid peroxidation, and growth factors regulating the repair of skeletal muscle injury, the mechanism of Chinese herbal compound on skeletal muscle exercise induced injury is systematically analyzed and discussed. In order to provide theoretical basis for sports training, public health and labor hygiene, one or two of the seven Chinese herbal prescriptions which have good effects on exercise-induced muscle injury are selected. The paper delivers guidelines for avoiding and decreasing exercise caused lactate accumulation and lactic acid. The results show that Chinese herbal fumigation lotion can effectively remove blood lactic acid, accelerate the recovery of muscle strength, and obviously eliminate exercise-induced muscle fatigue.																	1868-5137	1868-5145															10.1007/s12652-020-02356-6		JUL 2020											
