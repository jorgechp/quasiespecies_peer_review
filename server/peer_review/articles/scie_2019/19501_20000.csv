PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Spatial Deployment of Heterogeneous Sensors in Complex Environments	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										3D surfaces; complex environments; heterogeneous sensors; optimal deployment		Studies on the deployment of sensors mostly involve a 2D plane or 3D volume. However, the optimal sensor deployment in field environments is actually the resource distribution on 3D surfaces. Compared with the traditional deployment environments, field environments are more complicated, owing to some interferences on the detection capability of sensors and limitations on the maneuverability of platforms. In this paper, an optimal sensor deployment algorithm in 3D complex environments is discussed. First, considering the characteristics of field environments, the maneuverability matrix of heterogeneous platforms was introduced as a constraint. Then, a non-isomorphic environment value distribution map was constructed to mark the differences among mission areas. Furthermore, the sensor detection range model was improved to better deal with the occlusion issue. Filially, based on the multi-objective particle swarm optimization (MOPSO) algorithm, a sensor deployment strategy was deployed for complex environments. Experiments demonstrated that the proposed algorithm can better deal with the sensor deployment problem in field environments, while improving the detection accuracy of the objects in mission areas.																	1343-0130	1883-8014				JAN	2020	24	1					95	100															
J								Learning Effects of Robots Teaching Based on Cognitive Apprenticeship Theory	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										cognitive apprenticeship theory; educational-support robot; collaborative learning; learning effect		In recent years, educational support robots that assist learners have attracted attention. The main role of teacher-type robots in previous research has been to teach students how to solve problems and to explain learning material. Under such conditions, students may not learn the material adequately due to their reliance on the support of the robot; this paper utilizes the cognitive apprenticeship theory in order to prevent this problem. The cognitive apprenticeship theory asserts that the support provided to a student should change according to the student's learning situation. Previous studies have reported that pedagogy based on the cognitive apprenticeship theory can improve students' learning skills. Therefore, we hypothesize that students' learning will improve when robots teach them how to solve questions based on the cognitive apprenticeship theory. In this paper, we uinvestigate the learning effects of robot teaching based on the cognitive apprenticeship theory in collaborative learning with junior high-school and university students. The results of this experiment suggest that collaborative learning with robots that employ the cognitive apprenticeship theory improves the learning of high-school and university students.																	1343-0130	1883-8014				JAN	2020	24	1					101	112															
J								Interval Data Envelopment Analysis for Inter-Group Data Usage	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										interval efficiency; relative evaluation; efficient frontier; privacy concern	EFFICIENCY; MODELS; DEA	Organizations are interested in exploiting the data from the other organizations for better analyses. Therefore, the data related policies of organizations should be sensitive to the data privacy issue, which has been widely discussed recently. The present study is focused on inter-group data usage for a relative evaluation. This research is based on the data envelopment analysis (DEA), which is used to measure the efficiency of a decision making unit (DMU) relatively employed within a group. In DEA, establishing an efficient frontier consisting of efficient DMUs is essential. We can obtain the efficiency values of a DMU by projecting it to the efficient frontier, and including in the efficiency interval via the interval DEA. When the original data of multiple groups are not open to each other, the alternative is to exchange the information corresponding to the efficient frontiers to estimate the efficiency intervals of a DMU in such a manner that the alternative is in the other groups. Therefore, in this paper, we propose a method to replace the efficient frontier with a weight vector set, from which it is not possible to reconstruct the original data. Considering the weight vector sets of multiple groups, a DMU has three types of efficiency intervals: in its own group, in each of the other groups, and in the integrated group. They provide rich insights on the DMU from a broad perspective, and this encourages inter-group data usage. In this process, we focus on two types of information reduction: one is from the efficient frontier to the weight vector set, and the other is from a union of the groups to the integrated group.																	1343-0130	1883-8014				JAN	2020	24	1					113	122															
J								IDT and Color Transfer-Based Color Calibration for Images Taken by Different Cameras	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										color calibration; camera calibration; color transfer	RECONSTRUCTION	Images of the same object taken by multiple different cameras should have the same color reproduction. However, the images sometimes show different color reproduction due to the individual differences of cameras or internal camera parameters automatically determined when the images are taken. Conventional color transfer methods can be used for unifying the color reproduction of images by transforming the color distribution of an image to that of a reference image. However, conventional methods do not always lead to a good color reproduction and sometimes result in the loss of color impression of original images. In the present paper, we propose a color calibration method for images of the same object taken by different cameras. Two color transfer methods are combined to realize color calibration without the loss of color impression of an original image. Resultant images obtained by the color transfer methods are appropriately mixed into an output image. In experiments, the proposed method is applied to a variety of images and the effectiveness of the proposed method is confirmed by subjective and objective evaluations.																	1343-0130	1883-8014				JAN	2020	24	1					123	133															
J								Conditional Generative Adversarial Networks to Model iPSC-Derived Cancer Stem Cells	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										conditional generative adversarial networks; pix2pix; iPSC-derived cancer stem cells; drug discovery; artificial intelligence		The realization of effective and low-cost drug discovery is imperative to enable people to easily purchase and use medicines when necessary. This paper reports a smart system for detecting iPSC-derived cancer stem cells by using conditional generative adversarial networks. This system with artificial intelligence (AI) accepts a normal image from a microscope and transforms it into a corresponding fluorescent-marked fake image. The AI system learns 10,221 sets of paired pictures as input. Consequently, the system's performance shows that the correlation between true fluorescent-marked images and fake fluorescent-marked images is at most 0.80. This suggests the fundamental validity and feasibility of our proposed system. Moreover, this research opens a new way for AI-based drug discovery in the process of iPSC-derived cancer stem cell detection.																	1343-0130	1883-8014				JAN	2020	24	1					134	141															
J								Exploratory Causal Analysis of Open Data: Explanation Generation and Confounder Identification	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										causal analysis; confounding; crowdsourcing; open data	MODELS	Open data are becoming increasingly available in various domains, and many organizations rely on making decisions according to data. Such decision making requires care to distinguish between correlations and causal relationships. Among data analysis tasks, causal relationship analysis is especially complex because of unobserved confounders. For example, to correctly analyze the causal relationship between two variables, the possible confounding effect of a third variable should be considered. In the open-data environment, however, it is difficult to consider all possible confounders in advance. In this paper, we propose a framework for exploratory causal analysis of open data, in which possible confounding variables are collected and incrementally tested from a large volume of open data. To the extent of the authors' knowledge, no framework has been proposed to incorporate data for possible confounders in causal analysis process. This paper shows an original way to expand causal structures and generate reasonable causal relationships. The proposed framework accounts for the effect of possible confounding in causal analysis by first using a crowdsourcing platform to collect explanations of the correlation between variables. Keywords are then extracted using natural language processing methods. The framework searches the related open data according to the extracted keywords. Finally, the collected explanations are tested using several automated causal analysis methods. We conducted experiments using open data from the World Bank and the Japanese government. The experimental results confirmed that the proposed framework enables causal analysis while considering the effects of possible confounders.																	1343-0130	1883-8014				JAN	2020	24	1					142	155															
J								Stepwise Noise Elimination for Better Motivational and Advisory Texts Classification	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										text classification; motivation; advisory systems; neural networks; dialogue systems		There is little research into designing artificial motivational agents. The end-goal of our studies is therefore to create a dialogue system that would motivate users to do their everyday tasks using natural language. In this paper, we present a method of distinguishing texts containing motivational advice from regular texts to sort out noise in training data for our dialogue system. We implemented a novel method of chaining two shallow networks together by utilizing the output results of the first network to determine the input for the second one. We achieved F-score of 0.94 and 0.97 with our proposed method. The contributions of this paper are threefold: first, we successfully identified 14 hand-crafted features that make a text motivational/advisory. Secondly, we were able to create a classifying algorithm that distinguishes motivational/advisory texts from regular ones. Finally, our proposed method can be applied to other text classification tasks.																	1343-0130	1883-8014				JAN	2020	24	1					156	168															
J								A Robot in a Human-Robot Group Learns Group Norms and Makes Decisions Through Indirect Mutual Interaction with Humans	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										social robotics; human-robot group; group norm; human-robot interaction		In this study, we investigate whether group norms occur in human-robot groups. At present, there are a number of studies that examine social robots' ways of responding, gesturing, and displaying emotion. However, sociality implies that robots not only exhibit human-like behaviors, but also display the tendency to adapt to a group of individuals. For robots to exhibit sociality, they must adapt to group norms without being told by the group members how to behave. Group norms refer to the unwritten, unspoken, and informal rules that are present in a group of individuals. In a previous study, we demonstrated that a robot model learned group norms in human groups [1]. In the present study, we investigate whether group norms occur in human-robot groups. To this end, we prepared quizzes with unclear and vague answers, and instructed participants to take the quizzes with the robot. The results of the quiz experiments demonstrated that the robot considered group norms in human-robot groups when making decisions; thus, group norms occurred in human-robot groups.																	1343-0130	1883-8014				JAN	2020	24	1					169	178															
J								Multiobjective great deluge algorithm with two-stage archive support	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multiobjective optimization; Metaheuristics; Great deluge algorithm; Memory-based search	OPTIMIZATION ALGORITHM; EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; COLONY OPTIMIZATION; MEMORY; SEARCH; IMPLEMENTATION; HYBRID; SYSTEM	A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC'09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC'09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization.																	0952-1976	1873-6769				JAN	2020	87								103239	10.1016/j.engappai.2019.103239													
J								Device free human gesture recognition using Wi-Fi CSI: A survey	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Human gesture recognition; Wi-Fi channel state information; Device free sensing; Model-based approaches; Learning-based approaches	INDOOR LOCALIZATION; INFORMATION; GLOVE	Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices.																	0952-1976	1873-6769				JAN	2020	87								103281	10.1016/j.engappai.2019.103281													
J								Therapy-driven Deep Glucose Forecasting	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Diabetes; Forecasting; Prediction; Deep learning; LSTM	MODEL-PREDICTIVE CONTROL; FREE-LIVING CONDITIONS; CLOSED-LOOP CONTROL; ARTIFICIAL PANCREAS; INSULIN DELIVERY; CONTROL ALGORITHM; HOME-USE; TYPE-1; IDENTIFICATION; NIGHT	The automatic regulation of blood glucose for Type 1 diabetes patients is the main goal of the artificial pancreas, a closed-loop system that exploits continue glucose monitoring data to define an optimal insulin therapy. One of the most successful approaches for developing the artificial pancreas is the model predictive control, which exhibits promising results on both virtual and real patients. The performance of such controller is highly dependent on the reliability of the glucose-insulin model used for prediction purpose, which is usually implemented with classic mathematical models. The main limitation of these models consists in the difficulties of modeling the physiological nonlinear dynamics typical of this system. The availability of big amount of in silico and in vivo data moved the attention to new data-driven methods which are able to easily overcome this problem. In this paper we propose Deep Glucose Forecasting, a deep learning approach for forecasting glucose levels, based on a novel, two-headed Long-Short Term Memory implementation. It takes in input the previous values obtained through continue glucose monitoring, the carbohydrate intake, the suggested insulin therapy and forecasts the interstitial glucose level of the patient. The proposed architecture has been trained on 100 virtual adult patients of the UVA/Padova simulator, and tested on both virtual and real patients. The proposed solution is able to generalize to new unseen data, outperforms classical population models and reaches performance comparable to classical personalized models when fine-tuning is exploited on real patients.																	0952-1976	1873-6769				JAN	2020	87								103255	10.1016/j.engappai.2019.103255													
J								A combined entropy-based approach for a proactive credit scoring	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										FinTech; Trust management; Business intelligence; Credit scoring; Data mining; Entropy	COLD-START PROBLEM; CLASSIFICATION ALGORITHMS; ENSEMBLE MODEL; SELECTION; AREA	Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases.																	0952-1976	1873-6769				JAN	2020	87								103292	10.1016/j.engappai.2019.103292													
J								A powerful variant of symbiotic organisms search algorithm for global optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Symbiotic organisms search; Quasi-oppositional based learning; Chaotic theory; Local search; Benchmark function; Engineering design; Global optimization	AUTOMATIC VOLTAGE REGULATOR; HYBRID GENETIC ALGORITHM; PID CONTROLLER; PERFORMANCE ANALYSIS; EFFICIENT DESIGN; OPPOSITION	This paper suggests a new variation to the existing symbiotic organisms search (SOS) algorithm developed by simulating three symbiotic strategies of mutualism, commensalism and parasitism used by the organisms. In the revised version called improved SOS (ISOS), the theory of quasi-oppositional based learning is employed during generation of initial population and in the parasitism phase to raise the possibility of getting closer to high-quality solutions. An efficient alternative for parasitism phase is also presented. The two upgraded parasitism strategies avoid the over exploration issue of original parasitism phase that causes unwanted longtime search in the inferior search space as the solution is already refined. To guide the algorithm perform an exhaustive search around the best solution in attempting to further improve the search model of ISOS, a chaotic local search based on the piecewise linear chaotic map is coupled into the proposed algorithm. Twentysix benchmark functions and three engineering design problems are tested and a contrast with other popular metaheuristics is widely established. Comparative results substantiate the great contribution of proposed ISOS algorithm in solving various optimization problems with superior global search capability and convergence characteristics which render it useful in handling global optimization problems.																	0952-1976	1873-6769				JAN	2020	87								103294	10.1016/j.engappai.2019.103294													
J								Group decision making under generalized fuzzy soft sets and limited cognition of decision makers	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Group decision making; Generalized fuzzy soft set; Bonferroni mean operator; Similarity; Cognition	SUPPLIER SELECTION; MEAN OPERATORS; ROUGH SET; INFORMATION	Typically, the decision making process assumes that the decision maker's cognition for all aspects of a problem is the same. However, inadequate experience, lack of knowledge, and time suggest otherwise. Therefore, to recognize the impact of the decision maker's cognition on the validity of the information provided, this paper develops a fuzzy group decision making method based on the generalized fuzzy soft set (GFSS). We apply the Bonferroni mean operators to develop the GFSS Bonferroni mean operator, which can be used for aggregating the information gleaned from the decision makers into collective information, and we construct the GFSS to revise the information provided by the decision makers (DMs). A similarity measure between the GFSSs is proposed and is used to identify the DMs' weights. Finally, an illustrative example highlights the proposed method and demonstrates the solution characteristics.																	0952-1976	1873-6769				JAN	2020	87								103344	10.1016/j.engappai.2019.103344													
J								A polynomial-fuzzy-model-based synchronization methodology for the multi-scroll Chen chaotic secure communication system	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Secure communication; Chaos synchronization; Polynomial fuzzy model; Sum-of-squares (SOS); Multi-scroll Chen chaotic systems	GUARANTEED COST CONTROL; STABILITY ANALYSIS; CONTROLLER-DESIGN; SUM	In this paper, a polynomial-fuzzy-model-based design methodology to synchronize multi-scroll Chen chaotic systems is proposed for secure communication. At first, the architecture of secure communication system (SCS) based on the synchronization of multi-scroll Chen chaotic systems is presented. Then, the master and slave multi-scroll Chen chaotic systems are transformed into the equivalent master and slave polynomial fuzzy models respectively. After that, the H-infinity polynomial fuzzy control design is proposed for synchronizing the master and slave multi-scroll Chen chaotic systems as well as restraining external disturbances. Moreover, for practical application, a constraint on the control input is also considered. The H-infinity polynomial fuzzy control design is represented in terms of sum-of-squares (SOS) conditions which can be efficiently solved by the polynomial optimization Matlab toolbox SOSOPT. Furthermore, simulation results show the effectiveness of the proposed polynomial-fuzzy-model-based control design methodology. After the control design, the polynomial-fuzzy-model-based chaotic synchronization methodology is applied to implement the SCS. Finally, three experiments are given to demonstrate the practicality of the implemented SCS.																	0952-1976	1873-6769				JAN	2020	87								103251	10.1016/j.engappai.2019.103251													
J								Some q-rung orthopair fuzzy Hamacher aggregation operators and their application to multiple attribute group decision making with modified EDAS method	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multiple attribute decision making; q-rung orthopair fuzzy sets; Hamacher; Maclaurin symmetric mean	SYMMETRIC MEAN OPERATORS; INTUITIONISTIC FUZZY; SIMILARITY MEASURE; VIKOR; OPERATIONS; INTENTION; NUMBERS; QUALITY; TOPSIS; TODIM	To provide a larger space for decision makers, q-rung orthopair fuzzy sets (q-ROFS) can express their uncertain information. As a generalization of the algebraic operations, and the Einstein t-conorm and t-norm, Hamacher operations have become significant in aggregation theory. In order to accurately integrate the input arguments of decision makers, the relation pattern between the arguments must be considered. In this paper, we analyze both the independent and interdependent relationship that exist between the input arguments based on the arithmetic mean and the Maclaurin symmetric mean (MSM) respectively. To be specific, we develop some new Hamacher operations for q-ROFS. In light of these operational laws, we further propose some q-rung orthopair fuzzy Hamacher aggregation operators, i.e., the q-rung orthopair fuzzy Hamacher average (q-ROFHA) operator, the weighted q-rung orthopair fuzzy Hamacher average (Wq-ROFHA) operator, the q-rung orthopair fuzzy Hamacher Maclaurin symmetric mean (q-ROFHMSM) operator and the weighted q-rung orthopair fuzzy Hamacher Maclaurin symmetric mean (Wq-ROFHMSM) operator. Meanwhile, some special cases and properties are examined. To solve q-rung orthopair fuzzy multiple attribute group decision making (q-ROFMAGDM) problems, we design a novel approach according to the Evaluation Based on Distance from Average Solution (EDAS) method. At the same time, with the aid of the best-worst method (BWM), we propose a new way to determine the attribute weight information. With respect to a mobile payment platform selection problem, we test the robustness and reliability of our proposed methodology.																	0952-1976	1873-6769				JAN	2020	87								103259	10.1016/j.engappai.2019.103259													
J								Extracting and tracking hot topics of micro-blogs based on improved Latent Dirichlet Allocation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Hot topic extraction and tracking; Micro-blog features; Latent dirichlet allocation model; Hot topic life cycle model	SHORT-TEXT; MODEL; TWITTER; INFORMATION	Micro-blog has changed people's life, study, and work styles. Every day, we want to know what public opinion news happens and how it evolves. Extracting and tracking these topics correctly help us better understand the latest public opinions and pay attention to their evolution. To extract topics from Microblog posts accurately, we adopt five unique features of micro-blogs to drive the joint probability distributions of all words and topics, and improve LDA into our topic extraction model(named MF-LDA). To track evolution trend of the topic, we propose a hot topic life cycle model (named HTLCM). We divide the HTLCM into five stages, namely, birth, growth, maturity, decline, and disappearance. The HTLCM determines whether a topic is the candidate hot topic or not and estimates hot topic evolution stages. On the other hand, we propose a hot topic tracking (shorten for HTT) algorithm which integrates MF-LDA and HTLCM. First, the HTT assigns candidate hot topics, which are labeled by HTLCM, to the corresponding time window according to the release time. Second, to obtain the hot topic in each time window, we input Micro-blog posts of each time window into MF-LDA in order. By analyzing changes in these hot topics, we track the changes in their contents. The experiment results show that MF-LDA has a lower perplexity and higher coverage rate than LDA under the same conditions. We conclude parameters of t h e Transition regions of our proposed HTLCM model. The MR, FR of our proposed HTLCM model are lower 'than 18%. The average P, R, F of the HTT algorithm are 85.64%, 84.97%, 85.66%, respectively. A practical application on topicFemale driver beats male driver in chengdu shows an excellent effect and practical significance of HTLCM model and HTT algorithm in extracting and tracking hot topics.																	0952-1976	1873-6769				JAN	2020	87								103279	10.1016/j.engappai.2019.103279													
J								ElHealth: Using Internet of Things and data prediction for elastic management of human resources in smart hospitals	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Internet of Things; Health; Smart hospitals; Data prediction; Human resources; Elasticity	MODEL	Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%.																	0952-1976	1873-6769				JAN	2020	87								103285	10.1016/j.engappai.2019.103285													
J								A novel target threat assessment method based on three-way decisions under intuitionistic fuzzy multi-attribute decision making environment	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Target threat assessment; Three-way decisions; Intuitionistic fuzzy decision; TOPSIS; Risk avoidance coefficient	THEORETIC ROUGH SET; TOPSIS; MODEL	Target threat assessment aims to rank targets threat based on their attributes and state information, which provide decision support for subsequent military decisions, e.g. weapon-target optimal assignment. Most existing threat assessment methods can only obtain ranking results, decision-makers usually need to subjectively choose priority targets to attack or interfere based on the preset threat level and ordering results, which does not meet the requirements of complex battlefield situation and uncertain information processing. A method is urgently needed, which can objectively produce threat classification results and automatically provide priority targets for combat. Therefore, we propose a novel target threat assessment method based on three-way decisions under intuitionistic fuzzy multi-attribute decision making environment. The core parts are the conditional probability of each target is estimated by intuitionistic fuzzy TOPSIS and the decision thresholds of each target are constructed by intuitionistic fuzzy evaluation values. The results of two numerical examples show that the proposed method can effectively deal with dynamic uncertain situation information, turn the traditional ranking results of two-way decisions to the objective classification results of three-way decisions and can flexibly reflect the acquisition of situation information by setting the risk avoidance coefficient.																	0952-1976	1873-6769				JAN	2020	87								103276	10.1016/j.engappai.2019.103276													
J								fierClass: A multi-signal, cepstrum-based, time series classifier	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Cepstrum; Time series; Classification; Frequency-domain analysis	FAULT-DETECTION; IDENTIFICATION; TRANSFORMATION; SYSTEMS	The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving.																	0952-1976	1873-6769				JAN	2020	87								103262	10.1016/j.engappai.2019.103262													
J								Pattern diagnosis for stochastic discrete event systems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Fault diagnosis; Stochastic discrete event system; Pattern; Finite state automaton; Diagnosability	DIAGNOSABILITY	In this paper, pattern diagnosis problem in stochastic discrete event system (SDES) is investigated. In a system, an abnormal state may be caused by the occurrence of several normal events. This kind of fault is called fault pattern. The diagnosis problem of fault pattern is defined as pattern diagnosis. Based on the notions of Adiagnosability and A-A-diagnosability in SDES, the definitions of PA-diagnosability and PAA-diagnosability for pattern diagnosability in SDES are presented in this paper. In addition, a necessary and sufficient condition for an SDES to be PA-diagnosable is proposed. The goal of this paper is diagnosing fault pattern in the real systems. Three-Tank Water Level Control System and Heating, Ventilation, and Air-conditioning System are described to illustrate our algorithm. Experimental results demonstrate that pattern diagnosability in SDES is more accurate than that in DES.																	0952-1976	1873-6769				JAN	2020	87								103305	10.1016/j.engappai.2019.103305													
J								A novel spherical fuzzy QFD method and its application to the linear delta robot technology development	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Spherical fuzzy sets; SF-TOPSIS; QFD; House of quality; Delta robot technology	QUALITY FUNCTION DEPLOYMENT; TECHNICAL ATTRIBUTES; SUPPLIER SELECTION; DECISION-MAKING; FRAMEWORK; EXTENSION; TOPSIS; SETS; AHP	Extensions of ordinary fuzzy sets have been put forward one after the other in the literature. The latest extension is spherical fuzzy sets theory proposed by Kutlu Gundogdu and Kahraman (2019a), which is based on three independent membership parameters defined on a unit sphere with a constraint related to their squared summation. This new extension presenting larger domains for each parameter is employed for production design in this paper. Quality Function Deployment (QFD) is a structured approach for defining customer needs or requirements, which translates them into the final product in order to satisfy these needs. Spherical fuzzy QFD (SF-QFD) under impreciness and vagueness involving linguistic evaluations rather than exact numerical values is proposed in this paper. The importance ratings and global weights of customer requirements (CR) and improvement directions of design requirements (DR) are successfully represented by using spherical fuzzy sets. Judgments of multi-customers/experts are aggregated by spherical fuzzy aggregation operators. A comparative analysis using SF-TOPSIS is applied for competitive firms. A linear delta robot technology design and evaluation is performed by the proposed SF-QFD and a competitive analysis is presented.																	0952-1976	1873-6769				JAN	2020	87								103348	10.1016/j.engappai.2019.103348													
J								Unsupervised feature selection via graph matrix learning and the low-dimensional space learning for classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Feature selection; Graph matrix; Dimensionality reduction; Manifold learning	REGRESSION	Unsupervised feature selection is a powerful tool to select a subset of features for effective representation of high-dimensional data. In this paper, we proposes a novel unsupervised feature selection method via the graph matrix learning and the low-dimensional space learning to obtain their individually optimized result. Furthermore, the global and local correlation of features have been taken into consideration through the low-rank constraint and the feature-level representation property on the graph matrix. Experimental analysis on 15 benchmark datasets verified that our proposed method outperformed the state-of-the-art feature selection methods in terms of classification performance.																	0952-1976	1873-6769				JAN	2020	87								103283	10.1016/j.engappai.2019.103283													
J								A new multi-objective differential evolution approach for simultaneous clustering and feature selection	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Clustering; Feature selection; Automatic clustering; Multi-objective differential evolution	ALGORITHM; OPTIMIZATION	Today's real-world data mostly involves incomplete, inconsistent, and/or irrelevant information that causes many drawbacks to transform it into an understandable format. In order to deal with such issues, data preprocessing is a proven discipline in data mining. One of the typical tasks in data preprocessing, feature selection aims to reduce the dimensionality in the data and thereby contributes to further processing. Feature selection is widely used to enhance the performance of a supervised learning algorithm (e.g., classification) but is rarely used in unsupervised tasks (e.g., clustering). This paper introduces a new multi-objective differential evolution approach in order to find relatively homogeneous clusters without the prior knowledge of cluster number using a smaller number of features from all available features in the data. To analyze the goodness of the introduced approach, several experiments are conducted on a various number of real-world and synthetic benchmarks using a variety of clustering approaches. From the analyzes through several different criteria, it is suggested that our method can significantly improve the clustering performance while reducing the dimensionality at the same time.																	0952-1976	1873-6769				JAN	2020	87								103307	10.1016/j.engappai.2019.103307													
J								Black Widow Optimization Algorithm: A novel meta-heuristic approach for solving engineering optimization problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Black Widow Optimization Algorithm (BWO); Bio-inspired algorithm; Nonlinear optimization; Combinatorial optimization; Engineering applications	PARTICLE SWARM OPTIMIZATION; HASSELTI THORELL ARANEAE; HARMONY SEARCH ALGORITHM; KRILL HERD ALGORITHM; SEXUAL CANNIBALISM; SIBLING CANNIBALISM; MALE SACRIFICE; EVOLUTIONARY; SELECTION; BEHAVIOR	Nature-inspired optimization algorithms can solve different engineering and scientific problems owing to their easiness and flexibility. There is no need for structural modifications of optimization problems to apply metaheuristic algorithms on them. Recently, meta-heuristic algorithms are becoming powerful methods for solving NP problems. In this paper, the authors propose a novel meta-heuristic algorithm suitable for continuous nonlinear optimization problems. The proposed method, Black Widow Optimization Algorithm (BWO), is inspired by the unique mating behavior of black widow spiders. This method includes an exclusive stage, namely, cannibalism. Due to this stage, species with inappropriate fitness are omitted from the circle, thus leading to early convergence. BWO algorithm is evaluated on 51 various benchmark functions to verify its efficiency in obtaining the optimal solutions for the problems. The obtained results indicate that the proposed algorithm has numerous advantages in different aspects such as early convergence and achieving optimized fitness value compared to other algorithms. Also, it has the capability of providing competitive and promising results. The research also solves three different challenging engineering design problems adopting BWO algorithm. The outcomes of the real case study problems prove the effectiveness of the proposed algorithm in solving real-world issues with unknown and challenging spaces.																	0952-1976	1873-6769				JAN	2020	87								103249	10.1016/j.engappai.2019.103249													
J								Finding the k shortest paths by ripple-spreading algorithms	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										k shortest paths problem; Ripple-spreading algorithm; Time window; Time-varying route network	VEHICLE-ROUTING PROBLEM; OPTIMIZATION; NETWORK; HEURISTICS	The k shortest paths problem (k-SPP) is fundamentally important to both theoretical and application researches on computational intelligence. Inspired by the natural ripple-spreading phenomenon that occurs on a water surface, this paper proposes a novel ripple-spreading algorithm (RSA). RSA differs from many existing methods which need to reconstruct route networks or to sweep the network for k times, and it can identify the k shortest paths by a single run of ripple relay race in the original route network. Besides the k-SPP in normal route networks, the RSA can also be extended, without losing optimality and effectiveness, to some time-window networks (where various waiting behaviors at nodes are introduced) and dynamical networks (where the network topology and link costs may change over time due to factors such as moving obstacles and spreading disasters). For one-to-all k-SPP, which aims to find all the k shortest paths from a given source to every other node in a network (no matter with or without time windows at nodes, and no matter whether the network topology and link costs can change over time or not), the RSA can still find out all required solutions using only a single run, while the computational complexity is exactly the same as that for the one-to-one k-SPP, i.e., O(k x N-L x N-ATU), where N-L is the number of links in the network, and N-ATU is the average simulated time units for a ripple to travel through a link. The comparative experimental results illustrate the effectiveness and efficiency of the proposed RSA.																	0952-1976	1873-6769				JAN	2020	87								103229	10.1016/j.engappai.2019.08.023													
J								An efficient hybrid multi-objective memetic algorithm for the frequency assignment problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-objective genetic algorithm; Memetic algorithm; Local search; Clonal selection; Receptor editing; Frequency assignment problem	GENETIC ALGORITHM; TAXONOMY; SOLVE	This paper investigates hybridization of multi-objective memetic algorithm and artificial immune system (AIS) for the Frequency Assignment Problem (FAP) in cellular mobile networks. The considered objectives to minimize are the total interference, the maximal interference, and the number of used frequencies. The proposed approach integrates FAP-specific local search into the evolutionary process to overcome the shortcoming of the multi-objective genetic algorithm, as well as clonal selection and receptor editing, which aims to improve the algorithm exploration and exploitation abilities. Based on the hypervolume metric, the proposed hybrid multi-objective algorithm produces high quality solutions as proved by the tests performed over COST259 instances and corroborated by the comparisons with the most frequently referred algorithms in the related literature. Furthermore, the effect and the behaviour of the main parameters of our algorithm and the interaction between them are analysed using the Design of Experiment (DOE).																	0952-1976	1873-6769				JAN	2020	87								103265	10.1016/j.engappai.2019.103265													
J								Distributed robust data clustering in wireless sensor networks using diffusion moth flame optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Diffusion strategy; Robust clustering; Distributed clustering; Moth flame optimization; Whale optimization; Average euclidean deviation	SOIL-WATER; ALGORITHM; OUTLIERS	Traditional K-Means based distributed clustering used in wireless sensor networks has limitation of getting stuck into local minima, thus many times results in giving inaccurate cluster partitions. To alleviate this drawback, evolutionary based robust distributed clustering techniques are proposed in this paper. These techniques have the capability to determine the global optima thus results in effective cluster partitioning. A moth flame optimization based method is proposed here which minimizes the infra-cluster distance to determine the optimal partition at every sensor node. A diffusion method of cooperation is employed by sharing the best moth position and corresponding fitness value (infra cluster distance) to the neighboring nodes. To introduce robustness, a weight based method for detection and removal of outliers is employed. In this method a weight based on volume and density is given to each data point for outlier detection, a larger weight is considered as an outlier. The simulation study is carried out on one synthetic and two real datasets. The performance of proposed approach is compared with diffusion particle swarm optimization (DPSO), diffusion whale optimization algorithm (DWOA), diffusion elephant herding optimization (DEHO) and distributed KMeans (DK-Means) in terms of Dunn's index, Silhouette index and time complexity. The minimum average Euclidean deviation of proposed diffusion moth flame optimization is 7.16%, 3.25%, 5.24% and 21.70% lower compared to DPSO, DWOA, DEHO and DK-Means respectively for cook agronomy farm dataset.																	0952-1976	1873-6769				JAN	2020	87								103342	10.1016/j.engappai.2019.103342													
J								Super-resolved thermal imagery for high-accuracy facial areas detection and analysis	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Super resolution; Deep learning; Thermal imagery; Object detection	INTERPOLATION; CLASSIFICATION	In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection.																	0952-1976	1873-6769				JAN	2020	87								103263	10.1016/j.engappai.2019.103263													
J								Non-convex hull based anomaly detection in CPPS	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										One-class classification; n-dimensional oriented non-convex hull; Anomaly detection; CPPS	SUPPORT VECTOR; DATA-COMPRESSION; FAULT-DIAGNOSIS; SIGNAL; MODEL	Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber-Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms.																	0952-1976	1873-6769				JAN	2020	87								103301	10.1016/j.engappai.2019.103301													
J								An approach based on linguistic spherical fuzzy sets for public evaluation of shared bicycles in China	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Shared bicycle; Linguistic spherical fuzzy sets; Linguistic spherical fuzzy MABAC method; Linguistic spherical fuzzy TODIM method; Multi-attribute decision making (MADM)	GROUP DECISION-MAKING; PERFORMANCE EVALUATION; SUPPLIER SELECTION; TODIM METHOD; OPERATORS; LOCATION; TOPSIS; MODEL; LOGISTICS; TRANSPORT	In the context of the booming sharing economy, shared bicycles as an important part of the sharing economy have been studied by many scholars, and these researches mainly focus on the socioeconomic characteristics of users and the system design level of shared bicycles, it is very necessary to study the evaluation method of shared bicycles which is a typical multi-attribute decision- making (MADM) problem. Firstly, the linguistic spherical fuzzy numbers (Lt-SFNs) is proposed to express the public's language evaluation information. Compared with the linguistic intuitionistic fuzzy numbers (LIFNs) and the linguistic q-rung orthopair fuzzy numbers (Lq-ROFNs), Lt-SFNs have a wider information expression range. Then, in order to integrate the language evaluation information, the linguistic spherical fuzzy weighted averaging (Lt-SFSWA) operator is proposed, which can aggregate the group linguistic evaluation information. Further, the MABAC (MultiAttributive Border Approximation area Comparison) method is extended to the linguistic spherical fuzzy environment and the Lt-SFS-MABAC method is proposed, which can process linguistic evaluation information and select an optimal alternative from a plurality of alternatives. At the same time, the TODIM (an acronym in Portuguese of Interactive and Multicriteria Decision Making) method is extended to the linguistic spherical fuzzy environment and the Lt-SFS-TODIM method is proposed. Lastly, we conducted sensitivity analysis and comparative analysis of the Lt-SFS-MABAC method, the Lt-SFS-TODIM method and the Lt-SFSWA method. The results show that the Lt-SFS-MABAC method is sensitive to weights, decision makers can use the Lt-SFS-MABAC method to make a realistic evaluation based on the actual environment.																	0952-1976	1873-6769				JAN	2020	87								103295	10.1016/j.engappai.2019.103295													
J								Aircraft detection in remote sensing image based on corner clustering and deep learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										High-resolution remote sensing image; Aircraft detection; Convolutional neural network (CNN); Corner detection; Mean-shift clustering	OBJECT DETECTION	Owing to the variations of aircraft type, pose, size and complex background, it remains difficult to detect aircraft effectively in remote sensing images, which plays a great significance in civilian and military. Classical aircraft detection algorithms still produce thousands of candidate regions and extract the features of candidate regions manually, which affects the detection performance. To address these difficulties encountered, an aircraft detection scheme based on corner clustering and Convolutional Neural Network (CNN) is proposed in this paper. The scheme is divided into two main steps: region proposal and classification. First, candidate regions are generated by utilizing mean-shift clustering algorithm to the corners detected on binary images. Then, the CNN is used for the feature extraction and classification of candidate regions that possibly contain the aircraft, and the location of the aircraft is finally determined after further screening. Compared with other classical methods, such as selective search (SS) + CNN, Edgeboxes + CNN and histogram of oriented gradient (HOG) + support vector machine (SVM), the proposed approach has a high accuracy and efficiency since it can automatically learn the essential features of the object from a large amount of data and produce fewer high quality candidate regions.																	0952-1976	1873-6769				JAN	2020	87								103333	10.1016/j.engappai.2019.103333													
J								Interval-valued fuzzy reasoning algorithms based on Schweizer-Sklar t-norms and its application	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Interval-valued fuzzy sets; Schweizer-Sklar t-norms; Triple I algorithms; Normalized Minkowski distance; Robustness	DELTA-EQUALITIES; ROBUSTNESS; INFERENCE; OPERATORS; SYSTEMS; SETS	Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer-Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis.																	0952-1976	1873-6769				JAN	2020	87								103313	10.1016/j.engappai.2019.103313													
J								A note on "Novel scaled prioritized intuitionistic fuzzy soft interaction averaging aggregation operators and their application to multi criteria decision making"	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										IFS set; SPIFSI aggregation operator; IFSMCDM problems	FUSION PROCESS	Garg and Arora (2018) pointed out that it is not appropriate to use the existing intuitionistic fuzzy soft (IFS) aggregations operators (Arora and Garg, 2018a,b), proposed by themselves, as these aggregation operators gives undesirable results under some circumstances. To resolve the flaws of these aggregation operators, Garg and Arora (2018) proposed the improved operational laws of IFS numbers and hence, proposed a new IFS aggregation operator (named as Scaled prioritized intuitionistic fuzzy soft interaction (SPIFSI) aggregation operator). Garg and Arora (2018) also proposed a SPIFSI aggregation operator based intuitionistic fuzzy soft multi criteria decision making (IFSMCDM) approach for solving IFSMCDM problems (Multi criteria decision making problems in which the rating value of each alternative over each criterion is represented by an IFS set). Although, Garg and Arora (2018) have claimed that their proposed SPIFSI aggregation operator is valid as it satisfies the idempotency, boundedness and monotonicity properties. But, in actual case, for the SPIFSI aggregation operator, proposed by Garg and Kumar (2018), the monotonicity property is not satisfying. Therefore, the SPIFSI aggregation operator, proposed by Garg and Arora (2018), and hence, the SPIFSI aggregation operator based IFSMCDM approach, are not valid. Since, in future, other researchers may use the SPIFSI aggregation operator in their research work. Also, the researchers may use the SPIFSI aggregation operator based IFSMCDM approach for solving real life IFSMCDM problems. Therefore, the aim of this note is to make the researchers aware about the discrepancies of the SPIFSI aggregation operator as well as the SPIFSI aggregation operator based IFMCDM approach. Furthermore, to make the researchers aware that improved operational laws of IFS numbers, proposed by Garg and Arora (2018), are also not valid.																	0952-1976	1873-6769				JAN	2020	87								103287	10.1016/j.engappai.2019.103287													
J								Parameters identification of Bouc-Wen hysteresis model for piezoelectric actuators using hybrid adaptive differential evolution and Jaya algorithm	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Differential evolution; Jaya algorithm; Parameters identification; Nonlinear hysteresis; Piezoelectric actuator	PARTICLE SWARM OPTIMIZATION; NONLINEAR-SYSTEMS	The nonlinearities hysteresis of the piezoelectric-PZT actuator can greatly degrade in precise positioning applications. Therefore, modeling and identifying the hysteresis parameter of PZT actuator are still many challenges nowadays. In this paper, the hybrid adaptive differential evolution and Jaya algorithm (aDE-Jaya) is proposed to identify the Bouc-Wen hysteresis model of a piezoelectric actuator. In the aDE-Jaya algorithm, the improvement is focused on a hybrid mutant operator "DE/rand/1" and Jaya operator tried to balance between two contradictory aspects of their performance: exploration and exploitation and adaptive control parameters (mutant factor F, crossover rate CR, population size NP) to enhance the convergence efficiency. To prove the effectiveness and robustness of the proposed aDE-Jaya algorithm, it is tested on 8 benchmark functions and compared with other state-of-the-art optimizations. The comparison results show that aDE-Jaya has better performance in convergence rate and accuracy. After that, aDE-Jaya is applied to identify the Bouc-Wen hysteresis model based on experimental input-output data. The identified Bouc-Wen hysteresis resulted is used to design the feedforward controller to test accurate identification. As a consequent, the proposed aDE-Jaya algorithm can successfully identify the highly hysteretic nonlinearity of the piezoelectric actuator with perfect precision.																	0952-1976	1873-6769				JAN	2020	87								103317	10.1016/j.engappai.2019.103317													
J								Understanding what the users say in chatbots: A case study for the Vietnamese language	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										User requests; Chatbots; Intent detection; Context extraction; Neural networks		This paper(1) presents a study on understanding what the users say in chatbot systems: the situation where users input utterances bots would hopefully (1) detect intents and (2) recognize corresponding contexts implied by utterances. This helps bots better understand what users are saying, and act upon a much wider range of actions. To this end, we propose a framework which models the first task as a classification problem and the second one as a two-layer sequence labeling problem. The framework explores deep neural networks to automatically learn useful features at both character and word levels. We apply this framework to building a chatbot in a Vietnamese e-commerce domain to help retail brands better communicate with their customers. Experimental results on four newly-built datasets demonstrate that deep neural networks could be able to outperform strong conventional machine-learning methods. In detecting intents, we achieve the best F-measure of 82.32%. In extracting contexts, the proposed method yields promising F-measures ranging from 78% to 91% depending on specific types of contexts.																	0952-1976	1873-6769				JAN	2020	87								103322	10.1016/j.engappai.2019.103322													
J								A study on leading machine learning techniques for high order fuzzy time series forecasting	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Fuzzy time series forecasting (FTSF); Restricted Boltzmann machine (RBM); Deep belief network (DBN); Support vector machine (SVM); Long short-term memory (LSTM); Fuzzy logical relationship (FLR)	LOGICAL RELATIONSHIPS; TEMPERATURE PREDICTION; NEURAL-NETWORKS; MODEL; ENROLLMENTS; INTERVALS; LENGTHS	Fuzzy time series forecasting (FTSF) methods avoid the basic assumptions of traditional time series forecasting (TSF) methods. The FTSF methods consist of four stages namely determination of effective length of interval, fuzzification of crisp time series data, modeling of fuzzy logical relationships (FLRs) and defuzzification. All the four stages play a vital role in achieving better forecasting accuracy. This paper addresses two key issues such as modeling FLRs and determination of effective length of interval. Three leading machine learning (ML) techniques, namely deep belief network (DBN), long short-term memory (LSTM) and support vector machine (SVM) are first time used for modeling the FLRs. Additionally, a modified average-based method is proposed to estimate the effective length of interval. The proposed FTSF-DBN, FTSF-LSTM and FTSF-SVM methods are being compared with three papers from the literature along with four crisp TSF methods using multilayer perceptron (MLP), LSTM, DBN and SVM. A total of fourteen time series datasets (Sun Spot, Lynx, Mumps and 11 TAIEX time series datasets i.e. 2000-2010) are considered for comparative performance analysis. Results revealed the statistical superiority of FTSF-SVM method and proposed improved average-based method based on the popular Friedman and Nemenyi hypothesis test. It is also observed that the proposed FTSF methods provide statistical superior performance than their crisp TSF counterparts.																	0952-1976	1873-6769				JAN	2020	87								103245	10.1016/j.engappai.2019.103245													
J								Interval type-2 fuzzy logic based transmission power allocation strategy for lifetime maximization of WSNs	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Transmission power allocation; Type-2 fuzzy; Wireless sensor network; Network lifetime	MANAGEMENT; NETWORK	In wireless sensor networks (WSNs), it is critical to design an advisable transmission power allocation strategy for balancing the latency and energy efficiency, and prolonging the lifetime of WSNs. However, some measured key parameters, e.g., data latency, energy consumption and communication radius, are with high levels of uncertainties, which deteriorate the transmission power allocation performance greatly. How to employ an advanced method to deal with the uncertainties and to further improve the network performance is a pressing issue. Type-2 fuzzy logic system (T2FLS) as a powerful tool for handling the uncertainties provides an effective way for designing such advisable allocation strategies. Therefore, this paper adopts the interval T2FLS (IT2FLS) to design the transmission power allocation (TPA) strategy for lifetime maximization of WSNs. Firstly, the problem of lifetime enhancement in WSNs is formulated in detail, and then it is converted into a TPA problem. Secondly, the IT2FLS method is applied to the transmission power decision making process for maximizing the lifetime of WSNs. In the designed IT2FLS-based TPA strategy, expected latency, residual energy and distance between nodes are taken as input variables, while the transmission power and communication radius are considered as the output variables. Finally, both simulation and experiment results are given. The results indicate that the proposed TPA strategy using IT2FLS can effectively realize the tradeoff between the latency and energy efficiency, and can prolong the network lifetime of the WSNs. Moreover, compared with other TPA strategies, including the minimum total energy algorithm, the flow augmentation algorithm and the type-1 fuzzy logic method, the proposed IT2FLS-based TPA strategy has obvious advantages in terms of network lifetime, average latency and energy consumption.																	0952-1976	1873-6769				JAN	2020	87								103269	10.1016/j.engappai.2019.103269													
J								A novel Hybrid Multi-objective Evolutionary Algorithm for the bi-Objective Minimum Diameter-Cost Spanning Tree (bi-MDCST) problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-objective; Spanning tree; Bounded diameter; Hybrid evolutionary; NP-hard; Meta-heuristic	SWARM OPTIMIZATION APPROACH; DESIGN; HEURISTICS	Given a connected, weighted, undirected graph G = (V, E), the bi-objective Minimum Diameter-Cost Spanning Tree (bi-MDCST) Problem aims to find spanning trees on G with minimal total edge weight as well as minimal diameter. The problem is known to be NP-hard and finds application in domains ranging from transportation to network design. An exact algorithm for the problem is given in the literature, but is computationally very expensive and hence applicable to only very small graphs; some fast heuristics, a multi-objective genetic algorithm (MOGA) and a fast non-dominated sorting genetic algorithm (NSGA2) have also been proposed in the literature for solving the problem and their performance studied on larger sized full graphs containing up to 250 vertices and 31,125 edges. This paper presents a novel Hybrid Multi-objective Evolutionary Algorithm (HMOEA) for the bi-MDCST problem which uses a representation that captures the genetic information of several spanning trees of varying diameters in a single chromosome, thereby enhancing the representational power of the population. The algorithm uses heuristic-based population seeding and combines linear time recombination with a fast mutation operator to more effectively balance the exploration and exploitation of the search space. The proposed HMOEA computes the optimal Pareto-front solutions in time that is several orders of magnitude lesser than that taken by the exact method, and is shown to obtain superior performance in terms of the convergence and distribution of solutions vis-a-vis the other two extant meta-heuristics on a wide range of benchmark graph instances. Results obtained by the HMOEA are also reported for larger sized Euclidean instances than attempted hitherto in the literature, containing up to 500 vertices and 124,750 edges, and for a benchmark suite of large fully connected random edge-weight graph instances introduced in this work for more effectively comparing the performance of algorithms for the bi-MDCST problem.																	0952-1976	1873-6769				JAN	2020	87								103237	10.1016/j.engappai.2019.103237													
J								Whale optimization algorithm-based Sugeno fuzzy logic controller for fault ride-through improvement of grid-connected variable speed wind generators	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Optimization method; Power system control; Power system operation; Wind energy	SOLAR PHOTOVOLTAIC MODELS; ENERGY-CONVERSION SYSTEM; TECHNICAL REQUIREMENTS; PARAMETER EXTRACTION; STORAGE UNIT; POWER; PMSG; DRIVEN; ENHANCEMENT; DESIGN	Due to the extensive penetration of wind power plants (WPPs) into the grid, grid codes have been imposed such that the WPPs stay linked to the grid during faults for a period to maintain the grid stability. This paper designs optimal Sugeno fuzzy logic controllers (FLCs) to improve the fault ride-through (FRT) ability of grid-connected WPPs. The meta-heuristic algorithm, whale optimization algorithm (WOA), is utilized to design the control rules and the Gaussian memberships of eight Sugeno FLCs, simultaneously, by minimizing the high dimensional multi-objective fitness function. The WOA-FLCs and the grid-connected gearless permanent magnet synchronous generator driven by a variable-speed wind turbine (VSWT-PMSG) are modeled using PSCAD/EMTDC environment. The effectiveness of the FRT ability of grid-connected VSWTPMSG is investigated during balanced and unbalanced grid fault conditions. The simulation results of using WOA-FLCs revealed fast time response, less overshoot, and small steady-state error compared with those achieved by using a genetic algorithm (GA) and grey wolf optimizer (GWO).																	0952-1976	1873-6769				JAN	2020	87								103328	10.1016/j.engappai.2019.103328													
J								Short-term natural gas consumption prediction based on Volterra adaptive filter and improved whale optimization algorithm	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Short-term natural gas consumption; Chaotic character recognition; Phase space reconstruction; Volterra adaptive filter; Improved whale optimization algorithm; Forecast	SUPPORT VECTOR REGRESSION; NEURAL-NETWORK; MULTIOBJECTIVE OPTIMIZATION; LYAPUNOV EXPONENTS; GENETIC ALGORITHM; SEARCH ALGORITHM; PRACTICAL METHOD; MODEL; COMBINATION; SIMULATION	Short-term natural gas consumption prediction is an important indicator of natural gas pipeline network planning and design, which is of great significance. The purpose of this study is to propose a novel hybrid forecast model in view of the Volterra adaptive filter and an improved whale optimization algorithm to predict the short-term natural gas consumption. Firstly, Gauss smoothing and C-C method is adopted to pretreat and reconstruct short-term natural gas consumption time series; secondly, to improve the performance of whale optimization algorithm, adaptive search-surround mechanism and spiral position and jumping behavior are introduced into it; Thirdly, Volterra adaptive filter is used to predict the short-term natural gas consumption, and the important parameters (e.g. embedding dimension) is optimized by improved whale optimization algorithm. Finally, an actual example is given to test the performance of the developed prediction model. The results indicate that (1) short-term natural gas consumption time series has chaotic characteristics; (2) performance of the improved whale optimization algorithm is better than some comparative algorithms (i.e. cuckoo optimization algorithm, etc. ) based on the different evaluation indicators; (3) exploration factor is the main operational factor; (4) the performance of the proposed prediction model is better than some advanced prediction models (e.g. back propagation neural network). It can be concluded that such an innovative hybrid prediction model may provide a reference for natural gas companies to achieve intelligent scheduling.																	0952-1976	1873-6769				JAN	2020	87								103323	10.1016/j.engappai.2019.103323													
J								General structure of Interval Type-2 fuzzy PI/PD controller of Takagi-Sugeno type	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Interval type-2 fuzzy control; Takagi-Sugeno (TS) controller; Variable gain controller; PI/PD controller; Mathematical model	STABILITY ANALYSIS; LOGIC SYSTEMS; PERFORMANCE; PI	In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional-Integral (PI) or fuzzy Proportional-Derivative (PD) controller of Takagi-Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik-Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers.																	0952-1976	1873-6769				JAN	2020	87								103273	10.1016/j.engappai.2019.103273													
J								A grey-layered ANP based decision support model for analyzing strategies of resilience in electronic supply chains	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Supply chain resilience; Supply chain risk; Resilient strategies; Grey theory; Analytic network process	RISK-MANAGEMENT; MITIGATION; INVENTORY; CONCEPTUALIZATION; SUSTAINABILITY; INTELLIGENCE; POSTPONEMENT; FLEXIBILITY; ALLOCATION; CONTRACTS	Augmented globalization and vertical integration have made contemporary supply chains an intricate network subject to a number of vulnerabilities. Preemptive measures are needed for dealing with mutable risks and vulnerabilities to safeguard robust supply chain systems. Supply chain risk management (SCRM) connotes a set of risk management responses essentially instigated to confront supply chain risks. As supply chain risks are intertwined, one resilient strategy for risk mitigation can moderate several supply chain risks. A complex decision making problem involving twelve major supply chain risks and twenty one resilient strategies for risk mitigation have been acknowledged in this research with archetypal focus on electronics manufacturing supply chains. A combination of Multi criteria decision aid (MCDA) and artificial intelligence (AI) is increasingly used in decision making of complex real world problems. A decision support model incorporating an amalgamation of grey theory and layered analytic network process (ANP) has been employed for quantifying various resilient strategies for risk mitigation. The proposed model was also applied in a practical setting taking a case study of an electronics manufacturing company. Sensitivity analysis was also conducted to ensure the robustness of obtained results. The combined methodology proposed in this research could be effectively used by top management, to pigeonhole the resilient supply chain strategies for better managing their supply chains.																	0952-1976	1873-6769				JAN	2020	87								103338	10.1016/j.engappai.2019.103338													
J								Power prediction for electric vehicles using online machine learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Electric vehicle; Power prediction; Kernel adaptive filters; Fixed-budget	RECURSIVE LEAST-SQUARES; RANGE; MASS	Accurate range prediction of an electric vehicle is an important open problem, affecting among others the adoption and market penetration of electric vehicles. Current range prediction systems suffer several practical limitations. Most critically these systems employ models in which all vehicle-specific parameters are required to be known. In this paper, we propose a methodology for predicting power and mission energy of electric vehicles that does not require knowledge of vehicle-specific parameters nor a drive-train model. The proposed method uses a data-driven approach grounded entirely on available vehicle sensor data. In particular, the predictive model is obtained by applying machine learning techniques, and in order to adapt to changing conditions in real-time, the specific class of kernel adaptive filtering algorithms is employed. Kernel adaptive filtering extends the theory of linear filters with concepts from kernel methods in order to construct nonlinear adaptive filtering algorithms that exhibit properties such as universal approximation capabilities and convexity in training, requiring only modest computational complexity. After providing an overview of the most relevant properties of kernel adaptive filters, we evaluate the proposed prediction methodology on data obtained in nine vehicle trial runs, comparing the performance of one linear adaptive filter, one online trained neural network and two state-of-the-art kernel adaptive filters.																	0952-1976	1873-6769				JAN	2020	87								103278	10.1016/j.engappai.2019.103278													
J								Improvement of Bagging performance for classification of imbalanced datasets using evolutionary multi-objective optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-objective evolutionary; Imbalanced datasets; Ensemble learning; Bagging; Undersampling; Diversity	SUPPORT VECTOR MACHINES; REJECTIVE MULTIPLE TEST; ENSEMBLE METHOD; SMOTE; ALGORITHMS; DIVERSITY; SYSTEM; MARGIN	Today, classification of imbalanced datasets, in which the samples belonging to one class is more than the samples pertaining to other classes, has been paid much attention owing to its vast application in real-world problems. Bagging ensemble method, as one of the most favorite ensemble learning algorithms can provide better performance in solving imbalanced problems when is incorporated with undersampling methods. In Bagging method, diversity of classifiers, performance of classifiers, appropriate number of bags (classifiers) and balanced training sets to train the classifiers are important factors in successfulness of Bagging so as to deal with imbalanced problems. In this paper, through inspiring of evolutionary undersampling (the new undersampling method for seeking the subsets of majority class samples) and taking the mentioned factors into account, i.e., diversity, performance of classifiers, number of classifiers and balanced training set, a multi-objective optimization undersampling is proposed. The proposed method uses multi-objective evolutionary to produce set of diverse, well-performing and (near) balanced bags. Accordingly, the proposed method provides the possibility of generating diverse and well-performing classifiers and determining the number of classifiers in Bagging algorithm. Moreover, two different strategies are employed in the proposed method so as to improve the diversity. In order to confirm the proposed method's efficiency, its performance is measured over 33 imbalanced datasets using AUC and then compared with 6 well-known ensemble learning algorithms. Investigating the obtained results of such comparisons using non-parametric statistical analysis demonstrate the dominancy of the proposed method compared to other employed techniques, as well.																	0952-1976	1873-6769				JAN	2020	87								103319	10.1016/j.engappai.2019.103319													
J								Optimizing speed/accuracy trade-off for person re-identification via knowledge distillation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Person re-identification; Network distillation; Image retrieval; Model compression; Surveillance		Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance.																	0952-1976	1873-6769				JAN	2020	87								103309	10.1016/j.engappai.2019.103309													
J								Semantic versus instance segmentation in microscopic algae detection	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Microscopic algae identification; Deep learning diatom segmentation; Semantic segmentation; SegNet; Instance segmentation; Mask-RCNN		Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading.																	0952-1976	1873-6769				JAN	2020	87								103271	10.1016/j.engappai.2019.103271													
J								A predictive model for the maintenance of industrial machinery in the context of industry 4.0	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Industry 4.0; Predictive maintenance; Machine Learning; Data analysis; Smart manufacturing; Intelligent prognostics tools	KALMAN FILTER; PROGNOSTICS; INTERNET; FUTURE	The Industry 4.0 paradigm is being increasingly adopted in the production, distribution and commercialization chains worldwide. The integration of the cutting-edge techniques behind it entails a deep and complex revolution - changing from scheduled-based processes to smart, reactive ones - that has to be thoroughly applied at different levels. Aiming to shed some light on the path towards such evolution, this work presents an Industry 4.0 based approach for facing a key aspect within factories: the health assessment of critical assets. This work is framed in the context of the innovative project SiMoDiM, which pursues the design and integration of a predictive maintenance system for the stainless steel industry. As a case of study, it focuses on the machinery involved in the production of high-quality steel sheets, i.e. the Hot Rolling Process, and concretely on predicting the degradation of the drums within the heating coilers of Steckel mills (parts with an expensive replacement that work under severe mechanical and thermal stresses). This paper describes a predictive model based on a Bayesian Filter, a tool from the Machine Learning field, to estimate and predict the gradual degradation of such machinery, permitting the operators to make informed decisions regarding maintenance operations. For achieving that, the proposed model iteratively fuses expert knowledge with real time information coming from the hot rolling processes carried out in the factory. The predictive model has been fitted and evaluated with real data from similar to 118k processes, proving its virtues for promoting the Industry 4.0 era.																	0952-1976	1873-6769				JAN	2020	87								103289	10.1016/j.engappai.2019.103289													
J								SECUR-AMA: Active Malware Analysis Based on Monte Carlo Tree Search for Android Systems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Active malware analysis; Reinforcement learning; Monte Carlo tree search	FRAMEWORK; CLASSIFICATION	We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset.																	0952-1976	1873-6769				JAN	2020	87								103303	10.1016/j.engappai.2019.103303													
J								Energy efficient multi-objective scheduling of tasks with interval type-2 fuzzy timing constraints in an Industry 4.0 ecosystem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Real-time embedded computing systems; Interval type-2 fuzzy sets; Fuzzy deadlines; Fuzzy earliness; Energy efficient scheduling; NSGA-II	LOGIC SYSTEMS; EVOLUTIONARY ALGORITHM; COMPUTATIONAL COST; GENETIC ALGORITHM; PROCESSING TIMES; NSGA-II; OPTIMIZATION; UNCERTAINTY; SETS; REPRESENTATION	Industrial systems usually draw huge energy to run various machines. The amount of energy requirement has again increased due to the automation of the industrial plants to make them Industry 4.0 compliant. As a result, demand of energy is on the rise in almost all manufacturing and industrial plants. The necessity of critical and smart manufacturing processes in Industry 4.0 and its increased energy requirements force us to look for energy efficient techniques for running the deployed computing systems, which are often embedded and integrated within larger machines and have to function under time constraints. Computational efficiency of these real-time embedded systems (RTESs) depends solely on the timely completion of tasks. Task execution with less energy consumption within critical timing constraints is a challenging issue for the designers of RTESs. Thus, task scheduling in these systems require sophisticated energy efficient mechanisms. However, energy efficiency and timeliness are two mutually contradictory objectives, since the former is achieved only with a significant compromise of the later. In this paper, we propose a novel approach, based on the popular multi-objective evolutionary algorithm, Non-dominated sorting genetic algorithm-II, to solve this problem. Moreover, in RTESs, precise prediction of timing constraints is difficult before runtime which causes a form of imprecision or uncertainty in the system. Therefore, we use type-2 fuzzy sets (T2 FSs) to model the timing constraints in RTESs and introduce novel algorithms for membership function generation and calculation of fuzzy earliness. Numerical as well as real-life examples are included to demonstrate our proposed technique.																	0952-1976	1873-6769				JAN	2020	87								103257	10.1016/j.engappai.2019.103257													
J								Barnacles Mating Optimizer: A new bio-inspired algorithm for solving engineering optimization problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Barnacles Optimization Algorithm; Benchmarked functions; Loss minimization; Meta-heuristic technique; Optimal reactive power dispatch	LEARNING-BASED OPTIMIZATION; LEAGUE COMPETITION ALGORITHM; META-HEURISTIC ALGORITHM; GLOBAL OPTIMIZATION; DESIGN; EVOLUTION; SYSTEMS	This paper presents a novel bio-inspired optimization algorithm namely the Barnacles Mating Optimizer (BMO) algorithm to solve optimization problems. The proposed algorithm mimics the mating behaviour of barnacles in nature for solving optimization problems. The BMO is first benchmarked on a set of 23 mathematical functions to test the characteristics of BMO in finding the optimal solutions. It is then applied to optimal reactive power dispatch (ORPD) problem to verify the reliability and efficiency of BMO. Extensive comparative studies with other algorithms are conducted and from the simulation results, it is observed that BMO generally provides better results and exhibits huge potential of BMO in solving real optimization problems.																	0952-1976	1873-6769				JAN	2020	87								103330	10.1016/j.engappai.2019.103330													
J								Complex object detection using deep proposal mechanism	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Object detection; Region proposal; Deep convolutional neural network; Joint learning		Complex object detection is one of the most important and challenging problems in computer vision tasks. To provide a high-performance complex object detection method, a deep proposal mechanism (DPM) is proposed. (1) A region proposal strategy is used to localize potential objects in most top methods; however, this process also represents an intractable computational bottleneck. Therefore, to localize potential complex effectively, a region proposal mechanism (RPM) is proposed. The mechanism shares common features with ResNet and achieves excellent performance. (2) To solve the problem of insufficient amount of labeled training data, an efficient semisupervised pretraining method, instead of traditional unsupervised pretraining, is carried out. (3) To further improve the computational speed, a novel joint learning strategy is introduced. Extensive experiments are performed, and the results show that DPM achieves much better performance than state-of-the-art methods.																	0952-1976	1873-6769				JAN	2020	87								103234	10.1016/j.engappai.2019.09.003													
J								Finite-interval-valued Type-2 Gaussian fuzzy numbers applied to fuzzy TODIM in a healthcare problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Type-2 fuzzy sets; Interval-valued Gaussian Type-2 fuzzy numbers; Fuzzy MCDM; Fuzzy economic evaluation; Healthcare; Imaging devices	ANALYTIC HIERARCHY PROCESS; DECISION-MAKING; LOGIC SYSTEMS; MEDICAL DEVICES; SETS; RANKING	Multi-criteria decision making (MCDM), especially fuzzy MCDM process, is the most suitable approach for evaluating strategic decisions. However, in most cases, the Type-1 fuzzy form is insufficient for addressing uncertainty. The Type-2 fuzzy set is a more powerful tool for characterizing uncertainty in complex problems for several domains. The symmetric shape of Gaussian functions better fits most real cases. In many areas, this Gaussian feature serves to describe complex situations in terms of knowledge representation and to resolve critical decision problems. Therefore, a wide usage of Gaussian Type-2 fuzzy sets is expected; however, because of their complexity, very few studies can be found in the literature. The details of Type-2 Gaussian fuzzy sets have not been thoroughly studied. The foundation of interval-valued Type-2 (IT2) Gaussian fuzzy sets with finite ranges, which are called Finite Interval Type-2 (FIT2) Gaussian fuzzy numbers, is proposed in this study. Then, arithmetic operations on FIT2 Gaussian fuzzy numbers and a ranking procedure are derived and adapted to the strategic selection process. The extended TODIM method with FIT2 Gaussian fuzzy numbers is integrated into a real economic evaluation of a medical device selection problem. This problem is detailed with technical and economical criteria. In this study, a healthcare device selection problem is analyzed from the perspectives of clinicians, biomedical engineers, and healthcare investors. The case study is characterized through an evaluation process in which different experts' perspectives are considered. Finally, a comparison of the results derived from different multi-criteria solution processes is presented.																	0952-1976	1873-6769				JAN	2020	87								103352	10.1016/j.engappai.2019.103352													
J								Combined weightless neural network FPGA architecture for deforestation surveillance and visual navigation of UAVs	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Classification; Weightless neural network; Artificial neural networks; UAVs	IMPLEMENTATION; ODOMETRY	This work presents a combined weightless neural network architecture for deforestation surveillance and visual navigation of Unmanned Aerial Vehicles (UAVs). Binary images, which are required for position estimation and UAV navigation, are provided by the deforestation surveillance circuit. Learned models are evaluated in a real UAV flight over a green countryside area, while deforestation surveillance is assessed with an Amazon forest benchmarking image data. Small utilization percentage of Field Programmable Gate Arrays (FPGAs) allows for a higher degree of parallelization and block processing of larger regions of input images.																	0952-1976	1873-6769				JAN	2020	87								103227	10.1016/j.engappai.2019.08.021													
J								Multi-agent architecture for information retrieval and intelligent monitoring by UAVs in known environments affected by catastrophes	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-agent architecture; Smart UAVs; Intelligent monitoring; Expert Systems; Crisis Management	MULTIROBOT COORDINATION; SYSTEM	The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored.																	0952-1976	1873-6769				JAN	2020	87								103243	10.1016/j.engappai.2019.103243													
J								An agent-based model for plausible wayfinding in pedestrian simulation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Pedestrian simulation; Multi-agent systems	TACTICAL LEVEL DECISIONS; ROUTE CHOICE; EVACUATION; BEHAVIOR	Modeling pedestrian decision making activities represents a serious challenge: different decisions are taken at distinct levels of abstraction, employing heterogeneous information and knowledge about the environment, from path planning to the regulation of distance from other pedestrians and obstacles present in the environment. Pedestrians, moreover, are not robots: although empirical observations show that they consider congestion when planning, there are also evidences that their decisions are not always optimal, even in normal situations. We present a model integrating and improving consolidated results mitigating the optimization effects of congestion aware path planning. In particular, we employ commonsense estimations of the effects of perceivable congestion instead of exact values, also embedding an imitation mechanism stimulating changes in planned decisions whenever another nearby pedestrian did the same. The model leads to improvements in quantitatively reproducing observed phenomena, both in a validation scenario as well as in a real-world situation: an interesting counterintuitive result, in which reducing available choices and exits actually reduces overall egress time, is also presented and discussed.																	0952-1976	1873-6769				JAN	2020	87								103241	10.1016/j.engappai.2019.103241													
J								A complex process fault diagnosis method based on manifold distribution adaptation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Grassmann manifold; Dynamic distribution adaptation; Instance reweighting; Domain adaptation		The main challenge for complex industrial processes is that the distribution of new process data is different from that of old process data, resulting in poor performance when monitoring the new process (target domain) using the model established by the old process (source domain) data. Moreover, new process data is often unlabeled, which is more common in actual industrial processes. Therefore, this paper proposes a novel fault diagnosis method for complex industrial processes based on Manifold Distribution Adaptation (MDA) to cope with this challenge. Specifically, in order to avoid feature degradation caused by feature transformation directly in the original feature space, MDA first maps source domain data and target domain data to Grassmann manifold through geodesic flow kernel. Second, dynamic distribution adaptation and source domain instance re-weighting are performed simultaneously on the Grassmann manifold to reduce the shift between the domains. Then, the base classifier trained by the adaptive source domain data can achieve accurate classification of the target domain data. Finally, the superiority of MDA is verified on the public transfer learning datasets and the actual industrial process datasets.																	0952-1976	1873-6769				JAN	2020	87								103267	10.1016/j.engappai.2019.103267													
J								Integrating the 2-tuple linguistic representation and soft set to solve supplier selection problems with incomplete information	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Supplier selection; Soft set; 2-tuple linguistic representation model; VIKOR method	EXTENDED VIKOR METHOD; AGGREGATION OPERATORS; DECISION-MAKING; FUZZY VIKOR; MODEL	With the advent of the information age, choosing suitable suppliers has become an important issue in supply chain management. Selection of the optimal supplier will ensure the competitiveness and customer satisfaction of the overall supply chain. However, assessment attribute values are sometimes missing or nonexistent in supplier selection. Moreover, experts often fail to confirm the rating values of assessment attributes, hesitating between linguistic term sets. Conversely, because experts have disparate expertise, experience, and backgrounds, they will use different linguistic term sets to evaluate the rating values of assessment attributes. These issues complicate supplier selection. To overcome these hurdles, this paper integrates the 2-tuple linguistic representation and soft set to solve supplier selection problems with incomplete information. To illustrate the application of the proposed approach, a real-life example of the selection of key anesthetic equipment is adopted. This paper also compares the results of this simulation with those of the arithmetic average, fuzzy VIKOR, and interval 2-tuple linguistic VIKOR methods. The proposed approach is more suitable and more accurate in solving supplier selection problems with uncertain and incomplete information.																	0952-1976	1873-6769				JAN	2020	87								103248	10.1016/j.engappai.2019.103248													
J								Novel supplier grading approach based on interval probability hesitant fuzzy linguistic TOPSIS	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										GVIOWAWA operator; Interval probability hesitant fuzzy linguistic; Euclidean distance; IPHFL-TOPSIS; Supplier grading	GROUP DECISION-MAKING; INDUCED AGGREGATION OPERATORS; TERM SETS; WEIGHTED AVERAGE; SELECTION; VIKOR	The supplier grading, a typical multi-attribute linguistic group decision-making (MALGDM) problem, is one of the important research topic in the uncertain linguistic environment, especially its aggregation of evaluation information and the establishment of grading approaches. In this paper, first we introduce the generalized interval probability hesitant fuzzy linguistic IOWA weighted average (GVIOWAWA) operator to aggregate the uncertain linguistic information with incomplete reliability. The GVIOWAWA operator can allow decision makers to select the appropriate parameters according to their needs. Then the interval probability hesitant fuzzy linguistic TOPSIS (IPHFL-TOPSIS) based on the interval probability hesitant fuzzy linguistic Euclidean distance is established. The IPHFL-TOPSIS model can effectively and objectively help businesses find the strategic cooperation supplier. Eventually, we give the numerical examples concerning the comprehensive assessment of material supplier and energy supplier to illustrate validity and applicability of the proposed approach and compare the proposed method with different parameters and methods to perform its flexibility.																	0952-1976	1873-6769				JAN	2020	87								103299	10.1016/j.engappai.2019.103299													
J								Fault prognostics by an ensemble of Echo State Networks in presence of event based measurements	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Prognostics; Missing data; Sliding bearing; Echo State Network; Ensemble; Differential evolution optimization	USEFUL LIFE PREDICTION; DIFFERENTIAL EVOLUTION; ELECTROLYTIC CAPACITOR; DATA-ACQUISITION; NEURAL-NETWORK; DIAGNOSIS; WORKING; SYSTEMS; ENERGY	Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods.																	0952-1976	1873-6769				JAN	2020	87								103346	10.1016/j.engappai.2019.103346													
J								Using fuzzy measures for modeling human perception of uncertainty in artificial intelligence	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Artificial intelligence; Fuzzy measures; Uncertainty modeling; Expected value of measure; Self-dual; Human perception		We observe the importance of modeling human perceptions of various types of information for the construction of A. I. systems and highlight the need for modeling human perceptions of uncertainty. We introduce the concept of a fuzzy measure, mu, and discuss its use as a general structure for the representation of knowledge about an uncertain variable. We note the ability of the fuzzy measure to model various formulations of uncertain information in a unified format. We look at various properties of fuzzy, particularly the ability to fuse multiple fuzzy measures to form new measures. We introduce various operations on fuzzy measures motivated from probability theory such as the determination of expected values and variances. We emphasize the importance of the Choquet integral in these operations. We discuss two characterizing features of a fuzzy measure, its entropy and attitudinal character, and note their usefulness in helping select an appropriate measure. We finally look at the issue of answering questions about variables having information about its value modeled by a fuzzy measure. Here we must come to grips with the fact that for most measures a fundamental property of the probability measure, Prob(A) + Prob((A) over bar) = 1, does not hold.																	0952-1976	1873-6769				JAN	2020	87								103228	10.1016/j.engappai.2019.08.022													
J								Manta ray foraging optimization: An effective bio-inspired optimizer for engineering applications	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Global optimization; Engineering design; Manta ray foraging optimization; Metaheuristic; Constrained problems; Optimization algorithm; Heuristic algorithm	PARTICLE SWARM OPTIMIZATION; DESIGN OPTIMIZATION; DIFFERENTIAL EVOLUTION; HEURISTIC OPTIMIZATION; SEARCH OPTIMIZATION; ALGORITHM; MOVEMENTS	A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo.																	0952-1976	1873-6769				JAN	2020	87								103300	10.1016/j.engappai.2019.103300													
J								A size-transferring radial basis function network for aero-engine thrust estimation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Aero-engine; Thrust estimation; High dimensional particle swarm optimization; Radial basis function neural network		Thrust regulation plays an important role in the aero-engine control. However, the thrust is unmeasurable in flight which poses a great challenge to the thrust control. Traditional thrust control methods are implemented by controlling the parameters tightly related to thrust and reserve enough safety margins to protect the engine. To realize the direct thrust control, the methods to estimate thrust is urgently required. In this paper, a new algorithm based on particle swarm optimization (PSO) and radial basis function neural network (RBFNN) is proposed to estimate the thrust. A strategy named "size-transferring" is developed to select and adjust the network size of the RBFNN. Besides, to solve the high-dimensional optimization problem during the estimation, a new approach based on the PSO algorithm is also illustrated. The successful application of the proposed algorithm to the aero-engine thrust estimation problem demonstrates its effectiveness.																	0952-1976	1873-6769				JAN	2020	87								103253	10.1016/j.engappai.2019.103253													
J								Medical image restoration method via multiple nonlocal prior constraints	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Medical image restoration; nonlocal prior; low-rank; sparsity; surfacelet transformation	SPARSITY; RECONSTRUCTION; DECOMPOSITION; RECOVERY; MRI	Medical image restoration is a fundamental issue in the area of medical signal processing, which aims recove high quality medical image from its degradation observation. Recently, the methods with nonlocal self-similarity prior have led to a great improvement on many medical image restoration tasks. Nevertheless, the nonlocal technique is generally embedded with only one kind of constraint, such as sparsity or low-rank in the conventional model, which limits their abilities and show good performance on certain prior. To address this problem, in this paper, we present a novel medical image restoration method with multiple nonlocal-based prior regularizations. The surfacelet transformation is introduced to construct a cubic sparsity constraint to a group of nonlocal similar patches. Likewise, due to the self-similarity existed in the medical image, two extra kinds of nonlocal-based priors, nonlocal total variation and nonlocal weighted low-rank, are also exploited to constrain the local smoothness and nonlocal relationship jointly. In this way, each of the designed priors can well recover a group of patches with similar structure. And then, the designed priors are combined into a unified proposed optimization framework, which will obtain the advantages from all of them simultaneously. Finally, to solve the objective function in the proposed framework, we develop an iterative numerical scenario based on alternating direction multipliers method. The extensive experiments on test medical images demonstrate that our proposed model outperforms the comparison methods on both of visual quality and objective evaluation results.																	1064-1246	1875-8967					2020	38	1					5	19		10.3233/JIFS-179375													
J								Research on brush face payment system based on internet artificial intelligence	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet; artificial intelligence; face payment; face recognition; third party payment system	RECOGNITION; MODEL	The facial information of a person has constant invariance, and at the same time, it is an ideal basis for identifying identity information through differences between different individuals. The use of face recognition in third-party payment platforms will greatly improve the efficiency and security of payment. Based on this, the paper studies and designs the third-party payment system for face recognition in the context of Internet artificial intelligence to improve the convenience and security of face payment.																	1064-1246	1875-8967					2020	38	1					21	28		10.3233/JIFS-179376													
J								Research on improved genetic algorithm in path optimization of aviation logistics distribution center	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Improved genetic algorithm; distribution center; path optimization; aviation logistics		At present, airlines cannot meet the requirements of increasing air cargo volume and cargo service quality by simply improving air transportation capacity. On the basis of increasing the construction of aviation logistics distribution centers, airlines also need to optimize the distribution route. By analyzing and comparing the application of genetic algorithm in the problem of distribution path optimization, this paper proposes a new adaptive genetic algorithm with adaptive mutation to improve the search ability in the local range, which has faster convergence speed than the general genetic algorithm. And solve the logistics node location model. The result proves that the algorithm is faster and more accurate, and can solve the transportation path problem in distribution.																	1064-1246	1875-8967					2020	38	1					29	37		10.3233/JIFS-179377													
J								Research on multi-user air cargo route selection based on improved genetic algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Improved genetic algorithm; multi-user transportation; air cargo; route optimization		The article combines mathematical models to describe the multi-customer air cargo routing problem. Aiming at the coding problems in standard genetic algorithms and the large number of non-feasible solutions after crossover and mutation, this paper proposes a new feasible method-two-generation competition and steady-state genetic operator design. Finally, the model solving method and steps based on improved genetic algorithm are given. The example calculation shows that the method can provide scientific and effective support for the decision of the route selection problem in the air cargo system.																	1064-1246	1875-8967					2020	38	1					39	46		10.3233/JIFS-179378													
J								An improved discrete consensus algorithm for reactive power sharing considering communication delay condition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Reactive power sharing; distributed generator; discrete consensus algorithm; time-delay		The difference of feeder impedance and the nonlinear load may deteriorate reactive power sharing and worsen the power quality, which may cause the breakdown of power supply in islanded microgrids. An enhanced discrete consensus algorithm considering information transmission delay is put forward in order to optimize the convergence dynamic. The proposed algorithm defines the independent iteration number of times variable for every agent, which can be transferred to its adjacent agents together with the communication data to control the times of the iteration calculation. The optimized control strategy is adopted for three-phase distributed generators to balance the reactive power sharing. Eventually, the simulation results show that the reactive power will be evenly shared among distributed generators in proportion to their rated capacity and the proposed algorithm achieves good dynamic performance during the transition.																	1064-1246	1875-8967					2020	38	1					47	53		10.3233/JIFS-179379													
J								A new aggregated search method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Aggregated search; dimension reduction; density clustering; information extraction		Aggregated search is the task of integrating results from potentially multiple specialized search services, or verticals (images, videos, news, etc.), into the Web search results. Major search engines perform what is known as Aggregated Search. Aggregated search is relatively new and its advantages need to be evaluated. With the increasing size of the data set on the network, different results retrieved from different dimensions are sometimes entirely different for a given search problem. In this paper, a new aggregated search algorithm was proposed. Firstly, the retrieval data in the vertical domain was textualized, and Doc2Vec makes the vector representation of the data. Then the results are aggregated and output by dimension reduction and density clustering. The experimental results show that the model achieves good accuracy in 20 given queries and significantly improves the aggregated search results. Discussion about the results also allowed us to identify some useful thoughts concerning the evaluation of AS approaches.																	1064-1246	1875-8967					2020	38	1					55	63		10.3233/JIFS-179380													
J								A dynamic game model analysis for friendship selection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet friend-making; actualization; game model; experimental study	ONLINE; ATTRACTION; BEHAVIOR	Befriending with net friends in real life is an important choice for people who live in a fast-paced lifestyle, but results are unsatisfactory and even cause a lot of malignant events. Therefore, this paper simulates the whole process of Internet friend-making through the construction of game model and field experiment, with the aim of showing the behavior logic of people with limited rational when they try to befriend with net friends in real life. This paper aims to give more behavior guidance to related participants. Results from this research show that in the process of making friends on the Internet, the basis for people befriending with net friends in real life is that they assume that their conditions are equal. But in off-line activities, they cannot improve their effectiveness, which then terminates their communication. Therefore, this paper suggests that Internet social users should distinguish Internet friend-making from traditional friend-making, and do not have too many unrealistic expectations of Internet friend-making.																	1064-1246	1875-8967					2020	38	1					65	73		10.3233/JIFS-179381													
J								Dynamic response and control accuracy optimization of marine hydraulic manipulator based on piecewise P and fuzzy PI control algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Piecewise P and fuzzy PI algorithms; accumulator; control accuracy; dynamic response; power bond graph; marine hydraulic manipulator; small pump	FAST SWITCHING VALVE; SOLENOID VALVE; SYSTEM; ACTUATOR; DESIGN	Marine hydraulic manipulator is limited by its size, weight and environment, so traditional hydraulic manipulator cannot be used for marine equipment. In this paper, large displacement pump is replaced by small displacement pump, accumulator is used as absorption pulsation replace by power source, and servo valve is replaced by solenoid valve. In order to improve the dynamic response and control accuracy of the marine hydraulic manipulator. Firstly, the bond graphs model of hydraulic manipulator supplied by oil are established and analyzed. Secondly, the dynamic response characteristics of solenoid valve under different supply voltage are analyzed. Thirdly, the piecewise P optimization of dynamic response and the fuzzy PI optimization of position control of hydraulic manipulator are carried out. Through piecewise P and fuzzy PI algorithms, not only the dynamic response of the marine manipulator is improved, but also the control accuracy of the marine manipulator is improved. Compared with the traditional hydraulic manipulator, the marine hydraulic manipulator installed volume reduced by 49%, the opening time of solenoid valve is increased by 36%, the response speed is increased by 73%, and the control accuracy is improved by 35%.																	1064-1246	1875-8967					2020	38	1					75	88		10.3233/JIFS-179382													
J								Multi-head attention model for aspect level sentiment analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Text sentiment classification; fine-grained sentiment analysis; attention mechanism		Aspect level sentiment classification task requires topical polarity classification for different description aspect. There is a polysemy in the same vocabulary, and the emotional polarity is different for different objects. Word embedding can capture semantic information but cannot adapt to the polysemy. Attention mechanism has achieved good performance in the above tasks; however, it is only able to get the degree of association between words and unable to get detailed descriptions. In this paper, the ELMOs model is used to adjust the polysemy of the word. The Transformer model is used to extract the features with the highest degree of relevance to the target object for emotional polarity classification. Our work contribution is to overcome the polysemy interference, and use the attention mechanism to model the network relationship between words, so that the model can extract important classification features according to different target words. Experiments on laptop and restaurant datasets demonstrate that our approach achieves a new state-of-the-art performance on a few benchmarks.																	1064-1246	1875-8967					2020	38	1					89	96		10.3233/JIFS-179383													
J								Research on network security subsystem based on digital signal	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Network video surveillance; embedded system; digital signal processor; network security subsystem		Communication network security is an important part of the digital signal processor. In particular, the process bus replaces the traditional hard wiring, which makes the network system extend from the second to the first time, greatly increasing the network scale and network traffic; the information transmitted on the process bus is absolutely large. Most requirements require strict real-time and high reliability. Therefore, information security is a major problem that threatens the security, stability, economy, and quality operation of network systems, and needs to be paid enough attention. This paper introduces the methods of information classification and information merging to improve the real-time information. It proposes to apply network security technologies such as information encryption technology, firewall technology, mobile agent, security management technology and virtual private network (VPN) technology to office network of the electric-power industry and analysed the specific application scenarios and effects.																	1064-1246	1875-8967					2020	38	1					97	103		10.3233/JIFS-179384													
J								Removal of rain from video based on dual-tree complex wavelet fusion	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Raindrops removal; dual-tree complex wavelet fusion; PCA; local energy	IMAGE	Bad weather has a negative effect on the perceptual quality and degrades the performance of computer vision system. Therefore, a rain removal method based on dual-tree complex wavelet fusion is proposed. The algorithm can be further used for video surveillance system and intelligent transportation and other fields. The method analyzes from the perspective of frequency domain, using the dual-tree complex wavelet decomposition: decomposing images into low frequency sub-images and high frequency sub-images, then developing the different fusion rules. For the low frequency sub-images, fusion rules using the principal component analysis. For the high frequency sub-images, fusion rules using the local energy matching. In this paper, an image edge enhancement algorithm based on fast guided filter is proposed, a SIFT feature matching method based on maximum likelihood estimation sampling and consistent(MLESAC) algorithm is proposed. Experiments results show that the proposed algorithm can improve the definition of images and restore the details of the target blocked by raindrops.																	1064-1246	1875-8967					2020	38	1					105	113		10.3233/JIFS-179385													
J								An image retrieval method of mammary cancer based on convolutional neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; VLAD; loss function; medical image retrieval	MAMMOGRAPHIC MASSES; CLASSIFICATION	This paper designs a brand-new image retrieval method of mammary cancer based on convolution neural network. This method simulates VLAD layer in CNN network structure, designs a trainable universal VLAD layer-NET VLAD layer, reduces dimensions and optimizes VLAD descriptors, applies structure from motion algorithm to automatically label samples, and obtains the minimum loss function value by a new training program of weakly supervised ranking loss. Experiments show that this method has improved retrieval performance compared with similar retrieval methods and non-network structure retrieval methods.																	1064-1246	1875-8967					2020	38	1					115	126		10.3233/JIFS-179386													
J								Medical brain image classification based on multi-feature fusion of convolutional neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; multi-feature fusion; heuristic search; medical image classification	BIG DATA	This paper presents a medical brain image algorithm based on multi-feature fusion. Feature extraction based on convolutional neural network was used as texture information, feature extraction based on voxel information was used as morphological feature, and then the two types of features were combined in series. Feature extraction based on convolutional neural network was used as texture information, feature extraction based on voxel information was used as morphological feature, and then the two types of features were combined in series. Then the heuristic search algorithm is used to optimize the feature selection stage. Based on the feature score table extracted by the recursive feature elimination method of support vector machine, the correlation between features is added. Moreover, through experimental analysis, the optimal value of the parameter K was selected according to the heuristic search, and the optimal feature subset was extracted after determining the value of the parameter K. Experiments show that compared with similar algorithms, this algorithm improves the accuracy and efficiency of the classification of brain images.																	1064-1246	1875-8967					2020	38	1					127	137		10.3233/IFS-179387													
J								Research on secure encryption method of multi-domain fiber network based on particle swarm optimization algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Particle swarm optimization algorithm; multi-domain fiber optic network; secure encryption		In order to improve the security of information storage in multi-domain optical network privacy protection, hybrid encoding and encryption of privacy protection information in multi-domain optical network is carried out. A hybrid coding and secure encryption technique based on particle swarm optimization (PSO) for privacy protection information in multidomain optical network is proposed. The coding mapping equation of privacy protection information security encryption in multi-domain optical network is constructed, and the privacy protection information of multi-domain fiber network is loaded into the coding mapping equation of security encryption of privacy protection information in multi-domain fiber network. A set of characteristic solutions describing the entropy function of the characteristic distribution of random encryption keys are obtained, and the elliptic mapping random linear combination coding and chaotic encryption key allocation are carried out. The privacy protection method of multi-domain optical fiber network is used to encrypt and decode information with mixed coding and steganography. Under the bilinear mapping coding system, the privacy-protected information in multidomain optical network is encrypted and encrypted, stored and transmitted confidently. The simulation results show that this encryption technique has better steganography performance and better secure transmission and storage performance for the privacy protection information mixed encoding encryption in multi-domain optical network.																	1064-1246	1875-8967					2020	38	1					139	145		10.3233/JIFS-179388													
J								Intelligent prediction algorithm of economic trend index based on rough set support vector machine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Rough set; support vector machine; economic trend index; intelligent prediction		In order to improve the ability of automatic estimation and prediction of economic trend index, an intelligent prediction model of economic trend index based on rough set support vector machine is proposed. The statistical analysis of intelligent prediction of economic trend index is carried out by using the equivalent approximate linear model, and the regression analysis model of intelligent prediction of economic trend index is established. Combining with the rough set support vector machine big data fusion technology, the feature extraction and information mining are carried out in the process of intelligent prediction of economic trend index, and the statistical time analysis series of economic trend index is constructed. The spatial distribution of economic trend index distribution series is reconstructed, and the economic trend is evaluated and predicted in the high dimensional economic trend index forecast series distribution space. The principal component characteristic analysis and fuzzy closeness analysis of economic trend index are carried out by using fuzzy relational degree scheduling method. Taking economic cost, economic development prospect and economic growth rate as constraint indexes, the method of multi-factor joint estimation is adopted. Realize economic trend index intelligent forecast. The simulation results show that the accuracy of fast estimation of economic trend index is high, the time cost is small, and the ability of intelligent prediction is stronger.																	1064-1246	1875-8967					2020	38	1					147	153		10.3233/JIFS-179389													
J								Similarity detection method of abnormal data in network based on data mining	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; abnormal data in network; population; optimized genetics; similarity; detection		With the continuous progress of network technology, some abnormal data are often confused in network data flow, which affects network security. In order to grasp the abnormal degree of abnormal data in networks and detect the similarity of abnormal data, an optimized genetic data mining algorithm is used to mine abnormal data in network, obtain the initial population of abnormal data mining and optimize genetic operation. On this basis, the network data type and the number of network data types are adaptively adjusted to obtain the optimal abnormal data mining results. Based on Euclidean distance, the similarity value of abnormal data in network is calculated, and the greater the similarity value is, the greater the similarity of abnormal data is and vice versa. The experimental results show that the average standard deviation of detection error and energy consumption of the proposed method are 0.00865 and 398J, respectively. This method is a reliable and energy-saving method for similarity detection of abnormal data in network, which provides an effective basis for grasping the anomaly degree of network data.																	1064-1246	1875-8967					2020	38	1					155	162		10.3233/JIFS-179390													
J								Research on enterprise management integration mechanism and information platform by internet of things	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Enterprise; information platform; internal control; Internet of Things; management integration	BIG DATA ANALYTICS; OF-THE-ART; NETWORKS; MODEL	In the era of the Internet of Things, the way of information dissemination and the speed of information computing have changed dramatically. New generation technologies such as big data, IOTs, cloud computing, and mobile internet have rapidly developed, and the information resources in the process of using customers have promoted to value creation activities. The guiding force, therefore, the service model must also follow the changes. This paper is based on the enterprise management integration mechanism and information platform research of the Internet of Things environment, and analyses its conceptual model, the relationship between the platform and related subjects, the value creation mechanism, the core business functions and the profit model, which have certain guiding significance for practice. The enterprise management integration mechanism and information platform research in the Internet of Things environment requires the parties to interact, share, and cooperate with each other to achieve common value creation. In addition, enterprises effectively research internal resource integration and information sharing, rationally streamline enterprise management institutions, thereby improving the quality and efficiency of business operations, giving full play to the role of internal control and risk management, so that enterprises can develop steadily.																	1064-1246	1875-8967					2020	38	1					163	173		10.3233/JIFS-179391													
J								A new robot collision detection method: A modified nonlinear disturbance observer based-on neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Robot collision detection; NDO; GMO; friction estimation; neural networks	IMPEDANCE CONTROL	Collision detection is the core issue in physical human-robot interactions, and many detection methods based on robot dynamic models have been proposed. However, model uncertainties, especially complicated friction, seriously affect the collision detection performance of these methods. In this paper, a nonlinear disturbance observer (NDO) originally proposed for friction estimation is applied for the first time in robot collision detection. To verify that the collision detection performance of the NDO is better than that of the classical generalized momentum observer (GMO), the detection sensitivity, robustness and external torque estimation accuracy of each method are compared and analyzed. Then, to eliminate the effects of friction uncertainties on the collision detection results, a modified nonlinear disturbance observer (MNDO) based on neural networks is proposed to improve the collision detection performance. To verify the effectiveness of the algorithm, simulations and experiments are conducted with a 6-DOF robot and two single-joint platforms. The results indicate that the proposed algorithm is accurate and effective.																	1064-1246	1875-8967					2020	38	1					175	186		10.3233/JIFS-179392													
J								A circular convolution based on compressed sensing imaging algorithm for FMCW CSAR	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										FMCW CSAR; compressed sensing; circular convolution; imaging	FAST FOURIER-TRANSFORMS; RECONSTRUCTION	The frequency modulated continuous wave circular synthetic aperture radar (FMCW CSAR) is a high resolution imaging radar, which plays an important role in target recognition. However, the traditional wave number domain imaging algorithm has a low resolution for the target far from the scene center, which limits its wide applications in CSAR imaging. In this paper, when the targets are sparse or compressible, a circular convolution algorithm based on compressed sensing is proposed to improve the resolution. In the algorithm, the circular convolution and the Fourier transform is used to reduce computation cost. What's more, the compressed sensing is applied to improve the imaging resolution for the target far from the scene center, which can effectively avoid the complex calculation of the system kernel function in the traditional wave number domain algorithm. Some simulation results illustrate the effectiveness of the proposed method.																	1064-1246	1875-8967					2020	38	1					187	196		10.3233/JIFS-179393													
J								Point cloud registration algorithm based on the volume constraint	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Point cloud alignment; FPFH; ICP		The traditional point cloud alignment method suffers from drawbacks such as a large extensive computation, low computing speed, and poor alignment accuracy. To overcome these problems, this paper proposes a fast and highly accurate algorithm based on fast point feature histograms (FPFH) algorithm and spatial constraints. The proposed algorithm first filters, denoises the point cloud dataset, and calculates the point cloud normal to obtain the FPFH eigenvalue. Then, the vertebral space is divided into three regions according to its location, and the feature points in each region are calculated. The Euclidean distance between a feature point and the boundary of the adjacent region, and the weight coefficient corresponding to the feature point are given according to the calculated distance. The method overcomes the defect that the query feature point has a large workload in the traditional ICP algorithm and improves the registration precision of the point cloud. The experimental results show that the proposed method effectively solves the problems of the traditional point cloud registration algorithm, can effectively reduce the mismatch rate of point cloud registration, and can improve the registration accuracy and stability without reducing the registration of the elements.																	1064-1246	1875-8967					2020	38	1					197	206		10.3233/JIFS-179394													
J								An innovative method to measure and predict drivers' behaviour in highway extra-long tunnels using time-series modelling	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Time-series modelling; NARX neural network; driver; behavior risk characteristics; safety speed difference		Previous studies lack a comprehensive evaluation model that combined the subjective perception of the driver and the objective driving environment. This work investigates the characteristics of drivers' behavior risk in highway extra-long tunnels. Real-vehicle tests were conducted in two typical extra-long tunnels and the speed of skilled and unskilled drivers were collected simultaneously. The quantified model of drivers' behavior risk was proposed based on the safety speed difference. The variation characteristics of behavior risk both inside the tunnel and ordinary highway were analysed. Further, the NARX neural network was used to predict real-time speed with the heart rate regarded as the input variable. Results showed that skilled drivers showed the highest behavior risk in the internal zone, while the highest value of unskilled drivers was at the exit zone in the tunnel section. Both two types of drivers presented the highest and the lowest behavior risk on the ordinary highway and the tunnel entrance zone respectively. The proposed NARX model could predict synchronous speed with high accuracy. These results of the present study concern the driver's risk characteristics in Internet ofVehicles and howto establish the automated driver model in the simulation driving environment.																	1064-1246	1875-8967					2020	38	1					207	217		10.3233/JIFS-179395													
J								(k, n) scalable secret image sharing with multiple decoding options	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Secret image sharing; scalable; decoding model; all-or-nothing; progress	HIGH-QUALITY; SHADOW SIZE; SCHEME; CONSTRUCTIONS	Traditional (k, n) secret image sharing provides all-or-nothing decoding model and (k, n) scalable secret image sharing provides progress decoding model during image reconstruction. Both these two decoding models are significance in various applications. However, the security level for real applications would change due to the dynamical environment, only one single decoding model cannot satisfy the changeable security requirement. In this work, we construct scalable secret image sharing schemes that provides both all-or-nothing and progress decoding models to satisfy the dynamical secure requirement. Each participant in our schemes only needs to keep one initial-shadow. During image reconstruction, the dealer selects the decoding model according to current security requirement, if progress model is chosen, initial shadows can achieve image reconstruction in progress model without any modification; else if all-or-nothing model is chosen, the dealer does not need to resent new shadows to participants, the initial-shadows can be updated to satisfy all-or-nothing model efficiently.																	1064-1246	1875-8967					2020	38	1					219	228		10.3233/JIFS-179396													
J								A multi-technique fusion approach for fault localization in manufacturing software	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fault localization; manufacturing software; spectrum information; multi-technique fusion; GAP	AGREEMENT; SPECTRUM	Fault localization is the critical but most expensive step in testing manufacturing software, effectively locating faults has become an increasingly concerned study. The existing spectrum-based fault localization techniques utilize spectrum information and specific prioritization algorithm to generate the suspiciousness as well as the ranking of statements. However, the effectiveness of fault localization in manufacturing software would be dramatically reduced once the statement involving bug is assigned with the same suspiciousness as other non-faulty statements. A multi-technique fusion approach (FA) is proposed based on suspicious rankings, which merges various of randomly selected fault localization techniques to minimize the difference between the numbers of statements that need to be examined (GAP) to find the bug respectively in the worst and best assumptions, further improve the effectiveness of fault localization. In addition, a novel metric for comparing fault localization techniques is developed. Experiments on Siemens Suite shows that our approach outperforms these selected techniques in the effectiveness.																	1064-1246	1875-8967					2020	38	1					229	238		10.3233/JIFS-179397													
J								Application research based on improved genetic algorithm in cloud task scheduling	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Task scheduling; genetic algorithm; phagocytosis; multi-population hybrid coevolutionary		Task scheduling in the cloud environment is a hot issue in current research. Aiming at the task scheduling problem in cloud environment, this paper analyses the scheduling model of cloud tasks, proposed an improved genetic algorithm (PGA) based on phagocytosis, changed the crossover operation of standard genetic algorithm (GA), formed a sub-chromosome individual after phagocytosis of two mother chromosomes, another individual was generated randomly, and the new individual generated after phagocytosis is determined by the fitness function and the load-balancing standard deviation, so that the evolution process can ensure a high proportion of high-quality individuals in the population. Ensure the diversity of the population. Then a multi-population hybrid coevolutionary genetic algorithm (MPHC GA) is adopted, which uses the Min-Min algorithm to generate initial multiple sub-populations, and these sub-populations are evolved by standard genetic algorithm (GA) and improved genetic algorithm (PGA) based on phagocytosis. The simulation results show that the proposed algorithm is effective in cloud task scheduling.																	1064-1246	1875-8967					2020	38	1					239	246		10.3233/JIFS-179398													
J								An optimal and dynamic elephant flow scheduling for SDN-based data center networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data center network; SDN; elephant flow scheduling; genetic algorithm; ant colony algorithm		With the rapid development of data center network, the traditional traffic scheduling method can easily cause problems such as link congestion and load imbalance. Therefore, this paper proposes a novel dynamic flow scheduling algorithm GA-ACO (Genetic Algorithm and Ant COlony algorithms). GA-ACO algorithm obtains the global perspective of the network under the SDN (Software defined network) architecture. It then calculates the global optimal path for the elephant flow on the congestion link, and reroutes it. Extensive experiments have been executed to evaluate the performance of the proposed GA-ACO algorithm. The simulation results show that, in comparison with ECMP and ACO-SDN algorithm, GA-ACO can not only reduce the maximum link utilization, but also improve the bandwidth effectively.																	1064-1246	1875-8967					2020	38	1					247	255		10.3233/JIFS-179399													
J								Research on smart EFK algorithm for electric vehicle battery packs management system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Electric vehicle; battery management system; smart EFK algorithm; extended kalman filter; state of charge	LITHIUM-ION BATTERIES; CHARGE ESTIMATION; PART 1; STATE	Battery management system (BMS) is an important part of electric vehicle that can improve the efficiency of battery and ensure the safety during operating process. State of charge (SOC) estimation is one of kernel technology of BMS. High precision of SOC estimation can not only increase service life of battery, but also enhance energy utilization efficiency and cut down the operational cost. Based on fully understand working principle of battery, three-order RC equivalent circuit is chose to simulate the external characteristics. Introduce the working theory of that combination of extended Kalman filter (EKF) and open circuit voltage (OCV), and the whole recursive algorithm of estimation SOC, using the OCV-SOC look-up table to solve the problem of initial value of SOC to a certain degree. The hardware circuit and software of BMS are designed. Hardware circuit consists of current measurement circuit, voltage measurement circuit, temperature measurement circuit and equalization circuit. The software is designed by C as the development language, the Microsoft Visual Studio 2008 as development environment and SQL Server 2008 as database management system. The experiments are carried out after the experimental platform of system hardware and software is constructed. The system realizes the real-time online monitoring of BMS, SOC estimation and etc. The result verifies the reliability and feasibility. The precision requirement of SOC estimation by using EKF algorithm and OCV-SOC look-up table meets the national standard for electric vehicle.																	1064-1246	1875-8967					2020	38	1					257	262		10.3233/JIFS-179400													
J								Virtual surgery system for liver tumor resection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Virtual surgery system; hepatocellular carcinoma (HCC); metastatic tumor; multi-detector CT (MDCT); spherical scoring filter; force-feedback device	REALITY; CT; SEGMENTATION; SIMULATORS; CONSTRUCTION; NEUROSURGERY; TECHNOLOGIES; FEATURES	Multi-phase computed tomography images are widely applied to doctors' routine interpretation of organic lesions or before human abdominal surgery. However, because of the lack of intuitive and three-dimensional (3D) observations, human factors lead to 80% of the clinical errors. A primary malignant tumor resection system based on a virtual reality (VR) helmet and a force-feedback device combined with a 3D printing model is proposed in this paper. First, we used the thin-plate spline (TPS) deformation method to register the different phase images. In the case of a metastatic tumor, a spherical scoring filter (SSF) model was built for searching the tumor pattern with edge detection and subtraction processing, from which the initial tumors were selected on the basis of the calculated score. For hepatocellular carcinoma cases, candidates were extracted as the areas without edges by using edge detection filters on the subtraction image between the equilibrium and arterial phase images. Finally, the false positive (FP) candidate was eliminated before obtaining the 3D shape of the liver tumor in the expansion process, thus providing an accurate volume of lesions for the surgical plan for the liver resection. The results showed that our virtual surgery system enabled the user to simulate the use of a scalpel for cutting and removing the organ labels; 3D display on a VR helmet with a deformable effect; and touching organs with a force-feedback device. Our virtual surgery tools could simulate all of the effects of the doctors' operations and make clinical practice more efficient.																	1064-1246	1875-8967					2020	38	1					263	276		10.3233/JIFS-179401													
J								Quickly evaluating the synergistic effects of top anti-cancer drugs by the computer high performance computing power and complex network visualization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Computer; visualization; pharmacogenetics; caner; anti-cancer drugs	CYTOSCAPE	Although it is very important to pay attention to the personalized medicine of tumour patients, there is quite lacking some methods to evaluate the synergistic effects of the top anti-cancer drugs in clinic. In the study, therefore, we used the network of pharmacogenetic and bioinformatics to discuss the complex interactions in the top anti-cancer drugs with the help of computer high performance operating power and network visualization. We got a total of 81 known targets from the top 10 anti-cancer drugs. The shared most targets were from the 5-fluorouracil, capecitabine and tegafur, occupying 4, 4 and 3 targets, respectively. Based on complex network visual effect of the computer, gene set enrichment analysis displayed that the targets were mainly on five major categories. All the data showed that the two groups of drugs having some synergistic effects (5-fluorouracil, capecitabine, tegafur and tamoxifen, letrozole, respectively), which is suggests the top anti-cancer drugs' synergic existing in combination drugs on clinic.																	1064-1246	1875-8967					2020	38	1					277	281		10.3233/JIFS-179402													
J								Framework of industrial networking sensing system based on edge computing and artificial intelligence	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Industrial networking; perceptual layer; edge computing; artificial intelligence	INTERNET	According to the large amount of and diversity of current industrial manufacturing data, on the basis of analyzed limitations of current industrial networking sensing system, a framework of industrial networking sensing system based on edge computing and artificial intelligence is put forward, including data-getting sensors group, first-level sensor routers, second-level sensor routers and backend severs. The above framework mainly is studied from three perspectives: structure level, workflow and key technologies, and the technology of artificial intelligence is the key to achieve this system with analysis of involved computational intelligence technology, information fusion technology and decision technology. This framework of industrial networking sensing system has important reference value on improving timeliness and certainty of industrial networking communications, largely reduces the entire power consumption and reduce stress of background sever.																	1064-1246	1875-8967					2020	38	1					283	291		10.3233/JIFS-179403													
J								Dynamic color design for multimodal industrial products based on genetic algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Genetic algorithm (GA); image; dynamic optimization; color design; multimodal industrial product (MMIP)	SYSTEM; IMAGE; FORM	A dynamic color design model and method are proposed for a class of industrial product color design problems with dynamic changes in the appearance of products and different appearance in different operating modes. These types of industrial products that have a variety of operating modes in the operating cycle are defined as multimodal industrial products (MMIPs). For a single mode, a single-modal industrial product (SMIP) color design model is established by using users' color image requirements as the design goal to transform the dynamic color design problem of products into weighting forms of several single modes and to realize the discretization of continuous problems. Finally, a dynamic color design model of MMIPs is established. A genetic algorithm (GA) is used to optimize this model to obtain the color design scheme that conforms to the users' dynamic image perception. Finally, by means of an example of color design about engineering machinery, this method is demonstrated to be feasible and applicable for dynamic color design optimization of MMIPs.																	1064-1246	1875-8967					2020	38	1					293	302		10.3233/JIFS-179404													
J								A novel wide-band polygonal slot circular antenna using the theory of characteristic modes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wearable antenna; TCM; feeding place; wide-band	WEARABLE ANTENNA	Theory of Characteristic Mode(TCM) has become a research hotspot in antenna design. The development of computational electromagnetic makes it possible to solve the eigenmode of microstrip antenna with arbitrary shape. In this paper, TCM with radiation boundary conditions is proposed to solve the input impedance of the antenna, and TCM based on differential equations is applied to the design of the circular wide-band wearable antenna with polygonal slot. By analyzing the radiation characteristics of different modes of the antenna, the optimal feeding place is selected to excite the desired mode by the normalized electric field distribution of different modes, and then the wide-band characteristics are obtained. Experiments show that the circular wide-band wearable antenna with polygonal slot presented in this paper has good performance with bandwidth enough to covers (5.725GHz-5.85 GHz) frequency band and certain flexural resistance from a certain angle for wearable intelligent equipment applications.																	1064-1246	1875-8967					2020	38	1					303	309		10.3233/JIFS-179405													
J								Improved random PWM modulation method based on preset carrier switching frequency	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Switching frequency; random PWM; harmonic suppression; preset carrier		To solve the problem of harmonic distortion generated by the output AC of inverters without considering the relationship between carrier switching frequency and carrier frequency, by analyzing the influence of carrier switching frequency on random carrier signal, it was concluded that the phase mismatch between the front and back carriers was the fundamental reason for the distortion of the output voltage of inverters when the preset carrier was switched randomly. And the switching frequency was randomized by switching the preset carrier, i.e., an improved kind of random carrier frequency modulation (RCFM). The simulation and experimental results showed that the proposed random PWM method could not only suppress the high-order harmonics of the output voltage more effectively, but also have better sinusoidal waveform than the conventional random PWM method.																	1064-1246	1875-8967					2020	38	1					311	318		10.3233/JIFS-179406													
J								Machine vision of textile testing and quality research	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine vision; defect; automatic detection; image processing; textiles		In today's globalized economy, various industries are promoting product transformation and upgrading. The textile industry is also facing a harsh international situation and fierce market competition. While constantly promoting the upgrade of automation equipment, the original quality control mode relying on manual testing has been unable to meet the modern production requirements and the market demand for product quality. This paper investigates the product inspection and quality control in the textile industry at home and abroad, and puts forward the application of machine vision technology in textile automated inspection and quality control, so As to strengthen the product quality control system and improve the product's physical quality. By studying the composition of machine vision detection technology, this paper studies the two core technologies of image acquisition and image processing in machine vision, summarizes and analyzes the common defects of textile Products, and proves that the common defects can be detected and repaired in time by machine vision detection method.																	1064-1246	1875-8967					2020	38	1					319	325		10.3233/JIFS-179407													
J								Innovative application for the wind farm control system with multi communication protocol converter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wind farm; monitoring system; communication protocol converter; protocol conversion		In wind farms with high integrated automation systems, different communicated structures, such as different communication protocols and communication mediums, exist among various intelligent equipment. Different communication protocols or media will lead to incompatibility of communication between various intelligent devices. Developing communication protocol converters is an effective way to solve such problems. In this way, the communication efficiency between intelligent devices will be affected to some extent, and then the operation of the whole wind farm will be affected. This study introduces a communication protocol converter integrated with a multiserial, multiprotocol, and multinetwork system. The protocol converter treats the microcontroller STM32F407 as the microcontroller unit of the converter to integrate multiple communication interfaces. Meanwhile, the compatibility problems of communication between the intelligent equipments in the wind field monitoring system can be solved using software to realize the conversion between the various communication protocols.																	1064-1246	1875-8967					2020	38	1					327	335		10.3233/JIFS-179408													
J								Provable secure attribute-based proxy signature	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Proxy signature; attribute-based; original signer; proxy signer; provable security; CDH problem		Provable security theory generally adopts the method of reduction, which makes use of the unsolvable mathematical problems in number theory to reduce the scheme to be safe. The idea of proof is a method of proof by contradiction: First, it is assumed that it is not difficult to solve the scheme presented in this paper, then the process of proving it, and finally, it is deduced that it is not difficult to calculate a certain difficult problem, which contradicts the difficulty of the mathematical problem. Then, it means that the assumption is not valid, and the scheme is proved to be safe. In this paper, the security of the scheme proposed in our previous paper is proved in detail. the scheme is proved to be secure against existential forgery under selective attributes and adaptive chosen-message attack. Its security can be reduced to the hardness of the computational Diffie-Hellman problem.																	1064-1246	1875-8967					2020	38	1					337	343		10.3233/JIFS-179409													
J								An innovative segmentation method with multi-feature fusion for 3D point cloud	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image processing; concave-convex segmentation; 3D point cloud; multi-feature fusion; region growth; colour feature		It is the key step of the classification and recognition to segment 3D point cloud. Aiming at the shortcomings of super-voxels concavo-convex segmentation algorithm for 3D point cloud, an efficient segmentation method based on multi-feature fusion is proposed. Firstly, the noises of 3D point cloud are removed by the statistical outlier removal filter, and the denoised 3D point cloud is simplified by the voxel grid filter. Secondly, it is divided into many voxels with the same size by the octree, and the voxels with the smallest mean curvature in local neighbourhood are used as seed voxels for regional growth to form super-voxels. Next, the super-voxels adjacency graph is structured and the concave-convex feature, continuous feature and colour feature between adjacent super-voxels are calculated as the weight on their edges. Finally, an arbitrary super-voxel is selected as the seed super-voxel to regional growth based on multi-feature weights of the edges in order to achieve the segmentation of the point cloud. The experimental results show that the proposed method whose segmentation speed, stability and accuracy are higher than existing methods greatly improves the over-segmentation or under-segmentation of the super-voxels segmentation algorithm.																	1064-1246	1875-8967					2020	38	1					345	353		10.3233/JIFS-179410													
J								Application of improved particle swarm algorithm to power source capacity optimization in multi-energy industrial parks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Particle swarm optimization (PSO); natural selection; chaos; industrial park; capacity planning	SYSTEM; DESIGN	Aiming at the optimization of power source capacity in multi-energy industrial parks, an economic optimization model with the lowest comprehensive cost of the system as the objective function was established, and an improved particle swarm optimization algorithm with natural selection strategy and chaos theory was proposed to optimize the model. This algorithm initialized particle fitness by chaotic mapping, added natural selection strategy to the iterative optimization process, and used chaotic ergodicity to search solution space. The test function simulation showed that the algorithm had the characteristics of fast convergence, high precision and being not easy to fall into local optimum. A case study of a certain area in Hebei Province, China, was selected to analyze the example, and the power source capacity optimization design scheme was obtained. The analysis results verified the effectiveness of the algorithm.																	1064-1246	1875-8967					2020	38	1					355	363		10.3233/JIFS-179411													
J								Front vehicle detection based on multi-sensor fusion for autonomous vehicle	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Obstacle detection; multi-sensor fusion; vehicle identification; AdaBoost algorithm	CALIBRATION	On account of the limitations of single sensor in obstacle detection, the paper investigates an obstacle detection method based on the fusion of 3D LiDAR and monocular visual. The spatial data fusion of the two sensors is realized according to their calibration results, and the time data fusion is realized by using double buffer technology. Considering the aspect ratio of vehicles, the image region of interest is determined based on the obstacle clustering of 3D LiDAR data. By using Haar-like features as effective characteristic of the front vehicle, integral figure is applied to extract Haar-like features of vehicle samples and non-vehicle samples. AdaBoost algorithm is used to choose weak classifiers to constitute strong classifiers, which combine into the cascade classifier. The cascade classifier has been trained to identify the vehicle target in the image region of interest. The relevant experimental results verify the effectiveness and real-time performance of the detection method.																	1064-1246	1875-8967					2020	38	1					365	377		10.3233/JIFS-179412													
J								Application of agricultural insect pest detection and control map based on image processing analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Threshold segmentation; graph segmentation; grab cut algorithm; saliency; maize diseases and pests		Image segmentation technology is a basic technology for image processing and analysis. As a typical interactive color image segmentation algorithm, grabbing segmentation has high precision, interactive operation and better segmentation effect in processing complex background segmentation, and has broad prospects in the field of agriculture. In this paper, the image segmentation algorithm of maize smut, Maize Head Smut and maize rust, which are three main diseases and insect pests, is studied by taking the high-yield crop Maize in Northeast China as an example. The image background in the static image editing is replaced by an improved one-time cutting algorithm. Through the adaptive combination of weights, the depth information and saliency information are combined into the grabbing color model. The improved image segmentation algorithm greatly improves the efficiency and accuracy of image segmentation, and achieves a good spot segmentation effect in the static image of corn pests and diseases, and has a high recognition. Do not rate. And it plays a predictive research effect in practical verification.																	1064-1246	1875-8967					2020	38	1					379	389		10.3233/JIFS-179413													
J								A motion simulation model for road network based crowdsourced map datum	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Semi-markov model; real road map; moving object; distribution rule; motion simulation model	DESIGN	Using the Semi-Markov decision model, we start with the real road map datum with a constructed logic network and construct the complex road network with random moving characteristics. First we translate the crowdsoured map datum into the vectorgraph in road network by the ArcGIS with the conversion of longitudinal and Latitude Coordinates to planar coordinates. In the motion simulation model all objects are sorted by the time of state change, and the moving object with the closest state change time to the current time are set at the front of the queue. And then, the moving object motion model based crowdsourced map datum is simulated. The experimental results for fitting and analysing the distribution rules of in-degree and out-degree show that the designed model can satisfy the Poission Distribution Rule on the cross node of Road Network based Uniform Distribution of moving object random motion, which conform to the characteristics of Distance Space and small-world network.																	1064-1246	1875-8967					2020	38	1					391	407		10.3233/JIFS-179414													
J								Detecting overlapping communities from micro blog network by additive spectral decomposition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Community detection; micro blog network; user interest; user interaction; heterogeneous network model; user similarity modelling		Detecting community structure is critical in analysing social networks which are flourishing and influencing every aspect of people's social life. Most social network systems are composed with complicated entity relations such and social interests, user relationships and their interactions. To understand how users interact with each other under the community level, its not enough to consider one kind of these relations while ignore the other. An united network model that can comprehensively integrate these relations is essential for community detection. Focusing on such kind of problem when dealing with social network with multiple relations, this paper proposes a heterogeneous network model which characterizes and constructs user similarity relations by combining both of users' interests and their interactions attributes. Based on the heterogeneous similarity model, an additive spectral decomposition algorithm is applied to detect overlapped communities from the network. The remarkable effect of our heterogeneous model is the ability to reveal most important attributes of the blog network. And, comparing to crisp clustering method, the additive spectral decomposition algorithm proposed is effective for finding overlapped user groups which is more reasonable among social networks where users tend to join multiple social groups. Results of experimental studies on real-world and synthetic datasets demonstrate the effectiveness of the algorithm with respect to the size, the distributive structure and the high dimensionality of the datasets.																	1064-1246	1875-8967					2020	38	1					409	416		10.3233/JIFS-179415													
J								Joint deep learning of angular loss and hard sample mining for person re-identification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Person re-identification; deep learning; angular loss; intelligent image processing		Person re-identification (ReID) is a critical work in the field of intelligent image processing and deep learning, which has attracted the attention of industry application. Person ReID focuses on matching person images obtained from non-overlapping camera views and finding the person-of-interest. An important unresolved problem is to obtain efficient metric for measuring the similarity among pedestrian images. Lately, deep learning with metric learning has become a general method for person ReID. Yet, previous methods mainly used a variety of distance to measure the similarity among samples. The way of distance measure is more sensitive when the scale changes. In this paper, we propose angular loss with hard sample mining (ALHSM) to learn better similarity metric for the person ReID. Our work uses the angular relationship in triangles as a measure of similarity, minimizing the angle at the negative point of the triangle. ALHSM combines with hard negative mining strategies, which learn better similarity metric and achieve advanced performance on several benchmark datasets. The experimental results show that our work is competitive compared to the state-of-the-art.																	1064-1246	1875-8967					2020	38	1					417	426		10.3233/JIFS-179416													
J								The deep learning word vector model using part of speech and sentiment information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; word vector; sentiment analysis; named entity recognition	CLASSIFICATION	Language Model is used to describe and calculate the probability of a reasonable sentence occurrence in natural language. In practical applications, language model as the core of natural language processing is often used in machine translation, information indexing, voice recognition, context processing such as sentiment recognition and other tasks. We will discuss advantages and weaknesses of traditional statistical language models and neural Network Language Models such as CBOW and Skip-gram. Keeping in view the traditional statistical language model and neural network model, we will try to put forward the word vector model based on part of speech and sentiment information (PSWV-model) in order to use more natural language information such as word order features, part of speech features, and sentiment polarity information under the framework of Mikolov's model. And finally we will present our deliberations on some advantages of PSWV model and other models including CBOW and Skip-Gram, CDNV in the NLP tasks including named entities recognition and sentiment polarity analysis.																	1064-1246	1875-8967					2020	38	1					427	440		10.3233/JIFS-179417													
J								A weighted recommendation algorithm based on multiview clustering of user	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommender systems; clustering; item attribute; weight; recommendation accuracy	COLD START; METADATA; SYSTEM; TRUST	Recommender systems are widely used to provide users with items they may be interested in without explicitly searching. However, they suffer from low accuracy and scalability problems. Although existing clustering techniques have been incorporated to solve these inherent problems, most of them fail to achieve further improvement in recommendation accuracy because of ignoring the correlations between items and the different effects of item attributes on recommendation results. In this article, we propose a novel recommendation algorithm to alleviate these issues to a large extent. First of all, users and items are clustered into multiple cluster subsets based on user-item rating matrix and item attribute deriving from domain experts, respectively. Then we use a selection method relying on item attribute to mine candidate items and only their predictions will be calculated in the next step, which can save the computation time greatly. Furthermore, by weighting the predictions with TF-IDF (Term Frequency-Inverse Document Frequency) weights, the top-N recommendations are generated to the target user for return. Finally, comparative experiments on two real datasets demonstrate that this algorithm provides superior recommendation accuracy in terms of MAE (Mean Absolute Error) and RMSE (Root Mean Square Error).																	1064-1246	1875-8967					2020	38	1					441	451		10.3233/JIFS-179418													
J								Experiment and research of seeding electromechanical control seeding system based on fuzzy control strategy	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Seed metering device; electronic control; fuzzy control		Traditional soybean seeders are driven by land wheels, which are easy to slip in complex operating conditions, resulting in the increased miss-seeding index and row-spacing coefficient of variation, etc. In order to solve these problems, a soybean electrical-control seeding system was designed in this paper. For improving the control accuracy of the electrical seeding system and achieving precise control of soybean seeding, the closed-loop control was adopted in the electric-driven Soybean Seeding system, the motor model of the electric-driven soybean seeding system was established and the transfer function of the motor was obtained. The PID control parameters were obtained by the Ziegler-Nichols PID tuning method, and the corresponding parameters were substituted into the control system simulation model established in MATLAB/SIMULINK. The conventional PID control system and the fuzzy PID control system were simulated respectively. Field trial results show that seeding with fuzzy PID control is better.																	1064-1246	1875-8967					2020	38	1					453	462		10.3233/JIFS-179419													
J								Finite element method and coupled mode theory coupling for accurate analysis of frequency splitting in wireless power transmission	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										magnetic resonant coupling; frequency splitting; coupled mode theory; critical coupling; coupled solution		Wireless power transmission technology avoids the problem of towed wires in the process of using electric energy, and increases the flexibility of using electricity. It is a hot research topic at present. In order to improve the system performance, a field-circuit coupling algorithm is proposed to analyze the system performance, with the help of the concept of supercomputing. Frequency splitting is a phenomenon in wireless power transfer (WPT) system when the coupling distance is less than the splitting point, the load power changes from a single-peak curve to a double-peak curve driven by two non-intrinsic resonant frequencies. Asymptotic coupled mode theory (CMT) method is used to analyse the frequency splitting phenomena in WPT system. It provides detailed information about interaction of field strength under different coupling states through coupled solution of FEM and CMT. Over coupling, critical coupling and under coupling are three typical states classified by frequency splitting. Experimental results are acquired by two helical resonators. The overall system reaches the critical coupling state when resonators space 1.5m and the total power on the load is 110W. Therefore, it is an efficient way to forecast transmission characteristics by using this method.																	1064-1246	1875-8967					2020	38	1					463	469		10.3233/JIFS-179420													
J								Compound dimensionality reduction based multi-dynamic kernel principal component analysis monitoring method for batch process with large-scale data sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Compound dimensionality reduction; discrete cosine transform; multi-dynamic kernel principal component analysis; large-scale data set; penicillin fermentation process		To reduce and eliminate the three problems for large-scale data sets that include high computational complexity, large storage space, and long time-consuming in complex batch process with inherent dynamics and nonlinearity, a novel approach based on multi-dynamic kernel principal component analysis (MDKPCA) by exploiting compound dimensionality reduction for fault detection is proposed. The method firstly uses discrete cosine transform (DCT) having strong energy aggregation and distance preserving property to realize dimensionality reduction without changing the essential characteristics of data. Then after the reduced dimension data is processed by inverse transformation, the dynamic kernel principal component analysis (DKPCA) model is established by combining the autoregressive moving average time series (ARMAX) model and kernel principal component analysis (KPCA) to handle the nonlinearity and dynamics in industrial process. Finally, one penicillin fermentation process case for fault monitoring is provided to test the effectiveness of the proposed method, where the comparison with multiway kernel principal component analysis (MKPCA) results is covered.																	1064-1246	1875-8967					2020	38	1					471	480		10.3233/JIFS-179421													
J								Performance analysis and improvement of large-scale antenna array system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Large-scale antenna array system; massive MIMO; spectral efficiency; interference mitigation	UNCERTAINTY PRINCIPLES; CHANNEL ESTIMATION	The explosive growth of wireless data services, especially more and more requirement for high-definition videos, demands higher capacity of future wireless communication systems to meet this trend. One efficient solution to this problem is to improve the spectral efficiency. This paper analyzes the basic principles of large-scale antenna array system, whose performance is verified by simulation. Key technologies on large-scale antenna array system, such as channel state information acquisition, antenna array design, code book design, are also studied. The results show that large-scale antenna array system can greatly improve spectral efficiency and reduce system energy consumption.																	1064-1246	1875-8967					2020	38	1					481	486		10.3233/JIFS-179422													
J								Internet public informatioan text data mining and intelligence influence analysis for user intent understanding	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Text intention mining; SVM; deep learning	PARAMETER INVERSION PROBLEM	We propose models based on SVM, Naive Bayes and deep learning to solve the consumption intention classification problem. Applying consumption intention mining to prediction tasks in social media. This paper discusses consumption intention towards a certain kind of product, i.e. movie, and uses movie consumption intention as an important feature in box office prediction. We combine consumption intention with traditional features used in the problem of box office prediction, and achieve a outperforms previous work of this problem We build a system based on linear regression which automatically predicts movies' total box office and opening weekend box office one day prior to the movie's release date.																	1064-1246	1875-8967					2020	38	1					487	494		10.3233/JIFS-179423													
J								Knowledge graph analysis and visualization of research trends on driver behavior	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Driver behavior; research trends; driving safety; knowledge graph; visualization	DRIVING BEHAVIORS; ROAD SAFETY; SIMULATION; VEHICLES	In order to trace the research trends and hotspots related to the field of driver behavior research both in China and worldwide, a bibliometric analysis was performed to systematically analyze 7208 scientific papers about driver behavior in the Web of Science database from 1957 to 2018, and the results were discussed from the 6 perspectives of growth trends of publications, geographical distribution, subject categories, institutions, authors and keywords distribution. The results show that research on driver behavior has been increasing rapidly since the 1990 s, but research in Europe and the United States started more than 20 years earlier than in China. Currently, transportation, engineering, computer science and behavioral science have replaced the traditional subject categories, becoming the most important domains in this field. Research on driver behavior focuses on safety and systems using simulation models which is interacted with almost everything concerning traffic operation and safety. The current trend is that more researchers are working on understand the driver-vehicle-system interactions under the environment of connected and autonomous vehicle. Further research is needed to focus on unsafe driving behavior based on driver-vehicle-system interactions. This study will provide scholars in related academia and industry with panoramic knowledge of driver behavior research, as well as research hotspots and future research directions.																	1064-1246	1875-8967					2020	38	1					495	511		10.3233/JIFS-179424													
J								Personalized disease treatment plan suggestion system based on big data and knowledge base	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Disease treatment plan; big data; knowledge base	DIAGNOSIS	Conventional methods for disease treatment plan rely too much on the subjective experience and theoretical knowledge of doctors The uncertain decisions of doctors may lead to wrong treatment plans. This paper provides a treatment suggestion system based on big data and knowledge base. The system no longer relies on the subjective experience of doctors, but relies on the objective historical data of treatment cases including disease information, personal information, treatment plans and the treatment effects of the treatment plans. The suggested treatment plans are much more targeted and reliable, thereby assisting doctors to determine disease treatment plans more quickly, accurately and reasonably.																	1064-1246	1875-8967					2020	38	1					513	521		10.3233/JIFS-179425													
J								Intuitionistic fuzzy sets and other fuzzy sets extensions representable by them	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued intuitionistic fuzzy set; Intuitionistic fuzzy set; Inconsistent intuitionistic fuzzy set; Picture fuzzy set; Neutrosophic fuzzy set		In the present paper we show that Inconsistent intuitionistic fuzzy sets, Picture fuzzy sets and Neutrosophic fuzzy sets are representable by Interval-valued intuitionistic fuzzy sets, which themselves are representable by an ordered pair of the standard Intuitionistic fuzzy sets.																	1064-1246	1875-8967					2020	38	1					525	530		10.3233/JIFS-179426													
J								On weakly prioritized multicriteria decision analysis with interval-valued hesitant fuzzy information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued hesitant fuzzy sets; weakly prioritized relationships; prioritized scoring operator; multicriteria decision analysis	AGGREGATION OPERATORS; PREFERENCE RELATIONS; SETS; RANKING	Hesitant fuzzy sets (HFSs) is a powerful tool in modelling and managing uncertainty of real-world decision-making problems. As a generalization of HFSs, interval-valued hesitant fuzzy sets (IVHFSs) can model preferences of decision makers with several possible interval values. Within the framework of IVHFSs, this work develops a weakly prioritized multicriteria decision analysis method for addressing problems where there exists a weakly ordered prioritization relationship over criteria. The prioritization relationship between criteria is characterized by the interval-valued hesitant fuzzy weights that are associated with criteria dependence on the satisfaction of the higher priority criteria. First, a novel prioritized scoring operator is developed to aggregate interval-valued hesitant fuzzy information coming from simultaneous sources with different priority levels. Then, we present an operator-based decision analysis method to address multicriteria decision making problems with weakly prioritized preferences over criteria. Finally, an illustrative example of global mineral investment is provided to show the application of the proposed method. Comparison analyses with existing prioritized multicriteria decision analysis methods show that the proposed method can effectively characterize weakly ordered prioritization relationship over the criteria as well as that it can avoid information loss in eliciting the final recommendations by introducing uncertain weights.																	1064-1246	1875-8967					2020	38	1					531	543		10.3233/JIFS-179427													
J								Turkish ground handling services firms assesment with neutrosophic multiobjective method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ground handling services; neutrosophic set; fuzzy logic; MULTIMOORA	DECISION-MAKING; TOURISM	Ground Handling Services (GHSs) contain detailed services for airplanes and air passengers when they remain on the airport. In this paper, GHSs received by disabled air passengers and the service provider firms were focused. Neutrosophic MULTIMOORA, a newly developed method, has been used for evaluation. Three Turkish GHSs firms are evaluated by eight criteria. The weights of evaluation criteria are determined by Analytic Hierarchy Process. Then algorithm of Neutrosophic MULTIMOORA method is applied to the problem and sensitivity analysis is presented. In the end, the conclusion is given. Contribution of this paper to the literature is using of the Neutrosophic MULTIMOORA Method firstly for evaluation of GHSs firms.																	1064-1246	1875-8967					2020	38	1					545	552		10.3233/JIFS-179428													
J								Certain convergences for intuitionistic fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy sets; Pointwise convergence; Gamma-convergence; Hausdroff metric; Supremum metric	GAMMA-CONVERGENCE; RESPECT	In this paper, the characterization of Gamma-convergence for the first countable topological spaces, characterization of convergence in supremum metric in general setting and some mutual relation between these convergences are discussed. The Gamma-convergence is defined as the Kuratowaski-Painleve convergence of the endographs of the intuitionistic fuzzy sets. The supremum metric is the supremum of Hausdroff distance among the eta-cuts of the intuitionistic fuzzy sets. To study these convergences is an important part of the theoretical fundamentals for intuitionistic fuzzy set theory. Some results are given as an application to variational analysis.																	1064-1246	1875-8967					2020	38	1					553	564		10.3233/JIFS-179429													
J								A novel integrated MCDM approach: An application for selection of the optimal Fiber optical access network strategy	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										MCDM; uncertainty; COPRAS; IVPFS	PYTHAGOREAN MEMBERSHIP GRADES; GROUP DECISION-MAKING; AGGREGATION OPERATORS; PROGRAMMING METHOD; COPRAS METHOD; FUZZY; NUMBERS	Multiple Criteria Decision Making (MCDM) methods have been widely used in literature with different extensions to handle uncertain information for years. The aim of this study is to propose an improved Complex Proportional Assessment (COPRAS) approach by integrating Interval-Valued Pythagorean Fuzzy Set (IVPFS) which is the extension of interval-valued intuitionistic fuzzy set (IVIFS) to deal with MCDM problems in the uncertain environment. It is evident from the previous studies in the literature, extension version of COPRAS method with IVPFSs is first proposed. In order to illustrate the applicability and accuracy of the proposed approach, a real-world application of selecting the optimal fiber optical access network strategy for Arnavutkoy district in Istanbul is presented. Sensitivity analysis is applied to examine the robustness of the proposed method. A comparative analysis of the proposed method with other existing methods is also conducted.																	1064-1246	1875-8967					2020	38	1					565	575		10.3233/JIFS-179430													
J								A double weighted fuzzy gamma naive bayes classifier	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Gamma statistical distribution; fuzzy classification; fuzzy statistics; double weighted naive bayes	AGREEMENT	Classifiers based on Gamma statistical distribution can be found in the scientific literature, but they assume the collected data doesn't present any errors. However, in some cases, information precision can not be guaranteed, then the fuzzy approach is convenient. Several methods found in the literature are not able to ponder the specific contribution of each class and/or feature for the classification tasks. This paper presents a proposal of a new classifier named Doubled Weighted Fuzzy Gamma Naive Bayes network (DW-FGamNB). This new classifier uses two types of weights in order to allow users to ponder the real contribution of each class and feature in the classification task. The theoretical development is presented, as well as results of its application on simulated multidimensional data using Gamma statistical distribution. A comparison among DW-FGamNB, Fuzzy Gamma Naive Bayes classifier, classical Gamma Naive Bayes classifier, Naive Bayes classifier, DecionTree-Naive Bayes, Decision Tree C4.5, Logistic Regression, Multilayer Perceptron Neural Network, Adaboost-M1, Radial Basis Function Network and Random Forest was performed. The results obtained showed that the DW-FGamNB produced the best performance, according to the Overall Accuracy Index, Kappa and Tau Coefficients, and diagnostic tests.																	1064-1246	1875-8967					2020	38	1					577	588		10.3233/JIFS-179431													
J								A novel approach for classification of occupational health and safety measures based on their effectiveness by using fuzzy Kano model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Safety measures; fuzzy kano model; accident prevention; construction accidents	CONSTRUCTION ACCIDENT CAUSALITY; QUALITY FUNCTION DEPLOYMENT; RISK-ASSESSMENT; CUSTOMER SATISFACTION; PREVENTION; INDUSTRY; MANAGEMENT; INJURIES; SERVICE; SITES	Risk identification and risk assessment are the most important issues among the occupational health and safety practices. In the industry, the identification of hazards and the risk assessment are often carried out by using subjective risk assessment methods. These methods usually do not provide a perspective on the nature of the measures to be taken. In order to keep sector-specific risks under control, as important as the assessment of risks is to take effective measures against these risks and to continuously monitor the effectiveness of these measures. Therefore, the main objective of this paper is to classify protective and preventive occupational health and safety measures implemented in the construction sector based on their efficiency by using Fuzzy Kano Model Approach. For this purpose, it is the first time, the feature classes defined by the conventional Kano Model and the Kano Model questionnaire have been reinterpreted in terms of occupational health and safety. Then, the proposed approach has been applied to classify the safety measures utilized in occupational health and safety in terms of their efficiency by using fuzzy Kano Model. According to obtain results, approximately 10% of the control measures which are effectively used at construction site are in Trusting Measure Class. The main contribution of this paper is to provide a new method for analysis of the effectiveness of occupational health and safety measures.																	1064-1246	1875-8967					2020	38	1					589	600		10.3233/JIFS-179432													
J								Classical, fuzzy, hesitant fuzzy and intuitionistic fuzzy analytic hierarchy processes applied to industrial maintenance management	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Analytic hierarchy process; hesitant fuzzy sets; intuitionistic fuzzy sets; maintenance management	SET	A multi-criteria problem involves the consideration of two or more criteria in the prioritization of alternative solutions. The Analytic Hierarchy Process (AHP) is a leading multi-criteria method. Consistency checking is a great advantage of AHP. Since in AHP priorities come from pairwise comparisons, it is possible to check the consistency of these comparisons. However, a problem occurs when comparisons fail the consistency check. Then, the excluding options are to review some comparisons (Option 1) or to keep the comparisons (Option 2). This paper presents an AHP application in the maintenance management of an industrial plant. Industrial maintenance is not in the core business of an organization. However, maintenance costs can account over 50% of production costs. One of the first maintenance management decisions is on the maintenance strategy. Shall maintenance anticipate the occurrence of failure? Or shall maintenance be performed after an equipment breakdown? Answering those questions with classical AHP resulted in inconsistent comparison matrices. In that case, Fuzzy AHP (FAHP) were applied, avoiding this situation. Therefore, the purpose of this paper is to present the applications of four AHP models: Classical AHP and three models of FAHP, including hesitant fuzzy sets and intuitionistic fuzzy sets. The application of Hesitant FAHP (HFAHP) and Intuitionistic FAHP (IFAHP) are the novelty of this paper. The four AHP models were also applied in the same case of maintenance management of an industrial plant. Results were very similar, but experts could express their preferred model.																	1064-1246	1875-8967					2020	38	1					601	608		10.3233/JIFS-179433													
J								Obtaining interval estimates of nonlinear model parameters based on combined soft computing tools	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval estimates of nonlinear model parameters; fuzzy nonlinear regression; fuzzy alpha-cut; NSGA-II; TOPSIS; FCM	FUZZY REGRESSION; OPTIMIZATION; NETWORK	Obtaining interval estimates of nonlinear model parameters is as important as point estimates of model parameters. Because the estimated value of the parameters cannot always be expressed as a single numerical quantity exactly. In this study, it is aimed to propose an interval estimation procedure for nonlinear model parameters with combining soft computing methods instead of using probabilistic assumptions. For this purpose, response and model parameters were presented as triangular fuzzy numbers (TFNs) in nonlinear regression model. The errors were defined as intervals through alpha-cut operations and minimized according to the least absolute deviation (LAD) metric. The novelty of the study is achieving the minimization in a multi-objective framework in which the objective functions are lower and upper bound of interval type error functions. The NSGA-II (Non-dominated Sorting Genetic Algorithm-II) and the TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) methods were used for multi-objective optimization (MOO) and multi-criteria decision making (MCDM) stages, respectively. Innovatively, in order to obtain reasonable interval estimates, predefined sized compromise solution set was composed and the fuzzy C-means (FCM) clustering algorithm was applied to the compromise set of interval estimates according to the predicted alpha-cut values. The proposed interval estimation approach is applied on a synthetic and a real data sets for application purpose.																	1064-1246	1875-8967					2020	38	1					609	618		10.3233/JIFS-179434													
J								Scientometric inspection of research progression in hesitant fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy logic; hesitant fuzzy sets; HFS; Scientometric analysis	LINGUISTIC TERM SETS; AGGREGATION; OPERATORS; DISTANCE	The concept of Hesitant Fuzzy Sets (HFS) came into picture where a set is created of the possible membership values that are willing to participate for contribution to the fuzzy sets. HFS has recently become very popular between researchers who are working on variants of fuzzy logic. This paper highlights the research queries related to the Scientometric analysis of HFS by studying 410 research publications from the Web of Science (v.5.31) database (from inception of Web of Science online data till 2017). This paper answers questions pertaining to the important terms and concepts for HFS, co-authorship patterns in HFS, dominating research areas of HFS, and countries with maximum research paper contribution, co-citation patterns for first authors and bibliographical coupling for organizations. A brief outline of the citation analysis is also made in the form of a sub-section.																	1064-1246	1875-8967					2020	38	1					619	626		10.3233/JIFS-179435													
J								Investment analysis using neutrosophic present and future worth techniques	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Investment analysis; present worth analysis; future worth analysis; neutrosophic sets; fuzzy sets	FUZZY; ALTERNATIVES; VALUATION; SYSTEMS	Investment analysis is a process of choosing the best alternative among investment alternatives that provide the best fit for a company. The process contains uncertainty, vagueness, fuzziness and insufficient data; therefore, this evaluating process needs experts' knowledge and judgments. Fuzzy set theory is a useful technique to capture experts' evaluations. This paper proposes the new present worth and future worth analysis techniques with single valued neutrosophic set. The proposed techniques allow using possible values of alternatives' data and membership function of alternatives assigned by experts. The techniques provide the literature with a new multiplication operator and a new term under the name "neutrosophic equivalent". An illustrative example shows the applicability of the techniques. Comparison analyses are realized with classical and simplified neutrosophic present and future worth techniques. The comparison results show that the proposed techniques overcome investment analysis problems effectively and efficiently.																	1064-1246	1875-8967					2020	38	1					627	637		10.3233/JIFS-179436													
J								Some new operations on Atanassov's intuitionistic fuzzy sets in decision-making problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy sets; operations; aggregation operator; decision making	GENERALIZED AGGREGATION OPERATORS	The arithmetic operations on intuitionistic fuzzy sets defined by Atanassov are the most popular in the intuitionistic fuzzy set theory. Based on these operations, many commonly used methods for solving the decision-making (DM) problems under intuitionistic fuzzy environment have been developed and various aggregation operators have been proposed. However, there have been revealed some undesirable properties of these operators, such as the inconsistency with the operations on the ordinary fuzzy sets (FSs), the non-monotonicity of the addition and multiplication operations or non-monotonicity under multiplication by a scalar. We show in this paper that these drawbacks pertain to also the recently proposed combined aggregations operators such as intuitionistic fuzzy Heronian mean (IFHM), intuitionistic fuzzy interaction partitioned Bonferroni mean (IFIPBM), intuitionistic fuzzy Dombi Bonferroni mean (IFDBM), intuitionistic fuzzy Maclaurin symmetric mean (IFMSM), Pythagorean fuzzy Maclaurin symmetric mean (PFMSM), q-rung orthopair fuzzy power Maclaurin symmetric mean (q-ROFPMSM), Muirhead mean (IFWMM) and intuitionistic fuzzy hybrid weighted arithmetic and geometric aggregation operators (IFHWAGA).This paper proposes some new arithmetic operations on Atanassov's intuitionistic fuzzy sets that have good algebraic properties, such as idempotency, commutativity, monotonicity and monotonicity under multiplication by a scalar. Based on the proposed operations, the intuitionistic fuzzy weighted arithmetic mean and intuitionistic fuzzy weighted geometric mean operators with the acceptable properties are developed. Some illustrative examples are performed to demonstrate effectiveness and reliability of our method. Finally, in order to verify the validity of the proposed method in solving real-life DM problems, an application example is conducted with a comparative analysis with other existing methods.																	1064-1246	1875-8967					2020	38	1					639	651		10.3233/JIFS-179437													
J								A novel IFCM integrated distance based hierarchical intuitionistic decision making procedure for agile supplier selection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy sets; intuitionistic fuzzy cognitive map; hierarchical decision making; agile supplier selection	PARTNER SELECTION; FUZZY; MODEL; CRITERIA	Companies should cope with uncertain and unpredictable changes while improving responsiveness capability to survive in dynamic market conditions. In recent years, agility concept becomes more and more popular to meet these requirements in management as well as manufacturing. From supply chain viewpoint, collaborating with agile suppliers is principal for constructing an agile supply chain, which is flexible, quick, and responsive. This paper aims to identify the most suitable agile supplier alternative by introducing a novel distance based hierarchical intuitionistic decision making procedure. The proposed methodology, which deals with uncertain and vague data, allows to represent hesitation by using intuitionistic fuzzy numbers rather than fuzzy numbers that fail to take into account hesitancy. The causal links among evaluation criteria are expressed with intuitionistic fuzzy cognitive map tool that enables to yield the importance degree of each criterion. Moreover, the hierarchical representation of the decision making problem provides an effective analysis with the presence of numerous criteria that are to be considered. The distance based framework is to minimize the distance to the ideal solution while maximizing the distance from the anti-ideal solution in order to determine the most appropriate agile supplier alternative. The case study is conducted in a dye manufacturer that performs in Turkish dye industry.																	1064-1246	1875-8967					2020	38	1					653	662		10.3233/JIFS-179438													
J								TODIM approach based on score function under hesitant 2-tuple linguistic environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hesitant 2-tuple linguistic set; score function; generalized distance measure; multi-criteria group decision making; TODIM method	GROUP DECISION-MAKING; AGGREGATION OPERATORS; INTUITIONISTIC FUZZY; REPRESENTATION MODEL; PROSPECT-THEORY; TERM SETS; DEAL; DISTANCE; VALUES	The hesitant 2-tuple linguistic set (H2TLS) as an important extension of the 2-tuple linguistic model, can effectively express the judgments of the decision makers (DMs) not only in qualitative aspects but also reflect the vagueness and hesitancy by assigning more than one translation parameters to every linguistic variable of the linguistic term set (LTS). The aim of this study is to extend the TODIM (an acronym in Portuguese of interactive and multi-criteria decision making) method, to solve multi-criteria group decision making (MCGDM) problems in the context of H2TLSs with completely unknown criteria weights. The TODIM method is developed on the basis of prospect theory which can effectively capture the psychological behavior of DMs during the decision analysis. In order to enhance the suitability and applicability of H2TLSs, this paper investigates first the generalized distance measure between hesitant 2-tuple linguistic elements (H2TLEs). Furthermore, a score function for H2TLEs is proposed and the dominance relations are defined by using this function. A TODIM method is established that can greatly help in solving MCGDM problems in which alternatives are assessed in the form of H2TLEs in the presence of certain criteria. A procedure for determining the criteria weights is also established as a follow up. Finally, a numerical example is offered and a comparison analysis of proposed extended TODIM method is made with other methods to check the validity and practicality of the proposed study.																	1064-1246	1875-8967					2020	38	1					663	673		10.3233/JIFS-179439													
J								Segmentation of indoor customer paths using intuitionistic fuzzy clustering: Process mining visualization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy c-means clustering; intuitionistic fuzzy sets; process mining; customer behaviors; indoor locations	BLUETOOTH TRACKING; BEHAVIOR; TRAJECTORIES; ALGORITHM	There are some studies and methods in the literature to understand customer needs and behaviors from the path. However, path analysis has a complex structure because the many customers can follow many different paths. Therefore, clustering methods facilitate the analysis of the customer location data to evaluate customer behaviors. Therefore, we aim to understand customer behavior by clustering their paths. We use an intuitionistic fuzzy c-means clustering (IFCM) algorithm for two-dimensional indoor customer data; case durations and the number of visited locations. Customer location data was collected by Bluetooth-based technology devices from one of the major shopping malls in Istanbul. Firstly, we create customer paths from customer location data by using process mining that is a technique that can be used to increase the understandability of the IFCM results. Moreover, we show with this study that fuzzy methods and process mining technique can be used together to analyze customer paths and gives more understandable results. We also present behavioral changes of some customers who have a different visit by inspecting their clustered paths.																	1064-1246	1875-8967					2020	38	1					675	684		10.3233/JIFS-179440													
J								Robustness analysis of the interval-valued fuzzy inference algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued fuzzy sets; distance metric; fuzzy metric spaces; triple I method; robustness	DISTANCE MEASURE; SETS; ENTROPY	A new distance metric between interval-valued fuzzy sets is proposed. The four special logic metric spaces based on the distance are structured. we analysis and compare the structures of the four logic metric spaces. It is shown that the Lukasiewicz logic metric space and Goguen logic metric space are more suitable for interval-valued fuzzy reasoning. Moreover, the robustness of interval-valued fuzzy reasoning triple I methods are studied in the two logic metric spaces. We prove that fuzzy reasoning triple I methods based on the interval-valued Lukasiewicz residuated implication and interval-valued Goguen residuated implication are robust.																	1064-1246	1875-8967					2020	38	1					685	696		10.3233/JIFS-179441													
J								Algorithmic pairs trading with expert inputs, a fuzzy statistical arbitrage framework	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										pairs trading; algorithmic trading; fuzzy statistical arbitrage		Pairs trading is a widespread market-neutral trading strategy aiming to utilize the relationship between pairs of financial instruments in efficient markets, where predictability of separate asset movements is theoretically not possible. The implication of trading pairs, following statistical analysis, is to buy the underpriced asset while short selling the overpriced. The predicted price relationship is determined through analysis of historical spread data between the members of the corresponding pair. The investor expects the price difference, in an efficient market, should converge and stocks return to their 'fair value', where the positions are closed and profit is realized. The main focus of this study is the contribution of the fuzzy engine to the existing pairs trading strategy. Widespread classical 'crisp' technique is chosen, utilized and compared with the developed 'fuzzy' model throughout the paper. In order to further improve this contribution, the expert opinions extracted from the Bloomberg database are also integrated into the fuzzy decision-making process. In most studies, transaction costs are simply ignored. As a final robustness check, the transaction costs are also considered. The improvement reached by the developed fuzzy technique is observed to be even more remarkable in this case.																	1064-1246	1875-8967					2020	38	1					697	707		10.3233/JIFS-179442													
J								Fuzzy cloud-fog computing approach application for human activity recognition in smart homes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy fog computing; fuzzy cloud computing; smart devices; UWB technology; activity recognition; protoforms; fuzzy temporal windows; fuzzy aggregation	SENSORS; INTERNET; THINGS; IOT	Fog Computing is an approach involving smart devices. These devices carry out data processing to provide collaborative services to reach a common goal, usually, in the cloud. In the fog computing paradigm, uncertainty and vagueness are inherent to the data processing due to the limitations of computational and communication capabilities of the smart devices. Fuzzy logic and protoforms represent a powerful tool to model and compute imprecise data presented within the fog-computing paradigm. In this paper, we present a fuzzy cloud-fog approach based on fuzzy temporal windows and fuzzy aggregation. The innovations of this paper are: i) to model the uncertainty involved in fog nodes linguistically, ii) to compute and distribute relevant linguistic information (protoforms), and iii) to publish the computed protoforms in the cloud to generate complex protoforms, which reach the common goal. This new fuzzy cloud-fog approach is applied to the problem of activity recognition in smart homes. In this context, the smart devices in a smart home are represented by fog nodes, which cooperate for activity recognition using a fuzzy cloud-fog computing approach to provide solutions in ambient-assisted living. Finally, to demonstrate the effectiveness of the proposal in handling situations/environments where multiple and heterogeneous devices are involved (such as UWB beacons, smart objects and smart wearable devices), a case study of the proposed fuzzy cloud-fog approach is implemented in the smart lab of the University of Jaen. So, the results obtained with the proposed approach in the case study are compared to the results obtained with a non-fuzzy approach with the aim of showing the advantages of the fuzzy methodology.																	1064-1246	1875-8967					2020	38	1					709	721		10.3233/JIFS-179443													
J								Smart container evaluation by neutrosophic MCDM method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Industry 4.0; smart container; multi criteria decision making; neutrosophic TOPSIS		The Industrial Revolution, which started with steam machines, has evolved into intelligent systems in which objects speak to each other in the name of Industry 4.0. Smart containers (SC), which are the latest in this evolution of containers and one of the key elements of logistic in Supply Chain Management System (SCMS), stand out with their flexibility, traceability, and contribution to the optimization of the supply chain. In this paper the existing properties of the currently evolving smart containers are compiled and the users' needs that will guide future designs are determined. In the application section, three different smart containers are evaluated according to seven different conflicting criteria in neutrosophic environment. By using neutrosophic TOPSIS method relative closeness coefficient of alternatives are calculated. Finally alternatives are ranked in descending order. The originality of the paper is that smart containers evaluation problem is handled by the neutrosophic MCDM method for the first time in the literature.																	1064-1246	1875-8967					2020	38	1					723	733		10.3233/JIFS-179444													
J								A consensus reaching process dealing with comparative linguistic expressions for group decision making: A fuzzy approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										group decision making; comparative linguistic expressions; hesitant fuzzy linguistic term sets; consensus reaching process	PREFERENCE RELATIONS; MODEL; SETS; SYSTEMS; TODIM; SCALE	Group Decision Making (GDM) deals with decision problems in which multiple experts, with their own attitudes and knowledge, evaluate different alternatives or solutions with the aim of achieving a common solution. In such cases disagreements can appear, which might led to failed solutions. To manage such conflicts, Consensus Reaching Processes (CRPs) have been added to the GDM solving process. GDM problems under uncertainty often model uncertainty by linguistic descriptors, being most of linguistic based CRPs based on the use of single linguistic terms for modelling experts' opinions, which cannot be expressive enough in some situations because of either the uncertainty involved or the experts' hesitancy. Therefore, this paper aims to fill this gap by proposing a novel consensus model dealing with GDM problems in which experts' preferences are elicited by means of Comparative Linguistic Expressions (CLEs) based on Hesitant Fuzzy Linguistic Term Sets, which allow to model the experts' hesitancy in a flexible way. Furthermore, CLEs are modelled by fuzzy membership functions in order to keep the fuzzy representation in the whole CRP and preserve as much information as possible. Additionally, the proposed model is implemented and integrated in an intelligent CRP support system, so-called AFRYCA 3.0 to carry out a case study about this new CRP and compare it with previous models.																	1064-1246	1875-8967					2020	38	1					735	748		10.3233/JIFS-179445													
J								Single & interval-valued neutrosophic AHP methods: Performance analysis of outsourcing law firms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Single-valued neutrosophic sets; interval-valued neutrosophic sets; AHP; outsourcing; law firm; MCDM	ANALYTIC HIERARCHY PROCESS; DECISION-MAKING; FUZZY AHP; SETS; HEALTH	Neutrosophic sets are an extension of intuitionistic fuzzy sets, providing a new approach to uncertainty with their three components: Truth, indeterminacy, and falsity. These parameters can be assigned independently which makes their sum equal to at most three. Neutrosophic sets have been extensively employed in the new extensions of multicriteria decision making methods (MCDM) in the literature. Law firms who are business entities formed by one or more lawyers are associations of lawyers who practice law. We propose a neutrosophic analytic hierarchy process (NAHP) for comparing the performances of these law firms in this paper. The performance of law offices is comparatively measured by the proposed NAHP. Linguistic assessments are used in this process rather than exact numerical evaluations. The illustrative problem hierarchy includes four criteria and four alternatives are given.																	1064-1246	1875-8967					2020	38	1					749	759		10.3233/JIFS-179446													
J								Pythagorean fuzzy (R, S)-norm discriminant measure in various decision making processes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean fuzzy set; discriminant measure; monotonicity; pattern recognition; medical diagnosis; multiple criteria decision making	SIMILARITY MEASURES; DIVERGENCE MEASURE; DISTANCE MEASURE; SETS; INFORMATION; DISSIMILARITY	In the present communication, a new (R, S)-norm discriminant measure of Pythagorean fuzzy sets has been proposed along with its various properties. Monotonic behavior with respect to the parameters R & S and their proof of validity have also been studied. Methodologies and necessary steps of the algorithms for various decision-making problems viz. pattern recognition problem, medical diagnosis problem and multi criteria decision-making problem have been outlined on the basis of the proposed information measure. For the sake of illustration, numerical example for each case has been provided. A comparative analysis for the considered applications has also been studied in contrast with the popular existing methodologies stating important observations and advantages.																	1064-1246	1875-8967					2020	38	1					761	777		10.3233/JIFS-179447													
J								A TOPSIS method by using generalized trapezoidal hesitant fuzzy numbers and application to a robot selection problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hesitant fuzzy set; generalized trapezoidal hesitant fuzzy number; distance measures; TOPSIS method; multiple attribute decision making; robot selection	MULTIATTRIBUTE DECISION-MAKING; AGGREGATION OPERATORS; INFORMATION AGGREGATION; SETS; HEIGHTS; MODEL	Selecting an appropriate robot among the alternative robots is a difficult problem for decision makers, since it is complicated to express attributes as crisp numbers in multiple attribute decision making problems. Generalized trapezoidal hesitant fuzzy numbers(GTHF-numbers) can be efficiently used to estimate information in the decision making process. This paper proposes an advanced type of TOPSIS (Technique for order preference by similarity to ideal solution) method, called TOPSIS method of GTHF-numbers. To do this, we introduce some novel distance measures including Hamming distance measure, Euclidean distance measure, lambda-generalized distance measure, lambda-generalized Hausdorff distance measure, gamma-hybrid Hamming distance, hybrid Euclidean distance and lambda-generalized hybrid distance measure on GTHF-numbers. Then, we develop a novel TOPSIS method of GTHF-numbers based on introduced distance measures. Finally, we provide a real example, for an auto company which desires to select a suitable robot for its production process, based on the proposed TOPSIS method of GTHF-numbers to prove the efficiency and the applicability of the proposed method.																	1064-1246	1875-8967					2020	38	1					779	793		10.3233/JIFS-179448													
J								The topological properties of intuitionistic fuzzy rough sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy rough sets; intuitionistic fuzzy topologies; intuition fuzzy logic; lattices	REPRESENTATION	In this paper, an in depth study is done on topological properties of intuitionistic fuzzy rough sets in light of different conditions like serial, strongly serial, left continuity, transitivity on intuitionistic fuzzy relations, t-norms, implicators by adopting a axiomatic approach with the ingredients of intuitionistic fuzzy logic. Numerous intuitionistic fuzzy topologies based on many different kinds of intuitionistic fuzzy relations are explored. Also, a special class of intuitionistic fuzzy relations known as T-similarity class has been studied algebraically and found interesting lattices to model real life problems for better applications of intuitionistic fuzzy rough sets.																	1064-1246	1875-8967					2020	38	1					795	807		10.3233/JIFS-179449													
J								Social open innovation platform design for science teaching by using pythagorean fuzzy analytic hierarchy process	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										System design; fuzzy sets; pythagorean fuzzy sets; analytic hierarchy process	GROUP DECISION-MAKING; FLEXIBILITY; EXTENSION; SETS	The education system is very important for the society. In this study, we try to define and prioritize the requirements of an n online platform which can be used as a source for collecting feedback from stakeholders of an educating system. The involved stakeholders are the course content generators, teachers, students and families of the students. In the application three experts used linguistic terms to evaluate the performance indicators and the weights are calculated using Pythagorean Fuzzy Analytic Hierarch Process (AHP) Method.																	1064-1246	1875-8967					2020	38	1					809	819		10.3233/JIFS-179450													
J								Intuitionistic fuzzy control of twin rotor multiple input multiple output systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy and intuitionistic fuzzy set; intuitionistic fuzzy control; twin rotor MIMO	OPERATORS	This paper proposes an intuitionistic fuzzy control method for twin rotor multi-input and multi-output (twin rotor MIMO) systems. Twin rotor MIMO systems are often used to measure the performance of control systems as they are extremely sensitive to environmental factors. The use of the intuitionistic fuzzy control method for modeling these uncertainties offers an effective way to increase the robustness of the control system to uncertainties in the structure of twin rotor MIMO systems. In this study, two intuitionistic fuzzy controllers are designed, namely for the main and tail rotors separately and then combine the outputs of these rotors. Also, this method is compared with the classical optimal PID method in terms of stability and performance by various simulations and experiments.																	1064-1246	1875-8967					2020	38	1					821	833		10.3233/JIFS-179451													
J								Multi-expert disaster risk management & response capabilities assessment using interval-valued intuitionistic fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Disaster management; risk management; interval-valued intuitionistic fuzzy sets; MCDM; TOPSIS	MULTICRITERIA DECISION-MAKING; PERFORMANCE; AHP	This study focuses on the evaluation of disaster risk management and response (DRMR) processes and capabilities using a multi-expert multi-criteria decision making (MCDM) framework. The proposed framework considers four sets of evaluation and performance criteria: risk knowledge and organization, risk reduction, disaster response management, and disaster response support; and 22 sub-criteria such as regulating risk management, financial management, energy, and public safety. To contend with random perception and utility, lack of information and subjectivity in the human (expert) judgment processes that could be present in expert-based models, the authors propose an interval-valued intuitionistic fuzzy sets (IVIFSs) approach. IVIFSs can handle high levels of uncertainty and define appropriate membership functions. Specifically, the proposed approach incorporates score judgement and possibility degree matrices, and estimates the local and global weights for each assessment criteria. And finally, evaluates the overall performances of the alternatives using intuitionistic fuzzy Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) approach. The authors implemented the framework to the Atlantico State of Colombia, and assessed the disaster risk management and response processes for each for the State's 23 municipalities. The authors discuss sensitivity analysis that illustrates the robustness of the results.																	1064-1246	1875-8967					2020	38	1					835	852		10.3233/JIFS-179452													
J								Neutrosophic alpha psi-connectedness	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neutrosophic closed set; Neutrosophic alpha psi-closed set; Neutrosophic alpha psi-connectedness between neutrosophic sets		Neutrosophic topological space is an extension of intuitionistic topological space and each triplet set in neutrosophic topological space contains membership, indeterminacy and non-membership values. Connected set in intuitionistic topological set contains membership and non-membership values and inderterminacy have not discussed in that set. This motivates the authors to propose this novel concept called neutrosophic alpha psi-connectedness. So we introduce the new notion of neutrosophic alpha psi-connectedness in neutrosophic topological spaces and investigate some properties of neutrosophic alpha psi-connectedness between sets and subsets of two sets. Some properties of this concept presented with numerical quantities to prove the non-existence.																	1064-1246	1875-8967					2020	38	1					853	857		10.3233/JIFS-179453													
J								A combined group decision making based IFCM and SERVQUAL approach for strategic analysis of airline service quality	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Airline service quality; group decision making; intuitionistic fuzzy cognitive map; scenario analysis; SERVQUAL	CUSTOMER SATISFACTION; BEHAVIORAL INTENTIONS; COGNITIVE MAPS; MODEL; PERCEPTIONS; AHP; EXPECTATIONS; SELECTION; CRITERIA; LOYALTY	Transportation becomes increasingly important for global trade, mobility, and economies. Passengers expect speed and quality from transportation services. As a result, the demand for air transportation is steadily rising. Higher demand for flights brings along new expectations and competition. In particular, airline companies are focused on service quality, knowing that it is a necessity to survive in a competitive market by meeting customer expectations. Hence, it is aimed that to analyze Airline Service Quality (ASQ) in Turkey. A SERVQUAL based model is developed and the criteria of the model is analyzed using group decision making (GDM) based intuitionistic fuzzy cognitive map (IFCM) approach. A real case is conducted in Turkey airline industry. By considering the hesitation, uncertainty and intuition of decision-making processes and the opinions of the decision makers, classical FCM approach is expanded to IF environment. Decision makers who are experts in Turkey airline industry, evaluate the relationship between the criteria. The importance ratings of the criteria are determined. Thanks to dynamic structure of IFCM, scenario analysis is performed to make strategic decisions. Originality of the paper is that it is the first study dealing with GDM based IFCM and SERVQUAL approach for ASQ in Turkey to the best of our knowledge.																	1064-1246	1875-8967					2020	38	1					859	872		10.3233/JIFS-179454													
J								Application of interval valued intuitionistic fuzzy TOPSIS for flood management	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										TOPSIS; interval valued intuitionistic fuzzy set; linguistic evaluation; decision making; flood management		The technique for order preference by similarity to ideal solution (TOPSIS) has been applied to numerous multi-criteria decision making (MCDM) problems where crisp numbers are utilised in defining linguistic evaluation. The interval valued intuitionistic fuzzy TOPSIS (IVIF TOPSIS) can offer a new decision making method in solving MCDM problems where interval valued intuitionistic fuzzy sets are utilised in defining linguistic terms. Differently from the TOPSIS, which directly utilised crisp numbers, this method introduces upper and lower intervals of memberships to capture wide arrays of uncertain and fuzzy information. In this paper, criteria and alternatives in flood management is investigated where the best alternative in flood mitigation approach can be identified using the IVIF TOPSIS. Four decision makers in flood management were invited to provide linguistic evaluation of seven alternatives with respect to seven criteria. Computational results indicate that the alternative 'pumping station' is identified as the best alternative of flood mitigation project. The findings of this study would benefit authority in suggesting the effective approach in flood mitigation initiatives.																	1064-1246	1875-8967					2020	38	1					873	881		10.3233/JIFS-179455													
J								Evaluation of legal debt collection services by using Hesitant Pythagorean (Intuitionistic Type 2) fuzzy AHP	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean fuzzy sets; Legal debt collection; Pythagorean AHP; Intuitionistic Type 2	CRITERIA DECISION-MAKING; PERFORMANCE EVALUATION; INSTITUTIONS	Managing the collection of unpaid debts is crucial for the financial survival of the companies. The long term unpaid debts are collected through legal debt collection processes. This legal process should be carried out by qualified lawyers. The companies with many subscribers usually work with legal debt collection offices outside the company rather than allocate internal resources for the management of this process. Evaluating the performances of legal debt collection offices and appropriate distribution of the relevant debtor files to different legal debt collection offices located in different regions are very important for optimizing the debt collection. One of the biggest GSM operators in Turkey that has millions of customers wants to enhance its legal debt collection process. Due to the high number of customer, the GSM operator works approximately one hundred legal debt collection offices which makes evaluation complex. This complex evaluation process should be objective, transparent, and represent the company vision and strategy. The legal debt collection offices should not only increase the total amount of collected debts but also avoid creating compliance problems and customer dissatisfaction. The evaluation of legal debt collection offices should involve both of these objective and subjective criteria. Yet, the evaluations involve hesitancy and vagueness. In this study, we use hesitant Pythagorean fuzzy sets for evaluating the performances of the legal debt collection offices and apply it the real data.																	1064-1246	1875-8967					2020	38	1					883	894		10.3233/JIFS-179456													
J								Approach for group decision making based on linguistic truth-valued intuitionistic fuzzy lattice	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Group decision making; linguistic truth-valued intuitionistic fuzzy lattice; linguistic truth-valued aggregation operator	OPERATORS; SETS	The group decision-making is a process that multiple experts participate in analysis and decision-making for multiple attributes, which can assist decision makers to set priorities and make the best decision. The linguistic truth-valued intuitionistic fuzzy lattice can better express both comparable and incomparable linguistic information, which can also better to deal with positive and negative linguistic information at the same time. To deal with the decision-making problem with fuzzy linguistic information, we propose an approach for group decision making based on linguistic truth-valued intuitionistic fuzzy lattice. For the comparable fuzzy linguistic information, the linguistic truth-valued intuitionistic fuzzy weighted averaging operator and the linguistic truth-valued intuitionistic fuzzy ordered weighted averaging operator are presented to aggregate evaluation information of multiple experts. For the incomparable fuzzy linguistic information, positive reference nearness degree and negative reference nearness degree are introduced to deal with incomparable result with different preferences. We discuss an algorithm of group decision making, in which the decision results are alternative according to the decision makers' preferences. A practical example is provided to illustrate the validity and rationality of the developed approach.																	1064-1246	1875-8967					2020	38	1					895	904		10.3233/JIFS-179457													
J								An intelligent approach for the evaluation of innovation projects	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Innovation project selection; fuzzy; multiple-criteria decision making; AHP; VIKOR	RESEARCH-AND-DEVELOPMENT; EXTENDED VIKOR METHOD; FUZZY AHP; DECISION-MAKING; PORTFOLIO MANAGEMENT; MCDM METHODS; SELECTION; SUSTAINABILITY; INDUSTRY	In this study, an intelligent approach is presented for the evaluation and selection of innovation projects. Selecting the best innovation project is a complicated multiple criteria decision making (MCDM) problem with several potentially competing quantitative and qualitative criteria. In this paper, two hesitant fuzzy MCDM methods; hesitant fuzzy Analytic Hierarchy Process (hesitant F-AHP) and hesitant fuzzy VIsekriterijumska optimizacija i KOmpromisno Resenje (hesitant F-VIKOR) are integrated to evaluate and rank innovation projects. In the hesitant fuzzy AHP-VIKOR, hesitant F-AHP is used to find fuzzy evaluation criteria weights and hesitant F-VIKOR is implemented to rank innovation project alternatives. A numerical example is given where five innovation projects are evaluated based on nine criteria by three decision makers.																	1064-1246	1875-8967					2020	38	1					905	915		10.3233/JIFS-179458													
J								SMOTEFRIS-INFFC: Handling the challenge of borderline and noisy examples in imbalanced learning for software defect prediction	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Software defect prediction; data sampling; fuzzy rough set; noise filtering	SAMPLING METHOD; CLASSIFICATION; FRAMEWORK; SETS	The object of Software Defect Prediction (SDP) is to identify modules that are prone to defect. This is achieved by training prediction models with datasets obtained by mining software historical depositories. When one acquires data through this approach, it often includes class imbalance which has an unequal class representation among their example. We hypothesize that the imbalance learning is not a problem in itself and decrease in performance is also influenced by other factors related to class distribution in the data. One of these is the existence of noisy and borderline examples. Thus, the objective of our research is to propose a novel preprocessing method using Synthetic Minority Over-Sampling Technique (SMOTE), Fuzzy-rough Instance Selection type II (FRIS-II) and Iterative Noise Filter based on the Fusion of Classifiers (INFFC) which can overcome these problems. The experimental results show that the new proposal significantly outperformed all the methods compared in this study.																	1064-1246	1875-8967					2020	38	1					917	933		10.3233/JIFS-179459													
J								A multi-criteria evaluation model based on hesitant fuzzy sets for blockchain technology in supply chain management	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Blockchain technology; supply chain management; hesitant fuzzy sets; AHP; TOPSIS	QUALITY FUNCTION DEPLOYMENT; LINGUISTIC TERM SETS; QUALIFLEX APPROACH; ADOPTION	Nowadays, more companies are trying to implement blockchain technology (BT) that enables to increase the quality of the products/services to their supply chains in order to improve their performance. BT can be applied to different sectors according to their specific needs. Evaluation of BT with respect to sectors needs considering several factors and it can be considered as a multi criteria decision making (MCDM) problem. In this paper, the appropriateness of BT in Supply Chain Management (SCM) according to different sectors has been evaluated by using a MCDM methodology based on hesitant fuzzy sets (HFSs). The suggested MCDM methodology consists of Delphi method, hesitant fuzzy Analytic Hierarchy Process (HF-AHP) and Hesitant Fuzzy Technique for Order Preference by Similarity to Ideal Solution (HF-TOPSIS) methods. In the first stage, the criteria and sub-criteria utilized for performance evaluation of BT in supply chain management have been determined by using Delphi method. The weights of main and sub-criteria have been obtained through HF-AHP method and finally, the alternative sectors have been ranked according to results of HF-TOPSIS method. For this aim, a hierarchical MCDM problem that consists of 5 main and 17 sub-criteria has been created and the alternative sectors have been evaluated. As a result, medicine/drug and jewelry sectors have been respectively determined as the most and the least suitable alternatives in order to implement BT by means of the proposed HFSs based methodology. Finally, a sensitivity analysis has been conducted to show the importance of the main criteria weights on ranking of alternatives.																	1064-1246	1875-8967					2020	38	1					935	946		10.3233/JIFS-179460													
J								Advanced fuzzy-based leak detection and size estimation for pipelines	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pipeline; T-S fuzzy algorithm; sliding mode technique; ARX-Laguerre system estimation; PI observer; leak detection; leak estimation; leak size classification; partial differential equation; backstepping algorithm	FAULT-DETECTION; SYSTEMS; ACTUATOR; SENSOR; LOCALIZATION; DIAGNOSIS; MODEL	The design of an effective procedure for leak detection, estimation, and leak size classification is necessary to maintain the healthy and safe operations of pipelines for conveying fluids and gas from one place to another. The complexities of nonlinear and uncertain behavior inherent in a pipeline lead to difficulty of detection, estimation, and leak size estimation. Hence, a robust hybrid leak detection and size estimation method based on the back stepping hyperbolic Takagi-Sugeno (T-S) fuzzy sliding mode extended ARX-Laguerre Proportional Integral (PI) observer for pipelines is presented. Because of the effects of gases and fluids in pipelines, accurate physical modeling of a pipeline is difficult. Consequently, the ARX-Laguerre technique is used for pipeline modeling in this study. Early detection of leaks is important to avoid product loss and other severe damage. To address this issue, the extended ARX-Laguerre PI observer is utilized to detect and estimate a leak. In addition, a T-S fuzzy technique is applied to an extended ARX-Laguerre PI observer to improve leak estimation in the presence of uncertainties. Thus, the T-S fuzzy sliding mode extended ARX-Laguerre PI observer adaptively improves the reliability, robustness, and estimation accuracy of leak detection and estimation. To leak size classification in the presence of uncertainties, the hyperbolic differential equations are governed by the T-S fuzzy extended ARX-Laguerre PI observer to find the exact solution for the kernels of a backstepping-based leak boundary. The leak estimation convergence error shows that the leak size estimation can be calculated independent of the location of the leak, which is the main contribution of this research. It is assumed that pressure and flowmeter sensors are available at the inlet and outlet of the pipeline. The effectiveness of the proposed robust backstepping hyperbolic T-S fuzzy sliding mode extended ARX-Laguerre PI observer was tested over an experimental dataset. According to the results, the proposed technique improved the leak detection, estimation, and size estimation.																	1064-1246	1875-8967					2020	38	1					947	961		10.3233/JIFS-179461													
J								A spherical fuzzy extension of MULTIMOORA method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Spherical fuzzy sets; multicriteria decision making; MULTIMOORA; personnel selection; TOPSIS	GROUP DECISION-MAKING; ROUGH SET MODELS; AGGREGATION OPERATORS; EXTENDED MULTIMOORA; SELECTION; NUMBERS; TOPSIS; RISK; (I	The three dimensional extensions of ordinary fuzzy sets such as intuitionistic fuzzy sets (IFS), Pythagorean fuzzy sets (PFS), and neutrosophic sets (NS) aim at collecting experts' judgments based on membership, non-membership and hesitancy degrees. Generalized three-dimensional spherical fuzzy sets have been introduced to the literature, including their arithmetic operations, aggregation operators, and defuzzfication operations. Expansion of classical Multi-Objective Optimization by a Ratio Analysis plus the Full Multiplicative Form (MULTIMOORA) has been performed by using ordinary fuzzy, hesitant fuzzy, intuitionistic fuzzy, and neutrosophic sets in the literature. I aim at developing the spherical fuzzy MULTIMOORA method since the spherical fuzzy point of view can contribute to this multicriteria decision environment. By using spherical fuzzy sets (SFS), the MULTIMOORA method can be more efficient for solving complex problems, which require evaluation and estimation under unreliable data environment. The validation of the proposed approach is shown through an illustrative example. Additionally, comparative analyses with neutrosophic MULTIMOORA and intuitionistic fuzzy TOPSIS methods are presented.																	1064-1246	1875-8967					2020	38	1					963	978		10.3233/JIFS-179462													
J								A general approach to fuzzy TOPSIS based on the concept of fuzzy multicriteria acceptability analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy number; fuzzy preference relation; ranking of fuzzy numbers; MCDA; TOPSIS; fuzzy TOPSIS; FMAA; overestimation	GROUP DECISION-MAKING; SUPPLIER SELECTION; REASONABLE PROPERTIES; MODEL; AHP; MANAGEMENT; EXTENSION; ENVIRONMENT; DESIGN	Within Multi-Criteria Decision Analysis (MCDA), the TOPSIS method and its fuzzy extensions, fuzzy TOPSIS (FTOPSIS) models, are widespread ones for solving multi-criteria decision problems. At the same time, FTOPSIS models, as a rule, are implemented based on approximate computations with the use of triangular and trapezoidal fuzzy numbers. This paper introduces a novel approach to fuzzy extension of TOPSIS with the use of fuzzy criteria values and fuzzy weight coefficients of the general type and implementing functions of fuzzy numbers based on standard fuzzy arithmetic and transformation methods. Within FTOPSIS, for ranking of fuzzy numbers/alternatives the concept of Fuzzy Multi-criteria Acceptability Analysis (FMAA) is implemented. The use of FMAA within Fuzzy MCDA(FMCDA) represents a systematical implementation of the concept of fuzzy decision analysis that "the decision taken in the fuzzy environment must be inherently fuzzy". FTOPSIS-FMAA model not only allows ranking the set of alternatives, but also provides the confidence measure for the rank obtained by this model. This approach also considers the overestimation problem, which arises within FMCDA and FTOPSIS-FMAA implementation. A case study on a multi-criteria housing development decision problem is introduced and explored by several FTOPSIS-FMAA models. Finally, a comparison of different FTOPSIS-FMAA models is implemented with the use of Monte Carlo simulation.																	1064-1246	1875-8967					2020	38	1					979	995		10.3233/JIFS-179463													
J								Wind turbine evaluation using the hesitant fuzzy AHP-TOPSIS method with a case in Turkey	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wind turbine; hesitant fuzzy sets; hesitant fuzzy analytic hierarchy process; hesitant fuzzy TOPSIS; multi criteria decision making	MULTICRITERIA DECISION-MAKING; MODEL; SELECTION	As energy security concerns push the countries to find more sustainable and renewable sources of energy, wind power became one of the fastest growing renewable energy source. As only a handful of wind turbine manufacturers established foothold in world markets, it is essential for managers to make right decisions regarding which wind turbines they will install in any given project. This study explores the literature on the wind turbine selection, solicits opinions of the industry experts to come up with a more realistic set of criteria and develops a decision making tool integrating hesitant fuzzy Analytic Hierarchy Process (AHP) with Technique-for-Order-Preference-by-Similarity-to-Ideal-Solution (TOPSIS). As wind turbine selection problem includes both quantitative and qualitative criteria, it is difficult to tackle with high uncertainty by using traditional techniques. Thus, hesitant fuzzy sets (HFS) which is an evolved fuzzy tool that deals with vagueness is utilized. In this study, hesitant fuzzy AHP is utilized to overcome the ambiguity, which occurs during criteria prioritization. In order to rank the alternatives, hesitant fuzzy TOPSIS is applied. By the help of this integrated approach, evaluation process becomes systematic and easy to deal with vagueness. The proposed method is demonstrated by a case study in Turkey.																	1064-1246	1875-8967					2020	38	1					997	1011		10.3233/JIFS-179464													
J								A novel hybrid approach based on intuitionistic fuzzy multi criteria group-decision making for environmental pollution problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy sets; multi criteria group decision making; PROMETHEE; SAW	PROCESS CAPABILITY INDEXES; RISK-ASSESSMENT; PROMETHEE	Due to the industrial emissions, poor disposal of wastes, mining, deforestation, increased use of fossil fuels and vicious agricultural activities, environmental pollution rapidly arises and becomes one of the most serious problems of the contemporary world. Humanity tries to abstain from living in polluted cities because of its effects adversely to quality of life. As the utilization of Multi-Criteria Decision Making (MCDM) techniques plays a key role in choosing the most suitable option between all feasible alternatives, this work proposes a hybrid MCDM method to rank the major cities from the least polluted to the most polluted according to the types of pollution. Owing to the capability to tackle imprecise and uncertain decision information, intuitionistic fuzzy (IF) sets are employed as well as some of the important properties of these concepts are studied. An integrated method combines IF Simple Additive Weighting (IF-SAW) for determining the weights of the types of pollution and IF Preference Ranking Organization Method for Enrichment Evaluations (PROMETHEE) technique for ranking the major cities. In ranking phase, modification and improvement of classical PROMETHEEinto the IF environment is accurately implemented. A Group Decision Making (GDM) process which deals with both quantitative and qualitative factors in an uncertain environment is developed. The effectiveness and applicability of the proposed methodology is numerically illustrated with real world data of environmental pollution in major cities and the study showed that IF-PROMETHEE method could be used in environmental pollution problem as an efficient method.																	1064-1246	1875-8967					2020	38	1					1013	1025		10.3233/JIFS-179465													
J								Social platform based interval valued intuitionistic fuzzy location recommendation system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Location based systems; recommendation systems; interval valued intuitionistic fuzzy sets	CONSUMER SENTIMENT; SERVICES; MEDIA; USER	The improvements in mobile technologies led to the wide adaptation and triggered the demand for location based services. In this respect, examining user similarities enable the analysis of user interests in terms of the determination of purchasing preferences and actual needs. User similarities are generally extracted from consumer life style, demographical information or the reflections from previously sent messages. In spite of the fact that these factors may not directly influence the purchasing decision, uncertain or lack of information can be encountered while establishing recommendation systems. Thus, researchers try to search other indicators that can reflect customer characteristics from spatial data, digital contribution in social media and search history for preferable representation of the changes in purchasing tendency. In this study, social platform based interval valued intuitionistic fuzzy location recommendation system is proposed by considering three common social platforms: Trip Advisor, Zomato and Foursquare. To perform restaurant offers to appropriate social platform users, a sentiment analysis is conducted to selected restaurants and number of negative, positive and neutral comments are extracted. After that, restaurant and location information are examined by using user, restaurant and location clustering via fuzzy clustering. Finally, intuitionistic fuzzy similarity matrix based collaborative filtering is used for restaurant offers to similar users.																	1064-1246	1875-8967					2020	38	1					1027	1042		10.3233/JIFS-179466													
J								Evaluation of positive employee experience using hesitant fuzzy analytic hierarchy process	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Employee experience; employee engagement; hesitant fuzzy sets; hesitant fuzzy analytic hierarchy process; fuzzy simple additive weighting; multi criteria decision making	DECISION-MAKING; SELECTION; PERFORMANCE; ENGAGEMENT; DISTANCE; SYSTEMS; SETS	In contemporary business world, employees are one of the core competencies of organizations and to attract, retain, and engage talented employees is crucial for organizations for sustainable success. Understanding and leveraging employee experience is one of the tending topics for organizations because positive employee experience affects employees' attachment, engagement and loyalty to the organization. Human resource management departments and leaders can apply different strategic initiations to boost employee experience in their organizations. In this study, we aim to design an integrated model for leaders and organizations to guide them for creating positive employee experience to have engaging, enjoyable, and productive work environment. The integrated model includes two phases: (1) evaluation of criteria affecting positive employee experience by hesitant fuzzy analytic hierarchy process (HFAHP) and (2) developing a practical scoring procedure to help companies with their self-assessments by using fuzzy simple additive weighting (FSAW) method. In the first phase, four main and sixteen sub-criteria are taken into consideration. For the second phase, the application of the integrated model is demonstrated with a numerical example from real world. The results indicate that for positive employee experience, leadership has the highest importance followed by human capitals' development opportunity, positive organizational culture, and communication.																	1064-1246	1875-8967					2020	38	1					1043	1058		10.3233/JIFS-179467													
J								A combined methodology for evaluation of electricity distribution companies in Turkey	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Electricity distribution market; data envelopment analysis; artificial neural network	DATA ENVELOPMENT ANALYSIS; ARTIFICIAL NEURAL-NETWORK; IDA-ANN-DEA; ENERGY EFFICIENCY; POWER-PLANTS; WIND FARMS; PERFORMANCE ASSESSMENT; CHINA; PREDICTION; OPTIMIZATION	Energy efficiency initiatives are now more noteworthy due to awareness and sensitivity in the use of resources in rational, optimal and effective ways. The uncertain and dynamic structure of the electricity distribution market requires continuous improvement and efficiency activities/strategic decisions by adding new investments. Energy efficiency assessment plays an important role in improving energy efficiency. In this study, Data Envelopment Analysis (DEA) was employed to investigate the efficiency performance of twenty-one electricity distribution companies in Turkey. The results of DEA revealed that seven of the twenty-one electricity distribution companies were efficiently attempted in Turkey. After utilizing the DEA model, an Artificial Neural Network (ANN) method based on DEA was constructed, and then the efficiency of each company was predicted. According to the proposed integrated model, with incorporating new/alternative electricity companies, investment plans can be easily evaluated from a real perspective, and their performances can be predicted accurately. The study is expected to assist direct energy decision-makers and investors and help them in their investment plans.																	1064-1246	1875-8967					2020	38	1					1059	1069		10.3233/JIFS-179468													
J								Fuzzy production systems: A state of the art literature review	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Production research; information management; fuzzy sets; intuitionistic fuzzy sets; type-n fuzzy sets; hesitant fuzzy sets; pythagorean fuzzy sets	PRODUCTION MANAGEMENT RESEARCH; DECISION-MAKING; MODEL; TIME; ALGORITHM; SELECTION	The problems in the production systems often involve complexity and imprecision. The traditional techniques can be insufficient to handle such problems. This uncertainty and vagueness can be treated with the fuzzy sets. The fuzzy sets are frequently utilized to optimize production system problems under imprecise, complex and subjective information. It is important to gain academic knowledge on how fuzzy sets and the new developments in the fuzzy set theory are utilized in production systems. In this study, we develop a state-of-the-art literature review for the usage of fuzzy sets in production system problems. The literature review is based on 3147 publications composed of 1832 articles, 1277 conference papers and 38 book chapters indexed by Scopus. We present the tabular and graphical results of the literature review. The literature review indicates that although both production literature and fuzzy literature have an increasing attention, at some areas of production systems fuzzy sets have limited usage.																	1064-1246	1875-8967					2020	38	1					1071	1081		10.3233/JIFS-179469													
J								Multidrug-resistant tuberculosis risk factors assessment with intuitionistic fuzzy cognitive maps	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multidrug-resistant tuberculosis; intuitionistic fuzzy cognitive maps; medical decision support; risk factors	DECISION-MAKING; SUPPORT-SYSTEM; DIAGNOSIS; MANAGEMENT; DEFAULT	Tuberculosis (TB) bacteria may develop resistance to the drugs, which are used in TB treatment. Multidrug-resistant TB (MDR-TB) is a type of TB that does not respond to at least rifampicin and isoniazid, the 2 most powerful anti-TB drugs. MDR-TB requires a more compelling treatment and it is more difficult to diagnose. The experience of physician is the key factor in the success of MDR-TB diagnose. The existence of TB bacteria in the body can be observed relatively faster with a standard sputum smear however, drug-susceptibility tests require nearly 45 days. To cope with this infectious disease, it is vital to estimate the resistance in a newly diagnosed TB patient to plan the initialization of the treatment in the testing period. Herein, the purpose of this study is to build a framework and establish a mathematical model that will help decision makers (physicians) while estimating the risk of multidrug resistance when a new tuberculosis patient arrives, using intuitionistic fuzzy cognitive maps (IFCM). Intuitionistic fuzzy sets are utilized to reflect the decision makers' hesitancy degrees in the model.																	1064-1246	1875-8967					2020	38	1					1083	1095		10.3233/JIFS-179470													
J								Location selection for WEEE recycling plant by using Pythagorean fuzzy AHP	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										WEEE; recycling; pythagorean fuzzy AHP	SUSTAINABLE RECOVERY NETWORK; CRITERIA DECISION-MAKING; ELECTRONIC EQUIPMENT; RISK-ASSESSMENT; WASTE; SYSTEM; SAFETY; EXTENSION	With the increasing environmental concerns, new regulations to decrease Waste Electric and Electronic Equipment (WEEE) have taken effect in many countries. Recycling is an important part of the challenge with the increasing amount of waste by time. Therefore, this study aims to investigate the facility location problem of WEEE recycling plant. Taking into consideration the requirement of the process to consider several conflicting factors from qualitative to quantitative factors, one of the MCDM techniques is employed. In order to handle uncertainties on human judgments during the evaluation process, the process is conducted under fuzzy environment. Once criteria affecting the decision-making process of the location of WEEE recycling plant are determined, the Pythagorean fuzzy AHP (PFAHP) method is applied to determine the priority weights of the criteria, sub-criteria and alternative locations. The results of the study show that transportation, recycling, and energy costs are the first three factors respectively, having the highest importance for the selection process, Electric and Electronic Equipment (EEE) usage habits and waste composition are the factors having the least importance.																	1064-1246	1875-8967					2020	38	1					1097	1106		10.3233/JIFS-179471													
J								Interval-valued neutrosophic hypothesis testing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued neutrosophic set; hypothesis testing; pythagorean fuzzy sets; hesitant fuzzy sets	NEYMAN-PEARSON LEMMA; AGGREGATION; UNCERTAINTY; SIMILARITY	Hypothesis testing is an important tool of statistical decision making. Classical hypothesis testing is based on a known probability distribution with known population parameters. However, since the data generally include vagueness and impreciseness, a fuzzy set approach should be used. In this paper, interval-valued neutrosophic sets (IVNSs) are used for the purpose of making statistical decisions. In the proposed neutrosophic hypothesis testing approach, neutrosophic linguistic data and neutrosophic parameters are used. Left-sided, right-sided and double-sided neutrosophic hypothesis tests are developed, illustrative example and sensitivity analysis are given.																	1064-1246	1875-8967					2020	38	1					1107	1117		10.3233/JIFS-179472													
J								Analysis of companies' digital maturity by hesitant fuzzy linguistic MCDM methods	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Digital maturity; hesitant fuzzy linguistic term sets; multi criteria decision making; AHP; ARAS	ARAS METHOD; MODEL; SELECTION; VIKOR; EXTENSION	Digital Transformation (DT) is the journey of using digital technologies to develop new business models and strategies. DT aims to achieve competitive advantage and to realize activities that will create efficiency in the corporate value chain. Digital Maturity Model (DMM) provides a practical approach to DT. There is a need for an analytical tool to analyze the significance of the factors in the DMM and to rank the companies according to their digital maturity. It is a multi-criteria decision-making (MCDM) problem with multiple factors under vagueness and impreciseness. Hesitant fuzzy linguistic term sets (HFLTS) is a technique used to facilitate Decision Makers' (DMs) judgment process in imprecise situations. HFLTS technique gives DMs possibility to use linguistic expressions with comparative judgments. This article introduces a decision framework based on the HFLTS, Hesitant Fuzzy Linguistic (HFL) Analytic Hierarchy Process (AHP) and HFL Additive Ratio ASsessment (ARAS) methods. It is aimed to provide a scientific method that helps to determine the most important criteria for companies' DMM and to rank companies. A case study about the banking sector is presented to verify the usability of this method.																	1064-1246	1875-8967					2020	38	1					1119	1132		10.3233/JIFS-179473													
J								Firing Rate Oscillation and Stochastic Resonance in Cortical Networks With Electrical-Chemical Synapses and Time Delay	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Neurons; Stochastic resonance; Synapses; Biological neural networks; Couplings; Chemicals; Delay effects; Cortical network; firing rate; stochastic resonance (SR); time delay	WORLD NEURONAL NETWORKS; EXCITATORY NETWORKS; SIGNAL-TRANSDUCTION; VISUAL-CORTEX; NOISE; SYNCHRONIZATION; SYSTEMS; ENHANCEMENT; CRAYFISH; CRICKET	The stochastic dynamics of neural network are studied and the phenomenon of stochastic resonance is found in inhibitory neurons, whose firing rate is close to the frequency of external stimulation. Time delay in the neural coupling process can induce multiple stochastic resonances, which appear intermittently at the integer multiples of the oscillation period of the input signal. It is found that the time delay can induce the periodic oscillation of neural firing rate, which may account for the occurrence of multiple stochastic resonances. In addition, the effect of synapses on firing rate oscillation and network resonance is investigated. As the strength of gap-junction and inhibitory-inhibitory chemical coupling is increased, the maximal resonant value increases, while the resonant frequency is unchanged. However, the resonant frequency and peak value increase with the coupling strength of excitatory-inhibitory chemical synapses. This difference may result from the interaction of excitation and inhibition within the cortical network. We further apply mean-field theory to time-delayed network model to validate the obtained numerical results. Both time delay and electrical-chemical synapses play an important role in firing rate oscillation and stochastic resonance within the cortical network, determining the ability to enhance the transmission of information in neural systems.																	1063-6706	1941-0034				JAN	2020	28	1					5	13		10.1109/TFUZZ.2018.2889022													
J								Extraction of SSVEPs-Based Inherent Fuzzy Entropy Using a Wearable Headband EEG in Migraine Patients	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Electroencephalography (EEG); inherent fuzzy entropy; migraine; steady-state visual evoked potential (SSVEP)	CANONICAL CORRELATION-ANALYSIS; EXCITABILITY; HABITUATION; POTENTIALS; MOBILE	Inherent fuzzy entropy is an objective measurement of electroencephalography (EEG) complexity reflecting the robustness of brain systems. In this study, we present a novel application of multiscale relative inherent fuzzy entropy using repetitive steady-state visual evoked potentials (SSVEPs) to investigate EEG complexity change between two migraine phases, i.e., interictal (baseline) and preictal (before migraine attacks) phases. We used a wearable headband EEG device with O1, Oz, O2, and Fpz electrodes to collect EEG signals from 80 participants [40 migraine patients and 40 healthy controls (HCs)] under the following two conditions: During resting state and SSVEPs with five 15-Hz photic stimuli. We found a significant enhancement in occipital EEG entropy with increasing stimulus times in both HCs and patients in the interictal phase, but a reverse trend in patients in the preictal phase. In the 1st SSVEP, occipital EEG entropy of the HCs was significantly lower than that of patents in the preictal phase (FDR-adjusted p < 0.05). Regarding the transitional variance of EEG entropy between the 1st and 5th SSVEPs, patients in the preictal phase exhibited significantly lower values than patients in the interictal phase (FDR-adjusted p < 0.05). Furthermore, in the classification model, the AdaBoost ensemble learning showed an accuracy of 81 +/- 6% and area under the curve of 0.87 for classifying interictal and preictal phases. In contrast, there were no differences in EEG entropy among groups or sessions by using other competing entropy models, including approximate entropy, sample entropy, and fuzzy entropy on the same dataset. In conclusion, inherent fuzzy entropy offers novel applications in visual stimulus environments and may have the potential to provide a preictal alert to migraine patients.																	1063-6706	1941-0034				JAN	2020	28	1					14	27		10.1109/TFUZZ.2019.2905823													
J								Fuzzy Clustering for Exploratory Analysis of EEG Event-Related Potentials	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Electroencephalography; Data processing; Clustering algorithms; Feature extraction; Brain modeling; Task analysis; Scalp; Electroencephalography (EEG); event-related potential (ERP); interval features; possibilistic clustering; unsupervised	C-MEANS ALGORITHM; CLASSIFICATION; KERNEL; ERP; INDEXES; ICA	We introduce an analysis method for electroencephalography (EEG) data, focused on event-related potentials (ERPs). Our approach is unsupervised and makes use of a fuzzy clustering algorithm based on the possibilistic framework and includes a data-driven noise and artifact rejection phase. Our contribution provides a general analysis tool, applicable to any ERP dataset, which can uncover the dataset's internal structure. The fuzzy clustering algorithm is the core of our method, since its fine-grained membership grades how much a sample belongs to a given cluster, making the method applicable even when groups have a certain overlap. Prior to the clustering step, we apply weights to the feature vectors, optimizing them in order to enhance the variance within the dataset, and we extract time-window-interval-based features inspired by interval arithmetic. We apply the data processing workflow to the analysis of a set of ERPs recorded during an emotional Go/NoGo task. We evaluate the performance of the unsupervised analysis by computing a measure based on the clusterization rate of trials in different experimental conditions. The results of the studied dataset show that the proposed method obtains a difference of clusterization rate of 69% in Go versus NoGo trials, when weights and interval features are applied to the data, improving previous work not including weights and interval features, which had a rate of 31%. Furthermore, when compared with the standard fuzzy c-means, our proposed possibilistic clustering algorithm outperforms it in terms of the clusterization rate. We also examine the effect of preprocessing the data with independent component analysis and removing noise-related components and observe that this does not improve significantly the obtained results. These findings demonstrate that our proposed method provides a valuable data processing workflow robust to EEG artifacts and able to produce a clustering that is coherent with the experimental conditions represented in the ERP dataset.																	1063-6706	1941-0034				JAN	2020	28	1					28	38		10.1109/TFUZZ.2019.2910499													
J								Multiple Stochastic Resonances and Oscillation Transitions in Cortical Networks With Time Delay	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Neurons; Delay effects; Biological neural networks; Oscillators; Couplings; Mathematical model; Stochastic resonance; Cortical network; oscillation; stochastic resonance (SR); time delay	NEURAL-NETWORKS; INHIBITORY INTERNEURONS; GAMMA-OSCILLATIONS; NOISE; SYNCHRONIZATION; ENHANCEMENT; MECHANISMS; SYNAPSES; BENEFITS; NEURONS	Stochasticity and oscillation play a vital role in neural signal processing. Time delay, which is inevitable in biological neural systems, has significant effect on the dynamics of neuronal networks. This paper provides an analysis of how time delay affects stochastic resonance and firing rate oscillation of cortical neuronal networks. A cortical network is established and mean-field theory is applied to analytically compute the dynamical response of networks. When the frequency of external stimulation is close to intrinsic frequency of neuronal networks, firing rate exhibits coherent oscillation and the phenomenon of stochastic resonance occurs in inhibitory neurons. Time delay can induce multiple stochastic resonances, which appear intermittently at integer multiples of the period of input signal, due to the transition of network dynamics induced by time delays. The fluctuation of membrane potential and instantaneous firing rate of cortical networks achieve maximal periodically with the variation of time delay. Furthermore, time delay and electrical coupling play complementary roles in determining network responses. Network oscillation can transit from unstable to stable when coupling strength exceeds a critical value. Transition threshold is lower for time delays close to integer multiples of input period where resonant response of cortical network enhances the formation of stable oscillation.																	1063-6706	1941-0034				JAN	2020	28	1					39	46		10.1109/TFUZZ.2018.2884229													
J								Self-Supervised Learning for Specified Latent Representation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Supervised learning; Shape; Semantics; Unsupervised learning; Encoding; Task analysis; Neural networks; Latent representation; neural networks; unsupervised learning	OBJECT DETECTION; REGRESSION	Current latent representation methods using unsupervised learning have no semantic meaning; thus, it is difficult to directly express their physical task in the real world. To this end, this paper attempts to propose a specified latent representation with physical semantic meaning. First, a few labeled samples are used to generate the framework of the latent space, and these labeled samples are mapped to framework nodes in the latent space. Second, a self-learning method using structured unlabeled samples is proposed to shape the free space between the framework nodes in the latent space. The proposed specified latent representation therefore possesses the advantages provided by both supervised and unsupervised learning. The proposed method is verified by numerical simulations and real-world experiments.																	1063-6706	1941-0034				JAN	2020	28	1					47	59		10.1109/TFUZZ.2019.2904237													
J								Supervised Network-Based Fuzzy Learning of EEG Signals for Alzheimer's Disease Identification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Electroencephalography; Electrodes; Brain modeling; Feature extraction; Neurological diseases; Machine learning; Alzheimer's disease (AD); electroencephalograph (EEG); functional network; fuzzy system; supervised learning	FUNCTIONAL BRAIN NETWORKS; GENETIC NETWORKS; POWER SPECTRUM; SYNCHRONIZATION; CONNECTIVITY; DIAGNOSIS; INFORMATION; DYNAMICS; BIOLOGY	Accurate identification of Alzheimer's disease (AD) with electroencephalograph (EEG) is crucial in the clinical diagnosis of neurological disorders. However, the effectiveness and accuracy of manually labeling EEG signals are barely satisfactory, due to lacking effective biomarkers. In this paper, we propose a novel machine learning method network-based Takagi-Sugeno-Kang (N-TSK) for AD identification which employs the complex network theory and TSK fuzzy system. With the construction of functional network of AD subjects, the topological features of weighted and unweighted networks are extracted. Taken the network parameters as independent inputs, a fuzzy-system-based TSK model is established and further trained to identify AD EEG signals. Experimental results demonstrate the effectiveness of the proposed scheme in AD identification and ability of N-TSK fuzzy classifiers. The highest accuracy can achieve 97.3% for patients with closed eyes and 94.78% with open eyes. In addition, the performance of weighted N-TSK largely exceeds unweighted N-TSK. By further optimizing the network features utilized in the N-TSK fuzzy classifiers, it is found that local efficiency and clustering coefficient are the most effective factors in AD identification. This work provides a potential tool for identifying neurological disorders from the perspective of functional networks with EEG signal, especially contributing to the diagnosis and identification of AD.																	1063-6706	1941-0034				JAN	2020	28	1					60	71		10.1109/TFUZZ.2019.2903753													
J								Modeling of the Adaptive Chemical Plume Tracing Algorithm of an Insect Using Fuzzy Inference	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Modulation; Neural activity; Adaptation models; Robots; Surges; Insects; Bio-inspired robotics; chemical plume tracing (CPT); fuzzy theory; neuroethology	CENTRAL-COMPLEX; PHEROMONE; NEURONS; LOCALIZATION; COCKROACH; TRACKING	In this paper, we focus on the chemical plume tracing (CPT) problem, which is a known engineering challenge. In nature, animals solve the CPT by adaptively modifying their behavior according to the environment. Therefore, we propose a CPT solution method with high engineering value by modeling the CPT algorithm of an animal. In this paper, we consider a male silkworm moth as a model. To perform CPT in a turbulent environment, the adaptive selection of the behavior plays an important role. Therefore, we performed simultaneous measurement experiments involving CPT behavior of the brain and analyzed the links between the brain's neural activity and behavioral patterns. We measured the brain's neural response in the lateral accessory lobe (LAL), which generates motion commands. We employed fuzzy inference to analyze the relationship between CPT behavior and LAL neural activity. As a result of analyzing the relationship between CPT behavior and LAL, we found that the moth modulates the behavior of the state transition probability depending on the odor frequency. We modeled the obtained phenomenon and verified its effectiveness through a constructive method. As a result, the search performance was improved compared to the conventional moth algorithm.																	1063-6706	1941-0034				JAN	2020	28	1					72	84		10.1109/TFUZZ.2019.2915187													
J								Classification of Motor Imagery Task by Using Novel Ensemble Pruning Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Electroencephalography; Feature extraction; Brain modeling; Discrete wavelet transforms; Task analysis; Optimization; Difference of convex algorithm; ensemble pruning; motor imagery; optimization; support vector machines (SVM)	BRAIN-COMPUTER-INTERFACE; SINGLE-TRIAL EEG; BCI; SELECTION; SIGNALS; RCMARS; MODEL	Brain-computer interface (BCI) assists communication for the disabled and handicapped. It is usually electroencephalogram (EEG) based and uses motor imagery (MI) in its operation. EEG signals are known for being nonstationary and are sensitive to artifacts from various sources such as the physical and mental state of the patient, their mood, their posture, and any external noise or distractions, etc. Processing of this data directly affects the classification accuracy, making it a critical step in any BCI system. Ensemble learning has been used for many kinds of BCI classification applications including MI and P300 event related potential, which has been proven to be robust. The purpose of this paper is to generate an algorithm that uses ensemble pruning method for EEG classification evoked by an MI task. In order to achieve this, we extracted the features of an EEG dataset and trained a range of support vector machines to make a diverse ensemble of classifiers. This ensemble is then pruned by using a novel optimization model by a difference of convex algorithm, which has not been used on EEG data before.																	1063-6706	1941-0034				JAN	2020	28	1					85	91		10.1109/TFUZZ.2019.2900859													
J								Fuzzified Image Enhancement for Deep Learning in Iris Recognition	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Iris recognition; Iris; Deep learning; Image enhancement; Feature extraction; Transforms; Standards; Capsule Network; convolutional neural network (CNN); deep learning; feature extraction; fuzzy operation; iris recognition; saliency map		Deep learning techniques such as convolutional neural network and capsule network have attained good results in iris recognition. However, due to the influence of eyelashes, skin, and background noises, the model often needs many iterations to retrieve informative iris patterns. Also because of some nonideal situations, such as reflection of glasses and facula on the eyeball, it is hard to detect the boundary of pupil and iris perfectly. Under such a circumstance, discarding the rest parts beyond the boundary may cause losing useful information. Hence, we use Gaussian, triangular fuzzy average, and triangular fuzzy median smoothing filters to preprocess the image by fuzzifying the region beyond the boundary to improve the signal-to-noise ratios. We applied the enhanced images through fuzzy operations to train deep learning methods, which speeds up the process of convergence and also increases the recognition accuracy rate. The saliency maps show that fuzzified image filters make the images more informative for deep learning. The proposed fuzzy operation of images may be a robust technique in many other deep-learning applications of image processing, analysis, and prediction.																	1063-6706	1941-0034				JAN	2020	28	1					92	99		10.1109/TFUZZ.2019.2912576													
J								Fuzzy General Linear Modeling for Functional Magnetic Resonance Imaging Analysis	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Functional magnetic resonance imaging; Blood flow; Shape; Pipelines; Brain modeling; Task analysis; Functional magnetic resonance imaging; fuzzy general linear model; fuzzy hemodynamic response function (HRF); fuzzy numbers	FALSE DISCOVERY RATE; EVENT-RELATED FMRI; HEMODYNAMIC-RESPONSE; BRAIN-TUMOR; REGRESSION; VARIABILITY; SIGNAL; CHALLENGE; AREAS; MRI	Functional magnetic resonance imaging (fMRI) is a key neuroimaging technique. The classic fMRI analysis pipeline is based on the assumption that the hemodynamic response (HR) is the same across brain regions, time, and subjects. Although convenient, there is ample evidence that this assumption does not hold, and that these differences result in inaccuracies in brain activity detection. This article presents a new fMRI processing pipeline that captures the intrinsic intra- and intersubject variability of the HR. At the core of this new pipeline is the definition of a fuzzy hemodynamic response function (HRF). The proposed pipeline includes a new fuzzy general linear model (GLM) able to handle the fuzzy HRF, including a practical realization based on the LR representation of fuzzy numbers. This article also describes how to obtain activation maps from the fuzzy GLM, and a methodology to compute the statistical power of the analysis. The method is evaluated in synthetic and real fMRI data and compared with other state-of-the-art techniques. The experiments based on synthetic data show that the fuzzy GLM approach is more robust under uncertainty regarding the true specific shape of the HR. The experiments based on the real data show an increased volume of the activated brain areas, suggesting that the proposed method is able to prevent false negative errors in the boundaries of target brain regions in which HR should be negligible.																	1063-6706	1941-0034				JAN	2020	28	1					100	111		10.1109/TFUZZ.2019.2936807													
J								H-infinity Filtering for Fuzzy Jumping Genetic Regulatory Networks With Round-Robin Protocol: A Hidden-Markov-Model-Based Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Genetic regulatory networks (GRNs); hidden Markov model (HMM); round-robin protocol (RRP); Takagi-Sugeno (T-S) model	STATE ESTIMATION; SYSTEMS; ROBUSTIFICATION; OPTIMIZATION; STABILITY; SPLINE; DELAYS	In this article, a method to design the filter for fuzzy jumping genetic regulatory networks is explored. The case when the filters cannot directly utilize the mode information of the plant is taken into account. A hidden Markov model is introduced to address such a problem. Furthermore, a mature scheduling method, namely round-robin protocol, is employed to optimize the data transmission in genetic regulatory networks. On the basis of the fuzzy model approach and the stochastic analysis technique, some novel conditions ensuring the $H_{\infty }$ performance and stochastic stability of the error system are established. The parameters of the filter can be presented via addressing the convex optimization problem. The feasibility of results is finally illustrated by considering a repressilator model subject to stochastic jumping parameters.																	1063-6706	1941-0034				JAN	2020	28	1					112	121		10.1109/TFUZZ.2019.2939965													
J								Adaptive Fuzzy Backstepping Dynamic Surface Control of Strict-Feedback Fractional-Order Uncertain Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Backstepping; Adaptive systems; Stability analysis; Asymptotic stability; Circuit stability; Lyapunov methods; Adaptive backstepping control; disturbance compensation; dynamic surface control (DSC); fuzzy logic systems (FLSs); strict-feedback fractional-order systems	TRACKING CONTROL; ASYMPTOTIC TRACKING; OBSERVER; SYNCHRONIZATION; STABILIZATION; MODEL	This paper presents a novel adaptive fuzzy backstepping dynamic surface control (DSC) scheme for a class of single-input single-output strict-feedback fractional-order uncertain nonlinear systems. The controlled systems contain unknown nonlinear functions and unknown external disturbances. Fuzzy logic systems are employed for approximating the unknown nonlinear functions. Further, an auxiliary function is introduced into the control function to simultaneously compensate the unknown external disturbance and the approximation error caused by fuzzy approximation, which erases the possible chattering phenomenon in the existing results. Meanwhile, a new DSC method based on the fractional-order filter is proposed to avoid the issue of explosion of complexity inherent in the backstepping procedure, which releases the limitation that the fractional-order derivative of the intermediate control function needs to be completly known in the existing references. Under certain assumptions, the stability of the closed-loop system is proved by using the fractional-order Lyapunov function stability criterion. Finally, contrastive simulation results are provided to validate the effectiveness of our proposed control strategy.																	1063-6706	1941-0034				JAN	2020	28	1					122	133		10.1109/TFUZZ.2019.2900602													
J								Adaptive Observer-Based Fault-Tolerant Tracking Control for T-S Fuzzy Systems With Mismatched Faults	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Observers; Fault tolerance; Fault tolerant systems; Fuzzy systems; Symmetric matrices; Adaptive systems; Adaptive observer; backsteppinglike technique; fault-tolerant; output tracking; T-S fuzzy systems	UNCERTAIN NONLINEAR-SYSTEMS; FAILURE COMPENSATION; ACTUATOR FAULT; LINEAR-SYSTEMS; CONTROL DESIGN; FILTER DESIGN; SENSOR	This paper is concerned with the observer-based fault-tolerant tracking controller design problem for a class of T-S fuzzy systems with mismatched faults and disturbances. First, a sequence of adaptive observers are constructed, based on which an iterative algorithm is given to obtain the estimations of the system states and the fault signals simultaneously. Different from the existing results which mainly focus on the compensation of matched faults, by employing an equivalent matrix transformation technique and the backsteppinglike technique, a novel observer-based fault-tolerant tracking control scheme is developed to compensate the effects of the mismatched faults and achieve the output tracking objective. Furthermore, it is proven that all the signals of the resulting closed-loop fuzzy system are uniformly ultimately bounded and the output tracking error converges to a bounded region whose upper bound is relevant to the design parameters and the estimate errors. Finally, two simulation examples are provided to illustrate the validity of the proposed controller design method.																	1063-6706	1941-0034				JAN	2020	28	1					134	147		10.1109/TFUZZ.2019.2900838													
J								Output Feedback Model Predictive Control of Interval Type-2 T-S Fuzzy System With Bounded Disturbance	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Estimation error; Observers; Output feedback; Fuzzy systems; Economic indicators; Optimization; Uncertainty; Fuzzy control; model predictive control (MPC); output feedback; stability analysis	FREE CONTROL MOVE; TAKAGI-SUGENO; ROBUST MPC; DESIGN; OBSERVER	In this paper, the problem of output feedback model predictive control (MPC) for interval Type-2 Takagi-Sugeno fuzzy systems with bounded disturbance is investigated. The output feedback MPC approach includes an offline design of the state observer to estimate true states and predict bounds of future estimation error sets, and an online problem that optimizes the controller gains to stabilize the closed-loop observer system. The dynamics of the estimation error system is determined by the offline designed observer gain, and bounds of which are online refreshed by scaling a minimal robust positively invariant (RPI) set via a scalar. The optimized controller gains steer the current estimated state from an RPI set into another one such that future estimated states are invariant in the subsequent RPI set. Convergence of the estimation error system and stability condition on the closed-loop observer system in terms of linear matrix inequalities are derived using the technique of S-procedure. The estimation error and estimated state converge within the corresponding time-varying RPI sets, and therefore, recursive feasibility of the optimization problem and input-to-state stability of the closed-loop observer system with respect to the estimation error and bounded disturbance are ensured. For reducing the online computational burden, a lookup table that stores the offline calculated controller gains with corresponding regions of attraction is offline constructed for online searching real-time controller gains. A simulation example is given to show the effectiveness of the approach.																	1063-6706	1941-0034				JAN	2020	28	1					148	162		10.1109/TFUZZ.2019.2900844													
J								CFM-BD: A Distributed Rule Induction Algorithm for Building Compact Fuzzy Models in Big Data Classification Problems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Big Data; Linguistics; Cluster computing; Fuzzy sets; Training; Runtime; Transforms; Apache spark; big data; evolutionary algorithms; fuzzy rule based classification systems (FRBCSs); probability integral transform; quantile function	DIMENSIONAL OVERLAP FUNCTIONS; SYSTEMS; MAPREDUCE; STRATEGIES	Interpretability has always been a major concern for fuzzy rule-based classifiers. The usage of human-readable models allows them to explain the reasoning behind their predictions and decisions. However, when it comes to Big Data classification problems, fuzzy rule based classifiers have not been able to maintain the good tradeoff between accuracy and interpretability that has characterized these techniques in non-Big-Data environments. The most accurate methods build models composed of a large number of rules and fuzzy sets that are too complex, while those approaches focusing on interpretability do not provide state-of-the-art discrimination capabilities. In this paper, we propose a new distributed learning algorithm named CFM-BD to construct accurate and compact fuzzy rule-based classification systems for Big Data. This method has been specifically designed from scratch for Big Data problems and does not adapt or extend any existing algorithm. The proposed learning process consists of three stages: Preprocessing based on the probability integral transform theorem; rule induction inspired by CHI-BD and Apriori algorithms; and rule selection by means of a global evolutionary optimization. We conducted a complete empirical study to test the performance of our approach in terms of accuracy, complexity, and runtime. The results obtained were compared and contrasted with four state-of-the-art fuzzy classifiers for Big Data (FBDT, FMDT, Chi-Spark-RS, and CHI-BD). According to this study, CFM-BD is able to provide competitive discrimination capabilities using significantly simpler models composed of a few rules of less than three antecedents, employing five linguistic labels for all variables.																	1063-6706	1941-0034				JAN	2020	28	1					163	177		10.1109/TFUZZ.2019.2900856													
J								Delay-Dependent Fuzzy Sampled-Data Synchronization of T-S Fuzzy Complex Networks With Multiple Couplings	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Couplings; Complex networks; Synchronization; Symmetric matrices; Delay effects; Nonlinear dynamical systems; Mathematical model; Delay-dependent fuzzy sampled-data control; partial coupling; sampled-data synchronization; Takagi-Sugeno (T-S) fuzzy complex networks	TIME-VARYING DELAY; DYNAMICAL NETWORKS; ROBUST STABILITY; CHAOTIC SYSTEMS; NEURAL-NETWORKS; STABILIZATION; DISCRETE; INEQUALITY	This paper studies the problem of synchronization for a class of Takagi-Sugeno (T-S) fuzzy complex networks, where the node dynamics may include partial coupling, diffusion coupling, discrete-time coupling, and delayed coupling. A fuzzy sampled-data control strategy that takes into account the time-delay effect is designed to solve the synchronization problem of such networks. Synchronization criteria are established for complex networks with the target node by constructing a modified time-dependent Lyapunov functional and using a mathematical induction approach. In contrast to most existing results, the constructed Lyapunov functional is neither necessarily positive on sampling intervals nor needlessly continuous at sampling instants. Moreover, a corresponding greater aperiodic sampling interval is obtained. Numerical simulation is presented to demonstrate the feasibility and validity of the proposed strategies.																	1063-6706	1941-0034				JAN	2020	28	1					178	189		10.1109/TFUZZ.2019.2901353													
J								Input-to-State Stabilization of Interval Type-2 Fuzzy Systems Subject to Cyberattacks: An Observer-Based Adaptive Sliding Mode Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Cyberattack; Fuzzy systems; Observers; Symmetric matrices; Nonlinear systems; Actuators; Adaptive systems; Cyberattacks; input-to-state stability (ISS); interval type-2 (IT2) fuzzy system; sliding mode control (SMC); sliding mode observer	STABILITY ANALYSIS; NONLINEAR-SYSTEMS; LOGIC SYSTEMS; DESIGN; SENSOR	This paper focuses on the sliding mode control (SMC) problem of interval type-2 (IT2) fuzzy systems subject to the unmeasurable state and cyberattacks. A key issue is how to design a state observer under the constraint that only the bounds of membership functions are known. To this end, this paper introduces two weighting factors to construct a new membership function. Besides, the concept of input-to-state stability (ISS) is utilized to deal with the residual term resulting from the cyberattacks and external disturbances. The sufficient condition is established such that the sliding mode dynamics and the estimated error dynamics are input-to-state stable. Furthermore, by online estimating the unknown parameters in upper bounds of cyberattacks and external disturbances, an adaptive sliding mode controller is synthesized such that the reachability of the prescribed sliding surface can be guaranteed and the effect of cyberattacks on the system performance can be effectively attenuated. Finally, the validity of the proposed method is illustrated by a mass-spring-damper system.																	1063-6706	1941-0034				JAN	2020	28	1					190	203		10.1109/TFUZZ.2019.2902105													
J								Spatially Arranged Sparse Recurrent Neural Networks for Energy Efficient Associative Memory	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Associative memory; Biological neural networks; Hardware; Neurons; Integrated circuit interconnections; Recurrent neural networks; Optimization; Associative memory; energy efficiency; Hopfield network; interconnection cost; iterative learning rule; sparse neural networks; sparse optimization	WIRING OPTIMIZATION; CAPACITY; RECOGNITION; SYSTEMS; STORAGE; MODEL	The development of hardware neural networks, including neuromorphic hardware, has been accelerated over the past few years. However, it is challenging to operate very large-scale neural networks with low-power hardware devices, partly due to signal transmissions through a massive number of interconnections. Our aim is to deal with the issue of communication cost from an algorithmic viewpoint and study learning algorithms for energy-efficient information processing. Here, we consider two approaches to finding spatially arranged sparse recurrent neural networks with the high cost-performance ratio for associative memory. In the first approach following classical methods, we focus on sparse modular network structures inspired by biological brain networks and examine their storage capacity under an iterative learning rule. We show that incorporating long-range intermodule connections into purely modular networks can enhance the cost-performance ratio. In the second approach, we formulate for the first time an optimization problem where the network sparsity is maximized under the constraints imposed by a pattern embedding condition. We show that there is a tradeoff between the interconnection cost and the computational performance in the optimized networks. We demonstrate that the optimized networks can achieve a better cost-performance ratio compared with those considered in the first approach. We show the effectiveness of the optimization approach mainly using binary patterns and apply it also to gray-scale image restoration. Our results suggest that the presented approaches are useful in seeking more sparse and less costly connectivity of neural networks for the enhancement of energy efficiency in hardware neural networks.																	2162-237X	2162-2388				JAN	2020	31	1					24	38		10.1109/TNNLS.2019.2899344													
J								A Unified Entropy-Based Distance Metric for Ordinal-and-Nominal-Attribute Data Clustering	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Distance measurement; Clustering algorithms; Task analysis; Data analysis; Benchmark testing; Entropy; Categorical data; clustering algorithms; data analysis; distance metric; entropy; order information; ordinal attribute	K-MEANS ALGORITHM; DISSIMILARITY MEASURE; ROUGH APPROXIMATION; FEATURE-SELECTION; SETS; ASSOCIATION	Ordinal data are common in many data mining and machine learning tasks. Compared to nominal data, the possible values (also called categories interchangeably) of an ordinal attribute are naturally ordered. Nevertheless, since the data values are not quantitative, the distance between two categories of an ordinal attribute is generally not well defined, which surely has a serious impact on the result of the quantitative analysis if an inappropriate distance metric is utilized. From the practical perspective, ordinal-and-nominal-attribute categorical data, i.e., categorical data associated with a mixture of nominal and ordinal attributes, is common, but the distance metric for such data has yet to be well explored in the literature. In this paper, within the framework of clustering analysis, we therefore first propose an entropy-based distance metric for ordinal attributes, which exploits the underlying order information among categories of an ordinal attribute for the distance measurement. Then, we generalize this distance metric and propose a unified one accordingly, which is applicable to ordinal-and-nominal-attribute categorical data. Compared with the existing metrics proposed for categorical data, the proposed metric is simple to use and nonparametric. More importantly, it reasonably exploits the underlying order information of ordinal attributes and statistical information of nominal attributes for distance measurement. Extensive experiments show that the proposed metric outperforms the existing counterparts on both the real and benchmark data sets.																	2162-237X	2162-2388				JAN	2020	31	1					39	52		10.1109/TNNLS.2019.2899381													
J								GeCo: Classification Restricted Boltzmann Machine Hardware for On-Chip Semisupervised Learning and Bayesian Inference	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Hardware; Training; Neurons; System-on-chip; Inference algorithms; Semisupervised learning; Bayes methods; Bayesian inference; classification restricted Boltzmann machine (ClassRBM); generative core (GeCo); generative model; hardware accelerator; neural network; on-chip learning; restricted Boltzmann machine (RBM); semisupervised		The probabilistic Bayesian inference of real-time input data is becoming more popular, and the importance of semisupervised learning is growing. We present a classification restricted Boltzmann machine (ClassRBM)-based hardware accelerator with on-chip semisupervised learning and Bayesian inference capability. ClassRBM is a specific type of Markov network that can perform classification tasks and reconstruct its input data. ClassRBM has several advantages in terms of hardware implementation compared to other backpropagation-based neural networks. However, its accuracy is relatively low compared to backpropagation-based learning. To improve the accuracy of ClassRBM, we propose the multi-neuron-per-class (multi-NPC) voting scheme. We also reveal that the contrastive divergence (CD) algorithm, which is commonly used to train RBM, shows poor performance in this multi-NPC ClassRBM. As an alternative, we propose an asymmetric contrastive divergence (ACD) training algorithm that improves the accuracy of multi-NPC ClassRBM. With the ACD learning algorithm, ClassRBM operates in the form of a combination of Markov Chain training and Bayesian inference. The experimental results on a field-programmable gate array (FPGA) board for a Modified National Institute of Standards and Technology data set confirm that the inference accuracy of the proposed ACD algorithm is 5.82% higher for a supervised learning case and 12.78% higher for a 1% labeled semisupervised learning case than the conventional CD algorithm. Also, the GeCo ver.2 hardware implemented on a Xilinx ZCU102 FPGA board was 349.04 times faster than the C simulation on CPU.																	2162-237X	2162-2388				JAN	2020	31	1					53	65		10.1109/TNNLS.2019.2899386													
J								Adaptive Neural Network Learning Controller Design for a Class of Nonlinear Systems With Time-Varying State Constraints	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Nonlinear systems; Artificial neural networks; Time-varying systems; Adaptive control; Lyapunov methods; Adaptive control; neural networks (NNs); time-varying barrier Lyapunov function; time-varying full-state constraints	BARRIER LYAPUNOV FUNCTIONS; PURE-FEEDBACK SYSTEMS; DYNAMIC SURFACE CONTROL; TRACKING CONTROL; DELAY SYSTEMS; STABILIZATION; ROBOTS	This paper studies an adaptive neural network (NN) tracking control method for a class of uncertain nonlinear strict-feedback systems with time-varying full-state constraints. As we all know, the states are inevitably constrained in the actual systems because of the safety and performance factors. The main contributions of this paper are that: 1) in order to ensure that the states do not violate the asymmetric time-varying constraint regions, an adaptive NN controller is constructed by introducing the asymmetric time-varying barrier Lyapunov function (TVBLF) and 2) the amount of the learning parameters is reduced by introducing a TVBLF at each step of the backstepping. Based on the Lyapunov stability analysis, it can be proven that all the signals in the closed-loop system are the semiglobal ultimately uniformly bounded and the time-varying full-state constraints are never violated. Finally, a numerical simulation is given, and the effectiveness of this adaptive control method can be verified.																	2162-237X	2162-2388				JAN	2020	31	1					66	75		10.1109/TNNLS.2019.2899589													
J								Event-Triggered Optimal Control With Performance Guarantees Using Adaptive Dynamic Programming	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimal control; Mathematical model; Artificial neural networks; Performance analysis; Dynamic programming; Indexes; Learning systems; Adaptive dynamic programming (ADP); event-triggered; neural network (NN); optimal control; performance guarantee	CONTINUOUS-TIME SYSTEMS; NONLINEAR-SYSTEMS; POLICY ITERATION; ALGORITHM	This paper studies the problem of event-triggered optimal control (ETOC) for continuous-time nonlinear systems and proposes a novel event-triggering condition that enables designing ETOC methods directly based on the solution of the Hamilton-Jacobi-Bellman (HJB) equation. We provide formal performance guarantees by proving a predetermined upper bound. Moreover, we also prove the existence of a lower bound for interexecution time. For implementation purposes, an adaptive dynamic programming (ADP) method is developed to realize the ETOC using a critic neural network (NN) to approximate the value function of the HJB equation. Subsequently, we prove that semiglobal uniform ultimate boundedness can be guaranteed for states and NN weight errors with the ADP-based ETOC. Simulation results demonstrate the effectiveness of the developed ADP-based ETOC method.																	2162-237X	2162-2388				JAN	2020	31	1					76	88		10.1109/TNNLS.2019.2899594													
J								3-D Learning-Enhanced Adaptive ILC for Iteration-Varying Formation Tasks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Nonlinear dynamical systems; Adaptation models; Adaptive learning; Task analysis; Control systems; Vehicle dynamics; Multi-agent systems; Adaptive iterative learning control (AILC); iteration-varying desired formation reference; nonlinear homogeneous and asynchronous multi-agent system (MAS); space-dimensional dynamic linearization	MULTIAGENT SYSTEMS; CONSENSUS; DESIGN; LINEARIZATION; COORDINATION	This paper explores the formation control problem of repetitive nonlinear homogeneous and asynchronous multiagent networks, where the early starting agent is designated as the parent, and the later starting agent with a small delayed time is designated as the child. Moreover, the desired formation reference is allowed to be different from iteration to iteration. A space-dimensional dynamic linearization method is presented to build the linear dynamic relationship between two parent-child agents in a networked system. Then, a 3-D learning-enhanced adaptive iterative learning control (3D-AILC) is proposed by utilizing the additional control information from previous time instants, iterative operations, and parent agents. In other words, the proposed method processes 3-D dynamics to strengthen its learnability, i.e., time dimension, iteration dimension, and space dimension. The desired formation signal is incorporated into the learning control law to compensate its iterative variation to achieve a fast and precise tracking performance. The proposed 3D-AILC is data based and does not use an explicit mechanistic model. The validity of the proposed approach is proven theoretically and tested through simulations as well. Moreover, the proposed method also works well with time-iteration-varying topologies and nonrepetitive uncertainties.																	2162-237X	2162-2388				JAN	2020	31	1					89	99		10.1109/TNNLS.2019.2899632													
J								Reservoir Computing Universality With Stochastic Inputs	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Echo state network (ESN); machine learning; reservoir computing; stochastic input; uniform system approximation; universality	AUTOREGRESSIVE CONDITIONAL HETEROSCEDASTICITY; FADING-MEMORY; APPROXIMATION; STATE; NETWORKS; SYSTEMS; MODEL; OPERATORS; VARIANCE; PROPERTY	The universal approximation properties with respect to L-p-type criteria of three important families of reservoir computers with stochastic discrete-time semi-infinite inputs are shown. First, it is proven that linear reservoir systems with either polynomial or neural network readout maps are universal. More importantly, it is proven that the same property holds for two families with linear readouts, namely, trigonometric state-affine systems and echo state networks, which are the most widely used reservoir systems in applications. The linearity in the readouts is a key feature in supervised machine learning applications. It guarantees that these systems can be used in high-dimensional situations and in the presence of large data sets. The L-p criteria used in this paper allow the formulation of universality results that do not necessarily impose almost sure uniform boundedness in the inputs or the fading memory property in the filter that needs to be approximated.																	2162-237X	2162-2388				JAN	2020	31	1					100	112		10.1109/TNNLS.2019.2899649													
J								Automatic Sleep Staging Employing Convolutional Neural Networks and Cortical Connectivity Images	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Sleep; Feature extraction; Electroencephalography; Biomedical imaging; Convolutional neural networks; Electrooculography; Automatic sleep staging; convolutional neural networks (CNNs); default mode network (DMN); functional connectivity features; minority class oversampling technique	DEFAULT MODE NETWORK; FUNCTIONAL CONNECTIVITY; BRAIN; EEG; SYSTEM; DYNAMICS; ENTROPY; SIGNALS; MEG	Understanding of the neuroscientific sleep mechanisms is associated with mental/cognitive and physical well-being and pathological conditions. A prerequisite for further analysis is the identification of the sleep macroarchitecture through manual sleep staging. Several computer-based approaches have been proposed to extract time and/or frequency-domain features with accuracy ranging from 80% to 95% compared with the golden standard of manual staging. However, their acceptability by the medical community is still suboptimal. Recently, utilizing deep learning methodologies increased the research interest in computer-assisted recognition of sleep stages. Aiming to enhance the arsenal of automatic sleep staging, we propose a novel classification framework based on convolutional neural networks. These receive as input synchronizations features derived from cortical interactions within various electroencephalographic rhythms (delta, theta, alpha, and beta) for specific cortical regions which are critical for the sleep deepening. These functional connectivity metrics are then processed as multidimensional images. We also propose to augment the small portion of sleep onset (N1 stage) through the Synthetic Minority Oversampling Technique in order to deal with the great difference in its duration when compared with the remaining sleep stages. Our results (99.85%) indicate the flexibility of deep learning techniques to learn sleep-related neurophysiological patterns.																	2162-237X	2162-2388				JAN	2020	31	1					113	123		10.1109/TNNLS.2019.2899781													
J								Cell-Coupled Long Short-Term Memory With $L$ -Skip Fusion Mechanism for Mood Disorder Detection Through Elicited Audiovisual Features	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Cell-coupled LSTM; denoizing autoencoder (DAE); L-skip multimodal fusion; mood disorder detection	BIPOLAR DISORDER; RATING-SCALE; MULTIMODAL FUSION; EMOTIONAL STIMULI; RECOGNITION; DEPRESSION; SPEECH; ADAPTATION	In early stages, patients with bipolar disorder are often diagnosed as having unipolar depression in mood disorder diagnosis. Because the long-term monitoring is limited by the delayed detection of mood disorder, an accurate and one-time diagnosis is desirable to avoid delay in appropriate treatment due to misdiagnosis. In this paper, an elicitation-based approach is proposed for realizing a one-time diagnosis by using responses elicited from patients by having them watch six emotion-eliciting videos. After watching each video clip, the conversations, including patient facial expressions and speech responses, between the participant and the clinician conducting the interview were recorded. Next, the hierarchical spectral clustering algorithm was employed to adapt the facial expression and speech response features by using the extended Cohn-Kanade and eNTERFACE databases. A denoizing autoencoder was further applied to extract the bottleneck features of the adapted data. Then, the facial and speech bottleneck features were input into support vector machines to obtain speech emotion profiles (EPs) and the modulation spectrum (MS) of the facial action unit sequence for each elicited response. Finally, a cell-coupled long short-term memory (LSTM) network with an $L$ -skip fusion mechanism was proposed to model the temporal information of all elicited responses and to loosely fuse the EPs and the MS for conducting mood disorder detection. The experimental results revealed that the cell-coupled LSTM with the $L$ -skip fusion mechanism has promising advantages and efficacy for mood disorder detection.																	2162-237X	2162-2388				JAN	2020	31	1					124	135		10.1109/TNNLS.2019.2899884													
J								Self-Tuning Neural Predictive Control Scheme for Ultrabattery to Emulate a Virtual Synchronous Machine in Autonomous Power Systems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Frequency control; Energy storage; Frequency measurement; Adaptive systems; Power system stability; Oscillators; Damping; Adaptive control; diesel generator (DG); frequency; microgrid; maximum power point tracking (MPPT); neural network (NN); predictive control; ultrabattery; virtual inertia	FREQUENCY CONTROL; ENERGY-STORAGE; WIND-SPEED; GENERATION; INVERTERS	An adaptive neural predictive controller (ANPC) is proposed for an ultrabattery energy storage system (UBESS) to enable its operation as a virtual synchronous machine (VSM) in an autonomous wind-diesel power system. The proposed VSM emulates the inertial response and oscillation damping capability of a typical synchronous machine (employed in conventional power plants) by adaptively controlling the power electronic interface of the UBESS. The control objective is to support the network frequency while ensuring efficient/economic use of the UBESS energy. During the load-generation mismatch, ANPC continuously searches for optimal VSM parameters to minimize the actual frequency variations, their rate of change of frequency (ROCOF), and the power flow through the UBESS while maintaining the state of the charge (voltage) of the ultrabattery bank to tackle subsequent disturbances. Simulations confirm that the proposed self-tuning VSM achieves similar performance as that of other VSM control schemes while substantially reducing the power flow through the UBESS and, hence, uses significantly less energy per hertz improvement (in frequency). An index is used to evaluate the performance of the proposed scheme. In addition, the self-tuning VSM has a better dynamic response (quantified as a reduction in ROCOF and settling times) while attenuating the frequency excursions for all simulated cases.																	2162-237X	2162-2388				JAN	2020	31	1					136	147		10.1109/TNNLS.2019.2899904													
J								Scalable Digital Neuromorphic Architecture for Large-Scale Biophysically Meaningful Neural Network With Multi-Compartment Neurons	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neurons; Computational modeling; Neuromorphics; Biological system modeling; Brain modeling; Hardware; Biological neural networks; Compartmental neuron (CMN) model; field-programmable gate array (FPGA); network on chip (NoC); neuromorphic engineering; spiking neural network (SNN)	MULTIPLIERLESS IMPLEMENTATION; MODEL; DYNAMICS; COMMUNICATION; INTEGRATION; FREQUENCY; SYSTEM; FPGA	Multicompartment emulation is an essential step to enhance the biological realism of neuromorphic systems and to further understand the computational power of neurons. In this paper, we present a hardware efficient, scalable, and real-time computing strategy for the implementation of large-scale biologically meaningful neural networks with one million multi-compartment neurons (CMNs). The hardware platform uses four Altera Stratix III field-programmable gate arrays, and both the cellular and the network levels are considered, which provides an efficient implementation of a large-scale spiking neural network with biophysically plausible dynamics. At the cellular level, a cost-efficient multi-CMN model is presented, which can reproduce the detailed neuronal dynamics with representative neuronal morphology. A set of efficient neuromorphic techniques for single-CMN implementation are presented with all the hardware cost of memory and multiplier resources removed and with hardware performance of computational speed enhanced by 56.59% in comparison with the classical digital implementation method. At the network level, a scalable network-on-chip (NoC) architecture is proposed with a novel routing algorithm to enhance the NoC performance including throughput and computational latency, leading to higher computational efficiency and capability in comparison with state-of-the-art projects. The experimental results demonstrate that the proposed work can provide an efficient model and architecture for large-scale biologically meaningful networks, while the hardware synthesis results demonstrate low area utilization and high computational speed that supports the scalability of the approach.																	2162-237X	2162-2388				JAN	2020	31	1					148	162		10.1109/TNNLS.2019.2899936													
J								Set-Membership Estimation for Complex Networks Subject to Linear and Nonlinear Bounded Attacks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Complex networks; State estimation; Estimation error; Channel estimation; Learning systems; Noise measurement; Complex networks; network attack; networked estimation; set-membership estimation	INFINITY STATE ESTIMATION; SYSTEMS	This paper is concerned with the set-membership estimation problem for complex networks subject to unknown but bounded attacks. Adversaries are assumed to exist in the nonsecure communication channels from the nodes to the estimators. The transmitted measurements may be modified by an attack function with added noise that is determined by the adversary but unknown to the estimators. A novel set-membership estimation model against unknown but bounded attacks is presented. Two sufficient conditions are derived to guarantee the existence of the set-membership estimators for the cases that the attack functions are linear and nonlinear, respectively. Two strategies for the design of the set-membership estimator gains are presented. The effectiveness of the proposed estimator design method is verified by two simulation examples.																	2162-237X	2162-2388				JAN	2020	31	1					163	173		10.1109/TNNLS.2019.2900045													
J								A New Timing Error Cost Function for Binary Time Series Prediction	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Timing; Time series analysis; Cost function; Forecasting; Prediction algorithms; Animals; Task analysis; Dynamic time warping (DTW); recurrent neural network; squared timing error (STE); time series forecasting	MODEL	The ability to make predictions is central to the artificial intelligence problem. While machine learning algorithms have difficulty in learning to predict events with hundreds of time-step dependencies, animals can learn event timing within tens of trials across a broad spectrum of time scales. This suggests strongly a need for new perspectives on the forecasting problem. This paper focuses on binary time series that can be predicted within some temporal precision. We demonstrate that the sum of squared errors (SSE) calculated at every time step is not appropriate for this problem. Next, we look at the advantages and shortcomings of using a dynamic time warping (DTW) cost function. Then, we propose the squared timing error (STE) that uses DTW on the event space and applies SSE on the timing error instead of at each time step. We evaluate all three cost functions on different types of timing errors, such as phase shift, warping, and missing events, on synthetic and real-world binary time series (heartbeats, finance, and music). The results show that STE provides more information about timing error, is differentiable, and can be computed online efficiently. Finally, we devise a gradient descent algorithm for STE on a simplified recurrent neural network. We then compare the performance of the STE-based algorithm to SSE- and logit-based gradient descent algorithms on the same network architecture. The results in real-world binary time series show that the STE algorithm generally outperforms all the other cost functions considered.																	2162-237X	2162-2388				JAN	2020	31	1					174	185		10.1109/TNNLS.2019.2900046													
J								Leveraging Coupled Interaction for Multimodal Alzheimer's Disease Diagnosis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Magnetic resonance imaging; Neuroimaging; Measurement; Kernel; Training; Brain modeling; Computer-aided AD; MCI diagnosis; coupled boosting (CB); coupled feature (CFR) representation; coupled metric ensemble (CME)	FEATURE REPRESENTATION; SPARSE REPRESENTATION; CLASSIFICATION; SEGMENTATION; MARGIN; PREDICTION; REGRESSION; PROGNOSIS; SELECTION; IMAGES	As the population becomes older worldwide, accurate computer-aided diagnosis for Alzheimer's disease (AD) in the early stage has been regarded as a crucial step for neurodegeneration care in recent years. Since it extracts the low-level features from the neuroimaging data, previous methods regarded this computer-aided diagnosis as a classification problem that ignored latent featurewise relation. However, it is known that multiple brain regions in the human brain are anatomically and functionally interlinked according to the current neuroscience perspective. Thus, it is reasonable to assume that the extracted features from different brain regions are related to each other to some extent. Also, the complementary information between different neuroimaging modalities could benefit multimodal fusion. To this end, we consider leveraging the coupled interactions in the feature level and modality level for diagnosis in this paper. First, we propose capturing the feature-level coupled interaction using a coupled feature representation. Then, to model the modality-level coupled interaction, we present two novel methods: 1) the coupled boosting (CB) that models the correlation of pairwise coupled-diversity on both inconsistently and incorrectly classified samples between different modalities and 2) the coupled metric ensemble (CME) that learns an informative feature projection from different modalities by integrating the intrarelation and interrelation of training samples. We systematically evaluated our methods with the AD neuroimaging initiative data set. By comparison with the baseline learning-based methods and the state-of-the-art methods that are specially developed for AD/MCI (mild cognitive impairment) diagnosis, our methods achieved the best performance with accuracy of 95.0% and 80.7% (CB), 94.9% and 79.9% (CME) for AD/NC (normal control), and MCI/NC identification, respectively.																	2162-237X	2162-2388				JAN	2020	31	1					186	200		10.1109/TNNLS.2019.2900077													
J								The Forbidden Region Self-Organizing Map Neural Network	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Self-organizing feature maps; Prototypes; Data visualization; Neurons; Lattices; Computer architecture; Forbidden regions; self-organizing maps (SOMs); unsupervised learning; vector quantization	FACILITY LOCATION; ALGORITHM; SOM	Self-organizing maps (SOMs) are aimed to learn a representation of the input distribution which faithfully describes the topological relations among the clusters of the distribution. For some data sets and applications, it is known beforehand that some regions of the input space cannot contain any samples. Those are known as forbidden regions. In these cases, any prototype which lies in a forbidden region is meaningless. However, previous self-organizing models do not address this problem. In this paper, we propose a new SOM model which is guaranteed to keep all prototypes out of a set of prespecified forbidden regions. Experimental results are reported, which show that our proposal outperforms the SOM both in terms of vector quantization error and quality of the learned topological maps.																	2162-237X	2162-2388				JAN	2020	31	1					201	211		10.1109/TNNLS.2019.2900091													
J								HONN-Based Adaptive ILC for Pure-Feedback Nonaffine Discrete-Time Systems With Unknown Control Directions	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Adaptive iterative learning control (ILC); discrete Nussbaum-type function; high-order neural network (HONN); pure-feedback nonaffine discrete-time systems (DTSs); unknown control directions (UCDs)	ITERATIVE LEARNING CONTROL; NONLINEAR-SYSTEMS; NEURAL-NETWORK; DESIGN; CSTR	Nearly all adaptive control techniques require that the control directions of dynamical systems are known in advance. In this paper, for a class of pure-feedback nonaffine discrete-time systems with unknown control directions (UCDs), a high-order neural network (HONN)-based adaptive iterative learning control (ILC) approach is presented to address a repetitive tracking control issue. The implicit function theorem is adopted to cope with the difficulty resulting from the nonaffine structure of control input. Employing a discrete Nussbaum-type function in the neural network weight adaptation law to suit the UCD, an HONN is used to iteratively estimate the ideal control signals. In addition, a novel dead-zone method is developed in the HONN-based adaptive ILC algorithm to enhance its robustness against nonrepetitive desired trajectories and random uncertainties in iterative initial errors and external disturbance. Consequently, the system output, except at the initial $n$ time instants, is demonstrated to asymptotically converge to an adjustable range of the desired trajectory along the iteration axis, while all of the system signals remain bounded during the entire ILC process. Two simulation examples show the feasibility of the adaptive ILC approach.																	2162-237X	2162-2388				JAN	2020	31	1					212	224		10.1109/TNNLS.2019.2900278													
J								Further Results on Adaptive Stabilization of High-Order Stochastic Nonlinear Systems Subject to Uncertainties	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Delays; Adaptive systems; Stochastic processes; Artificial neural networks; Nonlinear systems; Uncertainty; Control systems; Adaptive control; high-order stochastic nonlinear systems; radial basis function neural network (RBF NN); time delay; unknown control gain	TIME-DELAY SYSTEMS; OUTPUT-FEEDBACK STABILIZATION; HOMOGENEOUS DOMINATION APPROACH; DYNAMIC SURFACE CONTROL; STATE-FEEDBACK; TRACKING CONTROL; SATURATION; PRINCIPLE; NETWORKS	This paper concerns the adaptive state-feedback control for a class of high-order stochastic nonlinear systems with uncertainties including time-varying delay, unknown control gain, and parameter perturbation. The commonly used growth assumptions on system nonlinearities are removed, and the adaptive control technique is combined with the sign function to deal with the unknown control gain. Then, with the help of the radial basis function neural network approximation approach and Lyapunov-Krasovskii functional, an adaptive state-feedback controller is obtained through the backstepping design procedure. It is verified that the constructed controller can render the closed-loop system semiglobally uniformly ultimately bounded. Finally, both the practical and numerical examples are presented to validate the effectiveness of the proposed scheme.																	2162-237X	2162-2388				JAN	2020	31	1					225	234		10.1109/TNNLS.2019.2900339													
J								Attack Detection and Approximation in Nonlinear Networked Control Systems Using Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Communication networks; Delays; Sensors; Observers; Actor-critic network; attack detection; attack estimation; event-triggered control; flow control; networked control system (NCS); neural network (NN); optimal control	CYBER-PHYSICAL SYSTEMS; DESIGN; DEFENSE	In networked control systems (NCS), a certain class of attacks on the communication network is known to raise traffic flows causing delays and packet losses to increase. This paper presents a novel neural network (NN)-based attack detection and estimation scheme that captures the abnormal traffic flow due to a class of attacks on the communication links within the feedback loop of an NCS. By modeling the unknown network flow as a nonlinear function at the bottleneck node and using a NN observer, the network attack detection residual is defined and utilized to determine the onset of an attack in the communication network when the residual exceeds a predefined threshold. Upon detection, another NN is used to estimate the flow injected by the attack. For the physical system, we develop an attack detection scheme by using an adaptive dynamic programming-based optimal event-triggered NN controller in the presence of network delays and packet losses. Attacks on the network as well as on the sensors of the physical system can be detected and estimated with the proposed scheme. The simulation results confirm theoretical conclusions.																	2162-237X	2162-2388				JAN	2020	31	1					235	245		10.1109/TNNLS.2019.2900430													
J								Multiple Instance Learning for Multiple Diverse Hyperspectral Target Characterizations	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Hyperspectral imaging; Training; Image color analysis; Detectors; Task analysis; Object detection; Diversity; human in loop; hyperspectral data; multiple instance learning (MIL); multiple target characterizations		A practical hyperspectral target characterization task estimates a target signature from imprecisely labeled training data. The imprecisions arise from the characteristics of the real-world tasks. First, accurate pixel-level labels on training data are often unavailable. Second, the subpixel targets and occluded targets cause the training samples to contain mixed data and multiple target types. To address these imprecisions, this paper proposes a new hyperspectral target characterization method to produce diverse multiple hyperspectral target signatures under a multiple instance learning (MIL) framework. The proposed method uses only bag-level training samples and labels, which solves the problems arising from the mixed data and lack of pixel-level labels. Moreover, by formulating a multiple characterization MIL and including a diversity-promoting term, the proposed method can learn a set of diverse target signatures, which solves the problems arising from multiple target types in training samples. The experiments on hyperspectral target detections using the learned multiple target signatures over synthetic and real-world data show the effectiveness of the proposed method.																	2162-237X	2162-2388				JAN	2020	31	1					246	258		10.1109/TNNLS.2019.2900465													
J								Learning-Based Robust Tracking Control of Quadrotor With Time-Varying and Coupling Uncertainties	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Uncertainty; Couplings; Rotors; Adaptation models; Optimal control; Nonlinear systems; Time-varying systems; Adaptive dynamic programming (ADP); learning-based control; neural networks; Quadrotor; robust tracking control; time-varying and coupling uncertainties	SLIDING MODE CONTROL; CONTROL DESIGN; CONTROL-SYSTEM; FLIGHT; STABILIZATION; MANIPULATORS; FRAMEWORK; VEHICLE	In this paper, a learning-based robust tracking control scheme is proposed for a quadrotor unmanned aerial vehicle system. The quadrotor dynamics are modeled including time-varying and coupling uncertainties. By designing position and attitude tracking error subsystems, the robust tracking control strategy is conducted by involving the approximately optimal control of associated nominal error subsystems. Furthermore, an improved weight updating rule is adopted, and neural networks are applied in the learning-based control scheme to get the approximately optimal control laws of the nominal error subsystems. The stability of tracking error subsystems with time-varying and coupling uncertainties is provided as the theoretical guarantee of learning-based robust tracking control scheme. Finally, considering the variable disturbances in the actual environment, three simulation cases are presented based on linear and nonlinear models of quadrotor with competitive results to demonstrate the effectiveness of the proposed control scheme.																	2162-237X	2162-2388				JAN	2020	31	1					259	273		10.1109/TNNLS.2019.2900510													
J								A Time Wave Neural Network Framework for Solving Time-Dependent Project Scheduling Problems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Optimal scheduling; Job shop scheduling; Biological neural networks; Neurons; Evolutionary computation; Schedules; Dijkstra's algorithm; project scheduling problem library (PSPLIB) data; time-dependent project scheduling problems (TPSPs); time wave neural network (TWNN)	ALGORITHM; OPTIMIZATION; MODELS; CONSTRAINTS; SEARCH	This paper considers the time-dependent project scheduling problem (TPSP). We propose a time wave neural network (TWNN) framework that is able to achieve the global optimal solution (viz., the optimal project schedule) of the TPSP, which is very difficult to obtain using conventional methods (e.g., Dijkstra's algorithm). The proposed TWNN is a time wave neuron-based neural network without a requirement for any training. In the design of a TWNN, the overall project network of the TPSP is viewed as a neural network, while each node is considered as a wave-based neuron. With this new perspective, the wave-based neuron is constructed based on seven parts: an input, a wave receiver, a neuron state, a time-window selector, a wave generator, a wave sender, and an output. The first three parts are used to receive the waves coming from the predecessor neurons, the fourth part is used to choose the optimal feasible time window, and the remaining three parts are utilized to generate waves for the successive neurons. The main idea of a TWNN is based on the following mechanism: a wave generated from a neuron (node) means that all previous arcs (subprojects) of this neuron have been completed. In particular, the global optimal project scheduling is obtained when a wave is generated by the final destination neuron. To evaluate the performance of a TWNN, the well-known project scheduling problem library data sets are modified and considered in a comparative analysis. Numerical examples are also utilized to demonstrate the robustness of the method.																	2162-237X	2162-2388				JAN	2020	31	1					274	283		10.1109/TNNLS.2019.2900544													
J								A Conclusive Analysis of the Finite-Time Behavior of the Discretized Pursuit Learning Automaton	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence; Markov processes; Learning automata; Eigenvalues and eigenfunctions; Pursuit algorithms; Maximum likelihood estimation; Discretized pursuit automaton (DPA); finite-time analysis (FTA); learning automaton; pursuit algorithms (PAs)	ALGORITHMS	This paper deals with the finite-time analysis (FTA) of learning automata (LA), which is a topic for which very little work has been reported in the literature. This is as opposed to the asymptotic steady-state analysis for which there are, probably, scores of papers. As clarified later, unarguably, the FTA of Markov chains, in general, and of LA, in particular, is far more complex than the asymptotic steady-state analysis. Such an FTA provides rigid bounds for the time required for the LA to attain to a given convergence accuracy. We concentrate on the FTA of the Discretized Pursuit Automaton (DPA), which is probably one of the fastest and most accurate reported LA. Although such an analysis was carried out many years ago, we record that the previous work is flawed. More specifically, in all brevity, the flaw lies in the wrongly "derived" monotonic behavior of the LA after a certain number of iterations. Rather, we claim that the property should be invoked is the submartingale property. This renders the proof to be much more involved and deep. In this paper, we rectify the flaw and reestablish the FTA based on such a submartingale phenomenon. More importantly, from the derived analysis, we are able to discover and clarify, for the first time, the underlying dilemma between the DPA's exploitation and exploration properties. We also nontrivially confirm the existence of the optimal learning rate, which yields a better comprehension of the DPA itself.																	2162-237X	2162-2388				JAN	2020	31	1					284	294		10.1109/TNNLS.2019.2900639													
J								Semisupervised Text Classification by Variational Autoencoder	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Decoding; Task analysis; Training; Semisupervised learning; Predictive models; Feature extraction; Generative models; semisupervised learning; text classification; variational autoencoder (VAE)		Semisupervised text classification has attracted much attention from the research community. In this paper, a novel model, the semisupervised sequential variational autoencoder (SSVAE), is proposed to tackle this problem. By treating the categorical label of unlabeled data as a discrete latent variable, the proposed model maximizes the variational evidence lower bound of the data likelihood, which implicitly derives the underlying label distribution for the unlabeled data. Analytical work indicates that the autoregressive nature of the sequential model is the crucial issue that renders the vanilla model ineffective. To remedy this, two types of decoders are investigated in the SSVAE model and verified. In addition, a reweighting approach is proposed to circumvent the credit assignment problem that occurs during the reconstruction procedure, which can further improve performance for sparse text data. Experimental results show that our method significantly improves the classification accuracy compared with other modern methods.																	2162-237X	2162-2388				JAN	2020	31	1					295	308		10.1109/TNNLS.2019.2900734													
J								A Novel Concept Drift Detection Method for Incremental Learning in Nonstationary Environments	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Microsoft Windows; Predictive models; Adaptation models; Computational modeling; Production; Probability density function; Concept drift detection; energy production prediction; incremental learning; model dissimilarity; online sequential extreme learning machine (OS-ELM)	NEURAL-NETWORK; FAULT-DETECTION; MACHINE; ONLINE; SYSTEM	We present a novel method for concept drift detection, based on: 1) the development and continuous updating of online sequential extreme learning machines (OS-ELMs) and 2) the quantification of how much the updated models are modified by the newly collected data. The proposed method is verified on two synthetic case studies regarding different types of concept drift and is applied to two public real-world data sets and a real problem of predicting energy production from a wind plant. The results show the superiority of the proposed method with respect to alternative state-of-the-art concept drift detection methods. Furthermore, updating the prediction model when the concept drift has been detected is shown to allow improving the overall accuracy of the energy prediction model and, at the same time, minimizing the number of model updatings.																	2162-237X	2162-2388				JAN	2020	31	1					309	320		10.1109/TNNLS.2019.2900956													
J								Attribute-Guided Network for Cross-Modal Zero-Shot Hashing	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Attribute; cross-modal hashing (CMH); image retrieval; zero-shot hashing (ZSH); zero-shot learning (ZSL)		Zero-shot hashing (ZSH) aims at learning a hashing model that is trained only by instances from seen categories but can generate well to those of unseen categories. Typically, it is achieved by utilizing a semantic embedding space to transfer knowledge from seen domain to unseen domain. Existing efforts mainly focus on single-modal retrieval task, especially image-based image retrieval (IBIR). However, as a highlighted research topic in the field of hashing, cross-modal retrieval is more common in real-world applications. To address the cross-modal ZSH (CMZSH) retrieval task, we propose a novel attribute-guided network (AgNet), which can perform not only IBIR but also text-based image retrieval (TBIR). In particular, AgNet aligns different modal data into a semantically rich attribute space, which bridges the gap caused by modality heterogeneity and zero-shot setting. We also design an effective strategy that exploits the attribute to guide the generation of hash codes for image and text within the same network. Extensive experimental results on three benchmark data sets (AwA, SUN, and ImageNet) demonstrate the superiority of AgNet on both cross-modal and single-modal zero-shot image retrieval tasks.																	2162-237X	2162-2388				JAN	2020	31	1					321	330		10.1109/TNNLS.2019.2904991													
J								Associative Memories With Synaptic Delays	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neurons; Logic gates; Delays; Organizations; Biological neural networks; Memory management; Delay effects; Associative knowledge graphs; associative memories; synaptic delays; synaptic efficacy	SEQUENCES	In this paper, we introduce a new concept of associative memories in which synaptic connections of the self-organizing neural network learn time delays between input sequence elements. Synaptic connections represent both the synaptic weights and expected delays between the network inputs. This property of synaptic connections facilitates recognition of time sequences and provides context-based associations between sequence elements. Characteristics of time delays are learned and are updated each time an input sequence is presented. There are no separate learning and testing modes typically used in other neural networks, as the network starts to predict the next input element as soon as there is no expected input signal. The network generates output signals useful for associative recall and prediction. These output signals depend on the presented input context and the knowledge stored in the graph. Such a mode of operation is preferred for the organization of episodic memories used to store the observed episodes and to recall them if a sufficient context is provided. The associative sequential recall is useful for the operation of working memory in a cognitive agent. Test results demonstrate that the network correctly recognizes the input sequences with variable delays and that it is more efficient than other recently developed sequential memory networks based on associative neurons.																	2162-237X	2162-2388				JAN	2020	31	1					331	344		10.1109/TNNLS.2019.2921143													
J								Augmenting Recurrent Neural Networks Resilience by Dropout	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Biological neural networks; Neurons; Recurrent neural networks; Resilience; Data models; Computational modeling; Deep learning; dependable machine learning; dropout; missing inputs; recurrent neural networks (RNNs)		This brief discusses the simple idea that dropout regularization can be used to efficiently induce resiliency to missing inputs at prediction time in a generic neural network. We show how the approach can be effective on tasks where imputation strategies often fail, namely, involving recurrent neural networks and scenarios where whole sequences of input observations are missing. The experimental analysis provides an assessment of the accuracy-resiliency tradeoff in multiple recurrent models, including reservoir computing methods, and comprising real-world ambient intelligence and biomedical time series.																	2162-237X	2162-2388				JAN	2020	31	1					345	351		10.1109/TNNLS.2019.2899744													
J								Noise Robust Projection Rule for Hyperbolic Hopfield Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Neural networks; Stability analysis; Algebra; Neurons; Computer simulation; Learning systems; Complex-valued Hopfield neural networks (CHNNs); hyperbolic Hopfield neural networks (HHNNs); projection rule; stability condition	ASSOCIATIVE MEMORY	A complex-valued Hopfield neural network (CHNN) is a multistate Hopfield model. Low noise tolerance is the main disadvantage of CHNNs. The hyperbolic Hopfield neural network (HHNN) is a noise robust multistate Hopfield model. In HHNNs employing the projection rule, noise tolerance rapidly worsened as the number of training patterns increased. This result was caused by the self-loops. The projection rule for CHNNs improves noise tolerance by removing the self-loops, however, that for HHNNs cannot remove them. In this brief, we extended the stability condition for the self-loops of HHNNs and modified the projection rule. Thus, the HHNNs had improved noise tolerance.																	2162-237X	2162-2388				JAN	2020	31	1					352	356		10.1109/TNNLS.2019.2899914													
J								PixISegNet: pixel-level iris segmentation network using convolutional encoder-decoder with stacked hourglass bottleneck	IET BIOMETRICS										image segmentation; iris recognition; feature extraction; image coding; decoding; convolutional neural nets; entropy codes; image classification; image restoration; image matching; siamese matching network; salient iris features; pixel-level iris segmentation network; stacked hourglass bottleneck; nonregular reflections; deep convolutional neural network; stacked hourglass network; cross-entropy loss; pixel-to-pixel classification loss; deep convolutional NN; encoder-decoder; image segmentation performance; content loss optimisation; hyper-parameterisation; Iris-DenseNet framework; iris image data sets; iris ROI image segmentation algorithm; biometric segmentation research; multiscale-multiorientation training; Pix-SegNet; train-once-test-all strategy; TOTA strategy; CASIA V3; 0 interval data sets; IIT-D interval data sets; UBIRIS-V2 interval data sets	LOCALIZATION; RECOGNITION; FEATURES	In this paper, we present a new iris ROI segmentation algorithm using a deep convolutional neural network (NN) to achieve the state-of-the-art segmentation performance on well-known iris image data sets. The authors' model surpasses the performance of state-of-the-art Iris DenseNet framework by applying several strategies, including multi-scale/ multi-orientation training, model training from scratch, and proper hyper-parameterisation of crucial parameters. The proposed PixISegNet consists of an autoencoder which primarily uses long and short skip connections and a stacked hourglass network between encoder and decoder. There is a continuous scale up-down in stacked hourglass networks, which helps in extracting features at multiple scales and robustly segments the iris even in an occluded environment. Furthermore, cross-entropy loss and content loss optimise the proposed model. The content loss considers the high-level features, thus operating at a different scale of abstraction, which compliments the cross-entropy loss, which considers pixel-to-pixel classification loss. Additionally, they have checked the robustness of the proposed network by rotating images to certain degrees with a change in the aspect ratio along with blurring and a change in contrast. Experimental results on the various iris characteristics demonstrate the superiority of the proposed method over state-of-the-art iris segmentation methods considered in this study. In order to demonstrate the network generalisation, they deploy a very stringent TOTA (i.e. train-once-test-all) strategy. Their proposed method achieves $E_1$E1 scores of 0.00672, 0.00916 and 0.00117 on UBIRIS-V2, IIT-D and CASIA V3.0 Interval data sets, respectively. Moreover, such a deep convolutional NN for segmentation when included in an end-to-end iris recognition system with a siamese based matching network will augment the performance of the siamese network.																	2047-4938	2047-4946				JAN	2020	9	1					11	24		10.1049/iet-bmt.2019.0025													
J								Automated cleaning of identity label noise in a large face dataset with quality control	IET BIOMETRICS										quality control; learning (artificial intelligence); search engines; face recognition; Internet; size 1; 0 m; quality control; ID label noise; automated cleaning; quality-based cleaning; CACD face dataset; ID label cleaning method; cleaning results; low-to-high-quality face verification experiments; deep model; low-quality images; low-quality face recognition; face ID labels; ID label error; face images; wrong identity labels; search engines; large-scale datasets		For face recognition, some very large-scale datasets are publicly available in recent years, which are usually collected from the Internet using search engines, and thus have many faces with wrong identity (ID) labels (outliers). Additionally, the face images in these datasets have different qualities because of uncontrolled situations. The authors propose a novel approach for cleaning the ID label error, handling face images in different qualities. The face ID labels cleaned by their method can train better models for low-quality face recognition since more low-quality images are correctly labelled for training a deep model. In their low-to-high-quality face verification experiments, the deep model trained on their cleaning results of MS-Celeb-1M.v1 face dataset outperforms the same model trained on the same dataset cleaned by the semantic bootstrapping method. They also apply their ID label cleaning method on a subset of the cross-age celebrity dataset (CACD) face dataset, in which their quality-based cleaning can deliver higher precision and recall than a previous method on detecting the ID label errors.																	2047-4938	2047-4946				JAN	2020	9	1					25	30		10.1049/iet-bmt.2019.0081													
J								Human forehead recognition: a novel biometric modality based on near-infrared laser backscattering feature image using deep transfer learning	IET BIOMETRICS										biometrics (access control); image recognition; feature extraction; neural nets; object recognition; image classification; learning (artificial intelligence); novel biometric modality; near-infrared laser backscattering feature image; deep transfer learning; human recognition systems; identity verification; human body; biometric technology; security system complexity; forehead feature images; near-infrared laser scanning system; state-of-the-art deep convolutional neural networks; human forehead recognition task; large-scale training data; feature representation knowledge; transfer learning approach	TISSUE THICKNESS; REGISTRATION	Human recognition systems are an essential tool for identity verification. Though various parts of the human body have been widely used as input data for decades, developing new biometric technology is still necessary to enhance the security system complexity. This article presents a novel biometric modality based on forehead feature images acquired from a specially designed near-infrared laser scanning system. The authors selected state-of-the-art deep convolutional neural networks (CNN), including VGGNet, ResNet, and Inception-v3, to demonstrate the human forehead recognition task. Though large-scale training data is generally required for learning a promising CNN model, they showed the feasibility to transfer the feature representation knowledge of the networks that were pre-trained on the data from a different domain and fine-tuned the target network on the limited dataset of forehead feature images. This transfer learning approach establishes the usability of human forehead recognition and allows us to implement this biometric modality for real-world application.																	2047-4938	2047-4946				JAN	2020	9	1					31	37		10.1049/iet-bmt.2019.0015													
J								Cadaver identification with dental radiographs using isoperimetric and nodal graph approach	IET BIOMETRICS										image segmentation; biometrics (access control); image matching; dentistry; feature extraction; edge detection; medical image processing; dental information; good biometric properties; dental radiographs; required dental data; nodal graph; shape information; nodal graphs; cadaver identification; biometrics; recognisable data; verifiable data; dental biometric technique	X-RAY IMAGES; SYSTEM; BIOMETRICS; ALIGNMENT; SHAPE	Biometrics in general allows a person to be identified and authenticated based on a set of recognisable and verifiable data, which are unique and specific to the human. In this study a dental biometric technique is presented based on dental radiographs. This method is to authenticate cadaver correctly and identify them properly based on dental information as teeth are very resistant to modest force effects and high temperatures and also possess good biometric properties. The dental radiographs are pre-processed and the required dental data is obtained by edge detection using isoperimetric algorithm. The obtained data from the query image is matched with the database image and the best match is obtained for authentication. In this study, a novel method is presented in which the tooth is also represented in the form of a nodal graph and the final matching with database is done with both shape information and nodal graphs obtained. The hit rate obtained is comparable to the existing algorithms.																	2047-4938	2047-4946				JAN	2020	9	1					38	45		10.1049/iet-bmt.2019.0064													
J								Degraded Image Semantic Segmentation With Dense-Gram Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantic segmentation; degraded images	QUALITY ASSESSMENT	Degraded image semantic segmentation is of great importance in autonomous driving, highway navigation systems, and many other safety-related applications and it was not systematically studied before. In general, image degradations increase the difficulty of semantic segmentation, usually leading to decreased semantic segmentation accuracy. Therefore, performance on the underlying clean images can be treated as an upper bound of degraded image semantic segmentation. While the use of supervised deep learning has substantially improved the state of the art of semantic image segmentation, the gap between the feature distribution learned using the clean images and the feature distribution learned using the degraded images poses a major obstacle in improving the degraded image semantic segmentation performance. The conventional strategies for reducing the gap include: 1) Adding image-restoration based preprocessing modules; 2) Using both clean and the degraded images for training; 3) Fine-tuning the network pre-trained on the clean image. In this paper, we propose a novel Dense-Gram Network to more effectively reduce the gap than the conventional strategies and segment degraded images. Extensive experiments demonstrate that the proposed Dense-Gram Network yields state-of-the-art semantic segmentation performance on degraded images synthesized using PASCAL VOC 2012, SUNRGBD, CamVid, and CityScapes datasets.																	1057-7149	1941-0042					2020	29						782	795		10.1109/TIP.2019.2936111													
J								3D Point Cloud Attribute Compression Using Geometry-Guided Sparse Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D point cloud; sparse representation; irregular structure; predictive coding; entropy coding		3D point clouds associated with attributes are considered as a promising paradigm for immersive communication. However, the corresponding compression schemes for this media are still in the infant stage. Moreover, in contrast to conventional image/video compression, it is a more challenging task to compress 3D point cloud data, arising from the irregular structure. In this paper, we propose a novel and effective compression scheme for the attributes of voxelized 3D point clouds. In the first stage, an input voxelized 3D point cloud is divided into blocks of equal size. Then, to deal with the irregular structure of 3D point clouds, a geometry-guided sparse representation (GSR) is proposed to eliminate the redundancy within each block, which is formulated as an l(0)-norm regularized optimization problem. Also, an inter-block prediction scheme is applied to remove the redundancy between blocks. Finally, by quantitatively analyzing the characteristics of the resulting transform coefficients by GSR, an effective entropy coding strategy that is tailored to our GSR is developed to generate the bitstream. Experimental results over various benchmark datasets show that the proposed compression scheme is able to achieve better rate-distortion performance and visual quality, compared with state-of-the-art methods.																	1057-7149	1941-0042					2020	29						796	808		10.1109/TIP.2019.2936738													
J								Query-based summarization of discussion threads	NATURAL LANGUAGE ENGINEERING										query-based summarization; discussion forums; reference summaries; word embeddings; evaluation	AGREEMENT	In this paper, we address query-based summarization of discussion threads. New users can profit from the information shared in the forum, Please check if the inserted city and country names in the affiliations are correct. if they can find back the previously posted information. However, discussion threads on a single topic can easily comprise dozens or hundreds of individual posts. Our aim is to summarize forum threads given real web search queries. We created a data set with search queries from a discussion forum's search engine log and the discussion threads that were clicked by the user who entered the query. For 120 thread-query combinations, a reference summary was made by five different human raters. We compared two methods for automatic summarization of the threads: a query-independent method based on post features, and Maximum Marginal Relevance (MMR), a method that takes the query into account. We also compared four different word embeddings representations as alternative for standard word vectors in extractive summarization. We find (1) that the agreement between human summarizers does not improve when a query is provided that: (2) the query-independent post features as well as a centroid-based baseline outperform MMR by a large margin; (3) combining the post features with query similarity gives a small improvement over the use of post features alone; and (4) for the word embeddings, a match in domain appears to be more important than corpus size and dimensionality. However, the differences between the models were not reflected by differences in quality of the summaries created with help of these models. We conclude that query-based summarization with web queries is challenging because the queries are short, and a click on a result is not a direct indicator for the relevance of the result.																	1351-3249	1469-8110				JAN	2020	26	1					3	29	PII S1351324919000123	10.1017/S1351324919000123													
J								Term evaluation metrics in imbalanced text categorization	NATURAL LANGUAGE ENGINEERING										Text classification; Class imbalance problem; Term evaluation; Machine learning	FEATURE-SELECTION; WEIGHTING SCHEME; CLASSIFICATION	This paper proposes four novel term evaluation metrics to represent documents in the text categorization where class distribution is imbalanced. These metrics are achieved from the revision of the four common term evaluation metrics: chi-square, information gain, odds ratio, and relevance frequency. While the common metrics require a balanced class distribution, our proposed metrics evaluate the document terms under an imbalanced distribution. They calculate the degree of relatedness of terms with respect to minor and major classes by considering their imbalanced distribution. Using these metrics in the document representation makes a better distinction between the documents of the minor and major classes and improves the performance of machine learning algorithms. The proposed metrics are assessed over three popular benchmarks (two subsets of Reuters-21578 and WebKB) by using four classification algorithms: support vector machines, naive Bayes, decision trees, and centroid-based classifiers. Our empirical results indicate that the proposed metrics outperform the common metrics in the imbalanced text categorization.																	1351-3249	1469-8110				JAN	2020	26	1					31	47	PII S1351324919000317	10.1017/S1351324919000317													
J								Robust stylometric analysis and author attribution based on tones and rimes	NATURAL LANGUAGE ENGINEERING										Stylometrics; Quantitative stylistics; Tone and rime motifs; Random forest; SVM; Author identification	CLASSIFICATION; FEATURES; IMPACT; GENRE	In this article, we propose an innovative and robust approach to stylometric analysis without annotation and leveraging lexical and sub-lexical information. In particular, we propose to leverage the phonological information of tones and rimes in Mandarin Chinese automatically extracted from unannotated texts. The texts from different authors were represented by tones, tone motifs, and word length motifs as well as rimes and rime motifs. Support vector machines and random forests were used to establish the text classification model for authorship attribution. From the results of the experiments, we conclude that the combination of bigrams of rimes, word-final rimes, and segment-final rimes can discriminate the texts from different authors effectively when using random forests to establish the classification model. This robust approach can in principle be applied to other languages with established phonological inventory of onset and rimes.																	1351-3249	1469-8110				JAN	2020	26	1					49	71	PII S135132491900010X	10.1017/S135132491900010X													
J								Estimating word-level quality of statistical machine translation output using monolingual information alone	NATURAL LANGUAGE ENGINEERING										Machine translation; Quality estimation; Neural networks		Various studies show that statistical machine translation (SMT) systems suffer from fluency errors, especially in the form of grammatical errors and errors related to idiomatic word choices. In this study, we investigate the effectiveness of using monolingual information contained in the machine-translated text to estimate word-level quality of SMT output. We propose a recurrent neural network architecture which uses morpho-syntactic features and word embeddings as word representations within surface and syntactic n-grams. We test the proposed method on two language pairs and for two tasks, namely detecting fluency errors and predicting overall post-editing effort. Our results show that this method is effective for capturing all types of fluency errors at once. Moreover, on the task of predicting post-editing effort, while solely relying on monolingual information, it achieves on-par results with the state-of-the-art quality estimation systems which use both bilingual and monolingual information.																	1351-3249	1469-8110				JAN	2020	26	1					73	94	PII S1351324919000111	10.1017/S1351324919000111													
J								Annotating a broad range of anaphoric phenomena, in a variety of genres: the ARRAU Corpus	NATURAL LANGUAGE ENGINEERING										coreference; anaphora; discourse; annotation; linguistic corpora	COREFERENCE; AGREEMENT	This paper presents the second release of arrau, a multigenre corpus of anaphoric information created over 10 years to provide data for the next generation of coreference/anaphora resolution systems combining different types of linguistic and world knowledge with advanced discourse modeling supporting rich linguistic annotations. The distinguishing features of arrau include the following: treating all NPs as markables, including non-referring NPs, and annotating their (non-) referentiality status; distinguishing between several categories of non-referentiality and annotating non-anaphoric mentions; thorough annotation of markable boundaries (minimal/maximal spans, discontinuous markables); annotating a variety of mention attributes, ranging from morphosyntactic parameters to semantic category; annotating the genericity status of mentions; annotating a wide range of anaphoric relations, including bridging relations and discourse deixis; and, finally, annotating anaphoric ambiguity. The current version of the dataset contains 350K tokens and is publicly available from LDC. In this paper, we discuss in detail all the distinguishing features of the corpus, so far only partially presented in a number of conference and workshop papers, and we also discuss the development between the first release of arrau in 2008 and this second one.																	1351-3249	1469-8110				JAN	2020	26	1					95	128	PII S1351324919000056	10.1017/S1351324919000056													
J								Voice assistance in 2019	NATURAL LANGUAGE ENGINEERING												The end of the calendar year always seems like a good time to pause for breath and reflect on what's been happening over the last 12 months, and that's as true in the world of commercial NLP as it is in any other domain. In particular, 2019 has been a busy year for voice assistance, thanks to the focus placed on this area by all the major technology players. So, we take this opportunity to review a number of key themes that have defined recent developments in the commercialization of voice technology.																	1351-3249	1469-8110				JAN	2020	26	1					129	136		10.1017/S1351324919000640													
J								Category-Aware Spatial Constraint for Weakly Supervised Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Weakly supervised learning; object detection; category-specific pixel gradient; multi-center regularization	OBJECT LOCALIZATION	Weakly supervised object detection has attracted increasing research attention recently. To this end, most existing schemes rely on scoring category-independent region proposals, which is formulated as a multiple instance learning problem. During this process, the proposal scores are aggregated and supervised by only image-level labels, which often fails to locate object boundaries precisely. In this paper, we break through such a restriction by taking a deeper look into the score aggregation stage and propose a Category-aware Spatial Constraint (CSC) scheme for proposals, which is integrated into weakly supervised object detection in an end-to-end learning manner. In particular, we incorporate the global shape information of objects as an unsupervised constraint, which is inferred from build-in foreground-and-background cues, termed Category-specific Pixel Gradient (CPG) maps. Specifically, each region proposal is weighted according to how well it covers the estimated shape of objects. For each category, a multi-center regularization is further introduced to penalize the violations between centers cluster and high-score proposals in a given image. Extensive experiments are done on the most widely-used benchmark Pascal VOC and COCO, which shows that our approach significantly improves weakly supervised object detection without adding new learnable parameters to the existing models nor changing the structures of CNNs.																	1057-7149	1941-0042					2020	29						843	858		10.1109/TIP.2019.2933735													
J								Neural Compatibility Modeling With Probabilistic Knowledge Distillation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-modal; compatibility modeling; probabilistic knowledge distillation	NETWORKS	In modern society, clothing matching plays a pivotal role in people's daily life, as suitable outfits can beautify their appearance directly. Nevertheless, how to make a suitable outfit has become a daily headache for many people, especially those who do not have much sense of aesthetics. In the light of this, many research efforts have been dedicated to the task of complementary clothing matching and have achieved great success relying on the advanced data-driven neural networks. However, most existing methods overlook the rich valuable knowledge accumulated by our human beings in the fashion domain, especially the rules regarding clothing matching, like "coats go with dresses" and "silk tops cannot go with chiffon bottoms". Towards this end, in this work, we propose a knowledge-guided neural compatibility modeling scheme, which is able to incorporate the rich fashion domain knowledge to enhance the performance of the compatibility modeling in the context of clothing matching. To better integrate the huge and implicit fashion domain knowledge into the data-driven neural networks, we present a probabilistic knowledge distillation (PKD) method, which is able to encode vast knowledge rules in a probabilistic manner. Extensive experiments on two real-world datasets have verified the guidance of rules from different sources and demonstrated the effectiveness and portability of our model. As a byproduct, we released the codes and involved parameters to benefit the research community.																	1057-7149	1941-0042					2020	29						871	882		10.1109/TIP.2019.2936742													
J								Exploiting Related and Unrelated Tasks for Hierarchical Metric Learning and Image Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hierarchical metric learning; multi-task learning; related and unrelated tasks; visual tree	TREE CLASSIFIERS; DATABASE	In multi-task learning, multiple interrelated tasks are jointly learned to achieve better performance. In many cases, if we can identify which tasks are related, we can also clearly identify which tasks are unrelated. In the past, most researchers emphasized exploiting correlations among interrelated tasks while completely ignoring the unrelated tasks that may provide valuable prior knowledge for multi-task learning. In this paper, a new approach is developed to hierarchically learn a tree of multi-task metrics by leveraging prior knowledge about both the related tasks and unrelated tasks. First, a visual tree is constructed to hierarchically organize large numbers of image categories in a coarse-to-fine fashion. Over the visual tree, a multitask metric classifier is learned for each node by exploiting both the related and unrelated tasks, where the learning tasks for training the classifiers for the sibling child nodes under the same parent node are treated as the interrelated tasks, and the others are treated as the unrelated tasks. In addition, the node-specific metric for the parent node is propagated to its sibling child nodes to control inter-level error propagation. Our experimental results demonstrate that our hierarchical metric learning algorithm achieves better results than other state-of-the-art algorithms.																	1057-7149	1941-0042					2020	29						883	896		10.1109/TIP.2019.2938321													
J								Pothole Detection Based on Disparity Transformation and Road Surface Modeling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Pothole detection; computer vision; road surface modeling; disparity map; golden section search; dynamic programming; surface normal	VEHICLE ROLL; 3D	Pothole detection is one of the most important tasks for road maintenance. Computer vision approaches are generally based on either 2D road image analysis or 3D road surface modeling. However, these two categories are always used independently. Furthermore, the pothole detection accuracy is still far from satisfactory. Therefore, in this paper, we present a robust pothole detection algorithm that is both accurate and computationally efficient. A dense disparity map is first transformed to better distinguish between damaged and undamaged road areas. To achieve greater disparity transformation efficiency, golden section search and dynamic programming are utilized to estimate the transformation parameters. Otsu's thresholding method is then used to extract potential undamaged road areas from the transformed disparity map. The disparities in the extracted areas are modeled by a quadratic surface using least squares fitting. To improve disparity map modeling robustness, the surface normal is also integrated into the surface modeling process. Furthermore, random sample consensus is utilized to reduce the effects caused by outliers. By comparing the difference between the actual and modeled disparity maps, the potholes can be detected accurately. Finally, the point clouds of the detected potholes are extracted from the reconstructed 3D road surface. The experimental results show that the successful detection accuracy of the proposed system is around 98.7% and the overall pixel-level accuracy is approximately 99.6%.																	1057-7149	1941-0042					2020	29						897	908		10.1109/TIP.2019.2933750													
J								Online Multi-Expert Learning for Visual Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object tracking; multi-expert; second-order quantile Methods; minimum entropy criterion	OBJECT TRACKING; PARTICLE FILTER	The correlation filters based trackers have achieved an excellent performance for object tracking in recent years. However, most existing methods use only one filter but ignore the information of the previous filters. In this paper, we propose a novel online multi-expert learning algorithm for visual tracking. In our proposed scheme, there are former trackers which retain the previous filters, and those trackers will give their predictions in each frame. The current tracker represents the filter of current frame, and both the current tracker and the former trackers constitute our expert ensemble. We use an adaptive Second-order Quantile strategy to learn the weights of each expert, which can take full advantage of all the experts. To simplify our model and remove some bad experts, we prune our models via a minimum entropy criterion. Finally, we propose a new update strategy to avoid the model corruption problem. Extensive experimental results on both OTB2013 and OTB2015 benchmarks demonstrate that our proposed tracker performs favorably against state-of-the-art methods.																	1057-7149	1941-0042					2020	29						934	946		10.1109/TIP.2019.2931082													
J								Spaghetti Labeling: Directed Acyclic Graphs for Block-Based Connected Components Labeling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Decision trees; Prediction algorithms; Vegetation; Labeling; Image processing; Task analysis; Forestry; Connected components labeling; optimal decision trees; direct acyclic graphs; image processing	FAST ALGORITHM	Connected Components Labeling is an essential step of many Image Processing and Computer Vision tasks. Since the first proposal of a labeling algorithm, which dates back to the sixties, many approaches have optimized the computational load needed to label an image. In particular, the use of decision forests and state prediction have recently appeared as valuable strategies to improve performance. However, due to the overhead of the manual construction of prediction states and the size of the resulting machine code, the application of these strategies has been restricted to small masks, thus ignoring the benefit of using a block-based approach. In this paper, we combine a block-based mask with state prediction and code compression: the resulting algorithm is modeled as a Directed Rooted Acyclic Graph with multiple entry points, which is automatically generated without manual intervention. When tested on synthetic and real datasets, in comparison with optimized implementations of state-of-the-art algorithms, the proposed approach shows superior performance, surpassing the results obtained by all compared approaches in all settings.																	1057-7149	1941-0042					2020	29	1					1999	2012		10.1109/TIP.2019.2946979													
J								Learning Sparse and Identity-Preserved Hidden Attributes for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Deep learning; Visualization; Feature extraction; Image reconstruction; Clothing; Training; Person re-identification; attribute learning; generation; discrimination	NETWORK	Person re-identification (Re-ID) aims at matching person images captured in non-overlapping camera views. To represent person appearance, low-level visual features are sensitive to environmental changes, while high-level semantic attributes, such as "short-hair" or "long-hair", are relatively stable. Hence, researches have started to design semantic attributes to reduce the visual ambiguity. However, to train a prediction model for semantic attributes, it requires plenty of annotations, which are hard to obtain in practical large-scale applications. To alleviate the reliance on annotation efforts, we propose to incrementally generate Deep Hidden Attribute (DHA) based on baseline deep network for newly uncovered annotations. In particular, we propose an auto-encoder model that can be plugged into any deep network to mine latent information in an unsupervised manner. To optimize the effectiveness of DHA, we reform the auto-encoder model with additional orthogonal generation module, along with identity-preserving and sparsity constraints. 1) Orthogonally generating: In order to make DHAs different from each other, Singular Vector Decomposition (SVD) is introduced to generate DHAs orthogonally. 2) Identity-preserving constraint: The generated DHAs should be distinct for telling different persons, so we associate DHAs with person identities. 3) Sparsity constraint: To enhance the discriminability of DHAs, we also introduce the sparsity constraint to restrict the number of effective DHAs for each person. Experiments conducted on public datasets have validated the effectiveness of the proposed network. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.																	1057-7149	1941-0042					2020	29	1					2013	2025		10.1109/TIP.2019.2946975													
J								Variational Bayesian Blind Color Deconvolution of Histopathological Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Deconvolution; Bayes methods; Manganese; Image segmentation; Solid modeling; Microscopy; Blind color deconvolution; histopathological images; Bayesian modeling and inference; variational Bayes	STAIN NORMALIZATION; IMMUNOHISTOCHEMISTRY; QUANTIFICATION	Most whole-slide histological images are stained with two or more chemical dyes. Slide stain separation or color deconvolution is a crucial step within the digital pathology workflow. In this paper, the blind color deconvolution problem is formulated within the Bayesian framework. Starting from a multi-stained histological image, our model takes into account both spatial relations among the concentration image pixels and similarity between a given reference color-vector matrix and the estimated one. Using Variational Bayes inference, three efficient new blind color deconvolution methods are proposed which provide automated procedures to estimate all the model parameters in the problem. A comparison with classical and current state-of-the-art color deconvolution algorithms using real images has been carried out demonstrating the superiority of the proposed approach.																	1057-7149	1941-0042					2020	29	1					2026	2036		10.1109/TIP.2019.2946442													
J								Deep Adversarial Metric Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Measurement; Training; Microstrip; Generators; Learning systems; Visualization; Task analysis; Metric learning; deep learning; adversarial learning; hard negative generation; multi-metric	FACE	Learning an effective distance measurement between sample pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negative samples usually account for the tiny minority in the training set, which may fail to fully describe the data distribution close to the decision boundary. In this paper, we present a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the original negative samples, which is widely applicable to existing supervised deep metric learning algorithms. Different from existing sampling strategies which simply ignore numerous easy negatives, our DAML aim to exploit them by generating synthetic hard negatives adversarial to the learned metric as complements. We simultaneously train the feature embedding and hard negative generator in an adversarial manner, so that adequate and targeted synthetic hard negatives are created to learn more precise distance metrics. As a single transformation may not be powerful enough to describe the global input space under the attack of the hard negative generator, we further propose a deep adversarial multi-metric learning (DAMML) method by learning multiple local transformations for more complete description. We simultaneously exploit the collaborative and competitive relationships among multiple metrics, where the metrics display unity against the generator for effective distance measurement as well as compete for more training data through a metric discriminator to avoid overlapping. Extensive experimental results on five benchmark datasets show that our DAML and DAMML effectively boost the performance of existing deep metric learning approaches through adversarial learning.																	1057-7149	1941-0042					2020	29	1					2037	2051		10.1109/TIP.2019.2948472													
J								Combining Faster R-CNN and Model-Driven Clustering for Elongated Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Proposals; Object detection; Adaptation models; Clustering algorithms; Detectors; Sports equipment; Training; Elongated objects; faster R-CNN; object detection; model-driven clustering; likelihood estimation	HISTOGRAMS; INVARIANT; NETWORKS	While analyzing the performance of state-of-the-art R-CNN based generic object detectors, we find that the detection performance for objects with low object-region-percentages (ORPs) of the bounding boxes are much lower than the overall average. Elongated objects are examples. To address the problem of low ORPs for elongated object detection, we propose a hybrid approach which employs a Faster R-CNN to achieve robust detections of object parts, and a novel model-driven clustering algorithm to group the related partial detections and suppress false detections. First, we train a Faster R-CNN with partial region proposals of suitable and stable ORPs. Next, we introduce a deep CNN (DCNN) for orientation classification on the partial detections. Then, on the outputs of the Faster R-CNN and DCNN, the algorithm of adaptive model-driven clustering first initializes a model of an elongated object with a data-driven process on local partial detections, and refines the model iteratively by model-driven clustering and data-driven model updating. By exploiting Faster R-CNN to produce robust partial detections and model-driven clustering to form a global representation, our method is able to generate a tight oriented bounding box for elongated object detection. We evaluate the effectiveness of our approach on two typical elongated objects in the COCO dataset, and other typical elongated objects, including rigid objects (pens, screwdrivers and wrenches) and non-rigid objects (cracks). Experimental results show that, compared with the state-of-the-art approaches, our method achieves a large margin of improvements for both detection and localization of elongated objects in images.																	1057-7149	1941-0042					2020	29	1					2052	2065		10.1109/TIP.2019.2947792													
J								Semantic Image Segmentation by Scale-Adaptive Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Semantics; Detectors; Training; Lips; Task analysis; Feature extraction; Semantic object parsing; human parsing; scale adaptive	OBJECTS	Semantic image segmentation is an important yet unsolved problem. One of the major challenges is the large variability of the object scales. To tackle this scale problem, we propose a Scale-Adaptive Network (SAN) which consists of multiple branches with each one taking charge of the segmentation of the objects of a certain range of scales. Given an image, SAN first computes a dense scale map indicating the scale of each pixel which is automatically determined by the size of the enclosing object. Then the features of different branches are fused according to the scale map to generate the final segmentation map. To ensure that each branch indeed learns the features for a certain scale, we propose a scale-induced ground-truth map and enforce a scale-aware segmentation loss for the corresponding branch in addition to the final loss. Extensive experiments over the PASCAL-Person-Part, the PASCAL VOC 2012, and the Look into Person datasets demonstrate that our SAN can handle the large variability of the object scales and outperforms the state-of-the-art semantic segmentation methods.																	1057-7149	1941-0042					2020	29	1					2066	2077		10.1109/TIP.2019.2941644													
J								Mask SSD: An Effective Single-Stage Approach to Object Instance Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; instance segmentation; feedback features; single-shot detector		We propose Mask SSD, an efficient and effective approach to address the challenging instance segmentation task. Based on a single-shot detector, Mask SSD detects all instances in an image and marks the pixels that belong to each instance. It consists of a detection subnetwork that predicts object categories and bounding box locations, and an instance-level segmentation subnetwork that generates the foreground mask for each instance. In the detection subnetwork, multi-scale and feedback features from different layers are used to better represent objects of various sizes and provide high-level semantic information. Then, we adopt an assistant classification network to guide per-class score prediction, which consists of objectness prior and category likelihood. The instance-level segmentation subnetwork outputs pixel-wise segmentation for each detection while providing the multi-scale and feedback features from different layers as input. These two subnetworks are jointly optimized by a multi-task loss function, which renders Mask SSD direct prediction on detection and segmentation results. We conduct extensive experiments on PASCAL VOC, SBD, and MS COCO datasets to evaluate the performance of Mask SSD. Experimental results verify that as compared with state-of-the-art approaches, our proposed method has a comparable precision with less speed overhead.																	1057-7149	1941-0042					2020	29	1					2078	2093		10.1109/TIP.2019.2947806													
J								Learning Latent Low-Rank and Sparse Embedding for Robust Image Feature Extraction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Subspace learning; feature extraction; low-rank embedding; l(2,1)-norm; face recognition	FACE RECOGNITION; PRESERVING PROJECTIONS; SELECTION; CLASSIFICATION; ALGORITHM	To defy the curse of dimensionality, the inputs are always projected from the original high-dimensional space into the target low-dimension space for feature extraction. However, due to the existence of noise and outliers, the feature extraction task for corrupted data is still a challenging problem. Recently, a robust method called low rank embedding (LRE) was proposed. Despite the success of LRE in experimental studies, it also has many disadvantages: 1) The learned projection cannot quantitatively interpret the importance of features. 2) LRE does not perform data reconstruction so that the features may not be capable of holding the main energy of the original "clean" data. 3) LRE explicitly transforms error into the target space. 4) LRE is an unsupervised method, which is only suitable for unsupervised scenarios. To address these problems, in this paper, we propose a novel method to exploit the latent discriminative features. In particular, we first utilize an orthogonal matrix to hold the main energy of the original data. Next, we introduce an l(2,1)-norm term to encourage the features to be more compact, discriminative and interpretable. Then, we enforce a columnwise l(2,1)-norm constraint on an error component to resist noise. Finally, we integrate a classification loss term into the objective function to fit supervised scenarios. Our method performs better than several state-of-the-art methods in terms of effectiveness and robustness, as demonstrated on six publicly available datasets.																	1057-7149	1941-0042					2020	29	1					2094	2107		10.1109/TIP.2019.2938859													
J								Convolutional Analysis Operator Learning: Acceleration and Convergence	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convolution; Training; Kernel; Convolutional codes; Computed tomography; Convergence; Image reconstruction; Convolutional regularizer learning; convolutional dictionary learning; convolutional neural networks; unsupervised machine learning algorithms; nonconvex-nonsmooth optimization; block coordinate descent; inverse problems; X-ray computed tomography	COORDINATE DESCENT METHOD; IMAGE-RECONSTRUCTION; SPARSE; OPTIMIZATION; ALGORITHM; DICTIONARIES	Convolutional operator learning is gaining attention in many signal processing and computer vision applications. Learning kernels has mostly relied on so-called patch-domain approaches that extract and store many overlapping patches across training signals. Due to memory demands, patch-domain methods have limitations when learning kernels from large datasets - particularly with multi-layered structures, e.g., convolutional neural networks - or when applying the learned kernels to high-dimensional signal recovery problems. The so-called convolution approach does not store many overlapping patches, and thus overcomes the memory problems particularly with careful algorithmic designs; it has been studied within the "synthesis" signal model, e.g., convolutional dictionary learning. This paper proposes a new convolutional analysis operator learning (CAOL) framework that learns an analysis sparsifying regularizer with the convolution perspective, and develops a new convergent Block Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve the corresponding block multi-nonconvex problems. To learn diverse filters within the CAOL framework, this paper introduces an orthogonality constraint that enforces a tight-frame filter condition, and a regularizer that promotes diversity between filters. Numerical experiments show that, with sharp majorizers, BPEG-M significantly accelerates the CAOL convergence rate compared to the state-of-the-art block proximal gradient (BPG) method. Numerical experiments for sparse-view computational tomography show that a convolutional sparsifying regularizer learned via CAOL significantly improves reconstruction quality compared to a conventional edge-preserving regularizer. Using more and wider kernels in a learned regularizer better preserves edges in reconstructed images.																	1057-7149	1941-0042					2020	29	1					2108	2122		10.1109/TIP.2019.2937734													
J								Optical-Flow Based Nonlinear Weighted Prediction for SDR and Backward Compatible HDR Video Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Motion compensation; Nonlinear optics; Optical distortion; Optical imaging; Dynamic range; Distortion; High dynamic range (HDR); compression; tone-mapping (TMO); backward-compatible; HEVC; weighted prediction; nonlinear illumination compensation	IMAGE	Tone Mapping Operators (TMO) designed for videos can be classified into two categories. In a first approach, TMOs are temporal filtered to reduce temporal artifacts and provide a Standard Dynamic Range (SDR) content with improved temporal consistency. This however does not improve the SDR coding Rate Distortion (RD) performances. A second approach is to design the TMO with the goal of optimizing the SDR coding rate-distortion performances. This second category of methods may lead to SDR videos altering the artistic intent compared with the produced HDR content. In this paper, we combine the benefits of the two approaches by introducing new Weighted Prediction (WP) methods inside the HEVC SDR codec. As a first step, we demonstrate the interest of the WP methods compared to TMO optimized for RD performances. Then we present the newly introduced WP algorithm and WP modes. The WP algorithm consists in performing a global motion compensation between frames using an optical flow, and the new modes are based on non linear functions in contrast with the literature using only linear functions. The contribution of each novelty is studied independently and in a second time they are all put in competition to maximize the RD performances. Tests were made for HDR backward compatible compression but also for SDR compression only. In both cases, the proposed WP methods improve the RD performances while maintaining the SDR temporal coherency.																	1057-7149	1941-0042					2020	29	1					2123	2138		10.1109/TIP.2019.2945685													
J								Discriminative and Uncorrelated Feature Selection With Constrained Spectral Analysis in Unsupervised Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Discriminative and uncorrelated feature selection; generalized uncorrelated constraint; constrained spectral analysis; relaxed regularization term; unsupervised learning	RECOGNITION	The existing unsupervised feature extraction methods frequently explore low-redundant features by an uncorrelated constraint. However, the constrained models might incur trivial solutions, due to the singularity of scatter matrix triggered by high-dimensional data. In this paper, we propose a regularized regression model with a generalized uncorrelated constraint for feature selection, which leads to three merits: 1) exploring the low-redundant and discriminative features; 2) avoiding the trivial solutions and 3) simplifying the optimization. Besides that, the local cluster structure is achieved via a novel constrained spectral analysis for the unsupervised learning, where Must-Links and Cannot-Links are transformed into a intrinsic graph and a penalty graph respectively, rather than incorporated into a mixed affinity graph. Accordingly, a discriminative and uncorrelated feature selection with constrained spectral analysis (DUCFS) is proposed with adopting F-norm regularization for interpolating between F-norm and l(2,1)-norm. Due to the flexible gradient and global differentiability, our model converges fast. Extensive experiments on benchmark datasets among several state-of-the-art approaches verify the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29	1					2139	2149		10.1109/TIP.2019.2947776													
J								Face Hallucination Using Cascaded Super-Resolution and Identity Priors	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face hallucination; deep learning; CNN; identity	IMAGE SUPERRESOLUTION; RESOLUTION; MODELS	In this paper we address the problem of hallucinating high-resolution facial images from low-resolution inputs at high magnification factors. We approach this task with convolutional neural networks (CNNs) and propose a novel (deep) face hallucination model that incorporates identity priors into the learning procedure. The model consists of two main parts: i) a cascaded super-resolution network that upscales the low-resolution facial images, and ii) an ensemble of face recognition models that act as identity priors for the super-resolution network during training. Different from most competing super-resolution techniques that rely on a single model for upscaling (even with large magnification factors), our network uses a cascade of multiple SR models that progressively upscale the low-resolution images using steps of 2 x. This characteristic allows us to apply supervision signals (target appearances) at different resolutions and incorporate identity constraints at multiple-scales. The proposed C-SRIP model (Cascaded Super Resolution with Identity Priors) is able to upscale (tiny) low-resolution images captured in unconstrained conditions and produce visually convincing results for diverse low-resolution inputs. We rigorously evaluate the proposed model on the Labeled Faces in the Wild (LFW), Helen and CelebA datasets and report superior performance compared to the existing state-of-the-art.																	1057-7149	1941-0042					2020	29	1					2150	2165		10.1109/TIP.2019.2945835													
J								Unsupervised Rotation Factorization in Restricted Boltzmann Machines	IEEE TRANSACTIONS ON IMAGE PROCESSING										Machine learning; neural networks; rotation-invariant features; restricted Boltzmann machines		Finding suitable image representations for the task at hand is critical in computer vision. Different approaches extending the original Restricted Boltzmann Machine (RBM) model have recently been proposed to offer rotation-invariant feature learning. In this paper, we present an extended novel RBM that learns rotation invariant features by explicitly factorizing for rotation nuisance in 2D image inputs within an unsupervised framework. While the goal is to learn invariant features, our model infers an orientation per input image during training, using information related to the reconstruction error. The training process is regularised by a Kullback-Leibler divergence, offering stability and consistency. We used the gamma-score, a measure that calculates the amount of invariance, to mathematically and experimentally demonstrate that our approach indeed learns rotation invariant features. We show that our method outperforms the current state-of-the-art RBM approaches for rotation invariant feature learning on three different benchmark datasets, by measuring the performance with the test accuracy of an SVM classifier. Our implementation is available at https://bitbucket.org/tuttoweb/rotinvrbm.																	1057-7149	1941-0042					2020	29	1					2166	2175		10.1109/TIP.2019.2946455													
J								Semi-Linearized Proximal Alternating Minimization for a Discrete Mumford-Shah Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convergence; Image restoration; Image edge detection; Minimization; Noise reduction; Degradation; Couplings; Segmentation; restoration; inverse problems; nonsmooth optimization; nonconvex optimization; proximal algorithms; PALM; Mumford-Shah	ALGORITHMS; REGULARIZATION; APPROXIMATION; SEGMENTATION; RESTORATION; FRAMEWORK; NONCONVEX	The Mumford-Shah model is a standard model in image segmentation, and due to its difficulty, many approximations have been proposed. The major interest of this functional is to enable joint image restoration and contour detection. In this work, we propose a general formulation of the discrete counterpart of the Mumford-Shah functional, adapted to nonsmooth penalizations, fitting the assumptions required by the Proximal Alternating Linearized Minimization (PALM), with convergence guarantees. A second contribution aims to relax some assumptions on the involved functionals and derive a novel Semi-Linearized Proximal Alternated Minimization (SL-PAM) algorithm, with proved convergence. We compare the performances of the algorithm with several nonsmooth penalizations, for Gaussian and Poisson denoising, image restoration and RGB-color denoising. We compare the results with state-of-the-art convex relaxations of the Mumford-Shah functional, and a discrete version of the Ambrosio-Tortorelli functional. We show that the SL-PAM algorithm is faster than the original PALM algorithm, and leads to competitive denoising, restoration and segmentation results.																	1057-7149	1941-0042					2020	29	1					2176	2189		10.1109/TIP.2019.2944561													
J								A Deep Learning Reconstruction Framework for Differential Phase-Contrast Computed Tomography With Incomplete Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; Computed tomography; Deep learning; Neural networks; Feature extraction; Absorption; Image reconstruction; tomography; computed tomography (CT); biomedical signal processing; biomedical imaging; reconstruction algorithms	CONVOLUTIONAL NEURAL-NETWORK; IMAGE QUALITY ASSESSMENT; ITERATIVE RECONSTRUCTION	Differential phase-contrast computed tomography (DPC-CT) is a powerful analysis tool for soft-tissue and low-atomic-number samples. Limited by the implementation conditions, DPC-CT with incomplete projections happens quite often. Conventional reconstruction algorithms face difficulty when given incomplete data. They usually involve complicated parameter selection operations, which are also sensitive to noise and are time-consuming. In this paper, we report a new deep learning reconstruction framework for incomplete data DPC-CT. It involves the tight coupling of the deep learning neural network and DPC-CT reconstruction algorithm in the domain of DPC projection sinograms. The estimated result is not an artifact caused by the incomplete data, but a complete phase-contrast projection sinogram. After training, this framework is determined and can be used to reconstruct the final DPC-CT images for a given incomplete projection sinogram. Taking the sparse-view, limited-view and missing-view DPC-CT as examples, this framework is validated and demonstrated with synthetic and experimental data sets. Compared with other methods, our framework can achieve the best imaging quality at a faster speed and with fewer parameters. This work supports the application of the state-of-the-art deep learning theory in the field of DPC-CT.																	1057-7149	1941-0042					2020	29	1					2190	2202		10.1109/TIP.2019.2947790													
J								An Optimized K-Harmonic Means Algorithm Combined with Modified Particle Swarm Optimization and Cuckoo Search Algorithm	JOURNAL OF INTELLIGENT SYSTEMS										K-means; k-harmonic means clustering; particle swarm optimization; Levy flight; local minimum	PSO	Among the data clustering algorithms, the k-means (KM) algorithm is one of the most popular clustering techniques because of its simplicity and efficiency. However, KM is sensitive to initial centers and it has a local optima problem. The k-harmonic means (KHM) clustering algorithm solves the initialization problem of the KM algorithm, but it also has a local optima problem. In this paper, we develop a new algorithm for solving this problem based on a modified version of particle swarm optimization (MPSO) algorithm and KHM clustering. In the proposed algorithm, MPSO is equipped with the cuckoo search algorithm and two new concepts used in PSO in order to improve the efficiency, fast convergence, and escape from local optima. MPSO updates the positions of particles based on a combination of global worst, global best with personal worst, and personal best to dynamically be used in each iteration of the MPSO. The experimental result on eight real-world data sets and two artificial data sets confirms that this modified version is superior to KHM and the regular PSO algorithm. The results of the simulation show that the new algorithm is able to create promising solutions with fast convergence, high accuracy, and correctness while markedly improving the processing time.																	0334-1860	2191-026X				JAN	2020	29	1					1	18		10.1515/jisys-2015-0009													
J								Texture Feature Extraction Using Intuitionistic Fuzzy Local Binary Pattern	JOURNAL OF INTELLIGENT SYSTEMS										Fuzzy local binary pattern; intuitionistic fuzzy sets; intuitionistic fuzzy local binary pattern; entropy	CLASSIFICATION; RETRIEVAL; INFORMATION; IMAGES; FILTER; MODEL	In this paper, intuitionistic fuzzy local binary for texture feature extraction (IFLBP) has been proposed to encode local texture from the input image. The proposed method extends the fuzzy local binary pattern approach by incorporating intuitionistic fuzzy sets in the representation of local patterns of texture in images. Intuitionistic fuzzy local binary pattern also contributes to more than one bin in the distribution of IFLBP values, which can further be used as a feature vector in the various fields of image processing. The performance of the proposed method has been demonstrated on various medical images and processing images of size 256 x 256. The obtained results validated the effectiveness and usefulness of our proposed method over the other reported methods, and new improvements are suggested.																	0334-1860	2191-026X				JAN	2020	29	1					19	34		10.1515/jisys-2016-0155													
J								Leaf Disease Segmentation From Agricultural Images via Hybridization of Active Contour Model and OFA	JOURNAL OF INTELLIGENT SYSTEMS										Active contour model; fruit fly; oppositional; energy; control pint		In this paper, an alternative active contour model (ACM) driven by an oppositional fruit fly algorithm (OFA) is presented. Unlike the traditional ACM variant, which is frequently caught in a local minimum, this methodology helps the focalizing of control points toward the global least of the energy function. In the proposed system, energy minimization is performed through a fruit fly algorithm, and every control point is compelled in a local search window. As for the local search window, the rectangular-shaped approach has been viewed. The results demonstrated that the fruit fly strategy utilizing polar coordinates is, for the most part, desirable over the fruit fly performed in rectangular shapes. Three performance metrics, such as the Jaccard index, the Dice index, and the Hausdorff distance, were utilized to validate the proposed strategy in real agricultural and synthetic images. From the results, it is clear that the proposed OFA technique shows a great option for the agricultural plant image segmentation process, considering any kind of disease that occurred in plant leaves.																	0334-1860	2191-026X				JAN	2020	29	1					35	52		10.1515/jisys-2017-0415													
J								Deadline Constrained Task Scheduling Method Using a Combination of Center-Based Genetic Algorithm and Group Search Optimization	JOURNAL OF INTELLIGENT SYSTEMS										Hybrid GSO-CBGA; scheduling; cloud computing; quality of service; IaaS providers		The present paper describes a hybrid group search optimization (GSO) and center-based genetic algorithm (CBGA)-based model for task scheduling in cloud computing. The proposed hybrid model combines the GSO, which has been successful in its application in task scheduling, with the use of the CBGA. The basic scheme of our approach is to utilize the benefits of both the GSO algorithm and CBGA excluding their disadvantages. In our work, we introduce the hybrid clouds, which are needed to determine which task to be outsourced and to what cloud provider. These choices ought to minimize the expense of running an allotment of the aggregate task on one or various public cloud providers while considering the application prerequisites, e.g. deadline constraints and data requirements. In the hybridization approach (HGSOCBGA), each dimension of a solution represents a task and the solution as a whole signifies all the task priorities. The vital issue is how to allocate the user tasks to exploit the profit of the infrastructure as a service (IaaS) provider while promising the quality of service (QoS). The generated solution proficiently assures the user-level QoS and improves the IaaS providers' credibility and economic benefit. The HGSOCBGA method also designs the hybridization process and suitable fitness function of the corresponding task. According to the evolved results, it has been found that our algorithm always outperforms the traditional algorithms.																	0334-1860	2191-026X				JAN	2020	29	1					53	70		10.1515/jisys-2017-0388													
J								Efficient Classification of DDoS Attacks Using an Ensemble Feature Selection Algorithm	JOURNAL OF INTELLIGENT SYSTEMS										Entropy; DDoS; classifier; MLP; information gain; chi-square; ensemble algorithm; feature selection		In the current cyber world, one of the most severe cyber threats are distributed denial of service (DDoS) attacks, which make websites and other online resources unavailable to legitimate clients. It is different from other cyber threats that breach security parameters; however, DDoS is a short-term attack that brings down the server temporarily. Appropriate selection of features plays a crucial role for effective detection of DDoS attacks. Too many irrelevant features not only produce unrelated class categories but also increase computation overhead. In this article, we propose an ensemble feature selection algorithm to determine which attribute in the given training datasets is efficient in categorizing the classes. The result of the ensemble algorithm when compared to a threshold value will enable us to decide the features. The selected features are deployed as training inputs for various classifiers to select a classifier that yields maximum accuracy. We use a multilayer perceptron classifier as the final classifier, as it provides better accuracy when compared to other conventional classification models. The proposed method classifies the new datasets into either attack or normal classes with an efficiency of 98.3% and also reduces the overall computation time. We use the CAIDA 2007 dataset to evaluate the performance of the proposed method using MATLAB and Weka 3.6 simulators.																	0334-1860	2191-026X				JAN	2020	29	1					71	83		10.1515/jisys-2017-0472													
J								Distributed Multi-agent Bidding-Based Approach for the Collaborative Mapping of Unknown Indoor Environments by a Homogeneous Mobile Robot Team	JOURNAL OF INTELLIGENT SYSTEMS										Distributed multi-agent architecture; collaborative mapping; geometric map; mobile robot team; Player/Stage simulator	TASK ALLOCATION; EXPLORATION	This paper deals with the problem of the collaborative mapping of unknown indoor environments by a homogeneous mobile robot team. For this aim, a distributed multi-agent coordination approach is proposed for the mapping process to offer a global view of the entire environment. First, the scheme starts by assigning the most suitable robots to the different zones of the environment to be mapped based on a bidding strategy. Then, while a Robot agent of the group explores its local surroundings and collects information about its neighborhood, it sends mapping data to the Human/Machine Interface agent to integrate them into a single global map. Furthermore, a geometric map representation and an algorithm based on obstacles and environment limits detection are used to provide an explicitly geometric representation of the workspace. For validation purposes, Player/Stage simulator is used to show the effectiveness of the proposed distributed approach and algorithms without needing a real multi-robot system and environment. Finally, various scenarios have been carried out and results are compared in terms of (i) required mapping time, (ii) accuracy of the global generated map, and (iii) number of exchanged messages between the agents.																	0334-1860	2191-026X				JAN	2020	29	1					84	99		10.1515/jisys-2017-0255													
J								An Efficient Technique for Three-Dimensional Image Visualization Through Two-Dimensional Images for Medical Data	JOURNAL OF INTELLIGENT SYSTEMS										3D image visualization; 3D volume; 3D modeling; image reconstruction; rendering		The main idea behind this work is to present three-dimensional (3D) image visualization through two-dimensional (2D) images that comprise various images. 3D image visualization is one of the essential methods for excerpting data from given pieces. The main goal of this work is to figure out the outlines of the given 3D geometric primitives in each part, and then integrate these outlines or frames to reconstruct 3D geometric primitives. The proposed technique is very useful and can be applied to many kinds of images. The experimental results showed a very good determination of the reconstructing process of 2D images.																	0334-1860	2191-026X				JAN	2020	29	1					100	109		10.1515/jisys-2017-0315													
J								Combined Multi-Agent Method to Control Inter-Department Common Events Collision for University Courses Timetabling	JOURNAL OF INTELLIGENT SYSTEMS										University course timetabling; imperialist competitive algorithm; genetic algorithm	SYSTEM; OPTIMIZATION; MODEL	University course timetabling is the scheduling of courses at different time slots in a university. The two important issues in this process are (i) the allocation of all events (professors, courses, and students) to resources (time slots daily/weekly and theory/practical classes) in a semester, and (ii) maximizing the satisfaction of common events (professors, courses, and students) among multiple departments. Accumulating evidences in university course timetabling problems suggest dividing the problem into several sub-problems. This study attempted to investigate the appropriateness of using the genetic algorithm (GA) and the imperialist competitive algorithm (ICA). The proposed technique consists of two steps: (i) using the proposed manipulated GA for solving the timetabling problem of each department, and (ii) eliminating the interference of common events among multiple departments and satisfying the hard and soft constraints by using ICA. Finally, a report on the efficiency of the methodology used in this study was obtained from the University of Tabriz in Iran and University of Udine in Italy. In this paper, the results are revealed in two ways: (i) reduction in the problems due to shrinking of the database and solving of the problems in parallel and (ii) solving the different parts of the problem by using various criterion results, increasing the common events satisfaction in that sub-problem. Eventually, the proposed model provided successful satisfaction of the hard constraints in <700 iterations with GA and elimination of interference in 40 iterations with ICA in most of the cases.																	0334-1860	2191-026X				JAN	2020	29	1					110	126		10.1515/jisys-2017-0249													
J								An Improved Particle Swarm Optimization Algorithm for Global Multidimensional Optimization	JOURNAL OF INTELLIGENT SYSTEMS										Particle swarm optimization; swarm intelligence; global optimization; multidimensional functions; collaborative learning	CONSTRAINED OPTIMIZATION; DIFFERENTIAL EVOLUTION; HYBRIDIZATION; STABILITY; DESIGN; POWER	This paper introduces a new variant of the particle swarm optimization (PSO) algorithm, designed for global optimization of multidimensional functions. The goal of this variant, called ImPSO, is to improve the exploration and exploitation abilities of the algorithm by introducing a new operation in the iterative search process. The use of this operation is governed by a stochastic rule that ensures either the exploration of new regions of the search space or the exploitation of good intermediate solutions. The proposed method is inspired by collaborative human learning and uses as a starting point a basic PSO variant with constriction factor and velocity clamping. Simulation results that show the ability of ImPSO to locate the global optima of multidimensional functions are presented for 10 well-know benchmark functions from CEC-2013 and CEC-2005. These results are compared with the PSO variant used as starting point, three other PSO variants, one of which is based on human learning strategies, and three alternative evolutionary computing methods.																	0334-1860	2191-026X				JAN	2020	29	1					127	142		10.1515/jisys-2017-0104													
J								A Kernel Probabilistic Model for Semi-supervised Co-clustering Ensemble	JOURNAL OF INTELLIGENT SYSTEMS										Co-clustering; co-cluster ensemble; semi-supervised learning; kernel probabilistic model; recommend system		Co-clustering is used to analyze the row and column clusters of a dataset, and it is widely used in recommendation systems. In general, different co-clustering models often obtain very different results for a dataset because each algorithm has its own optimization criteria. It is an alternative way to combine different co-clustering results to produce a final one for improving the quality of co-clustering. In this paper, a semi-supervised co-clustering ensemble is illustrated in detail based on semi-supervised learning and ensemble learning. A semi-supervised co-clustering ensemble is a framework for combining multiple base co-clusterings and the side information of a dataset to obtain a stable and robust consensus co-clustering. First, the objective function of the semi-supervised co-clustering ensemble is formulated according to normalized mutual information. Then, a kernel probabilistic model for semi-supervised co-clustering ensemble (KPMSCE) is presented and the inference of KPMSCE is illustrated in detail. Furthermore, the corresponding algorithm is designed. Moreover, different algorithms and the proposed algorithm are used for experiments on real datasets. The experimental results demonstrate that the proposed algorithm can significantly outperform the compared algorithms in terms of several indices.																	0334-1860	2191-026X				JAN	2020	29	1					143	153		10.1515/jisys-2017-0513													
J								Pythagorean Hesitant Fuzzy Information Aggregation and Their Application to Multi-Attribute Group Decision-Making Problems	JOURNAL OF INTELLIGENT SYSTEMS										Pythagorean hesitant fuzzy set; PHFWA operator; PHFWG operator; PHFOWA operator; PHFOWG operator; group decision making	MEMBERSHIP GRADES; SETS; TOPSIS	In this paper, we introduce the concept of the Pythagorean hesitant fuzzy set (PHFS), which is the generalization of the intuitionistic hesitant fuzzy set under the restriction that the square sum of its membership degrees is <= 1. In decision making with PHFSs, aggregation operators play a key role because they can be used to synthesize multidimensional evaluation values represented as Pythagorean hesitant fuzzy values into collective values. Under PlIFS environments, Pythagorean hesitant fuzzy ordered weighted averaging and Pythagorean fuzzy ordered weighted geometric operators are used to aggregate the Pythagorean hesitant fuzzy values. The main advantage of these operators is that they provide more accurate and valuable results. Furthermore, these operators are applied to decision-making problems in which experts provide their preferences in the Pythagorean hesitant fuzzy environment to show the validity, practicality, and effectiveness of the new approach. Finally, we compare the proposed approach to the existing methods.																	0334-1860	2191-026X				JAN	2020	29	1					154	171		10.1515/jisys-2017-0231													
J								Using an Efficient Optimal Classifier for Soil Classification in Spatial Data Mining Over Big Data	JOURNAL OF INTELLIGENT SYSTEMS										MapReduce framework; principal component analysis; neural network; grey wolf optimization; accuracy; precision; recall; F-measure		This article proposes an effectual process for soil classification. The input data of the proposed procedure is the Harmonized World Soil Database. Preprocessing aids to generate enhanced representation and will use minimum time. Then, the MapReduce framework divides the input dataset into a complimentary portion that is held by the map task. In the map task, principal component analysis is used to reduce the data and the outputs of the maps are then contributed to reduce the tasks. Lastly, the proposed process is employed to categorize the soil kind by means of an optimal neural network (NN) classifier. Here, the conventional NN is customized using the optimization procedure. In an NN, the weights are optimized using the grey wolf optimization (GWO) algorithm. Derived from the classifier, we categorize the soil category. The performance of the proposed procedure is assessed by means of sensitivity, specificity, accuracy, precision, recall, and F-measure. The analysis results illustrate that the recommended artificial NN-GWO process has an accuracy of 90.46%, but the conventional NN and k-nearest neighbor classifiers have an accuracy value of 75.3846% and 75.38%, respectively, which is the least value compared to the proposed procedure. The execution is made by Java within the MapReduce framework using Hadoop.																	0334-1860	2191-026X				JAN	2020	29	1					172	188		10.1515/jisys-2017-0209													
J								A Bayesian Multiresolution Approach for Noise Removal in Medical Magnetic Resonance Images	JOURNAL OF INTELLIGENT SYSTEMS										Modeling of wavelet coefficients; MRI image; de-noising; Bayesian estimator	MAXIMUM-LIKELIHOOD-ESTIMATION; WAVELET; MRI; REDUCTION; TRANSFORM	A Bayesian approach using wavelet coefficient modeling is proposed for de-noising additive white Gaussian noise in medical magnetic resonance imaging (MRI). In a parallel acquisition process, the magnetic resonance image is affected by white Gaussian noise, which is additive in nature. A normal inverse Gaussian probability distribution function is taken for modeling the wavelet coefficients. A Bayesian approach is implemented for filtering the noisy wavelet coefficients. The maximum likelihood estimator and median absolute deviation estimator are used to find the signal parameters, signal variances, and noise variances of the distribution. The minimum mean square error estimator is used for estimating the true wavelet coefficients. The proposed method is simulated on MRI. Performance and image quality parameters show that the proposed method has the capability to reduce the noise more effectively than other state-of-the-art methods. The proposed method provides 8.83%, 2.02%, 6.61%, and 30.74% improvement in peak signal-to-noise ratio, structure similarity index, Pratt's figure of merit, and Bhattacharyya coefficient, respectively, over existing well-accepted methods. The effectiveness of the proposed method is evaluated by using the mean squared difference (MSD) parameter. MSD shows the degree of dissimilarity and is 0.000324 for the proposed method, which is less than that of the other existing methods and proves the effectiveness of the proposed method. Experimental results show that the proposed method is capable of achieving better signal-to-noise ratio performance than other tested de-noising methods.																	0334-1860	2191-026X				JAN	2020	29	1					189	201		10.1515/jisys-2017-0402													
J								Gbest-Guided Artificial Bee Colony Optimization Algorithm-Based Optimal Incorporation of Shunt Capacitors in Distribution Networks under Load Growth	JOURNAL OF INTELLIGENT SYSTEMS										Optimal capacitor placement; IPSO; GABC algorithm; LSF; IVS; PLI; IVM; load growth	RADIAL-DISTRIBUTION SYSTEMS; VOLTAGE STABILITY; PLACEMENT; ALLOCATION; COMPENSATION; LOCATION; COST	In this work, a new technique is introduced for optimal incorporation of shunt capacitors (SCs) in distribution networks. This technique has been compared to other sensitivity-based approaches such as loss sensitivity factor, index vector method, power loss index, and index of voltage stability. In the proposed technique, the optimal positions as well as the ratings of SCs are identified through an optimization algorithm. In sensitivity-based approaches, the positions of SCs are determined through a sensitivity approach and the optimal ratings of SCs are computed through an optimization algorithm. The main target of this study is to minimize the total annual cost and power loss of the network under load growth. This has been done through the population-based Gbest-guided artificial bee colony (GABC) optimization technique. Furthermore, the outcomes obtained through the GABC algorithm are compared to those from the iteration particle swarm optimization algorithm. The whole work along with the proposed methodology has been demonstrated on standard 34-bus and 118-bus distribution networks for SC placement. The results show that it reduces the total annual expense of the network to a great value. Consequently, it improves the total power loss reduction, enhances the voltage profile and power factor, and reduces the total voltage deviation. The obtained numerical outcomes through the proposed technique have been compared with the published literature outcomes to show the viability and superiority of the algorithm.																	0334-1860	2191-026X				JAN	2020	29	1					202	222		10.1515/jisys-2017-0238													
J								Graded Soft Expert Set as a Generalization of Hesitant Fuzzy Set	JOURNAL OF INTELLIGENT SYSTEMS										Hesitant fuzzy set; soft set; soft expert set; graded soft expert set	DECISION-MAKING; INFORMATION FUSION; AGGREGATION; REDUCTION; OPERATORS	Hesitant fuzzy sets play a vital role in decision analysis. Although they have been proved to be a landmark in evaluating information, there are certain deficiencies in their structure. Also, in decision analysis with the aid of hesitant fuzzy sets, the relative importance of the decision makers according to their area of expertise is ignored completely, which may be misleading in some situations. These sorts of issues have been resolved in this work by using graded soft expert (GSE) sets. The proposed structure is a modified form of soft expert sets. Some basic operations have been introduced, and certain laws satisfied by them have carefully been investigated. With the aid of GSE sets, a decision-making algorithm (accompanied with an example) has been developed in which experts have been given due weightage according to their area of expertise.																	0334-1860	2191-026X				JAN	2020	29	1					223	236		10.1515/jisys-2016-0312													
J								Universal Liver Extraction Algorithm: An Improved Chan-Vese Model	JOURNAL OF INTELLIGENT SYSTEMS										Liver segmentation; Chan-Vese model; histogram-based threshold segmentation; computerized tomography; magnetic resonance image	IMAGE SEGMENTATION; SET	Liver segmentation is important to speed up liver disease diagnosis. It is also useful for detection, recognition, and measurement of objects in liver images. Sufficient work has been carried out until now, but common methodology for segmenting liver image from CT scan, MRI scan, PET scan, etc., is not available. The proposed methodology is an effort toward developing a general algorithm to segment liver image from abdominal computerized tomography (CT) scan and magnetic resonance imaging (MRI) scan images. In the proposed algorithm, pixel intensity range of the liver portion is obtained by cropping a random section of the liver. Using its histogram, threshold values are calculated. Further, threshold-based segmentation is performed, which separates liver from abdominal CT scan image/abdominal MRI scan image. Noise in the liver image is reduced using median filter, and the quality of the image is improved by sigmoidal function. The image is then converted into binary image. The Chan-Vese (C-V) model demands an initial contour, which evolves outward. A novel algorithm is proposed to identify the initial contour inside the liver without user intervention. This initial contour propagates outward and continues until the boundary of the liver is identified accurately. This process terminates by itself when the entire boundary of the liver is detected. The method has been validated on CT images and MRI images. Results on the variety of images are compared with existing algorithms, which reveal its robustness, effectiveness, and efficiency.																	0334-1860	2191-026X				JAN	2020	29	1					237	250		10.1515/jisys-2017-0362													
J								Software Effort Estimation Using Modified Fuzzy C Means Clustering and Hybrid ABC-MCS Optimization in Neural Network	JOURNAL OF INTELLIGENT SYSTEMS										Cost estimation; neural network; clustering; fuzzy		In a software development process, effective cost estimation is the most challenging activity. Software effort estimation is a crucial part of cost estimation. Management cautiously considers the efforts and benefits of software before committing the required resources to that project or order for a contract. Unfortunately, it is difficult to measure such preliminary estimation, as it has only little information about the project at an early stage. In this paper, a new approach is proposed; this is based on reasoning by the soft computing approach to calculate the effort estimation of the software. In this approach, rules are generated based on the input dataset. These rules are then clustered for better estimation. In our proposed method, we use modified fuzzy C means for clustering the dataset. Once the clustering is done, various rules are obtained and these rules are given as the input to the neural network. Here, we modify the neural network by incorporating optimization algorithms. The optimization algorithms employed here are the artificial bee colony (ABC), modified cuckoo search (MCS), and hybrid ABC-MCS algorithms. Hence, we obtain three optimized sets of rules that are used for the effort estimation process. The performance of our proposed method is investigated using parameters such as the mean absolute relative error and mean magnitude of relative error.																	0334-1860	2191-026X				JAN	2020	29	1					251	263		10.1515/jisys-2017-0121													
J								Handwritten Indic Script Recognition Based on the Dempster-Shafer Theory of Evidence	JOURNAL OF INTELLIGENT SYSTEMS										Handwritten script recognition; Dempster-Shafer theory; Indic scripts; gray-level co-occurrence matrix; Gabor wavelet transform	CLASSIFIER COMBINATION; IDENTIFICATION; FEATURES; TEXTURE; BANGLA	In a multilingual country like India, script recognition is an important pre-processing footstep necessary for feeding any document to an optical character recognition (OCR) engine, which is, in general, script specific. The present work evaluates the performance of an ensemble of two MLP (multi-layer perceptron) classifiers, each trained on different feature sets. Here, two complementary sets of features, namely, gray-level co-occurrence matrix (GLCM) and Gabor wavelets transform coefficients are extracted from each of the handwritten text-line and word images written in 12 official scripts used in Indian subcontinent, which are then fed into an individual classifier. In order to improve the overall recognition rate, a powerful combination approach based on the Dempster-Shafer (DS) theory is finally employed to fuse the decisions of two MLP classifiers. The performance of the combined decision is compared with those of the individual classifiers, and it is noted that a significant improvement in recognition accuracy (about 4% for text-line data and 6% for word level data) has been achieved by the proposed methodology.																	0334-1860	2191-026X				JAN	2020	29	1					264	282		10.1515/jisys-2017-0431													
J								An Integrated Intuitionistic Fuzzy AHP and TOPSIS Approach to Evaluation of Outsource Manufacturers	JOURNAL OF INTELLIGENT SYSTEMS										Outsourcing; interval-valued intuitionistic fuzzy sets; multi-criteria decision-making; outsource manufacturer; TOPSIS	LINGUISTIC AGGREGATION OPERATORS; REVERSE LOGISTICS PROVIDER; DECISION-MAKING APPROACH; MCDM APPROACH; SUPPLIER EVALUATION; VENDOR SELECTION; MODEL; PRIORITIZATION; TECHNOLOGY; PROMETHEE	Outsourcing is the action of contracting a specific task, function, or process to an external company instead of using an organisation's resources. The history of outsourcing goes back to the 1980s when it was used for cost reduction in non-core business operations. Over time, outsourcing has moved to more strategic areas and has become an important factor in business performance. The selection of the best alternative among alternative outsource manufacturers is a multi-criteria decision-making problem. In this study, the fuzzy set theory is used to capture the uncertainty embedded into the decision problem. In this paper, an interval-valued intuitionistic fuzzy Analytic Hierarchy Process and Technique for Order of Preference by Similarity to Ideal Solution-based methodology is proposed, and an application is provided for the evaluation of outsource manufacturers.																	0334-1860	2191-026X				JAN	2020	29	1					283	297		10.1515/jisys-2017-0363													
J								Automatically Assess Day Similarity Using Visual Lifelogs	JOURNAL OF INTELLIGENT SYSTEMS										Lifelogging; day similarity; similarity measure; EDUB; DTW	OF-THE-ART; TIME; ALGORITHMS; RETRIEVAL; VIDEO	Today, we witness the appearance of many lifelogging cameras that are able to capture the life of a person wearing the camera and which produce a large number of images everyday. Automatically characterizing the experience and extracting patterns of behavior of individuals from this huge collection of unlabeled and unstructured egocentric data present major challenges and require novel and efficient algorithmic solutions. The main goal of this work is to propose a new method to automatically assess day similarity from the lifelogging images of a person. We propose a technique to measure the similarity between images based on the Swain's distance and generalize it to detect the similarity between daily visual data. To this purpose, we apply the dynamic time warping (DTW) combined with the Swain's distance for final day similarity estimation. For validation, we apply our technique on the Egocentric Dataset of University of Barcelona (EDUB) of 4912 daily images acquired by four persons with preliminary encouraging results. Methods: The search strategy was designed for high sensitivity over precision, to ensure that no relevant studies were lost. We performed a systematic review of the literature using academic databases (ACM, Scopus, etc.) focusing on themes of day similarity, automatically assess day similarity, assess day similarity on EDUB, and assess day similarity using visual lifelogs. The study included randomized controlled trials, cohort studies, and case-control studies published between 2006 and 2017.																	0334-1860	2191-026X				JAN	2020	29	1					298	310		10.1515/jisys-2017-0364													
J								A Novel Bio-Inspired Algorithm Based on Social Spiders for Improving Performance and Efficiency of Data Clustering	JOURNAL OF INTELLIGENT SYSTEMS										Data clustering; Information processing systems; Swarm intelligence; Social spider optimization; K means clustering	SWARM; BEHAVIOR	Since the last decade, the collective intelligent behavior of groups of animals, birds or insects have attracted the attention of researchers. Swarm intelligence is the branch of artificial intelligence that deals with the implementation of intelligent systems by taking inspiration from the collective behavior of social insects and other societies of animals. Many meta-heuristic algorithms based on aggregative conduct of swarms through complex interactions with no supervision have been used to solve complex optimization problems. Data clustering organizes data into groups called clusters, such that each cluster has similar data. It also produces clusters that could be disjoint. Accuracy and efficiency are the important measures in data clustering. Several recent studies describe bio-inspired systems as information processing systems capable of some cognitive ability. However, existing popular bio-inspired algorithms for data clustering ignored good balance between exploration and exploitation for producing better clustering results. In this article, we propose a bio-inspired algorithm, namely social spider optimization (SSO), for clustering that maintains a good balance between exploration and exploitation using female and male spiders, respectively. We compare results of the proposed algorithm SSO with K means and other nature-inspired algorithms such as particle swarm optimization (PSO), ant colony optimization (ACO) and improved bee colony optimization (IBCO). We find it to be more robust as it produces better clustering results. Although SSO solves the problem of getting stuck in the local optimum, it needs to be modified for locating the best solution in the proximity of the generated global solution. Hence, we hybridize SSO with K means, which produces good results in local searches. We compare proposed hybrid algorithms SSO + K means (SSOKC), integrated SSOKC (ISSOKC), and interleaved SSOKC (IISSOKC) with K means + PSO (KPSO), K means + genetic algorithm (KGA), K means +artificial bee colony (KABC) and interleaved K means + IBCO (IKIBCO) and find better clustering results. We use sum of intra-cluster distances (SICD), average cosine similarity, accuracy and inter-cluster distance to measure and validate the performance and efficiency of the proposed clustering techniques.																	0334-1860	2191-026X				JAN	2020	29	1					311	326		10.1515/jisys-2017-0178													
J								Discriminative Training Using Noise Robust Integrated Features and Refined HMM Modeling	JOURNAL OF INTELLIGENT SYSTEMS										Automatic speech recognition; MFCC; GFCC; genetic algorithm; PSO; PLP; discriminative training; MMI; MPE	SPEECH RECOGNITION; OPTIMIZATION	The classical approach to build an automatic speech recognition (ASR) system uses different feature extraction methods at the front end and various parameter classification techniques at the back end. The Mel-frequency cepstral coefficients (MFCC) and perceptual linear prediction (PLP) techniques are the conventional approaches used for many years for feature extraction, and the hidden Markov model (IIMM) has been the most obvious selection for feature classification. however, the performance of MFCC-HMM and PIP-I IMM-based ASR system degrades in real-time environments. The proposed work discusses the implementation of discriminatively trained Hindi ASR system using noise robust integrated features and refined I IMM model. It sequentially combines MFCC with PLP and MFCC with gammatone-frequency cepstral coefficient (GFCC) to obtain MF-PLP and MF-GFCC integrated feature vectors, respectively. The I IMM parameters are refined using genetic algorithm (GA) and particle swarm optimization (PSO). Discriminative training of acoustic model using maximum mutual information (MMI) and minimum phone error (MPE) is preformed to enhance the accuracy of the proposed system. The results show that discriminative training using MPE with MF-GFCC integrated feature vector and PSO-HMM parameter refinement gives significantly better results than the other implemented techniques.																	0334-1860	2191-026X				JAN	2020	29	1					327	344		10.1515/jisys-2017-0618													
J								Self-Adaptive Mussels Wandering Optimization Algorithm with Application for Artificial Neural Network Training	JOURNAL OF INTELLIGENT SYSTEMS										Mussels wandering optimization; self-adaptive; metaheuristic; neural networks; pattern classification	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; PERFORMANCE; MACHINE	The mussels wandering optimization (MWO) is a recent population-based metaheuristic optimization algorithm inspired ecologically by mussels' movement behavior. The MWO has been used successfully for solving several optimization problems. This paper proposes an enhanced version of MWO, known as the enhanced-mussels wandering optimization (E-MWO) algorithm. The E-MWO aims to overcome the MWO shortcomings, such as lack in explorative ability and the possibility to fall in premature convergence. In addition, the E-MWO incorporates the self-adaptive feature for setting the value of a sensitive algorithm parameter. Then, it is adapted for supervised training of artificial neural networks, whereas pattern classification of real-world problems is considered. The obtained results indicate that the proposed method is a competitive alternative in terms of classification accuracy and achieve superior results in training time.																	0334-1860	2191-026X				JAN	2020	29	1					345	363		10.1515/jisys-2017-0292													
J								A Framework for Image Alignment of TerraSAR-X Images Using Fractional Derivatives and View Synthesis Approach	JOURNAL OF INTELLIGENT SYSTEMS										Image alignment; feature detection; feature description; affine camera model; view synthesis; fractional derivatives	SCALE	Conventional integer order differential operators suffer from poor feature detection accuracy and noise immunity, which leads to image misalignment. A new affine-based fractional order feature detection algorithm is proposed to detect syntactic and semantic structures from the backscattered signal of a TerraSAR-X band stripmap image. To further improve the alignment accuracy, we propose to adapt a view synthesis approach in the standard pipeline of feature-based image alignment. Experiments were performed to test the effectiveness and robustness of the view synthesis approach using a fractional order feature detector. The evaluation results showed that the proposed method achieves high precision and robust alignment of look-angle-varied TerraSAR-X images. The affine features detected using the fractional order operator are more stable and have strong capacity to reduce sturdy speckle noise.																	0334-1860	2191-026X				JAN	2020	29	1					364	377		10.1515/jisys-2017-0381													
J								Intelligent Systems for Structural Damage Assessment	JOURNAL OF INTELLIGENT SYSTEMS										Fuzzy inference system (FIS); Mamdani; Sugeno; artificial neural network (ANN); maximum interstory drift ratio (MISDR); structural damage estimation	EMPIRICAL MODE DECOMPOSITION	This research provides a comparative study of intelligent systems in structural damage assessment after the occurrence of an earthquake. Seismic response data of a reinforced concrete structure subjected to 100 different levels of seismic excitation are utilized to study the structural damage pattern described by a well-known damage index, the maximum inter-story drift ratio (MISDR). Through a time-frequency analysis of the accelerograms, a set of seismic features is extracted. The aim of this study is to analyze the performance of three different techniques for the set of the proposed seismic features: an artificial neural network (ANN), a Mamdani-type fuzzy inference system (FIS), and a Sugeno-type FIS. The performance of the models is evaluated in terms of the mean square error (MSE) between the actual calculated and estimated MISDR values derived from the proposed models. All models provide small MSE values. Yet, the ANN model reveals a slightly better performance.																	0334-1860	2191-026X				JAN	2020	29	1					378	392		10.1515/jisys-2017-0193													
J								Some Interval-Valued Pythagorean Fuzzy Einstein Weighted Averaging Aggregation Operators and Their Application to Group Decision Making	JOURNAL OF INTELLIGENT SYSTEMS										Interval-valued Pythagorean fuzzy Einstein weighted averaging operator; interval-valued Pythagorean fuzzy Einstein ordered weighted averaging operator; group decision making	OPERATIONS	In this paper, we introduce the notion of Einstein aggregation operators, such as the interval-valued Pythagorean fuzzy Einstein weighted averaging aggregation operator and the interval-valued Pythagorean fuzzy Einstein ordered weighted averaging aggregation operator. We also discuss some desirable properties, such as idempotency, boundedness, commutativity, and monotonicity. The main advantage of using the proposed operators is that these operators give a more complete view of the problem to the decision makers. These operators provide more accurate and precise results as compared the existing method. Finally, we apply these operators to deal with multiple-attribute group decision making under interval-valued Pythagorean fuzzy information. For this, we construct an algorithm for multiple-attribute group decision making. Lastly, we also construct a numerical example for multiple-attribute group decision making.																	0334-1860	2191-026X				JAN	2020	29	1					393	408		10.1515/jisys-2017-0212													
J								Fuzzy Adaptive Genetic Algorithm for Improving the Solution of Industrial Optimization Problems	JOURNAL OF INTELLIGENT SYSTEMS										Optimization; genetic algorithms; fuzzy inference system; adaptive genetic algorithms; industrial problems	PREDICTION; CROSSOVER; MUTATION	In the industrial and manufacturing fields, many problems require tuning of the parameters of complex models by means of exploitation of empirical data. In some cases, the use of analytical methods for the determination of such parameters is not applicable; thus, heuristic methods are employed. One of the main disadvantages of these approaches is the risk of converging to "suboptimal" solutions. In this article, the use of a novel type of genetic algorithm is proposed to overcome this drawback. This approach exploits a fuzzy inference system that controls the search strategies of genetic algorithm on the basis of the real-time status of the optimization process. In this article, this method is tested on classical optimization problems and on three industrial applications that put into evidence the improvement of the capability of avoiding the local minima and the acceleration of the search process.																	0334-1860	2191-026X				JAN	2020	29	1					409	422		10.1515/jisys-2016-0343													
J								Approach to Multiple Attribute Group Decision Making Based on Hesitant Fuzzy Linguistic Aggregation Operators	JOURNAL OF INTELLIGENT SYSTEMS										Linguistic term; group decision making; operator; hesitant fuzzy linguistic term set	TERM SETS; MODEL	Inspired by the nonlinear weighted average operator, this paper proposes several generalized power average operators to aggregate hesitant fuzzy linguistic decision information. It is worth noting that the new operators take both the location and date weight information and the relative closeness of the decision-making information into consideration, a characteristic that results in objectivity and fairness in a group decision making. Moreover, we demonstrate some useful properties of the operators and discuss their associations. A new approach based on the designed operators is then proposed for hesitant fuzzy linguistic multiple attribute group decision-making problems, in which the attribute weights are known or unknown. Finally, this paper demonstrates the efficiency and feasibility of the proposed method through a numerical example.																	0334-1860	2191-026X				JAN	2020	29	1					423	439		10.1515/jisys-2017-0192													
J								Cubic Ordered Weighted Distance Operator and Application in Group Decision-Making	JOURNAL OF INTELLIGENT SYSTEMS										Cubic ordered weighted distance operator; cubic set; group decision-making problem; ordered weighted distance operator	AGGREGATION OPERATORS; FUZZY AGGREGATION; OWA AGGREGATION; INFORMATION; ENTROPY	Group decision-making is a very useful technique for ranking the group of alternatives. The ordered weighted distance (OWD) operator is a new tool in group decision-making problems. In this paper, we apply the OWD operator on cubic information. We develop a new operator, the so-called cubic OWD (COWD) operator, and study the different properties of it. We also discuss some particular cases of COWD. Finally, we develop a general algorithm for group decision-making problems using the COWD operator and give an application to the group decision-making problem.																	0334-1860	2191-026X				JAN	2020	29	1					440	458		10.1515/jisys-2017-0029													
J								Fault Signal Recognition in Power Distribution System using Deep Belief Network	JOURNAL OF INTELLIGENT SYSTEMS										Classification; Deep belief network; Fault signal; Power distribution system; Wavelet decomposition	FEATURE-SELECTION; CLASSIFICATION; LOCATION; KERNEL; SCHEME	Nowadays, electrical power system is considered as one of the most complicated artificial systems all over the globe, as social and economic development depends on intact, consistent, stable and economic functions. Owing to diverse random causes, accidental failures occur in electrical power systems. Considering this issue, this article aimed to propose the use of deep belief network (DBN) in detecting and classifying fault signals such as transient, sag and swell in the transmission line. Here, wavelet-decomposed fault signals are extracted and the fault is diagnosed based on the decomposed signal by the DBN model. Further, this article provides the performance analysis by determining the types I and II measures and root-mean-square-error (RMSE) measure. In the performance analysis, it compares the performance of the DBN model to various conventional models like linear support vector machine (SVM), quadratic SVM, radial basis function SVM, polynomial SVM, multilayer perceptron SVM, Levenberg-Marquardt neural network and gradient descent neural network models. The simulation results validate that the proposed DBN model effectively detects and classifies the fault signal in power distribution system when compared to the traditional model.																	0334-1860	2191-026X				JAN	2020	29	1					459	474		10.1515/jisys-2017-0499													
J								Selector: PSO as Model Selector for Dual-Stage Diabetes Network	JOURNAL OF INTELLIGENT SYSTEMS										Diabetes classification; RBFNN; PNN; optimal number of clusters; highly dense regions	RADIAL BASIS FUNCTION; NEURAL-NETWORK; CLASSIFICATION; ENSEMBLE; OPTIMIZATION; DISEASE	Diabetes is a chronic disease caused by insulin deficiency, and it should be detected in the early stages for effective treatment. In this paper, the Diabetes-Network (Dia-Net) is proposed to increase diabetes predictive accuracy. The proposed Dia-Net is a dual-stage network. It combines both optimized probabilistic neural network (OPNN) and optimized radial basis function neural network (ORBFNN) in the first stage. Hence, Dia-Net possesses the advantages of both the models. In the second stage, the linear support vector machine is used. As the dataset size increases, both RBFNN and PNN perform better, but both suffers from complexity and computational problems. To address these problems, in this paper, particle swarm optimization-based clustering is employed for discovering centers in high-dense regions. This reduces the size of the hidden layer of both RBFNN and PNNs. Experiments are carried out on the Pima Indians Diabetes dataset. The Experimental results showed that the proposed Dia-Net model outperformed individual as well as state-of-the-art models.																	0334-1860	2191-026X				JAN	2020	29	1					475	484		10.1515/jisys-2017-0394													
J								Oppositional Gravitational Search Algorithm and Artificial Neural Network-based Classification of Kidney Images	JOURNAL OF INTELLIGENT SYSTEMS										Probabilistic principal component analysis (PPCA); oppositional gravitational search algorithm (OGSA); artificial neural network (ANN); k-nearest neighbors (KNN); genetic algorithm-based ANN (GA-ANN)	DISEASE; SEGMENTATION; SOFTWARE	Ultrasound (US) imaging has been broadly utilized as part of kidney diagnosis because of its ability to show structural abnormalities like cysts, stones, and infections as well as information about kidney function. The main aim of this research is to effectively classify normal and abnormal kidney images through US based on the selection of relevant features. In this study, abnormal kidney images were classified through gray-scale conversion, region-of-interest generation, multi-scale wavelet-based Gabor feature extraction, probabilistic principal component analysis-based feature selection and adaptive artificial neural network technique. The anticipated method is executed in the working platform of MATLAB, and the results were analyzed and contrasted. Results show that the proposed approach had 94% accuracy and 100% specificity. In addition, its false-acceptance rate is 0%, whereas that of existing methods is not <27%. This shows the precise prediction level of the proposed approach, compared with that of existing methods.																	0334-1860	2191-026X				JAN	2020	29	1					485	496		10.1515/jisys-2017-0458													
J								Improving Image Search through MKFCM Clustering Strategy-Based Re-ranking Measure	JOURNAL OF INTELLIGENT SYSTEMS										Content-based image retrieval; re-ranking; image retrieval; MKFCM; four-level DWT; query image	FEATURE-SELECTION; RERANKING	The main intention of this research is to develop a novel ranking measure for content-based image retrieval system. Owing to the achievement of data retrieval, most commercial search engines still utilize a text-based search approach for image search by utilizing encompassing textual information. As the text information is, in some cases, noisy and even inaccessible, the drawback of such a recovery strategy is to the extent that it cannot depict the contents of images precisely, subsequently hampering the execution of image search. In order to improve the performance of image search, we propose in this work a novel algorithm for improving image search through a multi-kernel fuzzy c-means (MKFCM) algorithm. In the initial step of our method, images are retrieved using four-level discrete wavelet transform-based features and the MKFCM clustering algorithm. Next, the retrieved images are analyzed using fuzzy c-means clustering methods, and the rank of the results is adjusted according to the distance of a cluster from a query. To improve the ranking performance, we combine the retrieved result and ranking result. At last, we obtain the ranked retrieved images. In addition, we analyze the effects of different clustering methods. The effectiveness of the proposed methodology is analyzed with the help of precision, recall, and F-measures.																	0334-1860	2191-026X				JAN	2020	29	1					497	514		10.1515/jisys-2017-0227													
J								Sparse Decomposition Technique for Segmentation and Compression of Compound Images	JOURNAL OF INTELLIGENT SYSTEMS										Compound images; segmentation; sparse decomposition	SCHEME; BLOCK	Compression of compound records and images can be more cumbersome than the original information since they can be a mix of text, picture and graphics. The principle requirement of the compound record or images is the nature of the compressed data. In this paper, diverse procedures are used under block-based classification to distinguish the compound image segments. The segmentation process starts with separation of the entire image into blocks by spare decomposition technique in smooth blocks and non smooth blocks. Gray wolf-optimization based FCM (fuzzy C-means) algorithm is employed to segment background, text, graphics, images and overlap, which are then individually compressed using adaptive Huffman coding, embedded zero wavelet and H.264 coding techniques. Exploratory outcomes demonstrate that the proposed conspire expands compression ratio, enhances image quality and additionally limits computational complexity. The proposed method is implemented on the working platform of MATLAB.																	0334-1860	2191-026X				JAN	2020	29	1					515	528		10.1515/jisys-2017-0360													
J								Automatic Genetic Fuzzy c-Means	JOURNAL OF INTELLIGENT SYSTEMS										Genetic algorithms; unsupervised learning; fuzzy clustering; evolutionary algorithms; gravitational search; differential evolution	DIFFERENTIAL EVOLUTION; CLUSTER-ANALYSIS; ALGORITHM; OPTIMIZATION	Fuzzy c-means is an efficient algorithm that is amply used for data clustering. Nonetheless, when using this algorithm, the designer faces two crucial choices: choosing the optimal number of clusters and initializing the cluster centers. The two choices have a direct impact on the clustering outcome. This paper presents an improved algorithm called automatic genetic fuzzy c-means that evolves the number of clusters and provides the initial centroids. The proposed algorithm uses a genetic algorithm with a new crossover operator, a new mutation operator, and modified tournament selection; further, it defines a new fitness function based on three cluster validity indices. Real data sets are used to demonstrate the effectiveness, in terms of quality, of the proposed algorithm.																	0334-1860	2191-026X				JAN	2020	29	1					529	539		10.1515/jisys-2018-0063													
J								Harmony Search Algorithm for Patient Admission Scheduling Problem	JOURNAL OF INTELLIGENT SYSTEMS										Harmony search algorithm; optimization; patient admission problem; metaheuristic; patient scheduling problem		The patient admission scheduling (PAS) problem is an optimization problem in which we assign patients automatically to beds for a specific period of time while preserving their medical requirements and their preferences. In this paper, we present a novel solution to the PAS problem using the harmony search (HS) algorithm. We tailor the HS to solve the PAS problem by distributing patients to beds randomly in the harmony memory (HM) while respecting all hard constraints. The proposed algorithm uses five neighborhood strategies in the pitch adjustment stage. This technique helps in increasing the variations of the generated solutions by exploring more solutions in the search space. The PAS standard benchmark datasets are used in the evaluation. Initially, a sensitivity analysis of the HS algorithm is studied to show the effect of its control parameters on the HS performance. The proposed method is also compared with nine methods: non-linear great deluge (NLGD), simulated annealing with hyper-heuristic (HH-SA), improved with equal hyperheuristic (HH-IE), simulated annealing (SA), tabu search (TS), simple random simulated annealing with dynamic heuristic (DHS-SA), simple random improvement with dynamic heuristic (DHS-OI), simple random great deluge with dynamic heuristic (DHS-GD), and biogeography-based optimization (BBO). The proposed HS algorithm is able to produce comparably competitive results when compared with these methods. This proves that the proposed HS is a very efficient alternative to the PAS problem, which can be efficiently used to solve many scheduling problems of a large-scale data.																	0334-1860	2191-026X				JAN	2020	29	1					540	553		10.1515/jisys-2018-0094													
J								Speech Signal Compression Algorithm Based on the JPEG Technique	JOURNAL OF INTELLIGENT SYSTEMS										Data compression; discrete cosine transforms; speech signal coding; speech reconstruction; DCT matrix; discrete cosine transforms		The main objective of this paper is to explore parameters usually adopted by the JPEG method and use them in speech signal compression. Speech compression is the technique of encoding the speech signal in some way that allows the same speech parameters to represent the whole signal. In other words, it is to eliminate redundant features of speech and keep only the important ones for the next stage of speech reproduction. In this paper, the proposed method is to adopt the JPEG scheme, which is usually used in digital image compression and digital speech signal compression. This will open the door to use some methods that are already designed to fit two-dimensional (2D) signals and use them in a 1D signal world. The method includes many priori steps for preparing the speech signal to make it more comparable with the JPEG technique. In order to examine the quality of the compressed data using the JPEG method, different quantization matrices with different compression levels have been employed within the proposed method. Comparison results between the original signal and the reproduced (decompressed) signal show a huge matching. However, different quantization matrices could reduce the quality of the reproduced signal, but in general, it is still within the range of acceptance.																	0334-1860	2191-026X				JAN	2020	29	1					554	564		10.1515/jisys-2018-0127													
J								i-Vector-Based Speaker Verification on Limited Data Using Fusion Techniques	JOURNAL OF INTELLIGENT SYSTEMS										Mel-frequency cepstral coefficient (MFCC); linear prediction cepstral coefficients (LPCC); linear prediction residual (LPR); linear prediction residual phase (LPRP); i-vector; feature-level fusion; score-level fusion	MIXTURE; IDENTIFICATION; INFORMATION; EXTRACTION; FEATURES	In many biometric applications, limited data speaker verification plays a significant role in practical-oriented systems to verify the speaker. The performance of the speaker verification system needs to be improved by applying suitable techniques to limited data condition. The limited data represent both train and test data duration in terms of few seconds. This article shows the importance of the speaker verification system under limited data condition using feature- and score-level fusion techniques. The baseline speaker verification system uses vocal tract features like mel-frequency cepstral coefficients, linear predictive cepstral coefficients and excitation source features like linear prediction residual and linear prediction residual phase as features along with i-vector modeling techniques using the NIST 2003 data set. In feature-level fusion, the vocal tract features are fused with excitation source features. As a result, on average, equal error rate (EER) is approximately equal to 4% compared to individual feature performance. Further in this work, two different types of score-level fusion are demonstrated. In the first case, fusing the scores of vocal tract features and excitation source features at score-level-maintaining modeling technique remains the same, which provides an average reduction approximately equal to 2% EER compared to feature-level fusion performance. In the second case, scores of the different modeling techniques are combined, which has resulted in EER reduction approximately equal to 4.5% compared with score-level fusion of different features.																	0334-1860	2191-026X				JAN	2020	29	1					565	582		10.1515/jisys-2017-0047													
J								Prediction of User Future Request Utilizing the Combination of Both ANN and FCM in Web Page Recommendation	JOURNAL OF INTELLIGENT SYSTEMS										Web page recommendation; K-means clustering; Levenberg-Marquardt; firefly; neural network; classification; prediction		This paper explains about the web page recommendation system. This procedure encompasses consumers' upcoming demand and web page recommendations. In the proposed web page recommendation system, potential and non-potential data can be categorized by use of the Levenberg-Marquardt firefly neural network algorithm, and forecast can be made by using the K-means clustering algorithm. Consequently, the projected representation demonstrates the infrequent contact format with the help of the representation that integrates the comparable consumer access model data that belong to the further consumer. Thereafter, the impending user data are specified to the clustering progression. The third phase of the projected process is collecting potential data with the aid of the improved fuzzy C-means clustering algorithm. The last step of our projected process is envisaging the upcoming demand for the subsequent consumer. The presentation of the projected procedure will be compared to the obtainable procedure.																	0334-1860	2191-026X				JAN	2020	29	1					583	595		10.1515/jisys-2017-0310													
J								Presentation of ACT/R-RBF Hybrid Architecture to Develop Decision Making in Continuous and Non-continuous Data	JOURNAL OF INTELLIGENT SYSTEMS										Cognitive architecture; connectionism; ACT/R model; RBF neural network	NEURAL-NETWORK; KNOWLEDGE; SELECTION	Computational models are based on symbolic architecture. For this reason, computational models function problematically in dynamic, noisy, and continuous environments. The ACT/R (Adaptive Control of Thought-Rational) model is also problematic, as it is purely based on symbolic architecture like other computational models. The ACT/R decision-making process is based on the production operator on the input subject set. This approach firstly does not make a non-linear mapping between input and the decision-making result in ACT/R. Secondly, it is not possible to decide on the input subjects with a continuous input range because of the need to introduce numerous rules. The objective of presenting the ACT/R-radial basis function (RBF) hybrid architecture method was to create a communication network between input concepts in which the reception of and decision making on a combination of subjects and symbols are possible. Moreover, a non-linear mapping between input and the decision-making result can be created. The said capabilities have been obtained by the combination of ACT/R with an RBF neural network and calculation of the decision-making centers in the said network using clustering. The empirical experiments indicate desirable results in this regard.																	0334-1860	2191-026X				JAN	2020	29	1					596	611		10.1515/jisys-2017-0470													
J								An Overview of Segmentation Algorithms for the Analysis of Anomalies on Medical Images	JOURNAL OF INTELLIGENT SYSTEMS										Segmentation; thresholding; Lankton active contour; watershed; atlas-based segmentation	ATLAS-BASED SEGMENTATION; CT IMAGES; NEURAL-NETWORK; MODEL; BRAIN; SNAKES; REGION; LUMEN; NOISE	Human disease identification from the scanned body parts helps medical practitioners make the right decision in lesser time. Image segmentation plays a vital role in automated diagnosis for the delineation of anatomical organs and anomalies. There are many variants of segmentation algorithms used by current researchers, whereas there is no universal algorithm for all medical images. This paper classifies some of the widely used medical image segmentation algorithms based on their evolution, and the features of each generation are also discussed. The comparative analysis of segmentation algorithms is done based on characteristics like spatial consideration, region continuity, computation complexity, selection of parameters, noise immunity, accuracy, and computation time. Finally, in this work, some of the typical segmentation algorithms are implemented on real-time datasets using Matlab 2010 software, and the outcome of this work will be an aid for the researchers in medical image processing.																	0334-1860	2191-026X				JAN	2020	29	1					612	625		10.1515/jisys-2017-0629													
J								Blind Restoration Algorithm Using Residual Measures for Motion-Blurred Noisy Images	JOURNAL OF INTELLIGENT SYSTEMS										Image de-blurring; image priors; point-spread function; edge response; mean square contingency coefficient; holoentropy; whiteness measure; estimation	PARAMETER	Image de-blurring is an inverse problem whose intent is to recover an image from the image affected badly with different environmental conditions. Usually, blurring can happen in various ways; however, de-blurring from a motion problem with or without noise can pose an important problem that is difficult to solve with less computation task. The quality of the restored image in iterative methods of blind motion de-blurring depends on the regularization parameter and the iteration number, which can be automatically or manually stopped. Blind de-blurring and restoration employing image de-blurring and whiteness measures are proposed in this paper to automatically decide the number of iterations. The technique has three modules, namely image de-blurring module, whiteness measures module, and image estimation module. New whiteness measures of hole entropy and mean-square contingency coefficient have been proposed in the whiteness measures module. Initially, the blurred image is de-blurred by the employment of edge responses and image priors using point-spread function. Later, whiteness measures are computed for the de-blurred image and, finally, the best image is selected. The results are obtained for all eight whiteness measures by employing evaluation metrics of increase in signal-to-noise ratio (ISNR), mean-square error, and structural similarity index. The results are obtained from standard images, and performance analysis is made by varying parameters. The obtained results for synthetically blurred images are good even under a noisy condition with Delta ISNR average values of 0.3066 dB. The proposed whiteness measures seek a powerful solution to iterative de-blurring algorithms in deciding automatic stopping criteria.																	0334-1860	2191-026X				JAN	2020	29	1					626	639		10.1515/jisys-2017-0140													
J								Extreme Learning Machine for Credit Risk Analysis	JOURNAL OF INTELLIGENT SYSTEMS										Credit risk analysis; extreme learning machine; naive Bayes; decision tree; multi-layer perceptron		Credit risk analysis is important for financial institutions that provide loans to businesses and individuals. Banks and other financial institutions generally face risks that are mostly of financial nature; hence, such institutions must balance risks and returns. Analyzing or determining risk levels involved in credits, finances, and loans can be performed through predictive analytic techniques, such as an extreme learning machine (ELM). In this work, we empirically evaluated the performance of an ELM for credit risk problems and compared it to naive Bayes, decision tree, and multi-layer perceptron (MLP). The comparison was conducted on the basis of a German credit risk dataset. The simulation results of statistical measures of performance corroborated that the ELM outperforms naive Bayes, decision tree, and MLP classifiers by 1.8248%, 16.6346%, and 5.8934%, respectively.																	0334-1860	2191-026X				JAN	2020	29	1					640	652		10.1515/jisys-2018-0058													
J								A Genetic Algorithm Approach for Group Recommender System Based on Partial Rankings	JOURNAL OF INTELLIGENT SYSTEMS										Group recommender systems; partial ranking; rank aggregation; genetic algorithm; bucket order	AGGREGATION; DISTRUST; TRUST	Many recommender systems frequently make suggestions for group consumable items to the individual users. There has been much work done in group recommender systems (GRSs) with full ranking, but partial ranking (PR) where items are partially ranked still remains a challenge. The ultimate objective of this work is to propose rank aggregation technique for effectively handling the PR problem. Additionally, in real applications, most of the studies have focused on PR without ties (PRWOT). However, the rankings may have ties where some items are placed in the same position, but where some items are partially ranked to be aggregated may not be permutations. In this work, in order to handle problem of PR in GRS for PRWOT and PR with ties (PRWT), we propose a novel approach to GRS based on genetic algorithm (GA) where for PRWOT Spearman foot rule distance and for PRWT Kendall tau distance with bucket order are used as fitness functions. Experimental results are presented that clearly demonstrate that our proposed GRS based on GA for PRWOT (GRS-GA-PRWOT) and PRWT (GRS-GA-PRWT) outperforms well-known baseline GRS techniques.																	0334-1860	2191-026X				JAN	2020	29	1					653	663		10.1515/jisys-2017-0561													
J								Improvements in Spoken Query System to Access the Agricultural Commodity Prices and Weather Information in Kannada Language/Dialects	JOURNAL OF INTELLIGENT SYSTEMS										Noise elimination; IVRS; ASR; accuracy; spoken query system	SPEECH ENHANCEMENT; ESTIMATORS; NOISE	In this paper, the improvements in the recently developed end to end spoken query system to access the agricultural commodity prices and weather information in Kannada language/dialects is demonstrated. The spoken query system consists of interactive voice response system (IVRS) call flow, automatic speech recognition (ASR) models and agricultural commodity prices, and weather information databases. The task specific speech data used in the earlier spoken query system had a high level of background and other types of noises as it is collected from the farmers of Karnataka state (a state in India that speaks the Kannada language) under uncontrolled environment. The different types of noises present in collected speech data had an adverse effect on the on-line and off-line recognition performances. To improve the recognition accuracy in spoken query system, a noise elimination algorithm is proposed in this work, which is a combination of spectral subtraction with voice activity detection (SS-VAD) and minimum mean square error spectrum power estimator based on zero crossing (MMSE-SPZC). The noise elimination algorithm is added in the system before the feature extraction part. In addition to this, alternate acoustic models are developed using subspace Gaussian mixture models (SGMM) and deep neural network (DNN). The experimental results show that these modeling techniques are more powerful than the conventional Gaussian mixture model (GMM) - hidden Markov model (HMM), which was used as a modeling technique for the development of ASR models to design earlier spoken query systems. The fusion of noise elimination technique and SGMM/DNN-based modeling gives a better relative improvement of 7% accuracy compared to the earlier GMM-HMM-based ASR system. The least word error rate (WER) acoustic models could be used in spoken query system. The on-line speech recognition accuracy testing of developed spoken query system (with the help of Karnataka farmers) is also presented in this work.																	0334-1860	2191-026X				JAN	2020	29	1					664	687		10.1515/jisys-2018-0120													
J								A One-Pass Approach for Slope and Slant Estimation of Tri-Script Handwritten Words	JOURNAL OF INTELLIGENT SYSTEMS										Slope; slant; handwritten word; oblique ellipse; eigenvector	SKEW DETECTION; ALGORITHM	Handwritten words can never complement printed words because the former are mostly written in either skewed or slanted form or in both. This very nature of handwriting adds a huge overhead when converting word images into machine-editable format through an optical character recognition system. Therefore, slope and slant corrections are considered as the fundamental pre-processing tasks in handwritten word recognition. For solving this, researchers have followed a two-pass approach where the slope of the word is corrected first and then slant correction is carried out subsequently, thus making the system computationally expensive. To address this issue, we propose a novel one-pass method, based on fitting an oblique ellipse over the word images, to estimate both the slope and slant angles of the same. Furthermore, we have developed three databases considering word images of three popular scripts used in India, namely Bangla, Devanagari, and Roman, along with ground truth information. The experimental results revealed the effectiveness of the proposed method over some state-of-the-art methods used for the aforementioned problem.																	0334-1860	2191-026X				JAN	2020	29	1					688	702		10.1515/jisys-2018-0105													
J								Secure Communication through MultiAgent System-Based Diabetes Diagnosing and Classification	JOURNAL OF INTELLIGENT SYSTEMS										Multi-agent diabetes; advanced encryption standard; elliptic curve cryptography; differential evaluation; multiple kernel support vector machine algorithm	GENE POLYMORPHISMS; TYPE-2; MELLITUS; ASSOCIATION; INSULIN	The main objective of the research is to provide a multi-agent data mining system for diagnosing diabetes. Here, we use multi-agents for diagnosing diabetes such as user agent, connection agent, updation agent, and security agent, in which each agent performs their own task under the coordination of the connection agent. For secure communication, the user symptoms are encrypted with the help of Elliptic Curve Cryptography and Optimal Advanced Encryption Standard. In Optimal Advanced Encryption Standard algorithm, the key values are optimally selected by means of differential evaluation algorithm. After receiving the encrypted data, the suggested method needs to find the diabetes level of the user through multiple kernel support vector machine algorithm. Based on that, the agent prescribes the drugs for the corresponding user. The performance of the proposed technique is evaluated by classification accuracy, sensitivity, specificity, precision, recall, execution time and memory value. The proposed method will be implemented in JAVA platform.																	0334-1860	2191-026X				JAN	2020	29	1					703	718		10.1515/jisys-2017-0353													
J								Development of a Two-Stage Segmentation-Based Word Searching Method for Handwritten Document Images	JOURNAL OF INTELLIGENT SYSTEMS										Word searching; HOG feature; topological feature; holistic word recognition; handwritten documents; QUWI database	RECOGNITION; RETRIEVAL; DATASET; QUWI	Word searching or keyword spotting is an important research problem in the domain of document image processing. The solution to the said problem for handwritten documents is more challenging than for printed ones. In this work, a two-stage word searching schema is introduced. In the first stage, all the irrelevant words with respect to a search word are filtered out from the document page image. This is carried out using a zonal feature vector, called pre-selection feature vector, along with a rule-based binary classification method. In the next step, a holistic word recognition paradigm is used to confirm a pre-selected word as search word. To accomplish this, a modified histogram of oriented gradients-based feature descriptor is combined with a topological feature vector. This method is experimented on a QUWI English database, which is freely available through the International Conference on Document Analysis and Recognition 2015 competition entitled "Writer Identification and Gender Classification." This technique not only provides good retrieval performance in terms of recall, precision, and F-measure scores, but it also outperforms some state-of-the-art methods.																	0334-1860	2191-026X				JAN	2020	29	1					719	735		10.1515/jisys-2017-0384													
J								Pythagorean Fuzzy Einstein Hybrid Averaging Aggregation Operator and its Application to Multiple-Attribute Group Decision Making	JOURNAL OF INTELLIGENT SYSTEMS										Pythagorean fuzzy set; PFEHA aggregation operator; multiple-attribute group decision-making problem		Pythagorean fuzzy set is one of the successful extensions of the intuitionis tic fuzzy set for handling uncertainties in information. Under this environment, in this paper, we introduce the notion of Pythagorean fuzzy Einstein hybrid averaging (PFEHA) aggregation operator along with some of its properties, namely idempotency, boundedness, and monotonicity. PFEHA aggregation operator is the generalization of Pythagorean fuzzy Einstein weighted averaging aggregation operator and Pythagorean fuzzy Einstein ordered weighted averaging aggregation operator. The operator proposed in this paper provides more accurate and precise results as compared to the existing operators. Therefore, this method plays a vital role in real-world problems. Finally, we applied the proposed operator and method to multiple-attribute group decision making.																	0334-1860	2191-026X				JAN	2020	29	1					736	752		10.1515/jisys-2018-0071													
J								Ensembles of Text and Time-Series Models for Automatic Generation of Financial Trading Signals from Social Media Content	JOURNAL OF INTELLIGENT SYSTEMS										Computational finance; machine learning; natural language processing; sentiment analysis; time-series classification; stock trading	EVENT	Event studies in finance have focused on traditional news headlines to assess the impact an event has on a traded company. The increased proliferation of news and information produced by social media content has disrupted this trend. Although researchers have begun to identify trading opportunities from social media platforms, such as Twitter, almost all techniques use a general sentiment from large collections of tweets. Though useful, general sentiment does not provide an opportunity to indicate specific events worthy of affecting stock prices. This work presents an event clustering algorithm, utilizing natural language processing techniques to generate newsworthy events from Twitter, which have the potential to influence stock prices in the same manner as traditional news headlines. The event clustering method addresses the effects of pre-news and lagged news, two peculiarities that appear when connecting trading and news, regardless of the medium. Pre-news signifies a finding where stock prices move in advance of a news release. Lagged news refers to follow-up or late-arriving news, adding redundancy in making trading decisions. For events generated by the proposed clustering algorithm, we incorporate event studies and machine learning to produce an actionable system that can guide trading decisions. The recommended prediction algorithms provide investing strategies with profitable risk-adjusted returns. The suggested language models present annualized Sharpe ratios (risk-adjusted returns) in the 5-11 range, while time-series models produce in the 2-3 range (without transaction costs). The distribution of returns confirms the encouraging Sharpe ratios by identifying most outliers as positive gains. Additionally, machine learning metrics of precision, recall, and accuracy are discussed alongside financial metrics in hopes of bridging the gap between academia and industry in the field of computational finance.																	0334-1860	2191-026X				JAN	2020	29	1					753	772		10.1515/jisys-2017-0567													
J								A Flame Detection Method Based on Novel Gradient Features	JOURNAL OF INTELLIGENT SYSTEMS										Flame detection; SVM; decision tree; PCA; gradient features	FIRE DETECTION; PROBABILISTIC APPROACH	In this study, we present a novel approach to efficiently detect the flame in multiple scenes in an image. The method uses a set of parametric representation named as Gradient Features (GF), to learn the features of flame color changes in the image. Different from the traditional color features of the flame, GF represents the color changes in RGB channels for further consideration. In this study, support vector machine was applied to generate a set of candidate regions and the decision tree model was used to judge flame regions based on GF. Some exclusive experiments were conducted to verify the validity and effectiveness of the proposed method. The results showed that the proposed method can accurately differentiate between yellow color light and sunrise scenes. A comparison with the state-of-the-art preceding methods showed that this method can utilize the symmetry of flame regions and achieve a better result.																	0334-1860	2191-026X				JAN	2020	29	1					773	786		10.1515/jisys-2017-0562													
J								Modeling and Optimization of a Liquid Flow Process using an Artificial Neural Network-Based Flower Pollination Algorithm	JOURNAL OF INTELLIGENT SYSTEMS										Process control; optimization; neural network; flower pollination algorithm	COMPUTATIONAL INTELLIGENCE; HYBRID	Controlling liquid flow is one of the most important parameters in the process control industry. It is challenging to optimize the liquid flow rate for its highly nonlinear nature. This paper proposes a model of liquid flow processes using an artificial neural network (NN) and optimizes it using a flower pollination algorithm (FPA) to avoid local minima and improve the accuracy and convergence speed. In the first phase, the NN model was trained by the dataset obtained from the experiments, which were carried out. In these conditions, the liquid flow rate was measured at different sensor output voltages, pipe diameter and liquid conductivity. The model response was cross-verified with the experimental results and found to be satisfactory. In the second phase of work, the optimized conditions of sensor output voltages, pipe diameter and liquid conductivity were found to give the minimum flow rate of the process using FPA. After cross-validation and testing subdatasets, the accuracy was nearly 94.17% and 99.25%, respectively.																	0334-1860	2191-026X				JAN	2020	29	1					787	798		10.1515/jisys-2018-0206													
J								Spectral Graph-based Features for Recognition of Handwritten Characters: A Case Study on Handwritten Devanagari Numerals	JOURNAL OF INTELLIGENT SYSTEMS										Writing styles; unconstrained cursiveness; primitive relationships; feature representation; graph representation; spectral graph embedding	NORMALIZED CUTS; BANGLA; DATABASES	Interpretation of different writing styles, unconstrained cursiveness and relationship between different primitive parts is an essential and challenging task for recognition of handwritten characters. As feature representation is inadequate, appropriate interpretation/description of handwritten characters seems to be a challenging task. Although existing research in handwritten characters is extensive, it still remains a challenge to get the effective representation of characters in feature space. In this paper, we make an attempt to circumvent these problems by proposing an approach that exploits the robust graph representation and spectral graph embedding concept to characterise and effectively represent handwritten characters, taking into account writing styles, cursiveness and relationships. For corroboration of the efficacy of the proposed method, extensive experiments were carried out on the standard handwritten numeral Computer Vision Pattern Recognition, Unit of Indian Statistical Institute Kolkata dataset. The experimental results demonstrate promising findings, which can be used in future studies.																	0334-1860	2191-026X				JAN	2020	29	1					799	813		10.1515/jisys-2017-0448													
J								A Grey Wolf Optimizer for Text Document Clustering	JOURNAL OF INTELLIGENT SYSTEMS										Text document clustering; optimization; metaheuristic; grey wolf optimizer; swarm intelligence	ALGORITHM	Text clustering problem (TCP) is a leading process in many key areas such as information retrieval, text mining, and natural language processing. This presents the need for a potent document clustering algorithm that can be used effectively to navigate, summarize, and arrange information to congregate large data sets. This paper encompasses an adaptation of the grey wolf optimizer (GWO) for TCP, referred to as TCP-GWO. The TCP demands a degree of accuracy beyond that which is possible with metaheuristic swarm-based algorithms. The main issue to be addressed is how to split text documents on the basis of GWO into homogeneous clusters that are sufficiently precise and functional. Specifically, TCP-GWO, or referred to as the document clustering algorithm, used the average distance of documents to the cluster centroid (ADDC) as an objective function to repeatedly optimize the distance between the clusters of the documents. The accuracy and efficiency of the proposed TCP-GWO was demonstrated on a sufficiently large number of documents of variable sizes, documents that were randomly selected from a set of six publicly available data sets. Documents of high complexity were also included in the evaluation process to assess the recall detection rate of the document clustering algorithm. The experimental results for a test set of over a part of 1300 documents showed that failure to correctly cluster a document occurred in less than 20% of cases with a recall rate of more than 65% for a highly complex data set. The high F-measure rate and ability to cluster documents in an effective manner are important advances resulting from this research. The proposed TCP-GWO method was compared to the other well-established text clustering methods using randomly selected data sets. Interestingly, TCP-GWO outperforms the comparative methods in terms of precision, recall, and F-measure rates. In a nutshell, the results illustrate that the proposed TCP-GWO is able to excel compared to the other comparative clustering methods in terms of measurement criteria, whereby more than 55% of the documents were correctly clustered with a high level of accuracy.																	0334-1860	2191-026X				JAN	2020	29	1					814	830		10.1515/jisys-2018-0194													
J								Classification of Masses in Digital Mammograms Using the Genetic Ensemble Method	JOURNAL OF INTELLIGENT SYSTEMS										Digital mammography; decision Tree; feature selection; classification; genetic algorithm; ensembles; AdaBoost; Random Forest; receiver operating characteristics curve	FEATURE-SELECTION; DATABASE	All over the world, breast cancer is the second leading cause of death in women above 40 years of age. To design an efficient classification system for breast cancer diagnosis, one has to use efficient algorithms for feature selection to reduce the feature space of mammogram classification. The current work investigates the use of hybrid genetic ensemble method for feature selection and classification of masses. Genetic algorithm (GA) is used to select a subset of features and to evaluate the fitness of the selected features, Adaptive boosting (AdaBoost) and Random Forest (RF) ensembles with 10-fold cross-validation are employed. The selected features are used to classify masses into benign or malignant using AdaBoost, RF, and single Decision Tree (DT) classifiers. The performance evaluation of classifiers indicates that AdaBoost outperforms both RF and single DT classifiers. AdaBoost achieves an accuracy of 96.15%, with 97.32% sensitivity, 95.90% specificity, and area under curve of A(z) = 0.982 +/- 0.004. The results obtained with the proposed method are better when compared with extant research work.																	0334-1860	2191-026X				JAN	2020	29	1					831	845		10.1515/jisys-2018-0091													
J								A Hybrid Grey Wolf Optimiser Algorithm for Solving Time Series Classification Problems	JOURNAL OF INTELLIGENT SYSTEMS										Classification problems; probabilistic neural networks; grey wolf optimiser; time series classification	NEURAL-NETWORK	One of the major objectives of any classification technique is to categorise the incoming input values based on their various attributes. Many techniques have been described in the literature, one of them being the probabilistic neural network (PNN). There were many comparisons made between the various published techniques depending on their precision. In this study, the researchers investigated the search capability of the grey wolf optimiser (GWO) algorithm for determining the optimised values of the PNN weights. To the best of our knowledge, we report for the first time on a GWO algorithm along with the PNN for solving the classification of time series problem. PNN was used for obtaining the primary solution, and thereby the PNN weights were adjusted using the GWO for solving the time series data and further decreasing the error rate. In this study, the main goal was to investigate the application of the GWO algorithm along with the PNN classifier for improving the classification precision and enhancing the balance between exploitation and exploration in the GWO search algorithm. The hybrid GWO-PNN algorithm was used in this study, and the results obtained were compared with the published literature. The experimental results for six benchmark time series datasets showed that this hybrid GWO-PNN outperformed the PNN algorithm for the studied datasets. It has been seen that hybrid classification techniques are more precise and reliable for solving classification problems. A comparison with other algorithms in the published literature showed that the hybrid GWO-PNN could decrease the error rate and could also generate a better result for five of the datasets studied.																	0334-1860	2191-026X				JAN	2020	29	1					846	857		10.1515/jisys-2018-0129													
J								Gray Method for Multiple Attribute Decision Making with Incomplete Weight Information under the Pythagorean Fuzzy Setting	JOURNAL OF INTELLIGENT SYSTEMS										Multiple attribute decision making; gray relational analysis (GRA); Pythagorean fuzzy numbers; incomplete weight information	MEMBERSHIP GRADES; LAST AGGREGATION; MEAN OPERATORS; TOPSIS; EXTENSION; NUMBERS	In this study, we developed an approach to investigate multiple attribute group decision-making (MAGDM) problems, in which the attribute values take the form of Pythagorean fuzzy numbers whose information about attribute weights is incompletely known. First, the Pythagorean fuzzy Choquet integral geometric operator is utilized to aggregate the given decision information to obtain the overall preference value of each alternative by experts. In order to obtain the weight vector of the criteria, an optimization model based on the basic ideal of the traditional gray relational analysis method is established, and the calculation steps for solving Pythagorean fuzzy MAGDM problems with incompletely known weight information are given. The degree of gray relation between every alternative and positive-ideal solution and negative-ideal solution is calculated. Then, a relative relational degree is defined to determine the ranking order of all alternatives by calculating the degree of gray relation to both the positive-ideal solution and negative-ideal solution simultaneously. Finally, an illustrative example is given to verify the developed approach and to demonstrate its practicality and effectiveness.																	0334-1860	2191-026X				JAN	2020	29	1					858	876		10.1515/jisys-2018-0099													
J								Multi-Agent System Based on the Extreme Learning Machine and Fuzzy Control for Intelligent Energy Management in Microgrid	JOURNAL OF INTELLIGENT SYSTEMS										Renewable energy; microgrid; multi-agent system; prediction; extreme learning machine; fuzzy logic control	ADAPTIVE NEURAL-CONTROL; NONLINEAR-SYSTEMS	Renewable energies constitute an alternative to fossil energies for several reasons. The microgrid can be assumed as the ideal way to integrate a renewable energy source in the production of electricity and give the consumer the opportunity to participate in the electricity market not just like a consumer but also like a producer. In this paper, we present a multi-agent system based on wind and photovoltaic power prediction using the extreme learning machine algorithm. This algorithm was tested on real weather data taken from the region of Tetouan City in Morocco. The process aimed to implement a microgrid located in Tetouan City and composed of different generation units (solar and wind energies were combined together to increase the efficiency of the system) and storage units (batteries were used to ensure the availability of power on demand as much as possible). In the proposed architecture, the microgrid can exchange electricity with the main grid; therefore, it can buy or sell electricity. Thus, the goal of our multi-agent system is to control the amount of power delivered or taken from the main grid in order to reduce the cost and maximize the benefit. To address uncertainties in the system, we use fuzzy logic control to manage the flow of energy, to ensure the availability of power on demand, and to make a reasonable decision about storing or selling electricity.																	0334-1860	2191-026X				JAN	2020	29	1					877	893		10.1515/jisys-2018-0125													
J								Deep CNN Combined With Relevance Feedback for Trademark Image Retrieval	JOURNAL OF INTELLIGENT SYSTEMS										Content-based image retrieval (CBIR); deep convolutional neural networks (DCNNs); particle swarm optimization (PSO); self-organizing map (SOM); relevance feedback; trademark		Trademark recognition and retrieval is a vital appliance component of content-based image retrieval (CBIR). Reduction in the semantic gap, attaining more accuracy, reduction in computation complexity, and hence in execution time, are the major challenges in designing and developing a trademark retrieval system. The direction of the proposed work takes into account these challenges by implementing trademark image retrieval through deep convolutional neural networks (DCNNs) integrated with a relevant feedback mechanism. The dataset features are optimized through particle swarm optimization (PSO), reducing the search space. These best/optimized features are given to the self-organizing map (SOM) for clustering at the preprocessing stage. The CNN model is trained on feature representations of relevant and irrelevant images, using the feedback information from the user bringing the marked relevant images closer to the query. Experimentation proved a significant performance when evaluated using FlickrLogos-27, FlickrLogos-32, and FlickrLogos-32 PLUS datasets, as illustrated in the performance results section.																	0334-1860	2191-026X				JAN	2020	29	1					894	909		10.1515/jisys-2018-0083													
J								Cognitively Motivated Query Abstraction Model Based on Associative Root-Pattern Networks	JOURNAL OF INTELLIGENT SYSTEMS										Query expansion; intelligent information retrieval; root-pattern-based search; human-computer interaction; semantic representation; associative word-network; statistical language model; associative root-pattern networks; fuzzy subsethood theorem		This paper attempts to investigate some aspects related to problems involved in textual intercognitive communication in the context of search queries. Furthermore, it aims at stressing on the root-pattern and morpho-phonetic dimension of a word meaning within a query, and its effects on understanding and predicting the intended information conveyed by some search patterns in a human language. As humans are inclined to use very few words possibly pervaded with vague and uncertain interpretational potential for requesting information, misinterpreting conveyed information in a query term might critically influence an inter-cognitive communication, particularly in case of Arabic- and Semitic-based computer systems. Furthermore, as phonetic patterns are involved in the mental word perception, an abstract morpho-phonetic query model is proposed based on the non-linearity of the morpho-phonetic characteristic of Arabic word cognition. This model suggests forming the intended query information by constructing morpho-phonetic query patterns relying on the most associative root-pattern subnetworks. An important advantage of this model resides in introducing the concept of query abstract morpho-phonetic vectors expressing query vector space. Furthermore, this approach suggests employing the fuzzy subsethood theorem as an assessment reflecting model accuracy and the closeness to human associative word-networks. Finally, it opens the discussion to consider indexing based on a higher level of abstraction, such as utilising patterns as cognitive search variables. Furthermore, as this model is capable of predicting most human associative query key terms, integrating these within certain human-machine interaction would improve inter-cognitive communications.																	0334-1860	2191-026X				JAN	2020	29	1					910	923		10.1515/jisys-2017-0549													
J								Improved Adaptive Neuro-Fuzzy Inference System Using Gray Wolf Optimization: A Case Study in Predicting Biochar Yield	JOURNAL OF INTELLIGENT SYSTEMS										Gray wolf optimization (GWO); adaptive neuro-fuzzy inference system (ANFIS); renewable energy production; biochar prediction	GENETIC ALGORITHM; ANFIS; REGRESSION; PYROLYSIS; DESIGN; LOGIC; STATE	This paper presents an alternative method for predicting biochar yields from biomass thermochemical processes. As biochar is considered a renewable and sustainable energy source, it has received more attention. Several methods have been presented to predict biochar, such as neural network (NN) and least square support vector machine (LS-SVM). However, each of them has its own drawbacks, such as getting stuck in a local optimum, which occurs in NN, and lack of uncertainty and time complexity, as in LS-SVM. Therefore, this paper avoids this limitation by using a hybrid method between the adaptive neuro-fuzzy inference system (ANFIS) and gray wolf optimization (GWO) algorithm. The proposed method is called ANFIS-GWO, which consists of two stages. In the first stage, GWO is used to learn the parameters of ANFIS using the training set. Meanwhile, in the second stage, the testing set is used to evaluate the performance of the proposed ANFIS-GWO method. Three experiments were performed to assess the performance of the proposed method. The first experiment used a set of UCI (University of California, Irvine) benchmark datasets to evaluate the effectiveness of ANFIS-GWO. The aim of the second experiment was to evaluate the performance of the proposed ANFIS- GWO method to predict biochar yield from manure pyrolysis. The third experiment aimed to estimate the values of input parameters of pyrolysis that maximize biochar production. The obtained results were compared to those of other methods, such as ANFIS using gradient descent, practical swarm optimization, genetic algorithm, whale optimization algorithm, sine-cosine algorithm, and LS-SVM. The results of the ANFIS-GWO method were >35% of the standard ANFIS and also better than those of other methods.																	0334-1860	2191-026X				JAN	2020	29	1					924	940		10.1515/jisys-2017-0641													
J								Predict Forex Trend via Convolutional Neural Networks	JOURNAL OF INTELLIGENT SYSTEMS										Deep learning; convolutional neural network (CNN); geometric Brownian motion (GBM); Forex (FX); trading strategies		Deep learning is an effective approach to solving image recognition problems. People draw intuitive conclusions from trading charts. This study uses the characteristics of deep learning to train computers in imitating this kind of intuition in the context of trading charts. The main goal of our approach is combining the time-series modeling and convolutional neural networks (CNNs) to build a trading model. We propose three steps to build the trading model. First, we preprocess the input data from quantitative data to images. Second, we use a CNN, which is a type of deep learning, to train our trading model. Third, we evaluate the model's performance in terms of the accuracy of classification. The experimental results show that if the strategy is clear enough to make the images obviously distinguishable the CNN model can predict the prices of a financial asset. Hence, our approach can help devise trading strategies and help clients automatically obtain personalized trading strategies.																	0334-1860	2191-026X				JAN	2020	29	1					941	958		10.1515/jisys-2018-0074													
J								Optimizing Integrated Features for Hindi Automatic Speech Recognition System	JOURNAL OF INTELLIGENT SYSTEMS										Automatic speech recognition; MFCC; GFCC; PSO; PLP; C-PSO; Q-PSO; HMM	PERFORMANCE EVALUATION; OPTIMIZATION; ALGORITHM; NOISE	An automatic speech recognition (ASR) system translates spoken words or utterances (isolated, connected, continuous, and spontaneous) into text format. State-of-the-art ASR systems mainly use Mel frequency (MF) cepstral coefficient (MFCC), perceptual linear prediction (PLP), and Gammatone frequency (GF) cepstral coefficient (GFCC) for extracting features in the training phase of the ASR system. Initially, the paper proposes a sequential combination of all three feature extraction methods, taking two at a time. Six combinations, MF-PLP, PLP-MFCC, MF-GFCC, GF-MFCC, GF-PLP, and PLP-GFCC, are used, and the accuracy of the proposed system using all these combinations was tested. The results show that the GF-MFCC and MF-GFCC integrations outperform all other proposed integrations. Further, these two feature vector integrations are optimized using three different optimization methods, particle swarm optimization (PSO), PSO with crossover, and PSO with quadratic crossover (Q-PSO). The results demonstrate that the Q-PSO-optimized GF-MFCC integration show significant improvement over all other optimized combinations.																	0334-1860	2191-026X				JAN	2020	29	1					959	976		10.1515/jisys-2018-0057													
J								A Novel Weakest t-norm based Fuzzy Fault Tree Analysis Through Qualitative Data Processing and Its Application in System Reliability Evaluation	JOURNAL OF INTELLIGENT SYSTEMS										Fuzzy failure possibility; fault tree analysis; weakest t-norm; fuzzy number	SAFETY; PROBABILITY; OPERATIONS	The quantification of the fuzzy fault tree analysis (FFTA) is based on fuzzy arithmetic operations. It is well known that the weakest t-norm (T-w)-based fuzzy arithmetic operations have some advantages. The T-w-based fuzzy arithmetic operations provide fuzzy results with less fuzziness and preserve the shape of fuzzy numbers. The purpose of this study is to develop a Tw-based fuzzy fault tree analysis (TBFFTA) to assess system reliability when only qualitative data such as expert opinions or decisions are available and described in linguistic terms. The developed TBFFTA applies T-w-based fuzzy arithmetic operations to evaluate the lower bound, best estimate, and upper bound top event probability of a system fault tree, where occurrence possibilities of basic events are characterized by triangular fuzzy membership functions. To demonstrate the applicability and feasibility of TBFFTA, a case study has been performed. The computed results have been compared with results analyzed by existing fuzzy approach. The comparative study concludes that TBFFTA reduces fuzzy spreads (uncertainty interval) and provides more exact fuzzy results.																	0334-1860	2191-026X				JAN	2020	29	1					977	993		10.1515/jisys-2018-0159													
J								FCNB: Fuzzy Correlative Naive Bayes Classifier with MapReduce Framework for Big Data Classification	JOURNAL OF INTELLIGENT SYSTEMS										Big data; classification; correlative naive Bayes classifier; fuzzy theory; MapReduce	MAP REDUCE SOLUTION; ALGORITHM; MACHINE	The term "big data" means a large amount of data, and big data management refers to the efficient handling, organization, or use of large volumes of structured and unstructured data belonging to an organization. Due to the gradual availability of plenty of raw data, the knowledge extraction process from big data is a very difficult task for most of the classical data mining and machine learning tools. In a previous paper, the correlative naive Bayes (CNB) classifier was developed for big data classification. This work incorporates the fuzzy theory along with the CNB classifier to develop the fuzzy CNB (FCNB) classifier. The proposed FCNB classifier solves the big data classification problem by using the MapReduce framework and thus achieves improved classification results. Initially, the database is converted to the probabilistic index table, in which data and attributes are presented in rows and columns, respectively. Then, the membership degree of the unique symbols present in each attribute of data is found. Finally, the proposed FCNB classifier finds the class of data based on training information. The simulation of the proposed FCNB classifier uses the localization and skin segmentation datasets for the purpose of experimentation. The results of the proposed FCNB classifier are analyzed based on the metrics, such as sensitivity, specificity, and accuracy, and compared with the various existing works.																	0334-1860	2191-026X				JAN	2020	29	1					994	1006		10.1515/jisys-2018-0020													
J								A Modified Jaya Algorithm for Mixed-Variable Optimization Problems	JOURNAL OF INTELLIGENT SYSTEMS										Modified Jaya algorithm; mixed variables; constraint handling; penalty function; balancing	PARTICLE SWARM OPTIMIZATION; LEARNING-BASED-OPTIMIZATION; DISCRETE OPTIMIZATION; MULTIOBJECTIVE OPTIMIZATION; NONLINEAR PROBLEMS; DESIGN; INTEGER	Mixed-variable optimization problems consist of the continuous, integer, and discrete variables generally used in various engineering optimization problems. These variables increase the computational cost and complexity of optimization problems due to the handling of variables. Moreover, there are few optimization algorithms that give a globally optimal solution for non-differential and non-convex objective functions. Initially, the Jaya algorithm has been developed for continuous variable optimization problems. In this paper, the Jaya algorithm is further extended for solving mixed-variable optimization problems. In the proposed algorithm, continuous variables remain in the continuous domain while continuous domains of discrete and integer variables are converted into discrete and integer domains applying bound constraint of the middle point of corresponding two consecutive values of discrete and integer variables. The effectiveness of the proposed algorithm is evaluated through examples of mixed-variable optimization problems taken from previous research works, and optimum solutions are validated with other mixed-variable optimization algorithms. The proposed algorithm is also applied to two-plane balancing of the unbalanced rigid threshing rotor, using the number of balance masses on plane 1 and plane 2. It is found that the proposed algorithm is computationally more efficient and easier to use than other mixed optimization techniques.																	0334-1860	2191-026X				JAN	2020	29	1					1007	1027		10.1515/jisys-2018-0273													
J								An Improved Robust Fuzzy Algorithm for Unsupervised Learning	JOURNAL OF INTELLIGENT SYSTEMS										Similarity measure; outlier detection; clustering; proximity degree	GROUP DECISION-MAKING; PREFERENCE RELATIONS; NOISE; MODEL	This paper presents a robust, dynamic, and unsupervised fuzzy learning algorithm (RDUFL) that aims to cluster a set of data samples with the ability to detect outliers and assign the numbers of clusters automatically. It consists of three main stages. The first (1) stage is a pre-processing method in which possible outliers are determined and quarantined using a concept of proximity degree. The second (2) stage is a learning method, which consists in auto-detecting the number of classes with their prototypes for a dynamic threshold. This threshold is automatically determined based on the similarity among the detected prototypes that are updated at the exploration of a new data. The last (3) stage treats quarantined samples detected from the first stage to determine whether they belong to some class defined in the second phase. The effectiveness of this method is assessed on eight real medical benchmark datasets in comparison to known unsupervised learning methods, namely, the fuzzy c-means (FCM), possibilistic c-means (PCM), and noise clustering (NC). The obtained accuracy of our scheme is very promising for unsupervised learning problems.																	0334-1860	2191-026X				JAN	2020	29	1					1028	1042		10.1515/jisys-2018-0030													
J								Hybridizing the Cuckoo Search Algorithm with Different Mutation Operators for Numerical Optimization Problems	JOURNAL OF INTELLIGENT SYSTEMS										Cuckoo search; evolutionary algorithms; mutation operators; optimization; metaheuristic	GENETIC ALGORITHM; KRILL HERD; EVOLUTIONARY; DESIGN; TESTS	The Cuckoo search (CS) algorithm is an efficient evolutionary algorithm inspired by the nesting and parasitic reproduction behaviors of some cuckoo species. Mutation is an operator used in evolutionary algorithms to maintain the diversity of the population from one generation to the next. The original CS algorithm uses the Levy flight method, which is a special mutation operator, for efficient exploration of the search space. The major goal of the current paper is to experimentally evaluate the performance of the CS algorithm after replacing the Levy flight method in the original CS algorithm with seven different mutation methods. The proposed variations of CS were evaluated using 14 standard benchmark functions in terms of the accuracy and reliability of the obtained results over multiple simulations. The experimental results suggest that the CS with polynomial mutation provides more accurate results and is more reliable than the other CS variations.																	0334-1860	2191-026X				JAN	2020	29	1					1043	1062		10.1515/jisys-2018-0331													
J								An Eflcient Lossless ROI Image Compression Using Wavelet-Based Modified Region Growing Algorithm	JOURNAL OF INTELLIGENT SYSTEMS										Medical image; region of interest; modified region growing; merging-based Huffman encoding; SPIHT; DWT; DCT; non-ROI		Nowadays, medical imaging and telemedicine are increasingly being utilized on a huge scale. The expanding interest in storing and sending medical images brings a lack of adequate memory spaces and transmission bandwidth. To resolve these issues, compression was introduced. The main aim of lossless image compression is to improve accuracy, reduce the bit rate, and improve the compression efficiency for the storage and transmission of medical images while maintaining an acceptable image quality for diagnosis purposes. In this paper, we propose lossless medical image compression using wavelet transform and encoding method. Basically, the proposed image compression system comprises three modules: (i) segmentation, (ii) image compression, and (iii) image decompression. First, the input medical image is segmented into region of interest (ROI) and non-ROI using a modified region growing algorithm. Subsequently, the ROI is compressed by discrete cosine transform and set partitioning in hierarchical tree encoding method, and the non-ROI is compressed by discrete-wavelet transform and merging-based Huffman encoding method. Finally, the compressed image combination of the compressed ROI and non-ROI is obtained. Then, in the decompression stage, the original medical image is extracted using the reverse procedure. The experimentation was carried out using different medical images, and the proposed method obtained better results compared to different other methods.																	0334-1860	2191-026X				JAN	2020	29	1					1063	1078		10.1515/jisys-2018-0180													
J								Predicting Automatic Trigger Speed for Vehicle-Activated Signs	JOURNAL OF INTELLIGENT SYSTEMS										Vehicle-activated signs; adaptive neuro-fuzzy inference systems; random forest; self-organising maps; trigger speed	LIMITS; FLOW; MAPS	Vehicle-activated signs (VAS) are speed-warning signs activated by radar when the driver speed exceeds a pre-set threshold, i.e. the trigger speed. The trigger speed is often set relative to the speed limit and is displayed for all types of vehicles. It is our opinion that having a static setting for the trigger speed may be inappropriate, given that traffic and road conditions are dynamic in nature. Further, different vehicle classes (mainly cars and trucks) behave differently, so a uniform trigger speed of such signs may be inappropriate to warn different types of vehicles. The current study aims to investigate an automatic VAS, i.e. one that could warn vehicle users with an appropriate trigger speed by taking into account vehicle types and road conditions. We therefore investigated different vehicle classes, their speeds, and the time of day to be able to conclude whether different trigger speeds of VAS are essential or not. The current study is entirely data driven; data are initially presented to a self-organising map (SOM) to be able to partition the data into different clusters, i.e. vehicle classes. Speed, time of day, and length of vehicle were supplied as inputs to the SOM. Further, the 85th percentile speed for the next hour is predicted using appropriate prediction models. Adaptive neurofuzzy inference systems and random forest (RF) were chosen for speed prediction; the mean speed, traffic flow, and standard deviation of vehicle speeds were supplied as inputs for the prediction models. The results achieved in this work show that RF is a reliable model in terms of accuracy and efficiency, and can be used in finding appropriate trigger speeds for an automatic VAS.																	0334-1860	2191-026X				JAN	2020	29	1					1079	1091		10.1515/jisys-2016-0329													
J								Group Recommender Systems - An Evolutionary Approach Based on Multi-expert System for Consensus	JOURNAL OF INTELLIGENT SYSTEMS										Group recommender systems; genetic algorithm; rank aggregation; consensus	FUSION PROCESS; FRAMEWORK; MODELS	Recommender systems have focused on algorithms for a recommendation for individuals. However, in many domains, it may be recommending an item, for example, movies, restaurants etc. for a group of persons for which some remarkable group recommender systems (GRSs) has been developed. GRSs satisfy a group of people optimally by considering the equal weighting of the individual preferences. We have proposed a multi-expert scheme (MES) for group recommendation using genetic algorithm (GA) MES-GRS-GA that depends on consensus techniques to further improve group recommendations. In order to deal with this problem of GRS, we also propose a consensus scheme for GRSs where consensus from multiple experts are brought together to make a single recommended list of items in which each expert represents an individual inside the group. The proposed GA based consensus scheme is modeled as many consensus schemes within two phases. In the consensus phase, we have applied GA to obtain the maximum utility offer for each expert and generated the most appropriate rating for each item in the group. In the recommendation generation phase, again GA has been employed to produce the resulting group profile, i.e. the list of ratings with the minimum sum of distances from the group members. Finally, the results of computational experiments that bear close resemblance to real-world scenarios are presented and compared to baseline GRS techniques that illustrate the superiority of the proposed model.																	0334-1860	2191-026X				JAN	2020	29	1					1092	1108		10.1515/jisys-2018-0081													
J								Enriching Documents by Linking Salient Entities and Lexical-Semantic Expansion	JOURNAL OF INTELLIGENT SYSTEMS										Document clustering; document enriching	NAMED ENTITIES	This paper explores a multi-strategy technique that aims at enriching text documents for improving clustering quality. We use a combination of entity linking and document summarization in order to determine the identity of the most salient entities mentioned in texts. To effectively enrich documents without introducing noise, we limit ourselves to the text fragments mentioning the salient entities, in turn, belonging to a knowledge base like Wikipedia, while the actual enrichment of text fragments is carried out using WordNet. To feed clustering algorithms, we investigate different document representations obtained using several combinations of document enrichment and feature extraction. This allows us to exploit ensemble clustering, by combining multiple clustering results obtained using different document representations. Our experiments indicate that our novel enriching strategies, combined with ensemble clustering, can improve the quality of classical text clustering when applied to text corpora like The British Broadcasting Corporation (BBC) NEWS.																	0334-1860	2191-026X				JAN	2020	29	1					1109	1121		10.1515/jisys-2018-0098													
J								A New Feature Selection Method for Sentiment Analysis in Short Text	JOURNAL OF INTELLIGENT SYSTEMS										Feature selection; sentiment analysis; short text; classification	CLASSIFICATION; TWITTER	In recent internet era, micro-blogging sites produce enormous amount of short textual information, which appears in the form of opinions or sentiments of users. Sentiment analysis is a challenging task in short text, due to use of formal language, misspellings, and shortened forms of words, which leads to high dimensionality and sparsity. In order to deal with these challenges, this paper proposes a novel, simple, and yet effective feature selection method, to select frequently distributed features related to each class. In this paper, the feature selection method is based on class-wise information, to identify the relevant feature related to each class. We evaluate the proposed feature selection method by comparing with existing feature selection methods like chi-square (chi(2)), entropy, information gain, and mutual information. The performances are evaluated using classification accuracy obtained from support vector machine, K nearest neighbors, and random forest classifiers on two publically available datasets viz., Stanford Twitter dataset and Ravikiran Janardhana dataset. In order to demonstrate the effectiveness of the proposed feature selection method, we conducted extensive experimentation by selecting different feature sets. The proposed feature selection method outperforms the existing feature selection methods in terms of classification accuracy on the Stanford Twitter dataset. Similarly, the proposed method performs competently equally in terms of classification accuracy compared to other feature selection methods in most of the feature subsets on Ravikiran Janardhana dataset.																	0334-1860	2191-026X				JAN	2020	29	1					1122	1134		10.1515/jisys-2018-0171													
J								Optimizing Software Modularity with Minimum Possible Variations	JOURNAL OF INTELLIGENT SYSTEMS										Modularization; restructuring; optimization; genetic algorithm; refactoring	ALGORITHM	Poor design choices at the early stages of software development and unprincipled maintenance practices usually deteriorate software modularity and subsequently increase system complexity. In object-oriented software, improper distribution of classes among packages is a key factor, responsible formodularity degradation. Many optimization techniques to improve the software modularity have been proposed in the literature. The focus of these optimization techniques is to produce modularization solutions by optimizing different design quality criteria. Such modularization solutions are good from the different aspect of quality; however, they require huge modifications in the existing modular structure to realize the suggested solution. Thus these techniques are costly and time consuming if applied at early stages of software maintenance. This paper proposes a search-based optimization technique to improve the modularity of the software system with minimum possible variation between the existing and produced modularization solution. To this contribution, a penalized fitness function, namely, penalized modularization quality, is designed in terms of modularization quality and the Move or Join Effectiveness Measure metric. Furthermore, this fitness function is used in both single-objective genetic algorithm (SGA) and multi-objective genetic algorithm (MGA) to generate the modularization. The effectiveness of the proposed remodularization approach is evaluated over five open-source and three random generated software systems. The experimentation results show that the proposed approach is able to generate modularization solutions with improved quality along with lesser perturbation compared to their non-penalty counterpart and at the same time it performs better with the MGA compared to the SGA. The proposed approach can be very useful, especially when total remodularization is not feasible/desirable due to lack of time or high cost.																	0334-1860	2191-026X				JAN	2020	29	1					1135	1150		10.1515/jisys-2018-0231													
J								Optimizing the Self-Organizing Team Size Using a Genetic Algorithm in Agile Practices	JOURNAL OF INTELLIGENT SYSTEMS										Agile; genetic algorithm; optimization; team size	SOFTWARE-DEVELOPMENT; SUCCESS FACTORS; PERFORMANCE; COMMUNICATION; ORGANIZATIONS; FRAMEWORK	In agile software processes, the issue of team size is an important one. In this work we look at how to find the optimal, or near optimal, self-organizing team size using a genetic algorithm (GA) which considers team communication efforts. Communication, authority, roles, and learning are the team's performance characteristics. The GA has been developed according to performance characteristics. A survey was used to evaluate the communication weight factors, which were qualitatively assessed and used in the algorithm's objective function. The GA experimentswere performed in different stages: each stage results were tested and compared with the previous results. The results show that self-organizing teams of sizes ranged from five to nine members scored more. The model can be improved by adding other team characteristics, i.e. software development efforts and costs.																	0334-1860	2191-026X				JAN	2020	29	1					1151	1165		10.1515/jisys-2018-0085													
J								Aspect-Oriented Sentiment Analysis: A Topic Modeling-Powered Approach	JOURNAL OF INTELLIGENT SYSTEMS										Latent Dirichlet allocation; sentiment analysis; topic modeling; e-commerce; aspect extraction; text mining		Because of exponential growth in the number of people who purchase products online, e-commerce organizations are vying for each other to offer innovative and improved services to its customers. Current platforms give its customers innovative services such as product recommendations based on their purchase histories and location, product comparison, and most importantly, a platform for expressing their experience and feedback. It is important for any e-commerce organization to analyze this feedback and to find out the sentiment of the customers for giving them better products and services. As large reviews may contain feedback in a mixed manner where a customer gives his opinion on different product features in the same review, finding out the exact sentiment is tedious. This work proposes aspect-specific sentiment analysis of product reviews using a well-sophisticated topic modeling algorithm called latent Dirichlet allocation (LDA). The topic words, thus, extracted are mapped with various aspects of an entity to perform the aspect-specific sentiment analysis on product reviews. Experiments with synthetic and real dataset show promising results compared to existing methods of sentiment analysis.																	0334-1860	2191-026X				JAN	2020	29	1					1166	1178		10.1515/jisys-2018-0299													
J								Feature Pair Index Graph for Clustering	JOURNAL OF INTELLIGENT SYSTEMS										Inverted index; inverted feature pair index; feature pair index graph; clustering	INVERTED INDEX; TEXT	Text documents are significant arrangements of various words, while images are significant arrangements of various pixels/features. In addition, text and image data share a similar semantic structural pattern. With reference to this research, the feature pair is defined as a pair of adjacent image features. The innovative feature pair index graph (FPIG) is constructed from the unique feature pair selected, which is constructed using an inverted index structure. The constructed FPIG is helpful in clustering, classifying and retrieving the image data. The proposed FPIG method is validated against the traditional KMeans++, KMeans and Farthest First cluster methods which have the serious drawback of initial centroid selection and local optima. The FPIG method is analyzed using Iris flower image data, and the analysis yields 88% better results than Farthest First and 28.97% better results than conventional KMeans in terms of sum of squared errors. The paper also discusses the scope for further research in the proposed methodology.																	0334-1860	2191-026X				JAN	2020	29	1					1179	1187		10.1515/jisys-2018-0338													
J								Tangramob: An Agent-Based Simulation Framework for Validating Urban Smart Mobility Solutions	JOURNAL OF INTELLIGENT SYSTEMS										Smart mobility; agent-based traffic simulations; smart urban planning		Estimating the effects of introducing a range of smart mobility solutions within an urban area is a crucial concern in urban planning. The lack of a simulator for the assessment of mobility initiatives forces local public authorities and mobility service providers to base their decisions on guidelines derived from common heuristics and best practices. These approaches can help planners in shaping mobility solutions; however, given the high number of variables to consider, the effects are not guaranteed. Therefore, a solution conceived respecting the available guidelines can result in a failure in a different context. In particular, difficult aspects to consider are the interactions between different mobility services available in a given urban area and the acceptance of a given mobility initiative by the inhabitants of the area. In order to fill this gap, we introduce Tangramob, an agent-based simulation framework capable of assessing the impacts of a smart mobility initiative within an urban area of interest. Tangramob simulates how urban traffic is expected to evolve as citizens start experiencing newly offered traveling solutions. This allows decision makers to evaluate the efficacy of their initiatives, taking into account the current urban system. In this paper, we provide an overview of the simulation framework along with its design. To show the potential of Tangramob, three mobility initiatives are simulated and compared in the same scenario. This demonstrates how it is possible to perform comparative experiments so as to align mobility initiatives to the user goals.																	0334-1860	2191-026X				JAN	2020	29	1					1188	1201		10.1515/jisys-2018-0321													
J								A New Algorithm Based on Magic Square and a Novel Chaotic System for Image Encryption	JOURNAL OF INTELLIGENT SYSTEMS										Chaotic system; magic square; image encryption; Lyapunov exponents		This article introduces a simple and effective new algorithm for image encryption using a chaotic system which is based on the magic squares. This novel 3D chaotic system is invoked to generate a random key to encrypt any color image. A number of chaotic keys equal to the size of the image are generated by this chaotic system and arranged into a matrix then divided into non-overlapped submatrices. The image to be encrypted is also divided into sub-images, and each sub-image is multiplied by a magic matrix to produce another set of matrices. The XOR operation is then used on the resultant two sets of matrices to produce the encrypted image. The strength of the encryption method is tested in two folds. The first fold is the security analysis which includes key space analysis and sensitivity analysis. In the second fold, statistical analysis was performed, which includes the correlation coefficients, information entropy, the histogram, and analysis of differential attacks. Finally, the time of encryption and decryption was computed and show very good results.																	0334-1860	2191-026X				JAN	2020	29	1					1202	1215		10.1515/jisys-2018-0404													
J								Video Steganography Using Knight Tour Algorithm and LSB Method for Encrypted Data	JOURNAL OF INTELLIGENT SYSTEMS										Data hiding; steganography; encryption; LSB; knight tour		This paper aims to propose a method for data hiding in video by utilizing the least significant bit (LSB) method and improving it by utilizing the knight tour algorithm for concealing the data inside the AVI video file and using a key function encryption method for encrypting the secret message. First, the secret message is encrypted by utilizing a mathematical equation. The key used in the equation is a set of random numbers. These numbers differ in each implementation to warrant the safety of the hidden message and to increase the security of the secret message. Then, the cover video was converted from a set of frames into separated images to take the advantage of the large size of video file. Afterward, the knight tour algorithm is utilized for random selecting of the pixels inside the frame utilized for embedding the secret message inside it to overcome the shortcoming of the conventional LSB method that utilized the serial selection of pixel and to increase the robustness and security of the proposed method. Afterward, the encrypted secret message is embedded inside the selected pixels by utilizing the LSB method in bits (7 and 8). The observational results have drawn that the proposed method has a superior performance compared to the previous steganography method in terms of quality by a high PSNR of 67.3638 dB and the lowest MSE of 0.2578. Furthermore, this method preserves the security where the secret message cannot be drawn out without knowing the decoding rules.																	0334-1860	2191-026X				JAN	2020	29	1					1216	1225		10.1515/jisys-2018-0225													
J								Clay-Based Brick Porosity Estimation Using Image Processing Techniques	JOURNAL OF INTELLIGENT SYSTEMS										68U10; 92E99; 68W40	POROUS-MEDIA; FLOW; MICROTOMOGRAPHY; SEGMENTATION; TOMOGRAPHY	This work intends to apprehend and emphasize the contribution of image-processing techniques and computer vision in the treatment of clay-based material known in Meknes region. One of the various characteristics used to describe clay in a qualitative manner is porosity, as it is considered one of the properties that with "kill or cure" effectiveness. For this purpose, we use scanning electron microscopy images, as they are considered the most powerful tool for characterising the quality of the microscopic pore structure of porous materials. We present various existing methods of segmentation, as we are interested only in pore regions. The results show good matching between physical estimation and Voronoi diagram-based porosity estimation.																	0334-1860	2191-026X				JAN	2020	29	1					1226	1234		10.1515/jisys-2018-0191													
J								AGCS Technique to Improve the Performance of Neural Networks	JOURNAL OF INTELLIGENT SYSTEMS										Adaptive genetic (AG) algorithm; cuckoo search (CS) algorithm; genetic algorithm (GA); back propagation algorithm (BPA); artificial neural network (ANN); Levy flight	ALGORITHMS; CANCER	In this paper, a fresh method is offered regarding training of particular neural networks. This technique is a combination of the adaptive genetic (AG) and cuckoo search (CS) algorithms, called the AGCS method. The intention of training a particular artificial neural network (ANN) is to obtain the finest weight load. With this protocol, a particular weight is taken into account as feedback, which is optimized by means of the hybrid AGCS protocol. In the beginning, a collection of weights is initialized and the similar miscalculation is discovered. Finally, during training of an ANN, we can easily overcome the training complications involving ANNs and also gain the finest overall performance with training of the ANN. We have implemented the proposed system in MATLAB, and the overall accuracy is about 93%, which is much better than that of the genetic algorithm (86%) and CS (88%) algorithm.																	0334-1860	2191-026X				JAN	2020	29	1					1235	1245		10.1515/jisys-2017-0423													
J								A Color Image Encryption Technique Based on Bit-Level Permutation and Alternate Logistic Maps	JOURNAL OF INTELLIGENT SYSTEMS										ACM; LSM; LTM; TSM; Lorenz system; alternate logistic map	CHAOTIC STANDARD; ALGORITHM; SCHEME; IMPLEMENTATION; STEGANOGRAPHY; DESIGN	The paper presents an approach to encrypt the color images using bit-level permutation and alternate logistic map. The proposed method initially segregates the color image into red, green, and blue channels, transposes the segregated channels from the pixel-plane to bit-plane, and scrambles the bit-plane matrix using Arnold cat map (ACM). Finally, the red, blue, and green channels of the scrambled image are confused and diffused by applying alternate logistic map that uses a four-dimensional Lorenz system to generate a pseudorandom number sequence for the three channels. The parameters of ACM are generated with the help of Logistic-Sine map and Logistic-Tent map. The intensity values of scrambled pixels are altered by Tent-Sine map. One-dimensional and two-dimensional logistic maps are used for alternate logistic map implementation. The performance and security parameters histogram, correlation distribution, correlation coefficient, entropy, number of pixel change rate, and unified averaged changed intensity are computed to show the potential of the proposed encryption technique.																	0334-1860	2191-026X				JAN	2020	29	1					1246	1260		10.1515/jisys-2018-0365													
J								A Hybrid of Deep CNN and Bidirectional LSTM for Automatic Speech Recognition	JOURNAL OF INTELLIGENT SYSTEMS										ASR; CNN; CNN-BLSTM; DNN; LSTM-RNN	NEURAL-NETWORKS; CLASSIFICATION; OPTIMIZATION	Deep neural networks (DNNs) have been playing a significant role in acoustic modeling. Convolutional neural networks (CNNs) are the advanced version of DNNs that achieve 4-12% relative gain in the word error rate (WER) over DNNs. Existence of spectral variations and local correlations in speech signal makes CNNs more capable of speech recognition. Recently, it has been demonstrated that bidirectional long short-term memory (BLSTM) produces higher recognition rate in acoustic modeling because they are adequate to reinforce higher-level representations of acoustic data. Spatial and temporal properties of the speech signal are essential for high recognition rate, so the concept of combining two different networks came into mind. In this paper, a hybrid architecture of CNN-BLSTM is proposed to appropriately use these properties and to improve the continuous speech recognition task. Further, we explore different methods like weight sharing, the appropriate number of hidden units, and ideal pooling strategy for CNN to achieve a high recognition rate. Specifically, the focus is also on how many BLSTM layers are effective. This paper also attempts to overcome another shortcoming of CNN, i.e. speaker-adapted features, which are not possible to be directly modeled in CNN. Next, various non-linearities with or without dropout are analyzed for speech tasks. Experiments indicate that proposed hybrid architecture with speaker-adapted features and maxout non-linearity with dropout idea shows 5.8% and 10% relative decrease in WER over the CNN and DNN systems, respectively.																	0334-1860	2191-026X				JAN	2020	29	1					1261	1274		10.1515/jisys-2018-0372													
J								Database Creation and Dialect-Wise Comparative Analysis of Prosodic Features for Punjabi Language	JOURNAL OF INTELLIGENT SYSTEMS										Speech corpus; dialects; tones; prosodic features; pitch; intensity; formants		The paper represents a Punjabi corpus in the agriculture domain. There are various dialects in the Punjabi language and the main concentration is on major dialects, i.e. Majhi, Malwai and Doabi for the present study. A speech corpus of 125 isolated words is taken into consideration. These words are uttered by 100 speakers, i.e. 60 Malwi dialect speakers (30 male and 30 female), 20 Majhi dialect speakers (10 male and 10 female) and 20 Doabi dialect speakers (10 male and 10 female). Tonemes, adhak (geminated) and nasal words are selected from the corpus. Recordings have been processed through two mediums. The paper also elaborates some distinctive features of the corpus. This corpus is of quite significance for the speech recognition system. Prosodic characteristics such as intonation, rhythm and stress create a crucial impact on the speech recognition system. These characteristics vary from language to language as well as various dialects of a language. This paper portrays a comparative analysis of isolated words prosodic features of Malwi, Majhi and Doabi dialects of Punjabi language. Analysis is done using the PRAAT tool. Pitch, intensity, formant I and formant II values are extracted for toneme, adhak, nasal (bindi) and nasal (tippi) words. For all kinds of words, there is a significant variation in pitch (fundamental frequency), intensity, formant I and formant II values of male and female speakers of Malwi, Majhi and Doabi dialects. A detailed analysis has been discussed throughout this paper.																	0334-1860	2191-026X				JAN	2020	29	1					1275	1282		10.1515/jisys-2019-2511													
J								Trapezoidal Linguistic Cubic Fuzzy TOPSIS Method and Application in a Group Decision Making Program	JOURNAL OF INTELLIGENT SYSTEMS										Trapezoidal linguistic cubic fuzzy number; Trapezoidal linguistic cubic fuzzy TOPSIS method; MCDM; Numerical application	AGGREGATION OPERATORS; NUMBERS; VARIABLES	The aim of this paper is to define some new operation laws for the trapezoidal linguistic cubic fuzzy number and Hamming distance. Furthermore, we define and use the trapezoidal linguistic cubic fuzzy TOPSIS method to solve the multi criteria decision making (MCDM) method. The new ranking method for trapezoidal linguistic cubic fuzzy numbers (TrLCFNs) are used to rank the alternatives. Finally, an illustrative example is given to verify and prove the practicality and effectiveness of the proposed method.																	0334-1860	2191-026X				JAN	2020	29	1					1283	1300		10.1515/jisys-2017-0560													
J								Histopathological Image Segmentation Using Modified Kernel-Based Fuzzy C-Means and Edge Bridge and Fill Technique	JOURNAL OF INTELLIGENT SYSTEMS										Deep convolutional neural network; kernel-based fuzzy c-means; lung cancer segmentation; normalization	LUNG-CANCER HISTOPATHOLOGY; PATHOLOGY	Histopathological lung cancer segmentation using region of interest is one of the emerging research area in the field of health monitoring system. In this paper, the histopathological images were collected from the database Stanford Tissue Microarray Database (TMAD). After image collection, pre-processing was performed using a normalization technique, which enhances the quality of the histopathological image by eliminating unwanted noise. After pre-processing, segmentation was carried out using the modified kernel-based fuzzy c-means clustering (KFCM) approach along with the edge bridge and fill technique (EBFT). It was a flexible high-level machine learning technique to localize the object in a complex template. The experimental result shows that the proposed approach segments the normal and abnormal cancer regions by means of precision, recall, specificity, accuracy, and Jaccard coefficient. The proposed methodology improved the classification accuracy in lung cancer segmentation up to 2.5-5% compared to the existing methods deep convolutional neural network (DCNN) and diffusion-weighted approach.																	0334-1860	2191-026X				JAN	2020	29	1					1301	1314		10.1515/jisys-2018-0316													
J								Proximal Support Vector Machine-Based Hybrid Approach for Edge Detection in Noisy Images	JOURNAL OF INTELLIGENT SYSTEMS										Edge detection; proximal support vector machine; structure tensor; Hessian matrix; anisotropic diffusion; regularization	TESTS	We propose a novel edge detector in the presence of Gaussian noise with the use of proximal support vector machine (PSVM). The edges of a noisy image are detected using a two-stage architecture: smoothing of image is first performed using regularized anisotropic diffusion, followed by the classification using PSVM, termed as regularized anisotropic diffusion-based PSVM (RAD-PSVM) method. In this process, a feature vector is formed for a pixel using the denoised coefficient's class and the local orientations to detect edges in all possible directions in images. From the experiments, conducted on both synthetic and benchmark images, it is observed that our RAD-PSVM approach outperforms the other state-of-the-art edge detection approaches, both qualitatively and quantitatively.																	0334-1860	2191-026X				JAN	2020	29	1					1315	1328		10.1515/jisys-2017-0566													
J								Early Detection of Parkinson's Disease by Using SPECT Imaging and Biomarkers	JOURNAL OF INTELLIGENT SYSTEMS										Parkinson's disease (PD); striatal binding ratio (SBR); biological biomarkers; multivariate logistic regression (MLR); risk prediction; deep learning	DOPAMINE TRANSPORTER; DIAGNOSIS; SHOWS	Precise and timely diagnosis of Parkinson's disease is important to control its progression among subjects. Currently, a neuroimaging technique called dopaminergic imaging that uses single photon emission computed tomography (SPECT) with I-123-Ioflupane is popular among clinicians for detecting Parkinson's disease in early stages. Unlike other studies, which consider only low-level features like gray matter, white matter, or cerebrospinal fluid, this study explores the non-linear relation between different biomarkers (SPECT + biological) using deep learning and multivariate logistic regression. Striatal binding ratios are obtained using I-123-Ioflupane SPECT scans from four brain regions which are further integrated with five biological biomarkers to increase the diagnostic accuracy. Experimental results indicate that this investigated approach can differentiate subjects with 100% accuracy. The obtained results outperform the ones reported in the literature. Furthermore, logistic regression model has been developed for estimating the Parkinson's disease onset probability. Such models may aid clinicians in diagnosing this disease.																	0334-1860	2191-026X				JAN	2020	29	1					1329	1344		10.1515/jisys-2018-0261													
J								Image Compression Based on Block SVD Power Method	JOURNAL OF INTELLIGENT SYSTEMS										Image compression; singular value decomposition; block SVD power method; lossy image compression; PSNR	SINGULAR-VALUE DECOMPOSITION; ALGORITHM	In recent years, the important and fast growth in the development and demand of multimedia products is contributing to an insufficiency in the bandwidth of devices and network storage memory. Consequently, the theory of data compression becomes more significant for reducing data redundancy in order to allow more transfer and storage of data. In this context, this paper addresses the problem of lossy image compression. Indeed, this new proposed method is based on the block singular value decomposition (SVD) power method that overcomes the disadvantages of MATLAB's SVD function in order to make a lossy image compression. The experimental results show that the proposed algorithm has better compression performance compared with the existing compression algorithms that use MATLAB's SVD function. In addition, the proposed approach is simple in terms of implementation and can provide different degrees of error resilience, which gives, in a short execution time, a better image compression.																	0334-1860	2191-026X				JAN	2020	29	1					1345	1359		10.1515/jisys-2018-0034													
J								Noise Reduction Using Modified Wiener Filter in Digital Hearing Aid for Speech Signal Enhancement	JOURNAL OF INTELLIGENT SYSTEMS										Noise; Wiener filter; real-valued FFT; power spectrum; floating point	VLSI IMPLEMENTATION; FFT; ARCHITECTURE; ERROR; LTE	Speech signals are usually affected by noises during the communication process. For suppressing the noise signal that is combined with the speech signal, a Wiener filter is adapted in digital hearing aids. Weiner filter plays an important role in noise suppression and enhancement by estimating the relation between the power spectra of the noise-affected speech signal and the noise signal. Power consumption and the hardware requirement are the important problems in adapting Weiner filter for major communication systems. In this work, we implemented an efficient Wiener filter and applied it for noise suppression along with a real-valued fast Fourier transform (FFT)/real-valued inverse FFT processor in digital hearing aids. The pipelined process was adopted for increasing the performance of the system. The proposed Wiener filter was designed to remove the iteration problems in the conventional Wiener filter. The division operation was replaced by an efficient inverse and multiplication operation in the proposed design. A modified architecture for matrix inversion with low computation complexity was implemented. The complete design computation was based on IEEE-754 standard single-precision floating-point numbers. TheWiener filter and the whole system architecture was implemented and designed on a Field Programmable Gate Array platform and simulated to validate the results in Xilinx ISE tools. An efficient reduction in power and area was obtained by adapting the proposed method for speech signal noise degradation. The performance of the proposed design was found to be 50.01% more efficient than that of existing designs.																	0334-1860	2191-026X				JAN	2020	29	1					1360	1378		10.1515/jisys-2017-0509													
J								Secure Fingerprint Authentication Using Deep Learning and Minutiae Verification	JOURNAL OF INTELLIGENT SYSTEMS										Biometrics; deep learning; convolutional neural network; inception model; minutiae; fingerprint		Nowadays, there has been an increase in security concerns regarding fingerprint biometrics. This problem arises due to technological advancements in bypassing and hacking methodologies. This has sparked the need for a more secure platform for identification. In this paper, we have used a deep Convolutional Neural Network as a pre-verification filter to filter out bad or malicious fingerprints. As deep learning allows the system to be more accurate at detecting and reducing false identification by training itself again and again with test samples, the proposed method improves the security and accuracy by multiple folds. The implementation of a novel secure fingerprint verification platform that takes the optical image of a fingerprint as input is explained in this paper. The given input is pre-verified using Google's pre-trained inception model for deep learning applications, and then passed through a minutia-based algorithm for user authentication. Then, the results are compared with existing models.																	0334-1860	2191-026X				JAN	2020	29	1					1379	1387		10.1515/jisys-2018-0289													
J								The Use of Natural Language Processing Approach for Converting Pseudo Code to C# Code	JOURNAL OF INTELLIGENT SYSTEMS										ACG; I-CASE; NLP; SRL; thematic role; verb classification	LOGIC-BASED APPROACH	Although current computer-aided software engineering tools support developers in composing a program, there is no doubt that more flexible supportive tools are needed to address the increases in the complexity of programs. This need can be met by automating the intellectual activities that are carried out by humans when composing a program. This paper aims to automate the composition of a programming language code from pseudocode, which is viewed here as a translation process for a natural language text, as pseudocode is a formatted text in natural English language. Based on this view, a new automatic code generator is developed that can convert pseudocode to C# programming language code. This new automatic code generator (ACG), which is called CodeComposer, uses natural language processing (NLP) techniques such as verb classification, thematic roles, and semantic role labeling (SRL) to analyze the pseudocode. The resulting analysis of linguistic information from these techniques is used by a semantic rule-based mapping machine to perform the composition process. CodeComposer can be viewed as an intelligent computer-aided software engineering (I_CASE) tool. An evaluation of the accuracy of CodeComposer using a binomial technique shows that it has a precision of 88%, a recall of 91%, and an F-measure of 89%.																	0334-1860	2191-026X				JAN	2020	29	1					1388	1407		10.1515/jisys-2018-0291													
J								Non-word Attributes' Eflciency in Text Mining Authorship Prediction	JOURNAL OF INTELLIGENT SYSTEMS										Machine learning; Stylometric; authorship attribution; SABA; non-word attribute		Literature scripts can be compared to paintings, in an artistic way as well as in the perspective of financial value, whereas the value of these scripts rise and fall depending on their author's popularity. Authors' scripts represent a specific style of writing that can be measured and compared using a text mining field called Stylometric. Stylometric analysis depends on some features called authorship attributes, and these attributes or features can be used in special algorithms and methods to reach that aim. Generally, each method selected in the Stylometric field uses a variety of attributes to reach higher prediction accuracy. The aim of this research is to improve the accuracy of authorship prediction in literary works based on the artistic writing style of the authors. To achieve that, a new set of attributes will be used with the Stylometric Authorship Balanced Attribution method, whichwas chosen in this research among several other machine language methods because of its delicateness in authorship prediction projects. The attributes that have been used by most of the researchers were word frequencies (single word, pair of words, or trio of words), which led to some prediction mistakes. In this research, a new set of attributes is used to decrease these mistakes. These proposed non-word attributes are named sentence length, special characters, and punctuation symbols. The results obtained by using these proposed attributes were excellent.																	0334-1860	2191-026X				JAN	2020	29	1					1408	1415		10.1515/jisys-2019-0068													
J								Design and Evaluation of Outlier Detection Based on Semantic Condensed Nearest Neighbor	JOURNAL OF INTELLIGENT SYSTEMS										UC Irvine Machine Learning Repository; normalization; outlier detection; semantic approach using condensed nearest neighbor (SACNN); Twitter	TWITTER; TOPICS	Social media contain abundant information about the events or news occurring all over the world. Social media growth has a greater impact on various domains like marketing, e-commerce, health care, e-governance, and politics, etc. Currently, Twitter was developed as one of the social media platforms, and now, it is one of the most popular social media platforms. There are 1 billion user's profiles and millions of active users, who post tweets daily. In this research, buzz detection in social media was carried out by the semantic approach using the condensed nearest neighbor (SACNN). The Twitter and Tom's Hardware data are stored in the UC Irvine Machine Learning Repository, and this dataset is used in this research for outlier detection. The min-max normalization technique is applied to the social media dataset, and additionally, missing values were replaced by the normalized value. The condensed nearest neighbor (CNN) is used for semantic analysis of the database, and based on the optimized value provided by the proposed method, the threshold is calculated. The threshold value is used to classify buzz and non-buzz discussions in the social media database. The result showed that the SACNN achieved 99% of accuracy, and relative error is less than the existing methods.																	0334-1860	2191-026X				JAN	2020	29	1					1416	1424		10.1515/jisys-2018-0476													
J								An Efficient Quality Inspection of Food Products Using Neural Network Classification	JOURNAL OF INTELLIGENT SYSTEMS										Histogram equalization; modified region growing segmentation; color histogram features; gray level co-variance matrix features; artificial neural network classifier; back-propagation algorithm	COMPUTER VISION; IMPROVING QUALITY; BEHAVIOR; SAFETY	Currently, there is a necessity for the expansion of precise, rapid, and intentional quality assurance with respect to the character of food and horticultural food items, because it is difficult to maintain and organize food products in an elevated quality and secure manner for the increasing population. In this article, we propose a procedure to resolve difficulties and to categorize food as either a broken or quality product. Therefore, the proposed process encompasses four segments, such as preprocessing, segmentation of broken division, feature extraction, and classification. At the first stage, the preprocessing method is used to remove all unnecessary noises. After that, modified region expansion-related segmentation is undertaken to segment the broken division of the food product. Then, feature extraction is used to remove the distinctive attributes of each food product to categorize their evaluation. Finally, the neural network classification procedure is used to examine the food quality. The proposed method is executed in the operational platform of MATLAB, and the consequences are examined by using obtainable methods.																	0334-1860	2191-026X				JAN	2020	29	1					1425	1440		10.1515/jisys-2018-0077													
J								Opposition Intensity-Based Cuckoo Search Algorithm for Data Privacy Preservation	JOURNAL OF INTELLIGENT SYSTEMS										PPDM; sanitization; restoration; key extraction; cuckoo search; opposition intensity	PRESERVING DATA; BIG DATA; SELECTION	Privacy-preserving data mining (PPDM) is a novel approach that has emerged in the market to take care of privacy issues. The intention of PPDM is to build up data-mining techniques without raising the risk of mishandling of the data exploited to generate those schemes. The conventional works include numerous techniques, most of which employ some form of transformation on the original data to guarantee privacy preservation. However, these schemes are quite multifaceted and memory intensive, thus leading to restricted exploitation of these methods. Hence, this paper intends to develop a novel PPDM technique, which involves two phases, namely, data sanitization and data restoration. Initially, the association rules are extracted from the database before proceeding with the two phases. In both the sanitization and restoration processes, key extraction plays a major role, which is selected optimally using Opposition Intensity-based Cuckoo Search Algorithm, which is the modified format of Cuckoo Search Algorithm. Here, four research issues, such as hiding failure rate, information preservation rate, and false rule generation, and degree of modification are minimized using the adopted sanitization and restoration processes.																	0334-1860	2191-026X				JAN	2020	29	1					1441	1452		10.1515/jisys-2018-0420													
J								M-HMOGA: A New Multi-Objective Feature Selection Algorithm for Handwritten Numeral Classification	JOURNAL OF INTELLIGENT SYSTEMS										M-HMOGA; handwritten numeral classification; feature selection; genetic algorithm; Bangla numerals; Devanagari numerals; Roman numerals	RECOGNITION	The feature selection process is very important in the field of pattern recognition, which selects the informative features so as to reduce the curse of dimensionality, thus improving the overall classification accuracy. In this paper, a new feature selection approach named Memory-Based Histogram-Oriented Multiobjective Genetic Algorithm (M-HMOGA) is introduced to identify the informative feature subset to be used for a pattern classification problem. The proposed M-HMOGA approach is applied to two recently used feature sets, namely Mojette transform and Regional Weighted Run Length features. The experimentations are carried out on Bangla, Devanagari, and Roman numeral datasets, which are the three most popular scripts used in the Indian subcontinent. In-house Bangla and Devanagari script datasets and Competition on Handwritten Digit Recognition (HDRC) 2013 Roman numeral dataset are used for evaluating our model. Moreover, as proof of robustness, we have applied an innovative approach of using different datasets for training and testing. We have used in-house Bangla and Devanagari script datasets for training the model, and the trained model is then tested on Indian Statistical Institute numeral datasets. For Roman numerals, we have used the HDRC 2013 dataset for training and the Modified National Institute of Standards and Technology dataset for testing. Comparison of the results obtained by the proposed model with existing HMOGA and MOGA techniques clearly indicates the superiority of M-HMOGA over both of its ancestors. Moreover, use of K-nearest neighbor as well as multi-layer perceptron as classifiers speaks for the classifier-independent nature of M-HMOGA. The proposed M-HMOGA model uses only about 45-50% of the total feature set in order to achieve around 1% increase when the same datasets are partitioned for training-testing and a 2-3% increase in the classification ability while using only 35-45% features when different datasets are used for training-testing with respect to the situation when all the features are used for classification.																	0334-1860	2191-026X				JAN	2020	29	1					1453	1467		10.1515/jisys-2019-0064													
J								Analogy-Based Approaches to Improve Software Project Effort Estimation Accuracy	JOURNAL OF INTELLIGENT SYSTEMS										Effort estimation; analogy-based estimation; classification; clustering; firefly optimization; fuzzy analogy; linear regression; multilayer perceptron; k-means algorithm; EM algorithm	EFFORT PREDICTION	In the discipline of software development, effort estimation renders a pivotal role. For the successful development of the project, an unambiguous estimation is necessitated. But there is the inadequacy of standard methods for estimating an effort which is applicable to all projects. Hence, to procure the best way of estimating the effort becomes an indispensable need of the project manager. Mathematical models are only mediocre in performing accurate estimation. On that account, we opt for analogy-based effort estimation by means of some soft computing techniques which rely on historical effort estimation data of the successfully completed projects to estimate the effort. So in a thorough study to improve the accuracy, models are generated for the clusters of the datasets with the confidence that data within the cluster have similar properties. This paper aims mainly on the analysis of some of the techniques to improve the effort prediction accuracy. Here the research starts with analyzing the correlation coefficient of the selected datasets. Then the process moves through the analysis of classification accuracy, clustering accuracy, mean magnitude of relative error and prediction accuracy based on some machine learning methods. Finally, a bio-inspired firefly algorithm with fuzzy analogy is applied on the datasets to produce good estimation accuracy.																	0334-1860	2191-026X				JAN	2020	29	1					1468	1479		10.1515/jisys-2019-0023													
J								Linear Regression Supporting Vector Machine and Hybrid LOG Filter-Based Image Restoration	JOURNAL OF INTELLIGENT SYSTEMS										Image processing; image restoration; linear regression-based SVM testing and training; peak signal-to-noise ratio; structural similarity index	CENTRALIZED SPARSE REPRESENTATION; NEURAL-NETWORK; ALGORITHM; OPTIMIZATION	The image restoration (IR) technique is a part of image processing to improve the quality of an image that is affected by noise and blur. Thus, IR is required to attain a better quality of image. In this paper, IR is performed using linear regression-based support vector machine (LR-SVM). This LR-SVM has two steps: training and testing. The training and testing stages have a distinct windowing process for extracting blocks from the images. The LR-SVM is trained through a block-by-block training sequence. The extracted block-by-block values of images are used to enhance the classification process of IR. In training, the imperfections on the image are easily identified by setting the target vectors as the original images. Then, the noisy image is given at LR-SVM testing, based on the original image restored from the dictionary. Finally, the image block from the testing stage is enhanced using the hybrid Laplacian of Gaussian (HLOG) filter. The denoising of the HLOG filter provides enhanced results by using block-by-block values. This proposed approach is named as LR-SVM-HLOG. A dataset used in this LR-SVM-HLOG method is the Berkeley Segmentation Database. The performance of LR-SVM-HLOG was analyzed as peak signal-to-noise ratio (PSNR) and structural similarity index. The PSNR values of the house and pepper image (color image) are 40.82 and 36.56 dB, respectively, which are higher compared to the inter- and intra-block sparse estimation method and block matching and three-dimensional filtering for color images at 20% noise.																	0334-1860	2191-026X				JAN	2020	29	1					1480	1495		10.1515/jisys-2018-0492													
J								Fractional Fuzzy Clustering and Particle Whale Optimization-Based MapReduce Framework for Big Data Clustering	JOURNAL OF INTELLIGENT SYSTEMS										Big data clustering; fractional theory; TSK clustering; MRF; PSO; WOA	DISCOVERY; ALGORITHM	The recent advancements in information technology and the web tend to increase the volume of data used in day-to-day life. The result is a big data era, which has become a key issue in research due to the complexity in the analysis of big data. This paper presents a technique called FPWhale-MRF for big data clustering using the MapReduce framework (MRF), by proposing two clustering algorithms. In FPWhale-MRF, the mapper function estimates the cluster centroids using the Fractional Tangential-Spherical Kernel clustering algorithm, which is developed by integrating the fractional theory into a Tangential-Spherical Kernel clustering approach. The reducer combines the mapper outputs to find the optimal centroids using the proposed Particle-Whale (P-Whale) algorithm, for the clustering. The P-Whale algorithm is proposed by combining Whale Optimization Algorithm with Particle Swarm Optimization, for effective clustering such that its performance is improved. Two datasets, namely localization and skin segmentation datasets, are used for the experimentation and the performance is evaluated regarding two performance evaluation metrics: clustering accuracy and DB-index. The maximum accuracy attained by the proposed FPWhale-MRF technique is 87.91% and 90% for the localization and skin segmentation datasets, respectively, thus proving its effectiveness in big data clustering.																	0334-1860	2191-026X				JAN	2020	29	1					1496	1513		10.1515/jisys-2018-0117													
J								Implementation of Improved Ship-Iceberg Classifier Using Deep Learning	JOURNAL OF INTELLIGENT SYSTEMS										Binary classification; deep learning; neural networks; pseudolabeling; semisupervised learning		The application of synthetic aperture radar (SAR) for ship and iceberg monitoring is important to carry out marine activities safely. The task of differentiating the two target classes, i.e. ship and iceberg, presents a challenge for operational scenarios. The dataset comprising SAR images of ship and iceberg poses a major challenge, as we are provided with a small number of labeled samples in the training set compared to a large number of unlabeled test samples. This paper proposes a semisupervised learning approach known as pseudolabeling to deal with the insufficient amount of training data. By adopting this approach, we make use of both labeled data (supervised learning) and unlabeled data (unsupervised learning) to build a robust convolutional neural network model that results in a superior binary classification performance of the proposed method.																	0334-1860	2191-026X				JAN	2020	29	1					1514	1522		10.1515/jisys-2018-0271													
J								Hybrid Approach for Face Recognition from a Single Sample per Person by Combining VLC and GOM	JOURNAL OF INTELLIGENT SYSTEMS										Face recognition; VLC; GOM	ORDINAL MEASURES; GABOR WAVELETS	This paper proposes a new face recognition system based on combining two feature extraction techniques: the Vander Lugt correlator (VLC) and Gabor ordinal measures (GOM). The proposed system relies on the execution speed of VLC and the robustness of GOM. In this system, we applied the Tan and Triggs and retina modeling enhancement techniques, which are well suited for VLC and GOM, respectively. We evaluated our system on the standard FERET probe data sets and on extended YaleB database. The obtained results exhibited better face recognition rates in a shorter execution time compared to the GOM technique.																	0334-1860	2191-026X				JAN	2020	29	1					1523	1534		10.1515/jisys-2018-0380													
J								Polarity Analysis of Customer Reviews Based on Part-of-Speech Subcategory	JOURNAL OF INTELLIGENT SYSTEMS										Sentiment analysis; opinion mining; polarity features; sentiment lexicon; POS tagger; SentiWordNet; natural language text	SENTIMENT ANALYSIS	Nowadays, sentiment analysis is a method used to analyze the sentiment of the feedback given by a user in an online document, such as a blog, comment, and review, and classifies it as negative, positive, or neutral. The classification process relies upon the analysis of the polarity features of the natural language text given by users. Polarity analysis has been an important subtask in sentiment analysis; however, detecting correct polarity has been a major issue. Different researchers have utilized different polarity features, such as standard part-of-speech (POS) tags such as adjectives, adverbs, verbs, and nouns. However, there seems to be a lack of research focusing on the subcategories of these tags. The aim of this research was to propose a method that better recognizes the polarity of natural language text by utilizing different polarity features using the standard POS category and the subcategory combinations in order to explore the specific polarity of text. Several experiments were conducted to examine and compare the efficacies of the proposed method in terms of F-measure, recall, and precision using an Amazon dataset. The results showed that JJ + NN + VB + RB + VBP + RP, which is a POS subcategory combination, obtained better accuracy compared to the baseline approaches by 4.4% in terms of F-measure.																	0334-1860	2191-026X				JAN	2020	29	1					1535	1544		10.1515/jisys-2018-0356													
J								A 4D Trajectory Prediction Model Based on the BP Neural Network	JOURNAL OF INTELLIGENT SYSTEMS										Air traffic; 4D trajectory prediction; BP neural network; k-means clustering; real-time prediction		To solve the problem that traditional trajectory prediction methods cannot meet the requirements of high-precision, multi-dimensional and real-time prediction, a 4D trajectory prediction model based on the backpropagation (BP) neural network was studied. First, the hierarchical clustering algorithm and the k-means clustering algorithm were adopted to analyze the total flight time. Then, cubic spline interpolation was used to interpolate the flight position to extract the main trajectory feature. The 4D trajectory prediction model was based on the BP neural network. It was trained by Automatic Dependent Surveillance - Broadcast trajectory from Qingdao to Beijing and used to predict the flight trajectory at future moments. In this paper, the model is evaluated by the common measurement index such as maximum absolute error, mean absolute error and root mean square error. It also gives an analysis and comparison of the predicted over-point time, the predicted over-point altitude, the actual over-point time and the actual over-point altitude. The results indicate that the predicted 4D trajectory is close to the real flight data, and the time error at the crossing point is no more than 1 min and the altitude error at the crossing point is no more than 50 m, which is of high accuracy.																	0334-1860	2191-026X				JAN	2020	29	1					1545	1557		10.1515/jisys-2019-0077													
J								A Blind Medical Image Watermarking for Secure E-Healthcare Application Using Crypto-Watermarking System	JOURNAL OF INTELLIGENT SYSTEMS										Crypto-watermarking; electronic health record; SHA-256; embedding; extraction; face image; advanced encryption standard (AES); segmentation and active contour	TAMPER DETECTION; TRANSFORM; SCHEME	A reliable medical image management must provide proper security for patient information. Protecting the medical information of the patients is a major concern in all hospitals. Digital watermarking is a procedure prevalently used to secure the confidentiality of medical information and maintain them, which upgrades patient health awareness. To protect the medical information, the robust and lossless patient medical information sharing system using crypto-watermarking method is proposed. The proposed system consists of two phases: (i) embedding and (ii) extraction. In this paper, we securely share three types of patient information, medical image, electronic health record (EHR), and face image from one hospital to another hospital. Initially, all the three inputs are encrypted and the information is concordant. In order to enhance the robustness of the crypto-watermarking system, the obtained bit stream is compressed, and the compressed bit streams are embedded into the cover image. The same process is repeated for the extraction process. The experimentation result is carried out using different medical images with EHR, and the effectiveness of the proposed algorithm is analyzed with the help of peak signal to noise ratio.																	0334-1860	2191-026X				JAN	2020	29	1					1558	1575		10.1515/jisys-2018-0370													
J								Discriminating Healthy Wheat Grains from Grains Infected with Fusarium graminearum Using Texture Characteristics of Image-Processing Technique, Discriminant Analysis, and Support Vector Machine Methods	JOURNAL OF INTELLIGENT SYSTEMS										Fusarium graminearum; image processing; classification; discrimination analysis; support vector machine	CLASSIFICATION; FEATURES; VISION	Among agricultural plants, wheat, with valuable foodstuffs such as proteins, vitamins, and minerals, provides about 25% of the world's food calories. Hence, providing its health conditions and quality is of great importance. One of the most important wheat diseases that causes a lot of damages to this product is Fusarium head blight (FHB). In most areas, the causal agent of disease is Fusarium graminearum. This disease not only decreases product quality and efficiency but also has harmful effects on humans and animals bymycotoxin production. FHB discrimination requires experimental work in special conditions and also experts, but these facilities may not be available at customs and other related grain health testing centers. In this study, discriminating healthy wheat grains and the grains infected with F. graminearum was performed with an image-processing technique, an accurate, rapid, and nondestructive method. First, healthy and infected wheat grains were selected, and then digital images of samples were prepared in randomized mass method using cameras and lightening chamber. Then using the image-processing technique, a total of 21 texture characteristics were obtained for each grain. Discrimination and classification of healthy and infected grains were done with 100% accuracy using extracted texture characteristics and two techniques mentioned above. The results of this research could be helpful in the development of automatic devices for rapid discrimination of healthy grains and grains infected with F. graminearum, one of the most destructive wheat diseases.																	0334-1860	2191-026X				JAN	2020	29	1					1576	1586		10.1515/jisys-2018-0430													
J								License Plate Recognition in Urban Road Based on Vehicle Tracking and Result Integration	JOURNAL OF INTELLIGENT SYSTEMS										Convolutional neural network (CNN); license plate recognition (LPR); low quality; result integration; vehicle tracking		Multiple surveillance cameras provide huge video resources that need further mining to collect traffic stream data such as license plate recognition (LPR). However, these surveillance cameras have limited spatial resolution, which may not always suffice to precisely recognize license plates by existing LPR systems. This work is focused on the LPR method in low-quality images from surveillance video screenshots on urban road. The methodology we proposed is based on vehicle tracking and result integration, and we recognize the plate with an end-to-end method without character segmentation. First, we track each vehicle to get vehicle tracking sequence. Moreover, a plate detector based on an object detection framework is trained to detect license plates of each vehicle from the sequence and a license plate sequence is formed. In addition, an end-to-end convolutional neural network architecture is applied to recognize license plates from the sequence. Finally, we integrate the recognition result of continuous frames to get the final result. Evaluation results on multiple datasets show that our method significantly outperforms others without segmentation or integration in real traffic scene.																	0334-1860	2191-026X				JAN	2020	29	1					1587	1597		10.1515/jisys-2018-0446													
J								Binary Genetic Swarm Optimization: A Combination of GA and PSO for Feature Selection	JOURNAL OF INTELLIGENT SYSTEMS										Feature selection; binary genetic swarm optimization; genetic algorithm; particle swarm optimization; UCI dataset; average weighted combination; sequential one-point flipping	ALGORITHM	Feature selection (FS) is a technique which helps to find the most optimal feature subset to develop an efficient pattern recognition model under consideration. The use of genetic algorithm (GA) and particle swarm optimization (PSO) in the field of FS is profound. In this paper, we propose an insightful way to perform FS by amassing information from the candidate solutions produced by GA and PSO. Our aim is to combine the exploitation ability of GA with the exploration capacity of PSO. We name this new model as binary genetic swarm optimization (BGSO). The proposed method initially lets GA and PSO to run independently. To extract sufficient information from the feature subsets obtained by those, BGSO combines their results by an algorithm called average weighted combination method to produce an intermediate solution. Thereafter, a local search called sequential one-point flipping is applied to refine the intermediate solution further in order to generate the final solution. BGSO is applied on 20 popular UCI datasets. The results were obtained by two classifiers, namely, k nearest neighbors (KNN) and multi-layer perceptron (MLP). The overall results and comparisons show that the proposed method outperforms the constituent algorithms in 16 and 14 datasets using KNN and MLP, respectively, whereas among the constituent algorithms, GA is able to achieve the best classification accuracy for 2 and 7 datasets and PSO achieves best accuracy for 2 and 4 datasets, respectively, for the same set of classifiers. This proves the applicability and usefulness of the method in the domain of FS.																	0334-1860	2191-026X				JAN	2020	29	1					1598	1610		10.1515/jisys-2019-0062													
J								Enhanced Twitter Sentiment Analysis Using Hybrid Approach and by Accounting Local Contextual Semantic	JOURNAL OF INTELLIGENT SYSTEMS										Twitter sentiment analysis; SentiWordNet; negation handling; lexical modifier; shift approach; sentiment analysis; negation exception		This paper addresses the problem of Twitter sentiment analysis through a hybrid approach in which SentiWordNet (SWN)-based feature vector acts as input to the classification model Support Vector Machine. Our main focus is to handle lexical modifier negation during SWN score calculation for the improvement of classification performance. Thus, we present naive and novel shift approach in which negation acts as both sentiment-bearing word and modifier, and then we shift the score of words from SWN based on their contextual semantic, inferred from neighbouring words. Additionally, we augment negation accounting procedure with a few heuristics for handling the cases in which negation presence does not necessarily mean negation. Experimental results show that the contextual-based SWN feature vector obtained through shift polarity approach alone led to an improved Twitter sentiment analysis system that outperforms the traditional reverse polarity approach by 2-6%. We validate the effectiveness of our hybrid approach considering negation on benchmark Twitter corpus from SemEval-2013 Task 2 competition.																	0334-1860	2191-026X				JAN	2020	29	1					1611	1625		10.1515/jisys-2019-0106													
J								Cloud Security: LKM and Optimal Fuzzy System for Intrusion Detection in Cloud Environment	JOURNAL OF INTELLIGENT SYSTEMS										Intrusion detection system; cloud computing; cloud security; greywolf optimization; leader-based k-means clustering; fuzzy logic system; KDD Cup 99		In cloud security, intrusion detection system (IDS) is one of the challenging research areas. In a cloud environment, security incidents such as denial of service, scanning, malware code injection, virus, worm, and password cracking are getting usual. These attacks surely affect the company and may develop a financial loss if not distinguished in time. Therefore, securing the cloud from these types of attack is very much needed. To discover the problem, this paper suggests a novel IDS established on a combination of a leader-based k-means clustering (LKM), optimal fuzzy logic system. Here, at first, the input dataset is grouped into clusters with the use of LKM. Then, cluster data are afforded to the fuzzy logic system (FLS). Here, normal and abnormal data are inquired by the FLS, while FLS training is done by the grey wolf optimization algorithm through maximizing the rules. The clouds simulator and NSL-Knowledge Discovery and DataBase (KDD) Cup 99 dataset are applied to inquire about the suggested method. Precision, recall, and F-measure are conceived as evaluation criteria. The obtained results have denoted the superiority of the suggested method in comparison with other methods.																	0334-1860	2191-026X				JAN	2020	29	1					1626	1642		10.1515/jisys-2018-0479													
J								Power Average Operators of Trapezoidal Cubic Fuzzy Numbers and Application to Multi-attribute Group Decision Making	JOURNAL OF INTELLIGENT SYSTEMS										Multi-attribute group decision making; trapezoidal cubic fuzzy number; Hausdorff metric; power average operator	GENERALIZED AGGREGATION OPERATORS; SIMILARITY MEASURE; MODELS; SETS; INFORMATION	Trapezoidal cubic fuzzy numbers (TzCFNs) are an extraordinary cubic fuzzy set on a real number set. TzCFNs are useful for dealing with well-known quantities in decision data and decision making problems themselves. This paper is about multi-attribute group decision making problems in which the attribute values are stated with TzCFNs, which are solved by developing a new decision method based on power average operators of TzCFNs. The new operation laws for TzCFNs are given. Hereby, the power average operator of real numbers is extended to four kinds of power average operators of TzCFNs, involving the power average operator of TzCFNs, the weighted power average operator of TzCFNs, the power ordered weighted average operator of TzCFNs, and the power hybrid average operator of TzCFNs. In the proposed group decision method, the individual overall evaluation values of alternatives are generated by using the power average operator of TzCFNs. Applying the hybrid average operator of TzCFNs, the specific general evaluation standards of alternatives are then combined into the collective ones, which are used to rank the alternatives. The example analysis shows the practicality and effectiveness of the proposed method.																	0334-1860	2191-026X				JAN	2020	29	1					1643	1661		10.1515/jisys-2018-0122													
J								Multi-View Video Synopsis via Simultaneous Object-Shifting and View-Switching Optimization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-view synopsis; surveillance cameras; view switching; graph cuts; dynamic programming	ENERGY MINIMIZATION; FRAMEWORK	We present a method for synopsizing multiple videos captured by a set of surveillance cameras with some overlapped field-of-views. Currently, object-based approaches that directly shift objects along the time axis are already able to compute compact synopsis results for multiple surveillance videos. The challenge is how to present the multiple synopsis results in a more compact and understandable way. Previous approaches show them side by side on the screen, which however is difficult for user to comprehend. In this paper, we solve the problem by joint object-shifting and camera view-switching. Firstly, we synchronize the input videos, and group the same object in different videos together. Then we shift the groups of objects along the time axis to obtain multiple synopsis videos. Instead of showing them simultaneously, we just show one of them at each time, and allow to switch among the views of different synopsis videos. In this view switching way, we obtain just a single synopsis results consisting of content from all the input videos, which is much easier for user to follow and understand. To obtain the best synopsis result, we construct a simultaneous object-shifting and view-switching optimization framework instead of solving them separately. We also present an alternative optimization strategy composed of graph cuts and dynamic programming to solve the unified optimization. Experiments demonstrate that our single synopsis video generated from multiple input videos is compact, complete, and easy to understand.																	1057-7149	1941-0042					2020	29						971	985		10.1109/TIP.2019.2938086													
J								Sparsification of core set models in non-metric supervised learning	PATTERN RECOGNITION LETTERS										Large scale indefinite learning; Krein space; Sparse models; Orthogonal matching pursuit; Core sets; Non-metric learning	INDEFINITE; CLASSIFICATION	Supervised learning employing positive semi definite kernels has gained wide attraction and lead to a variety of successful machine learning approaches. The restriction to positive semi definite kernels and a hilbert space is common to simplify the mathematical derivations of the respective learning methods, but is also limiting because more recent research indicates that non-metric, and therefore non positive semi definite, data representations are often more effective. This challenge is addressed by multiple approaches and recently dedicated algorithms for so called indefinite learning have been proposed. Along this line, the Krein space Support Vector Machine (KSVM) and variants are very efficient classifiers for indefinite learning problems, but with a non-sparse decision function. This very dense decision function prevents practical applications due to a costly out of sample extension. We focus on this problem and provide two post processing techniques to sparsify models as obtained by a Krein space SVM approach. In particular we consider the indefinite Core Vector Machine and indefinite Core Vector Regression Machine which are both efficient for psd kernels, but suffer from the same dense decision function, if the Krein space approach is used. We evaluate the influence of different levels of sparsity and employ a Nystrom approach to address large scale problems. Experiments show that our algorithm is similar efficient as the non-sparse Krein space Support Vector Machine but with substantially lower costs, such that also problems of larger scale can be processed. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						1	7		10.1016/j.patrec.2019.10.024													
J								GF-Net: Improving machine reading comprehension with feature gates	PATTERN RECOGNITION LETTERS										Gate mechanism for linguistic feature; Gated feature network; Machine reading comprehension		Machine reading comprehension (MRC) is a field of question-answering in which computers understand given passages and answer related questions. Several previous models have tried to combine the use of linguistic and word embedding features to improve the performance of MRC; however, they could not obtain successful results because of feature interference problems caused by simple concatenation of the two. To resolve these problems, a machine reading comprehension model called gated feature network (GF-Net) is proposed in which linguistic features are selectively used according to their roles in the process of answer selection. In the GF-Net, the weights of the linguistic features are automatically controlled through gate mechanisms called feature gates. In the experiments with Stanford Question Answering Dataset SQuAD, the MRC models with feature gates showed a 0.67%p higher average of exact match (EM) and 0.64%p higher average of F1-score than models without feature gates. In addition, the GF-Net outperformed the previous MRC models to which feature gates were added. Based on these experimental results, it is concluded that the gate mechanism can contribute to an improvement in the performance of MRC models and the architecture of the GF-Net is suitable for the task of MRC. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						8	15		10.1016/j.patrec.2019.10.030													
J								Improved local search for graph edit distance	PATTERN RECOGNITION LETTERS										Graph edit distance; Local search; Stochastic warm start		The graph edit distance (GED) measures the dissimilarity between two graphs as the minimal cost of a sequence of elementary operations transforming one graph into another. This measure is fundamental in many areas such as structural pattern recognition or classification. However, exactly computing GED is NP-hard. Among different classes of heuristic algorithms that were proposed to compute approximate solutions, local search based algorithms provide the tightest upper bounds for GED. In this paper, we present K-REFINE and RANDPOST. K-REFINE generalizes and improves an existing local search algorithm and performs particularly well on small graphs. RANDPOST is a general warm start framework that stochastically generates promising initial solutions to be used by any local search based GED algorithm. It is particularly efficient on large graphs. An extensive empirical evaluation demonstrates that both K-REFINE and RANDPOST perform excellently in practice. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						19	25		10.1016/j.patrec.2019.10.028													
J								Ethnicity classification by the 3D Discrete Landmarks Model measure in Kendall shape space	PATTERN RECOGNITION LETTERS										Ethnicity classification; Facial Landmark Model; Kendall shape space	GENDER CLASSIFICATION; FACE	Knowledge of human ethnicity constitutes important biometric information. An automated ethnicity classification is a good first step in facial analysis. However, most ethnicity classification methods require a complex feature extraction and model training process. We propose a novel ethnicity classification method based on the analysis of facial landmarks in Kendall shape space. Facial features with different relative positions have a close relationship with ethnicity. Facial landmarks can represent positions of facial features. We build a Discrete Landmarks Model (DLM) based on facial landmarks and construct an ethnicity classification model based on the DLM analysis. The clear advantages of our method are that it is fully automated; requires no complex data preprocessing, feature extraction or a complex training process; results in a fast and accurate classification process. We estimate the effectiveness of our method experimentally, using public databases such as Texas3D, FRGC2.0, BU-3DFE and BU-4DFE. On average, our method can achieve a 95% ethnicity classification rate with each classification attempt in 2.0 s. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						26	32		10.1016/j.patrec.2019.10.035													
J								Incremental bit-quads count in component trees: Theory, algorithms, and optimization	PATTERN RECOGNITION LETTERS										Mathematical morphology; Component tree; Bit-quads; Attributes	SEGMENTATION	Component tree is a full image representation which encodes all connected components from upper (resp. lower) level sets of a given image through the inclusion relation. Information from this representation can be used in many image processing and computational vision applications, e.g. connected filtering, image segmentation, feature extraction, among others. In general, each node of a component tree represents a connected component of a level set and stores attributes which describes features of this connected component. This paper presents a review of a previously published method to compute attributes such as area, perimeter, and number of Euler by incrementally counting patterns while traversing nodes of a component tree. This method foundation is further detailed in this paper by presenting a novel theoretical background and algorithm correctness intuition. We also present a novel approach for this algorithm showing improvements for run-time execution and precision analysis. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						33	40		10.1016/j.patrec.2019.10.036													
J								Combinatorial space of watershed hierarchies for image characterization	PATTERN RECOGNITION LETTERS										Mathematical morphology; Hierarchies; Gromov-Hausdorff distance; Combination of hierarchies		We propose a framework for image characterization using hierarchies of segmentations. For this purpose, we structure the space of hierarchies using the Gromov-Hausdorff distance. We propose different ways of combining hierarchies and study their properties thanks to the GH distance. We then expose how to leverage the combinatorial space of hierarchies to derive efficient image representations. This framework opens a path for a controlled exploration and use of the combinatorial space of hierarchies. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						41	47		10.1016/j.patrec.2019.11.002													
J								Merge-and-simplify operation for compact combinatorial pyramid definition	PATTERN RECOGNITION LETTERS										Hierarchical image representation; Combinatorial pyramids; 2D combinatorial map	GENERALIZED MAP PYRAMIDS; IMAGE SEGMENTATION; RECEPTIVE-FIELDS; CONTRACTION	Image pyramids are employed for years in digital image processing. They permit to store and use different scales/levels of details of an image. To represent all the topological information of the different levels, combinatorial pyramids have proved having many interests. But, when using an explicit representation, one drawback of this structure is the memory space required to store such a pyramid. In this paper, this drawback is solved by defining a compact version of combinatorial pyramids. This definition is based on the definition of a new operation, called "merge-and-simplify", which simultaneously merges regions and simplifies their boundaries. Our experiments show that the memory space of our solution is much smaller than the one of the original version. Moreover, the computation time of our solution is faster, because there are less levels in our pyramid than in the original one. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						48	55		10.1016/j.patrec.2019.11.009													
J								Learning the Principles of Art History with convolutional neural networks	PATTERN RECOGNITION LETTERS										Convolutional neural networks; Fine art; High-level image features; Wolfflin	DISCOVERY	Understanding the historical transformation of artistic styles implies the recognition of different stylistic properties. From a computer vision perspective, stylistic properties represent complex image features. In our work we explore the use of convolutional neural networks for learning features that are relevant for understanding properties of artistic styles. We focus on stylistic properties described by Heinrich Wolfflin in his book Principles of Art History (1915). Wolfflin identified five key visual principles, each defined by two contrasting concepts. We refer to each principle as one high-level image feature that measures how much each of the contrasting concepts is present in an image. We introduce convolutional neural network regression models trained to predict values of the five Wolfflin's features. We provide quantitative and qualitative evaluations of those predictions, as well as analyze how the predicted values relate to different styles and artists. The outcome of our analysis suggests that the models learn to discriminate meaningful features that correspond to the visual characteristics of concepts described by Wolfflin. This indicates that the presented approach can be used to enable new ways of exploring fine art collections based on image features relevant and well-known within art history. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						56	62		10.1016/j.patrec.2019.11.008													
J								Multi-scale sequential network for semantic text segmentation and localization	PATTERN RECOGNITION LETTERS												We present a novel method for semantic text document analysis which in addition to localizing text it labels the text in user-defined semantic categories. More precisely, it consists of a fully-convolutional and sequential network that we apply to the particular case of slide analysis to detect title, bullets and standard text. Our contributions are twofold: (1) A multi-scale network consisting of a series of stages that sequentially refine the prediction of text and semantic labels (text, title, bullet); (2) A synthetic database of slide images with text and semantic annotation that is used to train the network with abundant data and wide variability in text appearance, slide layouts, and noise such as compression artifacts. We evaluate our method on a collection of real slide images collected from multiple conferences, and show that it is able to localize text with an accuracy of 95%, and to classify titles and bullets with accuracies of 94% and 85% respectively. In addition, we show that our method is competitive on scene and born-digital image datasets, such as ICDAR 2011, where it achieves an accuracy of 91.1%. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						63	69		10.1016/j.patrec.2019.11.001													
J								Robust ECG biometrics using GNMF and sparse representation	PATTERN RECOGNITION LETTERS										ECG; Biometrics; GNMF; Sparse representation; L1 norm	NEURAL-NETWORK; RECOGNITION; FACE	As a vital sign, Electrocardiogram (ECG) has highly discriminative characteristics in the field of biometrics. This paper aims to propose a novel robust ECG biometric method based on graph regularized nonnegative matrix factorization (GNMF) and sparse representation. First, after ECG signal pre-processing and heartbeat segmentation, GNMF is used to reduce the dimensions of each heartbeat. In GNMF, an affinity graph is constructed to encode the geometrical information and label information in order to obtain more discriminative features. Second, in order to seek highly discriminability of ECG, the sparse representation is utilized to perform final feature extraction. We evaluate the method on two public datasets: ECG-ID and MIT-BIH Arrhythmia (MITDB). When fusing three heartbeats as a test sample, the accuracy achieves 98.03% and 100% on the ECG-ID dataset and the MITDB dataset, respectively. Experimental results show that the proposed method is robust for within-session and across-session of the ECG signal. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						70	76		10.1016/j.patrec.2019.11.005													
J								Lungs cancer classification from CT images: An integrated design of contrast based classical features fusion and selection	PATTERN RECOGNITION LETTERS										Lungs cancer; Contrast normalization; Multiple features; Fusion; Selection	SEGMENTATION; TUMOR	Lung cancer is a fatal type of cancer and it causes of severe deaths of approximately 422 people every day, worldwide. However, an early diagnosis is an expedient requirement for increasing the chances of human survival. In this regard the existing techniques of tumor detection, CT scans are mostly utilized to recognize the infected regions, nevertheless, the major challenges of CT images are low visibility of tumor regions, negative rates, to name but a few. In this work, we propose a novel design of contrast stretching based classical features fusion process for localizing the of lungs cancer classification. The proposed method encompasses three significant steps: firstly, contrast of original CT images is improved by gamma correction max intensity weights approach. Secondly, multiple texture, point, and geometric features are extracted from contrast images, which are later subjected to a serial canonical correlation-based fusion. Thirdly, zero values and negative features are replaced by an entropy-based approach, followed by weighted NCA for selection. Most discriminate features are fed into ensemble classifier for final classification. The validation of the proposed method is conducted on publicly available dataset: Lungs Data Science Bowl 2017 to achieving maximum accuracy of 99.4%. The numerical results clearly show that the performance of our proposed method outperforms in comparison with several existing methods with greater accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						77	85		10.1016/j.patrec.2019.11.014													
J								Reduced data set for multi-target recognition using compressed sensing frame	PATTERN RECOGNITION LETTERS										Compressed sensing frame; Clustering; Dictionary coding; Vector space; Multi-target recognition; Tiny feature	SIGNAL RECONSTRUCTION; FACE RECOGNITION; OBJECT; ALGORITHM; RECOVERY	Oceanographic plankton classification for many images is time consuming, especially for low-contrast images obtained when the water is dirty and opaque due to impurities. A novel, overcomplete dictionary algorithm is studied by analyzing the sparse characteristics of the image matrix using pixel values. The features in hyperspace are mapped onto a specifically designed vector space. Thus, mathematically, the algorithm exhibits faster calculating convergence and has a strong expressivity for the selective signal in the vector space with a lower signal loss rate. The clustering method based on the dictionary can classify planktons for species counting, which can enable high-speed, multi-object recognition of planktons in turbid water. The experimental results demonstrate that if less data (up to 60%) is processed for each image, a recall rate and accuracy greater than 75% and a structural similarity for the reconstructed image greater than 0.9 can be achieved. (C) 2019 The Author(s). Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						86	91		10.1016/j.patrec.2019.10.027													
J								Delaunay triangulation based text detection from multi-view images of natural scene	PATTERN RECOGNITION LETTERS												Text detection in the wild is still considered as a challenging issue to the researchers because of its several real time applications like forensic application, where CCTV camera captures images at different angles of the same scene. Unlike the existing methods that consider a single view captured orthogonally for text detection, this paper considers multi-view (view-1 and view-2 of the same spot) of the same scene captured at different angles or different height distances for text detection. For each pair of the same scene, the proposed method extracts features that describe characteristics of text components based on Delaunay Triangulation (DT), namely corner points, area and cavity of the DT. The features of corresponding DT in view-1 and view-2 are compared through cosine distance measure to estimate the similarity between two components of respective view-1 and view-2. If the pair satisfies the similarity condition, the components are considered as Candidate Text Components (CTC). In other words, these are the common components for view-1 and view-2 that satisfy the similarity condition. From each CTC of view-1 and view-2, the proposed method finds nearest neighbor components to restore the components of the same text line based on estimating degree of similarly between CTC and neighbor components using Chi-square and cosine distance measures. Furthermore, the proposed method uses a recognition step to detect correct texts by comparing recognition results of view-1 and view-2. The same recognition step is used for removing false positives to improve the performance of the proposed method. Experimental results on our own dataset, which contains pair of images of different situations, and the standard datasets, namely, ICDAR 2013, MSRATD-500, CTW1500, Total-text, ICDAR 2017 MLT and COCO-text, show that the proposed method outperforms the existing methods. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						92	100		10.1016/j.patrec.2019.11.021													
J								Depth estimation from stereo cameras through a curved transparent medium	PATTERN RECOGNITION LETTERS										Depth from stereo; Camera calibration; Refraction; Parametric surface model	RECONSTRUCTION	In this paper, we propose a novel method for estimating depth values by stereo cameras through a curved transparent medium that causes refraction. Our method takes both the surface shape of the medium and the refraction into account. We model that the rays from the stereo cameras are refracted by a curved transparent medium whose inner surface is represented by a parametric model, assuming that the medium has constant thickness. The parameters of the model are estimated using a constrained optimization simply by attaching several markers on the inner surface. The depth value is then estimated by the triangulation considering the refraction based on the model. The experimental results show that our method yields consistently high error reduction rates with respect to the baseline method without considering the refraction caused by the medium. In addition, our method provides satisfactory estimates for various shapes of the medium. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						101	107		10.1016/j.patrec.2019.11.012													
J								Selective feature connection mechanism: Concatenating multi-layer CNN features with a feature selector	PATTERN RECOGNITION LETTERS										Feature combination; Network architecture; Selective feature connection mechanism; Convolutional neural network		Different layers of deep convolutional neural networks(CNNs) can encode different-level information. High-layer features always contain more semantic information, and low-layer features contain more detail information. However, low-layer features suffer from the background clutter and semantic ambiguity. During visual recognition, the feature combination of the low-layer and high-level features plays an important role in context modulation. If directly combining the high-layer and low-layer features, the background clutter and semantic ambiguity may be caused due to the introduction of detailed information. In this paper, we propose a general network architecture to concatenate CNN features of different layers in a simple and effective way, called Selective Feature Connection Mechanism (SFCM). Low-level features are selectively linked to high-level features with a feature selector which is generated by high-level features. The proposed connection mechanism can effectively overcome the above-mentioned drawbacks. We demonstrate the effectiveness, superiority, and universal applicability of this method on multiple challenging computer vision tasks, including image classification, scene text detection, and image-to-image translation. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						108	114		10.1016/j.patrec.2019.11.015													
J								Brain tumor classification based on DWT fusion of MRI sequences using convolutional neural network	PATTERN RECOGNITION LETTERS										Sequences; CNN; DWT; Global thresholding; Filter	OF-THE-ART; SEGMENTATION; MODEL; FRAMEWORK; FEATURES; CNN	Tumor in brain is an anthology of anomalous cells. It leads to increase in death rate among humans. Therefore, in this manuscript, a fusion process is proposed to combine structural and texture information of four MRI sequences (T1C, T1, Flair and T2) for the detection of brain tumor. A discrete wavelet transform (DWT) along with Daubechies wavelet kernel is utilized for fusion process which provides a more informative tumor region as compared to an individual single sequence of MRI. After the fusion process, a partial differential diffusion filter (PDDF) is applied to remove noise. A global thresholding method is used for segmenting tumor region which is then fed to proposed convolutional neural network (CNN) model for finally differentiating tumor and non-tumor regions. Five publicly available datasets i.e., BRATS 2012, BRATS 2013, BRATS 2015, BRATS 2013 Leader board and BRATS 2018 are used for proposed method evaluation. The results show that fused images provide better results as compared to individual sequences on benchmark datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						115	122		10.1016/j.patrec.2019.11.016													
J								Integrating prediction and reconstruction for anomaly detection	PATTERN RECOGNITION LETTERS										Anomaly detection; Reconstruction; Future frame prediction		Anomaly detection in videos refers to identifying events that rarely or shouldn't happen in a certain context. Among all existing methods, the idea of reconstruction or future frame prediction is predominant for detecting anomalies. Reconstruction methods try to minimize the reconstruction errors of training data, but cannot guarantee large reconstruction errors for abnormal events. Future frame prediction methods follow the concept that normal events are predictable while abnormal ones are unpredictable. However, the results may drop rapidly since prediction is not robust to the noise in real-world surveillance videos. In this paper, we propose an approach that combines the advantages and balances the disadvantages of these two methods. An end-to-end network is designed to conduct future frame prediction and reconstruction sequentially. Future frame prediction makes the reconstruction errors large enough to facilitate the identification of abnormal events, while reconstruction helps enhance the predicted future frames from normal events. Specifically, we connect two U-Net blocks in the generator. One block works in the form of frame prediction, and the other tries to reconstruct the frames generated by the former block. Experiments over several benchmark datasets demonstrate the superiority of our method over previous state-of-the-art approaches, while running in real-time at 30 frames per second. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						123	130		10.1016/j.patrec.2019.11.024													
J								MASAT: A fast and robust algorithm for pose-graph initialization	PATTERN RECOGNITION LETTERS											EFFICIENT; ALIGNMENT	In this paper, we propose a novel algorithm to compute the initial structure of pose-graph based Simultaneous Localization and Mapping (SLAM) systems. We perform a Breadth-First Search (BFS) on the graph in order to obtain multiple votes regarding the location of a certain robot position from all of its previously processed neighbors. Next, we define the initial location of a pose as the average of the multiple alternatives. By adopting the proposed initialization approach, the number of iterations needed for optimization is significantly reduced while the computational complexity remains lightweight. We perform quantitative evaluation on various 2D and 3D benchmark datasets to demonstrate the advantages of the proposed method. (C) 2019 The Authors. Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						131	136		10.1016/j.patrec.2019.11.010													
J								An end-to-end deep learning system for medieval writer identification	PATTERN RECOGNITION LETTERS										Deep learning; Transfer learning; Writer identification; Row detection; Avila bible; Digital paleography	RETRIEVAL	This paper presents an end-to-end system to identify writers in medieval manuscripts. The proposed system consists in a three-step model for detection and classification of lines in the manuscript and page writer identification. The first two steps are based on deep neural networks trained with transfer learning techniques and specialized to solve the task in hand. The third stage is a weighted majority vote row-decision combiner that assigns to each page a writer. The main goal of this paper is to study the applicability of deep learning in this context when a relatively small training dataset is available. We tested our system with several state-of-the-art deep architectures on a digitized manuscript known as the Avila Bible, using only 9.6% of the total pages for training. Our approach proves to be very effective in identifying page writers, reaching a peak of 96.48% of accuracy and 96.56% of F1 score. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						137	143		10.1016/j.patrec.2019.11.025													
J								Hyperspectral anomaly detection via density peak clustering	PATTERN RECOGNITION LETTERS										Anomaly detection; Density peak clustering; Hyperspectral image	CLASSIFICATION	In the last few years, a density peak clustering algorithm (DP) has demonstrated its advantages in hyperspectral data analysis and processing. In this letter, we take the benefits of the DP algorithm to the hyperspectral anomaly detection, to circumvent two negative aspects which affect the detection performance: The untenable supposition of the Gaussian distribution and the contamination of the background statistics caused by anomalies. Specifically, the proposed DP-based hyperspectral anomaly detection method is implemented as follows: A hyperspectral image (HSI) is first divided into local windows to address computationally expensive density computations. In each local window, the DP is performed to calculate the density of each pixel. Finally, we detect anomalies using the obtained density map, based on that anomalies are generally with low probability of existence in the image and thus have low densities. Experimental results obtained on four real hyperspectral datasets demonstrate that the detection performance of the proposed method is superior to some widely used anomaly detection methods. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						144	149		10.1016/j.patrec.2019.11.022													
J								An integrated design of particle swarm optimization (PSO) with fusion of features for detection of brain tumor	PATTERN RECOGNITION LETTERS										BSE; PSO; GA; LBP; Deep features; ANN	IMAGE SEGMENTATION; MR-IMAGES; SEMIAUTOMATIC SEGMENTATION; NEURAL-NETWORKS; CLASSIFICATION; MACHINE; REGIONS	Tumor in brain is a major cause of death in human beings. If not treated properly and timely, there is a high chance of it to become malignant. Therefore, brain tumor detection at an initial stage is a significant requirement. In this work, initially the skull is removed through brain surface extraction (BSE) method. The skull removed image is then fed to particle swarm optimization (PSO) to achieve better segmentation. In the next step, Local binary patterns (LBP) and deep features of segmented images are extracted and genetic algorithm (GA) is applied for best features selection. Finally, artificial neural network (ANN) and other classifiers are utilized to classify the tumor grades. The publicly available complex brain datasets such as RIDER and BRATS 2018 Challenge are utilized for evaluation of method and attained 99% maximum accuracy. The results are also compared with existing methods which evident that the presented technique provided improved outcomes which are clear proof of its effectiveness and novelty. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						150	157		10.1016/j.patrec.2019.11.017													
J								A bag of constrained informative deep visual words for image retrieval	PATTERN RECOGNITION LETTERS										Deep Informative Patch; Bag of Visual Words; LCVQE; Mutual Information	K-MEANS	In this paper, we propose a bag of constrained informative deep visual words (BoCIDVW) model for image retrieval. Informative patches from each image are first obtained using patch entropy values. Each such patch is represented by deep features extracted through VGG16-Net. Two sets of constraints, namely, the must-link (ML) and the cannot-link (CL), are obtained for each deep informative patch in an unsupervised manner from its mutual information values (with other patches). The patches are then quantized using the Linear-time Constrained Vector Quantization Error (LCVQE), a fast yet accurate constrained K-means algorithm. The resulting clusters, which we term constrained informative deep visual words, are employed to label each patch. Finally, a bag (histogram) of constrained informative visual words is developed for image retrieval. Experiments on three different publicly available datasets demonstrate the merit of the proposed formulation. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						158	165		10.1016/j.patrec.2019.11.011													
J								Explaining digital humanities by aligning images and textual descriptions	PATTERN RECOGNITION LETTERS										Visual-semantic retrieval; Semi-supervised learning; Cultural heritage		Replicating the human ability to connect Vision and Language has recently been gaining a lot of attention in the Computer Vision and the Natural Language Processing communities. This research effort has resulted in algorithms that can retrieve images from textual descriptions and vice versa, when realistic images and sentences with simple semantics are employed and when paired training data is provided. In this paper, we go beyond these limitations and tackle the design of visual-semantic algorithms in the domain of the Digital Humanities. This setting not only advertises more complex visual and semantic structures but also features a significant lack of training data which makes the use of fully-supervised approaches infeasible. With this aim, we propose a joint visual-semantic embedding that can automatically align illustrations and textual elements without paired supervision. This is achieved by transferring the knowledge learned on ordinary visual-semantic datasets to the artistic domain. Experiments, performed on two datasets specifically designed for this domain, validate the proposed strategies and quantify the domain shift between natural images and artworks. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						166	172		10.1016/j.patrec.2019.11.018													
J								Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model	PATTERN RECOGNITION LETTERS										Video captioning; Convolution; Long short-term memory; Fine-grained		As a fundamental problem in visual understanding, video captioning has attracted much attention from both computer vision and natural language processing communities. Despite recent emergence of video captioning methods, how to generate diverse and fine-grained video description is far from being solved. To this end, this work makes the following contributions. First, a novel high-quality video captioning system featuring hierarchical long short-term memory structure and dual-stage loss is designed to translate videos to sentences. Second, we incorporate the convolutional architecture into our captioning system with the aim of generating diverse and fine-grained description. Third, we propose a novel performance evaluation metric named LTMS to assess the fine-grained captions. The experimental results on the benchmark datasets MSVD and MSR-VTT indicate the effectiveness of the proposed model, achieving superior performance over state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						173	180		10.1016/j.patrec.2019.11.003													
J								Active deep neural network features selection for segmentation and recognition of brain tumors using MRI images	PATTERN RECOGNITION LETTERS										Brain tumor; Contrast improvement; Deep saliency method; Features extraction; Optimization; Recognition	CNN; FUSION	Glioma is a kind of brain tumor that can arise at a distinct location along with dissimilar appearance and size. The high-grade glioma (HGG) is a serious kind of cancer when compare to low-graded glioma (LGG). The manual diagnosis process of these tumors is tiring and time consuming. Therefore, in clinical practices, MRI is useful to assess gliomas as it provides essential information of tumor regions. In this manuscript, an active deep learning-based feature selection approach is suggested to segment and recognize brain tumors. Contrast enhancement is made in the primary step and supplied to SbDL for saliency map construction, which later converts into binarized form by applying simple thresholding. In the classification phase, the Inception V3 pre-trained CNN model is employed for deep feature extraction. These features are simply concatenated along with dominant rotated LBP (DRLBP) for better texture analysis. Later, the concatenated vector is optimized through particle swarm optimization (PSO), so as to classify using softmax classifier. The experiments are conducted in two phases. At first, the segmentation approach SbDL is validated on BRATS2017 and BRATS2018 datasets. The achieved dice score for the BRAST2017 dataset is 83.73% for core tumor, 93.7% for the whole tumor and 79.94% for enhanced tumor. For BRATS2018 dataset, dice score obtained is 88.34% (core), 91.2% (whole) and 81.84% (enhanced). At the second, the classification strategy is applied on BRATS2013, 2014, 2017 and 2018 with an average accuracy of more than 92%. The overall results show that the presented method outperforms for both segmentation and classification of brain tumors. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						181	189		10.1016/j.patrec.2019.11.019													
J								Compressing the CNN architecture for in-air handwritten Chinese character recognition	PATTERN RECOGNITION LETTERS										In-air handwriting; Handwritten Chinese character Recognition; Convolutional neural network; Network compression		Since the convolutional neural network (CNN) has brought great breakthroughs in the field of computer vision, it recently has been introduced to the in-air handwritten Chinese character recognition (IAHCCR) to achieve better recognition performance. However, the CNN is typically over-parameterized and contains lots of redundant filters or parameters. This leads the CNN to suffer from huge computation cost and considerable storage usage, limiting its deployments to resource-constrained devices like mobile phones and intelligent TVs. In this paper, we propose a unified algorithm to effectively compress the CNN for IAHCCR with little accuracy loss. Specifically, we first utilize the channel pruning strategy to simplify the network structure, and then adopt the network quantization technique to represent parameters with lower precision. We conduct experiments on the in-air handwriting dataset IAHCC-UCAS2016, where the baseline CNN achieves the state-of-the-art accuracy of 95.33% with 15.5 MB of storage. After the compression, we achieve 12.4 x storage saving and 1.7 x theoretical acceleration with only 0.17% accuracy loss. Moreover, evaluations on other benchmark datasets including ICDAR-2013 and MNIST further demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						190	197		10.1016/j.patrec.2019.11.028													
J								What is the minimum training data size to reliably identify writers in medieval manuscripts?	PATTERN RECOGNITION LETTERS										Palaeography; Mediaeval handwritings; Writer identification; Training strategies	IDENTIFICATION	One of the most important research topics in the field of palaeography is the identification of the different scribes who participated in the writing process of a medieval book. Using traditional palaeographic tools, a palaeographer spends a lot of time reading, measuring and comparing thousands of letters or graphic signs. The aim is to evaluate different characteristics, such as height or width of letters, distance between characters, angles of inclination, number and type of abbreviations etc., which allow a reliable identification of the scribes who contributed to the production of a given manuscript. Despite the growing scientific interest that has been observed in recent years in the use of computer techniques applied to palaeographic research, a general agreement has not yet been reached among researchers, either about the effectiveness of automatic analysis tools, or on the features that should be considered to perform such an analysis. However, in the context of a highly standardized school, the use of some basic page layout features can be very useful for automatically identifying the presence of different hands. In this context, the aim of our study is to verify whether it is possible to strongly reduce the amount of data a palaeographer must analyse manually, in an attempt to answer the following question: what is the minimum size of the training set that allows a classification system to identify the different scribal hands reliably? To this purpose, we have considered two well-known and highly efficient classification techniques, progressively varying the size of the training set and comparing the corresponding classification results. To improve the classification reliability, we have also introduced a multi-expert classification architecture, enabling an easy implementation of a reject option. The experimental results, performed on two large sets of digital images extracted from two entire 12th-century Bibles, show that using only a few pages of these bibles as a training set, it is possible to identify automatically the scribal hands in the remaining pages with great reliability. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						198	204		10.1016/j.patrec.2019.11.030													
J								Moving object detection under different weather conditions using full-spectrum light sources	PATTERN RECOGNITION LETTERS										Full-spectrum Light Sources; Weather Conditions Classification; Moving Object Detection; Thermal Infrared Camera	IMAGE; SEGMENTATION	The moving object detection always remains an active field of research given the variety of challenges related to this topic. In fact, most of the challenges related to the low illumination and weather conditions (fog, snow, rain, etc.) remain unresolved and require more developments. In this paper, our intrinsic objective is to overcome these challenges using an effective moving object detection method. Unlike most works in the literature that use one of the two infrared or visible spectra independently, we proposed a Moving Object Detection method based on background modeling using the Full-Spectrum Light Sources (FSLS-MOD). To better ensure the adaptability and independence of the moving object speeds and sizes, the principle of the inter-frame differences' methods is introduced in the background modeling stage. Furthermore, we applied a new strategy to switch between the spectra allowing us to benefit from the advantages of each spectrum and carry out a better moving object detection even in bad weather conditions. An experimental study by quantitative and qualitative evaluations proved the robustness and effectiveness of our proposed method of moving object detection using the switching strategy between full-spectrum light sources under different illuminations and weather conditions. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						205	212		10.1016/j.patrec.2019.11.004													
J								DELP-DAR system for license plate detection and recognition	PATTERN RECOGNITION LETTERS										LP detection; LP recognition; Deep learning; Mask R-CNN		Automatic License Plate detection and Recognition (ALPR) is a quite popular and active research topic in the field of computer vision, image processing and intelligent transport systems. ALPR is used to make detection and recognition processes more robust and efficient in highly complicated environments and backgrounds. Several research investigations are still necessary due to some constraints such as: completeness of numbering systems of countries, different colors, various languages, multiple sizes and varied fonts. For this, we present in this paper an automatic framework for License Plate (LP) detection and recognition from complex scenes. Our framework is based on mask region convolutional neural networks used for LP detection, segmentation and recognition. Although some studies have focused on LP detection, LP recognition, LP segmentation or just two of them, our study uses the maskr-cnn in the three stages. The evaluation of our framework is enhanced by four datasets for different countries and consequently with various languages. In fact, it tested on four datasets including images captured from multiple scenes under numerous conditions such as varied orientation, poor quality images, blurred images and complex environmental backgrounds. Extensive experiments show the robustness and efficiency of our suggested Extensive experiments show the robustness and efficiency of our suggested system that achieves in accuracy rate 99.3% on AOLP and 98.9% on Caltech dataset. Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						213	223		10.1016/j.patrec.2019.11.007													
J								In-classroom learning analytics based on student behavior, topic and teaching characteristic mining	PATTERN RECOGNITION LETTERS										Student behavior analysis; Topic modeling; Audio analysis; Sequential mining		The automatic analysis of students' in-classroom behavior is valuable to evaluate the effect of teaching. Recent studies of in-classroom video analysis mainly focus on lecture content, positions and identities of students. In this paper, we propose to analyze the students' concentration degree to the teacher or teaching content. Specifically, we detect students' faces, track faces, and analyze the students' behavior, i.e. raising or downing faces and corresponding head orientations to the teacher, teaching content or not. Besides, texts are obtained from the teacher's speech and the course topics taught in the class are extracted. Audio features of the teacher's speech are extracted and analyzed. Finally, the correlation of the students' concentration degree with the course topics, audio features are analyzed. This analysis can help teachers find the effective teaching characteristic to better improve students' concentration degree. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						224	231		10.1016/j.patrec.2019.11.023													
J								Segmentation strategies for the alpha-tree data structure	PATTERN RECOGNITION LETTERS										Segmentation; Clustering; Alpha-tree	CONSTRAINED CONNECTIVITY; OPERATORS; IMAGE; LATTICES	The alpha-tree is a versatile algorithm for color image segmentation employing attribute constraints to control the partitioning of the alpha-connected image space. Attribute constraints are enforced using a maximization strategy that returns the set of the largest connected components complying with these constraints assuming no prior violations from nested sub-components. This article presents two new strategies extending the way this set is defined. These are the non-target clustering and attribute maximization strategies, that give access to segments that could not be defined previously. Collectively they enable the handling of texture-rich regions that cannot be clustered into meaningful segments, and compute the unsupervised segmentation of images by seeking for extreme attribute values. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						232	239		10.1016/j.patrec.2019.11.027													
J								Data security based on homographic function	PATTERN RECOGNITION LETTERS										Data security; Substitution cipher; Homographic function; Chaos-based cryptography; Henon map; Lightweight encryption algorithm		In this paper, we adapt a homographic function to encrypt digital images, which takes into account the value of the pixel as well as its position in the image. The homographic function is a function represented as a quotient of two affine functions in a switching field K; it is represented by four different coefficients of zeros, which must verify certain conditions. One take the field K equal Z/257Z switching field (257 is a prime number), (all its non-zero elements are invertible, we take into account the following condition if we have 0 as the pixel value we put 256), 256 is the inverse of 256 in Z/257Z. We will show that this a homophonic substitution, which can represent the confusion part in a modern cryptographic system. We strengthened this proposal by a diffusion method and, we have used the Henon attractor of is 2-dimensional; each pixel will be permuted to another position of the encrypted image. We managed to build a cryptosystem who passed all the statistic tests, and that has a secret key of 253 bits in length. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						240	246		10.1016/j.patrec.2019.10.032													
J								Deeply learned pore-scale facial features with a large pore-to-pore correspondences dataset	PATTERN RECOGNITION LETTERS										Pore-scale facial features; Pore-scale patch dataset; Deep learning	FACE RECOGNITION	Similar to fingerprints and irises, pore-scale facial features can be used to distinguish human identities effectively. However, without pore-to-pore correspondences dataset, there are no deep learning based methods for pore-scale facial features. Actually, it is hard to establish a large pore-to-pore correspondences dataset due to the existing high-resolution face databases are uncalibrated and nonsynchronous. In this paper, we employ a constraint based on 3D facial model and construct a large pore-to-pore correspondences dataset. This dataset is then used to train a Convolutional Neural Network (CNN) to generate the novel pore-scale facial features - Deeply Learned Pore-scale Facial Features (DLPFF). The experiments show that our learning based method achieves the state-of-the-art matching performance on the Bosphorus facial database and has good generalization. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				JAN	2020	129						247	254		10.1016/j.patrec.2019.10.021													
J								Component trees for image sequences and streams	PATTERN RECOGNITION LETTERS										Hierarchical representation; Max-tree; Satellite image time series	ATTRIBUTE FILTERS; COMPUTATION; OPERATORS	Morphological hierarchies now form a well-established framework for (still) image modeling and processing. However, their extension to time-related data remains largely unexplored. In this paper, we address such a topic and show how to analyze image sequences with tree-based representations. To do so, we distinguish between three kinds of models, namely spatial, temporal and spatial-temporal hierarchies. For each of them, we review different strategies to build the hierarchy from an image sequence. We also propose some algorithms to update such trees when new images are appended to the series and we compared the time complexity with tree building from scratch. We illustrate our findings with the max and min-tree structures built on grayscale data provided by Satellite Image Time Series that are gathering a growing interest in Earth Observation. Besides, we provide a comparative study for different hierarchies with classification experiments. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						255	262		10.1016/j.patrec.2019.11.038													
J								Securing biometric user template using modified minutiae attributes	PATTERN RECOGNITION LETTERS										Security; Privacy; Fingerprint; Authentication; Revocability; Biometrics	FINGERPRINT SHELL; CODE; PRIVACY	The minutiae points information of a fingerprint is generally saved directly in the database as a template for the user. It has been deduced through numerous research works that the original fingerprint of a user can be obtained from the minutiae points information. As the databases are prone to various attacks, their security becomes a huge concern in fingerprint based authentication systems. Hereby, a novel technique has been introduced which is based on the modification of the minutiae attributes. The user template generated through the proposed technique is extremely secure and robust. The proposed technique achieved 1.63%, 1%, and 2.43% EER under stolen-key attack scenario for FVC2002 DPI, FVC2002 DB2, and FVC2002 DB3 fingerprint databases respectively. The proposed technique achieved 0% EER under different-key scenario. Highly encouraging results are obtained that show the viability and effectiveness of the proposed technique. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						263	270		10.1016/j.patrec.2019.11.037													
J								Deep-learning framework to detect lung abnormality - A study with chest X-Ray and lung CT scan images	PATTERN RECOGNITION LETTERS											NODULES; CLASSIFICATION; SEGMENTATION	Lung abnormalities are highly risky conditions in humans. The early diagnosis of lung abnormalities is essential to reduce the risk by enabling quick and efficient treatment. This research work aims to propose a Deep-Learning (DL) framework to examine lung pneumonia and cancer. This work proposes two different DL techniques to assess the considered problem: (i) The initial DL method, named a modified AlexNet (MAN), is proposed to classify chest X-Ray images into normal and pneumonia class. In the MAN, the classification is implemented using with Support Vector Machine (SVM), and its performance is compared against Softmax. Further, its performance is validated with other pre-trained DL techniques, such as AlexNet, VGG16, VGG19 and ResNet50. (ii) The second DL work implements a fusion of handcrafted and learned features in the MAN to improve classification accuracy during lung cancer assessment. This work employs serial fusion and Principal Component Analysis (PCA) based features selection to enhance the feature vector. The performance of this DL frame work is tested using benchmark lung cancer CT images of LIDC-IDRI and classification accuracy (97.27%) is attained. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						271	278		10.1016/j.patrec.2019.11.013													
J								Scale-invariant batch-adaptive residual learning for person re-identification	PATTERN RECOGNITION LETTERS										Person re-identification; Scale invariant residual network; Batch adaptive triplet loss; Deep metric learning		The problem of person re-identification (re-ID) deals with matching two similar persons in probe and gallery sets. The underlying pattern matching task can become more complex as similar persons can appear in different scales in the two sets. In this paper, we address this challenging problem of scale-invariant person re-ID. As a solution, we propose two scale-invariant residual networks with a new loss function for deep metric learning. The first network, termed as Scale Invariant Triplet Network (SITriNet), is deeper and is trained from the pre-trained weights. In contrast, the second network, named Scale-Invariant Siamese Resnet-32 (SISR-32), is shallower and uses training from the scratch. Deep metric learning for both the networks are realized through a batch adaptive triplet loss function. Extensive comparisons and ablation studies on the benchmark Market-1501 and CUHK03 datasets clearly demonstrate the effectiveness of the proposed formulation. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						279	286		10.1016/j.patrec.2019.11.032													
J								A time-series prediction framework using sequential learning algorithms and dimensionality reduction within a sparsification approach	PATTERN RECOGNITION LETTERS												The adaptive kernel filters are sequential learning algorithms that operate in a particular functional space called a reproducing kernel Hilbert space. However, their performance depends on the selection of two hyper-parameters, i.e., kernel bandwidth and learning-rate. Besides, as these algorithms train the model using a sequence of input vectors, their computation scales with the number of samples. In this work, we propose to address the previous challenges of these sequential learning algorithms. The proposed framework, unlike similar methods, maximizes the correntropy function to optimize the bandwidth and learning-rate parameters. Further, we introduce a sparsification strategy based on dimensionality reduction to remove redundant samples. The framework is tested on both synthetic and real-world data sets, showing convergence to relatively low values of mean-square-error. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						287	292		10.1016/j.patrec.2019.11.031													
J								Developed Newton-Raphson based deep features selection framework for skin lesion recognition	PATTERN RECOGNITION LETTERS										Skin cancer; Contrast stretching; Lesion localization; Deep features; Best features	CLASSIFICATION; SEGMENTATION; STRATEGY	Melanoma is the fatal form of skin cancer; however, its diagnosis at the primary stages significantly reduces the mortality rate. These days, the increasing numbers of skin cancer patients have boosted the requirement for a care decision support system - capable of detecting the lesions with high accuracy. In this work, a method is proposed for skin cancer localization and recognition by implementing a novel combination of a deep learning model and iteration-controlled Newton-Raphson (IcNR) based feature selection method. The proposed framework follows three primary steps - lesion localization through faster region based convolutional neural network (RCNN), deep feature extraction, and feature selection by IcNR approach. In the localization step, a new contrast stretching approach based on bee colony method (ABC) is being followed. The enhanced images along with their ground truths are later plugged into Fast-RCNN to get segmented images. A pre-trained model, DenseNet201, is utilized to extract deep features via transfer learning, which are later subjected to selection step using proposed IcNR approach. The selected most discriminant features are finally utilized for classification using multilayered feed forward neural networks. Tests are performed on ISBI2016 and ISBI2017 datasets to achieving an accuracy of 94.5% and 93.4%, respectively. Simulation results reveal that the proposed technique outperforms existing methods with greater accuracy, and time. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						293	303		10.1016/j.patrec.2019.11.034													
J								A torus model for optical flow	PATTERN RECOGNITION LETTERS										Optical flow; Computational topology; Persistent homology; Fiber bundle; Zigzag persistence	PERSISTENT HOMOLOGY ANALYSIS; TOPOLOGICAL PERSISTENCE; NONLINEAR STATISTICS	We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video Sintel, collected by Butler et al. in A naturalistic open source movie for optical flow evaluation. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast 3 x 3 patches from this video are well-modeled by a torus, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						304	310		10.1016/j.patrec.2019.11.029													
J								Wasserstein GAN based on Autoencoder with back-translation for cross-lingual embedding mappings	PATTERN RECOGNITION LETTERS										Cross-lingual word embeddings; Bilingual lexicon induction; Wasserstein GAN; Back-translation		Recent works about learning cross-lingual word mappings (CWMs) focus on relaxing the requirement of bilingual signals through generative adversarial networks (GANs). GANs based models intend to enforce source embedding space to align target embedding space. However, existing GANs based models cannot exploit the underlying information of target-side for an alignment standard in the training, which may lead to some suboptimal results of CWMs. To address this problem, we propose a novel method, named Wasserstein GAN based on autoencoder with back-translation (ABWGAN) that can effectively exploit the target-side information and improve the performance of GANs based models. ABWGAN is an innovative combination of preliminary mappings learning and back-translation with target-side (BT-TS). In the proposed BT-TS, we back-translate target-side embeddings with preliminary CWMs to learn the final cross-lingual mappings, which enables to improve the quality of the preliminary mappings by reusing the target-side samples. Experimental results on three language pairs demonstrate the effectiveness of the proposed ABWGAN. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						311	316		10.1016/j.patrec.2019.11.033													
J								Enhancing sentient embodied conversational agents with machine learning	PATTERN RECOGNITION LETTERS										Embodied conversational agents; Machine learning; Virtual tutors		Within the area of intelligent User Interfaces, we propose what we call Sentient Embodied Conversational Agents (SECAs): virtual characters able to engage users in complex conversations and to incorporate sentient capabilities similar to the ones humans have. This paper introduces SECAs together with their architecture and a publicly available software library that facilitates their inclusion in applications -such as educational and elder-care- requiring proactive and sensitive agent behaviours. In fact, we illustrate our proposal with a virtual tutor embedded in an educational application for children. The evaluation was performed in two stages: firstly, we tested a version with basic textual processing capabilities; and secondly, we evaluated a SECA with Machine-Learning-enhanced user understanding capabilities. The results show a significant improvement in users' perception of the agent's understanding capability. Indeed, the Response Error Rate decreased from 22.31% to 11.46% using ML techniques. Moreover, 99.33% of the participants consider the global experience of talking with the virtual tutor with sentient capabilities to be satisfactory. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JAN	2020	129						317	323		10.1016/j.patrec.2019.11.035													
J								Artificial plant optimization algorithm to detect infected leaves using machine learning	EXPERT SYSTEMS										convolutional neural network; histogram of oriented gradients; improved artificial plant optimization; machine learning	SOLVE TOY MODEL	Plant leaves play an important role in the diagnosis of plant diseases. Losses from such diseases can have a significant economic as well as environmental impact. Thus, examination of leaves into a healthy or infected carries substantial importance. An improved artificial plant optimization (IAPO) algorithm using machine learning has been introduced that identifies the plant diseases and categorize the leaves into healthy and infected on a private dataset of 236 images. Features are extracted from the images using histogram of oriented gradients (descriptor). The concepts of artificial plant optimization are then applied to study the features of healthy leaves using IAPO. A machine learning algorithm has been created to make the model adaptive with varied datasets. The degree of infection is eventually computed, and the leaves with infection greater than a certain calculated threshold are classified as infected leaves. The results show that IAPO can be used for classification of infected and healthy leaves and this algorithm can be generalized to solve problems in other domains as well. The proposed IAPO is also compared with other classification algorithms including k-nearest neighbours, support vector machine, random forest and convolution neural network that show accuracies of 78.24%, 83.48%, 87.83%, and 91.26%, respectively, whereas IAPO shows quite accurate results in classification of leaves with an accuracy of 97.45% on training set and 95.0% accuracy on test set.																	0266-4720	1468-0394														e12501	10.1111/exsy.12501		JAN 2020											
J								New Image Watermarking Algorithm Based on DWT and Pixel Movement Function PMF	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Wavelet transforms; image watermarking; image quality evaluation	SCHEME	In this paper, we propose a new algorithm of image watermarking based on Discrete Wavelet Transform (DWT) including a function for pixels movement. The proposed algorithm uses DWT of two levels in order to compact a higher energy in component LL1, and Contrast Sensitivity Function (CSF) to improve the invisibility and robustness, the new Function of Pixel Movement (PMF) is applied to increase the security properties. Pixel Movement Function (PMF) is a function of N iteration inside each block, this function required a changeable key K calculated in each iteration N for the position of each block. Numerical experiments are performed to demonstrate that the proposed method can improve watermarking quality in terms of imperceptibility of watermark, capacity of insertion and robustness against different attacks such as Joint Photographic Experts Group (JPEG) compression, noise addition and geometrical attacks.																	1683-3198					JAN	2020	17	1					1	7		10.34028/iajit/17/1/1													
J								A Novel Evidence Distance in Power Set Space	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Evidence theory; evidence distance; data function; target recognition system	FUZZY; SIMILARITY; CLASSIFIER; NETWORKS	Distance measure of evidence presented has been used to measure the similarity of two bodies of evidence. However, it is not considered that the probability distribution on a power set is able to assign to its subsets not only single elements. In this paper a novel approach is proposed to measure the distance of evidence. And some properties that the novel approach has, such as nonnegativity, symmetry, triangular inequality, downward compatibility and higher sensitivity, is proved. Numerical example and real application are used to strictly illustrate the effciency of the new distance.																	1683-3198					JAN	2020	17	1					8	15		10.34028/iajit/17/1/2													
J								A Neuro-Fuzzy System to Detect IPv6 Router Alert Option DoS Packets	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										DoS attacks; IPv6 router alert option; Neuro-Fuzzy; IPv6 network security	NETWORK; ANFIS	Detecting the denial of service attacks that solely target the router is a maximum security imperative in deploying IPv6 networks. The state-of-the-art Denial of Service detection methods aim at leveraging the advantages of flow statistical features and machine learning techniques. However, the detection performance is highly affected by the quality of the feature selector and the reliability of datasets of IPv6 flow information. This paper proposes a new neuro-fuzzy inference system to tackle the problem of classifying the packets in IPv6 networks in crucial situation of small-supervised training dataset. The proposed system is capable of classifying the IPv6 router alert option packets into denial of service and normal by utilizing the neuro-fuzzy strengths to boost the classification accuracy. A mathematical analysis from the fuzzy sets theory perspective is provided to express performance benefit of the proposed system. An empirical performance test is conducted on comprehensive dataset of IPv6 packets produced in a supervised environment. The result shows that the proposed system overcomes robustly some state-of-the-art systems.																	1683-3198					JAN	2020	17	1					16	25		10.34028/iajit/17/1/3													
J								Machine Learning Based Prediction of Complex Bugs in Source Code	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Software bugs; software mefrics; machine learning; fault prediction model	SOFTWARE; SEVERITY; METRICS	During software development and maintenance phases, the fixing of severe bugs are mostly very challenging and needs more efforts to fix them on a priority basis. Several research works have been performed using software metrics and predict fault-prone software module. In this paper, we propose an approach to categorize different types of bugs according to their severity and priority basis and then use them to label software mefrics' data. Finally, we used labeled data to train the supervised machine learning models for the prediction of fault prone software modules. Moreover, to build an effective prediction model, we used genetic algorithm to search those sets of mefrics which are highly correlated with severe bugs.																	1683-3198					JAN	2020	17	1					26	37		10.34028/iajit/17/1/4													
J								Designing Punjabi Poetry Classifiers Using Machine Learning and Different Textual Features	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Classification; naive bayes; hyper pipes; k-nearest neighbour; punjabi; poetry; support vector machine; word net		Analysis of poetic text is very challenging from computational linguistic perspective. Computational analysis of literary arts, especially poetry, is very difficult task for classification. For library recommendation system, poetries can be classified on various metrics such as poet, time period, sentiments and subject matter. In this work, content-based Punjabi poetry classifier was developed using Weka toolset. Four different categories were manually populated with 2034 poems Nature and Festival (NAFE), Linguistic and Patriotic (LIPA), Relation and Romantic (RORE), Philosophy and Spiritual (PHSP) categories consists of 505, 399, 529 and 601 numbers of poetries, respectively. These poetries were passed to various pre-processing sub phases such as tokenization, noise removal, stop word removal, and special symbol removal. 31938 extracted tokens were weighted using Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme. Based upon poetry elements, three different textual features (lexical, syntactic and semantic) were experimented to develop classifier using different machine learning algorithms. Naive Bayes (NB), Support Vector Machine, Hyper pipes and K-nearest neighbour algorithms were experimented with textual features. The results revealed that semantic feature performed better as compared to lexical and syntactic. The best performing algorithm is SVM and highest accuracy (76.02%) is achieved by incorporating semantic information associated with words.																	1683-3198					JAN	2020	17	1					38	44		10.34028/iajit/17/1/5													
J								Extracting Word Synonyms from Text using Neural Approaches	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Neural networks; semantic similarity; word representations; natural language processing		Extracting synonyms from textual corpora using computational techniques is an interesting research problem in the Natural Language Processing (NLP) domain. Neural techniques (such as Word2Vec) have been recently utilized to produce distributional word representations (also known as word embeddings) that capture semantic similarity/relatedness between words based on linear context. Nevertheless, using these techniques for synonyms extraction poses many challenges due to the fact that similarity between vector word representations does not indicate only synonymy between words, but also other sense relations as well as word association or relatedness. In this paper, we tackle this problem using a novel 2-step approach. We first build distributional word embeddings using Word2Vec then use the induced word embeddings as an input to train a feed-forward neutral network using annotated dataset to distinguish between synonyms and other semantically related words.																	1683-3198					JAN	2020	17	1					45	51		10.34028/iajit/17/1/6													
J								Privacy-Preserving for Distributed Data Streams: Towards l-Diversity	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										k-anonymity; l-diversity; data streams and clustering		Privacy-preserving data publishing have been studied widely on static data. However, many recent applications generate data streams that are real-time, unbounded, rapidly changing, and distributed in nature. Recently, few work addressed k-anonymity and l-diversity for data streams. Their model implied that if the stream is distributed, it is collected at a central site for anonymization. In this paper, we propose a novel distributed model where distributed streams are first anonymized by distributed (collecting) sites before merging and releasing. Our approach extends Continuously Anonymizing STreaming data via adaptive cLustEring (CASTLE) [4], a cluster-based approach that provides both k-anonymity and l-diversity for centralized data streams. The main idea is for each site to construct its local clustering model and exchange this local view with other sites to globally construct approximately the same clustering view. The approach is heuristic in a sense that not every update to the local view is sent, instead triggering events are selected for exchanging cluster information. Extensive experiments on a real data set are performed to study the introduced Information Loss (IL) on different settings. First, the impact of the different parameters on IL are quantified. Then k-anonymity and l-diversity are compared in terms of messaging cost and IL. Finally, the effectiveness of the proposed distributed model is studied by comparing the introduced IL to the IL of the centralized model (as a lower bound) and to a distributed model with no communication (as an upper bound). The experimental results show that the main contributing factor to IL is the number of attributes in the quasi-identifier (50%-75%) and the number of sites contributed about 1% and this proves the scalability of the proposed approach. In addition, providing l-diversity is shown to introduce about 25% increase in IL when compared to k-anonymity. Moreover, 35% reduction in IL is achieved by messaging cost (in bytes) of about 0.3% of the data set size.																	1683-3198					JAN	2020	17	1					52	64		10.34028/iajit/17/1/7													
J								Generating Sequence Diagrams from Arabic User Requirements using MADA plus TOKAN Tool	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										UML; automated software engineering; sequence diagram; arabic user requirements		A new semi-automated approach for generating sequence diagrams from Arabic user requirements is presented. In this novel approach, the Arabic user requirements are parsed using a natural language processing tool called MADA+TOKAN to generate the Part Of Speech (POS) tags of the parsed user requirements, then a set of heuristics are applied on the resulted tags to obtain the sequence diagram components; objects, messages and work flow transitions (messages). The generated sequence diagram is expressed using Extensible Markup Language (XMI) to be drawn using sequence diagrams drawing tools. Our approach achieves better results than students in generating sequence diagrams. It also has better accuracy in generating the participants and less accuracy in generating messages exchanged between participants. The proposed approach is validated using a set of experiments involving a set of real cases evaluated by a group of software engineers and a group of graduate students who are familiar with sequence diagrams.																	1683-3198					JAN	2020	17	1					65	72		10.34028/iajit/17/1/8													
J								An Improved Grey Wolf Optimization Algorithm Based Task Scheduling in Cloud Computing Environment	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Virtualization; cloud computing; gwo; task scheduling; optimization; resource; cloudsim and Qos	GENETIC ALGORITHM; SEARCH	The demand for massive computing power and storage space has been escalating in various fields and in order to satisfy this need a new technology known as cloud computing is introduced. The capability of providing these services effectively and economically has made cloud computing technology more popular. With the advent of virtualization, IT services being offered have started to shift to cloud computing. Virtualization had paved way for resource availability in an inexhaustible manner. As Cloud Computing is still at its unrefined form and to derive its full potential more analysis is needed. The way in which resources and tasks get allocated in cloud environment requires more analysis. This in turn accounts for the Quality of Services (QoS) of the services offered by cloud service providers. This paper proposes to simulate the Performance-Cost Grey Wolf Optimization (PCGWO) algorithm based to achieve optimization in the process of allocation of resources and tasks in cloud computing domain using CloudSim toolkit. The main purpose is to lower both the processing time and cost in accordance to objective function. The superiority of proposed technique is evident from the simulation results that show a comprehensive reduction in task completion time and cost. Also using this technique more no. of tasks can be efficiently completed within the deadline. Thus the results indicate that in accordance to performance the PCGWO method fares better than existing algorithms.																	1683-3198					JAN	2020	17	1					73	81		10.34028/iajit/17/1/9													
J								Training Convolutional Neural Network for Sketch Recognition on Large-Scale Dataset	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Sketch recognition; vgg16 convolutional neural network; contextual features; strokes traverse; joint bayesian		With the rapid development of computer vision technology, increasingly more focus has been put on image recognition. More specifically, a sketch is an important hand-drawn image that is garnering increased attention. Moreover, as handheld devices such as tablets, smartphones, etc. have become more popular, it has become increasingly more convenient for people to hand-draw sketches using this equipment. Hence, sketch recognition is a necessary task to improve the performance of intelligent equipment. In this paper, a sketch recognition learning approach is proposed that is based on the Visual Geometry Group16 Convolutional Neural Network (VGG16 CNN). In particular, in order to diminish the effect of the number of sketches on the learning method, we adopt a strategy of increasing the quantity to improve the diversity and scale of sketches. Initially, sketch features are extracted via the prefrained VGG16 CNN. Additionally, we obtain contextual features based on the traverse stroke scheme. Then, the VGG16 CNN is trained using a joint Bayesian method to update the related network parameters. Moreover, this network has been applied to predict the labels of input sketches in order to automatically recognize the label of a sketch. Last but not least, related experiments are conducted, and the comparison of our method with the state-of-the-art methods is performed, which shows that our approach is superior and feasible.																	1683-3198					JAN	2020	17	1					82	89		10.34028/iajit/17/1/10													
J								A Novel Amended Dynamic Round Robin Scheduling Algorithm for Timeshared Systems	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										CPU; scheduling algorithm; round rabin scheduling FCFS; ADRR		Central Processing Unit (CPU) is the most significant resource and its scheduling is one of the main functions of an operating system. In timeshared systems, Round Robin (RR) is most widely used scheduling algorithm. The efficiency of RR algorithm is influenced by the quantum time, if quantum is small, there will be overheads of more context switches and if quantum time is large, then given algorithm will perform as First Come First Served (FCFS) in which there is more risk of starvation. In this paper, a new CPU scheduling algorithm is proposed named as Amended Dynamic Round Robin (ADRR) based on CPU burst time. The primary goal of ADRR is to improve the conventional RR scheduling algorithm using the active quantum time notion. Quantum time is cyclically adjusted based on CPU burst time. We evaluate and compare the performance of our proposed ADRR algorithm based on certain parameters such as, waiting time, turnaround time etc. and compare the performance of our proposed algorithm. Our numerical analysis and simulation results in MATLAB reveals that ADRR outperforms other well-known algorithms such as conventional Round Robin, Improved Round Robin (IRR), Optimum Multilevel Dynamic Round Robin (OMDRR) and Priority Based Round Robin (PRR).																	1683-3198					JAN	2020	17	1					90	98		10.34028/iajit/17/1/11													
J								Modeling and Verification of ARINC 653 Hierarchical Preemptive Scheduling	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										ARINC653; Schedulability analysis; Model checking; UPPAAL	IMPLEMENTATION	Avionics Application Standard Software Interface (ARINC 653) is a software specification for space and time partitioning in safety-critical avionics real-time operating systems. Correctly designed task schedulers are crucial for ARINC 653 running systems. This paper proposes a model-checking-based method for analyzing and verifying ARINC 653 scheduling model. Based on priced timed automata theory, an ARINC 653 scheduling system was modeled as a priced timed automata network. The schedulability of the system was described as a set of temporal logic expressions, and was analyzed and verified by a model checker. Our research shows that it is feasible to use model checking to analyze task schedulability in an ARINC 653 hierarchical scheduling system. The method discussed modeled preemptive scheduling by using the stop/watch features of priced timed automata. Unlike traditional scheduling analysis techniques, the proposed approach uses an exhaustive method to automate analysis of the schedulability of a system, resulting in a more precise analysis.																	1683-3198					JAN	2020	17	1					99	106		10.34028/iajit/17/1/12													
J								Large Universe Ciphertext-Policy Attribute-Based Encryption with Attribute Level User Revocation in Cloud Storage	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Ciphertext-policy attribute-based encryption; outsourced decryption; large universe; attribute level user revocation		Ciphertext-Policy Attribute-Based Encryption (CP-ABE), especially large universe CP-ABE that is not bounded with the attribute set, is getting more and more extensive application in the cloud storage. However, there exists an important challenge in original large universe CP-ABE, namely dynamic user and attribute revocation. In this paper, we propose a large universe CP-ABE with efficient attribute level user revocation, namely the revocation to an attribute of some user cannot influence the common access of other legitimate attributes. To achieve the revocation, we divide the master key into two parts: delegation key and secret key, which are sent to the cloud provider and user separately. Note that, our scheme is proved selectively secure in the standard model under "q-type" assumption. Finally, the performance analysis and experimental verification have been carried out in this paper, and the experimental results show that, compared with the existing revocation schemes, although our scheme increases the computational load of storage Service Provider (CSP) in order to achieve the attribute revocation, it does not need the participation of Attribute Authority (AA), which reduces the computational load of AA. Moreover, the user does not need any additional parameters to achieve the attribute revocation except of the private key, thus saving the storage space greatly.																	1683-3198					JAN	2020	17	1					107	117		10.34028/iajit/17/1/13													
J								Face Identification based Bio-Inspired Algorithms	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Support vector machines; projection pursuit; particle swarm optimization; genetic algorithm; kurtosis index; friedman index	EXPLORATORY PROJECTION PURSUIT; SUPPORT VECTOR MACHINES; CLASSIFICATION; DIMENSION	Most biometric identification applications suffer from the curse of dimensionality as the database size becomes very large, which could negatively affect both the identification performance and speed. In this paper, we use Projection Pursuit (PP) methods to determine clusters of individuals. Support Vector Machine (SVM) classifiers are then applied on each cluster of users separately. PP clustering is conducted using Friedman and Kurtosis projection indices optimized by Genetic Algorithm and Particle Swarm Optimization methods. Experimental results obtained using YALE face database showed improvement in the performance and speed of face identification system.																	1683-3198					JAN	2020	17	1					118	127		10.34028/iajit/17/1/14													
J								Improved Steganography Scheme based on Fractal Set	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Steganography; data hiding; security; julia set; mandelbrot set; fractal set		Steganography is the art of hiding secret data inside digital multimedia such as image, audio, text and video. It plays a significant role in current trends for providing secure communication and guarantees accessibility of secret information by authorised parties only. The Least-Significant Bit (LSB) approach is one of the important schemes in steganography. The majority of LSB-based schemes suffer from several problems due to distortion in a limited payload capacity for stego-image. In this paper, we have presented an alternative steganographic scheme that does not rely on cover images as in existing schemes. Instead, the image which includes the secure hidden data is generated as an image of a curve. This curve is resulted from a series of computation that is carried out over the mathematical chaotic fractal sets. The new scheme aims at improving the data concealing capacity, since it achieves limitless concealing capacity and disposes of the likelihood of the attackers to realise any secret information from the resulted stego-image. From the security side, the proposed scheme enhances the level of security as the scheme depends on the exact matching between secret information and the generated fractal (Mandelbrot-Julia) values. Accordingly, a key stream is created based on these matches. The proposed scheme is evaluated and tested successfully from different perspectives.																	1683-3198					JAN	2020	17	1					128	136		10.34028/iajit/17/1/15													
J								A Combined Method of Skin-and Depth-based Hand Gesture Recognition	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Gesture recognition; microsoft kinect; inception model; depth	COLOR DETECTION	Kinect is a promising acquisition device that provides useful information on a scene through color and depth data. There has been a keen interest in utilizing Kinect in many computer vision areas such as gesture recognition. Given the advantages that Kinect provides, hand gesture recognition can be deployed efficiently with minor drawbacks. This paper proposes a simple and yet efficient way of hand gesture recognition via segmenting a hand region from both color and depth data acquired by Kinect v1. The Inception model of the image recognition system is used to check the reliability of the proposed method. Experimental results are derived from a sample dataset of Microsoft Kinect hand acquisitions. Under the appropriate conditions, it is possible to achieve high accuracy in close to real time.																	1683-3198					JAN	2020	17	1					137	145		10.34028/iajit/17/1/16													
J								Discrete Spider Monkey Optimization for Travelling Salesman Problem	APPLIED SOFT COMPUTING										Travelling Salesman Problem; Swap Sequence; Swap Operator; Partial Search; Spider Monkey Optimization	GRAVITATIONAL SEARCH ALGORITHM; BEE COLONY ALGORITHM; GENETIC ALGORITHMS; SWARM ALGORITHM; INTELLIGENCE; PRIORITIZATION; EVOLUTIONARY; METHODOLOGY; MODEL	Meta-heuristic algorithms inspired by biological species have become very popular in recent years. Collective intelligence of various social insects such as ants, bees, wasps, termites, birds, fish, has been investigated to develop a number of meta-heuristic algorithms in the general domain of swarm intelligence (SI). The developed SI algorithms are found effective in solving different optimization tasks. Travelling Salesman Problem (TSP) is the combinatorial optimization problem where a salesman starting from a home city travels all the other cities and returns to home city in the shortest possible path. TSP is a popular problem due to the fact that the instances of TSP can be applied to solve real-world problems, implication of which turns TSP into a standard test bench for performance evaluation of new algorithms. Spider Monkey Optimization (SMO) is a recent addition to SI algorithms based on the social behaviour of spider monkeys. SMO implicitly adopts grouping and regrouping for the interactions to improve solutions; such multi-population approach is the motivation of this study to develop an effective method for TSP. This paper presents an effective variant of SMO to solve TSP called discrete SMO (DSMO). In DSMO, every spider monkey represents a TSP solution where Swap Sequence (SS) and Swap Operator (SO) based operations are employed, which enables interaction among monkeys in obtaining the optimal TSP solution. The SOs are generated using the experience of a specific spider monkey as well as the experience of other members (local leader, global leader, or a randomly selected spider monkey) of the group. The performance and effectiveness of the proposed method have been verified on a large set of TSP instances and the outcomes are compared to other well-known methods. Experimental results demonstrate the effectiveness of the proposed DSMO for solving TSP. (C) 2019 Published by Elsevier B.V.																	1568-4946	1872-9681				JAN	2020	86								105887	10.1016/j.asoc.2019.105887													
J								New feature selection methods based on opposition-based learning and self-adaptive cohort intelligence for predicting patient no-shows	APPLIED SOFT COMPUTING										Artificial intelligence; Machine learning; Feature selection; Opposition-based learning; Metaheuristic; Cohort intelligence; Health care; Classification	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; BROKEN APPOINTMENTS; SEARCH ALGORITHM; TIME; CLASSIFICATION; RANDOMNESS; WRAPPER	Patient no-shows have significant adverse effects on healthcare systems. Therefore, predicting patients' no-shows is necessary to use their appointment slots effectively. In the literature, filter feature selection methods have been prominently used for patient no-show prediction. However, filter methods are less effective than wrapper methods. This paper presents new wrapper methods based on three variants of the proposed algorithm, Opposition-based Self-Adaptive Cohort Intelligence (OSACI). The three variants of OSACI are referred to in this paper as OSACI-Init, OSACI-Update, and OSACI-Init_Update, which are formed by the integration of Self-Adaptive Cohort Intelligence (SACI) with three Opposition-based Learning (OBL) strategies; namely: OBL initialization, OBL update, and OBL initialization and update, respectively. The performance of the proposed algorithms was examined and compared with that of Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Differential Evolution (DE), and SACI in terms of AUC, sensitivity, specificity, dimensionality reduction, and convergence speed. Patient no-show data of a primary care clinic in upstate New York was used in the numerical experiments. The results showed that the proposed algorithms outperformed the other compared algorithms by achieving higher dimensionality reduction and better convergence speed while achieving comparable AUC, sensitivity, and specificity scores. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105866	10.1016/j.asoc.2019.105866													
J								A Bolasso based consistent feature selection enabled random forest classification algorithm: An application to credit risk assessment	APPLIED SOFT COMPUTING										Bootstrap-lasso; Stable feature selection; BS-RF; Credit risk assessment; Random forest in credit risk	STABLE FEATURE-SELECTION; GENETIC ALGORITHM; PROBABILITY; CLASSIFIERS; DEFAULT	Credit risk assessment has been a crucial issue as it forecasts whether an individual will default on loan or not. Classifying an applicant as good or bad debtor helps lender to make a wise decision. The modern data mining and machine learning techniques have been found to be very useful and accurate in credit risk predictive capability and correct decision making. Classification is one of the most widely used techniques in machine learning. To increase prediction accuracy of standalone classifiers while keeping overall cost to a minimum, feature selection techniques have been utilized, as feature selection removes redundant and irrelevant attributes from dataset. This paper initially introduces Bolasso (Bootstrap-Lasso) which selects consistent and relevant features from pool of features. The consistent feature selection is defined as robustness of selected features with respect to changes in dataset Bolasso generated shortlisted features are then applied to various classification algorithms like Random Forest (RF), Support Vector Machine (SVM), Naive Bayes (NB) and K-Nearest Neighbors (K-NN) to test its predictive accuracy. It is observed that Bolasso enabled Random Forest algorithm (BS-RF) provides best results forcredit risk evaluation. The classifiers are built on training and test data partition (70:30) of three datasets (Lending Club's peer to peer dataset, Kaggle's Bank loan status dataset and German credit dataset obtained from UCI). The performance of Bolasso enabled various classification algorithms is then compared with that of other baseline feature selection methods like Chi Square, Gain Ratio, ReliefF and stand-alone classifiers (no feature selection method applied). The experimental results shows that Bolasso provides phenomenal stability of features when compared with stability of other algorithms. Jaccard Stability Measure (JSM) is used to assess stability of feature selection methods. Moreover BS-RF have good classification accuracy and is better than other methods in terms of AUC and Accuracy resulting in effectively improving the decision making process of lenders. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105936	10.1016/j.asoc.2019.105936													
J								Ordinal Priority Approach (OPA) in Multiple Attribute Decision-Making	APPLIED SOFT COMPUTING										Multiple Attribute Decision-Making; Group Decision-Making; Ordinal Priority Approach	ANALYTIC NETWORK PROCESS; MCDM METHODS; SELECTION; AGGREGATION; NORMALIZATION; JUDGMENTS; RANKING; WEIGHT; TOPSIS; MADM	The current study aims to present a new method called Ordinal Priority Approach (OPA) in Multiple Attribute Decision-Making (MADM). This method can be used in individual or group decision-making (GDM). In the case of GDM, through this method, we first determine the experts and their priorities. The priority of experts may be determined based on their experience and/or knowledge. After prioritization of the experts, the attributes are prioritized by each expert. Meanwhile, each expert ranks the alternatives based on each attribute, and the sub-attributes if any. Ultimately, by solving the presented linear programming model of this method, the weights of the attributes, alternatives, experts, and sub-attributes would be obtained simultaneously. A significant advantage of the proposed method is that it does not make use of pairwise comparison matrix, decision-making matrix (no need for numerical input), normalization methods, averaging methods for aggregating the opinions of experts (in GDM) and linguistic variables. Another advantage of this method is the possibility for experts to only comment on the attributes and alternatives for which they have sufficient knowledge and experience. The validity of the proposed model has been evaluated using several group and individual instances. Finally, the proposed method has been compared with other methods such as AHP, BWM, TOPSIS, VIKOR, PROMETHEE and QUALIFLEX. Based on comparisons among the weights and ranks using Spearman and Pearson correlation coefficients, the proposed method has an applicable performance compared with other methods. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105893	10.1016/j.asoc.2019.105893													
J								Chest X-ray enhancement to interpret pneumonia malformation based on fuzzy soft set and Dempster-Shafer theory of evidence	APPLIED SOFT COMPUTING										Synchrotron x-ray tomography; Image enhancement; Fuzzy sets; Fuzzy soft sets; Dempster-Shafer theory	AUTOMATIC CONTRAST ENHANCEMENT; IMAGE QUALITY ASSESSMENT; MR-IMAGES; FRAMEWORK; ENTROPY	Image enhancement algorithms are commonly used to increase the contrast and visual quality of low-dose x-ray images. This paper proposes an automated enhancement method using soft fuzzy sets with a new decision-making scheme based on Dempster-Shafer theory of evidence for the visual interpretation of pneumonia malformation in low-dose x-ray images, called as XEFSDS. The XEFSDS model first generates an original source x-ray image into a complementary image, then each original and complement image is applied to the characterized image object and background areas of fuzzy space. The S-function is utilized to define fuzzy soft sets for the classification of gray level ambiguity in both images, and hence a decision criterion via Dempster-Shafer approach and fuzzy interval has been adapted to discriminate uncertainties on the pixel intensity and the spatial information. Modified membership grade operations have been performed on each object/background area, and Werner's AND/OR operator (an aggregation operator) has been utilized to build a new membership function from two modified membership functions. Finally, an enhanced image is obtained from the new membership function via defuzzification. Experiments on different pneumonia X-ray images demonstrate that the XEFSDS scheme produces better results than the existing methods. To show the advantages of the XEFSDS scheme, we have executed a segmentation based examination on enhanced image for the detection of pneumonia malformation as well as abnormal lobe (lobar pneumonia) or bronchopneumonia. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105889	10.1016/j.asoc.2019.105889													
J								Dual-branch residual network for lung nodule segmentation	APPLIED SOFT COMPUTING										Lung nodule segmentation; Residual neural networks; Deep learning; Computer-aided diagnosis	CONVOLUTIONAL NEURAL-NETWORKS; COMPUTED-TOMOGRAPHY IMAGES; FALSE-POSITIVE REDUCTION; BRAIN-TUMOR SEGMENTATION; SMALL PULMONARY NODULES; CT SCANS; DATABASE CONSORTIUM; ALGORITHMS; MODEL; GRADIENT	An accurate segmentation of lung nodules in computed tomography (CT) images is critical to lung cancer analysis and diagnosis. However, due to the variety of lung nodules and the similarity of visual characteristics between nodules and their surroundings, a robust segmentation of nodules becomes a challenging problem. In this study, we propose the Dual-branch Residual Network (DB-ResNet) which is a data-driven model. Our approach integrates two new schemes to improve the generalization capability of the model: (1) the proposed model can simultaneously capture multi-view and multi-scale features of different nodules in CT images; (2) we combine the features of the intensity and the convolutional neural networks (CNN). We propose a pooling method, called the central intensity-pooling layer (CIP), to extract the intensity features of the center voxel of the block, and then use the CNN to obtain the convolutional features of the center voxel of the block. In addition, we designed a weighted sampling strategy based on the boundary of nodules for the selection of those voxels using the weighting score, to increase the accuracy of the model. The proposed method has been extensively evaluated on the LIDC-IDRI dataset containing 986 nodules. Experimental results show that the DB-ResNet achieves superior segmentation performance with the dice similarity coefficient (DSC) of 82.74% on the dataset. Moreover, we compared our results with those of four radiologists on the same dataset. The comparison showed that our DSC was 0.49% higher than that of human experts. This proves that our proposed method is as good as the experienced radiologist. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105934	10.1016/j.asoc.2019.105934													
J								A new convolutional neural network model for peripapillary atrophy area segmentation from retinal fundus images	APPLIED SOFT COMPUTING										Deep learning; Medical image segmentation; Peripapillary atrophy segmentation; Convolutional neural networks; Fully convolutional network	PARAPAPILLARY ATROPHY; AUTOMATED DIAGNOSIS; LEVEL SET	Peripapillary atrophy (PPA) is a clinical finding, which reflects the atrophy of retina layer and retinal pigment epithelium. The size of PPA area is a useful medical indicator, as it is highly associated with many diseases such as glaucoma and myopia. Therefore, separating the PPA area from retinal images, which is called PPA area segmentation, is very important. It is a challenging task, because PPA areas are irregular and non-uniform, and their contours are blurry and change gradually. To solve these issues, we transform the PPA area segmentation task into a task of segmenting another two areas with relatively regular and uniform shapes, and then propose a novel multi-task fully convolutional network (MFCN) model to jointly extract them from retinal images. Meanwhile, we take edge continuity of the target area into consideration. To evaluate the performance of the proposed model, we conduct experiments on images with PPA areas labelled by experts and achieve an average precision of 0.8928, outperforming the state-of-the-art models. To demonstrate the application of PPA segmentation in medical research, we apply PPA related features based on the segmented PPA area on differentiating glaucomatous and physiologic large cup cases. Experiment conducted on real datasets confirms the effectiveness of using these features for glaucoma diagnosis. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105890	10.1016/j.asoc.2019.105890													
J								Hierarchical Learning Water Cycle Algorithm	APPLIED SOFT COMPUTING										Metaheuristic; Water cycle algorithm; Hierarchical learning; Active target choosing	SWARM OPTIMIZATION ALGORITHM; EVAPORATION RATE; EVOLUTION; BEHAVIOR	In order to improve the global searching ability of Water Cycle Algorithm (WCA), the hierarchical learning concept is introduced and the Hierarchical Learning WCA (HLWCA) is proposed in this paper. The underlying idea of HLWCA is to divide the solutions into collections and give these collections with hierarchy differences. One of the collections has a higher hierarchy than others and utilizes an exploration-inclined updating mechanism. The solutions in this high hierarchy collection are the exemplars of other collections. The other collections are sorted according to the exemplars' function value and the solutions in these collections actively choose whether to follow their own exemplar or not. Through different updating mechanisms of collections, the global searching ability is improved while the fast convergence and strong local search ability of WCA are retained. The proposed HLWCA is firstly experimented on IEEE CEC 2017 benchmark suite to testify its performance on complex numerical optimization tasks. Then, it is tested on four practical design benchmark problems to verify its ability of solving real-world problems. The experimental results illustrate the efficiency of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105935	10.1016/j.asoc.2019.105935													
J								An enhanced Bacterial Foraging Optimization and its application for training kernel extreme learning machine	APPLIED SOFT COMPUTING										Bacterial Foraging Optimization; Chaotic local search; Gaussian mutation; Kernel extreme learning machine	PARTICLE SWARM OPTIMIZATION; GLOBAL OPTIMIZATION; DIFFERENTIAL EVOLUTION; COLONY OPTIMIZATION; ALGORITHM; CHEMOTAXIS; STRATEGY	The Bacterial Foraging Optimization (BFO) algorithm is a swarm intelligent algorithm widely used in various optimization problems. However, BFO suffers from multiple drawbacks, including slow convergence speed, inability to jump out of local optima and fixed step length. In this study, an enhanced BFO with chaotic chemotaxis step length, Gaussian mutation and chaotic local search (CCGBFO) is proposed for overcoming the existing weakness of original BFO. First, a chaotic chemotaxis step length operation is used to produce adaptive chemotaxis step length. Then, by combining the optimal position in the current bacteria with the Gaussian mutation operation to make full use of the information of the optimal position. Finally, a chaotic local search is introduced into the chemotaxis step to ensure that the algorithm can explore a large search space in the early stage. The performance of CCGBFO was evaluated on a comprehensive set of numerical benchmark functions including IEEE CEC2014 and CEC2017 problems. In addition, CCGBFO was also used to tune the key parameters of kernel extreme learning machine for dealing with the real-world problems. The experimental results show that the proposed CCGBFO significantly outperforms the original BFO in terms of both convergence speed and solution accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105884	10.1016/j.asoc.2019.105884													
J								A new approach to formulate fuzzy regression models	APPLIED SOFT COMPUTING										Fuzzy regression model; Fuzzy product core; Mathematical programming	LEAST-SQUARES ESTIMATION; INPUT; DISTANCE	A fuzzy regression model is developed to construct the relationship between the response and explanatory variables in fuzzy environments. To enhance explanatory power and take into account the uncertainty of the formulated model and parameters, a new operator, called the fuzzy product core (FPC), is proposed for the formulation processes to establish fuzzy regression models with fuzzy parameters using fuzzy observations that include fuzzy response and explanatory variables. In addition, the sign of parameters can be determined in the model-building processes. Compared to existing approaches, the proposed approach reduces the amount of unnecessary or unimportant information arising from fuzzy observations and determines the sign of parameters in the models to increase model performance. This improves the weakness of the relevant approaches in which the parameters in the models are fuzzy and must be predetermined in the formulation processes. The proposed approach outperforms existing models in terms of distance, mean similarity, and credibility measures, even when crisp explanatory variables are used. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105915	10.1016/j.asoc.2019.105915													
J								Do you know your customer? Bank risk assessment based on machine learning	APPLIED SOFT COMPUTING										Know your customer; Risk assessment; Machine learning; Support vector machines; Decision trees		Know Your Customer (KYC) data can serve as a valuable risk assessment tool for banks by providing information that can identify customers who are more likely to default on a loan. This study aims to provide an accurate risk assessment tool using unique KYC data and machine-learning techniques to overcome problems in existing risk detection methods. This study proposes that the bank branch is the best level at which to determine the degree of default risk, and can also provide insight into patterns of suspicious transactions. Bank managers and regulators can focus on suspicious behavior at specific branches to increase overall compliance and reduce the risk of illegal activity. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105779	10.1016/j.asoc.2019.105779													
J								A novel deep learning method based on attention mechanism for bearing remaining useful life prediction	APPLIED SOFT COMPUTING										Remaining useful life prediction; Recurrent neural network; Attention mechanism	CONVOLUTIONAL NEURAL-NETWORK; FAULT-DIAGNOSIS; PROGNOSTICS; MACHINERY	Rolling bearing is a key component in rotation machine, whose remaining useful life (RUL) prediction is an essential issue of constructing condition-based maintenance (CBM) system. However, recent data-driven approaches for bearing RUL prediction still require prior knowledge to extract features, construct health indicate (HI) and set up threshold, which is inefficient in the big data era. In this paper, a pure data-driven method for bearing RUL prediction with little prior knowledge is proposed. This method includes three steps, i.e., features extraction, HI prediction and RUL calculation. In the first step, five band-pass energy values of frequency spectrum are extracted as features. Then, a recurrent neural network based on encoder-decoder framework with attention mechanism is proposed to predict HI values, which are designed closely related with the RUL values in this paper. Finally, the final RUL value can be obtained via linear regression. Experiments carried out on the dataset from PRONOSTIA and comparison with other novel approaches demonstrate that the proposed method achieves a better performance. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105919	10.1016/j.asoc.2019.105919													
J								Maximizing receiver operating characteristics convex hull via dynamic reference point-based multi-objective evolutionary algorithm	APPLIED SOFT COMPUTING										Receiver operating characteristic; Evolutionary optimization; Reference point	ROC CURVE; OPTIMIZATION; CLASSIFIERS; AREA; TESTS	The receiver operating characteristic convex hull (ROCCH) is a popular technique for analyzing the performance of classifiers, which is particularly effective for the tasks with unbalanced data distribution. Although maximization of ROCCH can be tackled as a bi-objective optimization problem, existing multi-objective evolutionary algorithms (MOEAs) encounter difficulties in obtaining an ROCCH, since ROCCH is always convex but the Pareto front obtained by MOEAs may be concave. To address the issue, in this paper, a dynamic reference point-based MOEA, namely DR-MOEA is proposed for maximizing ROCCH performance. Specifically, in DR-MOEA, a reference point-based sorting is suggested, where the solutions are sorted by their distances to the reference points instead of Pareto dominance. Hence an ROCCH rather than a Pareto front is expected to be obtained. In addition, a reference point adaptation strategy is also designed, with which the reference points are dynamically adjusted during the evolutionary process, and the performance of DR-MOEA is further enhanced. Empirical studies are conducted by comparing the proposed algorithm with several state-of-the-arts on different data sets. Experimental results demonstrate the superiority of DR-MOEA over the comparison methods in solving the ROCCH maximization problem. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105896	10.1016/j.asoc.2019.105896													
J								Missing multi-label learning with non-equilibrium based on classification margin	APPLIED SOFT COMPUTING										Multi-label learning; Label completion; Information entropy; Classification margin; Non-equilibrium	FEATURE-SELECTION; ALGORITHM; MACHINE; TOOL	Multi-labels are more suitable for the ambiguity of the real world. However, missing labels are common in multi-label learning datasets; this results in unbalanced labeling and label diversity, which directly affect the performance of multi-label learning. Therefore, the classification and modeling of imbalanced data in missing multi-label learning are problems that need to be urgently solved Current methods mostly focus on combining sampling techniques with cost-sensitive learning and incorporating label correlation to improve the performance of the classifier, but generally they do not consider label loss caused by label cost. In fact, labeling unknown instances is often affected by the threshold of the discriminant function, especially for the label types near the threshold. Based on our previous research, we believe that information such as data distribution density and label density can be integrated into the label correlation, and that the classification margin can be expanded to effectively solve the labeling quality of labels near the threshold. Therefore, in this paper we propose a non-equilibrium multi-label learning algorithm based on the classification margin and aimed at completing the missing labels. First, the classification margin is proposed, and the label space is expanded by the label density. Then, the information entropy is used to measure the correlation between labels, and the label confidence matrix is constructed. The label confidence matrix is then unbalanced using the positive and negative label density, and the non-equilibrium label confidence matrix is used for label completion to obtain an informative label completion matrix. Finally, the kernel extreme learning machine and the label completion matrix are used for linear prediction. The experimental results show that the proposed algorithm has some advantages over other multi-label learning algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105924	10.1016/j.asoc.2019.105924													
J								Analysis of brain sub regions using optimization techniques and deep learning method in Alzheimer disease	APPLIED SOFT COMPUTING										Alzheimer; Brain sub regions; Deep learning; Hippocampus; Optimization	MILD COGNITIVE IMPAIRMENT; CUCKOO SEARCH ALGORITHM; CORPUS-CALLOSUM; SEGMENTATION; HIPPOCAMPUS; MRI	Automatic segmentation of brain sub regions such as White Matter (GM), Corpus Callosum (CC), Grey Matter (WM) and Hippocampus (HC) is a challenging task due to the variations in the structure of the brain. The development of an automation process will help in identifying the Alzheimer disease (AD). The maximum distinctive AD related to neuronal loss in the brain sub regions is GM, WM, CC and HC. Several nature optimization algorithms are being developed to get an optimum solution for segmenting these kinds of very hectic regions. In this work, brain sub regions have been considered to diagnose the AD using four various optimization algorithms such as Genetic Algorithm (GA), Particle Swarm Optimization algorithm (PSO), Grey Wolf Optimization (GWO) and Cuckoo Search (CS). Among these optimization techniques, GWO shows promising results due to the proper selection of global optimum solution. The segmented regions have been classified using deep learning classifier and it has been validated with Ground Truth (GT) images. The results prove that GWO is capable to segment the brain sub regions with high accuracy of 98% similarity among the ground truth and segmented region. Then, the segmented regions are classified using deep learning classifier and the results show the high accuracy of 95%. From the above sub regions, HC proves better classification accuracy of 95%, sensitivity as 95% and specificity as 94% compared to all other regions. Based on the output of segmentation and classification measures it is clearly observed that, the proposed method provide better performance than other methods. Finally, the normal and AD subjects are also validated clinically with mini mental state examination (MMSE) score. From this evaluation it is observed that, the proposed work is highly correlated with MMSE score. Therefore, the proposed pipeline witnessed that the HC region is the major factor for diagnosing AD. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105857	10.1016/j.asoc.2019.105857													
J								Classification of spatio-temporal trajectories from Volunteer Geographic Information through fuzzy rules	APPLIED SOFT COMPUTING										Trajectory classification; Fuzzy rule classifier; Volunteer Geographic Information; Tourist mobility	TRANSPORTATION MODES; PREDICTION; PATTERNS	Volunteer Geographic Information (VGI) is one of the key enablers of the mobility mining discipline. This work introduces a novel data-driven methodology to create a classifier of spatio-temporal trajectories based on VGI. Although other solutions have been proposed, they usually do not fully consider the low resolution and uncertainty of VGI due to its inherent human nature. The proposed approach introduces a classifier based on fuzzy rules that are able to deal with this kind of data. The solution is applied in a use case for real-time detection of tourists and local citizens' flows and it is compared with a well-established trajectory classifier exhibiting quite promising results. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105916	10.1016/j.asoc.2019.105916													
J								Human action recognition using two-stream attention based LSTM networks	APPLIED SOFT COMPUTING										Human action recognition; Visual attention mechanism; LSTM network; Deep feature correlation layer	NEURAL-NETWORKS; MACHINE; IMAGE	It is well known that different frames play different roles in feature learning in video based human action recognition task. However, most existing deep learning models put the same weights on different visual and temporal cues in the parameter training stage, which severely affects the feature distinction determination. To address this problem, this paper utilizes the visual attention mechanism and proposes an end-to-end two-stream attention based LSTM network. It can selectively focus on the effective features for the original input images and pay different levels of attentions to the outputs of each deep feature maps. Moreover, considering the correlation between two deep feature streams, a deep feature correlation layer is proposed to adjust the deep learning network parameter based on the correlation judgement. In the end, we evaluate our approach on three different datasets, and the experiments results show that our proposal can achieve the state-of-the-art performance in the common scenarios. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105820	10.1016/j.asoc.2019.105820													
J								Ensemble approach based on bagging, boosting and stacking for short-term prediction in agribusiness time series	APPLIED SOFT COMPUTING										Bagging; Boosting; Agricultural commodity; Ensemble regression; Stacking; Time series	SUPPORT VECTOR REGRESSION; MACHINE LEARNING TECHNIQUES; GOLD-PRICE FLUCTUATIONS; MODELS; SYSTEM; METHODOLOGY; CONSUMPTION; VOLATILITY; ALGORITHMS; PARAMETERS	The investigation of the accuracy of methods employed to forecast agricultural commodities prices is an important area of study. In this context, the development of effective models is necessary. Regression ensembles can be used for this purpose. An ensemble is a set of combined models which act together to forecast a response variable with lower error. Faced with this, the general contribution of this work is to explore the predictive capability of regression ensembles by comparing ensembles among themselves, as well as with approaches that consider a single model (reference models) in the agribusiness area to forecast prices one month ahead. In this aspect, monthly time series referring to the price paid to producers in the state of Parana, Brazil for a 60 kg bag of soybean (case study 1) and wheat (case study 2) are used. The ensembles bagging (random forests - RF), boosting (gradient boosting machine - GBM and extreme gradient boosting machine - XGB), and stacking (STACK) are adopted. The support vector machine for regression (SVR), multilayer perceptron neural network (MLP) and K-nearest neighbors (KNN) are adopted as reference models. Performance measures such as mean absolute percentage error (MAPE), root mean squared error (RMSE), mean absolute error (MAE), and mean squared error (MSE) are used for models comparison. Friedman and Wilcoxon signed rank tests are applied to evaluate the models' absolute percentage errors (APE). From the comparison of test set results, MAPE lower than 1% is observed for the best ensemble approaches. In this context, the XGB/STACK (Least Absolute Shrinkage and Selection Operator-KNN-XGB-SVR) and RF models showed better performance for short-term forecasting tasks for case studies 1 and 2, respectively. Better APE (statistically smaller) is observed for XGB/STACK and RF in relation to reference models. Besides that, approaches based on boosting are consistent, providing good results in both case studies. Alongside, a rank according to the performances is: XGB, GBM, RF, STACK, MLP, SVR and KNN. It can be concluded that the ensemble approach presents statistically significant gains, reducing prediction errors for the price series studied. The use of ensembles is recommended to forecast agricultural commodities prices one month ahead, since a more assertive performance is observed, which allows to increase the accuracy of the constructed model and reduce decision-making risk. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105837	10.1016/j.asoc.2019.105837													
J								Automatic determination of digital modulation types with different noises using Convolutional Neural Network based on time-frequency information	APPLIED SOFT COMPUTING										Modulation type classification; Digital modulation; Deep learning; Convolutional neural network (CNN); Short-time Fourier transform (STFT)	CLASSIFICATION; SIGNALS	In this study, a novel digital modulation classification model has been proposed for automatically recognizing six different modulation types including amplitude shift keying (ASK), frequency shift keying (FSK), phase-shift keying (PSK), quadrate amplitude shift keying (QASK), quadrate frequency shift keying (QFSK), and quadrate phase-shift keying (QPSK). The determination of modulation type is significant in military communication, satellite communication systems, and submarine communication. To classify the modulation types, we have proposed a two-stage hybrid method combining short-time Fourier transform (STFT) and convolutional neural network (CNN). In the first stage, as the data source, the time-frequency information from these modulation signals have been extracted with STFT. This information has been obtained as 2D images to feed the input of the CNN deep learning method. In the second stage, the obtained 2D time-frequency information has been given to the input of the CNN algorithm to classify the modulation types. In this work, noises at various SNR values from 0 dB to 25 dB were created and added to the modulated signals. Even in the presence of noise, the proposed hybrid deep learning model achieved excellent results in the noised-modulation signals. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105834	10.1016/j.asoc.2019.105834													
J								Unsupervised feature learning for environmental sound classification using Weighted Cycle-Consistent Generative Adversarial Network	APPLIED SOFT COMPUTING										Environmental sound classification; Generative Adversarial Network (GAN); Cycle-Consistent GAN; K-means plus; Random forests	QUALITY ASSESSMENT; AUDIO; RECOGNITION	In this paper we propose a novel environmental sound classification approach incorporating unsupervised feature learning via the spherical K-Means++ algorithm and a new architecture for high-level data augmentation. The audio signal is transformed into a 2D representation using a discrete wavelet transform (DWT). The DWT spectrograms are then augmented by a novel architecture for cycle-consistent generative adversarial network. This high-level augmentation bootstraps generated spectrograms in both intra-and inter-class manners by translating structural features from sample to sample. A codebook is built by coding the DWT spectrograms with the speeded-up robust feature detector and the K-Means++ algorithm. The Random forest is the final learning algorithm which learns the environmental sound classification task from the code vectors. Experimental results in four benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and DCASE-2017) have shown that the proposed classification approach outperforms most of the state-of-the-art classifiers, including convolutional neural networks such as AlexNet and GoogLeNet, improving the classification rate between 3.51% and 14.34%, depending on the dataset. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105912	10.1016/j.asoc.2019.105912													
J								Robust Heterogeneous C-means	APPLIED SOFT COMPUTING										Robust; Loss function; Clustering; C-Means; Half-quadratic; Heterogeneous	MEANS ALGORITHM; FUZZY; FCM	Fuzzy c-means is one of the popular algorithms in clustering, but it has some drawbacks such as sensitivity to outliers. Although many correntropy based works have been proposed to improve the robustness of FCM, fundamentally a proper error function is required to apply to FCM. In this paper, we present a new perspective based on the expected loss (or risk) to FCM method to provide different kinds of robustness such as robustness to outliers, to the volume of clusters and robustness in noisy environments. First, we propose Robust FCM method (RCM) by defining a loss function as a least square problem and benefiting the correntropy to make FCM robust to outliers. Furthermore, we utilize the half-quadratic (HQ) optimization as a problem-solving method. Second, inspiring by the Bayesian perspective, we define a new loss function based on correntropy as a distance metric to present Robust Heterogeneous C-Means (RHCM) by utilizing direct clustering (DC) method. DC helps RHCM to have robust initialization. Besides, RHCM will make some robust cluster centers in noisy environments and is capable of clustering the elliptical or spherical shaped data accurately, regardless of the volume of each cluster. The results are shown visually on some synthetic datasets including the noisy ones, the UCI repository and also on real image dataset that was gathered manually from 500px social media. Also, for evaluation of the clustering results, several validity indices are calculated. Experimental results indicate the superiority of our proposed method over the base FCM, DC, KFCM, two new methods called GPFCM and GEPFCM and a method called DC-KFCM that we created for the comparison purpose. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105885	10.1016/j.asoc.2019.105885													
J								Probabilistic tree-based representation for solving minimum cost integer flow problems with nonlinear non-convex cost functions	APPLIED SOFT COMPUTING										Minimum cost flow problem; Genetic algorithm; Representation scheme; Mixed integer nonlinear programming; Taguchi experimental design	DYNAMIC-PROGRAMMING APPROACH; SEQUENCE-DEPENDENT SETUP; GENETIC ALGORITHM; TRANSPORTATION PROBLEM; OPTIMIZATION; FLOWSHOPS; DESIGN; TIMES	The minimum cost flow problem (MCFP) is the most generic variation of the network flow problem which aims to transfer a commodity throughout the network to satisfy demands. The problem size (in terms of the number of nodes and arcs) and the shape of the cost function are the most critical factors when considering MCFPs. Existing mathematical programming techniques often assume the cost functions to be linear or convex. Unfortunately, the linearity and convexity assumptions are too restrictive for modelling many real-world scenarios. In addition, many real-world MCFPs are large-scale, with networks having a large number of nodes and arcs. In this paper, we propose a probabilistic tree-based genetic algorithm (PTbGA) for solving large-scale minimum cost integer flow problems with nonlinear non-convex cost functions. We first compare this probabilistic tree-based representation scheme with the priority-based representation scheme, which is the most commonly-used representation for solving MCFPs. We then compare the performance of PTbGA with that of the priority-based genetic algorithm (PrGA), and two state-of-the-art mathematical solvers on a set of MCFP instances. Our experimental results demonstrate the superiority and efficiency of PTbGA in dealing with large-sized MCFPs, as compared to the PrGA method and the mathematical solvers. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105951	10.1016/j.asoc.2019.105951													
J								A hybrid non-linear time-varying double-weighted particle swarm optimization for solving non-convex combined environmental economic dispatch problem	APPLIED SOFT COMPUTING										Combined environmental economic dispatch (CEED); MGAIPSO; Non-convex optimization; Robust constraint handling technique; Generators operational constraints	LOAD DISPATCH; DIFFERENTIAL EVOLUTION; SEARCH ALGORITHM; PHYSICAL CONSTRAINTS; GENERATING-UNITS; PSO; STRATEGY	Fossil-fuel based power sources cause environmental pollution such as the degradation of air quality and climate change, which negatively impacts the life on the earth. Consequently, this demands that the power generation should consider the optimal management of thermal sources that are aimed at minimizing the emission of gasses in the generation mix. The production volume of multi-pollutant gasses (SO2, NOx, and CO2) can be reduced through a combined environmental economic dispatch (CEED) approach. This study has proposed a hybrid algorithm based on a novel combination of a modified genetic algorithm and an improved version of particle swarm optimization abbreviated as MGAIPSO to solve CEED problem. The study utilizes three robust operators to enhance the performance of the proposed hybrid algorithm. In GA, a uniformly weighted arithmetic crossover and a normally distributed mutation operator have been implemented to produce elite off-springs in each iteration and diversify the solutions in the search space. In the case of PSO, a non-linear time-varying double-weighted (NLTVDW) technique is developed to obtain a substantial balance between exploration and exploitation. To further enhance the exploitation ability of the MGAIPSO, this study has implemented two movements correctional methods to continuously monitor and amend the position and velocity of the particles. Several numerical case studies ranging from small to large-scale are carried out to validate the practicality of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105894	10.1016/j.asoc.2019.105894													
J								An improved whale optimization algorithm for forecasting water resources demand	APPLIED SOFT COMPUTING										Water demand forecasting; Whale optimization algorithm; Social learning; Social network; Wavelet mutation; CEC2017 benchmark functions	NEURAL-NETWORK; DESIGN; DECOMPOSITION; MODELS; ARIMA	Water demand forecasting can promote the rational use of water resources and alleviate the pressure on water demand. By analyzing the use of water resources, this paper establishes three models of water demand forecasting, logarithmic model, linear and exponential combination model and linear, exponential and logarithmic hybrid models. In order to accurately estimate the demand for water resources, an improved whale optimization algorithm based on social learning and wavelet mutation strategy is proposed. The new algorithm designs a new linear incremental probability, which increases the possibility of global search of the algorithm. Based on the social learning principle, the social ranking and social influence are used to construct the social network for the individual, and the adaptive neighborhood learning strategy based on the network relationship is established to achieve the exchange and sharing of information between groups. The Morlet wavelet mutation mechanism is integrated to realize the dynamic adjustment of the mutation space, which enhances the ability of the algorithm to escape from local optimization. The latest CEC2017 benchmark functions confirms the superiority of the proposed algorithm. The water consumption from 2004 to 2016 in Shaanxi Province of China is used for the experiment. The results show that the performance of the proposed algorithm for solving the three water resources forecasting model is better in comparison to other algorithms. The prediction accuracy is as high as 99.68%, which verified the validity of the model and the practicality of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105925	10.1016/j.asoc.2019.105925													
J								OCE-NGC: A neutrosophic graph cut algorithm using optimized clustering estimation algorithm for dermoscopic skin lesion segmentation	APPLIED SOFT COMPUTING										Dermoscopic images; Skin lesion; Genetic algorithm; Neutrosophic c-means; Neutrosophic graph cut	GREY WOLF OPTIMIZER; FUZZY	Automated skin lesion segmentation is one of the most crucial stages in dermoscopic images based diagnosis. To guarantee efficient unsupervised clustering-based segmentation, a histogram-based clustering estimation (HBCE) algorithm can be used to obtain the initial number of clusters with their corresponding centroids. Accordingly, the present work introduced a novel skin lesion segmentation algorithm, called optimized clustering estimation for neutrosophic graph cut algorithm (OCE-NGC). Firstly, the genetic algorithm (GA) is used to optimize the HBCE procedure by finding its optimal threshold values which are functions of a factor, called beta to be optimized. This optimization process guarantees the optimal determination of the initial number of clusters and their corresponding centroids for further use in the proposed clustering process. Thus, the skin lesion dermoscopic images are then mapped into the neutrosophic set (NS) domain which is computed by the neutrosophic c-means (NCM). The NCM groups the pixels in the dermoscopic images using the pre-determined optimal number of clusters obtained by the optimized HBCE. Finally, a cost function of the graph cut (GC) algorithm is defined in the NS domain for the segmentation process. The experimental results established the superiority of the proposed OCE-NGC approach in comparison with the traditional HBCE with NCM only, the traditional HBCE with the NGC, and the typical GC. In a public dataset, the proposed approach achieved 97.12% and 86.28% average accuracy and average Jaccard (JAC) values, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105931	10.1016/j.asoc.2019.105931													
J								Metaheuristics for maximization of obstacles constrained area coverage in heterogeneous wireless sensor networks	APPLIED SOFT COMPUTING										Area coverage optimization; Genetic algorithm; Particle swarm optimization; Heuristic initialization; Wireless sensor network; Monte Carlo; Virtual force algorithm	DEPLOYMENT	Wireless Sensor Networks (WSNs) collect and transfer environmental data from a predefined field to a base station to be processed and analyzed. A major problem in designing WSNs is coverage maximization, in which a given number of sensor nodes must be deployed in a way that maximizes area coverage of a given network, without violating practical constraints. This is a known NP-hard problem and thus requires metaheuristic approaches for practical problem sizes. Two metaheuristics, namely Genetic Algorithm and Particle Swarm Optimization are proposed to tackle this problem. Our new contributions include a partial use of heuristic initialization, new fitness function, modified virtual force algorithm, addition of a uniform deceleration to the calculation of inertia weight and addition of the influence of sub-populations' head individuals. The proposed algorithms are comprehensively experimented and compared with the current state-of-the-art for the equivalent problem without obstacles. Experimental results not only suggest which algorithms should be applied to which cases, but also provide insights into parameter settings, effects of heuristic initialization and effects of virtual force algorithm in each case. These conclusions are meaningful for our future research on obstacles constrained area coverage problems related to connectivity and lifetime of WSNs. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105939	10.1016/j.asoc.2019.105939													
J								Estimating incomplete information in group decision making: A framework of granular computing	APPLIED SOFT COMPUTING										Group decision making; Incomplete information; Consistency; Fuzzy preference relation; Information granularity	FUZZY PREFERENCE RELATIONS; CONSENSUS MODEL; MULTIPLICATIVE CONSISTENCY; LINGUISTIC INFORMATION; SUPPORT-SYSTEM; PSO; ALLOCATION	A general assumption in group decision making scenarios is that of all individuals possess accurate knowledge of the entire problem under study, including the abilities to make a distinction of the degree up to which an alternative is better than other one. However, in many real world scenarios, this may be unrealistic, particularly those involving numerous individuals and options to choose from conflicting and dynamics information sources. To manage such a situation, estimation methods of incomplete information, which use own assessments provided by the individuals and consistency criteria to avoid discrepancy, have been widely employed under fuzzy preference relations. In this study, we introduce the information granularity concept to estimate missing values supporting the objective of obtaining complete fuzzy preference relations with higher consistency levels. We use the concept of granular preference relations to form each missing value as a granule of information in place of a crisp number. This offers the flexibility that is required to estimate the missing information so that the consistency levels related to the complete fuzzy preference relations are as higher as possible. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105930	10.1016/j.asoc.2019.105930													
J								Weak-restriction bi-objective optimization algorithm for scheduling with rejection on non-identical batch processing machines	APPLIED SOFT COMPUTING										Parallel batch machines; Non-identical capacities; Selection restriction; Rejection cost; Ant colony optimization	ANT COLONY OPTIMIZATION; SINGLE-MACHINE; MINIMIZING MAKESPAN; GENETIC ALGORITHM; PARALLEL MACHINES; RELEASE TIMES; JOB SIZES; CAPACITIES; CRITERIA; HEURISTICS	We investigate the problem of scheduling a set of jobs with arbitrary sizes and unequal weights on a set of parallel batch machines with non-identical capacities. The objective is to minimize the makespan of the accepted jobs and the total rejection penalty of the rejected jobs, simultaneously. To address the studied problem, a Pareto-based ant colony optimization algorithm with the first job selection probability (FPACO) is proposed. A weak-restriction selection strategy is proposed to obtain the desirability of candidate jobs. Two objective-oriented heuristic information and pheromone matrices are designed, respectively, to record the experience in different search dimensions. Moreover, a local optimization algorithm is incorporated to improve the solution quality. Finally, the proposed algorithm is compared with four existing algorithms through extensive simulation experiments. The experimental results indicate that the proposed algorithm outperforms all of the compared algorithms within a reasonable time. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105914	10.1016/j.asoc.2019.105914													
J								A large group linguistic Z-DEMATEL approach for identifying key performance indicators in hospital performance management	APPLIED SOFT COMPUTING										Hospital management; Key performance indicator; DEMATEL method; Linguistic Z-number; Large group decision-making	WASTE TREATMENT TECHNOLOGIES; DECISION-MAKING; HEALTH-CARE; MAXIMIZING CONSENSUS; SUCCESS FACTORS; QUALITY; SETS; FRAMEWORK	In hospital management, performance measurement is of vital importance for improving healthcare service quality. The performance of a healthcare organization is often influenced by numerous indicators, and it is unrealistic to manage them all due to the restriction of resources. In addition, the performance measurement for improvement relates to the benefits of many departments, and it is necessary for large number of experts with different backgrounds to participate in the evaluation process of healthcare indicators. In response, this study develops a large group evaluation approach using linguistic Z-numbers and decision-making trial and evaluation laboratory (DEMATEL) to determine key performance indicators (KPIs) for hospital management. For this approach, the complex and uncertain interrelation evaluations among indicators are given by experts using linguistic Z-numbers. An extended DEMATEL method is proposed to determine KPIs based on the cause and effect relationships of performance indicators. Finally, a case study in a rehabilitation hospital is presented to illustrate the effectiveness and usefulness of the proposed large group linguistic Z-DEMATEL approach. The results indicate that incidents/errors, accidents/adverse events, nosocomial infection, nursing technology pass rate, and length of stay are KPIs for the given application. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105900	10.1016/j.asoc.2019.105900													
J								A fully fuzzy best-worst multi attribute decision making method with triangular fuzzy number: A case study of maintenance assessment in the hospitals	APPLIED SOFT COMPUTING										Best-worst method; Multiple criteria evaluation; Fuzzy mathematical programming; Fuzzy numbers	HIERARCHY PROCESS; LOCATION SELECTION; SUPPORT-SYSTEM; BENCHMARKING; TOPSIS; MODEL; VIKOR; SUSTAINABILITY; IDENTIFICATION; LOGISTICS	This paper introduces the best-worst method to solve multi-attribute decision-making (MADM) problems in the fuzzy environment. In the proposed method, there is no need to do all the possible pairwise comparisons. In other words, only reference comparisons should be done. Reference comparisons consist of assessing the relative fuzzy preference of the best criterion (alternative) over others and all the criteria (alternatives) over the worst one. Afterward, a fully fuzzy linear mathematical model will be formulated and solved to determine the weight of the criteria. The same action will be performed to find the score of alternatives. This method has some interesting and valuable characteristics: (a) less required data for pairwise comparison, (b) high ability to provide a reliable solution, (c) it is an autonomous method along with its high capability to accompany another method. To evaluate the performance, it is compared with another fuzzy MADM method in an example. Furthermore, we apply this method for the maintenance evaluation of hospitals in Bojnord. The computational study confirms the high efficiency and satisfactory performance of the method, and results are validated by a low consistency ratio. Furthermore, the suggested methodology outperforms fuzzy AHP and well verified in the test instance. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105882	10.1016/j.asoc.2019.105882													
J								Attention embedded residual CNN for disease detection in tomato leaves	APPLIED SOFT COMPUTING										Attention; CNN; Residual connections; Tomato; Deep learning	SYSTEM	Automation in plant disease detection and diagnosis is one of the challenging research areas that has gained significant attention in the agricultural sector. Traditional disease detection methods rely on extracting handcrafted features from the acquired images to identify the type of infection. Also, the performance of these works solely depends on the nature of the handcrafted features selected. This can be addressed by learning the features automatically with the help of Convolutional Neural Networks (CNN). This research presents two different deep architectures for detecting the type of infection in tomato leaves. The first architecture applies residual learning to learn significant features for classification. The second architecture applies attention mechanism on top of the residual deep network. Experiments were conducted using Plant Village Dataset comprising of three diseases namely early blight, late blight, and leaf mold. The proposed work exploited the features learned by the CNN at various processing hierarchy using the attention mechanism and achieved an overall accuracy of 98% on the validation sets in the 5-fold cross-validation. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105933	10.1016/j.asoc.2019.105933													
J								An adaptive constrained type-2 fuzzy Hammerstein neural network data fusion scheme for low-cost SINS/GNSS navigation system	APPLIED SOFT COMPUTING										Constrained estimation; Low-cost navigation; Data fusion; SINS/GNSS; Type-2 fuzzy Hammerstein neural network	CUBATURE KALMAN FILTER; GPS/INS INTEGRATION; GPS OUTAGES; GNSS/INS; ALGORITHM	In low-cost micro-electro mechanical system (MEMS)-grade strap-down inertial navigation system (SINS), failure to compensate inertial sensors errors as well as un-modeled uncertainties in SINS could result in exponentially divergence in overall performance of low-cost SINSs. This study deals with the enhancement of low-cost SINS accuracy in combination of global navigation satellite system (GNSS). In this respect, a novel adaptive constrained integrated scheme for SINS/GNSS is developed based on type-2 fuzzy Hammerstein neural network (T2FHNN). To this aim, a gray-box Hammerstein neural network model are defined based on clear interpretation with the physical nature of the inertial sensors error. In addition a knowledge-based type-2 fuzzy programming extracted from inertial sensors data is also used for managing the learning rate of Hammerstein neural networks. Some vehicular real-world tests have been carried out in order to show the effectiveness and feasibility of the proposed integration scheme in the long-term performance and accuracy of the proposed navigation algorithm. The results indicate that the proposed integration algorithm improved the navigation accuracy, reliability and stability in the presence of state constraints of the stand-alone SINS during signal blockage of GNSS. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105917	10.1016/j.asoc.2019.105917													
J								Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods	APPLIED SOFT COMPUTING										Feature selection; Text classification; MCDM; Small sample dataset	INFORMATION GAIN; ALGORITHMS	The evaluation of feature selection methods for text classification with small sample datasets must consider classification performance, stability, and efficiency. It is, thus, a multiple criteria decision-making (MCDM) problem. Yet there has been few research in feature selection evaluation using MCDM methods which considering multiple criteria. Therefore, we use MCDM-based methods for evaluating feature selection methods for text classification with small sample datasets. An experimental study is designed to compare five MCDM methods to validate the proposed approach with 10 feature selection methods, nine evaluation measures for binary classification, seven evaluation measures for multi-class classification, and three classifiers with 10 small datasets. Based on the ranked results of the five MCDM methods, we make recommendations concerning feature selection methods. The results demonstrate the effectiveness of the used MCDM-based method in evaluating feature selection methods. (C) 2019 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				JAN	2020	86								105836	10.1016/j.asoc.2019.105836													
J								An enhanced protein secondary structure prediction using deep learning framework on hybrid profile based features	APPLIED SOFT COMPUTING										Biological computing; Convolutional neural network; Deep learning; Protein secondary structure; Bidirectional recurrent neural network; Sequence profiles	RECURRENT NEURAL-NETWORKS; MODELS	Accurate protein secondary structure prediction (PSSP) is essential to identify structural classes, protein folds, and its tertiary structure. To identify the secondary structure, experimental methods exhibit higher precision with the trade-off of high cost and time. In this study, we propose an effective prediction model which consists of hybrid features of 42-dimensions with the combination of convolutional neural network (CNN) and bidirectional recurrent neural network (BRNN). The proposed model is accessed on four benchmark datasets such as CB6133, CB513, CASP10, and CAP11 using Q3, Q8, and segment overlap (Sov) metrics. The proposed model reported Q3 accuracy of 85.4%, 85.4%, 83.7%, 81.5%, and Q8 accuracy 75.8%, 73.5%, 72.2%, and 70% on CB6133, CB513, CASP10, and CAP11 datasets respectively. The results of the proposed model are improved by a minimum factor of 2.5% and 2.1% in Q3 and Q8 accuracy respectively, as compared to the popular existing models on CB513 dataset. Further, the quality of the Q3 results is validated by structural class prediction and compared with PSI-PRED. The experiment showed that the quality of the Q3 results of the proposed model is higher than that of PSI-PRED. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105926	10.1016/j.asoc.2019.105926													
J								Dynamic dispatching system using a deep denoising autoencoder for semiconductor manufacturing	APPLIED SOFT COMPUTING										Deep denoising autoencoder; Novelty detection; Dispatching rule selection; Storage allocation; Class imbalance problem	NOVELTY DETECTION; RULE SELECTION; SUPPORT	Deep denoising autoencoders (DDAE), which are variants of the autoencoder, have shown outstanding performance in various machine learning tasks. In this study, we propose using a DDAE to address a dispatching rule selection problem that represents a major problem in semiconductor manufacturing. Recently, the significance of dispatching systems for storage allocation has become more apparent because operational issues lead to transfer inefficiency, resulting in production losses. Further, recent approaches have overlooked the possibility of a class imbalance problem in predicting the best dispatching rule. The main purpose of this study is to examine DDAE-based predictive control of the storage dispatching systems to reduce idle machines and production losses. We conducted an experimental evaluation to compare the predictive performance of DDAE with those of five other novelty detection algorithms. Finally, we compared our adaptive approach with the optimization and existing heuristic approaches to demonstrate the effectiveness and efficiency of the proposed method. The experimental results demonstrated that the proposed method outperformed the existing methods in terms of machine utilizations and throughputs. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105904	10.1016/j.asoc.2019.105904													
J								Optimization strategies for Microgrid energy management systems by Genetic Algorithms	APPLIED SOFT COMPUTING										Microgrids; Genetic algorithms; Fuzzy systems; Energy management systems	DEMAND RESPONSE; DC	Grid-connected Microgrids (MGs) have a key role for bottom-up modernization of the electric distribution network forward next generation Smart Grids, allowing the application of Demand Response (DR) services, as well as the active participation of prosumers into the energy market. To this aim, MGs must be equipped with suitable Energy Management Systems (EMSs) in charge to efficiently manage in real time internal energy flows and the connection with the grid. Several decision making EMSs are proposed in literature mainly based on soft computing techniques and stochastic models. The adoption of Fuzzy Inference Systems (FISs) has proved to be very successful due to their ease of implementation, low computational run time cost, and the high level of interpretability with respect to more conventional models. In this work we investigate different strategies for the synthesis of a FIS (i.e. rule based) EMS by means of a hierarchical Genetic Algorithm (GA) with the aim to maximize the profit generated by the energy exchange with the grid, assuming a Time Of Use (TOU) energy price policy, and at the same time to reduce the EMS rule base system complexity. Results show that the performances are just 10% below to the ideal (optimal) reference solution, even when the rule base system is reduced to less than 30 rules. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105903	10.1016/j.asoc.2019.105903													
J								Fault diagnostics between different type of components: A transfer learning approach	APPLIED SOFT COMPUTING										Fault diagnostics; Deep learning; Transfer learning; Convolutional Neural Network (CNN); Multi-layer Perceptron (MLP)	DEEP; PROGNOSTICS; NETWORK	Transfer learning methods have been successfully applied into many fields for solving the problem of performance degradation in evolving working conditions or environments. This paper expands the range of transfer learning application by designing an integrated approach for fault diagnostics with different kinds of components. We use two deep learning methods, Convolutional Neural Network (CNN) and Multi-layer Perceptron (MLP), to train several base models with a mount of source data. Then the base models are transferred to target data with different level of variations, including the variations of working load and component type. Case Western Reserve University bearing dataset and 2009 PHM Data Challenge gearbox dataset are used to validate the performance of proposed approach. Experimental results show that proposed approach can improve the diagnostic accuracy not only between the working conditions from the same component but also different components. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105950	10.1016/j.asoc.2019.105950													
J								Air quality prediction by neuro-fuzzy modeling approach	APPLIED SOFT COMPUTING										Air quality; Fuzzy rules; Fuzzy neural network; Time series; Clustering; Optimization algorithms	PARTICULATE MATTER; FORECASTING PM10; NETWORK MODELS; MULTILAYER PERCEPTRON; NONLINEAR-REGRESSION; PM2.5 CONCENTRATIONS; LUNG-CANCER; MORTALITY; SYSTEM; OZONE	This paper proposes an air quality prediction system based on the neuro-fuzzy network approach. Historical time series data are employed to derive a set of fuzzy rules, or equivalently a neuro-fuzzy network, for forecasting air pollutant concentrations and environmental factors in the future. Due to the uncertainty of the involved impact factors, fuzzy elements are added to the forecasting system. First of all, training data are partitioned into fuzzy clusters whose membership functions are characterized by the estimated means and variances. From these fuzzy clusters, fuzzy rules are extracted and a four-layer fuzzy neural network is constructed. Then genetic, particle swarm optimization, and steepest descent backpropagation algorithms are applied to train the network. The network outputs, derived through the fuzzy inference process, produce the forecast air pollutant concentrations or air quality indices. Our proposed approach has the following advantages: (1) Adding fuzzy elements can more appropriately deal with the uncertainty of the impact factors involved; (2) The distribution of training data can be described properly by fuzzy clusters with statistical means and variances; (3) Fuzzy rules are extracted automatically from the training data, instead of being supplied manually by human experts; (4) The obtained fuzzy rules are of high quality, and their parameters can be optimized effectively. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105898	10.1016/j.asoc.2019.105898													
J								A new hesitant fuzzy linguistic approach for multiple attribute decision making based on Dempster-Shafer evidence theory	APPLIED SOFT COMPUTING										Multiple attribute decision making (MADM); Hesitant fuzzy linguistic term sets (HFLTSs); Dempster-Shafer evidence theory (DSET)	TERM SETS; AGGREGATION OPERATORS; CONSENSUS; WEIGHTS; MAGDM	Hesitant fuzzy linguistic term sets (HFLTSs) are useful tool to represent qualitative information in multiple attribute decision making (MADM), and Dempster-Shafer evidence theory (DSET) has some advantages in denoting and fusing uncertain information. The goal of this paper is to develop a new hesitant fuzzy linguistic (HFL) MADM approach based on the DSET. To realize this goal, we propose a method of converting the original decision matrix expressed by HFLTSs into the evidence matrix with HFLTSs, and develop a weight-determining model for MADM problems with HFL information. Further, in order to integrate the evidences with HFLTSs under all attributes, we propose a combination algorithm for MADM problems based on the combination rule of DSET. Based on these studies, we develop a HFL-DSET approach for MADM problems with unknown weights. Furthermore, an applicable example for supplier selection is used to illustrate the proposed approach. Lastly, some comparative analyses with other HFL-MADM methods are conducted to show the feasibility and superiority of the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105897	10.1016/j.asoc.2019.105897													
J								Multi robot distance based formation using Parallel Genetic Algorithm	APPLIED SOFT COMPUTING										Parallel Genetic Algorithm; Multi robot; Consensus; Formation; Distributed Artificial Intelligence	TIME	In this paper an alternative method to achieve distance based formation is presented. The method uses Genetic Algorithms to find a suitable solution based on angle and distance, and an appropriate constant velocity to avoid collisions. The designed algorithm is extended to a parallel scheme to improve its performance and achieve Artificial Distributed Intelligence, in which the robots share, through solution migration, the best ways to converge to desired distances while avoiding collisions, finally reaching consensus on the solution. The algorithm is tested using simulations and real robots experiments. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105929	10.1016/j.asoc.2019.105929													
J								Stackelberg-Nash equilibrium of pricing and inventory decisions in duopoly supply chains using a nested evolutionary algorithm	APPLIED SOFT COMPUTING										Inventory; Pricing; Supply chain; Game theory; Bi-level optimization; Threshold Accepting; Differential Evolution	INTEGRATED PRODUCTION-INVENTORY; REPLENISHMENT POLICIES; COMPETING RETAILERS; ORDERING DECISIONS; OPTIMIZATION; STRATEGIES; DEMAND; SYSTEM; PERISHABILITY; NETWORK	Pricing and inventory control in a competing environment, as separate entities, have attracted much attention from academics and practitioners. However, integrating these decisions in a competitive setting has not been significantly analyzed by academics, but is of great significance to practitioners. In this study, the joint decision on price and inventory control of a deterioration product is investigated in a duopoly setting. We consider two competing supply chains, each consisting of one manufacturer and one retailer. Each manufacturer, as the leader of their supply chain determines the wholesale price to maximize their profit, while the retailer as the follower should determine the retail price and inventory cycle to maximize his or her profit. Using a game theoretic approach, we formulate in-chain, and chain-to-chain competition as a bi-level programming problem, and analyze Stackelberg-Nash equilibrium of the problem. Furthermore, two versions of a nested algorithm are proposed to obtain the equilibrium. Both versions employ a modified threshold-accepting (TA) algorithm to solve the first level of the problem. However, while the first version utilizes the modified TA algorithm to deal with the second level of the problem, the second version applies a differential evolution (DE) approach. Eventually, a numerical study is carried out not only to compare two developed versions of the algorithm, but also to implement the sensitivity analysis of main parameters. Based on numerical experiments, although the accuracy of both versions of algorithm are alike, using TA is more computationally efficient than using DE. Furthermore, despite the permissibility of partial backlogging, it has never occurred in equilibrium points due to in-chain and chain-to-chain competition. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105922	10.1016/j.asoc.2019.105922													
J								Training data augmentation: An empirical study using generative adversarial net-based approach with normalizing flow models for materials informatics	APPLIED SOFT COMPUTING										Density estimation; Normalizing flows; Data augmentation; Generative adversarial nets; Materials informatics	NETWORKS; NOISE	We address the issue of small data size for training models for regression problems, which is a significant issue in materials science. Many density estimators that use generative models based on deep neural networks have been proposed. With generative models, normalizing flows can provide exact density estimations. Using normalizing flows, we address training data augmentation issue, where we use a real-valued non-volume preserving model (real-NVP) as the normalizing flow. A generative adversarial net (GAN)-based training method is applied to improve real-NVP training using real-NVP as the generator. Using kernel ridge regression trained by generated data, generalization performance was measured for evaluating the models. Experiments were conducted with seven benchmark datasets and a dataset of ionic conductivity of materials to compare the GAN-based real-NVP to state-of-the-art models, such as real-NVP and masked autoregressive flows. The experimental results demonstrated that the GAN-based real-NVP was comparable to state-of-the-art models and implied that the data sampled by the GAN-based real-NVP were available as new training data. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105932	10.1016/j.asoc.2019.105932													
J								Evolving granular feedback linearization: Design, analysis, and applications	APPLIED SOFT COMPUTING										Robust control; Evolving systems; Feedback linearization; Adaptive control	FUZZY; STABILIZATION; CONTROLLER	Exact feedback linearization is a method for nonlinear control which amounts to cancel the nonlinearities of a nonlinear system such that the resulting closed-loop dynamics is linear. The effectiveness of exact feedback linearization relies on a precise description of the system nonlinearities. This paper suggests a novel robust control approach for adaptive control of nonlinear systems called robust granular feedback linearization. The approach employs an instance of evolving the participatory learning algorithm to continuously estimate unknown nonlinearities and cancel their effects in the control loop. Under mild conditions, the robust granular feedback linearization is ensured to be Lyapunov stable by using convex methods. Simulation experiments with a surge tank is used to evaluate and to compare the performance of the robust granular feedback linearization against exact feedback linearization and an adaptive controller based on bacterial foraging. The results indicate that the robust granular feedback linearization outperforms both, the exact and the adaptive foraging controllers. The effectiveness of robust granular feedback linearization is further testified in an actual surge tank control system application. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105927	10.1016/j.asoc.2019.105927													
J								Enhanced whale optimization algorithm for maximum power point tracking of variable-speed wind generators	APPLIED SOFT COMPUTING										Benchmark functions; Optimization methods; Whale optimization algorithm; Wind generator	GREY WOLF OPTIMIZER; META-HEURISTIC OPTIMIZATION; SOLAR PHOTOVOLTAIC MODELS; SALP SWARM ALGORITHM; GLOBAL OPTIMIZATION; DIFFERENTIAL EVOLUTION; PARAMETERS ESTIMATION; STRATEGY; EXTRACTION; BEHAVIOR	This paper proposes an enhancement of the meta-heuristic whale optimization algorithm (WOA) for maximum power point tracking (MPPT) of variable-speed wind generators. First of all, twenty-three benchmark functions tested the enhanced whale optimization algorithm (EWOA). Then the statistical results of EWOA compared with the results of other algorithms (WOA, salp swarm algorithm (SSA), enhanced SSA (ESSA), grey wolf optimizer (GWO), augmented GWO (AGWO), and particle swarm optimization (PSO). Also, the non-parametric statistical test and convergence curves proved the superiority and the speed of the EWOA. After that, the EWOA and WOA are implemented to design optimal Takagi-Sugeno fuzzy logic controllers (FLCs) to enhance the MPPT control of variable-speed wind generators. Moreover, real wind speed data has confirmed the robustness of optimal EWOA-MPPT. In conclusion, the simulation results revealed that the EWOA is a promising algorithm to be applied for solving different engineering problems. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105937	10.1016/j.asoc.2019.105937													
J								A self-organized speciation based multi-objective particle swarm optimizer for multimodal multi-objective problems	APPLIED SOFT COMPUTING										Multimodal optimization; Multi-objective optimization; Particle swarm optimizer; Niching technique	GENETIC ALGORITHM; MODEL	This paper proposes a self-organized speciation based multi-objective particle swarm optimizer (SS-MOPSO) to locate multiple Pareto optimal solutions for solving multimodal multi-objective problems. In the proposed method, the speciation strategy is used to form stable niches and these niches/subpopulations are optimized to search and maintain Pareto-optimal solutions in parallel. Moreover, a self-organized mechanism is proposed to improve the efficiency of the species formulation as well as the performance of the algorithm. To maintain the diversity of the solutions in both the decision and objective spaces, SS-MOPSO is incorporated with the non-dominated sorting scheme and special crowding distance techniques. The performance of SS-MOPSO is compared with a number of the state-of-the-art multi-objective optimization algorithms on fourteen test problems. Moreover, the proposed SS-MOSPO is also employed to solve a real-life problem. The experimental results suggest that the proposed algorithm is able to solve the multimodal multi-objective problems effectively and shows superior performance by finding more and better distributed Pareto solutions. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105886	10.1016/j.asoc.2019.105886													
J								Optimizing an integrated inventory-routing system for multi-item joint replenishment and coordinated outbound delivery using differential evolution algorithm	APPLIED SOFT COMPUTING										Joint replenishment; Differential evolution algorithm; Coordinated outbound delivery; Independent delivery; Traveling salesman problem	MODEL; OPTIMIZATION; WAREHOUSE	A practical and new joint replenishment and delivery (JRD) problem that considers coordinated outbound delivery policy of multiple items (named JRCD) is studied. The proposed JRCD model aims to balance the joint replenishment, inventory holding, and delivery costs by deciding on the replenishment and outbound delivery schedule of each item. The indirect grouping policy is utilized in the JRCD problem, so items dispatched jointly in each outbound delivery are identified on the basis of the replenishment frequency and delivery frequency. Once the matching of items and retailers in each outbound delivery is confirmed, the optimal route can be subsequently obtained by solving the traveling salesman problem. To solve this complex optimization problem, an intelligent algorithm based on differential evolution is utilized because of its superior performance in handling similar complex problems. Basic and extended numerical examples are used to verify the effectiveness of the proposed algorithm. A comparison between the proposed JRCD and JRD with independent delivery is conducted with examples of varying cost parameters. Results provide interesting insights and useful guidelines for managers to create a reasonable policy for effectively controlling their total cost. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105863	10.1016/j.asoc.2019.105863													
J								Unconstrained texture classification using efficient jet texton learning	APPLIED SOFT COMPUTING										Derivative of Gaussian (DtG); Jet texton learning; Local jet vector (LJV); Texture classification	ROTATION-INVARIANT; PATTERN; SCALE; REPRESENTATION; RECOGNITION; DESCRIPTOR; FEATURES	This paper proposes a simple and effective texture recognition method that uses a new class of jet texton learning. In this approach, first a Jet space representation of the image is derived from a set of derivative of Gaussian (DtGs) filter responses upto 2nd order (R-6), so called local jet vector (LJV), which satisfies the scale space properties, where the combinations of local jets preserve the intrinsic local structure of the image in a hierarchical way and are invariant to image translation, rotation and scaling. Next, the jet textons dictionary is learned using K-means clustering algorithm from DtGs responses, followed by a contrast Weber law normalization pre-processing step. Finally, the feature distribution of jet texton is considered as a model which is utilized to classify texture using a non-parametric nearest regularized subspace (Nrs) classifier. Extensive experiments on three large and well-known benchmark database for texture classification like KTH-TIPS, Brodatz and CUReT show that the proposed method achieves state-of-the-art performance, especially when the number of available training samples is limited. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105910	10.1016/j.asoc.2019.105910													
J								Machine learning integrated credibilistic semi supervised clustering for categorical data	APPLIED SOFT COMPUTING										Categorical data; Credibilistic clustering; Friedman test; Fuzzy set; Machine learning; Possibilistic measure; Semi supervised clustering; Statistical significance	NONNEGATIVE MATRIX FACTORIZATION; K-MODES ALGORITHM; DATA SETS; SPARSE; SEARCH; TOOL	In real life, availability of correctly labeled data and handling of categorical data are often acknowledged as two major challenges in pattern analysis. Thus, clustering techniques are employed on unlabeled data to group them according to homogeneity. However, clustering techniques fail to make a decision while data are uncertain, ambiguous, vague, coincidental and overlapping in nature. Hence, in this case, the use of semi supervised technique can be useful. On the other hand, real life datasets are majorly categorical in nature, where natural ordering in attribute values is missing. This special property of categorical values with the inherent characteristics like uncertainty, ambiguity and vagueness makes clustering more complicated than numerical data. In recent times, credibilistic measure shows better performance over fuzzy and possibilistic measures while considering similar inherent characteristics in numerical data. Thus, these facts motivated us to propose a semi supervised clustering technique using credibilistic measure with the integration of machine learning techniques to address the above mentioned challenges of clustering categorical data. This semi supervised technique first clusters the dataset into K subsets with the proposed Credibilistic K-Mode, where credibilistic measure helps to determine the homogeneity by avoiding coincident clustering problem as well as finds the points those are certain to the clusters. Thereafter, in the second part of the semi supervised technique, clustered dataset is used to build a supervised model for classification of other unlabeled or uncertain data. This technique not only handles the unlabeled data better, but also yields improved results for uncertain or ambiguous data e.g, if the credibilistic measure is same for a data point in multiple classes. The results of the proposed technique are demonstrated quantitatively and visually in comparison with widely used state-of-the-art methods for eight synthetic and four real life datasets. Finally, statistical tests have been conducted to judge the statistical significance of the results produced by the proposed technique. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105871	10.1016/j.asoc.2019.105871													
J								Stockwell transform of time-series of fMRI data for diagnoses of attention deficit hyperactive disorder	APPLIED SOFT COMPUTING										Attention deficit hyperactivity disorder (ADHD); Fuzzy entropy; Stockwell transform (ST); Two-sample Kolmogorov-Smirnov (K-S) test; Support vector machine (SVM)	SEIZURE DETECTION; FEATURE-SELECTION; CONNECTIVITY; CLASSIFICATION; REPRESENTATION; NETWORKS; DISEASE	Attention deficit hyperactivity disorder (ADHD) is a common brain disorder among children. It presents various symptoms, hence, utilizing the information obtained from functional magnetic resonance imaging (fMRI) time-series data can be useful. Finding functional connections in typically developed control (TDC) and ADHD patients can be helpful in classification. The aim of this paper is to present a multifold method for the study of fMRI data to diagnose ADHD patients. In the proposed method, first, by applying the Stockwell transform (ST), we obtain detailed information about the time-series of the region of interests (ROIs) in the time and frequency domains. ST provides information about the variations of each ROI during the time. Thereafter, time-frequency domains are partitioned into sub-matrices and then, their fuzzy entropies are calculated as features. Next, discriminative features are chosen by using the two-sample Kolmogorov-Smirnov (K-S) test. Finally, the data are classified by the leave-one-out cross-validation (LOOCV) method using the support vector machine (SVM) classifier. To see the effectiveness of the proposed method, the experiments are performed on the ADHD-200 database. We consider different scenarios including classification of TDCs and ADHDs as well as classification of ADHD subtypes. We also assess the performance by considering the age and sex as phenotypic information. The proposed method gives good results in the classification procedure and identifying the connection paths between ROIs. The results indicate that the proposed method can distinguish ADHD disorder in a more accurate manner in comparison with other methods. The connectivity paths show that there is a reduction in the input of cerebellar regions and the left mid orbitofrontal cortex in ADHDs compared to TDCs. (C) 2019 Published by Elsevier B.V.																	1568-4946	1872-9681				JAN	2020	86								105905	10.1016/j.asoc.2019.105905													
J								Efficient optimization technique for multiple DG allocation in distribution networks	APPLIED SOFT COMPUTING										Distributed generations; Optimal allocation; Power loss minimization; Sine cosine algorithm; Chaos theory	DISTRIBUTION-SYSTEMS; DIFFERENTIAL EVOLUTION; OPTIMAL PLACEMENT; GENERATION ALLOCATION; POWER-SYSTEMS; ENERGY-LOSS; ALGORITHM; INTEGRATION; LOAD; LOCATION	In the last few decades, interest in the integration of Distributed Generators (DGs) into distribution networks has been increased due to their benefits such as enhance power system reliability, reduce the power losses and improve the voltage profile. These benefits can be increased by determining the optimal DGs allocation (location and size) into distribution networks. This paper proposes an efficient optimization technique to optimally allocate the multiple DG units in distribution networks. This technique is based on Sine Cosine Algorithm (SCA) and chaos map theory. As any random search-based optimization algorithm, SCA faces some issues such as low convergence rate and trapping in local solutions during the exploration and exploitation phases. This issue can be addressed by developing Chaotic SCA (CSCA). CSCA is mainly based on the iterative chaotic map which used to update the random parameters of SCA instead of using the random probability distribution. The iterative chaotic map is applied for single and multi-objective SCA. The proposed technique is validated using two stranded IEEE radial distribution feeders; 33 and 69-nodes. Comprehensive comparison among the proposed technique, the original SCA, and other competitive optimization techniques are carried out to prove the effectiveness of CSCA. Finally, a complete study is performed to address the impact of the intermittent nature of renewable energy resource on the distribution system. Hence, typical loads and generation (represented in PV power) profiles are applied. The result proves that the CSCA is more efficient to solve the optimal multiple DGs allocation with minimum power loss and high convergence rate. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105938	10.1016/j.asoc.2019.105938													
J								A decentralized Artificial Immune System for solution selection in Cyber-Physical Systems	APPLIED SOFT COMPUTING										Artificial Immune System; Immune Network; Decentralized and distributed system; Self-adaptive	MODEL	Centralization has become a de facto standard for implementing networked environments such as the Cyber-Physical Systems (CPS). Though easy to implement and control, centralized systems are difficult and expensive to scale in terms of the number of devices and the flow of information. This set of circumstances calls for a decentralized and distributed architecture for realizing such networked systems. However, due to the absence of global information in decentralized systems, one of the primary challenges is to find the best solution for problems distributed across the devices which are part of the CPS. Since the problems are distributed and no participating device has access to the full information, the devices may need to interact and share the information to select the best solution for a problem occurred. In this paper, we present a decentralized and distributed mechanism, which adapts to a stream of varying problems and continuously evolves and learns the best mappings between the problems and their associated solutions. The proposed approach integrates the concepts propounded in the three major Immune theories and can cater to real-world situations. The evolved mappings are shared across the physical network, thereby accelerating the search for the best set of solutions. In order to validate the performance of the proposed mechanism, we present the results obtained from solving a problem of sorting a stream of varying data in an emulated decentralized and distributed manner. To substantiate its working in real-world scenarios, we also describe the results obtained by embodying the system in real robots that discover the best path-following algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105920	10.1016/j.asoc.2019.105920													
J								Consolidation assessment using Multi Expression Programming	APPLIED SOFT COMPUTING										Geotechnical engineering; Evolutionary computation; Multi-Expression Programming; Consolidation; Prediction	FORMULATION; COEFFICIENT; CONCRETE; EQUATIONS; BEHAVIOR; STRESS; SOILS	In this study, new approximate solutions for consolidation have been developed in order to hasten the calculations. These solutions include two groups of equations, one can be used to calculate the average degree of consolidation and the other one for computing the time factor (inverse functions). Considering the complicated nature of consolidation, an evolutionary computation technique called Multi-Expression Programming was applied to generate several non-piecewise models which are accurate and straightforward enough for different purposes for calculating the degree of consolidation for each depth and its average as well for the whole soil layer. The parametric study was also performed to investigate the impact of each input parameter on the predicted consolidation degree of developed models for each depth. Moreover, the results of the consolidation test carried out on four different clays attained from the literature showed the proper performance of the proposed models. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105842	10.1016/j.asoc.2019.105842													
J								A multi-objective robust possibilistic model for technology portfolio optimization considering social impact and different types of financing	APPLIED SOFT COMPUTING										Technology portfolio selection; Financial resource allocation; Multi-objective robust possibilistic programming; Fuzzy programming; Augmented epsilon-constraint	DATA ENVELOPMENT ANALYSIS; SUPPLY CHAIN; PROGRAMMING APPROACH; PROJECT SELECTION; DESIGN; NETWORK	With respect to limited financial resources, prioritization of technology fields in order to be supported financially is a matter of paramount significance that governmental organizations, such as "Technology Development Funds (TDFs)", face with. Innovation and technology development, as the cornerstone of the economic development of countries, requires making decisions in terms of assigning the best-suited form of financial resources mainly by governments. Accordingly, this study addresses a multi-objective portfolio optimization problem in a multi-period setting with the aim of maximizing the created jobs - as a key factor in social welfare - as well as intended profit while minimizing the risk of inappropriate portfolio selection. To formulate the proposed mathematical model, different financing methods, technology readiness levels (TRL), and return on investment (ROI) associated with each technological project are taken into account. Afterward, to deal with the uncertainty arisen from fuzzy parameters, the Multi-Objective Robust Possibilistic Programming approach (MORPP) is applied, the performance of which is examined under several computational tests. Finally, to illustrate the performance of the proposed model and its applicability in practice, the computational results are shown through a real case study in Iran Innovation & Prosperity Fund (IIPF). The results show that selecting small and medium-sized enterprises (SMEs) for being financed, is the best option when increasing job creation is considered in portfolio optimization. Furthermore, the comparison of the MORPP model results with the deterministic model shows that the solutions obtained from the robust possibilistic approach outweighed the deterministic model. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105892	10.1016/j.asoc.2019.105892													
J								Decentralized adaptive neural network control of cascaded DC-DC converters with high voltage conversion ratio	APPLIED SOFT COMPUTING										Cascaded DC-DC converters; Decentralized control; Adaptive backstepping neural network control; Voltage control; Uncertainty and interaction estimator	SLIDING-MODE CONTROL; DISCONTINUOUS CONDUCTION MODES; BUCK/BOOST CONVERTER; CONTROL STRATEGY; STABILITY; SYSTEMS; DYNAMICS; LOADS; MOTOR; CCM	Decentralized output voltage tracking of cascaded DC-DC converters is an interesting topic to obtain a high voltage conversion ratio. The control purpose is challenging due to the load resistance changes, renewable energy supply voltage variations and interaction of the individual converters. In this paper, four novel decentralized adaptive neural network controllers are designed on the cascaded DC-DC buck and boost converters under load and DC supply voltage uncertainties. In the beginning, individual buck and boost converter average models that can operate in both continuous and discontinuous conduction modes are derived. Then, the interconnected and decentralized state-space models of cascaded buck and boost converters are extracted. These models are highly nonlinear with unknown uncertainties which can be estimated by neural networks. Further, two decentralized adaptive backstepping neural network voltage controllers are proposed on cascaded buck converters to deal with uncertainties and interactions. However, these control strategies are not applicable to a boost converter due to its non-minimum phase nature. Then, two novel decentralized adaptive neural network with a conventional proportional-integral reference current generator are developed on the cascaded boost converters. Practical stability of the overall system is guaranteed for the proposed controllers using Lyapunov stability theorem. Finally, four control strategies provide good quality of output voltage in the presence of uncertainties and interactions. Comparative simulations are carried out on cascaded buck and boost converters to validate the effectiveness and performance of the designed methods. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105878	10.1016/j.asoc.2019.105878													
J								Optimizing supply chain network for perishable products using improved bacteria foraging algorithm	APPLIED SOFT COMPUTING										Perishable product; Supply chain management; Meta-heuristic algorithm	ROUTING PROBLEM; HEURISTIC ALGORITHM; GENETIC ALGORITHM; INVENTORY MODEL; FLEET SIZE; UNCERTAINTY	In a supply chain environment, time delay has a significant impact on the success of perishable products. A major concern is therefore aimed at development of a holistic optimized approach in a supply chain environment for perishable products. Thus, integration of production, inventory and, distribution of perishable products in a supply chain environment are the challenging tasks for practitioners and researchers. In general, the standard optimal supply chain model cannot work for perishable products. There is therefore, a need for a holistic model that focuses on the consolidation of the processes. Shorter product shelf-life, temperature control, requirement of strict tractability, large number of product variants, and a large volume of goods handled are the major challenges in a supply chain environment for perishable products. The present work focuses on the development of a holistic model which uses improved bacteria forging algorithm (IBFA) for solving the formulated model. We have proposed and analyzed some general properties of the model and, finally applied it to a three-stage supply chain problem using an IBFA. Two case studies have been considered for support and demonstration of the integrated perishable supply chain network problem. Results obtained from IBFA reveal that the proposed model is more useful for decision makers while considering optimal supply chain network for perishable products. Finally, validation of results has been carried out using bacteria forging algorithm (BFA). The computational performance of the proposed algorithm proves that IBFA is instrumental in effectively handling the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105921	10.1016/j.asoc.2019.105921													
J								Human factors risk assessment: An integrated method for improving safety in clinical use of medical devices	APPLIED SOFT COMPUTING										Medical device; Risk assessment; "swiss cheese'' model; SHEL model; Failure mode and effects analysis (FMEA)	CARE FAILURE MODE; EXTENDED MULTIMOORA; REASONING APPROACH; HUMAN ERROR; FMEA; PRIORITIZATION; FRAMEWORK	Medical devices play a critical role in care and treatment. The human-related failures can significantly affect the safety of patients in clinical use of medical devices. This study develops a comprehensive risk assessment model for identification and evaluation of failures which may occur in the clinical use of medical devices. First, the "Swiss cheese'' model and SHEL model (the acronym of software, hardware, environment, and liveware) are integrated to comprehensively identify the potential human errors. Then, a new failure mode and effects analysis (FMEA) approach improved by rough set theory and grey relational analysis is developed to assess the risk of the identified failures. The proposed method integrates the strengths of the "Swiss cheese'' and SHEL model in identifying human failures from both the vertical and horizontal perspectives of the system, and the advantages of the improved FMEA approach in flexibly manipulating vague information in risk evaluation without much priori information. Finally, the proposed method is applied in clinical use of respirator to verify its efficiency and effectiveness. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105918	10.1016/j.asoc.2019.105918													
J								Optimal parameter tuning of Modified Active Disturbance Rejection Control for unstable time-delay systems using an AHP combined Multi-Objective Quasi-Oppositional Jaya Algorithm	APPLIED SOFT COMPUTING										Active Disturbance Rejection Control; Multi-objective optimization; Unstable time-delay systems; Jaya Algorithm	2-DEGREE-OF-FREEDOM CONTROL SCHEME; DESIGN; RULES	Active Disturbance Rejection Control (ADRC) is an innovative control paradigm which emerged as a viable alternative to the traditional control design methods. However, the applicability of ADRC is limited to stable minimum phase systems. This paper investigates the problem of handling unstable time-delay systems under ADRC framework. To this end, an optimally tuned modified Active Disturbance Rejection Control (MADRC) scheme is proposed. The scheme consists of a model-assisted extended state observer designed using the known model information and a state feedback control law. The tuning of MADRC scheme is formulated as a multi-objective optimization problem with tracking and disturbance rejection performances as the objectives to be met simultaneously. Solution for this multi-objective optimization problem is carried out in two stages. The first stage is meant for generating a set of Pareto-optimal solutions with the help of Multi-Objective Quasi Oppositional Jaya Algorithm (MOQO-Jaya), while the second stage is for selection of the best among the available alternatives using Analytical Hierarchy Process (AHP). Simulation studies conducted on different unstable systems with time-delay illustrates the efficacy of the proposed scheme. Further, tracking and disturbance rejection performances are also assessed in the presence of nominal and perturbed conditions. Finally, the proposed method is extended to a Two-Input and Two-Output (TITO) process. Stability study is carried out by translating the proposed structure into a standard two degree of freedom framework. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105881	10.1016/j.asoc.2019.105881													
J								A gradient boosting decision tree based GPS signal reception classification algorithm	APPLIED SOFT COMPUTING										GPS; GBDT; Urban canyon; Multipath; NLOS		In urban areas, GPS signals are often reflected or blocked by buildings, which causes multipath effects and non-line-of-sight (NLOS) reception respectively consequently degrading GPS positioning performance. While improved receiver design can reduce the effect of multipath to some extent, it cannot deal with NLOS. Modelling methods based on measurements have shown promise to reduce the effect of NLOS signal reception. However, this depends on their ability to accurately and reliably classify line-of-sight (LOS), multipath and NLOS signals. The traditional method is based on one feature using signal strength as measured by the carrier to noise ratio, C/N-0. However, this feature is ineffective in capturing the characteristics of multipath and NLOS in all environments. In this paper, to improve the accuracy of signal reception classification, we are using the three features of C/N-0, pseudorange residuals and satellite elevation angle with a gradient boosting decision tree (GBDT) based classification algorithm. Experiments are carried out to compare the proposed algorithm with classifiers based on decision tree, distance weighted k-nearest neighbour (KNN) and the adaptive network-based fuzzy inference system (ANFIS). Test results from static receivers in urban environments, show that the GBDT based algorithm achieves a classification accuracy of 100%, 82% and 86% for LOS, multipath and NLOS signals, respectively. This is superior to the other three algorithms with the corresponding results of 100%, 82% and 84% for the Distance-Weighted KNN, 99%, 70% and 65% for the ANFIS and 98%, 35% and 95% for the traditional decision tree. With the NLOS detection and exclusion, the proposed GBDT with multi-feature based method can provide a positioning accuracy improvement of 34.1% compared to the traditional C/N-0 based method. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105942	10.1016/j.asoc.2019.105942													
J								Value-added tax fraud detection with scalable anomaly detection techniques	APPLIED SOFT COMPUTING										Unsupervised anomaly detection; Tax fraud detection; Scalable algorithms	OUTLIER DETECTION; CLASSIFICATION; PERFORMANCE	The tax fraud detection domain is characterized by very few labelled data (known fraud/legal cases) that are not representative for the population due to sample selection bias. We use unsupervised anomaly detection (AD) techniques, which are uncommon in tax fraud detection research, to deal with these domain issues. We analyse a unique dataset containing the VAT declarations and client listings of all Belgian VAT numbers pertaining to ten sectors. Our methodology consists in applying AD methods to firms belonging to the same sector and enables an efficient auditing strategy that can be adopted by tax authorities worldwide. The high lifts and hit rates observed in most sectors demonstrate the success of this approach. Sectoral differences exist due to varying market conditions and legal requirements across sectors and we show that the optimal AD method is sector dependent. We focus on three methodological problems that show issues in the related literature. (1) Can we design suitable input features? We develop new fraud indicators from specific fields of the VAT form and client listings and show the predictive value of the combination of these features. (2) Can we design fast algorithms to deal with the large data sizes that can occur in the tax domain? New methods are developed and we demonstrate their scalability both theoretically as well as empirically. (3) How should fraud detection performance be assessed? A new evaluation methodology is proposed that provides reliable performance indications and guarantees that fraud cases are effectively detected by the proposed methods. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105895	10.1016/j.asoc.2019.105895													
J								An angle based evolutionary algorithm with infeasibility information for constrained many-objective optimization	APPLIED SOFT COMPUTING										Constrained many-objective optimization; Infeasibility information; Constrained dominance relation; Convergence; Diversity	NONDOMINATED SORTING APPROACH; DECOMPOSITION; SEARCH	Recently, angle-based approaches have shown promising for unconstrained many-objective optimization problems (MaOPs), but few of them are extended to solve constrained MaOPs (CMaOPs). Moreover, due to the difficulty in searching for feasible solutions in high-dimensional objective space, the use of infeasible solutions comes to be more important in solving CMaOPs. In this paper, an angle based evolutionary algorithm with infeasibility information is proposed for constrained many-objective optimization, where different kinds of infeasible solutions are utilized in environmental selection and mating selection. To be specific, an angle-based constrained dominance relation is proposed for non-dominated sorting, which gives infeasible solutions with good diversity the same priority to feasible solutions for escaping from the locally feasible regions. As for diversity maintenance, an angle-based density estimation is developed to give the infeasible solutions with good convergence a chance to survive for next generation, which is helpful to get across the large infeasible barrier. In addition, in order to utilize the potential of infeasible solutions in creating high-quality offspring, a modified mating selection is designed by considering the convergence, diversity and feasibility of solutions simultaneously. Experimental results on two constrained many-objective optimization test suites demonstrate the competitiveness of the proposed algorithm in comparison with five existing constrained many-objective evolutionary algorithms for CMaOPs. Moreover, the effectiveness of the proposed algorithm on a real-world problem is showcased. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105911	10.1016/j.asoc.2019.105911													
J								Structural block driven enhanced convolutional neural representation for relation extraction	APPLIED SOFT COMPUTING										Relation extraction; Deep learning; CNNs; Dependency parsing	ANOMALY DETECTION	In this paper, we propose a novel lightweight relation extraction approach of structural block driven convolutional neural learning. Specifically, we detect the essential sequential tokens associated with entities through dependency analysis, named as a structural block, and only encode the block on a block-wise and an inter-block-wise representation, utilizing multi-scale Convolutional Neural Networks (CNNs). This is to (1) eliminate the noisy from irrelevant part of a sentence; meanwhile (2) enhance the relevant block representation with both block-wise and inter-block-wise semantically enriched representation. Our method has the advantage of being independent of long sentence context since we only encode the sequential tokens within a block boundary. Experiments on two datasets i.e., SemEval2010 and KBP37, demonstrate the significant advantages of our method. In particular, we achieve the new state-of-the-art performance on the KBP37 dataset; and comparable performance with the state-of-the-art on the SemEval2010 dataset. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105913	10.1016/j.asoc.2019.105913													
J								Advanced backtracking search optimization algorithm for a new joint replenishment problem under trade credit with grouping constraint	APPLIED SOFT COMPUTING										Joint replenishment problem; Trade credit; Grouping constraint; Backtracking search optimization algorithm	DIFFERENTIAL EVOLUTION ALGORITHM; ECHO STATE NETWORK; PERMISSIBLE DELAY; EOQ MODEL; QUANTITY; INVENTORY; HEURISTICS; RESOURCE; MEMORY; PRICE	In the real business situation, suppliers usually provide retailers with forward financing to decrease inventory or increase demand. Moreover, some heterogeneous goods are not allowed to transport together, or a penalty cost is incurred when heterogeneous goods are transported at the same time. This research proposes a practical multi-item joint replenishment problem (JRP) by considering trade credit and grouping constraint in accordance with the practical situation. The JRP aims to find reasonable item replenishment frequencies and each group's basic replenishment cycle time so that the overall cost can be minimized. Four intelligent algorithms, which include an advanced backtracking search optimization algorithm (ABSA), genetic algorithm (GA), differential evolution (DE) and backtracking search optimization algorithm (BSA), are provided to solve this problem. Findings of contrastive example verify that ABSA is superior to GA, DE, and BSA, which have been validated to be effective algorithms. Randomly generated problems are used to test the performance of ABSA. Results indicate ABSA is more effective and stable to resolve the proposed JRP than the other algorithms. ABSA is a good solution for the proposed JRP with heterogeneous items under trade credits. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105953	10.1016/j.asoc.2019.105953													
J								An improved random forest-based rule extraction method for breast cancer diagnosis	APPLIED SOFT COMPUTING										Breast cancer diagnosis; Rule extraction; Random forest; Interpretability; MOEAs	MULTIOBJECTIVE GENETIC OPTIMIZATION; SUPPORT VECTOR MACHINES; DECISION TREES; FEATURE-SELECTION; ENSEMBLE; CLASSIFICATION; MODEL; ALGORITHM; SURVIVABILITY; STATISTICS	Breast cancer has been becoming the main cause of death in women all around the world. An accurate and interpretable method is necessary for diagnosing patients with breast cancer for well-performed treatment. Nowadays, a great many of ensemble methods have been widely applied to breast cancer diagnosis, capable of achieving high accuracy, such as Random Forest. However, they are black-box methods which are unable to explain the reasons behind the diagnosis. To surmount this limitation, a rule extraction method named improved Random Forest (RF)-based rule extraction (IRFRE) method is developed to derive accurate and interpretable classification rules from a decision tree ensemble for breast cancer diagnosis. Firstly, numbers of decision tree models are constructed using Random Forest to generate abundant decision rules available. And then a rule extraction approach is devised to detach decision rules from the trained trees. Finally, an improved multi-objective evolutionary algorithm (MOEA) is employed to seek for an optimal rule predictor where the constituent rule set is the best trade-off between accuracy and interpretability. The developed method is evaluated on three breast cancer data sets, i.e., the Wisconsin Diagnostic Breast Cancer (WDBC) dataset, Wisconsin Original Breast Cancer (WOBC) dataset, and Surveillance, Epidemiology and End Results (SEER) breast cancer dataset. The experimental results demonstrate that the developed method can primely explain the black-box methods and outperform several popular single algorithms, ensemble learning methods, and rule extraction methods from the view of accuracy and interpretability. What is more, the proposed method can be popularized to other cancer diagnoses in practice, which provides an option to a more interpretable, more accurate cancer diagnosis process. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105941	10.1016/j.asoc.2019.105941													
J								Assessment of traffic congestion with ORESTE method under double hierarchy hesitant fuzzy linguistic environment	APPLIED SOFT COMPUTING										Double hierarchy hesitant fuzzy linguistic term sets; Double hierarchy hesitant fuzzy linguistic; ORESTE method; Score function; Traffic congestion	GROUP DECISION-MAKING; TERM SETS; PREFERENCE RELATIONS; MULTIMOORA; CRITERIA; MODEL	With the new generation of information technology development and the promotion of the Internet, local governments turn their attention to the construction of intelligent transportation systems. More and more cities began building intelligent transportation which has been widely used to monitor urban traffic. Experts can evaluate urban traffic congestion based on the information collected from the big data of intelligent transportation. In recent two years, double hierarchy hesitant fuzzy linguistic term set has been widely used to depict explicit evaluation information, which is straightforward and broad-spectrum. When evaluating traffic congestion in a city, decision makers can utilize double hierarchy hesitant fuzzy linguistic term sets to express vague information. Moreover, the ORESTE method is an applicative method which can select a reliable alternative by subdividing alternatives and reduce the loss of information in the conversion process. In this paper, we propose a double hierarchy hesitant fuzzy linguistic ORESTE method and a new score function of double hierarchy hesitant fuzzy linguistic term set. The method raises a new perspective to reduce the error from other methods and the new score function derives a robust decision-making result. Then, we apply the double hierarchy hesitant fuzzy linguistic ORESTE method to solve a practical case involving choosing the congested city by evaluating the 5S traffic congestion model. Finally, we compare the double hierarchy hesitant fuzzy linguistic ORESTE method with other methods such as the classical ORESTE method and the double hierarchy hesitant fuzzy linguistic MULTIMOORA to illustrate the advantages of our method. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105864	10.1016/j.asoc.2019.105864													
J								Adaptive entropy weighted picture fuzzy clustering algorithm with spatial information for image segmentation	APPLIED SOFT COMPUTING										Image segmentation; Picture fuzzy clustering; Spatial information; Weight entropy	LOCAL INFORMATION; FCM	Image segmentation has been broadly applied in computer vision and image analysis. However, many segmentation methods suffer from limited accuracy for noisy images. To improve the robustness of the existing picture fuzzy clustering and solve the problem of selecting spatial constraint parameter, a novel picture fuzzy clustering is proposed. Firstly, a novel symmetric regularizing term is constructed to solve the time-consuming problem of existing picture fuzzy clustering, and the corresponding fuzzy clustering is proposed. Secondly, considering the correlation between current pixel and its neighboring pixels, the objective function is modified by adaptive weighting fusion of local mean information, and the maximum weight entropy constraint is embedded into it to solve the difficulty of parameter selection. Finally, the local spatial information constraint item of the current pixel is constructed by using its neighboring picture fuzzy partition information and is utilized to modify the picture fuzzy partition information of current pixel to correct the clustering center. Results show the proposed algorithm has some potential advantages in segmentation accuracy and anti-noise robustness. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105888	10.1016/j.asoc.2019.105888													
J								Algorithm for improving additive consistency of linguistic preference relations with an integer optimization model	APPLIED SOFT COMPUTING										Group decision making; Linguistic preference relation; Additive consistency; Integer optimization	GROUP DECISION-MAKING; CONSENSUS MODEL; INDIVIDUAL CONSISTENCY; INCONSISTENCIES; COMPATIBILITY; IMPROVEMENT; OPERATORS; SETS	Linguistic preference relation (LPR) composed by linguistic terms can well express decision makers' (DMs') qualitative preference opinion by comparing alternatives with each other. The investigation of its consistency becomes an important issue to guarantee the rationality of the decision making solutions. Therefore, it is significant to investigate the consistency measure and the consistency improving approach for LPRs. In this paper we present a new method for group decision making (GDM) with LPRs. First, an additive consistency index is introduced on the basis of the information of the original LPR to check whether a LPR is acceptably additive consistency. For unacceptably additively consistent LPR, an integer optimization model is further developed to obtain the acceptably additively consistent LPR. Moreover, the optimization model can guarantee the integrity of the information of the LPR with acceptably additive consistency. Then, with respect to GDM with LPRs, an entropy weight method is proposed to determine the weights of DMs. Finally, the proposed methods are implemented in two numerical examples including a GDM problem. Meanwhile, the comparative analysis with existing methods are discussed in detail to demonstrate the validity of the proposed methods. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105955	10.1016/j.asoc.2019.105955													
J								Search-based procedural content generation for GVG-LG	APPLIED SOFT COMPUTING										Genetic algorithm; Procedural content generation; Search-based procedural content generation; General video game level generation		Search-based Procedural Content Generation has been proven an efficient technique for the generation of different and diverse types of content. In this article, we generate general levels for 2D games using the FI2POP genetic algorithm. Generating entertaining levels is subjective. Therefore in this work, we focus on the aesthetics and difficulty of a level. For experimentation purposes, we generated levels for five different games in the General Video Game Level Generation track. Our results indicate that the generated levels are symmetrical, balanced, dense and reachable. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105909	10.1016/j.asoc.2019.105909													
J								A decomposition-based many-objective artificial bee colony algorithm with reinforcement learning	APPLIED SOFT COMPUTING										Swarm intelligence; Artificial bee colony; Many-objective optimization; Reinforcement learning; Decomposition strategy	DYNAMIC OPTIMIZATION; EVOLUTIONARY ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; FIREFLY ALGORITHM; INTELLIGENCE; FRAMEWORK; STRATEGY	When optimizing many-objective optimization problems (MaOPs), the optimization effect is normally related to the problem types. Therefore, enhancing the generalization ability is essential to the application of the algorithms. In this paper, a novel decomposition-based Artificial bee colony algorithm (ABC) for MaOP optimization, MaOABC/D-LA, is presented to enhance the generalization ability. A reinforcement learning-based searching strategy is designed in the MaOABC/D-LA, with which the algorithm adjusts its searching actions according to their performance. And a variant of the onlooker bee mechanism is proposed to balance the optimization quality. To investigate performance of the proposed algorithm, a comparison experiment is conducted. The experimental results show that the MaOABC/D-LA outperforms the peer algorithms in efficiency and solution quality for MaOPs with different types of features. This indicates the proposed method has a definite effect on improving generalization ability. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105879	10.1016/j.asoc.2019.105879													
J								Electrocardiogram soft computing using hybrid deep learning CNN-ELM	APPLIED SOFT COMPUTING										Electrocardiogram (ECG) signals; MIT-BIH dataset; Extreme learning machine; Classification	ECG BEAT CLASSIFICATION; NEURAL-NETWORK; ENHANCEMENT; COMPRESSION; ALGORITHM; PCA	Electrocardiogram (ECG) can reflect the state of human heart and is widely used in clinical cardiac examination. However, the electrocardiogram signal is very weak, the anti-interference ability is poor, easy to be affected by the noise. Doctors face difficulties in diagnosing arrhythmias. Therefore, automatic recognition and classification of ECG signals is an important and indispensable task. Since the beginning of the 21 st century, deep learning has developed rapidly and has shown the most advanced performance in various fields. This paper presents a method of combining (Convolutional neural network) CNN and ELM (extreme learning machine). The accuracy rate is 97.50%. Compared with the state-of-the-art methods, this method improves the accuracy of ECG automatic classification and has good generalization ability. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JAN	2020	86								105778	10.1016/j.asoc.2019.105778													
J								Toward Training Recurrent Neural Networks for Lifelong Learning	NEURAL COMPUTATION												Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.																	0899-7667	1530-888X				JAN	2020	32	1					1	35		10.1162/neco_a_01246													
J								A Continuous-Time Analysis of Distributed Stochastic Gradient	NEURAL COMPUTATION											CONTRACTION ANALYSIS; COMPUTATION; DESCENT	We analyze the effect of synchronization on distributed stochastic gradient algorithms. By exploiting an analogy with dynamical models of biological quorum sensing, where synchronization between agents is induced through communication with a common signal, we quantify how synchronization can significantly reduce the magnitude of the noise felt by the individual distributed agents and their spatial mean. This noise reduction is in turn associated with a reduction in the smoothing of the loss function imposed by the stochastic gradient approximation. Through simulations on model nonconvex objectives, we demonstrate that coupling can stabilize higher noise levels and improve convergence. We provide a convergence analysis for strongly convex functions by deriving a bound on the expected deviation of the spatial mean of the agents from the global minimizer for an algorithm based on quorum sensing, the same algorithm with momentum, and the elastic averaging SGD (EASGD) algorithm. We discuss extensions to new algorithms that allow each agent to broadcast its current measure of success and shape the collective computation accordingly. We supplement our theoretical analysis with numerical experiments on convolutional neural networks trained on the CIFAR-10 data set, where we note a surprising regularizing property of EASGD even when applied to the non-distributed case. This observation suggests alternative second-order in time algorithms for nondistributed optimization that are competitive with momentum methods.																	0899-7667	1530-888X				JAN	2020	32	1					36	96		10.1162/neco_a_01248													
J								On Kernel Method-Based Connectionist Models and Supervised Deep Learning Without Backpropagation	NEURAL COMPUTATION											REPRESENTATIONS	We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to "kernelize" (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an l-layer feedforward network for classification, where l >= 2 can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.																	0899-7667	1530-888X				JAN	2020	32	1					97	135		10.1162/neco_a_01250													
J								Storing Object-Dependent Sparse Codes in a Willshaw Associative Network	NEURAL COMPUTATION											INFORMATION-STORAGE; MEMORY; CAPACITY	Willshaw networks are single-layered neural networks that store associations between binary vectors. Using only binary weights, these networks can be implemented efficiently to store large numbers of patterns and allow for fault-tolerant recovery of those patterns from noisy cues. However, this is only the case when the involved codes are sparse and randomly generated. In this letter, we use a recently proposed approach that maps visual patterns into informative binary features. By doing so, we manage to transform MNIST handwritten digits into well-distributed codes that we then store in a Willshaw network in autoassociation. We perform experiments with both noisy and noiseless cues and verify a tenuous impact on the recovered pattern's relevant information. More specifically, we were able to perform retrieval after filling the memory to several factors of its number of units while preserving the information of the class to which the pattern belongs.																	0899-7667	1530-888X				JAN	2020	32	1					136	152		10.1162/neco_a_01243													
J								A Robust Model of Gated Working Memory	NEURAL COMPUTATION											SPIKING NETWORK MODEL; PREFRONTAL CORTEX; SYNAPTIC MECHANISMS; NEURAL-NETWORK; DYNAMICS; REPRESENTATION; INFORMATION; COMPUTATION; SELECTION; TIME	Gated working memory is defined as the capacity of holding arbitrary information at any time in order to be used at a later time. Based on electrophysiological recordings, several computational models have tackled the problem using dedicated and explicit mechanisms. We propose instead to consider an implicit mechanism based on a random recurrent neural network. We introduce a robust yet simple reservoir model of gated working memory with instantaneous updates. The model is able to store an arbitrary real value at random time over an extended period of time. The dynamics of the model is a line attractor that learns to exploit reentry and a nonlinearity during the training phase using only a few representative values. A deeper study of the model shows that there is actually a large range of hyperparameters for which the results hold (e.g., number of neurons, sparsity, global weight scaling) such that any large enough population, mixing excitatory and inhibitory neurons, can quickly learn to realize such gated working memory. In a nutshell, with a minimal set of hypotheses, we show that we can have a robust model of working memory. This suggests this property could be an implicit property of any random population, that can be acquired through learning. Furthermore, considering working memory to be a physically open but functionally closed system, we give account on some counterintuitive electrophysiological recordings.																	0899-7667	1530-888X				JAN	2020	32	1					153	181		10.1162/neco_a_01249													
J								An FPGA Implementation of Deep Spiking Neural Networks for Low-Power and Fast Classification	NEURAL COMPUTATION											CATEGORIZATION; PROCESSOR; NEURONS	A spiking neural network (SNN) is a type of biological plausibility model that performs information processing based on spikes. Training a deep SNN effectively is challenging due to the nondifferention of spike signals. Recent advances have shown that high-performance SNNs can be obtained by converting convolutional neural networks (CNNs). However, the large-scale SNNs are poorly served by conventional architectures due to the dynamic nature of spiking neurons. In this letter, we propose a hardware architecture to enable efficient implementation of SNNs. All layers in the network are mapped on one chip so that the computation of different time steps can be done in parallel to reduce latency. We propose new spiking max-pooling method to reduce computation complexity. In addition, we apply approaches based on shift register and coarsely grained parallels to accelerate convolution operation. We also investigate the effect of different encoding methods on SNN accuracy. Finally, we validate the hardware architecture on the Xilinx Zynq ZCU102. The experimental results on the MNIST data set show that it can achieve an accuracy of 98.94% with eight-bit quantized weights. Furthermore, it achieves 164 frames per second (FPS) under 150 MHz clock frequency and obtains 41x speed-up compared to CPU implementation and 22 times lower power than GPU implementation.																	0899-7667	1530-888X				JAN	2020	32	1					182	204		10.1162/neco_a_01245													
J								Iterative Retrieval and Block Coding in Autoassociative and Heteroassociative Memory	NEURAL COMPUTATION											NEURAL ASSOCIATIVE MEMORY; FAMILIARITY DISCRIMINATION; BIDIRECTIONAL RETRIEVAL; STORAGE CAPACITY; NETWORKS; MODELS; DYNAMICS	Neural associative memories (NAM) are perceptron-like single-layer networks with fast synaptic learning typically storing discrete associations between pairs of neural activity patterns. Gripon and Berrou (2011) investigated NAM employing block coding, a particular sparse coding method, and reported a significant increase in storage capacity. Here we verify and extend their results for both heteroassociative and recurrent autoassociative networks. For this we provide a new analysis of iterative retrieval in finite autoassociative and heteroassociative networks that allows estimating storage capacity for random and block patterns. Furthermore, we have implemented various retrieval algorithms for block coding and compared them in simulations to our theoretical results and previous simulation data. In good agreement of theory and experiments, we find that finite networks employing block coding can store significantly more memory patterns. However, due to the reduced information per block pattern, it is not possible to significantly increase stored information per synapse. Asymptotically, the information retrieval capacity converges to the known limits C=ln2 approximate to 0.69 and C=(ln2)/4 approximate to 0.17 also for block coding. We have also implemented very large recurrent networks up to n=2 center dot 106 neurons, showing that maximal capacity C approximate to 0.2 bit per synapse occurs for finite networks having a size n approximate to 105 similar to cortical macrocolumns.																	0899-7667	1530-888X				JAN	2020	32	1					205	260		10.1162/neco_a_01247													
J								Optimal Sampling of Parametric Families: Implications for Machine Learning	NEURAL COMPUTATION											INFORMATION; PREDICTORS; THEOREM	It is well known in machine learning that models trained on a training set generated by a probability distribution function perform far worse on test sets generated by a different probability distribution function. In the limit, it is feasible that a continuum of probability distribution functions might have generated the observed test set data; a desirable property of a learned model in that case is its ability to describe most of the probability distribution functions from the continuum equally well. This requirement naturally leads to sampling methods from the continuum of probability distribution functions that lead to the construction of optimal training sets. We study the sequential prediction of Ornstein-Uhlenbeck processes that form a parametric family. We find empirically that a simple deep network trained on optimally constructed training sets using the methods described in this letter can be robust to changes in the test set distribution.																	0899-7667	1530-888X				JAN	2020	32	1					261	279		10.1162/neco_a_01251													
J								Multivariate Gaussian and Student-t process regression for multi-output prediction	NEURAL COMPUTING & APPLICATIONS										Multivariate Gaussian process; Multivariate Student-t process; Gaussian process regression; Student-t process regression; Multi-output prediction; Stock investment strategy; Industrial sector; Time series prediction		Gaussian process model for vector-valued function has been shown to be useful for multi-output prediction. The existing method for this model is to reformulate the matrix-variate Gaussian distribution as a multivariate normal distribution. Although it is effective in many cases, reformulation is not always workable and is difficult to apply to other distributions because not all matrix-variate distributions can be transformed to respective multivariate distributions, such as the case for matrix-variate Student-t distribution. In this paper, we propose a unified framework which is used not only to introduce a novel multivariate Student-t process regression model (MV-TPR) for multi-output prediction, but also to reformulate the multivariate Gaussian process regression (MV-GPR) that overcomes some limitations of the existing methods. Both MV-GPR and MV-TPR have closed-form expressions for the marginal likelihoods and predictive distributions under this unified framework and thus can adopt the same optimization approaches as used in the conventional GPR. The usefulness of the proposed methods is illustrated through several simulated and real-data examples. In particular, we verify empirically that MV-TPR has superiority for the datasets considered, including air quality prediction and bike rent prediction. At last, the proposed methods are shown to produce profitable investment strategies in the stock markets.																	0941-0643	1433-3058				APR	2020	32	8					3005	3028		10.1007/s00521-019-04687-8		DEC 2019											
J								A Combined Approach for the Binarization of Historical Tibetan Document Images	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Historical Tibetan documents; image binarization; background extraction of color images; channels combination		It is common that historical Tibetan documents belonging to historical collections are poorly preserved and are prone to degradation processes. This causes many challenges that can be addressed by image binarization, the most common of which is stains. A lack of uniform standard datasets makes it difficult to evaluate binarization effects. Motivated by the poor effects and difficulty of evaluating the binarization of historical Tibetan document images, a combined approach is proposed that aims to improve overall performance. The method includes the following parts: first, image generation through standard binarization and the background extraction of color images, which are both used for image synthesis in preparing for an evaluation. Then, preliminary binarization processing is implemented through channel combination in the lab color space and through local binarization. The synthetic images are used to select the coefficient when the channels are combined. Furthermore, Local Binary Pattern (hereafter LBP) and image smoothing is carried out after the combination of the channels to obtain the outline of the text area. Finally, the final binarization image is obtained by combining the preliminary binarization image and the text area contour image. Our method achieved top performance compared to other methods after a large number of synthetic image tests with a variety of background types.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954038	10.1142/S0218001419540387													
J								A Hybrid Model Based on Logistic Regression Algorithm and Extraction Algorithm Using Reward Extremum to Real-Time Detect Blade Icing Alarm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Logistic regression algorithm; blade icing alarm; real-time detect; extraction algorithm using reward extremism; machine learning	WIND TURBINE-BLADES; OBSERVABILITY	Generally, the blades of wind turbine installed in cold regions often encounter the situation of icing on their surface in winter, which affects the performance of wind turbine greatly and reduces power generation. Blade icing of wind turbine is a typical alarm related to multivariates, such as environment temperature, humidity, blade rotation and so on. To successfully achieve a supervised classification, it is necessary to extract the correlated features with blade icing. This paper proposes an extraction algorithm using reward extremism which can extract a group of variables correlated to blade icing from many monitoring variables of wind turbine, and they affect each other. This algorithm, using reward function which can measure the reward or value of alarm problem, can effectively distinguish the variables correlated to blade icing from the uncorrelated ones without knowing the mean of variables and the relations between them. Whether the blade of wind turbine is iced or not is a binary logic, so logistic regression algorithm is better choice to detect blade icing. Hence, this paper proposes a hybrid model using logistic regression algorithm and extraction algorithm using reward extremism, which can be built by machine learning with historic monitoring data. This model can real-time detect blade icing by real-time monitoring data. A correlation variable set can be obtained by the application of extraction algorithm using reward extremism on the historic monitoring data; and then the weights of the correlation variables in the set can be gained from the historic data by using logistic regression algorithm, so a blade icing detection pattern can be found using logistic regression algorithm; finally, whether the blade of wind turbine is iced in delay time or not can be real-time detected with the help of the hybrid model by real-time monitoring data. It is proved by wind turbine data that this hybrid model can work well not only to get correlation variable set but also to improve the generalization performance of logistic regression and achieve better prediction results. The performance of the model on blade icing alarm of wind turbine provides a new way to find out multivariate correlation to alarm from the mass historic monitoring data.																	0218-0014	1793-6381				DEC 31	2019	33	14							1955016	10.1142/S0218001419550164													
J								A Steganography Embedding Method Based on Hamming Coding and Histogram-Preserving	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Steganography; Hamming coding; histogram preserving; relative entropy	IMAGE STEGANOGRAPHY; CODE; CAPACITY; SYSTEMS	This paper proposes an information hiding algorithm using matrix embedding with Hamming codes and histogram preservation in order to keep the histogram of the image unchanged before and after hiding information in digital media. First, the algorithm uses matrix embedding with Hamming codes to determine the rewriting bits of the original image, rewrite and flip them, and successfully embed the secret information. Then, according to the idea of a break-even point, a balanced pixel frequency adaptive algorithm is proposed and each embedded bit of secret information is detected and compensated by the adjacent bit of histogram data, so that the histogram change of the image before and after information hiding is minimized At present, most of the histogram distortion values after steganography are generally over 1000 or even higher. As a contrast, the method proposed in this paper can keep the histogram distortion values to be less than 1000. The feasibility and effectiveness of the algorithm are verified by relative entropy analysis as well. The experimental results also show that the algorithm performs well in steganographic analyses of images.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954039	10.1142/S0218001419540399													
J								Network Intrusion Feature Map Node Equalization Algorithm Based on Modified Variable Step-Size Constant Modulus	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Network intrusion; feature map; node; MISO-VSS-MCMA; channel	BLIND EQUALIZATION	When the network is subject to intrusion and attack, the node output channel equalization will be affected, resulting in bit error and distortion in the output of network transmission symbols. In order to improve the anti-attack ability and equalization of network node, a network intrusion feature map node equalization algorithm based on modified variable step-size constant modulus blind equalization algorithm (MISO-VSS-MCMA) is proposed. In this algorithm, the node transmission channel model after network intrusion is constructed, and sequential processing is performed to intruded nodes with the variable structure feedback link control method. With diversity spread spectrum technology, the channel loss after network intrusion is compensated and the network intrusion map feature is extracted. According to the extracted feature amount, channel equalization processing is performed for the cost function with the MISO-VSS-MCMA method to reduce the damage of network intrusion to the channel. Simulation results show that in node transmission channel equalization after network intrusion, this algorithm can reduce the error bit rate of signal transmission in network, and provide a good ability of correcting phase deflection in the output constellation, thus avoiding the error bit distortion and channel damage caused by network intrusion to the signal with a good equalization effect. This algorithm provides stronger convergence and map concentration, which demonstrates that its anti-interference and signal recovery capabilities are better, so it improves the anti-attack ability of the network.																	0218-0014	1793-6381				DEC 31	2019	33	14							1955015	10.1142/S0218001419550152													
J								The Finger-Based Interactive Projection using a Monocular Camera	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Interactive projection; moire fringe filtering; motion detection; curve extraction; gloved finger recognition; touch detection	TRACKING	Ordinary projection screen is not sensitive to interaction, it cannot meet the demands of teaching, virtual reality, and other applications. Due to the fact that people always use hands to complete a variety of human-computer interaction, the finger-based interactive projection technology is worth being researched. In this paper, an ordinary monocular camera is used to acquire video frame on projection screen, and the touch signal of finger in frame is used as the input of interactive projection system. Because the differences between spatial frequency of common digital camera and the projection screen is small, the frame obtained from camera will contain moire fringe, which needs to be filtered in image frequency domain. Then the difference between current frame edge and previous frame edge is calculated to obtain moving object edge clues. According to these clues, the most possible contour curve is searched in current frame edge, and the curve is fitted by polynomial approximation method. Its curvature integration is used to match with the curvature integration of finger template curve. After that the fingers in the curve are recognized. Because color information is not needed, this method can be used to recognize gloved fingers. Finally, finger shadow is used to judge whether the finger touches projection screen to complete interactive process. The experiments of writing and collaboratively rotating picture on projector screen show that this method can effectively complete interactive operation with the projection screen and can realize the multi-user operation.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954034	10.1142/S021800141954034X													
J								A Tracking Window Adaptive Compressive Tracking Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										CT algorithm; tracking window; multi-scale; feature normalization	ROBUST VISUAL TRACKING	In the original compression tracking algorithm, the size of the tracking box is fixed. There should be better tracking results for scale-invariant objects, but worse tracking results for scale-variant objects. To overcome this defect, a scale-adaptive compressive tracking (CT) algorithm is proposed. First of all, the imbalance of the gray and texture features in the original CT algorithm is balanced by the multi-feature method, which makes the algorithm more robust. Then, searching different candidate regions by using the method of multi-scale search along with feature normalization makes the features extracted from different scales comparable. Finally, the candidate region with the maximum discriminate degree is selected as the object region. Thus, the tracking-box size is adaptive. The experimental results show that when the object scale changes, the improving CT algorithm has higher accuracy and robustness than the original CT algorithm.																	0218-0014	1793-6381				DEC 31	2019	33	14							1959045	10.1142/S0218001419590456													
J								A Convolutional Neural Network for Aspect-Level Sentiment Classification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Convolutional neural network; aspect-level sentiment		Sentiment analysis, including aspect-level sentiment classification, is an important basic natural language processing (NLP) task. Aspect-level sentiment can provide complete and in-depth results. Words with different contexts variably influence the aspect-level sentiment polarity of sentences, and polarity varies based on different aspects of a sentence. Recurrent neural networks (RNNs) are regarded as effective models for handling NLP and have performed well in aspect-level sentiment classification. Extensive literature exists on sentiment classification that utilizes convolutional neural networks (CNNs); however, no literature on aspect-level sentiment classification that uses CNNs is available. In the present study, we develop a CNN model for handling aspect-level sentiment classification. In our model, attention-based input layers are incorporated into CNN to introduce aspect information. In our experiment, in which a benchmark dataset from Twitter is compared with other models, incorporating aspect information into CNN improves aspect-level sentiment classification performance without using syntactic parser or other language features.																	0218-0014	1793-6381				DEC 31	2019	33	14							1959046	10.1142/S0218001419590468													
J								Combinatorial Detection Algorithm for Copy Number Variations Using High-throughput Sequencing Reads	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Copy number variation; hidden Markov model; split read; combinatorial detection algorithm; high-throughput sequencing	VARIANTS; POLYMORPHISM; INSERTION; DELETION	Copy number variation (CNV) is a prevalent kind of genetic structural variation which leads to an abnormal number of copies of large genomic regions, such as gain or loss of DNA segments larger than 1 kb. CNV exists not only in human genome but also in plant genome. Current researches have testified that CNV is associated with many complex diseases. In this paper, guanine-cytosine (GC) bias, mappability and their effect on read depth signals in sequencing data are discussed first. Subsequently, a new correction method for GC bias and an improved combinatorial detection algorithm for CNV using high-throughput sequencing reads based on hidden Markov model (CNV-HMM) are proposed. The corrected read depth signals have lower correlation with GC content, mappability of reads and the width of analysis window. Then we create a hidden Markov model which maps the reads onto the reference genome and records the unmapped reads. The unmapped reads are counted and normalized. The CNV-HMM detects the abnormal signal of read count and gains the candidate CNVs using the expectation maximization (EM) algorithm. Finally, we filter the candidate CNVs using split reads to promote the performance of our algorithm. The experiment result indicates that the CNV-HMM algorithm has higher accuracy and sensitivity for CNVs detection than most current detection algorithms.																	0218-0014	1793-6381				DEC 31	2019	33	14							1950022	10.1142/S021800149500228													
J								Automatic Plaque Segmentation in Coronary Optical Coherence Tomography Images	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Coronary atherosclerotic heart disease; plaque; optical coherence tomography; adaptive weight; convolutional neural network; random walk	DISEASE; CT; LESIONS	Coronary optical coherence tomography (OCT) is a new high-resolution intravascular imaging technology that clearly depicts coronary artery stenosis and plaque information. Study of coronary OCT images is of significance in the diagnosis of coronary atherosclerotic heart disease (CAD). We introduce a new method based on the convolutional neural network (CNN) and an improved random walk (RW) algorithm for the recognition and segmentation of calcified, lipid and fibrotic plaque in coronary OCT images. First, we design CNN with three different depths (2, 4 or 6 convolutional layers) to perform the automatic recognition and select the optimal CNN model. Then, we device an improved RW algorithm. According to the gray-level distribution characteristics of coronary OCT images, the weights of intensity and texture term in the weight function of RW algorithm are adjusted by an adaptive weight. Finally, we apply mathematical morphology in combination with two RWs to accurately segment the plaque area. Compared with the ground truth of clinical segmentation results, the Jaccard similarity coefficient (JSC) of calcified and lipid plaque segmentation results is 0.864, the average symmetric contour distance (ASCD) is 0.375 mm, the JSC and ASCD reliabilities are 88.33% and 92.50% respectively. The JSC of fibrotic plaque is 0.876, the ASCD is 0.349 mm, the JSC and ASCD reliabilities are 90.83% and 95.83% respectively. In addition, the average segmentation time (AST) does not exceed 5 s. Reliable and significantly improved results have been achieved in this study. Compared with the CNN, traditional RW algorithm and other methods. The proposed method has the advantages of fast segmentation, high accuracy and reliability, and holds promise as an aid to doctors in the diagnosis of CAD.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954035	10.1142/S0218001419540351													
J								Mapping the Global Knowledge Domain for Building Information Models	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										BIM; mapping knowledge domain; centrality; cluster analysis; co-citation	BIM	This paper presents the method and findings of a visual knowledge domain map used to explore the development and evolution of Building information modeling (BIM) technology. Metadata taken from 806 literature records is used to visualize the state of BIM research. The records were published between 1998 and 2017, and Citespace software is employed to visualize key data about the research contained on the Web of Science platform, including nationality of research institutions, research themes, co-citations of periodicals, authorship, literature sources, and the development of the discipline over time. The distribution of power in this field is also examined by interrogating the BIM knowledge base, identifying research hotspots to provide a reference for assessing the current state-of-the-art.																	0218-0014	1793-6381				DEC 31	2019	33	14							1959044	10.1142/S0218001419590444													
J								Robust Multibit Image Watermarking Based on Contrast Modulation and Affine Rectification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image watermarking; robustness; contrast modulation; affine transform; image rectification	QUANTIZATION INDEX MODULATION; DIGITAL WATERMARKING	In this paper, we present a robust multibit image watermarking scheme to undertake the common image-processing attacks as well as affine distortions. This scheme combines contrast modulation and effective synchronization for large payload and high robustness. We analyze the robustness, payload, and the lower bound of fidelity. Regarding watermark resynchronization under affine distortions, we develop a self-referencing rectification method to detect the distortion parameters for reconstruction by the center of mass in affine covariant regions. The effectiveness and advantages of the proposed scheme are confirmed by experimental results, which show the superior performance as comparing against several state-of-the-art watermarking methods.																	0218-0014	1793-6381				DEC 31	2019	33	14							1954036	10.1142/S0218001419540363													
