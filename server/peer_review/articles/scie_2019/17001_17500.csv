PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								GMMSP on GPU	JOURNAL OF REAL-TIME IMAGE PROCESSING										GPGPU; CUDA; Superpixel; Image segmentation	SUPERPIXELS	Superpixel segmentation is a fundamental task in computer vision. Existing works contribute to superpixel segmentation either by improving segmentation accuracy or by reducing execution time. The former modifies existing models or develops new models to improve accuracy. The latter accelerates existing implementations or reduces algorithm complexity to improve execution rate. This work falls into the second category. Recently, a superpixel algorithm using Gaussian mixture model (GMMSP) achieves state-of-the-art performance in accuracy. After exploring this algorithm, we reached new conclusions on GMMSP that unlock potential concerning fine-grain parallelism implementation. We implement GMMSP with CUDA and make it run on GPUs. Experiments are conducted to validate the consistency between CPU and GPU implementations and to evaluate the performance of our implementation with respect to a serial and an OpenMP implementation. When we consider a full implementation with a postprocessing step executed on CPU to guarantee connectivity constraint, the proposed implementation achieves a speedup of 21x compared to the OpenMP implementation for images of size 240 x 320, using NVIDIA GTX 1080. It is also mentionable that we achieve a performance of over 1000 FPS on GTX 1080 (speedup of 77x compared to the OpenMP implementation) if the connectivity constraint is not included.																	1861-8200	1861-8219				APR	2020	17	2					245	257		10.1007/s11554-018-0762-3													
J								Hierarchical prediction-based motion vector refinement for video frame-rate up-conversion	JOURNAL OF REAL-TIME IMAGE PROCESSING										Frame-rate up-conversion; Adaptive multi-layered block matching criterion; Hierarchical prediction-based motion vector refinement; Robust dual-weighted motion vector smoothing		Motion-compensated frame-rate up-conversion (MC-FRUC) often exploits either bilateral motion estimation (ME) or unidirectional ME with a fixed block size, which constrains the perceptual quality of up-converted video. In this paper, an advanced MC-FRUC approach is proposed by exploiting hierarchical prediction-based motion vector refinement. To reduce block mismatching in texture regions and color areas, an adaptive multi-layered block matching criterion is designed to extract color and edge information, which is integrated with motion information as constraint term. A hierarchical prediction-based motion vector refinement approach is proposed to obtain more accurate and dense motion vector fields (MVFs). To eliminate the outliers of MVFs, a robust dual-weighted motion vector smoothing scheme is adopted by using both spatial correlation and reliability of neighboring blocks. Experimental results show that the proposed approach has low computational complexity and outperforms state-of-the-art works in both objective and subjective qualities of interpolated frames.																	1861-8200	1861-8219				APR	2020	17	2					259	273		10.1007/s11554-018-0767-y													
J								In-loop perceptual model-based rate-distortion optimization for HEVC real-time encoder	JOURNAL OF REAL-TIME IMAGE PROCESSING										HEVC; Rate-distortion optimization (RDO); Motion attention; Visual distortion sensitivity	IMAGE QUALITY ASSESSMENT; MOTION ESTIMATION; BIT ALLOCATION; VIDEO; EFFICIENCY; ALGORITHM	In this paper, a novel High Efficiency Video Coding (HEVC)-compliant perceptual rate-distortion optimization (RDO) scheme is proposed based on motion attention and visual distortion sensitivity models, which both fully utilize in-loop coding information of HEVC. In detail, the motion attention model is designed by using the motion vectors (MVs) estimated during the inter-prediction process. The MV field is refined based on maximum a posteriori (MAP) estimation to remove MV outliers and improve the model's efficiency. In addition, the visual distortion sensitivity is modeled by using the spatiotemporal energy of AC coefficients, which are obtained from HEVC transform process. Then, these two models are incorporated together into the RDO process. As a result, the Lagrange multiplier and quantization parameter are adjusted adaptively in an analytical way. Since the two models are calculated within the HEVC coding loop, the complexity increase is limited. The experimental results indicate that the proposed perceptual RDO scheme can achieve significantly better rate-VQM performance than the conventional RDO scheme. Specifically, the BD-rate can reach a maximum 24.45% and an average 13.68% reduction in terms of the Bjontegaard Delta metric compared to HEVC practical encoder x265.																	1861-8200	1861-8219				APR	2020	17	2					293	311		10.1007/s11554-018-0772-1													
J								Accelerating kernel classifiers through borders mapping	JOURNAL OF REAL-TIME IMAGE PROCESSING										Class borders; Multi-dimensional root finding; Adaptive Gaussian filtering; Nonparametric statistics; Variable kernel density estimation	SENIORITY LOGIC; CLASSIFICATION	Support vector machine (SVM) and other kernel techniques represent a family of powerful statistical classification methods with high accuracy and broad applicability. Because they use all or a significant portion of the training data, however, they can be slow, especially for large problems. Piecewise linear classifiers are similarly versatile, yet have the additional advantages of simplicity, ease of interpretation and, if the number of component linear classifiers is not too large, speed. Here we show how a simple, piecewise linear classifier can be trained from a kernel-based classifier in order to improve the classification speed. The method works by finding the root of the difference in conditional probabilities between pairs of opposite classes to build up a representation of the decision boundary. When tested on 17 different datasets, it succeeded in improving the classification speed of a SVM for 12 of them by up to two orders of magnitude. Of these, two were less accurate than a simple, linear classifier. The method is best suited to problems with continuum features data and smooth probability functions. Because the component linear classifiers are built up individually from an existing classifier, rather than through a simultaneous optimization procedure, the classifier is also fast to train.																	1861-8200	1861-8219				APR	2020	17	2					313	327		10.1007/s11554-018-0769-9													
J								Efficient approximate core transform and its reconfigurable architectures for HEVC	JOURNAL OF REAL-TIME IMAGE PROCESSING										Discrete cosine transform (DCT); Approximation; High efficiency video coding (HEVC); FPGA-based hardware implementation	FAST COMPUTATIONAL ALGORITHM; DISCRETE COSINE; DESIGN	This paper describes a new approximate transform for the high efficiency video coding (HEVC). A 8 x 8 discrete cosine transform (DCT) approximation is proposed and then down-sampled or expanded to generate the 4 x 4, 16 x 16, and 32 x 32 approximate matrices. The proposed 8 x 8 approximation is carried out in part by neighbourhood in order to take the advantage of adjacent pixels correlation of natural images. Hence, rather than approximating the odd basis vectors of DCT kernel by referring to their intrinsic values, we choose to quantize that by taking into account their signs and positions. The proposed approximation matrices respect the properties of transform matrices prescribed by HEVC like orthogonality and bit-length of the basis vector elements. Furthermore, they have nearly the same arithmetic complexity and hardware requirement as those of recently proposed related methods, but involve significantly less error energy. Moreover, a reconfigurable design based on the 8 x 8 approximation transform is proposed in order to allow the simultaneous computation of eight 4-, four 8-, two 16-, or one 32-point approximate DCTs. It is found that the reconfigurable design can involve nearly 26% less area-delay product (ADP) when compared with the separate non-reconfigurable designs. Experimental results obtained from FPGA prototype and HM simulations have demonstrated the advantages of the proposed transforms.																	1861-8200	1861-8219				APR	2020	17	2					329	339		10.1007/s11554-018-0768-x													
J								Online action recognition from RGB-D cameras based on reduced basis decomposition	JOURNAL OF REAL-TIME IMAGE PROCESSING										Human action recognition; Depth motion map; Collaborative representation classifier; Greedy algorithm; Reduced basis decomposition	REGULARIZATION	Human action recognition from RGB-D cameras has recently become one of the major fields of research. While accuracy improvement was given more importance in previous action/gesture recognition methods, there are opportunities to work on improving the computational efficiency too. This paper introduces an efficient dimensionality reduction technique and classification mechanism to recognize actions from depth motion map features. For our proposed work, a recently introduced technique called reduced basis decomposition ( RBD) is employed, which manages faster dimensional reduction with its unique mechanism of generating compressed basis vectors. The RBD has an offline error-determination and an online approximation mechanism, and it is faster than PCA/SVD. For classification, this paper employs a Probabilistic Collaborative Representation Classifier (Pro-CRC). The recommended classifier works based on probability in connection with l(2)-regularization. The combined effect of the methods above helps in achieving the state-of-the-art efficiency. In the standard protocol tests carried out in the MSR-Action3D dataset, our proposed method achieved a considerable accuracy of 91.7% which is better than the currently efficient method. Further, our proposed method also proved its effectiveness in the challenging, subject-generic test with a reported accuracy of 89.64% and an average accuracy of 85.70% in the cross fixed tests which included 252 combinations of all the subjects without repetition.																	1861-8200	1861-8219				APR	2020	17	2					341	356		10.1007/s11554-018-0778-8													
J								FPGA Implementation of Optimized Karhunen-Loeve Transform for Image Processing Applications	JOURNAL OF REAL-TIME IMAGE PROCESSING										Modified divider architecture; Optimized square root architecture; Error correction; Optimized Karhunen-Loeve Transform (O-KLT)Architecture; FPGA implementation	HARDWARE ALGORITHM; KL	The various transformation techniques play vital role in the field of Digital Image Processing. In this paper, we propose FPGA implementation of optimized Karhunen-Loeve transform for image processing applications. The Data Format Conversion block is introduced to represent the input data to suitable format and are fed to the Covariance computation block to calculate corresponding covariance values with accuracy. The Optimized Square Root block has been designed in the Eigenvalue computation block to obtain eigenvalues which are in turn fed to the Eigenvector computation block to produce eigenvectors using Modified divider. Further the Karhunen-Loeve Transformed matrix of the input data is obtained by performing multiplication of eigenvectors with covariance values in the matrix multiplication block. The errors are introduced due to fixed point binary calculations and are minimized by novel Error correction block. The proposed architecture is tested on Sparan-6 (XC6SLX45-3CSG324) FPGA board. The performance of the architecture is compared with respect to hardware utilization and accuracy of various existing techniques to prove the efficiency.																	1861-8200	1861-8219				APR	2020	17	2					357	370		10.1007/s11554-018-0776-x													
J								All-hardware SIFT implementation for real-time VGA images feature extraction	JOURNAL OF REAL-TIME IMAGE PROCESSING										Scale-invariant feature transform (SIFT); Field Programmable Gate Array (FPGA); Parallel architecture; Pipeline architecture	SCALE	This paper presents a real time hardware implementation of the scale invariant feature transform (SIFT) algorithm. To achieve real time requirements, pipeline structures have been widely exploited both in the keypoint extraction and in the descriptor generation stages. Simplifications to the original algorithm have been also applied to allow a simpler hardware implementation. The proposed architecture has been synthesized on a Xilinx Virtex 5 FPGA. It generates 3072 descriptor vectors for VGA images at 99 frames per second at a clock rate of 100 MHz.																	1861-8200	1861-8219				APR	2020	17	2					371	382		10.1007/s11554-018-0781-0													
J								Image analyzer for stereoscopic camera rig alignment	JOURNAL OF REAL-TIME IMAGE PROCESSING										Computer aided analysis; Stereo image processing; Image fusion; Motion pictures		The paper presents a versatile solution facilitating calibration of stereoscopic camera rigs for 3D cinematography and machine vision. Manual calibration of the rig and the camera can easily take several hours. The proposed device eases this process by providing the camera operator with several predefined analyses of the images from the cameras. The Image Analyzer is a compact stand-alone device designed for portable 19 '' racks. Almost all video processing is performed on a modern Xilinx FPGA. It is supported by an ARM computer which provides control and video streaming over the Ethernet. The article presents its hardware, firmware and software architectures. The main focus is put on the image processing system implemented in the FPGA.																	1861-8200	1861-8219				APR	2020	17	2					383	391		10.1007/s11554-018-0779-7													
J								Stereo vision architecture for heterogeneous systems-on-chip	JOURNAL OF REAL-TIME IMAGE PROCESSING										Embedded systems; Systems-on-chip; Stereo vision; FPGA-based SoCs	HARDWARE ARCHITECTURE; DISPARITY ESTIMATION; IMPLEMENTATION; ALGORITHM; DESIGN	Stereo vision is a crucial operation in state-of-the-art computer vision applications. Several efficient algorithms have been recently proposed either to increase the quality of the produced disparity maps or to reach higher computational speed, or both. Among them, hardware-oriented algorithms are desirable when the main objective is including stereo vision in portable consumer electronic and multimedia systems. Modern FPGA-based platforms allow all-programmable heterogeneous embedded systems to be realized as SoCs and they are certainly appropriate also to implement stereo vision. This paper proposes a novel stereo vision algorithm and a specific implementation suitable for heterogeneous SoC FPGA-based embedded systems. A complete embedded system design is also demonstrated using the Xilinx Zynq-7000 SoCs device family. The novel algorithm has been characterized in terms of accuracy by referring to the benchmark sets Middlebury and Kitti. When 640 x 480 8-bit greyscale stereo pairs are processed, the fastest prototype, realized within the XC7Z020 and the XC7Z045 device, respectively, exhibits an 81 and 101 fps frame rate. Conversely, the cheapest implementation occupies only 52691 LUTs, 59715 FFs, 93 DSPs, 40 BRAM18k and 6 BRAM36k memory blocks. In comparison to several counterparts existing in literature, the novel algorithm can achieve higher accuracies, and makes the proposed complete system design able to reach the most favourable accuracy/performance trade-off.																	1861-8200	1861-8219				APR	2020	17	2					393	415		10.1007/s11554-018-0782-z													
J								A light tracker for online multiple pedestrian tracking	JOURNAL OF REAL-TIME IMAGE PROCESSING										Shallow network; Motion pattern; Multiple pedestrian tracker	FILTERS	We propose a novel real-time multiple pedestrian tracker for videos acquired from both static and moving cameras in unconstrained real-world environment. In such scenes, trackers always suffer from noisy detections and frequent occlusions. Existing methods usually use complex learning approaches and a large number of training samples to get discriminative appearance features. However, this leads to high computational cost and hardly works in occlusions (missing detections) and undistinguishable appearance. Addressing this, we design a light two-stage tracker. Firstly, a shallow net with two layers of full convolution is proposed to encode appearance. Compared with other deep architectures and sophisticated learning approaches, our shallow net is efficient and robust enough without any online updating. Secondly, we design a motion model to deal with noisy detections and missing objects caused by motion blur or occlusion. By mining the motion pattern, our tracker can reliably predict the object location under challenging scenes. Furthermore, we propose a speedup version to verify our robustness and the possibility of using in online applications. Extensive experiments are implemented on multiple object tracking benchmarks, MOT15 and MOT17. The performance is competitive over a number of state-of-the-art trackers and demonstrates that our tracker is very promising for real-time applications.																	1861-8200	1861-8219															10.1007/s11554-020-00962-3		APR 2020											
J								State of the Art in Bionic Hands	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Bibliometric survey; bionic hand; hand prosthesis; information analysis; prosthetic hand	TARGETED MUSCLE REINNERVATION; NONINVASIVE SENSORY FEEDBACK; UPPER-LIMB PROSTHESES; AMPUTATION; EVOLUTION; CLASSIFICATION; REPLACEMENT; TECHNOLOGY; FINGER; DESIGN	Prosthetic hands have made a significant influence on the quality of life of people with upper arm amputation. Research on prosthetic hands today is focused on replicating the functionalities of the biological hands. The present article provides a bibliometric survey on bionic hands, done through a compilation of a scientific publications database on the field of prosthetic hands spanning the last two decades. Through network-based information analysis, meaningful patterns are inferred, and several key questions significant to bionic prosthetic hands are answered. The article gives an insight into the growth, progress, and future trend in bionic hand prostheses, including identifying subject areas and discovering distinct research communities within the subject. The analysis clearly reveals the rapid expansion of bionic hand research over the last two decades, which is expected to rise considerably in near future.																	2168-2291	2168-2305				APR	2020	50	2					116	130		10.1109/THMS.2020.2970740													
J								Active Compliance Control Reduces Upper Body Effort in Exoskeleton-Supported Walking	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Compliance control; locomotion control; lower body exoskeleton	DESIGN; REHABILITATION; INDIVIDUALS; JOINT; ROBOT	This article presents a locomotion controller for lower limb exoskeletons so as to enable the combined robot and user system to exhibit compliant walking characteristics when interacting with the environment. This is of critical importance to reduce the excessive ground reaction forces during the walking task execution with the aim of improved environmental interaction capabilities. In robot-aided walking support for paraplegics, the user has to actively use his/her upper limbs via crutches to ensure overall balance. By virtue of this requisite, several issues may particularly arise during touchdown instants, e.g., upper body orientation fluctuates, shoulder joints are subject to excessive loading, and arms may need to exert extra forces to counterbalance these effects. In order to reduce the upper body effort via compliant locomotion, the controller is designed to manage the force/position tradeoff by using an admittance controller in each joint. For proof of concept, a series of exoskeleton-aided walking experiments were conducted with the participation of nine healthy volunteers, four of whom additionally walked on an irregular surface for further performance evaluation. The results suggest that the proposed locomotion controller is advantageous over conventional high-gain position tracking in decreasing undesired oscillatory torso motion and total arm force, adequately reducing the required upper body effort.																	2168-2291	2168-2305				APR	2020	50	2					144	153		10.1109/THMS.2019.2961969													
J								Determination of the Gain for a Walking Speed Amplifying Belt Using Brain Activity	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Ergonomics; gait recognition; human-computer interaction; human-robot interaction	OPTICAL-FLOW; AGING SOCIETY; TOOL-USE; MOBILITY; RECOGNITION; EVOLUTION; CORTICES; VEHICLE; SYSTEMS	Movement/walking assistance devices have the great advantage of supporting quality of life for the elderly in an aging society. To strike a balance between efficiency of movement and employment of the elderly user's own body, we developed a smart mobility system called Tread-walk, which is controlled by the user walking on a treadmill and amplifies the user's walking speed. Since the user's walking speed is different from the speed at which the Tread-walk moves, users experience a mismatch between their visual optical flow and somatic sense. In this article, we validate the feasibility of an amplifying gain decision method that analyzes user brain activity. To control Tread-walk, the visual sense is integrated with somatosensation in the parietal area of the brain and controlled in the medial prefrontal cortex. Therefore, first, we measure the parietal area when the participants walk while looking at their virtual optical flow. Second, we measure the medical prefrontal cortex when the participants control Tread-walk 2. These experiments are carried out for a variety of speed amplifying gains. We find that the brain activates significantly at amplification gain K = 1.1-1.7 in the virtual optical flow experiment and K = 1.5-2.0 in the Tread-walk experiment; this brain activation represents the amplification gain at which the visual and somatosensory senses seem to receive similar input. In conclusion, the brain would activate the most significantly at the most appropriate amplification gain.																	2168-2291	2168-2305				APR	2020	50	2					154	164		10.1109/THMS.2019.2961974													
J								A Unifying Theory of Driver Perception and Steering Control on Straight and Winding Roads	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Vehicles; Roads; Visualization; Control theory; Windings; Vehicle dynamics; Task analysis; Driver steering; multiloop control; preview information; system identification; visual perception	MANUAL CONTROL; VISUAL CONTROL; MODEL; PREVIEW; INFORMATION; FLOW	Novel driver support systems potentially enhance road safety by cooperating with the human driver. To optimize the design of emerging steering support systems, a profound understanding of driver steering behavior is required. This article proposes a new theory of driver steering, which unifies visual perception and control models. The theory is derived directly from measured steering data, without any a priori assumptions on driver inputs or control dynamics. Results of a human-in-the-loop simulator experiment are presented, in which drivers tracked the centerline of straight and winding roads. Multiloop frequency response function (FRF) estimates reveal how drivers use visual preview, lateral position feedback, and heading feedback for control. Classical control theory is used to model all three FRF estimates. The model has physically interpretable parameters, which indicate that drivers minimize the bearing angle to an "aim point" (located 0.25-0.75 s ahead) through simple compensatory control, both on straight and winding roads. The resulting unifying perception and control theory provides a new tool for rationalizing driver steering behavior, and for optimizing modern steering support systems.																	2168-2291	2168-2305				APR	2020	50	2					165	175		10.1109/THMS.2019.2947551													
J								How Communicating Features can Help Pedestrian Safety in the Presence of Self-Driving Vehicles: Virtual Reality Experiment	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Autonomous vehicles (AVs); communicating features; human-automation interaction; pedestrian safety; virtual reality (VR)	DRIVERS; INTERSECTIONS; QUESTIONNAIRE; VALIDATION; AUTOMATION; BEHAVIOR; ETHICS; DESIGN; RISK	Virtual reality (VR) has proven to be a useful tool for conducting human factors research in interface design. With the development and promotion of autonomous vehicle (AV) technology, researchers are focusing on designing different types of interfaces to induce trust in road users toward this new technology. In this article, VR is used to investigate pedestrians' understanding of proposed designs for external features on AVs. We are also interested in investigating how the presence of an operator inside the vehicle influences pedestrians' preference for features. VR headset tracking, survey-based responses, and video-recorded body movements are used to collect data on pedestrian responses to three operator statuses and seven feature types. Pedestrians prefer both "walk" in text and verbal message saying safe to cross as clear and comforting features on an AV. They perceive the distracted operator condition as to be an inconvenient situation even for the AV equipped with visual or audible features. Older people find the features more helpful and people with higher innovativeness rate the feature ideas with higher ratings.																	2168-2291	2168-2305				APR	2020	50	2					176	186		10.1109/THMS.2019.2960517													
J								Improving traffic flow forecasting with relevance vector machine and a randomized controlled statistical testing	SOFT COMPUTING										Data-driven learning; Relevance vector machine; Kernel learning; Open-set big data understanding; Statistical testing	PREDICTION; REGRESSION; NETWORKS	High-accuracy traffic flow forecasting is vital to the development of intelligent city transportation systems. Recently, traffic flow forecasting models based on the kernel method have been widely applied due to their great generalization capability. The aim of this article is twofold: A novel kernel learning method, relevance vector machine, is employed to short-term traffic flow forecasting so as to capture the inner correlation between sequential traffic flow data, it is a type of nonlinear model which is accurate and using only a small number of relevant basis functions automatically selected. So that it can find concise data representations which are adequate for the learning task retaining as much information as possible. On the other hand, the sample size for learning has a significant impact on forecasting accuracy. How to balancing the relationship between the sample size and the forecasting accuracy is an important research topic. A randomized controlled statistical testing is layout to evaluating the impacts of sample size of the new proposed traffic flow forecasting model. The experimental results show that the new model achieves similar or better forecasting and generalization performance compared to some old ones; besides, it is less sensitive to the size of learning sample.																	1432-7643	1433-7479				APR	2020	24	8			SI		5485	5497		10.1007/s00500-018-03693-7													
J								Study on a storage location strategy based on clustering and association algorithms	SOFT COMPUTING										Cluster; Association rules; Class-based storage; Order picking; Picking efficiency	CLOUD	In this paper, we study the improvement of a storage location strategy through the use of big data technology, including data collection, cluster analysis and association analysis, to improve order picking efficiency. A clustering algorithm is used to categorize the types of goods in orders. Classification is performed based on the turnover of goods, value, sales volume, favorable commodity ratings, whether free shipping is provided and whether cash on delivery is supported. An association algorithm is used to determine the relationships among goods by studying the habits of consumers who buy them. A method for improving the class-based storage strategy is proposed. The picking distance of the improved storage strategy is compared with that of the traditional strategy via simulation experiments. The picking efficiency is shown to be enhanced by the improved strategy.																	1432-7643	1433-7479				APR	2020	24	8			SI		5499	5516		10.1007/s00500-018-03702-9													
J								Scalable detection of botnets based on DGA Efficient feature discovery process in machine learning techniques	SOFT COMPUTING										Botnet; Domain generation algorithm; DGA; Machine Learning; Natural language processing		Botnets are evolving, and their covert modus operandi, based on cloud technologies such as the virtualisation and the dynamic fast-flux addressing, has been proved challenging for classic intrusion detection systems and even the so-called next-generation firewalls. Moreover, dynamic addressing has been spotted in the wild in combination with pseudo-random domain names generation algorithm (DGA), ultimately leading to an extremely accurate and effective disguise technique. Although these concealing methods have been exposed and analysed to great extent in the past decade, the literature lacks some important conclusions and common-ground knowledge, especially when it comes to Machine Learning (ML) solutions. This research horizontally navigates the state of the art aiming to polish the feature discovery process, which is the single most time-consuming part of any ML approach. Results show that only a minor fraction of the defined features are indeed practical and informative, especially when considering 0-day botnet identification. The contributions described in this article will ease the detection process, ultimately enabling improved and more scalable solutions for DGA-based botnets detection.																	1432-7643	1433-7479				APR	2020	24	8			SI		5517	5537		10.1007/s00500-018-03703-8													
J								An efficient index structure for distributed k-nearest neighbours query processing	SOFT COMPUTING										k-Nearest neighbour query; Distributed query processing; Moving objects		Many location-based services are supported by the moving k-nearest neighbour (k-NN) query, which continuously returns the k-nearest data objects for a query point. Most of existing approaches to this problem have focused on a centralized setting, which show poor scalability to work around massive-scale and distributed data sets. In this paper, we propose an efficient distributed solution for k-NN query over moving objects to tackle the increasingly large scale of data. This approach includes a new grid-based index called Block Grid Index (BGI), and a distributed k-NN query algorithm based on BGI. There are three advantages of our approach: (1) BGI can be easily constructed and maintained in a distributed setting; (2) the algorithm is able to return the results set in only two iterations. (3) the efficiency of k-NN query is improved. The efficiency of our solution is verified by extensive experiments with millions of nodes.																	1432-7643	1433-7479				APR	2020	24	8			SI		5539	5550		10.1007/s00500-018-3548-4													
J								Information retrieval methodology for aiding scientific database search	SOFT COMPUTING										Information retrieval; Systematic literature review; Text mining; Vector space model; Support vector machine		During literature reviews, and specially when conducting systematic literature reviews, finding and screening relevant papers during scientific document search may involve managing and processing large amounts of unstructured text data. In those cases where the search topic is difficult to establish or has fuzzy limits, researchers require to broaden the scope of the search and, in consequence, data from retrieved scientific publications may become huge and uncorrelated. However, through a convenient analysis of these data the researcher may be able to discover new knowledge which may be hidden within the search output, thus exploring the limits of the search and enhancing the review scope. With that aim, this paper presents an iterative methodology that applies text mining and machine learning techniques to a downloaded corpus of abstracts from scientific databases, combining automatic processing algorithms with tools for supervised decision-making in an iterative process sustained on the researchers' judgement, so as to adapt, screen and tune the search output. The paper ends showing a working example that employs a set of developed scripts that implement the different stages of the proposed methodology.																	1432-7643	1433-7479				APR	2020	24	8			SI		5551	5560		10.1007/s00500-018-3568-0													
J								Identity-based data storage scheme with anonymous key generation in fog computing	SOFT COMPUTING										Fog computing; Key escrow; ID-based proxy pre-encryption; Collusion attack; DBDH; Data storage	ATTRIBUTE-BASED ENCRYPTION; PROXY RE-ENCRYPTION; CLOUD STORAGE; SECURE; EFFICIENT	Identity-based proxy pre-encryption is a good candidate to achieve data sharing. When it is deployed to fog computing scenarios, it can provide more flexible access control service than being deployed to cloud computing for end-users since fog nodes are physically close to end-users. However, the existing IB-PRE schemes exist several security flaws. First, all IB-PRE schemes exist key escrow problem, which makes that the PKG can decrypt all ciphertexts of the users. Second, one re-encryption key can transform all ciphertexts of the delegator into all ciphertexts of the delegatee, which makes the scheme cannot provide fine-grained access control. Third, most of IB-PRE schemes cannot provide the user revocation and prevent collusion attacks. To overcome the above problems, in the paper, we propose an identity-based data storage scheme with anonymous key generation which is applied to fog computing. And then it is shown to provably secure in the random oracle model. By comparing with other existing schemes, our scheme has some advantages over the other schemes in terms of security properties. Finally, by experiment analysis, the result shows our scheme is efficient with respect to computational cost and communication overhead.																	1432-7643	1433-7479				APR	2020	24	8			SI		5561	5571		10.1007/s00500-018-3593-z													
J								WOA plus BRNN: An imbalanced big data classification framework using Whale optimization and deep neural network	SOFT COMPUTING										Whale optimizer; Deep learning; Neural network; Imbalanced big data; Data mining; Classification	SUPPORT VECTOR MACHINE; DATA ANALYTICS; ALGORITHM	Nowadays, big data plays a substantial part in information knowledge analysis, manipulation, and forecasting. Analyzing and extracting knowledge from such big datasets are a very challenging task due to the imbalance of data distribution, which could lead to a biased classification results and wrong decisions. The standard classifiers are not capable of handling such datasets. Hence, a new technique for dealing with such datasets is required. This paper proposes a novel classification framework for big data that consists of three developed phases. The first phase is the feature selection phase, which uses the Whale optimization algorithm (WOA) for finding the best set of features. The second phase is the preprocessing phase, which uses the SMOTE algorithm and the LSH-SMOTE algorithm for solving the class imbalance problem. Lastly, the third phase is WOA + BRNN algorithm, which is using the Whale optimization algorithm for training a deep learning approach called bidirectional recurrent neural network for the first time. Our proposed algorithm WOA-BRNN has been tested against nine highly imbalanced datasets one of them is big dataset in terms of area under curve (AUC) against four of the most common use machine learning algorithms (Naive Bayes, AdaBoostM1, decision table, random tree), in addition to GWO-MLP (training multilayer perceptron using Gray Wolf Optimizer), then we test our algorithm over four well-known datasets against GWO-MLP and particle swarm optimization (PSO-MLP), genetic algorithm (GA-MLP), ant colony optimization (ACO-MLP), evolution strategy (ES-MLP), and population-based incremental learning (PBIL-MLP) in terms of classification accuracy. Experimental results proved that our proposed algorithm WOA + BRNN has achieved promising accuracy and high local optima avoidance, and outperformed four of the most common use machine learning algorithms, and GWO-MLP in terms of AUC.																	1432-7643	1433-7479				APR	2020	24	8			SI		5573	5592		10.1007/s00500-019-03901-y													
J								Automatic keyphrase extraction using word embeddings	SOFT COMPUTING										Keyphrase extraction; Random-walk-based keyphrase extraction model; Word embedding; Phrase scoring model	EFFICIENT	Unsupervised random-walk keyphrase extraction models mainly rely on global structural information of the word graph, with nodes representing candidate words and edges capturing the co-occurrence information between candidate words. However, using word embedding method to integrate multiple kinds of useful information into the random-walk model to help better extract keyphrases is relatively unexplored. In this paper, we propose a random-walk-based ranking method to extract keyphrases from text documents using word embeddings. Specifically, we first design a heterogeneous text graph embedding model to integrate local context information of the word graph (i.e., the local word collocation patterns) with some crucial features of candidate words and edges of the word graph. Then, a novel random-walk-based ranking model is designed to score candidate words by leveraging such learned word embeddings. Finally, a new and generic similarity-based phrase scoring model using word embeddings is proposed to score phrases for selecting top-scoring phrases as keyphrases. Experimental results show that the proposed method consistently outperforms eight state-of-the-art unsupervised methods on three real datasets for keyphrase extraction.																	1432-7643	1433-7479				APR	2020	24	8			SI		5593	5608		10.1007/s00500-019-03963-y													
J								A privacy-preserving multi-keyword search approach in cloud computing	SOFT COMPUTING										Privacy preserving; Data privacy; Cloud privacy; Data encryption; Multi-keyword search; Searchable indexing schemes	ENCRYPTION; EFFICIENT	Cloud computing provides the users with the ability to outsource their data to a third-party cloud storage for cost-effective management of resources and on-demand network access. However, outsourcing the data to a third-party location may raise concerns about data privacy. To maintain the user's privacy, users tend to encrypt their sensitive data before outsourcing it. Encrypting the data will preserve its privacy, but at the same time, it makes the searching process for a specific keyword a time-consuming and challenging process, mainly if the encryption key is not provided. On the other hand, the data owner should be able to perform multiple keyword searches to retrieve specific documents that are relevant to the search query. This paper proposes a new privacy-preserving multi-keyword search approach for the cloud outsourced data. The objective of the proposed approach is to allow the data owners and the authorized users to retrieve the most relevant data with minimum computation and communication overhead, and reduced false positives (irrelevant documents) and searching time. To evaluate the proposed approach, the NSF research dataset is used. Results demonstrate that the proposed method achieves better searching time and overall performance of the cloud environment regarding computation and communication overhead as well as false positives in comparison with other approaches.																	1432-7643	1433-7479				APR	2020	24	8			SI		5609	5631		10.1007/s00500-019-04033-z													
J								A novel metaheuristic inspired by Hitchcock birds' behavior for efficient optimization of large search spaces of high dimensionality	SOFT COMPUTING										Optimization; Metaheuristic; Swarm intelligence; Adaptive algorithm	CHICKEN SWARM; ALGORITHM	In this paper, a new optimization algorithm called the Hitchcock bird-inspired algorithm (HBIA) is proposed. It is inspired by the aggressive bird behavior portrayed by Alfred Hitchcock in the 1963 thriller "The Birds." It is noteworthy to emphasize that the bird's behavior as shown in the movie is itself inspired by a considered natural birds behavior when faced with extreme conditions. HBIA is a stochastic swarm intelligence algorithm that captures the essence of the fictional behavior of the phenomenon of birds throughout the Hitchcock's film and model an optimization mechanism. The algorithm is based on the attack pattern of birds in the film, which has the stages of lurking, attack and reorganization, defined by the initialization, movement strategies in the search space and strategy of local minimum escape, respectively. The technique has as differential the use of adaptive parameters, a discretized random initialization and the use of the beta distribution. In contrast to the existing ones, the proposed technique provides an efficient optimization in high-dimensionality cost functions, using adaptive parameters, a discretized random initialization and the use of the beta distribution. Its performance is analyzed and compared to classic techniques, such as PSO, ABC and CS, as well as to the existing adaptive techniques, such as sine cosine algorithm, whale optimization algorithm, teaching-learning-based optimization and vortex search. HBIA's performance is investigated by several experiments implemented through eight cost functions. The results show that the HBIA can find more satisfactory solutions in large search spaces and high dimensionality of the evaluated cost functions when compared to the existing optimization methods.																	1432-7643	1433-7479				APR	2020	24	8			SI		5633	5655		10.1007/s00500-019-04102-3													
J								On one-time cookies protocol based on one-time password	SOFT COMPUTING										Web session security; Session hijacking; One-time cookies; One-time password; Hash chain		Cookies are used for tracking user sessions in Web servers. Though, the security vulnerability of cookies may cause session being hijacked. To resist attacks, Dacosta et al. proposed one-time cookies (OTC) protocol. Unfortunately, one primary weakness is its availability relying on time synchronization between two machines, while the other is using a fixed session key to generate OTC during session period, turning possible adversaries to crack the key. Motivated by these shortcomings, a novel OTC protocol based on a one-time password (OTP) is proposed in the paper. The protocol adopts the OTP algorithm based on a hash chain to avoid time synchronization problems and generate a dynamic key for improving the security of OTC. For efficiency, we also enhanced the OTP algorithm. Security analysis and experimental results show that the proposed OTC protocol is promising to deliver high security and minimal burden on performance.																	1432-7643	1433-7479				APR	2020	24	8			SI		5657	5670		10.1007/s00500-019-04138-5													
J								Secure and efficient big data deduplication in fog computing	SOFT COMPUTING										Fog computing; Secure deduplication; Proof of ownership; Efficiency	CLOUD; OWNERSHIP; PRIVACY; PROOF	With the rapid development of the Internet of Things, the massive amount of big data generated by the Internet of Things terminals and the real-time processing requirements have brought enormous challenges. A two-tier computing model consisting solely of two entities, cloud and user, will not be sufficient to support processing large numbers of concurrent data requests. Therefore, fog computing was proposed. How to realize the secure and efficient deduplication of ciphertext in fog computing has become a new research topic. In this paper, we firstly present a new decentralized deduplication structure and then show how to apply it to construct a secure and efficient big data deduplication scheme in fog computing. The cloud server, in the proposed paper, can quickly determine which fog server needs to be traversed to search duplicate data, and instead of traversing all fog servers. This significantly improves the efficiency of big data deduplication in fog computing. Furthermore, the proposed scheme allows fog server to verify whether the user possesses the ownership of the data. Performance analysis and experimental results show the proposed scheme has less overheads than existing schemes.																	1432-7643	1433-7479				APR	2020	24	8			SI		5671	5682		10.1007/s00500-019-04215-9													
J								Core-reviewer recommendation based on Pull Request topic model and collaborator social network	SOFT COMPUTING										Pull Request; Core-reviewer recommendation; GitHub; LDA; Social network	ALGORITHM; GITHUB	Pull Request (PR) is a major contributor to external developers of open-source projects in GitHub. PR reviewing is an important part of open-source software developments to ensure the quality of project. Recommending suitable candidates of reviewer to the new PRs will make the PR reviewing more efficient. However, there is not a mechanism of automatic reviewer recommendation for PR in GitHub. In this paper, we propose an automatic core-reviewer recommendation approach, which combines PR topic model with collaborators in the social network. First PR topics will be extracted from PRs by the latent Dirichlet allocation, and then the collaborator-PR network will be constructed with the connection between collaborators and PRs, and the influence of each collaborator will be calculated via the improved PageRank algorithm which combines with HITS. Finally, the relationship between topics and collaborators will also be built by the history of PR reviewing. When a new PR presents, a collaborator will be chosen as a core reviewer according to the influence of collaborators and the relationship between the new PR and collaborators. The experiment results show in the matching score calculation processing, the influence of collaborators shows higher than that with the expert, and the recommendation precision is better than 70%.																	1432-7643	1433-7479				APR	2020	24	8			SI		5683	5693		10.1007/s00500-019-04217-7													
J								IoT transaction processing through cooperative concurrency control on fog-cloud computing environment	SOFT COMPUTING										Cloud computing; Fog computing optimistic concurrency control; Transaction processing; Internet of Things	INTERNET; THINGS; MANAGEMENT; EDGE	In cloud-fog environments, the opportunity to avoid using the upstream communication channel from the clients to the cloud server all the time is possible by fluctuating the conventional concurrency control protocols. Through the present paper, the researcher aimed to introduce a new variant of the optimistic concurrency control protocol. Through the deployment of augmented partial validation protocol, IoT transactions that are read-only can be processed at the fog node locally. For final validation, update transactions are the only ones sent to the cloud. Moreover, the update transactions go through partial validation at the fog node which makes them more opportunist to commit at the cloud. This protocol reduces communication and computation at the cloud as much as possible while supporting scalability of the transactional services needed by the applications running in such environments. Based on numerical studies, the researcher assessed the partial validation procedure under three concurrency protocols. The study's results indicate that employing the proposed mechanism shall generate benefits for IoT users. These benefits are obtained from transactional services. We evaluated the effect of deployment the partial validation at the fog node for the three concurrency protocols, namely AOCCRBSC, AOCCRB and STUBcast. We performed a set of intensive experiments to compare the three protocols with and without such deployment. The result reported a reduction in miss rate, restart rate and communication delay in all of them. The researcher found that the proposed mechanism reduces the communication delay significantly. They found that the proposed mechanism shall enable low-latency fog computing services of the IoT applications that are a delay sensitive.																	1432-7643	1433-7479				APR	2020	24	8			SI		5695	5711		10.1007/s00500-019-04220-y													
J								An empirical study on effects of electronic word-of-mouth and Internet risk avoidance on purchase intention: from the perspective of big data	SOFT COMPUTING										E-word-of-mouth; Internet risk avoidance; Purchase intention; Enterprise information systems; Big data	HOTEL SELECTION FACTORS; GENDER-DIFFERENCES; SERVICE QUALITY; ONLINE; CONSUMERS; BEHAVIOR; REVIEWS; MODEL; TRUST; INFORMATION	This study investigated the influence of the usefulness of E-word-of-mouth (eWOM) and Internet risk avoidance on consumers' purchase intention. In particular, because consumers typically exhibit gender difference in their purchase intentions, this study adopted a quasi-experimental design and developed four situational questionnaires on gender difference. The objective of this study was to understand the influence of related factors on consumers' hotel reservation intention. A total of 512 effective data were collected via online questionnaires. The results showed that eWOM and Internet risk avoidance were significantly and positively correlated with consumers' purchase intention, and eWOM had higher predictive power than Internet risk avoidance did. The results reveal that both male and female respondents emphasized hotel facilities, and their purchase intention was positively influenced by related positive comments. However, female respondents paid more attention to hotel service quality than male respondents did.																	1432-7643	1433-7479				APR	2020	24	8			SI		5713	5728		10.1007/s00500-019-04300-z													
J								Using the DEMATEL model to expose core causal items of LibQUAL for improving library service quality: from the perspective of big data	SOFT COMPUTING										Service quality; Decision Making Trial and Evaluation Laboratory (DEMATEL); LibQUAL; Decision making; Big data	IMPORTANCE-PERFORMANCE ANALYSIS; DECISION-MAKING; MCDM MODEL; PERCEPTIONS; INFORMATION; UNIVERSITY; SELECTION; PLUS; REASSESSMENT; SATISFACTION	This study aims to expose the "causal core" items of LibQUAL(+)(TM) from professional librarians' views. The objective is to prioritize limited resources to improve the most influential factors affecting the library service quality. Valid LibQUAL(+)(TM) questionnaires from thirty-two university library administrators in Taiwan are collected for analysis. The Decision Making Trial and Evaluation Laboratory (DEMATEL) technique is employed to build causal maps that can clearly locate the twenty-two LibQUAL(+)(TM) question items into four quadrants: core causal, inferior causal, inferior effect, and core effect. The DEMATEL causal maps can facilitate the library administrators to prioritize the limited resources for improvement. Five "causal core" items of LibQUAL(+)(TM) are identified, including the library staff should be motivated with high willingness to help users and giving users individual attention; the library amenities should provide a quiet space for individual activities; the library Web site should allow users to locate and access information easily and individually. The survey data represent what professional librarians' think of the service quality in their libraries. The findings may serve as useful guidance to allocate limited budgets for library service improvement. This paper has pinpointed the "core causal" items of LibQUAL(+)(TM) for advancing library administration. It is the first of its kind in the literature to use the DEMATEL technique to identify the "core causal" items of LibQUAL(+)(TM) for improving library service quality.																	1432-7643	1433-7479				APR	2020	24	8			SI		5729	5739		10.1007/s00500-019-04308-5													
J								Application of Kalman filter to Model-based Prognostics for Solenoid Valve	SOFT COMPUTING										Kalman filter; Prognostics; Remaining useful life; Solenoid valve	LIFE PREDICTION; DEGRADATION; RELIABILITY; DESIGN; STATE	Solenoid valves (SVs) are electromechanical components, which are used as actuators in various application environments and play crucial roles in control systems, and their breakdown may result in a system crash. Therefore, this paper explores a Kalman filter (KF)-based method to predict the remaining useful life (RUL) of SVs, so that the SVs can be replaced or maintained before their failure bringing a catastrophic consequence for engineering system. In this paper, a degradation signal is extracted from the driven current, which can be monitored conveniently with a non-contact current sensor. Based on an empirical linear degradation model, the KF is adopted to track the degradation state and the degradation rate and to capture the uncertainties. The Monte Carlo sampling and kernel density estimation are used to propagate the uncertainties and estimate the probability distribution of RUL, respectively. To verify our methods, a degradation experiment is designed. The experiment results show that the degradation signal extracted from the driven current can indeed reflect the degradation state of SVs. By comparing the proposed method with other state of the arts prognostic approaches, it shows that the proposed KF method preforms better and has a higher prediction accuracy than other methods.																	1432-7643	1433-7479				APR	2020	24	8			SI		5741	5753		10.1007/s00500-019-04311-w													
J								A watermarking scheme based on rotating vector for image content authentication	SOFT COMPUTING										Content authentication; JPEG; JPEG2000; Semi-fragile watermark; Tampering localization	SEMI-FRAGILE WATERMARKING; ROBUST; ATTACKS	Semi-fragile watermarking technique for digital image, as a technology for content authentication, aims at telling malicious tampering from content-preserving operators. However, with the resolution of imaging sensors increasing and the explosive growth of digital images on the internet, before authorization, watermarked images tend to undergo such double-compression environments as: first compression (JPEG/JPEG2000) for release, decoding, application processing (conventional signal processing such as nosing, filtering, cropping, and scaling/security attack/malicious tampering), and second compression (JPEG2000/JPEG) for release again. In this paper, based on rotating vector, we propose a novel watermarking expression method that can describe carrier semantics to a certain extent and its modulation algorithm for digital image, and analyze the stability of the watermarked data theoretically. Then, a semi-fragile watermarking scheme is proposed for image content authentication to effectively distinguish malicious content manipulation from content-preserving operations in the double-compression application environment, which expands the application scope of content authentication based on watermarking technology.																	1432-7643	1433-7479				APR	2020	24	8			SI		5755	5772		10.1007/s00500-019-04318-3													
J								A dynamic trust model in internet of things	SOFT COMPUTING										Trust model; Trust prediction; Internet of things; Markov chain	REPUTATION; MANAGEMENT; IOT	Indicating trust or distrust of a node is a key issue in the trust management of IoT. However, there are some challenges for the management with anonymous nodes, inaccurate communication, etc. Due to huge number of nodes in IoT, one of the possible solutions is predicting trust and distrust values. In this paper, we propose a dynamic trust model based on direct and indirect trust computation, and the most important part of a model, trust prediction. The prediction method mainly depends on the combination of exponential smoothing and a Markov chain. We employ exponential smoothing to predict trustiness and a Markov chain to fix the deviation of the prediction. To test our scheme, we create a simulation to evaluate its performance and effectiveness. The simulation results are encouraging.																	1432-7643	1433-7479				APR	2020	24	8			SI		5773	5782		10.1007/s00500-019-04319-2													
J								Development and assessment of a haptic-enabled holographic surgical simulator for renal biopsy training	SOFT COMPUTING										Haptic; Holographic; Patient-specific; Renal biopsy; Surgical training	VIRTUAL-REALITY SIMULATOR; ACCESS; SYSTEM	In this paper, a high-immersive surgical training system for renal biopsy using holographic demonstration and haptic feedback is presented to push the limitation of virtual medical training development. Proposed system is including: holographic visual rendering pipeline reconstructed by the patient-specific CT images; haptic rendering pipeline implemented by the dual-hands 6-DOF haptic devices connected with surgical instruments of image guide and immersive 3D display learning operating room environment. Twenty-four medical students and eight experienced thoracic surgeons from the Yunnan First People's Hospital are invited to evaluate our holographic-based training system through the subjective and objective assessment test, respectively. Experiment result from the face, content, improvement, and construct evaluations demonstrated a high performance than the existing VR-based trainer, especially the puncture accuracy of medical students' group, was improved by 30.8% after training.																	1432-7643	1433-7479				APR	2020	24	8			SI		5783	5794		10.1007/s00500-019-04341-4													
J								Research on key issues of gesture recognition for artificial intelligence	SOFT COMPUTING										PCA; SVM; Gesture recognition		Gesture recognition has become a hot spot in the direction of artificial intelligence and has great research significance. At present, some classical algorithms, such as the neural network method and the hidden Markov method, have the disadvantages of large computational complexity and long training time. This paper proposes the support vector machine (SVM) algorithm to realize gesture recognition. In order to make the recognition more accurate, SVM is combined with the principal component analysis (PCA) algorithm, performs the dimensionality reduction on the gesture image to form the PCA + SVM algorithm for gesture recognition. At the same time, a new dynamic gesture recognition processing method is proposed, and its effectiveness is proved by various methods. Using open-source computer vision library (OPENCV), the algorithm is simulated on visual studio 2015 environment. The results show that the algorithm has an excellent recognition effect.																	1432-7643	1433-7479				APR	2020	24	8			SI		5795	5803		10.1007/s00500-019-04342-3													
J								RobNet: real-time road-object 3D point cloud segmentation based on SqueezeNet and cyclic CRF	SOFT COMPUTING										Environment perception; Point cloud segmentation; Deep learning; Computer vision; UAV; Autopilot	EXTRACTION	In order to realize real-time 3D environment perception of UAVs and autopilot in low-altitude complex road scenes, a neural network model RobNet based on SqueezeNet and cyclic CRF for real-time 3D point cloud segmentation is proposed to segment the road objects in real time. Firstly, the unordered, scattered 3D point cloud data are preprocessed into a standard data format similar to an image by a spherical mapping method. Then, at the macro-level of the model design, the SqueezeNet network with the residual connection optimization is selected as the basic network of the model, and then, the conditional random field (CRF) algorithm which is processed into the cyclic network structure is used to refine the segmentation result. Finally, the construction of the basic network, the cyclic network and the network parameter settings in the model is elaborated at the micro-level. The experimental results show that the RobNet model proposed in this paper can segment the target in the road scene better. The segmentation callback rate of the three types of vehicles, pedestrians and cyclists is increased by 28, 2 and 17%, respectively, compared with the VoxelNet network. The higher callback rate is in line with the safe movement specifications for drones and autonomous driving. At the same time, the proposed model parameters are small, 98.5% smaller than the classic network AlexNet, and are easy to deploy on a platform with limited computing resources. The RobNet model in the Robot Operating System (ROS) framework engineering deployment and implementation experimental data shows that the model meets the real-time and stability requirements of the drone and automatic driving application, engineering code can run in real time at 12 Hz, the standard deviation of each frame's running time is around 4.5 ms.																	1432-7643	1433-7479				APR	2020	24	8			SI		5805	5818		10.1007/s00500-019-04355-y													
J								MABAC method for multiple attribute group decision making under picture 2-tuple linguistic environment	SOFT COMPUTING										Multiple attribute group decision making (MAGDM); Picture 2-tuple linguistic sets (P2TLSs); MABAC model; P2TLNs MABAC model; Renewable energy power generation project	AGGREGATION OPERATORS; TODIM METHOD; REPRESENTATION MODEL; STOCK-MARKET; VIKOR METHOD; FUZZY; SELECTION; CHINA	In this article, we extend multi-attributive border approximation area comparison (MABAC) approach to the multiple attribute group decision making with picture 2-tuple linguistic numbers. We review the concept of picture 2-tuple linguistic sets and introduce its corresponding score function, accuracy function, and operational laws. In addition, we propose two aggregation operators of picture 2-tuple linguistic numbers and then develop a method by combining traditional MABAC model with the overall picture 2-tuple linguistic evaluation information. Our proposed method is increasingly accurate and valid even when the conflicting attributes are considered. We also provide a numerical instance for assessing and selecting the renewable energy power generation project to demonstrate the efficacy of our novel model. Finally, we compare our proposed approach with other traditional operators to further show its benefits.																	1432-7643	1433-7479				APR	2020	24	8			SI		5819	5829		10.1007/s00500-019-04364-x													
J								An attention mechanism and multi-granularity-based Bi-LSTM model for Chinese Q&A system	SOFT COMPUTING										NLP; Artificial intelligence; Long short-term memory; Question-answering system	CLOUD	Natural language processing (NLP) is one of the key techniques in intelligent question-answering (Q&A) systems. Although recurrent neural networks and long short-term memory (LSTM) networks exhibit obvious advantages on well-known English Q&A datasets, they still suffer from several defects including indeterminateness, polysemy and the lack of changing morphology in Chinese, which results in complex NLP on large and diverse Chinese Q&A datasets. In this paper, we first analyze limitations of applying LSTM and bidirectional LSTM (Bi-LSTM) models to noisy Chinese Q&A datasets. Then, we focus on integrating attention mechanisms and multi-granularity word segmentation into Bi-LSTM and propose an attention mechanism and multi-granularity-based Bi-LSTM model (AM-Bi-LSTM) which combines the improved attention mechanism with a novel processing of multi-granularity word segmentation to handle the complex NLP in Chinese Q&A datasets. Furthermore, similarity of questions and answers is formulated to implement the quantitative computation which helps to achieve better performance in Chinese Q&A systems. Finally, we verify the proposed model on a noisy Chinese Q&A dataset. The experimental results demonstrate that the novel AM-Bi-LSTM model achieves significant improvement on evaluation metrics of accuracy, mean average precision and so on. Moreover, the experimental results indicate that the novel AM-Bi-LSTM model outperforms baseline methods and other LSTM-based models.																	1432-7643	1433-7479				APR	2020	24	8			SI		5831	5845		10.1007/s00500-019-04367-8													
J								Weighted-fusion feature of MB-LBPUH and HOG for facial expression recognition	SOFT COMPUTING										Facial expression recognition; Feature extraction; Weighted-fusion feature; Support vector machine	INFORMATION; AAM	Obtaining a useful and discriminative feature for facial expression recognition (FER) is a hot research topic in computer vision. In this paper, we propose a novel facial expression representation for FER. Firstly, we select the appropriate parameter of multi-scale block local binary pattern uniform histogram (MB-LBPUH) operator to filter the facial images for representing the holistic structural features. Then, normalizing the filtered images into a uniform basis reduces the computational complexity and remains the full information. An MB-LBPUH feature and a HOG feature are concatenated to fuse a new feature representation for characterizing facial expressions. At the same time, weighting the MB-LBPUH feature can remove the data unbalance from a fusion feature. The weighted-fusion feature reflects not only global facial expressions structure patterns but also characterizes local expression texture appearance and shape. Finally, we utilize principal component analysis for dimensionality reduction and employ support vector machine to classification. Experimental results demonstrate that the proposed algorithm exhibits superior performance compared with the existing algorithms on JAFFE, CK+, and BU-3DFE datasets.																	1432-7643	1433-7479				APR	2020	24	8			SI		5859	5875		10.1007/s00500-019-04380-x													
J								Adaptive wavelet transform model for time series data prediction	SOFT COMPUTING										Stock price prediction; Wavelet transform; Adaptive; Long short-term memory	HYBRID; PERFORMANCE	With the development of cloud computing and big data, stock prediction has become a hot topic of research. In the stock market, the daily trading activities of stocks are carried out at different frequencies and cycles, resulting in a multi-frequency trading mode of stocks , which provides useful clues for future price trends: short-term stock forecasting relies on high-frequency trading data, while long-term forecasting pays more attention to low-frequency data. In addition, stock series have strong volatility and nonlinearity, so stock forecasting is very challenging. In order to explore the multi-frequency mode of the stock , this paper proposes an adaptive wavelet transform model (AWTM). AWTM integrates the advantages of XGboost algorithm, wavelet transform, LSTM and adaptive layer in feature selection, time-frequency decomposition, data prediction and dynamic weighting. More importantly, AWTM can automatically focus on different frequency components according to the dynamic evolution of the input sequence, solving the difficult problem of stock prediction. This paper verifies the performance of the model using S&P500 stock dataset. Compared with other advanced models, real market data experiments show that AWTM has higher prediction accuracy and less hysteresis.																	1432-7643	1433-7479				APR	2020	24	8			SI		5877	5884		10.1007/s00500-019-04400-w													
J								Cloud-assisted secure biometric identification with sub-linear search efficiency	SOFT COMPUTING										Cloud computing; Privacy; Biometric identification; R-tree	ENCRYPTED DATA; ACCESS-CONTROL	Cloud computing has been one of the critical solutions to reduce heavy storage and computation burden of biometric identification. To protect the privacy of biometric data against untrusted cloud servers, outsourced biometric databases are usually encrypted by users. Performing biometric identification over encrypted data without revealing privacy to cloud servers attracts more and more attention. Several secure biometric identification solutions have been proposed to solve this challenging problem. However, these schemes still suffer from various limitations, such as low search efficiency and heavy computation burden on users. In this paper, we propose a novel cloud-assisted biometric identification scheme based on the asymmetric scalar-product preserving encryption (ASPE) and spatial data structures such as the R-tree index, which simultaneously achieves sub-linear search efficiency and low computation burden on users. Specifically, we construct an R-tree index on the biometric dataset and encrypt the index with ASPE. Then we modify the original search algorithm in the R-tree index and design a secure search algorithm based on ASPE to find the nearest neighbor result over the encrypted R-tree index. Through theoretical analysis and extensive experiments, we demonstrate the effectiveness and efficiency of our proposed approach.																	1432-7643	1433-7479				APR	2020	24	8			SI		5885	5896		10.1007/s00500-019-04401-9													
J								Small-scale moving target detection in aerial image by deep inverse reinforcement learning	SOFT COMPUTING										Aerial image; Deep inverse reinforcement; Small-scale target detection	GRADIENT	It proposes a deep inverse reinforcement learning method for slow and weak moving targets detection in aerial video. Differential gray images of adjacent frames are used as the network model input, and the feature network layer extracts the candidate moving target regions through the multi-layer convolution. The candidate target information is used as the initial layer of the policy network. The expert trajectory is used to adjust and optimize the feature convolution network model and the policy fully connected network model to realize the training the reward return function and the expert policy. In the stage of autonomous improvement policy, the policy model is re-optimized by unmarked aerial video, and deep inverse reinforcement learning and nonlinear policy network are used to make decision on moving target position and size information. The target size of the multi-group aerial video test set is 10 * 10 pixels. Experimental results show that the proposed algorithm has the advantage of the nonlinear policy of the neural network compared with the traditional moving target detection algorithm, and the detection result is more accurate. At the same time, compared with the traditional marginal programming (MMP) method and the structured classification based (SCIRL) method, the proposed algorithm shows obvious advantages in the accuracy of aerial video moving target detection.																	1432-7643	1433-7479				APR	2020	24	8			SI		5897	5908		10.1007/s00500-019-04404-6													
J								Random orthocenter strategy in interior search algorithm and its engineering application	SOFT COMPUTING										Random orthocenter strategy; Levy flight; Control parameters; ROISA; Global optimization	OPTIMIZATION ALGORITHM; DESIGN; ISA	Determining how to improve the global search ability and adaptability of an algorithm without reducing the convergence speed is still a major challenge for most meta-heuristic algorithms. This paper proposes a new random orthocenter strategy combined with a Levy flight strategy to improve the interior search algorithm (ISA). The random orthocenter strategy is to randomly select a point outside the element and mirror to form a triangle and to solve the image of the element based on the orthocentre, which offsets the unique control parameters in the algorithm. The Levy flight strategy further prevents the algorithm from falling into local optimization. Thirteen benchmark functions and two engineering problems are selected for simulation tests. The experimental results show that the random orthocenter ISA significantly improves the global optimization and adaptability and has advantages on application in complex practical engineering optimization problems.																	1432-7643	1433-7479				APR	2020	24	8			SI		5933	5948		10.1007/s00500-019-04498-y													
J								Weakly supervised facial expression recognition via transferred DAL-CNN and active incremental learning	SOFT COMPUTING										Deep convolutional neural network; Facial expression recognition; Two-stage transfer learning; Weakly supervised active incremental learning		In recent years, facial expression recognition (FER) has becoming a growing topic in computer vision with promising applications on virtual reality and human-robot interaction. Due to the influence of illumination, individual differences, attitude variation, etc., facial expression recognition with robust accuracy in complex environment is still an unsolved problem. Meanwhile, with the wide use of social communication, massive data are uploaded to the Internet; the effective utilization of those data is still a challenge due to noisy label phenomenon in the study of FER. To resolve the above-mentioned problems, firstly, a double active layer-based CNN is established to recognize the facial expression with high accuracy by learning robust and discriminative features from the data, which could enhance the robustness of network. Secondly, an active incremental learning method was utilized to tackle the problem of using Internet data. During the training phase, a two-stage transfer learning method is explored to transfer the relative information from face recognition to FER task to alleviate the inadequate training data in deep convolution network. Besides, in order to make better use of facial expression data from Web site and further improve the FER accuracy, Unconstrained Facial Expression Database from Web site database is built in this paper. Extensive experiments performed on two public facial expression recognition databases FER 2013 and SFEW 2.0 have demonstrated that the proposed scheme outperforms the state-of-the-art methods, which could achieve 67.08% and 51.90%, respectively.																	1432-7643	1433-7479				APR	2020	24	8			SI		5971	5985		10.1007/s00500-019-04530-1													
J								DroidDeep: using Deep Belief Network to characterize and detect android malware	SOFT COMPUTING										Android malicious app; Deep Belief Network; Static analysis; Behavioral characteristics		Android operating system and corresponding applications (app) are becoming increasingly popular, because the characteristics (open source, support the third-party app markets, etc.) of the Android platform, which cause the amazing pace of Android malware, poses a great threat to this platform. To solve this security issue, a comprehensive and accurate detection approach should be designed. Many research works dedicate to achieve this goal, including code analysis and machine learning methods, but these kinds of works cannot analyze large amount of Android applications comprehensively and effectively. We propose DroidDeep, which uses a Deep Belief Network model to classify Android malicious app. This proposed approach first collects 11 different kinds of static behavioral characteristics from a large amount of Android applications. Second, we design a Deep Belief Network algorithm to select unique behavioral characteristics from the collected static behavioral characteristics. Third, we detect zero-day Android malicious applications based on selected behavioral characteristics. We choose a dataset which mix with Android benign and malicious applications to evaluate the proposed method. The laboratory results show that the proposed method can obtain a higher detection accuracy (99.4%). Moreover, the proposed approach costs 6 s in average when analyzing and detecting each Android application.																	1432-7643	1433-7479				APR	2020	24	8			SI		6017	6030		10.1007/s00500-019-04589-w													
J								An improved MPPT control strategy based on incremental conductance method	SOFT COMPUTING										Maximum power point tracking (MPPT); Constant voltage method; Incremental conductance (INC); Particle swarm optimization (PSO)	ALGORITHM	Photovoltaic cells efficiency can be effectively improved by maximum power point tracking (MPPT) technology. An improved MPPT control strategy is proposed to solve the current problems of poor convergence speed and accuracy of incremental conductance method. In this method, the P-U characteristic curve is divided into three sections: non-MPP sections, MPP-like section and MPP sections. In the non-MPP section, the constant voltage method is adopted to reduce the tracking time. In the MPP-like section, the incremental conductance method is adopted and its step size is improved, which effectively reduces the tracking time. In MPP section, particle swarm algorithm is adopted to improve tracking accuracy. Taking light intensity and temperature variation as examples, the proposed method and the traditional method are simulated respectively. Simulation results show that compared with the constant voltage method, the accuracy can be improved by more than 4% when the temperature or light intensity is changed, while maintaining the tracking speed. Compared with the traditional incremental conductivity method, the method can reduce the tracking time by 33% and improve the tracking accuracy by 1% when the light intensity or temperature changes.																	1432-7643	1433-7479				APR	2020	24	8			SI		6039	6046		10.1007/s00500-020-04723-z													
J								Trade-off between exploration and exploitation with genetic algorithm using a novel selection operator	COMPLEX & INTELLIGENT SYSTEMS										Genetic algorithm; Selection pressure; Selection operators; Statistical analysis; Traveling salesman problem	TRAVELING SALESMAN PROBLEM; SCHEDULING PROBLEM; OPTIMIZATION; ADAPTATION; CROSSOVER	As an intelligent search optimization technique, genetic algorithm (GA) is an important approach for non-deterministic polynomial (NP-hard) and complex nature optimization problems. GA has some internal weakness such as premature convergence and low computation efficiency, etc. Improving the performance of GA is a vital topic for complex nature optimization problems. The selection operator is a crucial strategy in GA, because it has a vital role in exploring the new areas of the search space and converges the algorithm, as well. The fitness proportional selection scheme has essence exploitation and the linear rank selection is influenced by exploration. In this article, we proposed a new selection scheme which is the optimal combination of exploration and exploitation. This eliminates the fitness scaling issue and adjusts the selection pressure throughout the selection phase. The chi 2 goodness-of-fit test is used to measure the average accuracy, i.e., mean difference between the actual and expected number of offspring. A comparison of the performance of the proposed scheme along with some conventional selection procedures was made using TSPLIB instances. The application of this new operator gives much more effective results regarding the average and standard deviation values. In addition, a two-tailed t test is established and its values showed the significantly improved performance by the proposed scheme. Thus, the new operator is suitable and comparable to established selection for the problems related to traveling salesman problem using GA.																	2199-4536	2198-6053				APR	2020	6	1					1	14		10.1007/s40747-019-0102-7													
J								On some distance measures of complex Pythagorean fuzzy sets and their applications in pattern recognition	COMPLEX & INTELLIGENT SYSTEMS										Complex fuzzy set; Complex intuitionistic fuzzy set; Complex Pythagorean fuzzy set; Distance measures	SIMILARITY MEASURES	The concept of complex fuzzy set (CFS) and complex intuitionistic fuzzy set (CIFS) is two recent developments in the field of fuzzy set (FS) theory. The significance of these concepts lies in the fact that these concepts assigned membership grades from unit circle in plane, i.e., in the form of a complex number instead from [0, 1] interval. CFS cannot deal with information of yes and no type, while CIFS works only for a limited range of values. To deal with these kinds of problems, in this article, the concept of complex Pythagorean fuzzy set (CPFS) is developed. The novelty of CPFS lies in its larger range comparative to CFS and CIFS which is demonstrated numerically. It is discussed how a CFS and CIFS could be CPFS but not conversely. We investigated the very basic concepts of CPFSs and studied their properties. Furthermore, some distance measures for CPFSs are developed and their characteristics are studied. The viability of the proposed new distance measures in a building material recognition problem is also discussed. Finally, a comparative study of the proposed new work is established with pre-existing study and some advantages of CPFS are discussed over CFS and CIFS.																	2199-4536	2198-6053				APR	2020	6	1					15	27		10.1007/s40747-019-0103-6													
J								Pythagorean Dombi fuzzy graphs	COMPLEX & INTELLIGENT SYSTEMS										Pythagorean fuzzy sets; T-norms and T-conorms; Pythagorean Dombi fuzzy graphs; Regularity of Pythagorean Dombi fuzzy graphs	DECISION-MAKING; MEMBERSHIP GRADES; OPERATORS; TOPSIS; SETS	Pythagorean fuzzy graph, a broadly used extension of fuzzy and intuitionistic fuzzy graph, is helpful in representing structural relationships between several objects where the relation between these objects is vague, while the Dombi operators with operational parameters have excellent flexibility. Utilizing these two concepts, this research paper proposes the novel concept of Pythagorean Dombi fuzzy graphs (PDFGs). Basically, graph terminology is employed for introducing Pythagorean fuzzy analogs of various fundamental graphical ideas using Dombi operator. Further, under Pythagorean Dombi fuzzy environment, regular, totally regular, strongly regular and biregular graphs are defined with appropriate illustration and some of their crucial properties are examined. Meanwhile, the notion of edge regularity of PDFG is also initiated with substantial characteristics. Finally, a numerical example related to evaluation of appropriate ETL software for a business intelligence project is presented to better understand PDFGs.																	2199-4536	2198-6053				APR	2020	6	1					29	54		10.1007/s40747-019-0109-0													
J								Accelerating evolutionary computation using a convergence point estimated by weighted moving vectors	COMPLEX & INTELLIGENT SYSTEMS										Evolutionary computation; Estimation of convergence point; Acceleration; Weight-based estimation		We introduce weighted moving vectors to increase the accuracy of estimating a convergence point of population and evaluate its efficiency. Key point is to weight moving vectors according to their reliability when a convergence point is calculated instead of equal weighting of the original method. We propose two different methods to evaluate the reliability of moving vectors. The first approach uses the fitness gradient information between starting points and terminal points of moving vectors for their weights. When a fitness gradient is bigger, the direction of a moving vector may have more potential, and a higher weight is given to it. The second one uses the fitness of parents, i.e., starting points of moving vectors, to give weights for moving vectors. Because an individual with higher fitness may have a high probability of being close to the optimal area, it should be given a higher weight, vice versa. If the estimated point is better than the worst individual in current population, it is used as an elite individual and replace the worst one to accelerate the convergence of evolutionary algorithms. To evaluate the performance of our proposal, we employ differential evolution and particle swarm optimization as baseline algorithms in our evaluation experiments and run them on 28 benchmark functions from CEC 2013. The experimental results confirmed that introducing weights can further improve the accuracy of an estimated convergence point, which helps to make EC search faster. Finally, some open topics are given to discuss.																	2199-4536	2198-6053				APR	2020	6	1					55	65		10.1007/s40747-019-0111-6													
J								A new similarity measure for Pythagorean fuzzy sets	COMPLEX & INTELLIGENT SYSTEMS										Pythagorean fuzzy value; Pythagorean fuzzy sets; Similarity	CRITERIA DECISION-MAKING; AGGREGATION OPERATORS; ACCURACY FUNCTION; EXTENSION; TOPSIS	One of the methods of studying on two sets is to calculate the similarity of two sets. Triangular norms and conorms generalize the basic connectives between fuzzy sets, intuitionistic fuzzy sets, Pythagorean fuzzy sets. In this paper we used triangular conorms (S-norm). The advantage of using S-norm is that the similarity order does not change using different norms. In fact, we are looking for a new definition for calculating the similarity of two Pythagorean fuzzy sets. To achieve this goal, using an S-norm, we first present a formula for calculating the similarity of two Pythagorean fuzzy values, so that they are truthful in similarity properties. Following that, we generalize a formula for calculating the similarity of the two Pythagorean fuzzy sets which prove truthful in similarity conditions. Finally, we give some examples of this method.																	2199-4536	2198-6053				APR	2020	6	1					67	74		10.1007/s40747-019-0114-3													
J								Improvement of query-based text summarization using word sense disambiguation	COMPLEX & INTELLIGENT SYSTEMS										Common sense knowledge; Expanding the query terms; Query-based text summarization; Semantic relatedness; Word sense disambiguation	CONCEPTNET; MODELS; GRAPH	In this paper, a query-based text summarization method is proposed based on common sense knowledge and word sense disambiguation. Common sense knowledge is integrated here by expanding the query terms. It helps in extracting main sentences from text document according to the query. Query-based text summarization finds semantic relatedness score between query and input text document for extracting sentences. The drawback with current methods is that while finding semantic relatedness between input text and query, in general they do not consider the sense of the words present in the input text sentences and the query. However, this particular method can enhance the summary quality as it finds the correct sense of each word of a sentence with respect to the context of the sentence. The correct sense for each word is being used while finding semantic relatedness between input text and query. To remove similar sentences from summary, similarity measure is computed among the selected sentences. Experimental result shows better performance than many baseline systems.																	2199-4536	2198-6053				APR	2020	6	1					75	85		10.1007/s40747-019-0115-2													
J								Towards an assessment framework of reuse: a knowledge-level analysis approach	COMPLEX & INTELLIGENT SYSTEMS										Reuse; Reuse process modelling; Software development ontology; Components' reuse; Ontologies		The process of assessing the suitability of reuse of a software component is complex. Indeed, software systems are typically developed as an assembly of existing components. The complexity of the assessment process is due to lack of clarity on how to compare the cost of adaptation of an existing component versus the cost of developing it from scratch. Indeed, often pursuit of reuse can lead to excessive rework and adaptation, or developing suites of components that often get neglected. This paper is an important step towards modelling the complex reuse assessment process. To assess the success factors that can underpin reuse, we analyze the cognitive factors that belie developers' behavior during their decision-making when attempting to reuse. This analysis is the first building block of a broader aim to synthesize a framework to institute activities during the software development lifecycle to support reuse.																	2199-4536	2198-6053				APR	2020	6	1					87	95		10.1007/s40747-019-0116-1													
J								Moment capacity estimation of spirally reinforced concrete columns using ANFIS	COMPLEX & INTELLIGENT SYSTEMS										Flexure failure; Moment capacity; Neuro-fuzzy system; Reinforced concrete; Spiral-reinforced concrete column	LONGITUDINAL REINFORCEMENT; PREDICTION; BEHAVIOR	This paper presents a predictive model based on adaptive neuro-fuzzy inference system namely ANFIS to determine the moment capacity of spiral-reinforced concrete columns. For this purpose, five input parameters including the longitudinal reinforcement index, transverse reinforcement index, axial force, diameter to length ratio and also shear force were considered to estimate the moment capacity. A collection of experimental database was applied to train and test the proposed system. This database includes 82 spiral-reinforced concrete columns (with flexure failure) which were reported in the literature and modified by PEER as a uniform database of cantilever columns. The model is created by fuzzy C-means algorithm with four cluster and Gaussian membership functions, also trained and tested by 70 and 12 datasets, respectively. It was concluded that the model of this study with high accuracy could be able to estimate the moment capacity.																	2199-4536	2198-6053				APR	2020	6	1					97	107		10.1007/s40747-019-00118-2													
J								Evaluating the sustainability of a smart technology application to mobile health care: the FGM-ACO-FWA approach	COMPLEX & INTELLIGENT SYSTEMS										Mobile; Health care; Smart technology; Fuzzy geometric mean; alpha-Cut operations; Fuzzy weighted average; Sustainability	ANT COLONY OPTIMIZATION; LOCATION-AWARE SERVICE; FUZZY; SENSOR; SYSTEM	Smart technologies present numerous opportunities for enhancing mobile health care. However, existing applications of smart technologies to mobile health care face several difficulties. As a result, whether a smart technology application to mobile health care will be sustainable is questionable. To address this issue, the fuzzy geometric mean (FGM)-alpha-cut operations (ACO)-fuzzy weighted average (FWA) approach is proposed in this study. In the proposed methodology, at first FGM is applied to aggregate multiple experts' opinions on the relative importance of a critical factor. Then, ACO is applied to derive the absolute fuzzy importance level of the critical factor. At last, FWA is applied to assess the sustainability of the smart technology application to mobile health care. The proposed methodology has been applied to assess the sustainability of thirteen smart technology applications to mobile health care. According to the experimental results, the most and least sustainable smart technology applications to mobile health care were smart mobile service and smart clothes, respectively. In addition, the ranking result using the proposed methodology was somewhat different from those using existing methods based on approximation.																	2199-4536	2198-6053				APR	2020	6	1					109	121		10.1007/s40747-019-00119-1													
J								Using compression to find interesting one-dimensional cellular automata	COMPLEX & INTELLIGENT SYSTEMS										Machine Learning; Clustering; Interestingness; Cellular Automata; PPM Compression		This paper proposes a novel method for finding interesting behaviour in complex systems based on compression. A new clustering algorithm has been designed and applied specifically for clustering 1D elementary cellular automata behaviour using the prediction by partial matching (PPM) compression scheme, with the results gathered to find interesting behaviours. This new algorithm is then compared with other clustering algorithms in Weka and the new algorithm is found to be more effective at grouping behaviour that is visually similar in output. Using PPM compression, the rate of change of the cross-entropy with respect to time is calculated. These values are used in combination with a clustering algorithm, such as k-means, to create a new set of clusters for cellular automata. An analysis of the data in each cluster is then used to determine if a cluster can be classed as interesting. The clustering algorithm itself was able to find unusual behaviours, such as rules 167 and 181 which have output that is slightly different from all the other Sierpinski Triangle-like patterns, because their apexes are off-centre by one cell. When comparing the new algorithm with other established ones, it was discovered that the new algorithm was more effective in its ability to group interesting and unusual cellular automata behaviours together.																	2199-4536	2198-6053				APR	2020	6	1					123	146		10.1007/s40747-019-00121-7													
J								High efficiency fault-detection and fault-tolerant control approach in Tennessee Eastman process via fuzzy-based neural network representation	COMPLEX & INTELLIGENT SYSTEMS										Tennessee Eastman process; Fault-detection; Fault-tolerant control; Fusion classifier; Fuzzy-based neural network representation	OPTIMIZATION; ALGORITHM; PLANT; FRAMEWORK; FUSION; LOGIC	We looked at the background of fault-detection and fault-tolerant control algorithms to propose a new high efficiency one with a focus on Tennessee Eastman process through fuzzy-based neural network representation. Due to the fact that the open-loop system may not be stabilized, an advanced control strategy to generate proper control signals needs to be designed. At first, to detect and identify the fault, data preprocessing theories have been considered. Based upon the matter disclosed, to provide a reliable decision-maker block, fusion classifier idea has been realized. For this one, raw data, time, and frequency characteristics are divided into various classification tools and finally the obtained knowledge combination regarding each one of them is adopted. It should be noted that the proposed implementation tools are taken into real consideration as the fuzzy-based neural network representation. Subsequently, the fault-tolerant control approach based on local controller regulation in case of each fault occurrence has been researched, which the investigated outcomes emphasize the effectiveness of the approach proposed here.																	2199-4536	2198-6053				APR	2020	6	1					199	212		10.1007/s40747-019-0094-3													
J								TOPSIS Method for Developing Supplier Selection with Probabilistic Linguistic Information	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multiple attribute group decision-making (MAGDM); Probabilistic linguistic term sets (PLTSs); TOPSIS method; Incomplete weight information; Green supplier selection	GROUP DECISION-MAKING; MEAN OPERATORS; TODIM METHOD; AGGREGATION OPERATORS; FUZZY-SETS; TERM SETS; ENTROPY; SIMILARITY; APPRAISAL; DISTANCE	In this paper, we investigate the probabilistic linguistic multiple attribute group decision-making (MAGDM) with incomplete weight information. In this method, the linguistic term sets (LTSs) is converted into probabilistic linguistic term sets (PLTSs). For deriving the weight information of the attribute, an optimization model is built on the basis of the fundamental idea of conventional TOPSIS method, by which the attribute weights can be decided. In addition, the optimal alternative(s) is decided by computing the shortest distance from the probabilistic linguistic positive ideal solution (PLPIS) and on the other side the farthest distance of the probabilistic linguistic negative ideal solution (PLNIS). The method has precise trait in probabilistic linguistic information processing. The information distortion and losing was avoided which happen formerly in the probabilistic linguistic information processing. In the end, a case study for green supplier selection is given to demonstrate the merits of the developed method. The results display that the approach is uncomplicated, valid and simple to compute.																	1562-2479	2199-3211				APR	2020	22	3					749	759		10.1007/s40815-019-00797-6													
J								A New Uncertainty Measure of Discrete Z-numbers	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Uncertainty measure; Z-numbers; Shannon entropy; Linguistic variable; Fuzzy numbers	DIVERGENCE MEASURE; FUZZY; ENTROPY; METHODOLOGY; MEMBERSHIP; TOPSIS; MODEL; SET	Today's modern decision-making problem is designated by not being the most effective fuzziness; however, additionally partial reliability also plays a crucial role. The incomplete and unreliable information may also affect the selection maker to earn inaccurate decisions, ensuing in monetary losses and wastes of resources. Thus, it is vital to describe the reliability of the facts. To cope with it entirely, a notion of Z-number, i.e., a pair of fuzzy sets modeling a probability-qualified fuzzy statement, is the most suitable medium to access it. In this paper, we spout a new technique to measure the uncertainty of discrete Z-numbers based on Shannon entropy. In the given approach, by using characteristics of Z-number, all the potential probability distributions are estimated by the maximum entropy method. Then, a new fuzzy subset of the Z-number is formed based on the probability distributions and the membership functions of the fuzzy number. Finally, the centroid of the formulated set is determined to rank the degree of the uncertainty of Z-number. The applicability of the delivered approach is read with some numerical examples related to the decision-making process.																	1562-2479	2199-3211				APR	2020	22	3					760	776		10.1007/s40815-020-00819-8													
J								Interval Fuzzy c-Regression Models with Competitive Agglomeration for Symbolic Interval-Valued Data	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Symbolic data analysis; Interval-valued data; Interval fuzzy c-regression models; Competitive agglomeration	NETWORKS	In this study, a novel approach, interval fuzzy c-regression models with competitive agglomeration (IFCRMCA), is proposed to deal with the symbolic interval-valued data. The proposed IFCRMCA approach can identify the partition of the interval-valued data using both the distances to the cluster centers and the errors of interval regression models for each cluster. Due to the concepts of competitive agglomeration is used in the proposed approach, the pre-determination of the cluster number in the proposed IFCRMCA is not necessary. Various real experiments are carried on and the experimentally results shows that the proposed approaches are superior to the existing approaches.																	1562-2479	2199-3211				APR	2020	22	3					891	900		10.1007/s40815-020-00816-x													
J								Color-Based Image Segmentation by Means of a Robust Intuitionistic Fuzzy C-means Algorithm	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Color image segmentation; Intuitionistic Fuzzy C-Means clustering; Lorentzian Redescending M-estimator	MEANS CLUSTERING-ALGORITHM; M-ESTIMATOR; EXTRACTION; HISTOGRAM; REGION	To yield well-suited image segmentation results, conventional clustering algorithms depend on customized hand-crafted features as well as an appropriate initialization process. This latter aspect is a mandatory pre-requisite for convergence of the algorithm, in other words, its efficiency impacts the quality of the result. In this work, we introduce an Intuitionistic Fuzzy C-Means clustering algorithm enhanced by means of Robust Statistics, which develops an outstanding image segmentation based on a basic feature such as the color information, and it requires a reduced iteration number to converge. The non-parametric Lorentzian Redescending M-estimator is used both at initialization and iterative stages of the clustering algorithm; since, it behaves such as a robust location estimator when the centroid vector is computed, and as a weighting when the membership matrix is updated. With the fusion of both techniques, we can guarantee that the introduced clustering algorithm can efficiently develop the task of segmentation of color images and pattern recognition processes. The robustness and effectiveness of this proposal is verified by experiments on the natural color images BSDS500 dataset, as well as a simulated dataset corrupted with atypical data.																	1562-2479	2199-3211				APR	2020	22	3					901	916		10.1007/s40815-020-00824-x													
J								An adaptive update model based on improved Long Short Term Memory for online prediction of vibration signal	JOURNAL OF INTELLIGENT MANUFACTURING										Vibration signal predicting; LSTM network; Test error; Model update; Online learning	NEURAL-NETWORKS	In industrial production, the characteristics of compressor vibration signal change with the production environment and other external factors. Therefore, to ensure the effectiveness of the model, the vibration signal prediction model needs to be updated constantly. Due to the complex structure of Long Short Term Memory (LSTM) network, the LSTM model is difficult to adapt to the scene of online update. Therefore, the update model based on LSTM is difficult to respond quickly to data changes, which affects the accuracy of the model. To solve this problem, the online learning algorithm is introduced into prediction model, Error-LSTM (E-LSTM) model is proposed in this paper. The main idea of E-LSTM model is to improve the accuracy and efficiency of the model according to test error of the model. First, the hidden layer neurons of LSTM network are divided into blocks, and only part of the modules are activated at each time step. The number of modules activated is determined by test error. Thus, the training speed of the model is accelerated and the efficiency of the model is improved. Second, the E-LSTM model can adaptively adjust the training method according to the data distribution characteristics, so as to improve the accuracy of updated model. In experimental part, two types of datasets are used to verify the performance of the proposed model. LSTM model is used for comparative experiments, and the results showed that the updating model based on E-LSTM is better than that based on LSTM in terms of model accuracy and efficiency.																	0956-5515	1572-8145															10.1007/s10845-020-01556-3		APR 2020											
J								Quantum based Whale Optimization Algorithm for wrapper feature selection	APPLIED SOFT COMPUTING										Quantum; Whale Optimization Algorithm; Bio-inspired technique; Evolutionary techniques; Swarm based techniques; Feature selection	GENETIC ALGORITHMS; FEATURE SUBSET; CLASSIFICATION; EVOLUTIONARY; DATASETS	In this paper, we propose the Quantum Whale Optimization Algorithm (QWOA) for feature selection, which is an amalgamation of the Quantum Concepts and the Whale Optimization Algorithm (WOA). The proposed method enhances the exploratory and exploitation power of the classical WOA, with the use of quantum bit representation of the individuals of the population and the quantum rotation gate operator as a variation operator. Modified mutation and crossover operators are also introduced for quantum-based exploration, shrinking and spiral movement of the whales in the proposed QWOA. The efficacy of the proposed method is compared with that of the conventional WOA and with well-known evolutionary, swarm and quantum algorithms with fourteen datasets from diversified domains. Experimental results demonstrate the superior performance of the proposed QWOA method. Statistical tests also demonstrate the significantly better performance of the QWOA in comparison to eight well-known meta-heuristic algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106092	10.1016/j.asoc.2020.106092													
J								Multi-objective path planning of an autonomous mobile robot using hybrid PSO-MFB optimization algorithm	APPLIED SOFT COMPUTING										Autonomous mobile robot; Robot path planning; Particle swarm optimization; Bat algorithm; Collision avoidance	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; NAVIGATION; AVOIDANCE	The main aim of this paper is to solve a path planning problem for an autonomous mobile robot in static and dynamic environments. The problem is solved by determining the collision-free path that satisfies the chosen criteria for shortest distance and path smoothness. The proposed path planning algorithm mimics the real world by adding the actual size of the mobile robot to that of the obstacles and formulating the problem as a moving point in the free-space. The proposed algorithm consists of three modules. The first module forms an optimized path by conducting a hybridized Particle Swarm Optimization-Modified Frequency Bat (PSO-MFB) algorithm that minimizes distance and follows path smoothness criteria. The second module detects any infeasible points generated by the proposed hybrid PSO-MFB Algorithm by a novel Local Search (LS) algorithm integrated with the hybrid PSO-MFB algorithm to be converted into feasible solutions. The third module features obstacle detection and avoidance (ODA), which is triggered when the mobile robot detects obstacles within its sensing region, allowing it to avoid collision with obstacles. The simulation results indicate that this method generates an optimal feasible path even in complex dynamic environments and thus overcomes the shortcomings of conventional approaches such as grid methods. Moreover, compared to recent path planning techniques, simulation results show that the proposed hybrid PSO-MFB algorithm is highly competitive in terms of path optimality. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106076	10.1016/j.asoc.2020.106076													
J								Towards the use of vector based GP to predict physiological time series	APPLIED SOFT COMPUTING										Ventilation; Physiological data; Machine learning; Genetic programming; Time series	HEART-RATE; VENTILATION; REGRESSION	Prediction of physiological time series is frequently approached by means of machine learning (ML) algorithms. However, most ML techniques are not able to directly manage time series, thus they do not exploit all the useful information such as patterns, peaks and regularities provided by the time dimension. Besides advanced ML methods such as recurrent neural network that preserve the ordered nature of time series, a recently developed approach of genetic programming, VE-GP, looks promising on the problem in analysis. VE-GP allows time series as terminals in the form of a vector, including new strategies to exploit this representation. In this paper we compare different ML techniques on the real problem of predicting ventilation flow from physiological variables with the aim of highlighting the potential of VE-GP. Experimental results show the advantage of applying this technique in the problem and we ascribe the good performances to the ability of properly catching meaningful information from time series. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106097	10.1016/j.asoc.2020.106097													
J								Multi-objective Cartesian Genetic Programming optimization of morphological filters in navigation systems for Visually Impaired People	APPLIED SOFT COMPUTING										Genetic programming; Cartesian Genetic Programming; Multi-objective optimization; NSGA-II; Mathematical morphology	ALGORITHM	Navigation systems for Visually Impaired People (VIP) have improved in the last decade, incorporating many features to ensure navigation safety. Such systems often use grayscale depth images to segment obstacles and paths according to distances. However, this approach has the common problem of unknown distances. While this can be solved with good quality morphological filters, these might be too complex and power demanding. Considering navigation systems for VIP rely on limited energy sources that have to run multiple tasks, fixing unknown distance areas without major impacts on power consumption is a definite concern. Multi-objective optimization algorithms might improve filters' energy efficiency and output quality, which can be accomplished by means of different quality vs. complexity trade-offs. This study presents NSGA2CGP, a multi-objective optimization method that employs the NSGA-II algorithm on top of Cartesian Genetic Programming to optimize morphological filters for incomplete depth images used by navigation systems for VIP. Its goal is to minimize output errors and structuring element complexity, presenting several feasible alternatives combining different levels of filter quality and complexity-both of which affect power consumption. NSGA2CGP-optimized filters were deployed into an actual embedded platform, so as to experimentally measure power consumption and execution time. We also propose two new fitness functions based on existing approaches from literature. Results showed improvements in visual quality, performance, speed and power consumption, thanks to our proposed error function, proving NSGA2CGP as a solid method for developing and evolving efficient morphological filters. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106130	10.1016/j.asoc.2020.106130													
J								Two cluster validity indices for the LAMDA clustering method	APPLIED SOFT COMPUTING										Cluster validity index; Fuzzy clustering; LAMDA; Fuzzy statistics; Data analysis	FUZZY; ALGORITHM; ENTROPY; OPERATORS	The learning algorithm and multivariable data analysis (LAMDA) is an algorithm to group quantitative and qualitative data, applying self-learning and/or directed learning. Usually, LAMDA automatically generates classes by assigning the best data partition to a class. To evaluate the data partitions generated by LAMDA, the internal evaluation is used to find the optimal number of clusters. For the LAMDA algorithm, the cluster validity (CV) is the most popular index which is based on inter-class contrast (ICC). However, other indices have not been defined for LAMDA and a comparative analysis is required to evaluate its performance. In this paper, two metrics called cluster validity index based on granulation error and the ratio of the distance (CVGED) and cluster validity index based on the ratios of covariance and distance (CVCOD) are proposed. Such indices are compared with the CV and ICC indices for two experiments: using a databases repository and selected open data and experimental laboratory data. According to the main results, CVGED and CVCOD have a better performance in compactness, separation, and coefficient of variation than ICC and CV for most of the selected repository databases but the accuracy is limited for the four indices. Nevertheless, CVCOD improves the quality of data partition when the open data and experimental laboratory data are used. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106102	10.1016/j.asoc.2020.106102													
J								Selection of eco-friendly cities in Turkey via a hybrid hesitant fuzzy decision making approach	APPLIED SOFT COMPUTING										Eco-friendly cities; Hesitant fuzzy linguistic term set (HFLTS); Multi-criteria decision making (MCDM); Additive ratio assessment (ARAS) method	LINGUISTIC TERM SETS; MULTIPLE CRITERIA ASSESSMENT; GENERATION TECHNOLOGIES; INTEGRATED MODEL; ARAS METHOD; ENERGY; METHODOLOGY; EXTENSION; RANKING; QUALITY	Environmental pollution can be defined as the alteration and deterioration of the natural structure and composition of the environment. Industrialization and population density in cities increase environmental pollution. Today, environmental pollution, a common problem in all countries, has reached dimensions that threaten nature and human health. In this context, this study focuses on the selection of eco-friendly cities in Turkey according to criteria such as average PM10 values at air quality measurement stations, forest area per km(2), and percentage of population receiving waste services, using the hesitant fuzzy linguistic term set (HFLTS)-based additive ratio assessment (ARAS) method. The multi-criteria HFLTS method is used to determine the weights assigned to environmental criteria. The ARAS method is used to obtain the final ranking of 81 cities in Turkey. Empirical results demonstrate that the proposed approach is viable in selecting eco-friendly cities. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106090	10.1016/j.asoc.2020.106090													
J								Non linear system identification using kernel based exponentially extended random vector functional link network	APPLIED SOFT COMPUTING										Kernel based exponentially extended; RVFLN; System identification; Water cycle algorithm; SISO system; Monte Carlo simulations	WATER CYCLE ALGORITHM; MODEL; OPTIMIZATION; MACHINE	Identification of nonlinear systems finds extensive applications in control design and stability analysis. To identify complex nonlinear systems, the neural network has drawn the attention of many researchers due to its broad application area. In this paper, an improved identification method based on Kernel Exponentially Extended Random Vector Functional Link Network (KERVFLN) has been proposed for nonlinear system identification. Good generalization capability, fast learning speed, simple architecture and the direct connection between input and output nodes along with non linear enhancement nodes with random weights of traditional Random Vector Functional Link Network (RVFLN) are very essential to industrial applications. To avoid the selection of the number of hidden nodes and hidden mapping function, kernel function has been used in this paper to increase the stability. The input is extended using trigonometric expansion which increases the accuracy of the algorithm when ever there is a sudden random change. In case of KERVFLN the number of enhancement nodes and its corresponding activation function need not to be known if its corresponding kernel function is given. To verify the accuracy of the proposed model, some benchmark Monte Carlo simulations and one SISO system are carried out through simulation study and the obtained results are compared with some established techniques such as original RVFLN, Extreme Learning Machine (ELM), and Least Mean Square (LMS). The efficiency of the proposed technique has been tested with the real time data set as well. The prediction accuracy of the proposed method KERVFLN is higher than the normal RVFLN for different nonlinear systems which is clear from the performance evaluation section. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106117	10.1016/j.asoc.2020.106117													
J								Blind de-convolution of images degraded by atmospheric turbulence	APPLIED SOFT COMPUTING										Convolutional autoencoder; Asymmetric U-net; Space target image; Blind restoration; Atmospheric turbulence	RESTORATION; CNN	Atmospheric turbulence can change the path and direction of light during the imaging of a target in space due to the random motion of the turbulent medium, resulting in severe image distortion. To correct geometric distortion, and reduce spatially and temporally varying blur, this paper proposes a convolutional network for blind deblurring atmospheric turbulence (BDATNet) that includes a feature extraction noise suppression block (FENSB), an asymmetric U-net, and an image reconstruction subnetwork (IRSubnetwork). A deblurring noise suppression block (DNSB) is used instead of the traditional convolution layer for the U-net. The core principle of this model is to suppress noise before deblurring. During convolutional encoding, the FENSB and DNSB can suppress noise and capture rich feature maps. To fuse information obtained from low-level and high-level features, the FENSB and IRSubnetwork are skip-connected to ensure the integrity of the former during image reconstruction. Moreover, the method of gradually increasing the difficulty of data to train the network is used to cause it to gradually converge from simple to complex, so that it can deal with images severely degraded by turbulence. The experimental results of real data and simulation data show that the BDATNet can restore details of the image and sharpen its edges, and can suppress noise. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106131	10.1016/j.asoc.2020.106131													
J								Using IoT technology for computer-integrated manufacturing systems in the semiconductor industry	APPLIED SOFT COMPUTING										Internet of Things (IoT); Radio Frequency Identification (RFID); Computer-Integrated Manufacturing (CIM); Feature Advantage Benefit (FAB); Manufacturing Execution System (MES)	NEURAL-NETWORK; OPTIMIZATION; ALGORITHM	The evolution of semiconductor manufacturing between lot and equipment has an increasingly complex relationship in the track-in/track-out process mechanism. Failure of reading or updating barcodes may cause some lots to scrap with mis-operation. In this paper, it proposed an Internet of Things (IoT) based Computer-Integrated Manufacturing (CIM) system used in Feature Advantage Benefit (FAB), that semiconductor company used Radio Frequency Identification (RFID) into 300-mm (FAB), and according to the information provided by the semiconductor company MTB (Manufacturing Technical Board). This solution to change the operation process flow to fit the CIM system characteristics of Fab in Manufacturing Execution System (MES). It also use IoT design strategy and the system architecture of this new IoT solution. The result of this research is to incorporate deep learning method in the IoT system into the current CIM system to reveal the benefits in FAB. It estimate to save about US$ 2.8M by adopting IoT RFID instead of tagging to identify a work in process lot in the initiation phase. The company can gain greater asset visibility, reduce the costs of man-power requirements and connect with the worldwide trend. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106065	10.1016/j.asoc.2020.106065													
J								Learning rules for Sugeno ANFIS with parametric conjunction operations	APPLIED SOFT COMPUTING										ANFIS; Fuzzy system; Differential evolution algorithm; Learning rule; Parametric fuzzy conjunction; t-norm	FUZZY INFERENCE SYSTEM; DIFFERENTIAL EVOLUTION; GENERALIZED CONJUNCTION; NEURAL-NETWORKS; OPTIMIZATION; LOGIC; ADAPTATION; ALGORITHM; FAMILIES; DESIGN	The paper presents a Sugeno Adaptive Neuro-Fuzzy Inference System with parametric conjunction operations architecture, ANFIS-CX. The advantages of using parametric conjunction operations in fuzzy models are discussed, and learning rules for system identification with such operations are proposed. These learning strategies can include steepest descent gradient, differential evolution and least square estimation algorithms for tuning antecedent, conjunction, and consequent parameters, respectively. The results of system identification by parameter tuning of conjunction operations in addition to or instead of parameter tuning of the input membership functions are presented. Simulation results show that parameter training in conjunction operations, composed of four basic t-norms, significantly improves the approximation capability of fuzzy models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106095	10.1016/j.asoc.2020.106095													
J								A FE model updating technique based on SAP2000-OAPI and enhanced SOS algorithm for damage assessment of full-scale structures	APPLIED SOFT COMPUTING										Damage assessment; Damage detection; Model updating; SAP2000; Symbiotic organisms search (SOS); Full-scale structures	SYMBIOTIC ORGANISMS SEARCH; ECONOMIC-DISPATCH; 2-STAGE METHOD; IDENTIFICATION; OPTIMIZATION	Although many existing damage diagnosis techniques based on the combination of optimization algorithms and finite element model updating have been studied and demonstrated to be promising, there are still some limitations that need to be improved to enhance their performance for the large and complex structures. In this regard, the present article proposes a FE model updating technique based on the existing commercial software SAP2000-OAPI and an enhanced symbiotic organisms search (ESOS) algorithm for damage assessment of full-scale structures. First, to overcome the complexities of FE simulation, the FE model of monitored structure is built in SAP2000 software for analyzing the dynamic behavior of the structure. Then, the damage assessment of the structure is set up in the form of an optimization problem in which the objective function is established based on a combination of flexibility matrix and modal assurance criterion (MAC). An improved version of SOS algorithm, called ESOS algorithm, is adopted to solve this optimization problem for detecting and quantifying any stiffness degradation induced by damage. To perform the iterative optimization task automatically, a link between MATLAB and SAP2000 is created by using the OAPI feature of SAP2000. Finally, the numerical investigations on two full-scale structures with considering measurement noise and sparse measured data demonstrate the feasibility of the proposed technique in predicting the actual damage sites and their severities. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106100	10.1016/j.asoc.2020.106100													
J								Tensor alternating least squares grey model and its application to short-term traffic flows	APPLIED SOFT COMPUTING										Multi-mode traffic flow data; Tensor Tucker decomposition; Alternating least squares; GM (1,1) model	FORECASTING-MODEL; PREDICTION; OPTIMIZATION; UNCERTAINTY; SVR	Traffic flow data, as an important data source for the research and development of intelligent transportation systems, contain abundant multi-mode features. In this paper, a high-dimensional multi-mode tensor is used to represent traffic flow data. The Tucker tensor decomposition least squares algorithm is used to establish the tensor alternating least squares GM (1,1) model by combining the modelling mechanism of the grey classical model GM (1,1) with the algorithm, and the modelling steps are obtained. To demonstrate the effectiveness of the new model, first, the multi-mode traffic flow data are represented by the tensor model, and the correlation of the traffic flow data is analysed. Second, two short-term traffic flow prediction cases are analysed, and the results show that the performance of the GM (1, 1) model based on the tensor alternating least squares algorithm is obviously better than that of the other models. Finally, the original tensor data and the approximate tensor data during the peak period from 8:00 to 8:30 a.m. for six consecutive Mondays are selected as the experimental data, and the effect of the new model is much better than that of the GM (1,1) model of the original tensor data. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106145	10.1016/j.asoc.2020.106145													
J								A new fuzzy multi-hop clustering protocol with automatic rule tuning for wireless sensor networks	APPLIED SOFT COMPUTING										Wireless Sensor Network (WSN); Clustering; Shuffle Frog Leaping Algorithm (SFLA); Multi-hop; Fuzzy Inference System (FIS)	DELAY TRADE-OFF; ROUTING PROTOCOL; ENERGY; ALGORITHM; AWARE; LOGIC; LIFETIME	In today's world, a major challenge is to conserve and make optimal use of energy. This is a critical matter in wireless sensor networks due to their wide application in different areas. More importantly, scant attention has been paid to the use of node energy for certain applications in such networks. This study used the Shuffled Frog Leaping Algorithm (SFLA) to propose a Fuzzy Multi-hop clustering protocol (FMSFLA). The SFLA is used for automated configuration and optimization of the rule-base table in a fuzzy inference system and five adjustable parameters in two phases, i.e. Cluster Head (CH) selection and parent selection, based on application features. The proposed protocol (FMSFLA) considers effective parameters including energy, distance from the base station (BS), the number of neighboring nodes, real node distance from the BS, mean route load, delay, overlap, and the problem of hot spots, to achieve the best application-based performance. The FMSFLA includes rounds, in each round the phases of CH selection, parent selection, cluster formation, and steady state are performed. In the CH selection phase, CHs are selected from candidate nodes based on the fuzzy output and energy threshold (i.e. a control parameter) with respect to the overlap rate of adjacent CHs. In our protocol, the parent selection phase began by determining the levels of CHs in the network. At the end of this phase, the parent of each CH is determined on the basis of the greatest fuzzy output based on application. In the cluster formation phase, the clusters are formed on the basis of the determined CHs. Finally, the information received by CHs is sent through their parents to the BS in the steady state phase. The FMSFLA is evaluated against the LEACH, LEACH-EP, LEACH-FL, ASLPR, SIF, and ERA protocols in terms of the number of alive nodes, received packets, and cluster heads in addition to their appropriate distribution rates and other parameters pertaining to the network lifetime and protocol scalability using three application-oriented scenarios. According to the simulation results, the FMSFLA functioned far better than the other protocols in all scenarios with respect to goals and application features. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106115	10.1016/j.asoc.2020.106115													
J								Texture selection for automatic music genre classification	APPLIED SOFT COMPUTING										Music genre classification; Sound texture selection; Music classification; Signal processing; Music information retrieval	CONVOLUTIONAL NEURAL-NETWORKS; ACOUSTIC FEATURES	Music Genre Classification is the problem of associating genre-related labels to digitised music tracks. It has applications in the organisation of commercial and personal music collections. Often, music tracks are described as a set of timbre-inspired sound textures. A subset of the sound textures is often selected to represent the entire track. In this paper, we evaluate the impact of texture selection on automatic music genre classification. Although previous work has selected textures by linear downsampling, no extensive work has been done to evaluate how texture selection benefits music genre classification. We also present a novel texture selector based on K-Means aimed to identify diverse sound textures within each track. Our results show that capturing texture diversity within tracks is important towards improving classification performance. Our results also indicate that our K-Means based texture selector is able to achieve significant improvements over the baseline with fewer textures per track than the other texture selectors evaluated. We also show that using multiple texture representations allows further opportunities for feature selection to improve classification performance. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106127	10.1016/j.asoc.2020.106127													
J								An efficient krill herd algorithm for color image multilevel thresholding segmentation problem	APPLIED SOFT COMPUTING										Image segmentation; Color image multilevel thresholding segmentation; Swarm intelligence algorithm; Krill herd algorithm	PARTICLE SWARM OPTIMIZATION; MINIMUM CROSS-ENTROPY; CUCKOO SEARCH ALGORITHM; FIREFLY ALGORITHM; TSALLIS ENTROPY; MULTITHRESHOLD; EVOLUTIONARY; PERFORMANCE; QUALITY; KAPURS	The conventional thresholding methods are very efficient for bi-level thresholding, but the computational complexity may be excessively high for color image multilevel thresholding. Color image multilevel thresholding segmentation can be considered as a constrained optimization problem, therefore swarm intelligence algorithms are widely used to reduce the complexity. In this paper, an efficient krill herd (EKH) algorithm is proposed to search optimal thresholding values at different level for color images and the Otsu's method, Kapur's entropy and Tsallis entropy are employed as objective functions. Seven different algorithms, KH without any genetic operators (KH I), KH with crossover operator (KH II), KH with crossover and mutation operators (KH IV), modified firefly algorithm (MFA), modified grasshopper optimization algorithm (MGOA), bat algorithm (BA) and water cycle algorithm (WCA), are compared with the EKH algorithm. Experiments are performed on ten color benchmark images in terms of optimal threshold values, objective values, PSNR, SSIM and standard deviation of the objective values at different levels. The experimental results show that the presented EKH algorithm is superior to the other algorithms for color image multilevel thresholding segmentation. On the other hand, Kapur's entropy is found to be more accurate and robust for color image multilevel thresholding segmentation. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106063	10.1016/j.asoc.2020.106063													
J								A long-term prediction approach based on long short-term memory neural networks with automatic parameter optimization by Tree-structured Parzen Estimator and applied to time-series data of NPP steam generators	APPLIED SOFT COMPUTING										Prognostics and health management; Time-series forecasting; Multi-step ahead prediction; Long-short term memory; Nuclear power plant prognostics; Steam generator		Developing an accurate and reliable multi-step ahead prediction model is a key problem in many Prognostics and Health Management (PHM) applications. Inevitably, the further one attempts to predict into the future, the harder it is to achieve an accurate and stable prediction due to increasing uncertainty and error accumulation. In this paper, we address this problem by proposing a prediction model based on Long Short-Term Memory (LSTM), a deep neural network developed for dealing with the long-term dependencies in time-series data. Our proposed prediction model also tackles two additional issues. Firstly, the hyperparameters of the proposed model are automatically tuned by a Bayesian optimization algorithm, called Tree-structured Parzen Estimator (TPE). Secondly, the proposed model allows assessing the uncertainty on the prediction. To validate the performance of the proposed model, a case study considering steam generator data acquired from different French nuclear power plants (NPPs) is carried out. Alternative prediction models are also considered for comparison purposes. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106116	10.1016/j.asoc.2020.106116													
J								A granular deep learning approach for predicting energy consumption	APPLIED SOFT COMPUTING										Deep learning; Long short-term memory network; Predicting energy consumption; Maximal overlap discrete wavelet transformation	NEURAL-NETWORK; ENSEMBLE	This paper proposes a granular deep learning approach consisting of maximal overlap discrete wavelet transformation (MODWT) and long short-term memory (LSTM) network for predicting the energy consumption of different sectors at macro levels. Input features are first evaluated using Boruta algorithm-based feature selection model. MODWT is then used to decompose the energy consumption time series to alienate the linear and nonlinear components. The LSTM network, a deep learning tool, is used to make predictions on individual sub-series at a granular level. The final prediction is obtained by aggregating the forecasts obtained on decomposed components. Statistical analyses rationalize the efficacy and superiority of the proposed hybrid framework over six other well-known prediction algorithms. Monthly data for residential, commercial, industrial and transportation sectors of the USA have been taken for analyses. It is observed that energy consumption in commercial and transportation sectors are easier to predict than residential and industrial sectors. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106091	10.1016/j.asoc.2020.106091													
J								An intensify Harris Hawks optimizer for numerical and engineering optimization problems	APPLIED SOFT COMPUTING										CEC2017; CEC2018; Multidisciplinary design; Meta-heuristics	GLOBAL OPTIMIZATION; INSPIRED ALGORITHM; UNIT COMMITMENT; KRILL HERD; SEARCH; IDENTIFICATION; EVOLUTION; FEATURES	Recently developed Harris Hawks Optimization has virtuous behavior for finding optimum solution in search space. However, it easily get trapped into local search space for constrained engineering optimization problems. In order to accelerate the global search phase of existing Harris Hawks optimizer and to stuck it out of local search space, the proposed research aims to explore the exploration phase of the existing optimizer, the hybrid variant of Harris Hawks optimizer has been developed using sinecosine algorithm and named as Hybrid Harris Hawks-Sine Cosine Algorithm (hHHO-SCA) The effectiveness of the proposed optimizer has been tested for various nonlinear, non-convex and highly constrained engineering design problem. In order to validate the results of the proposed algorithm, 65 standard benchmark problems including CEC2017, CEC2018 and eleven multidisciplinary engineering design optimization problems has been taken into consideration. After verification it has been observed that the outcomes of the proposed hHHO-SCA optimization algorithm is much better than standard sine-cosine optimization algorithm, Harris Hawks Optimizer, Ant Lion Optimizer algorithm, Moth Flame Optimization algorithm, grey wolf optimizer algorithm, and others recently described metaheuristics, heuristics and hybrid type optimization search algorithm and proposed algorithm endorses its effectiveness in multi-disciplinary design and engineering optimization problems. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106018	10.1016/j.asoc.2019.106018													
J								Navigational analysis of multiple humanoids using a hybrid regression-fuzzy logic control approach in complex terrains	APPLIED SOFT COMPUTING										Navigation; Humanoid NAO; RA-FLC; Petri-Net; Hybridization		In the current work, a hybrid navigational control architecture combining regression analysis with fuzzy logic control has been proposed for smooth and hassle-free motion planning of humanoids. In the proposed hybrid scheme, sensory information regarding obstacle distances are initially supplied to the regression controller, and an interim turning angle is obtained as the preliminary output based on the preloaded training pattern of the regression model. In the next phase, interim turning angle is again supplied to the fuzzy controller to generate the ultimate turning angle which eventually guides the humanoid to take a safe direction of turn while avoiding any obstacle present in the work environment. The working of the developed hybrid model is validated through simulation and real-time environments, and satisfactory results have been obtained from comparisons of selected navigational parameters along with a minimal percentage of deviations. To avoid possible chances of inter-collision for navigation of multiple humanoids in a common platform, a Petri-Net model has been integrated with the developed hybrid control scheme. Finally, the developed motion planning model is also assessed against another existing navigational controller, and significant performance enhancement is obtained. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106088	10.1016/j.asoc.2020.106088													
J								Hybrid change point detection for time series via support vector regression and CUSUM method	APPLIED SOFT COMPUTING										SVR-ARMA model; ARMA forecasting; Change point test; Hybrid method; LSCUSUM test	MODELS; PARAMETERS; MACHINES; SQUARES	This study considers the change point testing problem regarding time series based on the location and scale-based cumulative sum (LSCUSUM) test constructed with the residuals obtained from support vector regression (SVR)-autoregressive moving average (ARMA) models. For this, we first estimate the model parameters in SVR-ARMA models from a training time series sample, in which a long AR model is fitted to the data to obtain residuals. We then use these as initial values of the error terms in SVR-ARMA (p, q) models and obtain the forecasting values recursively until the updated error terms converge to a certain limit. Finally, we select an optimal order of p, q with the root mean square error (RMSE) and use the forecasting errors from this selected model as the residuals for constructing the LSCUSUM test. Monte Carlo simulations are performed to evaluate the validity of the test. A real data example is provided for illustration. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106101	10.1016/j.asoc.2020.106101													
J								Generating Kranok patterns with an interactive evolutionary algorithm	APPLIED SOFT COMPUTING										Interactive evolutionary algorithm; Thai drawing; Kranok pattern; Bezier curve; Generated art	GENETIC ALGORITHM; DESIGN; COMPUTATION	The kranok pattern is a classical Thai pattern, often seen in the ornamentation of Thai religious artifacts such as Tripitaka cabinets, temple doors and coffins. All Thai trainee artists must practice drawing the kranok pattern because it is a fundamental motif in Thai traditional decorative art. Individual skilled artists and instructors have their own preferred ways of drawing the pattern, and their styles differ in ornamental details. Nevertheless, all make use of similar basic structures. Most trainees learn a range of styles by observing experienced artists. After exploring the methods of drawing used by experts and published in textbooks, we designed an algorithm for automatically generating kranok patterns. We used an interactive evolutionary algorithm (IEA) to improve the aesthetic appeal of the generated patterns in response to users' feedback. The aim of our work was not to replace artists with machines, but to enhance human artistic expression. Specifically, the work aimed to help users without artistic skills to create kranok patterns in their own style. The algorithm facilitated the creation of a variety of personalized kranok patterns - diverse in their expressive curvature, refinement and proportions - that were satisfying to the varying preferences of a range of users. We also analyzed the proposed algorithm's behavior in terms of its convergence to generate specific shapes. The proposed method was examined by 28 respondents (27 Thai and 1 foreign) who were selected to include representatives of both sexes as well as experts in both Thai drawing and evolutionary algorithms. The results from our questionnaires showed that all respondents were satisfied with the generated kranok patterns: one respondent was 'completely satisfied', seventeen 'very satisfied', seven 'moderately satisfied', and three 'slightly satisfied'. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106121	10.1016/j.asoc.2020.106121													
J								Reconstruction method with the learned regularizer for imaging problems in electrical capacitance tomography	APPLIED SOFT COMPUTING										Image reconstruction; Least square support vector machine; Random projection; Ensemble learning; Learned regularization; Regularized imaging method; Differential evolution algorithm; Inverse problem; Electrical capacitance tomography	ALTERNATING DIRECTION METHOD; DIFFERENTIAL EVOLUTION; INVERSION ALGORITHM; SIGNAL RECOVERY; OPTIMIZATION; MULTIPLIERS; ITERATION; SELECTION; ONLINE	The electrical capacitance tomography (ECT) is an attractive tomography method for process monitoring applications across different tasks and domains, but low quality images deteriorate its reliability and applicability. In order to change this situation, a potent method is developed to reduce reconstruction artifacts in this study. The learned regularization (LR) from a new ensemble learning method that integrates the advantageous properties of the least square support vector machine (LSSVM) method, the random projection (RP) method and the sparse matrix regression (SMR) solved by the differential evolution (DE) algorithm is proposed to increase the elasticity in mining and utilizing the prior knowledge. A potent model for imaging is devised by simultaneously taking advantage of the domain knowledge of the reconstruction targets (RTs) and the LR. The iterative split Bregman (ISB) is extended into a simple but powerful solver for the devised model by leveraging the forward backward splitting (FBS) algorithm and the soft thresholding (ST) algorithm to solve sub-problems efficiently. The imaging method proposed in the study is validated to successfully work on a series of testing tasks with the significant improvement of the reconstruction quality (RQ) over the popular imaging methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106126	10.1016/j.asoc.2020.106126													
J								Remaining useful life prediction using multi-scale deep convolutional neural network	APPLIED SOFT COMPUTING										Remaining useful life; Convolutional neural network; Multi-scale; Deep learning	PROGNOSTICS; SYSTEMS; RELIABILITY; MODEL; FRAMEWORK; ENSEMBLE	Accurate and reliable remaining useful life (RUL) assessment result provides decision-makers valuable information to take suitable maintenance strategy to maximize the equipment usage and avoid costly failure. The conventional RUL prediction methods include model-based and data-driven. However, with the rapid development of modern industries, the physical model is becoming less capable of describing sophisticated systems, and the traditional data-driven methods have limited ability to learn sophisticated features. To overcome these problems, a multi-scale deep convolutional neural network (MS-DCNN) which have powerful feature extraction capability due to its multi-scale structure is proposed in this paper. This network constructs a direct relationship between Condition Monitoring (CM) data and ground-RUL without using any prior information. The MS-DCNN has three multi-scale blocks (MS-BLOCKs), where three different sizes of convolution operations are put on each block in parallel. This structure improves the network's ability to learn complex features by extracting features of different scales. The developed algorithm includes three stages: data pre-processing, model training, and RUL prediction. After the min-max normalization pre-processing, the data is sent to the MS-DCNN network for parameter training directly, and the associated RUL value can be estimated base on the learned representations. Regularization helps to improve prediction accuracy and alleviate the overfitting problem. We evaluate the method on the available modular aeropropulsion system simulation data (C-MAPSS dataset) from NASA. The results show that the proposed method achieves good prognostics performance compared with other network architectures and state-of-the-art methods. RUL prediction result is obtained precisely without increasing the calculation burden. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106113	10.1016/j.asoc.2020.106113													
J								Dynamic granularity selection based on local weighted accuracy and local likelihood ratio	APPLIED SOFT COMPUTING										Granular computing; Dynamic granularity selection; Neighborhood granularity; Neighborhood rough set; Classification	OPTIMAL SCALE SELECTION; ROUGH SETS; GRANULATION	Granular computing aims to develop a granular view for interpreting and solving problems, in which granularity selection is a key problem and has received extensive attention in recent years. Existing studies select the same granularity for all samples. In fact, different samples may prefer to different granularities. To address this issue, dynamic granularity selection is proposed in this paper. Namely, granularity selection is considered with respect to specific sample. Two indices, denoted as local weighted accuracy and local likelihood ratio, are introduced to compute the weight of granularity. Subsequently, an algorithm called DGS_LWA-LLS is given for dynamic granularity selection, in which the granularity with the largest weight is considered to be optimal The weight of granularity is related to specific sample, thus the weights of a granularity may be different with different samples. Consequently, different granularities will be selected with respect to different samples. Experiments were carried out based on neighborhood granularity to explain the necessity of granularity selection and to validate the rationality and effectiveness of DGS_LWA-LLS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106087	10.1016/j.asoc.2020.106087													
J								Configuration space evolutionary algorithm for multi-objective unequal-area facility layout problems with flexible bays	APPLIED SOFT COMPUTING										Multi-objective optimization; Configuration space evolutionary algorithm; Flexible bay structure; Pareto-optimal solutions; Facility layout problem	NONDOMINATED SORTING APPROACH; OBJECTIVE GENETIC ALGORITHM; ANT COLONY OPTIMIZATION; TABU SEARCH; KNOWLEDGE; FRAMEWORK; SYSTEM	Facility layout problem (FLP) which deals with the layout of facilities within a given plant floor is an NP-hard combinatorial optimization problem. This paper studies multi-objective unequal-area facility layout problems (UA-FLPs) with the flexible bay structure (FBS), whose objectives refer to the material handling cost, the closeness relationship, the distance requirement and the aspect ratio of facilities. In recent years, some successes have been achieved by multi-objective evolutionary algorithms (MOEAs) for solving various kinds of optimization problems with multiple conflicting objectives. However, traditional MOEAs face a great challenge in the convergence and diversity of solutions for UA-FLPs. In this paper, a novel MOEA called the configuration space evolutionary (CSE) algorithm is developed to solve the UA-FLPs with multiple objectives. We consider a mating pool called a configuration (solution) bank in the CSE, and use evolutionary operations (selection, novel crossover and mutation) to produce new configurations of the pool. By introducing a measure of the radius d(spaces) of the configuration bank, whose value is gradually reduced to narrow the search space, the convergence of solutions in the CSE is controlled. A method of the nearest and farthest candidate solution based on objective function normalization is combined with the fast non-dominated sorting to choose the Pareto-optimal solutions, which is good for the algorithm to keep diversity of the obtained solutions. The main contributions of this study lie in the use of a mechanism of evolution of population in the algorithm based on a configuration bank, and the use of a selection strategy based on the nearest and farthest candidate solution method, in order to improve the convergence and diversity of solutions. Experiments are carried out on eight different representative instances and performance metrics from the literature. Compared with the existing MOEAs, the CSE is able to find the better results and show better performance. The numerical experiments confirm the effectiveness of the CSE for solving multi-objective UA-FLPs. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106052	10.1016/j.asoc.2019.106052													
J								Semi-supervised learning quantization algorithm with deep features for motor imagery EEG Recognition in smart healthcare application	APPLIED SOFT COMPUTING										Convolutional neural networks; Semi-supervised classification; EEG Recognition; Smart healthcare; Cartesian K-means		This paper depicts a novel semi-supervised classification model with convolutional neural networks (CNN) for EEG Recognition. The performance of popular machine learning algorithm usually rely on the number of labeled training samples, such as the deep learning approaches,sparse classification approaches and supervised learning approaches. However, the labeled samples are very difficulty to get for electroencephalography(EEG) data. In addition, most deep learning algorithms are usually time-consuming in the process of training. Considering these problems, in this article, a novel semi-supervised quantization algorithm based on the cartesian K-means algorithm is proposed, which named it as the semi-supervised cartesian K-means (SSCK), we use the CNN models pre-trained on motor imagery samples to create deep features, and then we applied it for motor imagery (MI) data classification. Unlike the traditional semi-supervised learning models that labeled information can be directly casted into the model training, label information can only be implicitly used in the semi-supervised learning strategy, in the semi-supervised learning algorithm, supervised information is integrated into the quantization algorithm by resorting a supervised constructed laplacian regularizer. Experimental results over four popular EEG datasets substantiate the efficiency and effectiveness of our proposed semi-supervised cartesian K-means. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106071	10.1016/j.asoc.2020.106071													
J								A clustering and dimensionality reduction based evolutionary algorithm for large-scale multi-objective problems	APPLIED SOFT COMPUTING										Large-scale multi-objective problems; Cooperative coevolution; Decision variable clustering; Dimensionality reduction	PARTICLE SWARM OPTIMIZATION; COOPERATIVE COEVOLUTION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; DECOMPOSITION; MOEA/D; SELECTION	When solving multi-objective problems (MOPs) with a large number of variables, analysis of the linkage between decision variables is maybe useful for avoiding "the curse of dimensionality". In this work, a clustering and dimensionality reduction based evolutionary algorithm for large-scale multi-objective problems is suggested, which focuses on clustering decision variables into two categories and then utilizes a dimensionality reduction approach to get a lower dimensional representation for those variables that affect the convergence of the evolution. The interdependence analysis is carried out next aiming to decompose the convergence variables into a number of subcomponents that are easier to be tackled. The algorithm presented in this article is promising on a series of test functions, and the outcome of these experiments reveal that our suggested algorithm is able to prominently enhance the performance; meanwhile it can save computing costs to a large extent compared with some latest evolutionary algorithms (EAs). In addition, the proposed algorithm can be extended to solve MOPs with dimensions up to 5000, with a good performance obtained. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106120	10.1016/j.asoc.2020.106120													
J								InOvIn: A fuzzy-rough approach for detecting overlapping communities with intrinsic structures in evolving networks	APPLIED SOFT COMPUTING										Incremental community; Embedded cluster; Density variation; Community within community; Overlapping community; Rough-fuzzy set; Social graphs	CLUSTER; VALIDATION; MODULARITY; ALGORITHM	Real-world networks, such as biological, biomedical and social networks, often contain overlapping and intrinsic communities. More significantly, such networks are growing or evolving over time, which leads to a continuous alteration of community structures. Detecting overlapping community together with intrinsic structures in evolving scenarios is one of the challenging tasks. Prior researches are limited in handling all the events together while designing a community detector. We propose an integrated solution, InOvIn (Intrinsic Overlapping Community Detection in Incremental Networks), for detecting overlapping, non-overlapping and intrinsic communities in evolving networks. Herein, we have explored a rough-fuzzy clustering approach for overlapping community detection. Fuzzy membership helps in soft decision making for deciding membership of a node towards a target community. While rough boundary of the communities decides the shared membership of a node in multiple communities. The node degree density variation measure is used to discover the existence of intrinsic community within a community. We assess the performance of InOvIn in light of twelve (12) popular real-world social networks. It may be noted that available real-world networks are lacking in labeled overlapping and intrinsic communities. Hence, we synthetically generate six (O6) networks with both overlapping and intrinsic communities. We demonstrate the superiority of InOvIn over contemporary community detection methods using ten (1O) different statistical assessment parameters. Interestingly, for the first time, our method detects intrinsic communities in PolBooks and Word Adjacencies networks. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106096	10.1016/j.asoc.2020.106096													
J								A two-stage robust optimisation for terminal traffic flow problem	APPLIED SOFT COMPUTING										Robust optimisation; Terminal traffic flow problem; Benders cuts selection scheme; Dynamic relative interior point	BENDERS DECOMPOSITION ALGORITHM; SCHEDULING AIRCRAFT LANDINGS; DELAY PROPAGATION; ROUTING PROBLEM; MIN-MAX; MODEL; METAHEURISTICS; SERVICE; REGRET; OPERATION	Airport congestion witnesses potential conflicts: insufficient terminal airspace and delay propagation within scrambled the competition in the terminal manoeuvring area. Re-scheduling of flights is needed in numerous situations, heavy traffic in air segments, holding patterns, runway schedules and airport surface operations. Robust optimisation for terminal traffic flow problem, providing a practical point of view in hedging uncertainty, can leverage the adverse effect of uncertainty and schedule intervention. To avoid delay propagation throughout the air traffic flow network and reduce the vulnerability to disruption, this research adopts a two-stage robust optimisation approach in terminal traffic flow. It further enhances the quality of Pareto-optimality Benders-dual cutting plane based on core point approximation in the second stage recourse decision. The efficiency of the cutting plane algorithm is evaluated by a set of medium sized real-life scenarios. The numerical results show that the proposed scheme outperforms the well-known Pareto-optimal cuts in Benders-dual method from the literature. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106048	10.1016/j.asoc.2019.106048													
J								A general framework and guidelines for benchmarking computational intelligence algorithms applied to forecasting problems derived from an application domain-oriented survey	APPLIED SOFT COMPUTING										Benchmarking computational intelligence algorithms; Knowledge-based general benchmarking framework; Forecasting applications; Computational intelligence engineering guidelines	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHMS; NEURAL-NETWORKS; PREDICTION; MODEL; PERFORMANCE; SYSTEM; EARTHQUAKES; PARAMETERS; MULTISTEP	Benchmarking computational intelligence algorithms provides valuable knowledge for selecting the best or, at least, the proper algorithm for a certain problem. The experimental results of the computational intelligence techniques applications in various domains, as well as the comparative studies that were reported in the literature can be analyzed and synthesized as development strategies for new successful applications of CI algorithms. Starting from an application domain-oriented survey of selected recently reported research work, the paper presents a general benchmarking framework applicable to computational intelligence algorithms and a set of guidelines for the selection of the best or more suitable CI algorithm for solving forecasting problems. Our approach proposes the integration of software and knowledge engineering best practice towards CI benchmarking, being a computational intelligence engineering methodology. The framework uses two knowledge bases, one for the application domain and one for the CI algorithms, providing heuristic knowledge for a more informed and efficient benchmarking, a case base in which solved problems are recorded with their solution and lessons that were learned, and a knowledge-based problem instance features selection. Some examples of how to apply the framework for problems of forecasting in seismology, environmental protection, hydrology and energy are also discussed. We point out that the framework might be implemented as a software tool (e.g. a decision support system) or as a tool suite. The main conclusion of our research work is that the integration of the derived knowledge from an application domain-oriented survey into the general benchmarking framework along with the set of guidelines for best or proper CI algorithms selection can improve significantly the forecasting accuracy and the response time, in case of real time forecasters. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106103	10.1016/j.asoc.2020.106103													
J								Failure mode and effects analysis (FMEA) for risk assessment based on interval type-2 fuzzy evidential reasoning method	APPLIED SOFT COMPUTING										Failure mode and effects analysis (FMEA); Interval type-2 fuzzy sets (IT2FSs); Evidential reasoning (ER); Risk assessment	EXTENDED MULTIMOORA METHOD; DECISION-MAKING; ASSESSMENT CAPABILITY; TOPSIS APPROACH; PRIORITIZATION; COMBINATION; ENHANCE; SYSTEMS; DEMATEL; SETS	Failure mode and effect analysis (FMEA) has been widely adopted to define, identity, and remove potential and recognized hazards. As an indicator in traditional FMEA, the risk priority number (RPN) is an effective tool for measuring risk and the calculation of RPN is also very simple. Nevertheless, there are many drawbacks in the conventional FMEA method. It is necessary to seek approaches that can make up for the deficiency of traditional FMEA method and strengthen assessment capability of ranking failure modes according to three relevant risk factors. This paper presents a way to combine interval type-2 fuzzy sets (IT2FSs) with evidential reasoning (ER) method, which is able to overcome some disadvantages of the conventional FMEA approach and deal with uncertainties more efficiently. First, we give a more precise expression of the risk factors in the form of IT2FSs and gain the relative weight of three risk factors. Second, one can judge the failure modes in relation to each risk factors with belief structures. Finally, the ER method is used to combine the belief structures under the weight of the three risk factors. To verify the feasibility of the method, an application for steam valve system is performed and the obtained results show the effectiveness of the method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106134	10.1016/j.asoc.2020.106134													
J								A novel reinforcement learning based grey wolf optimizer algorithm for unmanned aerial vehicles (UAVs) path planning	APPLIED SOFT COMPUTING										Unmanned aerial vehicles (UAVs); Three-dimensional path planning; Reinforcement learning; Grey wolf optimizer		Unmanned aerial vehicles (UAVs) have been used in wide range of areas, and a high-quality path planning method is needed for UAVs to satisfy their applications. However, many algorithms reported in the literature may not feasible or efficient, especially in the face of three-dimensional complex flight environment. In this paper, a novel reinforcement learning based grey wolf optimizer algorithm called RLGWO has been presented for solving this problem. In the proposed algorithm, the reinforcement learning is inserted that the individual is controlled to switch operations adaptively according to the accumulated performance. Considering that the proposed algorithm is designed to serve for UAVs path planning, four operations have been introduced for each individual: exploration, exploitation, geometric adjustment, and optimal adjustment. In addition, the cubic B-spline curve is used to smooth the generated flight route and make the planning path be suitable for the UAVs. The simulation experimental results show that the RLGWO algorithm can acquire a feasible and effective route successfully in complicated environment. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106099	10.1016/j.asoc.2020.106099													
J								Dynamic Histogram Equalization for contrast enhancement for digital images	APPLIED SOFT COMPUTING										Contrast enhancement; Dynamic Histogram Equalization; Swarm optimization; Magnetic Resonance Imaging	ALGORITHM	Magnetic Resonance Imaging (MRI) is an efficient tool, produced by applying radio waves and magnetic fields which is being useful in the diagnosis of various diseases like cancer, epilepsy and stroke etc. The quality of the resulting image is needed to be enhanced because it is challenging for the specialists to investigate. Modified Histogram Equalization on Fuzzy based Improved Particle Swarm Optimization (FIPSO) is proposed for Dynamic Histogram Equalization which resolves this problem through image contrast enhancement. The details of an images are captured by smoothing and it uses Gaussian function to distribute pixel intensity to nearest pixel. It uses normal distribution and here blur is removed by applying Non subsampled Contourlet Transform. Then local maxima are calculated to extract dark and bright pixel values. The smoothed images are fuzzified with TSK (Takagi-SugenoKang) model and it provides importance to all the local maxima intervals. An Improved particle swarm optimization (IPSO) algorithm is obtained by combining Galactic Swarm Optimization (GSO) with PSO which equalizes histogram of an image. FIPSO algorithm is used to the minimum contrast images of MRI brain images. Non-subsampled Contourlet transform (NSCT) based modified histogram equalization enhances image contrast. Here IPSO generates optimum values and these value are used to calculate cumulative distribution function in histogram equalization. The quality measures demonstrate that the current equalization technique attains highest performance against existing techniques in terms of brightness and contrast. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106114	10.1016/j.asoc.2020.106114													
J								Constrained design optimization of selected mechanical system components using Rao algorithms	APPLIED SOFT COMPUTING										Constrained design optimization; Mechanical components; Rao algorithms	MULTIOBJECTIVE OPTIMIZATION; HEAT-EXCHANGERS; ECONOMIC OPTIMIZATION; OPTIMUM DESIGN; BEARING	The design optimization of mechanical system components, like bearings, pulleys, springs, etc. is an essential issue due to the current competitive market. The performance of any mechanical system depends on the design of their mechanical components. The design optimization of these mechanical components is a difficult task due to the intricate design constraints and mixed type design variables (i.e., continuous, discrete, and integer). This paper explores the performance of Rao algorithms on the design optimization of selected mechanical system components. The designs obtained using Rao algorithms are compared with the designs obtained using other optimization algorithms in previous studies. The comparison of results shows the ability and the efficiency of Rao algorithms for solving complex design optimization problems of mechanical components. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106141	10.1016/j.asoc.2020.106141													
J								An adaptive framework against android privilege escalation threats using deep learning and semi-supervised approaches	APPLIED SOFT COMPUTING										Mobile malware; Internet of things; Deep-learning; Semi-supervised learning; Feature extraction and selection	MALWARE DETECTION; CLASSIFICATION; IDF	The immense popularity of Android makes it a primary target of malicious attackers and developers which brings a significant threat from malicious applications for android users through the escalation of the abuse of android permissions and inter-component communication (ICC) mechanism. Therefore, protecting android users from malicious developers and applications is crucial for Android market and communities. As malicious applications can hide their malicious behavior and change the behaviors frequently by abusing the android's ICC mechanism and related vulnerabilities, it is a challenging task to identify them accurately before it becomes a prevalent reason for users' privacy and data breach. Therefore, it is essential to develop such a malware detection engine that will ensure zeroday detection. In this research, we propose an adaptive framework which can learn the behavior of malware from the usage of permissions and their escalations. For our adaptive framework, we proposed two different detection models using deep learning and semi-supervised approaches. The proposed detection models can extract knowledge from unlabeled apps to identify the new malicious behavior using the unsupervised training nature of deep learning and clustering techniques and their integration to the supervised detection engine. Thus, our adaptive framework learns about new malicious apps and their behavior without supervised labeling by manual expert and can ensure zero-day protection. The proposed detection models have been tested on a real mobile malware test-bed and data set. The Experimental results show that the deep learning and semi-supervised based models achieve 99.024% of accuracies, more effective for zero-day protection and outperform other existing supervised detection engines. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106089	10.1016/j.asoc.2020.106089													
J								A wildfire warning system applied to the state of Acre in the Brazilian Amazon	APPLIED SOFT COMPUTING										Wildfire indexes; Fuzzy system; Machine learning; Forest fires	FOREST-FIRE RISK; ROC CURVE; NEURAL-NETWORK; AREA; DANGER	In this paper, we present a dynamic wildfire warning map that combines both spatial and weather information. In particular, our wildfire early warning model is obtained by aggregating two indexes called wildfire risk and wildfire danger. The wildfire risk index, which is based on georeferenced features such as altitude and forest type, measures the fuel necessary for a wildfire to start at a certain location on a map. The wildfire danger uses weather conditions to yield temporal information concerning the possibility of a wildfire to spread. Machine learning techniques and fuzzy logic operations are used to determine the wildfire risk and danger indexes from available data. Although both wildfire risk and wildfire danger indexes can be used separately, using concepts from fuzzy logic, they can be combined to yield a wildfire warning system that takes into account both weather and static information. We illustrate the wildfire early warning model by considering weather and geographical data for the state of Acre. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106075	10.1016/j.asoc.2020.106075													
J								An enhanced decentralized artificial immune-based strategy formulation algorithm for swarms of autonomous vehicles	APPLIED SOFT COMPUTING										Artificial immune system; Autonomous vehicle swarm; Decentralized path planning; Optimal task allocation; Swarm intelligence	SYSTEM; TRACKING; NETWORKS; UAVS	This work presents an algorithmic approach to the problem of strategy assignment to the members of a swarm of autonomous vehicles. The proposed methodology draws inspiration from the artificial immune system (AIS), where a large number of antibodies cooperate in order to protect an organism from foreign threats by local exchange of information. The decentralized nature of the methodology does not suffer from problems like the need of a central control unit, the high maintenance costs and the risks associated with having a single point of system failure, which are common to centralized control techniques. Decentralized and distributed optimization schemes employ simple algorithms, which are fast, robust and can run locally on an autonomous unit due to their low processing power requirements. In contrast to standard AIS-based decentralized schemes, the proposed methodology makes use of a dynamic formulation of the available strategies and avoids the possibility of choosing an invalid strategy, which may lead to inferior swarm performance. The methodology is further enhanced by a dual strategy activation decay technique and a blind threat-follow rule. Statistical testing on different case studies based on "enemy search and engage" type scenarios in a simulated environment demonstrates the superior performance of the proposed algorithm against the standard AIS, an enhanced AIS version and a centralized particle swarm optimization (PSO) based methodology. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106135	10.1016/j.asoc.2020.106135													
J								Cooperative coevolution of real predator robots and virtual robots in the pursuit domain	APPLIED SOFT COMPUTING										Swarm intelligence; Cooperative coevolution; Particle swarm optimization; Pursuit domain; Virtual robot		The pursuit domain, or predator-prey problem is a standard testbed for the study of coordination techniques. In spite that its problem setup is apparently simple, it is challenging for the research of the emerged swarm intelligence. This paper presents a particle swarm optimization (PSO) based cooperative coevolutionary algorithm for the (predator) robots, called CCPSO-R, where real and virtual robots coexist in an evolutionary algorithm (EA). Virtual robots sample and explore the vicinity of the corresponding real robots and act as their action spaces, while the real robots consist of the real predators who actually pursue the prey robot without fixed behavior rules under the immediate guidance of the fitness function, which is designed in a modular manner with very limited domain knowledge. In addition, kinematic limits and collision avoidance considerations are integrated into the update rules of robots. Experiments are conducted on a scalable swarm of predator robots with 4 types of preys, the results of which show the reliability, generality, and scalability of the proposed CCPSO-R. Comparison with a representative dynamic path planning based algorithm Multi-Agent Real-Time Pursuit (MAPS) further shows the effectiveness of CCPSO-R. Finally, the codes of this paper are public available at: https://github.com/LijunSun90/pursuitCCPSOR. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106098	10.1016/j.asoc.2020.106098													
J								An easy-to-use real-world multi-objective optimization problem suite	APPLIED SOFT COMPUTING										Evolutionary multi-objective optimization; Test problems; Real-world problems	NONDOMINATED SORTING APPROACH; EVOLUTIONARY ALGORITHMS; MIXED-INTEGER; DESIGN OPTIMIZATION; GENETIC ALGORITHM; MOEA/D; DECOMPOSITION; PERFORMANCE; SELECTION	Although synthetic test problems are widely used for the performance assessment of evolutionary multi-objective optimization algorithms, they are likely to include unrealistic properties which may lead to overestimation/underestimation. To address this issue, we present a multi-objective optimization problem suite consisting of 16 bound-constrained real-world problems. The problem suite includes various problems in terms of the number of objectives, the shape of the Pareto front, and the type of design variables. 4 out of the 16 problems are multi-objective mixed-integer optimization problems. We provide Java, C, and Matlab source codes of the 16 problems so that they are available in an offthe-shelf manner. We examine an approximated Pareto front of each test problem. We also analyze the performance of six representative evolutionary multi-objective optimization algorithms on the 16 problems. In addition to the 16 problems, we present 8 constrained multi-objective real-world problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106078	10.1016/j.asoc.2020.106078													
J								Multiple scale self-adaptive cooperation mutation strategy-based particle swarm optimization	APPLIED SOFT COMPUTING										Particle swarm optimization; Premature convergence; Multi-scale Gaussian mutations; Uniform mutation; Self-adaptive mutation threshold	NEURAL-NETWORK; ALGORITHM	Particle Swarm Optimization (PSO) algorithm has lately received great attention due to its powerful search capacity and simplicity in implementation. However, previous studies have demonstrated that PSO still suffers from two key drawbacks of premature convergence and slow convergence, especially when dealing with multi-modal optimization problems. In order to address these two issues, we propose a multiple scale self-adaptive cooperative mutation strategy-based particle swarm optimization algorithm (MSCPSO) in this paper. In the proposed approach, we adopt multi-scale Gaussian mutations with different standard deviations to promote the capacity of sufficiently searching the whole solution space. In the adopted multi-scale mutation strategy, large-scale mutation can make populations explore the global solution space and rapidly locate the better solution area at the early stage, thus avoiding the premature convergence and simultaneously speeding up the convergence, while small-scale mutation can allow the populations to more accurately exploit the local best solution area during the later stage, thus improving the accuracy of final solution. In order to guarantee the convergence speed while avoiding premature convergence, the standard deviations for multi-scale Gaussian mutations would be reduced with the increase of iterations, which can make populations pay more attention to local accurate solution exploitation during the later evolution stage and consequently speed up the convergence. In addition, the threshold for each dimension to execute mutation is also dynamically adjusted according to its previous mutation frequency, which can allow MSCPSO to better balance the global and local search capacities, thus avoiding premature convergence without reducing convergence speed. The extensive experimental results on various benchmark optimization problems demonstrate that the proposed approach is superior to other existing PSO techniques with good robustness. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106124	10.1016/j.asoc.2020.106124													
J								An infinite-resolution grid snapping technique based on fuzzy theory	APPLIED SOFT COMPUTING										CAD; Sketch-based interface; Fuzzy theory		In ordinary CAD systems with pointing devices, users input geometric objects by inputting their feature points one by one through pointing and dragging operations. Thus, each of the feature points is snapped into place interactively, and the geometric objects are aligned on the specified grid as a result. In sketch-based CAD systems with pen input devices, users simply draw a freehand stroke that the system recognizes as a geometric object, and then the system must automatically snap all of the feature points to the grid by a batch process. In this case, it is not easy for the user to set up the grid resolution in advance, because the appropriate resolution changes according to the drawing. Therefore, a multi-resolution fuzzy grid snapping (MFGS) technique has been proposed. In MFGS, the appropriate snapping resolution is dynamically selected in a multi-resolution grid system according to the roughness of the drawing manner. However, the multiplicity of the grid resolutions in MFGS is limited to a finite number, and grid snapping outside this range of resolutions cannot be appropriately handled. In this paper, we propose infinite-resolution fuzzy grid snapping (IFGS), in which there is an infinite number of grid resolutions, and experimentally demonstrate that IFGS effectively resolves the problems of MFGS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106112	10.1016/j.asoc.2020.106112													
J								Chaos-based Vortex Search algorithm for solving inverse kinematics problem of serial robot manipulators with offset wrist	APPLIED SOFT COMPUTING										Vortex search; Chaos map; Serial robot manipulator with offset wrist; Inverse kinematics problem	NUMERICAL FUNCTION OPTIMIZATION	Vortex Search (VS) algorithm is a single-solution-based optimization algorithm that requires the high maximum number of iterations (NOI) to solve optimization problems. In this study, two methods were proposed to reduce the required maximum NOI of the VS algorithm. These methods are based on using ten chaos maps with the VS algorithm and provide improvements in the exploration and exploitation abilities of the algorithm for reducing the required maximum NOI. Ten chaos-based VS algorithms (CVSs) were obtained by combining these methods with the VS algorithm. The performances of the CVS algorithms were tested by fifty benchmark functions. The results were evaluated in terms of some statistical values and a pairwise statistical test, Wilcoxon Signed-Rank Test. According to the results, it was found that the CVS algorithm obtained by using the Gauss-Mouse chaos map was the best algorithm. And also, it was shown that the proposed CVS algorithm performs better than the classical VS algorithm, even when its maximum NOI was ten times less than the maximum NOI of the VS algorithm. Additionally, the effects of the proposed methods in the exploration and the exploitation abilities of the VS algorithm were visually shown and a comparison about algorithm processing time was presented. In order to test the performance of the proposed CVS algorithm in solving the real-world optimization problems, the inverse kinematics problem of a six Degrees Of Freedom (DOF) serial robot manipulator with offset wrist was solved with both the proposed CVS algorithm and the VS algorithm for two different types of trajectories. The results showed that the proposed algorithm outperforms the VS algorithm in terms of the objective function values and position errors of the end-effector of the serial robot manipulator. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106074	10.1016/j.asoc.2020.106074													
J								Fuzzy dissimilarity color histogram equalization for contrast enhancement and color correction	APPLIED SOFT COMPUTING										Fuzzy dissimilarity; Adaptive histogram equalization; Gamma correction; Hue Deviation Index; Saturation	IMAGE-CONTRAST	Many statistical histogram-based methods perform intensity transformation on gray levels in the statistical histogram. This may cause over-enhancement due to dominating portions of the histogram. Various methods tackle this problem by overwhelming the dominating portions and improving the inferior components. Though, this may change the natural appearances of the image and results in degraded visual quality. In order to attenuate such limitations, an efficient method called Fuzzy Dissimilarity Adaptive Histogram Equalization with Gamma Correction (FDAHE-GC) algorithm is proposed. In this work, a Fuzzy Dissimilarity Histogram (FDH) is obtained from the neighborhood characteristics of an intensity. An intensity mapping function, constructed from FDH is applied to enhance the contrast and natural characteristics of an image. Finally, the gamma correction is employed to enhance the dark regions. In order to tune the fine details and to improve visual appearance of an image, the proposed FDAHE-GC algorithm is applied to the intensity value of HSI space. The performance of the presented method is evaluated with different existing methods using image quality assessment tools such as entropy, Colorfulness (C), Hue Deviation Index (HDI), Saturation, Contrast Enhancement Factor (CEF) and Gradient (G). The investigational results tested on standard benchmark test images with visual inspection shows the superiority of the proposed FDAHE-GC algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106077	10.1016/j.asoc.2020.106077													
J								Preventing epidemic spreading in networks by community detection and memetic algorithm	APPLIED SOFT COMPUTING										Epidemic spreading; Memetic algorithm; Community detection; Epidemic threshold	INFLUENCE MAXIMIZATION; EVOLUTIONARY	Targeted immunization is a commonly used strategy in preventing epidemic spreading. Traditional methods immunize targeted nodes based on specific global or local network structures instead of optimization. In this paper, we propose a novel community-based immunization strategy to select targeted immunization nodes based on optimization. The proposed algorithm consists of three steps. First, community structures are discovered by community detection algorithm. Second, possible candidates are narrowed down based on the structure properties of community. Finally, a novel memetic algorithm is designed to select immunization nodes from the candidate set. In the final step, epidemic threshold is adopted as objective function and then targeted immunization is formulated as an optimization problem. To solve this optimization problem, a novel memetic algorithm is designed. Experimental results demonstrate that the proposed algorithm outperforms some state-of-the-art immunization algorithms in optimizing epidemic threshold. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106118	10.1016/j.asoc.2020.106118													
J								A benchmark data set for aircraft type recognition from remote sensing images	APPLIED SOFT COMPUTING										Benchmark; Airplane type recognition; Remote sensing images; Pattern recognition	OBJECT RECOGNITION; SEGMENTATION	Aircraft type recognition from remote sensing images has many civil and military applications. In images obtained with modern technologies such as high spatial resolution remote sensing, even details of aircraft can become visible. With this, the identification of aircraft types from remote sensing images becomes possible. However, the existing methods for this purpose have mostly been evaluated on different data sets and under different experimental settings. This makes it hard to compare their results and judge the progress in the field. Moreover, the data sets used are often not publicly available, which brings difficulties to reproduce the works for fair comparison. This severely limits the progress of research and the state of the art is not entirely clear. To address this problem, we introduce a new benchmark data set for aircraft type recognition from remote sensing images. This data set is called Multi-Type Aircraft Remote Sensing Images (MTARSI), which contains 9'385 images of 20 aircraft types, with complex backgrounds, different spatial resolutions, and complicated variations in pose, spatial location, illumination, and time period. The publicly available MTARSI data set allows researchers to develop more accurate and robust methods for both remote sensing image processing and interpretation analysis of remote sensing object. We also provide a performance analysis of state-of-the-art aircraft type recognition and deep learning approaches on MTARSI, which serves as baseline result on this benchmark. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106132	10.1016/j.asoc.2020.106132													
J								Constructing a health indicator for roller bearings by using a stacked auto-encoder with an exponential function to eliminate concussion	APPLIED SOFT COMPUTING										Roller bearings; Deep learning; Stacked auto-encoder; Health indicator; Exponent function	ROTATING MACHINERY; FAULT-DIAGNOSIS; COULOMBIC EFFICIENCY; NEURAL-NETWORKS; LEARNING-METHOD; DEEP NETWORK; AUTOENCODER; DEGRADATION; PROGNOSIS; TOOL	Most deep-learning models, especially stacked auto-encoders (SAES), have been used in recent years for the diagnosis of faults in rotating machinery. However, very few studies have reported on health indicator (HI) construction by using SAES in deep learning. SAES have a good feature-extraction ability when several hidden layers are used to reconstruct the original input. In this study, we first introduce a method to reduce dependence on prior knowledge that is based on SAES and enables extraction of the preliminary degradation trend from the bearing's frequency domain directly. Second, to construct the final HI and improve the monotonicity of the indicators, an exponential function is used to eliminate global severe vibration after an SAE has extracted the preliminary degradation trend. To prove the effect of our presented method, some other HI construction models, such as root mean square, kurtosis, approximate entropy, permutations entropy, empirical mode decomposition-singular value decomposition, K-means/K-medoids, and various time-frequency fusion indicators are used for comparison. Moreover, to prove that the exponential-function effect exceeds other severe vibration-eliminating methods, examples of the latter methods such as exponentially weighted moving-average and outlier detection are used for comparative analysis. Finally, the results shows that our proposed model is better than the above-mentioned existing models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106119	10.1016/j.asoc.2020.106119													
J								Multivariable grey prediction evolution algorithm: A new metaheuristic	APPLIED SOFT COMPUTING										Engineering design problems; Evolutionary algorithms; Grey prediction; MGM(1,n)	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; MODEL; GAS	The theoretical foundation of the grey prediction system, proposed by Deng J. in 1982, is built on the fact that appropriate conversion can transform unordered data to series data with an approximate exponential law under certain conditions. Inspired by the grey prediction theory, this paper introduces a novel evolutionary algorithm based on the multivariable grey prediction model MGM(1,n), called MGPEA. The proposed MGPEA considers the population series of an evolutionary algorithm as a time series. It first transforms the population data to series data with an approximate exponential law and then forecasts its next population using MGM(1,n). Philosophically, MGPEA implements the optimizing process by forecasting the development trend of the genetic information chain of a population sequence. The performance of MGPEA is validated on CEC2005 benchmark functions, CEC2014 benchmark functions and a test suite composed of five engineering constrained design problems. The comparative experiments show the effectiveness and superiority of MGPEA. The proposed MGPEA could be regard as a case of constructing metaheuristics by using the grey prediction model. It is hoped that this design idea leads to more metaheuristics inspired by other prediction models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106086	10.1016/j.asoc.2020.106086													
J								A constrained multi-objective evolutionary algorithm based on decomposition and dynamic constraint-handling mechanism	APPLIED SOFT COMPUTING										Constrained multi-objective optimization; MOEA/D; Constraint-handling techniques; Epsilon constraint-handling; Differential evolution	DIFFERENTIAL EVOLUTION; OPTIMIZATION PROBLEMS; MOEA/D	Constrained multi-objective optimization problems (CMOPs) are common in real-world engineering application, and are difficult to solve because of the conflicting nature of the objectives and many constraints. Some constrained multi-objective evolutionary algorithms (CMOEAs) have been developed for CMOPs, but they still suffer from the problems of easily getting trapped into local optimal solutions and low convergence. This paper introduces a multi-objective evolutionary algorithm based on decomposition and dynamic constraint-handling mechanism (MOEA/D-DCH) to tackle this issue. Firstly, the dynamic constraint-handling mechanism divides the search modes into the unconstrained search mode and the constrained search mode, which are dynamically adjusted by the generation number and the proportion of feasible solutions in the population. This mechanism could lead to a faster convergence than the traditional constraint-handling mechanisms. For the constrained search mode, an improved epsilon constraint-handling method is used to enhance the diversity of the population. Then, an individual update mechanism based on the best feasible solution of each sub-problem is designed to update the feasible individuals for maintaining the convergence of the feasible solutions. Finally, MOEA/D-DCH dynamically regulates the parameters of the differential evolution operator to enhance the local search ability. Experiments on 21 benchmark test functions are conducted to test MOEA/D-DCH and five other typical CMOEAs. Meanwhile, a real-world problem is employed to evaluate the practical performance of MOEA/D-DCH. MOEA/D-DCH achieves significantly better results than the other five algorithms on most of the test problems. The results indicate the effectiveness and competitiveness of MOEA/D-DCH for solving CMOPs. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106104	10.1016/j.asoc.2020.106104													
J								Energy-efficient face detection and recognition scheme for wireless visual sensor networks	APPLIED SOFT COMPUTING										Visual sensor network; Detection-accuracy and energy-consumption trade-off; Energy efficient face detection; Distinctive vector; Face recognition	PERFORMANCE	Energy-efficient and robust face detection and recognition scheme can be useful for many application fields such as security and surveillance in multimedia and visual sensor network (VSN). VSN consists of wireless resources-constrained nodes that are equipped with low-energy CMOS cameras for monitoring. On the one hand, captured images are meaningful multimedia-data that impose high energy consumption to be processed and transmitted. On the other hand, visual sensor (VS) is a battery-powered node with limited life-time. This situation leads to a trade-off between detection-accuracy and power-consumption. This trade-off is considered as the most major challenge for applications using multimedia data in wireless environments such as VSN. For optimizing this trade-off, a novel face detection and recognition scheme has been proposed in this paper based on VSN. In this scheme, detection phase is performed at VS and recognition phase is accomplished at the base station (sink). The contributions of this paper are in three folds: 1. Fast and energy-aware face-detection algorithm is proposed based on omitting non-human blobs and feature-based face detection in the considered human-blobs. 2. A novel energy-aware and secure algorithm for extracting light-weight discriminative vector of detected face-sequence to be sent to sink with low transmission-cost and high security level. 3. An efficient face recognition algorithm has been performed on the received vectors at the sink. The performance of our proposed scheme has been evaluated in terms of energy-consumption, detection and recognition accuracy. Experimental results, performed on standard datasets (FERET, Yale and CDnet) and on personal datasets, demonstrate the superiority of our scheme over the recent state-of-the-art methods. (C) 2019 Published by Elsevier B.V.																	1568-4946	1872-9681				APR	2020	89								106014	10.1016/j.asoc.2019.106014													
J								Adaptive LSSVM based iterative prediction method for NOx concentration prediction in coal-fired power plant considering system delay	APPLIED SOFT COMPUTING										Time series prediction; Soft sensor; Least square support vector machine; Cyber-physical system; Transfer entropy	SUPPORT VECTOR MACHINE; EMISSION; MODEL; OPTIMIZATION; SELECTION; SENSOR; CO2	Obtaining accurate and real-time value of the pollution concentration is fundamental to effective and energy-saving operation for pollution controlling in coal-fired power plants. However, accurate measurements for NOx concentration cannot be guaranteed, due to the intrinsic hardware and software design in sensors. In this paper, a prediction method, including variables processing and model regression, is proposed for NOx concentration measurement. Specifically, a set of variables is firstly selected adaptively as an input set by using modified transfer entropy (TE), while the relationships among them are guaranteed to be as weak as possible. Then, the input set can cover features without introducing redundant information to the prediction model , and the system delay is reduced based on the TE and sequential displacement. After the variables are processed, a forgetting factor online least square support vector machine (FFOLSSVM) is constructed to predict NOx concentration timely and accurately. The proposed method is the first work that takes the system delay into consideration for NOx prediction model, without mechanism analysis. The simulation indicates that the computational time and prediction accuracy requirements are sufficiently guaranteed by the proposed model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106070	10.1016/j.asoc.2020.106070													
J								Multi-objective railway alignment optimization considering costs and environmental impacts	APPLIED SOFT COMPUTING										Railway alignment design; Multi-objective optimization; Railway costs; Environmental impacts; Particle swarm optimization	HORIZONTAL ALIGNMENT; VERTICAL ALIGNMENT; DISTANCE TRANSFORM; PSO ALGORITHM; SELECTION; STEPWISE; MODELS; INDEX	With increasing transportation requirements in mountainous regions, railways are encroaching ever more on environmentally-sensitive areas in those regions. Selecting an economical and eco-friendly railway alignment can effectively minimize negative impacts on mountain environments while also reducing costs. To this end, this paper formulates the alignment design problem as a multi-objective optimization model, which includes both economic and environmental objectives. Two new quantitative indexes for measuring environmental impacts are proposed to reflect the degree of vegetation destruction and soil erosion. A multi-objective optimization method based on the particle swarm optimization (PSO) algorithm is proposed for seeking non-dominated solutions. New update mechanisms for dealing with the multi-objective optimization problem are devised. A local repair algorithm based on a customized crossover operator is designed to save promising alignment alternatives during the search process. Two real-world cases are used to demonstrate the effectiveness of the proposed method. The results show that it can trade off the economic and environmental objectives and bypass all the pre-specified forbidden zones, thus providing designers a set of non-dominated alignment alternatives. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106105	10.1016/j.asoc.2020.106105													
J								Hybridization of cognitive computing for food services	APPLIED SOFT COMPUTING										Linear regression; Random forest; Cognitive computing; Food service robots; Data mining; Restaurant industry; Fusion	RANDOM FOREST; PREDICTION; REGRESSION; CLASSIFICATION; FIELD	The application of data mining technology to food services and the restaurant industry has certain social value. By predicting customer traffic and needs, a restaurant can prepare a reasonable amount of meals for customers according to predicted needs which is conducive to improving the dining experience of customers and also improving the quality of food preparation and making the restaurant itself operate more efficiently. In recent years, we have seen the use of collaborative robots for use in the fast food industry. In Asia and more specifically in Japan, we have seen many fast-food chains implement the use of robots to better serve their customers. By studying the linear regression algorithm and the random forest algorithm, this paper proposes a new interwoven novel fusion approach of combining both algorithms and applies the new model to restaurant data to assist in the prediction of customer traffic in the restaurant industry. This predictive algorithm using cognitive techniques can assist these newly place robots in the food industry better serve their client base and in doing so make the industry more efficient. Experimental, comparison, and analysis are reported in the paper. The error rate of the fusion solution is reduced by approximately 5.503% compared with the linear regression algorithm and is approximately 3.719% lower than the error rate of the random forest algorithm. Results show that the new fusion algorithm can achieve better prediction results of customer traffic prediction for the restaurant industry. Furthermore, we also provide a new take on the application of data mining technology in the restaurant industry itself. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106051	10.1016/j.asoc.2019.106051													
J								An effective automatic system deployed in agricultural Internet of Things using Multi-Context Fusion Network towards crop disease recognition in the wild	APPLIED SOFT COMPUTING										Multiclass crop disease recognition; Convolutional Neural Network; Internet of Things; Multi-Context Fusion Network; ContextNet	CLASSIFICATION; FEATURES; MACHINE	Automatic crop disease recognition in the wild is a challenging topic in modern intelligent agriculture due to the appearance variances and cluttered background among crop diseases. To overcome these obstacles, the popular methods are to design a Convolutional Neural Network (CNN) model that extracts visual features and identifies crop disease images based on these features. These methods work well on laboratory environment under simple background but achieve low accuracy and poor robustness in processing the raw images captured from practical fields that contain inevitable noises. In this case, Internet of Things (IoT) is attracting increasing attention, with many alternatives to collect high-level contextual information that helps modern recognition system to effectively identify crop diseases in the wild. Motivated by the usefulness of agricultural IoT, a deep learning system using a novel approach named Multi-Context Fusion Network (MCFN), is developed to be deployed in agricultural IoT towards practical crop disease recognition in the wild. Our MCFN firstly adopts a standard CNN backbone to extract highly discriminative and robust visual features from over 50,000 in-field crop disease samples. Next, we exploit contextual features collected from image acquisition sensors as prior information to assist crop disease classification and reduce false positives in our presented ContextNet. Finally, a deep fully connected network is designed to fuse visual features as well as contextual features and output the crop disease prediction. Experimental results on 77 common crop diseases captured in our newly built domain specific dataset show that MCFN with the deep fusion model outperforms the state-of-the-art methods in wild crop disease recognition, and achieves a good identification accuracy of 97.5%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				APR	2020	89								106128	10.1016/j.asoc.2020.106128													
J								Regression and progression in stochastic domains	ARTIFICIAL INTELLIGENCE										Knowledge representation; Reasoning about action; Reasoning about knowledge; Reasoning about uncertainty; Cognitive robotics	NOISY SENSORS; LOGIC; KNOWLEDGE; CALCULUS	Reasoning about degrees of belief in uncertain dynamic worlds is fundamental to many applications, such as robotics and planning, where actions modify state properties and sensors provide measurements, both of which are prone to noise. With the exception of limited cases such as Gaussian processes over linear phenomena, belief state evolution can be complex and hard to reason with in a general way, especially when the agent has to deal with categorical assertions, incomplete information such as disjunctive knowledge, as well as probabilistic knowledge. Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive, allowing for such belief states to be expressed naturally as constraints. While that proposal is powerful, the task of how to plan effectively is not addressed. In fact, at a more fundamental level, the task of projection, that of reasoning about beliefs effectively after acting and sensing, is left entirely open. To aid planning algorithms, we study the projection problem in this work. In the reasoning about actions literature, there are two main solutions to projection: regression and progression. Both of these have proven enormously useful for the design of logical agents, essentially paving the way for cognitive robotics. Roughly, regression reduces a query about the future to a query about the initial state. Progression, on the other hand, changes the initial state according to the effects of each action and then checks whether the formula holds in the updated state. In this work, we show how both of these generalize in the presence of degrees of belief, noisy acting and sensing. Our results allow for both discrete and continuous probability distributions to be used in the specification of beliefs and dynamics. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103247	10.1016/j.artint.2020.103247													
J								Ethical approaches and autonomous systems	ARTIFICIAL INTELLIGENCE										Consequentialism; Deontology; Virtue ethics; Reasoning about actions; Ethical agents	ARGUMENTS; VALUES; NORMS	In this paper we consider how the three main approaches to ethics - deontology, consequentialism and virtue ethics - relate to the implementation of ethical agents. We provide a description of each approach and how agents might be implemented by designers following the different approaches. Although there are numerous examples of agents implemented within the consequentialist and deontological approaches, this is not so for virtue ethics. We therefore propose a novel means of implementing agents within the virtue ethics approach. It is seen that each approach has its own particular strengths and weaknesses when considered as the basis for implementing ethical agents, and that the different approaches are appropriate to different kinds of system. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103239	10.1016/j.artint.2020.103239													
J								Epistemic graphs for representing and reasoning with positive and negative influences of arguments	ARTIFICIAL INTELLIGENCE										Abstract argumentation; Epistemic argumentation; Bipolar argumentation	ABSTRACT ARGUMENTATION; SEMANTICS; SUPPORT; BIPOLAR; PERSUASION; FRAMEWORKS	This paper introduces epistemic graphs as a generalization of the epistemic approach to probabilistic argumentation. In these graphs, an argument can be believed or disbelieved up to a given degree, thus providing a more fine-grained alternative to the standard Dung's approaches when it comes to determining the status of a given argument. Furthermore, the flexibility of the epistemic approach allows us to both model the rationale behind the existing semantics as well as completely deviate from them when required. Epistemic graphs can model both attack and support as well as relations that are neither support nor attack. The way other arguments influence a given argument is expressed by the epistemic constraints that can restrict the belief we have in an argument with a varying degree of specificity. The fact that we can specify the rules under which arguments should be evaluated and we can include constraints between unrelated arguments permits the framework to be more context-sensitive. It also allows for better modelling of imperfect agents, which can be important in multi-agent applications. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103236	10.1016/j.artint.2020.103236													
J								Definability for model counting	ARTIFICIAL INTELLIGENCE										Definability; Model counting	SEARCH ALGORITHM; CNF; SAT; COMPLEXITY; ELIMINATION; GRASP	We define and evaluate a new preprocessing technique for propositional model counting. This technique leverages definability, i.e., the ability to determine that some gates are implied by the input formula Sigma. Such gates can be exploited to simplify Sigma without modifying its number of models. Unlike previous techniques based on gate detection and replacement, gates do not need to be made explicit in our approach. Our preprocessing technique thus consists of two phases: computing a bipartition < l, O > of the variables of Sigma where the variables from O are defined in E in terms of I, then eliminating some variables of O in Sigma. Our experiments show the computational benefits which can be achieved by taking advantage of our preprocessing technique for model counting. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103229	10.1016/j.artint.2019.103229													
J								Story embedding: Learning distributed representations of stories based on character networks	ARTIFICIAL INTELLIGENCE										Character network; Story analytics; Computational narrative; Story embedding; Story2Vec	COOCCURRENCE	This study aims to learn representations of stories in narrative works (i.e., creative works that contain stories) using fixed-length vectors. Vector representations of stories enable us to compare narrative works regardless of their media or formats. To computationally represent stories, we focus on social networks among characters (character networks). We assume that the structural features of the character networks reflect the characteristics of stories. By extending substructure-based graph embedding models, we propose models to learn distributed representations of character networks in stories. The proposed models consist of three parts: (i) discovering substructures of character networks, (ii) embedding each substructure (Char2Vec), and (iii) learning vector representations of each character network (Story2Vec). We find substructures around each character in multiple scales based on proximity between characters. We suppose that a character's substructures signify its 'social roles'. Subsequently, a Char2Vec model is designed to embed a social role based on co-occurred social roles. Since character networks are dynamic social networks that temporally evolve, we use temporal changes and adjacency of social roles to determine their co-occurrence. Finally, Story2Vec models predict occurrences of social roles in each story for embedding the story. To predict the occurrences, we apply two approaches: (i) considering temporal changes in social roles as with the Char2Vec model and (ii) focusing on the final social roles of each character. We call the embedding model with the first approach 'flow-oriented Story2Vec.' This approach can reflect the context and flow of stories if the dynamics of character networks is well understood. Second, based on the final states of social roles, we can emphasize the denouement of stories, which is an overview of the static structure of the character networks. We name this model as 'denouement-oriented Story2Vec.' In addition, we suggest 'unified Story2Vec' as a combination of these two models. We evaluated the quality of vector representations generated by the proposed embedding models using movies in the real world. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103235	10.1016/j.artint.2020.103235													
J								Synchronous bidirectional inference for neural sequence generation	ARTIFICIAL INTELLIGENCE										Sequence to sequence learning; Bidirectional inference; Beam search; Machine translation; Summarization		In sequence to sequence generation tasks (e.g. machine translation and abstractive summarization), inference is generally performed in a left-to-right manner to produce the result token by token. The neural approaches, such as LSTM and self-attention networks, are now able to make full use of all the predicted history hypotheses from left side during inference, but cannot meanwhile access any future (right side) information and usually generate unbalanced outputs (e.g. left parts are much more accurate than right ones in Chinese-English translation). In this work, we propose a synchronous bidirectional inference model to generate outputs using both left-to-right and right-to-left decoding simultaneously and interactively. First, we introduce a novel beam search algorithm that facilitates synchronous bidirectional decoding. Then, we present the core approach which enables left-to-right and right-to-left decoding to interact with each other, so as to utilize both the history and future predictions simultaneously during inference. We apply the proposed model to both LSTM and self-attention networks. Furthermore, we propose a novel fine-tuning based parameter optimization algorithm in addition to the simple two-pass strategy. The extensive experiments on machine translation and abstractive summarization demonstrate that our synchronous bidirectional inference model can achieve remarkable improvements over the strong baselines. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				APR	2020	281								103234	10.1016/j.artint.2020.103234													
J								Gehrlein stability in committee selection: parameterized hardness and algorithms	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Committee selection; Social choice; Parameterized complexity	PATHS	In a multiwinner election based on the Condorcet criterion, we are given a set of candidates, and a set of voters with strict preference rankings over the candidates. A committee is weakly Gehrlein stable (WGS) if each committee member is preferred to each non-member by at least half of the voters. Recently, Aziz et al. [IJCAI 2017] studied the computational complexity of finding a WGS committee of size k. They show that this problem is NP-hard in general and polynomial-time solvable when the number of voters is odd. In this article, we initiate a systematic study of the problem in the realm of parameterized complexity. We first show that the problem is W[1]-hard when parameterized by the size of the committee. To overcome this intractability result, we use a known reformulation of WGS as a problem on directed graphs and then use parameters that measure the "structure" of these directed graphs. We show that the problem is fixed parameter tractable and admits linear kernels with respect to these parameters; and also present an exact-exponential time algorithm with running in time O(1.2207(n) n(O(1))), where n denotes the number of candidates.																	1387-2532	1573-7454				APR	2020	34	1							27	10.1007/s10458-020-09452-z													
J								An anytime algorithm for optimal simultaneous coalition structure generation and assignment	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Coalition structure generation; Assignment; Coordination; Coalition formation; Combinatorial optimization	COOPERATIVE GAMES; MULTIAGENT SYSTEMS; TASK ALLOCATION; N-PLAYERS	An important research problem in artificial intelligence is how to organize multiple agents, and coordinate them, so that they can work together to solve problems. Coordinating agents in a multi-agent system can significantly affect the system's performance-the agents can, in many instances, be organized so that they can solve tasks more efficiently, and consequently benefit collectively and individually. Central to this endeavor is coalition formation-the process by which heterogeneous agents organize and form disjoint groups (coalitions). Coalition formation often involves finding a coalition structure (an exhaustive set of disjoint coalitions) that maximizes the system's potential performance (e.g., social welfare) through coalition structure generation. However, coalition structure generation typically has no notion of goals. In cooperative settings, where coordination of multiple coalitions is important, this may generate suboptimal teams for achieving and accomplishing the tasks and goals at hand. With this in mind, we consider simultaneously generating coalitions of agents and assigning the coalitions to independent alternatives (e.g., tasks/goals), and present an anytime algorithm for the simultaneous coalition structure generation and assignment problem. This combinatorial optimization problem hasmany real-world applications, including forming goal-oriented teams. To evaluate the presented algorithm's performance, we present five methods for synthetic problem set generation, and benchmark the algorithm against the industry-grade solver CPLEXusing randomized data sets of varying distribution and complexity. To test its anytime-performance, we compare the quality of its interim solutions against those generated by a greedy algorithm and pure random search. Finally, we also apply the algorithm to solve the problem of assigning agents to regions in a major commercial strategy game, and show that it can be used in game-playing to coordinate smaller sets of agents in real-time.																	1387-2532	1573-7454				APR	2020	34	1							29	10.1007/s10458-020-09450-1													
J								The impact of agent definitions and interactions on multiagent learning for coordination in traffic management domains	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multiagent systems; Cooperation and coordination; Intelligent agents; Agent definitions	SYSTEMS; FRAMEWORK	The state-action space of an individual agent in a multiagent team fundamentally dictates how the individual interacts with the rest of the team. Thus, how an agent is defined in the context of its domain has a significant effect on team performance when learning to coordinate. In this work we explore the trade-offs associated with these design choices, for example, having fewer agents in the team that individually are able to process and act on a wider scope of information about the world versus a larger team of agents where each agent observes and acts in a more local region of the domain. We focus our study on a traffic management domain and highlight the trends in learning performance when applying different agent definitions. In addition, we analyze the impact of agent failure for different agent definitions and investigate the ability of the team to learn new coordination strategies when individual agents become unresponsive.																	1387-2532	1573-7454				APR	2020	34	1							21	10.1007/s10458-020-09442-1													
J								Quantifying atrial anatomy uncertainty from clinical data and its impact on electro-physiology simulation predictions	MEDICAL IMAGE ANALYSIS										Uncertainty quantification; Cardiac models; Principal component analysis; Medical image processing	MODEL; GEOMETRY; HEART	Patient-specific computational models of structure and function are increasingly being used to diagnose disease and predict how a patient will respond to therapy. Models of anatomy are often derived after segmentation of clinical images or from mapping systems which are affected by image artefacts, resolution and contrast. Quantifying the impact of uncertain anatomy on model predictions is important, as models are increasingly used in clinical practice where decisions need to be made regardless of image quality. We use a Bayesian probabilistic approach to estimate the anatomy and to quantify the uncertainty about the shape of the left atrium derived from Cardiac Magnetic Resonance images. We show that we can quantify uncertain shape, encode uncertainty about the left atrial shape due to imaging artefacts, and quantify the effect of uncertain shape on simulations of left atrial activation times. (C) 2019 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				APR	2020	61								101626	10.1016/j.media.2019.101626													
J								Deep Atlas Network for Efficient 3D Left Ventricle Segmentation on Echocardiography	MEDICAL IMAGE ANALYSIS										3D left ventricle segmentation; Echocardiography; Deep atlas network; Information consistency constraint	ACTIVE APPEARANCE MODELS; NONRIGID REGISTRATION; CARDIAC MR; ULTRASOUND; SPARSE; SHAPE; QUANTIFICATION; REPRESENTATION; DELINEATION; FRAMEWORK	We proposed a novel efficient method for 3D left ventricle (LV) segmentation on echocardiography, which is important for cardiac disease diagnosis. The proposed method effectively overcame the 3D echocardiography's challenges: high dimensional data, complex anatomical environments, and limited annotation data. First, we proposed a deep atlas network, which integrated LV atlas into the deep learning framework to address the 3D LV segmentation problem on echocardiography for the first time, and improved the performance based on limited annotation data. Second, we proposed a novel information consistency constraint to enhance the model's performance from different levels simultaneously, and finally achieved effective optimization for 3D LV segmentation on complex anatomical environments. Finally, the proposed method was optimized in an end-to-end back propagation manner and it achieved high inference efficiency even with high dimensional data, which satisfies the efficiency requirement of clinical practice. The experiments proved that the proposed method achieved better segmentation results and a higher inference speed compared with state-of-the-art methods. The mean surface distance, mean hausdorff surface distance, and mean dice index were 1.52 mm, 5.6 mm and 0.97 respectively. What's more, the method is efficient and its inference time is 0.02s. The experimental results proved that the proposed method has a potential clinical application for 3D LV segmentation on echocardiography. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101638	10.1016/j.media.2020.101638													
J								Detecting genetic associations with brain imaging phenotypes in Alzheimer's disease via a novel structured SCCA approach	MEDICAL IMAGE ANALYSIS										Brain imaging genetics; Sparse canonical correlation analysis (SCCA); Fused pairwise group Lasso; Graph guided pairwise group Lasso	CANONICAL CORRELATION-ANALYSIS; SPARSE; REGRESSION; ALGORITHM; MCI; AD	Brain imaging genetics becomes an important research topic since it can reveal complex associations between genetic factors and the structures or functions of the human brain. Sparse canonical correlation analysis (SCCA) is a popular bi-multivariate association identification method. To mine the complex genetic basis of brain imaging phenotypes, there arise many SCCA methods with a variety of norms for incorporating different structures of interest. They often use the group lasso penalty, the fused lasso or the graph/network guided fused lasso ones. However, the group lasso methods have limited capability because of the incomplete or unavailable prior knowledge in real applications. The fused lasso and graph/network guided methods are sensitive to the sign of the sample correlation which may be incorrectly estimated. In this paper, we introduce two new penalties to improve the fused lasso and the graph/network guided lasso penalties in structured sparse learning. We impose both penalties to the SCCA model and propose an optimization algorithm to solve it. The proposed SCCA method has a strong upper bound of grouping effects for both positively and negatively highly correlated variables. We show that, on both synthetic and real neuroimaging genetics data, the proposed SCCA method performs better than or equally to the conventional methods using fused lasso or graph/network guided fused lasso. In particular, the proposed method identifies higher canonical correlation coefficients and captures clearer canonical weight patterns, demonstrating its promising capability in revealing biologically meaningful imaging genetic associations. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101656	10.1016/j.media.2020.101656													
J								SDCT-AuxNet(theta): DCT augmented stain deconvolutional CNN with auxiliary classifier for cancer diagnosis	MEDICAL IMAGE ANALYSIS										Acute lymphoblastic leukemia; ALL diagnosis; Cell classification; Convolutional neural network; Deep learning	ACUTE LYMPHOBLASTIC-LEUKEMIA; COMPUTER-AIDED DETECTION; NETWORKS; FEATURES	Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell cancer across the globe. With the popularity of convolutional neural networks (CNNs), computer-aided diagnosis of cancer has attracted considerable attention. Such tools are easily deployable and are cost-effective. Hence, these can enable extensive coverage of cancer diagnostic facilities. However, the development of such a tool for ALL cancer was challenging so far due to the non-availability of a large training dataset. The visual similarity between the malignant and normal cells adds to the complexity of the problem. This paper discusses the recent release of a large dataset and presents a novel deep learning architecture for the classification of cell images of ALL cancer. The proposed architecture, namely, SDCT-AtaNet(theta) is a 2-module framework that utilizes a compact CNN as the main classifier in one module and a Kernel SVM as the auxiliary classifier in the other one. While CNN classifier uses features through bilinear-pooling, spectral-averaged features are used by the auxiliary classifier. Further, this CNN is trained on the stain deconvolved quantity images in the optical density domain instead of the conventional RGB images. A novel test strategy is proposed that exploits both the classifiers for decision making using the confidence scores of their predicted class labels. Elaborate experiments have been carried out on our recently released public dataset of 15114 images of ALL cancer and healthy cells to establish the validity of the proposed methodology that is also robust to subject-level variability. A weighted Fl score of 94.8% is obtained that is best so far on this challenging dataset. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101661	10.1016/j.media.2020.101661													
J								Improving cardiac MRI convolutional neural network segmentation on small training datasets and dataset shift: A continuous kernel cut approach	MEDICAL IMAGE ANALYSIS										Cardiac MRI segmentation; Normalized cuts; Continuous max-flow; Convex optimization	AUTOMATED SEGMENTATION; LEFT-VENTRICLE; HEART	Cardiac magnetic resonance imaging (MRI) provides a wealth of imaging biomarkers for cardiovascular disease care and segmentation of cardiac structures is required as a first step in enumerating these biomarkers. Deep convolutional neural networks (CNNs) have demonstrated remarkable success in image segmentation but typically require large training datasets and provide suboptimal results that require further improvements. Here, we developed a way to enhance cardiac MRI multi-class segmentation by combining the strengths of CNN and interpretable machine learning algorithms. We developed a continuous kernel cut segmentation algorithm by integrating normalized cuts and continuous regularization in a unified framework. The high-order formulation was solved through upper bound relaxation and a continuous max-flow algorithm in an iterative manner using CNN predictions as inputs. We applied our approach to two representative cardiac MRI datasets across a wide range of cardiovascular pathologies. We comprehensively evaluated the performance of our approach for two CNNs trained with various small numbers of training cases, tested on the same and different datasets. Experimental results showed that our approach improved baseline CNN segmentation by a large margin, reduced CNN segmentation variability substantially, and achieved excellent segmentation accuracy with minimal extra computational cost. These results suggest that our approach provides a way to enhance the applicability of CNN by enabling the use of smaller training datasets and improving the segmentation accuracy and reproducibility for cardiac MRI segmentation in research and clinical patient care. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101636	10.1016/j.media.2020.101636													
J								Multi-task learning for the segmentation of organs at risk with label dependence	MEDICAL IMAGE ANALYSIS										Segmentation of organs at risk; Encoder-decoder networks; Label dependence; Multi-label classification	NEURAL-NETWORKS; LOCALIZATION	Automatic segmentation of organs at risk is crucial to aid diagnoses and remains a challenging task in medical image analysis domain. To perform the segmentation, we use multi-task learning (MTL) to accurately determine the contour of organs at risk in CT images. We train an encoder-decoder network for two tasks in parallel. The main task is the segmentation of organs, entailing a pixel-level classification in the CT images, and the auxiliary task is the multi-label classification of organs, entailing an image-level multi-label classification of the CT images. To boost the performance of the multi-label classification, we propose a weighted mean cross entropy loss function for the network training, where the weights are the global conditional probability between two organs. Based on MTL, we optimize the false positive filtering (FPF) algorithm to decrease the number of falsely segmented organ pixels in the CT images. Specifically, we propose a dynamic threshold selection (DTS) strategy to prevent true positive rates from decreasing when using the FPF algorithm. We validate these methods on the public ISBI 2019 segmentation of thoracic organs at risk (SegTHOR) challenge dataset and a private medical organ dataset. The experimental results show that networks using our proposed methods outperform basic encoder-decoder networks without increasing the training time complexity. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101666	10.1016/j.media.2020.101666													
J								Segmentation of breast ultrasound image with semantic classification of superpixels	MEDICAL IMAGE ANALYSIS										Image segmentation; Semantic classification; Breast tumor; Ultrasound	GRAPH-BASED SEGMENTATION; MODEL	Breast cancer is a great threat to females. Ultrasound imaging has been applied extensively in diagnosis of breast cancer. Due to the poor image quality, segmentation of breast ultrasound (BUS) image remains a very challenging task. Besides, BUS image segmentation is a crucial step for further analysis. In this paper, we proposed a novel method to segment the breast tumor via semantic classification and merging patches. The proposed method firstly selects two diagonal points to crop a region of interest (ROI) on the original image. Then, histogram equalization, bilateral filter and pyramid mean shift filter are adopted to enhance the image. The cropped image is divided into many superpixels using simple linear iterative clustering (SLIC). Furthermore, some features are extracted from the superpixels and a bag-of-words model can be created. The initial classification can be obtained by a back propagation neural network (BPNN). To refine preliminary result, k-nearest neighbor (KNN) is used for reclassification and the final result is achieved. To verify the proposed method, we collected a BUS dataset containing 320 cases. The segmentation results of our method have been compared with the corresponding results obtained by five existing approaches. The experimental results show that our method achieved competitive results compared to conventional methods in terms of TP and FP, and produced good approximations to the hand-labelled tumor contours with comprehensive consideration of all metrics (the F1-score = 89.87% +/- 4.05%, and the average radial error = 9.95% +/- 4.42%). (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101657	10.1016/j.media.2020.101657													
J								Self-calibrated brain network estimation and joint non-convex multi-task learning for identification of early Alzheimer's disease	MEDICAL IMAGE ANALYSIS										Early stage of Alzheimer's disease (AD); Brain network estimation; Self-calibration; Multi-modal classification; Joint non-convex multi-task learning	MILD COGNITIVE IMPAIRMENT	Detection of early stages of Alzheimer's disease (AD) (i.e., mild cognitive impairment (MCI)) is important to maximize the chances to delay or prevent progression to AD. Brain connectivity networks inferred from medical imaging data have been commonly used to distinguish MCI patients from normal controls (NC). However, existing methods still suffer from limited performance, and classification remains mainly based on single modality data. This paper proposes a new model to automatically diagnosing MCI (early MCI (EMCI) and late MCI (LMCI)) and its earlier stages (i.e., significant memory concern (SMC)) by combining low-rank self-calibrated functional brain networks and structural brain networks for joint multi-task learning. Specifically, we first develop a new functional brain network estimation method. We introduce data quality indicators for self-calibration, which can improve data quality while completing brain network estimation, and perform correlation analysis combined with low-rank structure. Second, functional and structural connected neuroimaging patterns are integrated into our multi-task learning model to select discriminative and informative features for fine MCI analysis. Different modalities are best suited to undertake distinct classification tasks, and similarities and differences among multiple tasks are best determined through joint learning to determine most discriminative features. The learning process is completed by non-convex regularizer, which effectively reduces the penalty bias of trace norm and approximates the original rank minimization problem. Finally, the most relevant disease features classified using a support vector machine (SVM) for MCI identification. Experimental results show that our method achieves promising performance with high classification accuracy and can effectively discriminate between different sub-stages of MCI. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101652	10.1016/j.media.2020.101652													
J								Adaptive sparse learning using multi-template for neurodegenerative disease diagnosis	MEDICAL IMAGE ANALYSIS										Neurodegenerative disease diagnosis; Adaptive sparse learning; Feature learning; Multi-template Multi-classification	PARKINSONS-DISEASE; FEATURE-SELECTION; CLASSIFICATION; BIOMARKERS; MACHINE; PROGRESSION; IMAGES	Neurodegenerative diseases are excessively affecting millions of patients, especially elderly people. Early detection and management of these diseases are crucial as the clinical symptoms take years to appear after the onset of neuro-degeneration. This paper proposes an adaptive feature learning framework using multiple templates for early diagnosis. A multi-classification scheme is developed based on multiple brain parcellation atlases with various regions of interest. Different sets of features are extracted and then fused, and a feature selection is applied with an adaptively chosen sparse degree. In addition, both linear discriminative analysis and locally preserving projections are integrated to construct a least square regression model. Finally, we propose a feature space to predict the severity of the disease by the guidance of clinical scores. Our proposed method is validated on both Alzheimer's disease neuroimaging initiative and Parkinson's progression markers initiative databases. Extensive experimental results suggest that the proposed method outperforms the state-of-the-art methods, such as the multi-modal multi-task learning or joint sparse learning. Our method demonstrates that accurate feature learning facilitates the identification of the highly relevant brain regions with significant contribution in the prediction of disease progression. This may pave the way for further medical analysis and diagnosis in practical applications. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101632	10.1016/j.media.2019.101632													
J								Temporal non-local means filtering for studies of intrinsic brain connectivity from individual resting fMRI	MEDICAL IMAGE ANALYSIS										Connectivity; FMRI; Non-local means filtering; Optimization	FUNCTIONAL CONNECTIVITY; ACTIVATION; CORTEX	Characterizing functional brain connectivity using resting functional magnetic resonance imaging (fMRI) is challenging due to the relatively small Blood-Oxygen-Level Dependent contrast and low signal-to-noise ratio. Denoising using surface-based Laplace-Beltrami (LB) or volumetric Gaussian filtering tends to blur boundaries between different functional areas. To overcome this issue, a time-based Non-Local Means (tNLM) filtering method was previously developed to denoise fMRI data while preserving spatial structure. The kernel and parameters that define the tNLM filter need to be optimized for each application. Here we present a novel Global PDF-based tNLM filtering (GPDF) algorithm that uses a data-driven kernel function based on a Bayes factor to optimize filtering for spatial delineation of functional connectivity in resting fMRI data. We demonstrate its performance relative to Gaussian spatial filtering and the original tNLM filtering via simulations. We also compare the effects of GPDF filtering against LB filtering using individual in-vivo resting fMRI datasets. Our results show that LB filtering tends to blur signals across boundaries between adjacent functional regions. In contrast, GPDF filtering enables improved noise reduction without blurring adjacent functional regions. These results indicate that GPDF may be a useful preprocessing tool for analyses of brain connectivity and network topology in individual fMRI recordings. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101635	10.1016/j.media.2020.101635													
J								On the identification of the blood vessel confounding effect in intravoxel incoherent motion (IVIM) Diffusion-Weighted (DW)-MRI in liver: An efficient sparsity based algorithm	MEDICAL IMAGE ANALYSIS										IVIM-MRI; Diffusion-Weighted MRI; Sparsity; Proximal optimization	PERFUSION; MRI	IntraVoxel Incoherent Motion (IVIM) Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI) is of great interest for evaluating tissue diffusion and perfusion and producing parametric maps in clinical applications for liver pathologies. However, the presence of macroscopic blood vessels (not capillaries) in a given Region of Interest (ROI) results in a confounding effect that bias the quantification of tissue perfusion. Therefore, it is necessary to identify those voxels affected by blood vessels. In this paper, an efficient algorithm for an automatic identification of blood vessels in a given ROl is proposed. It relies on the sparsity of the spatial distribution of blood vessels. This sparsity prior can be easily incorporated using the all-voxel IVIM-MRI model introduced in this paper. In addition to the identification of blood vessels, the proposed algorithm provides a quantification of blood vessels, tissue diffusion and tissue perfusion of all voxels in a given ROI, in one single step. Besides, two strategies are proposed in this paper to deal with the nonnegativity of the model parameters. The efficiency of the proposed algorithm compared to the Non-Negative Least Square (NNLS)-based method, recently introduced to deal with the confounding blood vessel effect in the IVIM-MRI model, is confirmed using both realistic and real DW-MR images. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101637	10.1016/j.media.2020.101637													
J								Learning metal artifact reduction in cardiac CT images with moving pacemakers	MEDICAL IMAGE ANALYSIS										Cardiac CT; Metal artifact reduction; Convolutional neural network	RAY COMPUTED-TOMOGRAPHY	Metal objects in the human heart such as implanted pacemakers frequently lead to heavy artifacts in reconstructed CT image volumes. Due to cardiac motion, common metal artifact reduction methods which assume a static object during CT acquisition are not applicable. We propose a fully automatic Dynamic Pacemaker Artifact Reduction (DyPAR+) pipeline which is built of three convolutional neural network (CNN) ensembles. In a first step, pacemaker metal shadows are segmented directly in the raw projection data by the SegmentationNets. Second, resulting metal shadow masks are passed to the InpaintingNets which replace metal-affected line integrals in the sinogram for subsequent reconstruction of a metal-free image volume. Third, the metal locations in a pre-selected motion state are predicted by the ReinsertionNets based on a stack of partial angle back-projections generated from the segmented metal shadow mask. We generate the data required for the supervised learning processes by introducing synthetic, moving pacemaker leads into 14 clinical cases without pacemakers. The SegmentationNets and the ReinsertionNets achieve average Dice coefficients of 94.16% +/- 2.01% and 55.60% +/- 4.79% during testing on clinical data with synthetic metal leads. With a mean absolute reconstruction error of 11.54 HU +/- 2.49 HU in the image domain, the InpaintingNets outperform the hand-crafted approaches PatchMatch and inverse distance weighting. Application of the proposed DyPAR+ pipeline to nine clinical test cases with real pacemakers leads to significant reduction of metal artifacts and demonstrates the transferability to clinical practice. Especially the SegmentationNets and InpaintingNets generalize well to unseen acquisition modes and contrast protocols. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101655	10.1016/j.media.2020.101655													
J								Dynamic coronary roadmapping via catheter tip tracking in X-ray fluoroscopy with deep learning based Bayesian filtering	MEDICAL IMAGE ANALYSIS										Dynamic coronary roadmapping; X-ray fluoroscopy; Catheter tip tracking; Deep learning; Bayesian filtering; Particle filter	ROBUST GUIDEWIRE TRACKING; RESPIRATORY MOTION; IMAGE; INSTRUMENTS; MODEL	Percutaneous coronary intervention (PCI) is typically performed with image guidance using X-ray angiograms in which coronary arteries are opacified with X-ray opaque contrast agents. Interventional cardiologists typically navigate instruments using non-contrast-enhanced fluoroscopic images, since higher use of contrast agents increases the risk of kidney failure. When using fluoroscopic images, the interventional cardiologist needs to rely on a mental anatomical reconstruction. This paper reports on the development of a novel dynamic coronary roadmapping approach for improving visual feedback and reducing contrast use during PCI. The approach compensates cardiac and respiratory induced vessel motion by ECG alignment and catheter tip tracking in X-ray fluoroscopy, respectively. In particular, for accurate and robust tracking of the catheter tip, we proposed a new deep learning based Bayesian filtering method that integrates the detection outcome of a convolutional neural network and the motion estimation between frames using a particle filtering framework. The proposed roadmapping and tracking approaches were validated on clinical X-ray images, achieving accurate performance on both catheter tip tracking and dynamic coronary roadmapping experiments. In addition, our approach runs in real-time on a computer with a single GPU and has the potential to be integrated into the clinical workflow of PCI procedures, providing cardiologists with visual guidance during interventions without the need of extra use of contrast agent. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101634	10.1016/j.media.2020.101634													
J								Automatic detection of rare pathologies in fundus photographs using few-shot learning	MEDICAL IMAGE ANALYSIS										Diabetic retinopathy screening; Rare conditions; Deep learning; Few-shot learning	DIABETIC-RETINOPATHY; VALIDATION; ALGORITHM; SYSTEM	In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101660	10.1016/j.media.2020.101660													
J								Domain-invariant interpretable fundus image quality assessment	MEDICAL IMAGE ANALYSIS										Fundus image quality assessment; Domain adaptation; Interpretability; Multi-task learning		Objective and quantitative assessment of fundus image quality is essential for the diagnosis of retinal diseases. The major factors in fundus image quality assessment are image artifact, clarity, and field definition. Unfortunately, most of existing quality assessment methods focus on the quality of overall image, without interpretable quality feedback for real-time adjustment. Furthermore, these models are often sensitive to the specific imaging devices, and cannot generalize well under different imaging conditions. This paper presents a new multi-task domain adaptation framework to automatically assess fundus image quality. The proposed framework provides interpretable quality assessment with both quantitative scores and quality visualization for potential real-time image recapture with proper adjustment. In particular, the present approach can detect optic disc and fovea structures as landmarks, to assist the assessment through coarse-to-fine feature encoding. The framework also exploit semi-tied adversarial discriminative domain adaptation to make the model generalizable across different data sources. Experimental results demonstrated that the proposed algorithm outperforms different state-of-the-art approaches and achieves an area under the ROC curve of 0.9455 for the overall quality classification. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101654	10.1016/j.media.2020.101654													
J								Learning a cortical parcellation of the brain robust to the MRI segmentation with convolutional neural networks	MEDICAL IMAGE ANALYSIS										Cortical parcellation; Deep-learning; Large cohorts	DIFFEOMORPHIC IMAGE REGISTRATION; OPEN ACCESS SERIES; AUTOMATIC SEGMENTATION; ATLAS; CLASSIFICATION; CORTEX; SULCI; ALIGNMENT; ACCURATE; CANCER	The parcellation of the human cortex into meaningful anatomical units is a common step of various neuroimaging studies. There have been multiple successful efforts to process magnetic resonance (MR) brain images automatically and identify specific anatomical regions, following atlases defined from cortical landmarks. Those definitions usually rely first on a high-quality brain surface reconstruction. On the other hand, when high accuracy is not a requirement, simpler methods based on warping a probabilistic atlas have been widely adopted. Here, we develop a cortical parcellation method for MR brain images based on Convolutional Neural Networks (ConvNets), a machine-learning method, with the goal of automatically transferring the knowledge obtained from surface analyses onto something directly applicable on simpler volume data. We train a ConvNet on a large (thousand) set of cortical ribbons of multiple MRI cohorts, to reproduce parcellations obtained from a surface method, in this case FreeSurfer. Further, to make the model applicable in a broader context, we force the model to generalize to unseen segmentations. The model is evaluated on unseen data of unseen cohorts. We characterize the behavior of the model during learning, and quantify its reliance on the dataset itself, which tends to give support for the necessity of large training sets, augmentation, and multiple contrasts. Overall, ConvNets can provide an efficient way to parcel MRI images, following the guidance established within more complex methods, quickly and accurately. The trained model is embedded within a open-source parcellation tool available at https://github.com/bthyreau/parcelcortex. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101639	10.1016/j.media.2020.101639													
J								Automatic diagnosis for thyroid nodules in ultrasound images by deep neural networks	MEDICAL IMAGE ANALYSIS										Thyroid cancer; Ultrasonography; Deep neural networks; Attention mechanism	CANCER; CLASSIFICATION; MANAGEMENT; TRENDS	Thyroid cancer is a disease in which the first symptom is a nodule in the thyroid region of the neck. It is one of the cancers with the highest incidences, and has the highest increase rate in the last thirty years. Ultrasonography is one of the most sensitive and widely used methods for detecting thyroid nodules. To assist in the analysis of thyroid ultrasound images, many computer-aided diagnosis methods have been proposed. Most of these methods perform diagnosis using only a single ultrasound image instead of using all images from an examination, which loses the overall information related to the thyroid nodules. However, in an ultrasound examination, the sonographer analyzes the thyroid nodule based on multiple images from different views. In the current study, a deep learning method is proposed to diagnose thyroid nodules using multiple ultrasound images in an examination as input. An attention-based feature aggregation network is proposed to automatically integrate the features extracted from multiple images in one examination, utilizing different views of the nodules to improve the performance of recognizing malignant nodules in the ultrasound images. To train and evaluate the proposed method, a large dataset is constructed. The experimental results demonstrate that our method achieves comparable performance with state-of-the-art methods for the diagnosis of thyroid ultrasound images. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101665	10.1016/j.media.2020.101665													
J								An improved deep network for tissue microstructure estimation with uncertainty quantification	MEDICAL IMAGE ANALYSIS										Tissue microstructure; Deep network; Separable dictionary; Uncertainty quantification	DIFFUSION; RECOVERY; SIGNAL	Deep learning based methods have improved the estimation of tissue microstructure from diffusion magnetic resonance imaging (dMRI) scans acquired with a reduced number of diffusion gradients. These methods learn the mapping from diffusion signals in a voxel or patch to tissue microstructure measures. In particular, it is beneficial to exploit the sparsity of diffusion signals jointly in the spatial and angular domains, and the deep network can be designed by unfolding iterative processes that adaptively incorporate historical information for sparse reconstruction. However, the number of network parameters is huge in such a network design, which could increase the difficulty of network training and limit the estimation performance. In addition, existing deep learning based approaches to tissue microstructure estimation do not provide the important information about the uncertainty of estimates. In this work, we continue the exploration of tissue microstructure estimation using a deep network and seek to address these limitations. First, we explore the sparse representation of diffusion signals with a separable spatial-angular dictionary and design an improved deep network for tissue microstructure estimation. The procedure for updating the sparse code associated with the separable dictionary is derived and unfolded to construct the deep network. Second, with the formulation of sparse representation of diffusion signals, we propose to quantify the uncertainty of network outputs with a residual bootstrap strategy. Specifically, because of the sparsity constraint in the signal representation, we perform a Lasso bootstrap strategy for uncertainty quantification. Experiments were performed on brain dMRI scans with a reduced number of diffusion gradients, where the proposed method was applied to two representative biophysical models for describing tissue microstructure and compared with state-of-the-art methods of tissue microstructure estimation. The results show that our approach compares favorably with the competing methods in terms of estimation accuracy. In addition, the uncertainty measures provided by our method correlate with estimation errors and produce reasonable confidence intervals; these results suggest potential application of the proposed uncertainty quantification method in brain studies. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101650	10.1016/j.media.2020.101650													
J								HeadLocNet: Deep convolutional neural networks for accurate classification and multi-landmark localization of head CTs	MEDICAL IMAGE ANALYSIS										Cochlear implants; Landmark localization; 3D image classification; 3D U-Net	ANATOMICAL STRUCTURES; FORESTS	Cochlear implants (Cls) are used to treat subjects with hearing loss. In a Cl surgery, an electrode array is inserted into the cochlea to stimulate auditory nerves. After surgery, Cls need to be programmed. Studies have shown that the cochlea-electrode spatial relationship derived from medical images can guide Cl programming and lead to significant improvement in hearing outcomes. We have developed a series of algorithms to segment the inner ear anatomy and localize the electrodes. But, because clinical head CT images are acquired with different protocols, the field of view and orientation of the image volumes vary greatly. As a consequence, visual inspection and manual image registration to an atlas image are needed to document their content and to initialize intensity-based registration algorithms used in our processing pipeline. For large-scale evaluation and deployment of our methods these steps need to be automated. In this article we propose to achieve this with a deep convolutional neural network (CNN) that can be trained end-to-end to classify a head CT image in terms of its content and to localize landmarks. The detected landmarks can then be used to estimate a point-based registration with the atlas image in which the same landmark set's positions are known. We achieve 99.5% classification accuracy and an average localization error of 3.45 mm for 7 landmarks located around each inner ear. This is better than what was achieved with earlier methods we have proposed for the same tasks. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				APR	2020	61								101659	10.1016/j.media.2020.101659													
J								Model primitives for hierarchical lifelong reinforcement learning	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Reinforcement learning; Task decomposition; Transfer; Lifelong learning; Hierarchical learning	CORTEX	Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Such decomposition can lead to immense sample efficiency gains in lifelong learning. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This article presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. Given these world models, this framework performs decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single-task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.																	1387-2532	1573-7454				APR	2020	34	1							28	10.1007/s10458-020-09451-0													
J								ForeSim-BI: A predictive analytics decision support tool for capacity planning	DECISION SUPPORT SYSTEMS										Decision support systems; Predictive analytics; Capacity planning; Forecasting; Maintenance	QUANTITATIVE FORECASTING METHODS; AIRCRAFT MAINTENANCE; MODELS; SYSTEMS; OPERATIONS; SELECTION	This paper proposes a decision support tool for maintenance capacity planning of complex product systems. The tool - ForeSim-BI - addresses the problem faced by maintenance organizations in forecasting the workload of future maintenance interventions and in planning an adequate capacity to face that expected workload. Developed and implemented from a predictive analytics perspective in the particular context of a Portuguese aircraft maintenance organization, the tool integrates four main modules: (1) a forecasting module used to predict future and unprecedented maintenance workloads from historical data; (2) a Bayesian inference module used to transform prior workload forecasts, resulting from the forecasting module, into predictive forecasts after observations on the maintenance interventions being predicted become available; (3) a simulation module used to characterize the forecasted total workloads through sets of random variables, including maintenance work types, maintenance work phases, and maintenance work skills; and (4) a Bayesian network module used to combine the simulated workloads with historical data through probabilistic inference. A linear programming model is also developed to improve the efficiency of the decision-making process supported by Bayesian networks. The tool uses real industrial data, comprising 171 aircraft maintenance projects collected at the host organization, and is validated by comparing its results with real observations of a given maintenance intervention to which predictions were made and with a model simulating current forecasting practices employed in industry. Significantly more accurate forecasts have been obtained with the proposed tool, resulting in an important cost saving potential for maintenance organizations.																	0167-9236	1873-5797				APR	2020	131								113266	10.1016/j.dss.2020.113266													
J								Online media coverage, consumer engagement and movie sales: A PVAR approach	DECISION SUPPORT SYSTEMS										Media coverage; Consumer engagement; Movie sales; Panel vector auto-regression	WORD-OF-MOUTH; BOX-OFFICE; PRODUCT SALES; REVIEWS; PUBLICITY; DYNAMICS; BEHAVIOR; SEARCH; TWEETS; IMPACT	The advent of new media, such as Google, Twitter, and Facebook, has offered the movie industry new opportunities to market movies and increase sales. In this study, we investigate the relationship between online media activities and movie sales. In particular, this paper examines how media coverage (online news and social media posts) and consumer engagement (information search and reading) affect the sales of original movies and sequels in the opening week. By applying the panel vector auto-regression (PVAR) method, we found that online news, social media posts, and information reading were positively related to future sales of originals and sequels, while information search was not a significant predictor of movie sales for originals. Media coverage had a stronger and more durable marketing effect on the sales of originals, and consumer engagement had a stronger effect on the sales of sequels. Compared with social media posts, online news had a higher promoting effect on consumer engagement for sequels. Further, online media coverage and consumer engagement also had mutual effects on each other. This study suggests that managers of movie studios and theaters should apply different channel strategies and allocate different resources for originals and sequels.																	0167-9236	1873-5797				APR	2020	131								113267	10.1016/j.dss.2020.113267													
J								Is optimal recommendation the best? A laboratory investigation under the newsvendor problem	DECISION SUPPORT SYSTEMS										Newsvendor; Decision support system; Algorithm aversion; Regret aversion; Behavioral operations management	DECISION-SUPPORT-SYSTEM; REGRET AVERSION; CONSEQUENCES; ACCEPTANCE; ALGORITHMS; BULLWHIP; FEEDBACK; BEHAVIOR; ADOPTION; MODELS	We investigate the impacts of the decision support system's recommendations on decision makers' psychology and decision behaviors under uncertain contexts where optimal solutions exist. As a representative of such contexts, the newsvendor problem is studied by using the method of laboratory experiments. Through providing an elaborately designed decision support system in Experiment I, we validate that the optimal recommendations help to alleviate human newsvendors' Pull-to-Center bias, i.e., the actual orders fall in the range between mean demand and optimal order that maximizes the expected profit theoretically, and decrease the bias asymmetry under two profit conditions (high or low). We also reveal that optimal recommendations can't eliminate the bias, as decision makers exhibit two competing psychological factors simultaneously when using the decision support system: algorithm aversion and regret aversion. Algorithm aversion persistently impedes them from following the superior recommendations, while regret aversion sometimes pulls them to approach to the recommendations driven by the feeling of experienced regret. Further, we redesign the decision support system in Experiment II and find that, although the conservative system recommendations are valueless compared with the optimal one, the well-designed radical system recommendations may eliminate the Pull-to-Center bias under the high-profit condition, through the interaction of the dominant regret aversion, dominated algorithm aversion, and the anchoring effect.																	0167-9236	1873-5797				APR	2020	131								113251	10.1016/j.dss.2020.113251													
J								Pricing and advertising for reward-based crowdfunding products in E-commerce	DECISION SUPPORT SYSTEMS										Crowdfunding; Regular selling; Price; Advertising; E-commerce	BIRDS	This paper studies a reward-based crowdfunding game wherein the creator first collects money through an E-crowdfunding platform and then, if successful, invests the raised money to fund promotional activities via online retailing. We derive the creator's equilibrium decisions and the corresponding payoff and show that they are significantly influenced by the potential demand and the gap between different types of buyer valuations. In particular, with the change in potential market size from low to high, the creator will choose investing none, part or all of the crowdfunded money to improve the advertising level. Meanwhile, when the market size is small, the creator tends to set a high price; when it becomes larger, the creator prefers menu prices or a low price to guarantee the success of the crowdfunding campaign. Comparing two scenarios, one with and one without considering advertising investment, creators may never choose the low price if they cannot raise enough money for advertising; however, that situation can never arise without considering the option of advertising investment. These results provide implications for launching a successful crowdfunding campaign and product promotion/ advertising in this E-commerce era.																	0167-9236	1873-5797				APR	2020	131								113231	10.1016/j.dss.2019.113231													
J								Conversational Recommender Systems and natural language: A study through the ConveRSE framework	DECISION SUPPORT SYSTEMS										Conversational Recommender Systems; Domain-independent recommender systems; Natural language processing	AGENT	Digital Assistants (DA) such as Amazon Alexa, Siri, or Google Assistant are now gaining great diffusion, since they allow users to execute a wide range of actions through messages in natural language. Even though DAs are able to complete tasks such as sending texts, making phone calls, or playing songs, they do not yet implement recommendation facilities. In this paper, we investigate the combination of Digital Assistants and Conversational Recommender Systems (CoRSs) by designing and implementing a framework named ConveRSE (Conversational Recommender System framEwork), for building chatbots that can recommend items from different domains and interact with the user through natural language. Since a CoRS architecture is generally composed of different elements, we performed an in-vitro experiment with two synthetic datasets, to investigate the impact that each component has on the CoRS in terms of recommendation accuracy. Additionally, an in-vivo experiment was carried out to understand how natural language influences both the cost of interaction and recommendation accuracy of a CoRS. Experimental results have revealed the most critical components in a CoRS architecture, especially in cold-start situations, and the main issues of the natural-language-based interaction. All the dialogues have been collected in a public available dataset.																	0167-9236	1873-5797				APR	2020	131								113250	10.1016/j.dss.2020.113250													
J								A decision-making support system for Enterprise Architecture Modelling	DECISION SUPPORT SYSTEMS										Enterprise Architecture; Viewpoint; Genetic algorithm; Reverse engineering; ArchiMate	MANAGEMENT	Companies are increasingly conscious of the importance of Enterprise Architecture (EA) to represent and manage IT and business in a holistic way. EA modelling has become decisive to achieve models that accurately represents behaviour and assets of companies and lead them to make appropriate business decisions. Although EA representations can be manually modelled by experts, automatic EA modelling methods have been proposed to deal with drawbacks of manual modelling, such as error-proneness, time-consumption, slow and poor re-adaptation, and cost. However, automatic modelling is not effective for the most abstract concepts in EA like strategy or motivational aspects. Thus, companies are demanding hybrid approaches that combines automatic with manual modelling. In this context there are no clear relationships between the input artefacts (and mining techniques) and the target EA viewpoints to be automatically modelled, as well as relationships between the experts' roles and the viewpoints to which they might contribute in manual modelling. Consequently, companies cannot make informed decisions regarding expert assignments in EA modelling projects, nor can they choose appropriate mining techniques and their respective input artefacts. This research proposes a decision support system whose core is a genetic algorithm. The proposal first establishes (based on a previous literature review) the mentioned missing relationships and EA model specifications. Such information is then employed using a genetic algorithm to decide about automatic, manual or hybrid modelling by selecting the most appropriate input artefacts, mining techniques and experts. The genetic algorithm has been optimized so that the system aids EA architects to maximize the accurateness and completeness of EA models while cost (derived from expert assignments and unnecessary automatic generations) are kept under control.																	0167-9236	1873-5797				APR	2020	131								113249	10.1016/j.dss.2020.113249													
J								Social influence tactics in e-commerce onboarding: The role of social proof and reciprocity in affecting user registrations	DECISION SUPPORT SYSTEMS										User onboarding; E-commerce; Social influence; Reciprocity; Social proof; User registration	MIXED-METHODS RESEARCH; ONLINE; BEHAVIOR; RECOMMENDATIONS; GUIDELINES; FRAMEWORK; NORM	Given the increasing competition to attract new customers, e-commerce providers (i.e., e-tailers) are being urged to optimize their website onboarding journeys. Design mechanisms employing social influence tactics seem to be promising avenues to effectively improve user onboarding experiences. Specifically, two social influence tactics-namely, reciprocity and social proof-are widely used in practice to reliably enhance user registrations. Yet, researchers have only tentatively examined how these combined tactics affect user onboarding behavior. While combining both tactics may hold synergistic potential, it may also be counterproductive, as too much social influence may backfire. We address this research gap by investigating the distinct and joint effects of reciprocity and social proof on user registration behavior. Drawing on two forms of reciprocity (i.e., utility- and monetary-based reciprocity), we conduct an online experiment (N = 249) and a related randomized field experiment (N = 475,495) to compare both reciprocity variants in combination with social proof and investigate their efficacy for actual user onboarding decisions (i.e., user registrations). Our results show that both social influence tactics and both reciprocity variants have positive effects on users' registration behavior if applied individually. However, when both tactics are employed together, the effect of reciprocity is moderated by social proof such that social proof nullifies the effect of monetary-based reciprocity whereas it amplifies the effect of utility-based reciprocity. Our study thereby contributes to a more advanced understanding of the interactive relationship between reciprocity and social proof and their joint effect on user onboarding decisions.																	0167-9236	1873-5797				APR	2020	131								113268	10.1016/j.dss.2020.113268													
J								Creating the best first impression: Designing online product photos to increase sales	DECISION SUPPORT SYSTEMS										Product photos; E-commerce; Online sales; First Sight; Consumer reviews	PRICE PROMOTIONS; MODEL; ATTENTION; REVIEWS; ATTRIBUTES; AESTHETICS; EXPERIENCE; INTENTION; CONSUMERS; PURCHASE	Effectively displaying goods in search results is valuable for B2C merchants to earn clicks from consumers and even increase sales. Taking down jackets and trousers sold on Tmall-China's largest B2C e-commerce platform-as the example, this paper collects data about three factors that influence consumers' first impression on their search results: the price of a product, the quantity of historical reviews, and a photo of the product. Among these factors, previous research shows that a product photo contains four attributes: brand logo, promotional information, street scenes, and model display. Focusing on these attributes, we apply a decision tree to explore customer purchasing patterns, which allows us to further investigate the influence of product photo attributes on sales volume by using a hierarchical regression model. Our research finds that among the products from the list of their search results, customers prefer those with many good historical reviews and low prices. In addition, gender that differentiates men's from women's clothing has a moderating effect on the relationship between photo attributes and product sales. The purchase decision of consumers shopping for men's clothing is susceptible to the influence of the product photo. Furthermore, different from the traditional view which shows that brands can reduce perceived risks and increase sales, this study finds that men's clothing sales are negatively affected by brand logo attribute in product photos. Finally, using models cannot significantly increase product sales among consumers shopping for either men's or women's clothing.																	0167-9236	1873-5797				APR	2020	131								113235	10.1016/j.dss.2019.113235													
J								The power of emotions in online decision making: A study of seller reputation using fMRI	DECISION SUPPORT SYSTEMS										Seller reputation; Decision-making; Signaling theory; Semiotics theory; fMRI; Price premium	ORBITOFRONTAL CORTEX; GOAL VALUES; PRICE; QUALITY; NEUROSCIENCE; COMPUTATION; SIGNALS; MARKETS; IMAGES; DLPFC	Online auctions use a variety of unique tools to provide quality signals for products and sellers in an attempt to overcome the online marketplace's limitations. These tools include feedback ratings and symbols that indicate reputation. However, surprisingly few studies have investigated the effects of reputation indicators in the decision-making process in online marketplaces. No prior study in our knowledge has examined the role of graphics indicators in online decision-making. We investigated the decision process and the price premium an individual was willing to pay for a product using an experiment with the Becker-DeGroot-Marschak procedure and event-related measures of brain activity. As signaling theory expects, results showed that the price premium paid for a product was higher when the product sold by a seller appeared with a high-reputation symbol. High-reputation seller indicators triggered significantly higher neural activity compared with low-reputation seller indicators in brain regions associated with emotions in the prefrontal cortex (the ventromedial prefrontal cortex). The degree of integrated cognitive value and cognitive and emotional value as represented by neural activity in the dorsolateral prefrontal cortex and VMPFC respectively was correlated with the price premium individuals bid for products. The results indicate the applicability of semeiotic theory along with signaling theory in the e-marketplace context and suggest fruitful avenues for further research.																	0167-9236	1873-5797				APR	2020	131								113247	10.1016/j.dss.2020.113247													
J								Bayesian neural networks for flight trajectory prediction and safety assessment	DECISION SUPPORT SYSTEMS										Deep learning; Big data; Trajectory prediction; Air transportation system; Bayesian neural network; Safety assessment	DEEP; INFERENCE; MODEL	Safety, as the most important concern in civil aviation, needs to be maintained at an acceptable level at all times in the air transportation system. This paper aims to increase en-route flight safety through the development of deep learning models for trajectory prediction, where model prediction uncertainty is characterized following a Bayesian approach. The proposed methodology consists of four steps. In the first step, a large volume of raw messages in Flight Information Exchange Model (FIXM) format streamed from Federal Aviation Administration are processed with a distributed computing engine Apache Spark to extract trajectory information in an efficient manner. In the second step, two types of deep learning models are trained to predict flight trajectory from different perspectives. Specifically, deep feedforward neural networks (DNN) are trained to make a one-step-ahead prediction on the deviation along latitude and longitude between the actual flight trajectory and target flight trajectory. In parallel, deep Long Short-Term Memory (LSTM) neural networks are trained to make longer-term predictions on the flight trajectory over several subsequent time instants. The DNN model is more accurate but has a single-step prediction horizon, whereas the LSTM model is less accurate but longer prediction horizon. Therefore, in the third step, the two different types of deep learning models are blended together to create a multi-fidelity prediction. After quantifying the discrepancy between the two model predictions in the current time instant, the DNN prediction is used to correct the LSTM prediction of flight trajectory along subsequent time instants accordingly. The multi-fidelity approach is expanded to multiple flights, and is then used to assess safety based on horizontal and vertical separation distance between two flights. Computational results illustrate the promising performance of the blended model in predicting the flight trajectory and assessing en-route flight safety.																	0167-9236	1873-5797				APR	2020	131								113246	10.1016/j.dss.2020.113246													
J								Non-intrusive contextual dynamic reconfiguration of ambient intelligent IoT systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Connected objects; Context awareness; IoT reconfiguration; Software architecture; Ambient intelligence		Internet of Things is the current evolution of the Internet, which is opening tremendous opportunities for a large number of novel applications that promise to revolutionize and improve the quality of human life. For this reason, much attention has been oriented towards this theme from different perspectives. The problem treated by this paper is the necessity of having a mechanism that enables IoT systems to perform with transparency without stops or breaks regardless of the changes that affect the surrounding context. We propose a contextual dynamic reconfiguration process to be applied on the architectural level of IoT systems; the process relies on autonomic computing MAPE-K loop. The originality of our work is the use of architectural styles to make reusable all the architectural evolutions applied on the system.																	1868-5137	1868-5145				APR	2020	11	4			SI		1365	1376		10.1007/s12652-018-1127-2													
J								A Context-aware adaptive algorithm for ambient intelligence DASH at mobile edge computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Adaptive streaming; DASH; Deep learning; Mobile edge computing; Adaptive algorithm	QUALITY	Adaptive streaming has recently emerged as a technology enabling high-quality streaming at various bitrates. One of the video streaming challenges remains in research topic nowadays that is choosing optimal segment base on network characteristics and streaming devices, such as network bandwidth, latency, the computational capacities of devices. Researchers have proposed many algorithms to overcome such issues within their predefined conditions. However, those proposed methods do not perform efficiently in the heterogeneous network today. Consequently, in this article, we present research on a context-aware adaptive algorithm for ambient intelligence dynamic adaptive employing mobile edge computing (MEC). Specifically, we apply deep learning in the adaptive algorithm which is installed at the MEC to assist clients in choosing the optimal streaming segments as well as reduce network latency. Furthermore, we apply the multilayer perceptron classifier with data obtained from various experiments of adaptive streaming algorithms then combine them in a general algorithm. In the analysis, we use network simulator NS3 as a tool to carry out the verification of our proposed method. As a result, the proposed research reduces network latency as well as improve quality streaming compared to existing approaches.																	1868-5137	1868-5145				APR	2020	11	4			SI		1377	1385		10.1007/s12652-018-1049-z													
J								Ambient context-based modeling for health risk assessment using deep neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ambient context; Data mining; Big data; Deep learning; Deep neural network; Healthcare	SERVICE	Context computing is a branch of ambient intelligence (AmI) research, which has been rapidly emerging in the support of intelligent smart health platform solution. To develop reliable ambient computing using the hybrid peer-to-peer network and Internet of Things, machine learning, deep learning, artificial intelligence, and context awareness have been applied. This study proposes an ambient context-based modeling for a health risk assessment using deep neural network. In the proposed method, we collected medical information from chronic disease patients such as EMR, PHR, and medical histories, as well as environmental data from a health platform. Subsequently, heterogeneous data are integrated through selecting, cleaning, modeling, and evaluating the collected raw data and then the context is created. The structured input data such as a sensor data are normalized by transforming the time domain data to the frequency domain information. Using a deep neural network, the normalized data are applied to create an ambient context. A deep neural network is composed of the following three layers: input layers with treated and untreated data; hidden layers where connection strength is trained as a weight; and output layers of trained results. In the deep neural network layers, the control of the weight of training data enables repeated learning to create an ambient context pattern. Using an ontology inference engine, unstructured/structured data, including individual health data and environmental information, and their context is presented as ontology metadata. In the knowledge base, hidden association relationships are discovered through mining. To inform the individual health conditions exposed to the individual environmental contexts, a health risk assessment model is developed with a set of the ambient context pattern learned with metadata and a deep neural network. The Minkowski distance formula, which defines a normalized geometrical distance between two nodes, is used to measure the similarity between the patients with chronical disease and the individual user based on the context. In the proposed model, the risk is represented as a similarity-based index. The risk assessment model can be implemented into the individual risk alert/prevention system. The model may significantly impact the healthcare industry as well as ambient intelligence research, thus contributing to improve the quality of human life of the future society.																	1868-5137	1868-5145				APR	2020	11	4			SI		1387	1395		10.1007/s12652-018-1033-7													
J								A study on the influencing factors of consumer information-seeking behavior in the context of ambient intelligence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ambient intelligence; Mobile Internet; Information seeking behavior; Information seeking effort	SEARCH	Based on the perspective of ambient intelligence, this study explores the influencing factors of consumer information-seeking behavior and its mechanism. It also introduces the two mediation variables, like information seeking motive and information seeking ability. It is proved by empirical analysis that mobile Internet trust and perceived risk have a positive impact on consumers' information seeking effort, and the information seeking motive plays a mediating role in the influence of the mobile Internet trust and perceived risk on consumers' information-seeking effort; mobile Internet involvement has a positive effect on the information seeking ability; However, perceived ease of use has no significant effect on information seeking ability, information seeking ability has no significant effect on information seeking effort. It is of practical significance both theoretically and practically to conduct in-depth discussions on consumers' information seeking behavior in combination with Chinese local elements.																	1868-5137	1868-5145				APR	2020	11	4			SI		1397	1404		10.1007/s12652-018-1005-y													
J								A study on smart factory-based ambient intelligence context-aware intrusion detection system using machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ambient intelligence; Smart factory; Context-aware; Machine learning; IDS	FUTURE	Digital transformation increasingly gains broad attentions from all the world and particularly studies on artificial intelligence, big data, cloud, and mobile are currently conducted. In addition, research based on ambient intelligence are also performed. Everything including condition information of all objects are shared on real time in AMI environment and all locations and objects are equipped with sensors. It acts intelligently such as decision-making. As sensors are equipped in locations and objects and connected with high-performance computer networks, users can receive information at any time and anywhere. In particular, the adoption of smart factory that turns all phases into automation and intellectualization based on cyber-physical system technology is proliferating. However, unexpected problems are likely to take place due to high complexity and uncertainty of smart factory. Thus, it is very likely to end manufacturing process, trigger malfunction, and leak important information. Although the necessity of analyzing threats to smart factory and systematic management is emphasized, there is insufficient research. In this paper, machine learning and context-aware intrusion detection system was built. The established system was effective to detection rate of anomaly signs and possibility of process achievement compared to the previous system.																	1868-5137	1868-5145				APR	2020	11	4			SI		1405	1412		10.1007/s12652-018-0998-6													
J								Examining effects of context-awareness on ambient intelligence of logistics service quality: user awareness compatibility as a moderator	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things (IoT); Ambient intelligence (AmI); Context-awareness; SERVQUAL; Logistics service quality	INTERACTIVITY; COMMUNICATION; INFORMATION; DISCOVERY; KNOWLEDGE; SELECTION; INTERNET	The internet of things has changed the way we interact with information system. With the pervasive information system facing various complexities of end clients, the development mode based on semantic association and context-awareness made it possible to provide personalized service for each client. In this paper, the context-awareness-based ambient intelligence predicts users' intention to use depending on the contexts they provide. By applying the prediction to logistics services, it can provide customized service to keep clients satisfied. A key issue in user-centered services is how to detect each user specific situation and choose a certain service that meets users' requirements the best, and then to provide support for real-time decision making. We believe that the complishment of ambient intelligence cannot be separated from technology support when it comes to intelligent behavior.																	1868-5137	1868-5145				APR	2020	11	4			SI		1413	1420		10.1007/s12652-018-1004-z													
J								Narrative context-based data-to-text generation for ambient intelligence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Natural language generation; Deep learning; Narrative context; Ambient intelligence		In this paper, we propose a language generation model for the world of ambient intelligence (AmI). Various devices in use today are connected to the Internet and are used to provide a considerable amount of information. Because language is the most effective way for humans to communicate with one another, one approach to controlling AmI devices is to use a smart assistant based on language systems. One such framework for data-to-text generation is the natural language generation (NLG) model that generates text from non-linguistic data. Previously proposed NLG models employed heuristic-based approaches to generate relatively short sentences. We find that such approaches are structurally inflexible and tend to generate text that is not diverse. Moreover, there are various domains where numerical values are important, such as sports, finance, and weather. These values need to be generated in terms of categorical information. (e.g., hits, homeruns, and strikeouts.) In the generated outputs, the numerical values often do not accurately correspond to categorical information. Our proposed data-to-text generation model provides both diversity and coherence of information through a narrative context and a copy mechanism. It allows for the learning of the narrative context and sentence structures from a domain corpus without requiring additional explanation of the intended category or sentential grammars. The results of experiments performed from various perspectives show that the proposed model generates text outputs containing diverse and coherent information.																	1868-5137	1868-5145				APR	2020	11	4			SI		1421	1429		10.1007/s12652-019-01176-7													
J								Ambient crop field monitoring for improving context based agricultural by mobile sink in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Agricultural field; Mobile sink path strategy; Sleep awake scheduling; Frontward communication	WIRELESS SENSOR NETWORKS; LIFETIME; DELAY	Climate-based agriculture is an essential proficiency that maximizes the context based agricultural yields via suitable ambient monitoring services modified with the information gained from the sensors. Monitoring weather in real time scenario is the primary criterion to observe the climate ambient of a farm. Several farms relevant troubles can be resolved by better realizing the ambient weather situation. Increasing agricultural productivity technique is proposed to improve the precision farming field. However, it creates an additional delay in transferring the information to farmer also it creates additional energy consumption. To overcome these problems, we propose ambient crop field monitoring for improving Context based agricultural by mobile sink in wireless sensor networks. Ambient monitoring objective is to increase the yield of crops while diminishing the use of the property. The mobile sink is introduced to collect the updated ambient information from the sensors and send it to the base station. This research work aims to design a better path using a sensor node to a mobile sink and mobile sink travelling path for reducing energy consumption and delay in the context based agricultural wireless sensor networks. In this scheme, the sensor nodes have formed the route based on mobile sink tree. But, it does not reduce the network delay better. To solve this issue, frontward communication area (FCA) based route selection is proposed. The FCA method reduces both the energy consumption and delay in the network since it selects the route by the quality of service parameters. This technique application is mainly used to smart agriculture. The simulation results show that the getting better 38.31% packet received rate and reducing 0.0115 J energy consumption in the network.																	1868-5137	1868-5145				APR	2020	11	4			SI		1431	1439		10.1007/s12652-019-01177-6													
J								A group preference-based item similarity model: comparison of clustering techniques in ambient and context-aware recommender systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender systems; Item similarity model; Ambient intelligent; Context-aware; Group preference; Clustering algorithms; KNN; K-means		In the study of collaborative filtering, scholars and professionals alike have given much attention to user responses of the "one-class" type, feedback like online transactions or "likes". Such behavior gauges have been integral to many ambient intelligent and context-aware recommendation systems, in which users are furnished with personalized lists of items according to their exhibited tastes. These one-class data, earlier studies have shown, are readily grasped by Bayesian personalized ranking, a pairwise preference assumption. Nevertheless, these works fail to make sufficient use of item similarity models using group preference. To improve performance, we argue in this paper, it is necessary to develop a model that yokes a User preference model to the Group Preference-based Similarity models (called UGPS). UCPG will produce a greater depth of interactions, we argue, because it takes on an entire set of items as opposed to the solitary item used previously. Moreover, a number of clustering methods have been put to work in group preference-based recommendation systems, but there is no consensus as to which offers superior accuracy. To gain clarity, we first built up a pair of versions of UGPS in order to assess the recommendation performances of different approaches to group generation: UGPS-1, which employed K-means, and UGPS-2, using K-NN-according to how efficiently they group their output. This comparison revealed that UGPS-1 tended to improve its recommendation performance as the number of groups and representative item sets grew. In contrast, UGPS-2 exhibited the opposite effect: recommendation performance declined as the number of groups and representative item sets expanded. Lastly, we consider how our UGPS system works with various sophisticated approaches on four real datasets, and demonstrate that UGPS produces more accurate recommendations.																	1868-5137	1868-5145				APR	2020	11	4			SI		1441	1449		10.1007/s12652-018-1039-1													
J								Neural-network based adaptive context prediction model for ambient intelligence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining; Neural network; Intelligence of Things; Context Computing; Ambient intelligence	SERVICE; SYSTEM	To overcome the limitations of the conventional medical service in terms of ageing and chronic diseases, AmI-based precision medicine has drawn particular attention. Precision medicine is a customized personal medical service using various information technologies such as personal health device, AI algorithm, image recognition, voice recognition, and natural language processing. In particular, the information technologies for follow-up care services for patients, such as context awareness, context information, and inference rules, are required. In PHD, contexts such as variable data include blood pressure, BMI, blood sugar, weather, and food. It has time-series characteristics, meaning that it changes often with time. Other kinds of health-related information, such as age, family history, smoking, and residential area, are intermittently changed. For inference that is highly related to a user, the context collected through AmI is presented with ontology. Ontology consists of a user's ambient data, weather data, and lifelog. Context is changed along with a user's ambient conditions and time. An inference engine is used to create the knowledge base and predict a change. This study proposes a neural-network based adaptive context prediction model for ambient intelligence. This is a learning model using neural network to calculate the similarity for recommendation in a mining lifecare platform. In a conventional prediction procedure, an error is used to update a weight. The proposed model learns the similarity weight of the users to become adapted to the user's ambient. Based on the knowledge base, user clustering and deviation from mean are applied to calculate the similarity weight. Collaborative filtering technology is used to predict a user's context and learn the similarity weight repeatedly using a neural network. According to the performance evaluation, the proposed neural-network based similarity weight method had the highest accuracy of prediction when the learning rate was 0.001. Consequently, we found that AmI is a new added-value technology to maintain a healthy lifestyle and contributes to developing the healthcare industry and improving the quality of life.																	1868-5137	1868-5145				APR	2020	11	4			SI		1451	1458		10.1007/s12652-018-0972-3													
J								Ambient intelligence architecture of MRPM context based 12-tap further desensitized half band FIR filter for EEG signal	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Very large scale integration (VLSI); Electroencephalogram (EEG); Finite impulse response (FIR); Multiplication and accumulation (MAC); Modified Russian Peasant multiplier (MRPM); Square root carry select adder (SQRTCSLA); Field programmable gate array (FPGA)		The half band filter constructed with cascade structure of FIR filter reduces the insensitivity of frequency response due to coefficient quantization. The coefficient insensitivity can be further reduced by desensitized half band FIR filter. A digital desensitized filter incorporates a first and a second half band filter joined in a cascade between an input and the output of the digital filter. The design for analyzing the electroencephalogram (EEG) signals with the half band finite impulse response (FIR) filter architecture. In this work the 12-tap further desensitized FIR half band employs an efficient modified Russian peasant multiplier (MRPM) with a square root carry select adder (SQRT CSLA) stage has been used to reduce the hardware in multiplier and accumulate (MAC) unit. The proposed filter design is used to analyze the EEG signals with reduced hardware in the health care monitoring for ambient environment. The proposed method offers 38.5% reduction in No. of LUTs, 47.49% reduction in No. of slices, 8.16% reduction in delay (ns) and 29.80% reduction in power (mw).																	1868-5137	1868-5145				APR	2020	11	4			SI		1459	1466		10.1007/s12652-019-01237-x													
J								PassContext and PassActions: transforming authentication into multi-dimensional contextual and interaction sequences	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; Non-text passwords; Security	KEYSTROKE DYNAMICS; SECURITY; BIOMETRICS; MEMORABILITY; USABILITY; SCHEME; FIELD	Authorized access to resources by legitimate users plays a crucial role in providing a secure and hassle-free user experience in the digital environments. Password remains the major authentication mechanism though there exist various drawbacks like leakage due to phishing and shoulder surfing, etc. This paper proposes two stronger transformations of the password termed as "PassContext" and "PassActions" which attempts to overcome the vulnerabilities in the plain-text password by harnessing the intricacies of human-computer interaction. The PassContext incorporates the hardware and software oriented context information along with the keyed-in password text during the verification process to provide improved authentication. The PassActions transforms the password from being text-only representation into a dynamic user interaction sequence which improves the strength of the password significantly. The proposed model incorporates methodologies to represent PassContext and PassActions for both validation and persistence purposes. The prototype implementations of PassContext and PassActions are evaluated with a suit of thirteen proposed measures, system usability survey (SUS) for usability analysis and with a well-established comparative framework.																	1868-5137	1868-5145				APR	2020	11	4			SI		1467	1494		10.1007/s12652-019-01336-9													
J								A fingerprint based crypto-biometric system for secure communication	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Biometric; template security; Diversity; Fingerprint; Minutiae; Revocability; Biometric cryptosystem	CRYPTOGRAPHIC KEY GENERATION; AGREEMENT PROTOCOL; AUTHENTICATION	To maintain secrecy of information during communication, cryptography is considered to be an impressive solution and cryptographic keys play an important role to ensure the security. However, these randomly derived keys (of 256 bits) are hard to memorize. Also, there is a threat of privacy invasion since the storage, protection and transmission of a key over a communication link may lead to information leakage. Therefore, researchers propose to utilize user's biometric trait to generate the cryptographic key in a session-based communication environment. This avoids the storage of cryptographic keys without negotiating on secrecy. The biometric-based key generation encompasses concerns over biometric template protection, biometric data sharing between users and revocable key generation from biometric. To address the aforementioned concerns, we propose a framework for secure communication between two users using a fingerprint-based crypto-biometric system. First, the feature bit-string are computed from the users' fingerprint. Next, revocable transformation is applied to derive the private keys of respective users. Then, the Diffie-Hellman (DH) algorithm is used to generate public keys from private keys of both sender and receiver, which are shared and further used to produce a symmetric cryptographic key at both ends. Here, the biometric data is neither stored nor shared which ensures the security of biometric data. Also, perfect forward secrecy is achieved using session keys. This work also provides the long-term protection of messages communicated between two users. It is evident from the experimental evaluation over four datasets of FVC2002, four datasets of FVC 2004, and NIST special database IV that the proposed framework is privacy-preserving and could be utilized for real access control systems.																	1868-5137	1868-5145				APR	2020	11	4			SI		1495	1509		10.1007/s12652-019-01437-5													
J								Contextual outlier detection for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Contextual outlier detection; Edge computing	ANOMALY DETECTION; EVENTS	The quality of dataset measured and collected by wireless sensor networks (WSN) is often affected by noise and error that are inherent to resource-constrained sensor nodes. The affected data points deviating from the normal pattern are termed as outlier(s). However, detected outlier can be a result of the occurrence of an actual event. Outlier detection techniques developed for WSNs perform binary labelling of the data points and does not indicate the context stating whether the outlier is the result of an actual event or the noise/error. This paper proposes a contextual outlier detection framework specifically designed for WSNs named as in-network contextual outlier detection on edge (INCODE). The proposed framework also estimates the degree of outlierness associated with the detected outlier(s) to provide better insight into the measured data point. Algorithms used in INCODE are designed around the edge computing concept to minimize the communication and computational complexities to make it suitable for resource-constrained WSN. The results suggest an impressive 98% accuracy in identifying the context of the outlier(s). The low communication and computational complexity suggest INCODE's suitability for resource constrained WSNs.																	1868-5137	1868-5145				APR	2020	11	4			SI		1511	1530		10.1007/s12652-019-01194-5													
J								A budget feasible peer graded mechanism for iot-based crowdsourcing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Crowdsourcing; IoT devices; Truthful; Budget feasible; Peer grading; Shapley value	INCENTIVE MECHANISMS	We develop and extend a line of recent works on the design of mechanisms for heterogeneous tasks assignment problem in 'crowdsourcing'. The budgeted market we consider consists of multiple task requesters and multiple IoT devices as task executers. In this, each task requester is endowed with a single distinct task along with the publicly known budget. Also, each IoT device has valuations as the cost for executing the tasks and quality, which are private. Given such scenario, the objective is to select a subset of IoT devices for each task, such that the total payment made is within the allotted quota of the budget while attaining a threshold quality. For the purpose of determining the unknown quality of the IoT devices we have utilized the concept of peer grading. In this paper, we have carefully crafted a truthful budget feasible mechanism for the problem under investigation that also allows us to have the true information about the quality of the IoT devices. Further, we have extended the set-up considering the case where the tasks are divisible in nature and the IoT devices are working collaboratively, instead of, a single entity for executing each task. We have designed the budget feasible mechanisms for the extended versions. The simulations are performed in order to measure the efficacy of our proposed mechanism.																	1868-5137	1868-5145				APR	2020	11	4			SI		1531	1551		10.1007/s12652-019-01219-z													
J								Fuzzy logic expert system for selecting robotic hands using kinematic parameters	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy logic; Expert system; Robotic hand; Robotic hands selection	DESIGN; FORCE; MANIPULATION; MECHANISM	Industry 4.0 is the current industrial revolution and robotics is an important factor for carrying out high dexterity manipulations. However, mechatronic systems are far from human capabilities and sophisticated robotic hands are highly priced. This paper describes a Fuzzy Logic Expert System (FLES) to map kinematic parameters from robotic hand features to the level of dexterity. The final goal is to obtain the adequate robotic hand that can do ranges of specific tasks according to the level of dexterity required. The FLES uses important kinematic parameters of the human hand/robotic hand: number of fingers, number of Degrees of Freedom (DoF), and number of contacts that grasping involves. As a result, several robotic hands are evaluated using the FLES to determine the type of dexterity task that corresponds to each robotic hand.																	1868-5137	1868-5145				APR	2020	11	4			SI		1553	1564		10.1007/s12652-019-01229-x													
J								Determination of optimal reserve contribution of thermal units to afford the wind power uncertainty	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wind farm integrations; Wind speed and load uncertainties; Stochastic analysis; Mixed-integer linear programming; Increased wind energy deployment; Emission reduction	LAGRANGIAN-RELAXATION; COMMITMENT; NETWORK	Due to unforeseen variations in wind speed profiles, wind farm integrations are recognized as intermittent and uncertain energy contributors. More specifically, integration of such renewable energy resources aligned with the conventional thermal units although reduces the emissions and brings about a clean environment, it introduces serious problems in assigning optimal and reliable level of these units in load supplying and spinning reserve provision. This situation is more intensified considering the uncertainties arisen by the power system loading demand. To facilitate such operational hurdles, the ongoing study puts forward an efficient model for assigning the optimal spinning reserve which accommodates the uncertainties in both the wind speed and load profiles. Stochastic behavior of these parameters is simulated by generating a proper number of scenarios through the Monte Carlo simulation (MCS) approach. Then, each of these scenarios is evaluated based on the established linear mixed integer approach in a deterministic fashion. Accordingly, a computationally efficient approach is obtained paving the way for real-world implementations and assuring the global optimum results. The proposed approach is applied to a 12-unit test system including 10 thermal units and 2 wind farms. Results are reflected in terms of the commitment status, energy dispatches, and reserve contributions of each committed unit. A comprehensive discussion is conducted to disclose the possible improvements.																	1868-5137	1868-5145				APR	2020	11	4			SI		1565	1576		10.1007/s12652-019-01231-3													
J								Automatic generation of entity-oriented summaries for reputation management	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Summarization; Search with diversity; Twitter; Microblogs; Online reputation management	CENTRALITY	Producing online reputation summaries for an entity (company, brand, etc.) is a focused summarization task with a distinctive feature: issues that may affect the reputation of the entity take priority in the summary. In this paper we (i) present a new test collection of manually created (abstractive and extractive) reputation reports which summarize tweet streams for 31 companies in the banking and automobile domains; (ii) propose a novel methodology to evaluate summaries in the context of online reputation monitoring, which profits from an analogy between reputation reports and the problem of diversity in search; and (iii) provide empirical evidence that producing reputation reports is different from a standard summarization problem, and incorporating priority signals is essential to address the task effectively.																	1868-5137	1868-5145				APR	2020	11	4			SI		1577	1591		10.1007/s12652-019-01255-9													
J								Hybrid opportunistic and position-based routing protocol in vehicular ad hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VANETs; Position-based routing; Opportunistic routing; Link expiration time; Link quality		Vehicular ad hoc networks (VANETs) are kind of mobile ad hoc networks (MANETs) which are used to provide communications between mobile vehicles in urban and highway scenario. Due to special characteristics of VANETs such as dynamic topology, frequent disconnection, high vehicular speed and propagation model, designing an efficient routing scheme is one of the most important key issues. In this paper, we propose a hybrid opportunistic and position-based routing protocol in VANETs by considering parameters such as position of nodes, link quality and node density. The proposed method uses a greedy forwarding scheme, in which a sender vehicle chooses a neighbor node with the highest geographical progress to increase the least number of hops between source node and destination vehicle node. Based on opportunistic and position based strategy, the proposed scheme selects optimal candidate nodes and determines appropriate priority for transmitting data. Also, the proposed scheme determines and removes the expired nodes from the routing process. The simulation results in ns-2 indicate performance improvement in terms of packet delivery rate (PDR), throughput and end-to-end delay.																	1868-5137	1868-5145				APR	2020	11	4			SI		1593	1603		10.1007/s12652-019-01316-z													
J								A logarithmic law based histogram modification scheme for naturalness image contrast enhancement	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Histogram equalization; Histogram modification; Image quality enhancement; Color images; Discrete cosine transform	CUCKOO SEARCH ALGORITHM; GAMMA CORRECTION; EQUALIZATION; BRIGHTNESS	In this paper, a new logarithmic histogram modification technique for image contrast enhancement with naturalness preservation has been proposed. Traditional histogram equalization scheme usually causes extreme contrast enhancement, which results in unnatural look and artifacts. The proposed technique first enhances the contrast of the image globally through addition and logarithmic law based modification scheme, thereafter the local details of the image are emphasized through the coefficient scaling directly in the compressed domain using discrete cosine transformation. The proposed method can enhance the image contrast uniformly with less number of parameters without losing its basic features. Experimental results show that the proposed method preserves the natural appearance of the image and yields better perceptual quality as compared to the state-of-the-art techniques.																	1868-5137	1868-5145				APR	2020	11	4			SI		1605	1627		10.1007/s12652-019-01258-6													
J								Modelling the effects of certain cyber-attack methods on urban autonomous transport systems, case study of Budapest	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Security-and-privacy; Autonomous-driving-and-communication; Vehicle-to-vehicle; roadside; Internet-communication; Vehicular-security; Cyberattack	VEHICLES	Based on the reviewed literature, the objective of the article is to investigate the most important cyberattack factors affecting the effectiveness of the connected and autonomous transport system. Accordingly, remotely implemented malicious interventions are in the focus of our study, especially considering cyberattacks on connected and autonomous vehicles. To introduce the expected impact of the analysed cyberattacks, certain cyberattack types are demonstrated in the paper. In the paper, malicious interventions are assumed to attack the transport network contrary to the individual road users, which assumption can strongly increase the risk of a given cyber-incident. To illustrate the effect of the evaluated cyberattacks, the transport system of Budapest is used. According to the results, some of the interventions might not be effective, while some of them can expectedly significantly increase travel time values of the network.																	1868-5137	1868-5145				APR	2020	11	4			SI		1629	1643		10.1007/s12652-019-01264-8													
J								Usability and shoulder surfing vulnerability of pattern passwords on mobile devices using camouflage patterns	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; Security and protection; Password pattern; Mobile security; Camouflage patterns		With the revolution of smart devices that have become the basis of our daily lives, the majority of users rely on them to save their personal and sensitive information. As a result, users are increasingly interested in authentication processes, which is a challenge for designers to provide a secure and usable authentication process. The pattern password is one of the most selected authentication methods, since the recent development in alternative authentication interfaces for smart phones, tablets and touch screens laptops. Although drawing a pattern seems easier than typing a password, it has a major security drawback which is the shoulder-surfing attack. Therefore, this paper proposes a shoulder-surfing resistance approach for mobile devices using Camouflage Patterns method which allows choosing a very short password, while insuring that the password remains hidden amongst a large number of nodes draws. Based on this approach, three techniques are introduced and implemented using an Android platform. An experimental study is conducted for evaluating the security and usability aspects. The results showed that the proposed approach is reasonably resistant against shoulder-surfing attacks and usable for users. Accordingly, this approach is recommended for designers in order to provide very simple and yet very complicated passwords, to be observed by the attacker, at the same time.																	1868-5137	1868-5145				APR	2020	11	4			SI		1645	1655		10.1007/s12652-019-01269-3													
J								Effect of human development level of countries on the web accessibility and quality in use of their municipality websites	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Web accessibility; Accessibility evaluation; Human development index; Automated evaluation tools; Web performance; Municipality websites	HUMAN-DEVELOPMENT INDEX; SITE ACCESSIBILITY; E-GOVERNMENT	This study aimed to explore the relationship between human development index (HDI) of countries and level of web accessibility and quality in use of their municipality websites. A list of 146 countries was obtained from the 2016 Global Human Development Report of the United Nations. Of these countries, 49 had a very high HDI, 42 had a high HDI, 33 had a medium HDI, and 22 had a low HDI. For the analysis of web accessibility and quality in use, the official municipality websites of capital cities of each country were found. These websites were tested using automated evaluation tools. The results showed that the global rank of municipality websites, their rank within the country of location, and percentage of incoming traffic within the country of location varied depending on their HDI. Furthermore, the number of websites that passed all WCAG 2.0 success criteria was very low. The analysis on whether the number of accessibility errors in the evaluated websites changed according to the country's HDI showed that for conformance level A, representing the "must satisfy" checkpoints, the difference was significant. The municipality websites had fewer errors in countries with a higher HDI.																	1868-5137	1868-5145				APR	2020	11	4			SI		1657	1667		10.1007/s12652-019-01284-4													
J								An effective variant ring signature-based pseudonym changing mechanism for privacy preservation in mixed zones of vehicular networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mixed zone; Ring signature; Pseudonym notifications; Location privacy; Intermediately trusted authority	LOCATION PRIVACY	The privacy information related to the location of the vehicles need to be concealed with utmost care in the vehicular network since its disclosure leads to a diversified number of attacks that degrades the performance of the network. But, the privacy of vehicles relies on a kind of pseudonym changing mechanism utilized in the mix zone of the vehicular networks. However, the strength of location privacy is influenced by the lower vehicular density and characteristics of decentralization in the mixed zone. In this paper, an effective variant ring signature-based pseudonym changing mechanism (EVRS-PCM) is contributed for privacy preservation under decentralization and reduced density of vehicles. This proposed EVRS-PCM approach uses a ring signature-based pseudonym changing procedure that potentially minimizes the number of pseudonyms notifications under the formation of mixed zones in the network for facilitating excellent location privacy. The results of the proposed EVRS-PCM approach confirmed an optimal rate of success rate, location privacy and the percentage decrease in the number of pseudonym notifications under the varying proportion of mixed zones, distances and vehicles in the network.																	1868-5137	1868-5145				APR	2020	11	4			SI		1669	1681		10.1007/s12652-019-01304-3													
J								HybTGR: a hybrid routing protocol based on topological and geographical information in vehicular ad hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vehicular ad hoc networks; Geographical routing; Topology-based routing; Link lifetime		Due to rapid changes in topology and high mobility of vehicles, routing in vehicular ad hoc networks needs special mechanisms. Due to these features, traditional routing protocols that are mainly developed for mobile ad hoc networks do not function well in these type of networks. To improve the efficiency of data routing, geographical routing method has been proposed by the researchers that use geographical information of nodes in the routing process. But this method relies only on the geographical information of network nodes and usually uses a greedy idea that may not perform well in some network situations. The main objective of this paper is to propose a hybrid routing protocol based on both geographical and topological information in Vehicular ad hoc networks and selecting the optimal route for data transmission. In the proposed protocol, based on the parameters like moving speed, link lifetime, the number of vehicles present in the vicinity of the node and distance to the destination node, a weight value is assigned to each network node. In this method, if the weight of a neighbor node is higher than that of the source node, geographical routing is used and otherwise, topology-based routing method will be used. The simulation results show that the proposed protocol due to the use of geographical and topological information, has improved packet loss rate, throughput, end-to-end delay and packet delivery ratio compared to LSGO, GpsrJ+ and GyTAR routing protocols.																	1868-5137	1868-5145				APR	2020	11	4			SI		1683	1695		10.1007/s12652-019-01332-z													
J								Edge computing enabled non-technical loss fraud detection for big data security analytic in Smart Grid	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart Grid security; Big data; Non-technical loss; Edge computing; Electricity theft; Fraud detection	ANOMALY DETECTION; INSPECTION; FRAMEWORK; NETWORKS; MODEL	With the fast development of Smart Grid globally, the security issues arise sharply and Non-Technical Loss (NTL) fraud is one of the major security issues. There are some existing NTL fraud detectors, however, when big data security challenges emerge in Smart Grid, none of them can detect NTL fraud for big data in Smart Grid. In this paper, we propose ENFD, an NTL detection scheme enabled by Edge Computing and big data analytic tools to address big data NTL fraud detection problem in Smart Grid. The research work provides us with experience of developing big data security solutions in Smart Grid. The experimental results show that ENFD can efficiently detect big data NTL frauds which cannot be detected by the state-of-the-art detectors. ENFD can detect small data NTL frauds as well and the average detection speed is about six to seven times that of the fastest detector exists in the literature.																	1868-5137	1868-5145				APR	2020	11	4			SI		1697	1708		10.1007/s12652-019-01381-4													
J								Crowd sensing aware disaster framework design with IoT technologies	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cellular and Wi-Fi networks; Fuzzy logic; Clustering; Crowd sensing; Disaster management; Internet of things (IoT); Online monitoring	INTERNET; SYSTEM; THINGS; MANAGEMENT	When a disaster occurs, a huge amount of inconsistent victim or damage information data is received by many different sources. Disaster management systems achieve the completion of a significantly vital task, which is to reduce the number of victims or amount of damage caused by a disaster, with real-time information monitoring infrastructure. A fundamental role of these systems that could help rescue teams is to make a quick and accurate decision about the region that will be affected by the disaster and the possible effects of the tragedy. Employing IoT solutions in these systems provides the possibility of rapidly and precisely orienting rescue teams to be dispatched to the disaster area and also quickly receive specific information about the effects of the disaster. To achieve this purpose, we present a post-disaster framework using the IoT communication technologies for disaster management based on the proposed crowd sensing clustering algorithm in this paper. The proposed framework provides information about the damage status of buildings with crowd density data along with efficient real-time data collection, data aggregation, and the process of monitoring dissemination stages. This framework realizes clustering of resident density by using the cellular networks and Wi-Fi connections and calculating the damage status of buildings through the designed and specifically implemented IoT unit data. Furthermore, it employs a fuzzy logic-based decision support system to manage the resources. The proposed framework, on real base stations and access points dataset, has shown significant results for identifying crowd densities with the highlighting status of buildings in the disaster area.																	1868-5137	1868-5145				APR	2020	11	4			SI		1709	1725		10.1007/s12652-019-01384-1													
J								Detecting deviations from activities of daily living routines using kinect depth maps and power consumption data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activity monitoring; Kinect camera; Fuzzy logic; Activities of daily living; Abnormality detection	ACTIVITY RECOGNITION; BEHAVIOR; FIELD	There has been an increase in the world's population of elderly persons who wish to live independently for as long as possible. This paper presents an unsupervised approach to help caregivers detect deviations from daily routines of elderly people living alone via inexpensive minimally intrusive sensors. The approach employs a power sensor to measure household composite power consumption and a small number of Kinect sensors in functional areas of the house to capture depth maps from the occupant's activities of daily living (ADLs). The ADLs in an unlabelled training dataset are identified based on associating the occupant's locations with specific power signatures on the power line. This training dataset is processed in order to model key features of ADLs, including the regularity and frequency of important activities. The approach uses a novel data-driven technique to define fuzzy sets over ADL attributes tailored to the occupant's behaviour patterns. The membership functions of these fuzzy sets are learned based on the data distribution of attributes. A set of fuzzy rules is generated to indicate the occupant's deviation from the normal routine of ADLs in subsequent data. The outputs of this monitoring system are reports on upward and downward deviations from normal behaviour patterns in the form of both numerical and linguistic information. The assessment of these scores over a long-term can help caregivers detect the warning signs of persistent drifts from the daily routine. As a proof of concept, the proposed monitoring approach was evaluated using two datasets collected from real-life settings. The fuzzy rule set obtained from the output of the proposed membership function generation technique was able to effectively monitor the ADLs of elderly people because it could accurately distinguish periods of deviations from the routine performance of ADLs. Compared to existing monitoring techniques, the proposed method required no prior information about the appliances in use and its output was considered to be more helpful for caregivers. The fuzzy inference system in this approach was found to be robust in regard to errors when identifying ADLs as it could effectively classify normal and abnormal behaviour patterns of the occupant despite errors in the list of the used appliances.																	1868-5137	1868-5145				APR	2020	11	4			SI		1727	1747		10.1007/s12652-019-01447-3													
J								CNNs hard voting for multi-focus image fusion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-focus; Image fusion; Hard voting; Ensemble learning; Deep learning; Convolution neural networks	SPATIAL-FREQUENCY; INFORMATION; PERFORMANCE; SIMILARITY	The main idea of image fusion is gathering the necessary features and information into one image. The multi-focus image fusion process is gathering this information from the focused areas of many images and the ideal fused image have all focused part from the input images. There are many studies of multi-focus image fusion in the spatial and transform domains. Recently the multi-focus image fusion methods based on deep learning have been emerged, and they have enhanced the decision map greatly. Nevertheless, the construction of an ideal initial decision map is still difficult and inaccessible. Therefore, the previous methods have high dependency on vast post-processing algorithms. This paper proposes a new convolution neural networks (CNNs) based on ensemble learning for multi-focus image fusion. This network uses hard voting of three branches CNNs that each branch is trained on three different datasets. It is very reasonable and reliable to use various models and datasets instead of just one and it would help to the network for improving the accuracy of classification. In addition, this paper introduces new simple arranging of the patches of the multi-focus datasets that is very useful in obtaining better classification accuracy. With this new arrangement of datasets, three types of multi-focus datasets are created with the help of gradient in the directions of vertical and horizontal. This paper illustrates that the initial segmented decision map of the proposed method is very cleaner than the others, and even it is cleaner than the other final decision maps after refined with a lot of post-processing algorithms. The conducted experimental results and analysis evidently validate that the proposed network have the cleanest initial decision map and the best quality of the output fused image compared to the other state of the art methods. These comparisons are performed with various qualitative and quantitative assessments that are assessed by several fusion metrics for demonstrating the superiority of the proposed network.																	1868-5137	1868-5145				APR	2020	11	4			SI		1749	1769		10.1007/s12652-019-01199-0													
J								Advanced lightweight multi-factor remote user authentication scheme for cloud-IoT applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; BAN logic; Cloud computing; Internet of things; Random oracle; Session key	KEY AGREEMENT SCHEME; SECURITY ENHANCEMENT; ACCESS-CONTROL; EFFICIENT; IMPROVEMENT; CRYPTANALYSIS; MANAGEMENT; INTERNET; TRUST	With the ongoing revolution of Internet-enabled devices, Internet of Things (IoT) has emerged as the most popular networking paradigm. The enormous amount of data generated from smart devices in IoT environment is one of the biggest concerns. Cloud computing has emerged as a key technology to process the generated data. The confidential data of user from IoT devices is stored in cloud server and the remote user can access this data anytime, anywhere and at any place from the cloud server. This makes remote user authentication a critical issue. This paper proposes a lightweight remote user authentication scheme for cloud-IoT applications. The formal security analysis using BAN logic and random oracle model confirms that the scheme is resilient to known security attacks. Furthermore, the scheme is formally verified using AVISPA tool which confirms the security against multiple security attacks.																	1868-5137	1868-5145				APR	2020	11	4			SI		1771	1794		10.1007/s12652-019-01225-1													
J								A deep architecture for depression detection using posting, behavior, and living environment data	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Depression detection; Deep learning; Social media	WEB	The World Health Organization (WHO) predicts that depression disorders will be widespread in the next 20 years. These disorders may affect a person's general health and habits such as altered sleeping and eating patterns in addition to their interpersonal relationships. Early depression detection and prevention therefore becomes an important issue. To address this critical issue, we recruited 1453 individuals who use Facebook frequently and collected their Facebook data. We then propose an automatic depression detection approach, named Deep Learning-based Depression Detection with Heterogeneous Data Sources (D3-HDS), to predict the depression label of an individual by analyzing his/her living environment, behavior, and the posting contents in the social media. The proposed method employs Recurrent Neural Networks to compute the posts representation of each individual. The representations are then combined with other content-based, behavior and living environment features to predict the depression label of the individual with Deep Neural Networks. To our best knowledge, this is the first attempt that simultaneously considers all the content-based, behavior, and living environment features for depression detection. The experiment results on a real dataset show that the performance of our approach significantly outperforms the other baselines.																	0925-9902	1573-7675				APR	2020	54	2					225	244		10.1007/s10844-018-0533-4													
J								Wikipedia enriched advertisement recommendation for microblogs by using sentiment enhanced user profiles	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Advertisement; Recommendation; Microblog; User profile; Sentiment; Wikipedia	WORD-OF-MOUTH; MATCHING APPROACH; SOCIAL MEDIA; SHORT TEXT; INFORMATION; MECHANISM; TWITTER	Advertisement recommendation on the Web is a popular research problem. For microblog platforms, different requirements arise due to the differences in the context of social media and social network. In this work, we propose an advertisement recommendation technique for microblogs. The proposed solution uses all contents of the messages (texts, captions, web links, hashtags), and enhances them with sentiment data and followee/follower interactions expressed as microblog posts to generate a new user model. As another novel feature, Wikipedia Good Pages are used as general background knowledge for matching user profiles and advertisement contents. On the basis of the similarity between advertisement vectors and user profile vectors, the most related advertisement for the selected user is determined. Evaluation results show that the proposed solution performs better for advertisement recommendation on microblog platform and works faster in comparison to other techniques.																	0925-9902	1573-7675				APR	2020	54	2					245	269		10.1007/s10844-018-0540-5													
J								A survey on group recommender systems	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Group recommender systems; Domain wise survey; Aggregation models	PEOPLE; USERS	Recommender systems are increasingly used in various domains like movies, travel, music, etc. The rise in social activities has increased the usage of recommender systems in general and group recommender systems in particular. A group recommender system is a system that recommends items to a group of users collectively, given their preferences. In addition to the user preferences, using social and behavioural aspects of group members to generate group recommendations will increase the quality of the content recommended in heterogeneous groups. Group recommender systems also address the cold start problem that arises in an individual recommendation system. This paper presents a survey on the state-of-the-art in group recommender systems concerning various domains. We discussed existing systems with respect to their aggregation and user preference models. This organisation is very useful to understand the intricacies with respect to each domain.																	0925-9902	1573-7675				APR	2020	54	2					271	295		10.1007/s10844-018-0542-3													
J								Evaluating content novelty in recommender systems	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Recommender Systems; Evaluation; Diversity; Novelty		Recommender systems are frequently evaluated using performance indexes based on variants and extensions of precision-like measures. As these measures are biased toward popular items, a list of recommendations simply must include a few popular items to perform well. To address the popularity bias challenge, new approaches for novelty and diversity evaluation have been proposed. On the one hand, novelty-based approaches model the quality of being new as apposed to that which is already known. Novelty approaches are commonly based on item views or user rates. On the other hand, diversity approaches model the quality of an item that is composed of different content elements. Diversity measures are commonly rooted in content-based features that characterize the diversity of the content of an item in terms of the presence/absence of a number of predefined nuggets of information. As item contents are also biased to popular contents (e.g., drama in movies or pop in music), diversity-based measures are also popularity biased. To alleviate the effect of popularity bias on diversity measures, we used an evaluation approach based on the degree of novelty of the elements that make up each item. We named this approach content novelty, as it mixes content and diversity approaches in a single and coherent evaluation framework. Experimental results show that our proposal is feasible and useful. Our findings demonstrate that the proposed measures yield consistent and interpretable results, producing insights that reduce the impact of popularity bias in the evaluation of recommender systems.																	0925-9902	1573-7675				APR	2020	54	2					297	316		10.1007/s10844-019-00548-x													
J								Action extraction from social networks	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Data mining; Actionable knowledge discovery; Action extraction; Social networks; Random walk	ACTION-RULES; KNOWLEDGE	Data mining methods focus on discovering models and patterns from large databases that summarize the data. However, generating such results is not an end in itself because their applicability is not straightforward. Ideally, the user would ultimately like to use them to decide what actions to take. Action mining explicitly emerged as a response to this need. Currently, most of the action mining methods rely on simple data which describes each object independently that means they do not take into account relationships between objects. In social networks, relationships enable an individual to influence another one, so ignoring them in action mining process would lead to miss some profitable actions. In this paper, we introduce action mining from social networks. In fact, our main contribution is to extract cost-effective actions which is formulated as an optimization problem where the objective is to learn actions consisting of the changes in the network that are likely to result in desired changes in the labels of intended individuals while minimizing the cost of the changes. Experiments confirm that the proposed approach performs much better than the current state-of-the-art in action mining.																	0925-9902	1573-7675				APR	2020	54	2					317	339		10.1007/s10844-019-00551-2													
J								Effective defense against fingerprinting attack based on autocorrelation property minimization approach	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Anonymity network; Tor; Fingerprinting attack; Defense model; Autocorrelation property		The website fingerprinting attack is one of the most important traffic analysis attacks that is able to identify a visited website in an anonymizing network such as Tor. It is shown that the existing defense methods against website fingerprinting attacks are inappropriate. In addition, they use large bandwidth and time overhead. In this study, we show that the autocorrelation property is the most important success factor of the website fingerprinting attack. We offer a new effective defense model to resolve this security vulnerability of the Tor anonymity network. The proposed defense model prevents information leakage from the passing traffic. In this regard, a novel mechanism is developed to make the traffic analysis a hard task. This mechanism is based on decreasing the entropy of instances by minimizing the autocorrelation property of them. By applying the proposed defense model, the accuracy of the most effective website fingerprinting attack reduces from 98% to the lowest success rate of the website fingerprinting attack, while the maximum bandwidth overhead of the network traffic remains on about 8%. Recall that the current best defense mechanisms reduce the accuracy of the attack to 23% with a minimum bandwidth overhead of more than 44%. Hence, the proposed defense model significantly reduces the accuracy of the website fingerprinting attack, while the bandwidth overhead increases very slightly (i.e., up to 8%).																	0925-9902	1573-7675				APR	2020	54	2					341	362		10.1007/s10844-019-00553-0													
J								Next-generation heartbeat classification with a column-store DBMS and UDFs	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										User defined functions; Machine learning deployment; Data management; Signal database	ECG BEAT CLASSIFICATION; ELECTROCARDIOGRAPHY	We live in a digital world where data is being generated at an always increasing rate. This creates the need to develop new technology not only for storing these vast amounts of data, but also for manipulating and analyzing it. It is through this data analysis that we can make decisions and generate knowledge. The medical field is no exception and healthcare and biomedical data must be stored and analyzed to gain insights that help in disease prevention and diagnostics. An example of this kind of data are electrocardiograms (ECG), whose careful analysis has proven to be of significant help to diagnose cardiovascular abnormalities. ECG recording devices can produce a very large amount of data in a short period of time. Usually abstracted as unstructured data, ECG digital signals have traditionally been stored and analyzed using file-based solutions for storage, and ad-hoc programs for data processing. We favor the idea that ECG signals can be abstracted as sets of tuples and stored in database relations. In this paper we present a proposal to store, manage, and analyze ECG data in a column-store database management system (DBMS). We provide extensive empirical evidence showing that incorporating complex analytical tasks such as ECG transformation and classification into a DBMS is not only feasible but also efficient and scalable. For this, we rely on the Structured Query Language provided by relational DBMSs, and the implementation of user defined functions.																	0925-9902	1573-7675				APR	2020	54	2					363	390		10.1007/s10844-019-00557-w													
J								Automatic keyphrase extraction: a survey and trends	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Information retrieval; Natural language processing; Text mining; Automatic keyphrase extraction; Supervised approaches; Unsupervised approaches; Deep learning	SYSTEM	Due to the exponential growth of textual data and web sources, an automatic mechanism is required to identify relevant information embedded within them. The utility of Automatic Keyphrase Extraction (AKPE) cannot be overstated, given its widespread adoption in many Information Retrieval (IR), Natural Language Processing (NLP) and Text Mining (TM) applications, and its potential ability to solve difficulties related to extracting valuable information. In recent years, a wide range of AKPE techniques have been proposed. However, they are still impaired by low accuracy rates and moderate performance. This paper provides a comprehensive review of recent research efforts on the AKPE task and its related techniques. More concretely, we highlight the common process of this task, while also illustrating the various approaches used (supervised, unsupervised, and Deep Learning) and released techniques. We investigate the major challenges that such techniques face and depict the specific complexities they address. Besides, we provide a comparison study of the best performing techniques, discuss why some perform better than others and propose recommendations to improve each stage of the AKPE process.																	0925-9902	1573-7675				APR	2020	54	2					391	424		10.1007/s10844-019-00558-9													
J								Self-adaptive weighted synthesised local directional pattern integrating with sparse autoencoder for expression recognition based on improved multiple kernel learning strategy	IET COMPUTER VISION										image representation; feature extraction; image classification; image texture; face recognition; learning (artificial intelligence); self-adaptive weighted synthesised local directional pattern; sparse autoencoder; improved multiple kernel learning strategy; facial expression recognition tasks; novel SW-SLDP feature descriptor; facial images; extracts sub-block features; directional intensity contrast; self-adaptive weights; sub-block feature; expressional image; expressional texture information; discriminative high-level feature; feature representation; IMKL strategy; high-level features	FACE RECOGNITION; ILLUMINATION	This study presents a novel method for solving facial expression recognition (FER) tasks which uses a self-adaptive weighted synthesised local directional pattern (SW-SLDP) descriptor integrating sparse autoencoder (SA) features based on improved multiple kernel learning (IMKL) strategy. The authors' work includes three parts. Firstly, the authors propose a novel SW-SLDP feature descriptor which divides the facial images into patches and extracts sub-block features synthetically according to both distribution information and directional intensity contrast. Then self-adaptive weights are assigned to each sub-block feature according to the projection error between the expressional image and neutral image of each patch, which can highlight such areas containing more expressional texture information. Secondly, to extract a discriminative high-level feature, they introduce SA for feature representation, which extracts the hidden layer representation including more comprehensive information. Finally, to combine the above two kinds of features, an IMKL strategy is developed by effectively integrating both soft margin learning and intrinsic local constraints, which is robust to noisy condition and thus improve the classification performance. Extensive experimental results indicate their model can achieve competitive or even better performance with existing representative FER methods.																	1751-9632	1751-9640				APR	2020	14	3					73	83		10.1049/iet-cvi.2018.5127													
J								3D driver pose estimation based on joint 2D-3D network	IET COMPUTER VISION										driver information systems; pose estimation; feature extraction; cameras; private driver data; joint 2D-3D network; three-dimensional driver pose estimation; computer-human interaction; convolutional neural networks; infrared image; point cloud; time-of-flight camera; 2D-3D network incorporating image-based		Three-dimensional (3D) driver pose estimation is a promising and challenging problem for computer-human interaction. Recently convolutional neural networks have been introduced into 3D pose estimation, but these methods have the problem of slow running speed and are not suitable for driving scenario. In this study, the proposed method is based on two types of inputs, infrared image and point cloud obtained from time-of-flight camera. The authors propose a joint 2D-3D network incorporating image-based and point-based feature to promote the performance of 3D human pose estimation and run on a high speed. For point cloud with invalid points, the authors first do preprocess and then design a denoising module to handle this problem. Experiments on private driver data set and public Invariant-Top View data set show that the proposed method achieves efficient and competitive performance on 3D human pose estimation.																	1751-9632	1751-9640				APR	2020	14	3					84	91		10.1049/iet-cvi.2019.0089													
J								Semi-supervised uncorrelated dictionary learning for colour face recognition	IET COMPUTER VISION										face recognition; learning (artificial intelligence); image colour analysis; computer vision; image recognition; feature extraction; visual databases; pattern classification; semisupervised uncorrelated dictionary learning; colour images; pattern recognition; machine learning; greyscale images; colour component images; colour difference information; decision-level similarity reduction; recognition effect; colour face recognition approach; colour components; face images; labelled colour face image samples; unlabelled colour face image samples; uncorrelated discriminating dictionaries; multiple public colour face image databases; dictionary decorrelation; structured dictionary learning; representative colour face recognition methods; recognition rates	DISCRIMINANT-ANALYSIS; SPACES; PCA; FEATURES; FUSION	Colour images are increasingly used in the fields of computer vision, pattern recognition and machine learning, since they can provide more identifiable information than greyscale images. Thus, colour face recognition has attracted accumulating attention. Its key problem is how to remove the similarity between colour component images and take full advantage of colour difference information. Decision-level similarity reduction between colour component images directly affects the recognition effect, but it has been found in no work. In this study, the authors propose a novel colour face recognition approach named semi-supervised uncorrelated dictionary learning (SUDL), which realises decision-level similarity reduction and fusion of all colour components in face images. SUDL employs the labelled and unlabelled colour face image samples into structured dictionary learning to achieve three uncorrelated discriminating dictionaries corresponding to three colour components of face images, and then uses these dictionaries and the sparse coding technique to make a classification decision. Experimental results in multiple public colour face image databases demonstrate that the dictionary decorrelation, structured dictionary learning and unlabelled samples used in the proposed approach are effective and reasonable, and the proposed approach outperforms several representative colour face recognition methods in recognition rates, despite of its poor time performance.																	1751-9632	1751-9640				APR	2020	14	3					92	100		10.1049/iet-cvi.2019.0125													
J								Crowd counting by the dual-branch scale-aware network with ranking loss constraints	IET COMPUTER VISION										learning (artificial intelligence); feature extraction; graph theory; convolutional neural nets; deep learning method; congested scene; VGG16; Branch_S; Branch_D; shallow fully convolutional network; deep fully convolutional network; high-level context features; image size differences; ranking loss function; Euclidean loss; dual-branch scale-aware network; ranking loss constraints; image crowd counting; density map	EVACUATION; TIME	Image crowd counting is a challenging problem. This study proposes a new deep learning method that estimates crowd counting for the congested scene. The proposed network is composed of two major components: the first ten layers of VGG16 are used as the backbone network, and a dual-branch (named as Branch_S and Branch_D) network is proposed to be the second part of the network. Branch_S extracts low-level information (head blob) through a shallow fully convolutional network and Branch_D uses a deep fully convolutional network to extract high-level context features (faces and body). Features learnt from the two different branches can handle the problem of scale variation due to perspective effects and image size differences. Features of different scales extracted from the two branches are fused to generate predicted density map. On the basis of the fact that an original graph must contain more or equal number of persons than any of its sub-images, a ranking loss function utilising the constraint relationship inside an image is proposed. Moreover, the ranking loss is combined with Euclidean loss as the final loss function. Our approach is evaluated on three benchmark datasets, and better results are achieved compared with the state-of-the-art works.																	1751-9632	1751-9640				APR	2020	14	3					101	109		10.1049/iet-cvi.2019.0704													
J								A Hybrid Discriminative Mixture Model for Cumulative Citation Recommendation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Mixture models; Task analysis; Training data; Knowledge based systems; Correlation; Encyclopedias; Cumulative citation recommendation; knowledge base acceleration; hybrid latent entity-document classes; mixture model	LOGISTIC-REGRESSION	This paper explores Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to deal with unseen entities without annotation. A compromised solution is to build a global entity-unspecific model for all entities without respect to the relationship information among entities, which cannot guarantee achieving a satisfactory result for each entity. Moreover, most previous methods can not adequately exploit prior knowledge embedded in entities or documents due to considering all kinds of features indifferently. In this paper, we propose a novel entity and document class-dependent discriminative mixture model by introducing one intermediate layer to model the correlation between entity-document pairs and hybrid latent entity-document classes. The model can better adjust to different types of entities and documents, and achieve better performance when dealing with a broad range of entity and document classes. An extensive set of experiments has been conducted on two offical datasets, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.																	1041-4347	1558-2191				APR 1	2020	32	4					617	630		10.1109/TKDE.2019.2893328													
J								Addressing the Item Cold-Start Problem by Attribute-Driven Active Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Collaboration; Recommender systems; Computer science; Task analysis; Learning systems; Uncertainty; Matrix decomposition; Recommender systems; active learning; cold-start; exploitation and exploration		In recommender systems, cold-start issues are situations where no previous events, e.g., ratings, are known for certain users or items. In this paper, we focus on the item cold-start problem. Both content information (e.g., item attributes) and initial user ratings are valuable for seizing users' preferences on a new item. However, previous methods for the item cold-start problem either (1) incorporate content information into collaborative filtering to perform hybrid recommendation, or (2) actively select users to rate the new item without considering content information and then do collaborative filtering. In this paper, we propose a novel recommendation scheme for the item cold-start problem by leveraging both active learning and items' attribute information. Specifically, we design useful user selection criteria based on items' attributes and users' rating history, and combine the criteria in an optimization framework for selecting users. By exploiting the feedback ratings, users' previous ratings and items' attributes, we then generate accurate rating predictions for the other unselected users. Experimental results on two real-world datasets show the superiority of our proposed method over traditional methods.																	1041-4347	1558-2191				APR 1	2020	32	4					631	644		10.1109/TKDE.2019.2891530													
J								An Efficient Approach to Finding Dense Temporal Subgraphs	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Dense subgraphs; temporal networks; statistics driven approaches; evolving convergence; big data		Dense subgraph discovery has proven useful in various applications of temporal networks. We focus on a special class of temporal networks whose nodes and edges are kept fixed, but edge weights regularly vary with timestamps. However, finding dense subgraphs in temporal networks is non-trivial, and its state of the art solution uses a filter-and-verification framework that is not scalable on large temporal networks. In this study, we propose a highly efficient approach to finding dense subgraphs in large temporal networks with $T$T timestamps. (1) We first develop a statistics-driven approach that employs hidden statistics to identifying $k$k time intervals, instead of $T(T+1)/2$T(T+1)/2 ones ($k$k is typically much smaller than $T$T), which strikes a balance between quality and efficiency. (2) After proving that the problem has no constant factor approximation algorithms, we design better heuristic algorithms to attack the problem, by connecting finding dense subgraphs with a variant of the Prize Collecting Steiner Tree problem. (3) Finally, we have conducted an extensive experimental study to verify that our approach is both effective and efficient.																	1041-4347	1558-2191				APR 1	2020	32	4					645	658		10.1109/TKDE.2019.2891604													
J								Feature Selection for Neural Networks Using Group Lasso Regularization	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Feature extraction; Task analysis; Sun; Optimization; Multilayer perceptrons; Input variables; Feature selection; Group Lasso; sparsity; neural networks; convergence	VARIABLE SELECTION; REGRESSION SHRINKAGE; GRADIENT-METHOD; CLASSIFICATION; NONSMOOTH; FRAMEWORK	We propose an embedded/integrated feature selection method based on neural networks with Group Lasso penalty. Group Lasso regularization is considered to produce sparsity on the inputs to the network, i.e., for selection of useful features. Lasso based feature selection using a multi-layer perceptron usually requires an additional set of weights, while our Group Lasso formulation does not require that. However, Group Lasso penalty is non-differentiable at the origin. This may lead to oscillations in numerical simulations and make it difficult to analyze theoretically. To address this issue, four smoothing Group Lasso penalties are introduced. A rigorous proof for the convergence of the proposed algorithm is presented under suitable assumptions. To verify the effectiveness, a three-step algorithmic architecture is adopted in implementation. Experimental results on several datasets validate the theoretical results and demonstrate the competitive performance of the proposed method.																	1041-4347	1558-2191				APR 1	2020	32	4					659	673		10.1109/TKDE.2019.2893266													
J								GERF: A Group Event Recommendation Framework Based on Learning-to-Rank	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Feature extraction; Task analysis; Bayes methods; Recommender systems; Prediction algorithms; Context modeling; Training; Group recommendation; event-based social networks; cold-start recommendation; learning-to-rank	SYSTEMS; MODEL	Event recommendation is an essential means to enable people to find attractive upcoming social events, such as party, exhibition, and concert. While growing line of research has focused on suggesting events to individuals, making event recommendation for a group of users has not been well studied. In this paper, we aim to recommend upcoming events for a group of users. We formalize group recommendation as a ranking problem and propose a group event recommendation framework GERF based on learning-to-rank technique. Specifically, we first analyze different contextual influences on user's event attendance, and extract preference of user to event considering each contextual influence. Then, the preference scores of the users in a group are taken as the features for learning-to-rank to model the preference of the group. Moreover, a fast pairwise learning-to-rank algorithm, Bayesian group ranking, is proposed to learn ranking model for each group. Our framework is easily to incorporate additional contextual influences, and can be applied to other group recommendation scenarios. Extensive experiments have been conducted to evaluate the performance of GERF on two real-world datasets and demonstrate the appealing performance of our method on both accuracy and time efficiency.																	1041-4347	1558-2191				APR 1	2020	32	4					674	687		10.1109/TKDE.2019.2893361													
J								Jointly Learning Topics in Sentence Embedding for Document Summarization	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Semantics; Predictive models; Feature extraction; Computational modeling; Task analysis; Context modeling; Training; Sentence embedding; Gaussian topics; summarization; relevance; and salience		Summarization systems for various applications, such as opinion mining, online news services, and answering questions, have attracted increasing attention in recent years. These tasks are complicated, and a classic representation using bag-of-words does not adequately meet the comprehensive needs of applications that rely on sentence extraction. In this paper, we focus on representing sentences as continuous vectors as a basis for measuring relevance between user needs and candidate sentences in source documents. Embedding models based on distributed vector representations are often used in the summarization community because, through cosine similarity, they simplify sentence relevance when comparing two sentences or a sentence/query and a document. However, the vector-based embedding models do not typically account for the salience of a sentence, and this is a very necessary part of document summarization. To incorporate sentence salience, we developed a model, called CCTSenEmb, that learns latent discriminative Gaussian topics in the embedding space and extended the new framework by seamlessly incorporating both topic and sentence embedding into one summarization system. To facilitate the semantic coherence between sentences in the framework of prediction-based tasks for sentence embedding, the CCTSenEmb further considers the associations between neighboring sentences. As a result, this novel sentence embedding framework combines sentence representations, word-based content, and topic assignments to predict the representation of the next sentence. A series of experiments with the DUC datasets validate CCTSenEmb's efficacy in document summarization in a query-focused extraction-based setting and an unsupervised ILP-based setting.																	1041-4347	1558-2191				APR 1	2020	32	4					688	699		10.1109/TKDE.2019.2892430													
J								Multi-Campaign Oriented Spatial Crowdsourcing	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Crowdsourcing; Throughput; Diversity reception; Linear programming; Companies; Advertising; Spatial crowdsourcing; crowd sensing; worker diversity	DIVERSITY	Recently, spatial crowdsourcing has been drawing increasing attention with its great potential in collecting geographical knowledge. The system throughput (number of assigned tasks) and workers' travel distance are two of many important factors in spatial crowdsourcing, and the improvement to one of them usually means the sacrifice of the other. However, most existing works resolve the trade-off between these two factors by simply targeting tasks within a bounding circle of each worker. In this paper, we compromise between the throughput and the distance by formulating these two factors as score terms in the objective function. This flexible formulation has the advantages of abandoning distant tasks and minimizing workers' travel distance for reachable tasks. Aside from that, we study the multi-campaign scenario of spatial crowdsourcing, which is not uncommon in practical applications while not yet discussed in existing works. The worker diversity of the campaigns is considered to be another goal and formulated as another score term in the objective function. Subsequently, the problem of multi-campaign oriented spatial crowdsourcing is to maximize the objective function comprised by the aforementioned score terms. We prove that the problem is NP-hard, thus, we propose several approximation solutions. Extensive experiments have been conducted to confirm the effectiveness and the efficiency of the devised solutions.																	1041-4347	1558-2191				APR 1	2020	32	4					700	713		10.1109/TKDE.2019.2893293													
J								NewMCOS: Towards a Practical Multi-Cloud Oblivious Storage Scheme	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Data privacy; oblivious RAM; cloud storage; access pattern		Encryption alone is not enough to protect data privacy, because access pattern leaks some sensitive information. Oblivious RAM (ORAM), the solution to this problem, is still far from practical deployment for heavy storage and communication/ computation overhead. To reduce them, an insightful idea was proposed to utilize non-colluding clouds to shift client computation and client-cloud communication to the clouds. The proposed multi-cloud ORAM achieved $O$O(1) client-cloud bandwidth cost and removed most of client computation. In this paper, we exploit "disconnected ORAM operation" and design "two-layer encryption" to further reduce these overheads. Experiments show that our proposed scheme, NewMCOS, significantly reduces evict cache size from GB/MB to KB level with about 2-3 times lower response time and 20 percent savings in bandwidth for clouds, compared to other schemes. Theoretically speaking, we reduce evict cache size from $O(\sqrt{N})$O(N) to $O(ZK)$O(ZK), where $N$N is the number of real data blocks, $K$K is the number of clouds ($2<K << \sqrt{N}$2<K<<N), and $Z$Z is the number of real blocks uploaded from the client for eviction. By employing "lazy eviction operation", the write frequency is reduced by $O(Z)$O(Z), the shuffling bandwidth cost is reduced by $\Omega (Z\; \log Z)$omega(ZlogZ). Meanwhile, NewMCOS is proved to be secure.																	1041-4347	1558-2191				APR 1	2020	32	4					714	727		10.1109/TKDE.2019.2891581													
J								On Combining Biclustering Mining and AdaBoost for Breast Tumor Classification	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Breast tumors; Feature extraction; Breast cancer; Ultrasonic imaging; Biclustering; ensemble learning of diagnostic rules; feature space dependent normalized distance; PC-DFS; computer-aided diagnosis	COMPUTER-AIDED DIAGNOSIS; DECISION TREE; ULTRASOUND; SYSTEM; MASSES; FEATURES; NODULES; US	Breast cancer is now considered as one of the leading causes of deaths among women all over the world. Aiming to assist clinicians in improving the accuracy of diagnostic decisions, computer-aided diagnosis (CAD) system is of increasing interest in breast cancer detection and analysis nowadays. In this paper, a novel computer-aided diagnosis scheme with human-in-the-loop is proposed to help clinicians identify the benign and malignant breast tumors in ultrasound. In this framework, feature acquisition is performed by a user-participated feature scoring scheme that is based on Breast Imaging Reporting and Data System (BI-RADS) lexicon and experience of doctors. Biclustering mining is then used as a useful tool to discover the column consistency patterns on the training data. The patterns frequently appearing in the tumors with the same label can be regarded as a potential diagnostic rule. Subsequently, the diagnostic rules are utilized to construct component classifiers of the Adaboost algorithm via a novel rules combination strategy which resolves the problem of classification in different feature spaces (PC-DFS). Finally, the AdaBoost learning is performed to discover effective combinations and integrate them into a strong classifier. The proposed approach has been validated using a large ultrasounic dataset of 1,062 breast tumor instances (including 418 benign cases and 644 malignant cases) and its performance was compared with several conventional approaches. The experimental results show that the proposed method yielded the best prediction performance, indicating a good potential in clinical applications.																	1041-4347	1558-2191				APR 1	2020	32	4					728	738		10.1109/TKDE.2019.2891622													
J								Reducing Web Page Complexity to Facilitate Effective User Navigation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Complexity theory; Navigation; Web pages; Usability; Measurement; Maintenance engineering; Data models; Website design; page complexity; user navigation; mathematical programming	WEBSITE STRUCTURE; REORGANIZATION; USABILITY; FRAMEWORK; IMPACTS; LINKAGE; SEARCH; DESIGN; MODEL	As a website evolves to align with users' changing information needs and interests, its structure can outgrow the original design, accumulating links and pages in unanticipated places. This increases complexity to both web pages and the navigation structure, which could cause difficulty in locating relevant links and information. Though the increasing complexity of website and its impact on users' psychological perception have been anecdotally well-recognized, the need to address it with a formal and rigorous method is understudied in the literature. This paper is one of the first studies to examine how to streamline website structures to enhance user navigation. We use a widely used metric in the literature - a page's outdegree (the number of links in a page) - as the measurement for complexity, because it not only serves as a good proximity for page complexity but also has a significant implication on website structure. We propose a method based on mathematical programming (MP) model that can significantly reduce users' cognitive load by effectively eliminating appropriate links from pages with high complexity. We have performed extensive experiments on both a real dataset and very large synthetic datasets with statistical similarities to the real dataset. The results indicate that our method not only significantly reduces web page and structure complexity with very small impact to user navigation, but also can be effectively solved and scales up remarkably well, suggesting it is useful for website maintenance on a progressive basis. In addition, we conduct a study to evaluate the performance of streamlined website structures using the real dataset and the results confirm the validity of our method.																	1041-4347	1558-2191				APR 1	2020	32	4					739	753		10.1109/TKDE.2019.2893242													
J								Scalable Spectral Clustering for Overlapping Community Detection in Large-Scale Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Clustering algorithms; Image edge detection; Benchmark testing; Partitioning algorithms; Social network services; Detection algorithms; Optimization; Community detection algorithm; overlapping communities; real-world network; spectral clustering; normalized cut	ALGORITHM; MODELS	While the majority of methods for community detection produce disjoint communities of nodes, most real-world networks naturally involve overlapping communities. In this paper, a scalable method for the detection of overlapping communities in large networks is proposed. The method is based on an extension of the notion of normalized cut to cope with overlapping communities. A spectral clustering algorithm is formulated to solve the related cut minimization problem. When available, the algorithm may take into account prior information about the likelihood for each node to belong to several communities. This information can either be extracted from the available metadata or from node centrality measures. We also introduce a hierarchical version of the algorithm to automatically detect the number of communities. In addition, a new benchmark model extending the stochastic blockmodel for graphs with overlapping communities is formulated. Our experiments show that the proposed spectral method outperforms the state-of-the-art algorithms in terms of computational complexity and accuracy on our benchmark graph model and on five real-world networks, including a lexical network and large-scale social networks. The scalability of the proposed algorithm is also demonstrated on large synthetic graphs with millions of nodes and edges.																	1041-4347	1558-2191				APR 1	2020	32	4					754	767		10.1109/TKDE.2019.2892096													
J								Similarity Join and Similarity Self-Join Size Estimation in a Streaming Environment	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Selectivity estimation; similarity join; size estimation; one pass algorithm; streaming data		We study the problem of similarity self-join and similarity join size estimation in a streaming setting where the goal is to estimate, in one scan of the input and with sublinear space in the input size, the number of record pairs that have a similarity within a given threshold. The problem has many applications in data cleaning and query plan generation, where the cost of a similarity join may be estimated before actually doing the join. On unary input where two records either match or don't match, the problem becomes join and self-join size estimation for which one-pass algorithms are readily available. Our work addresses the problem for $d$d-ary input, for $d \geq 1$d >= 1, where the degree of similarity can vary from 1 to $d$d. We show that our proposed algorithm gives an accurate estimate and scales well with the input size. We provide error bounds and time and space costs, and conduct an extensive experimental evaluation of our algorithm, comparing its estimation accuracy to a few competitors, including some multi-pass algorithms. Our results show that given the same space, the proposed algorithm has an order of magnitude less error for a large range of similarity thresholds.																	1041-4347	1558-2191				APR 1	2020	32	4					768	781		10.1109/TKDE.2019.2893175													
J								SRA: Secure Reverse Auction for Task Assignment in Spatial Crowdsourcing	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Privacy; reverse auction; spatial crowdsourcing; task assignment		In this paper, we study a new type of spatial crowdsourcing, namely competitive detour tasking, where workers can make detours from their original travel paths to perform multiple tasks, and each worker is allowed to compete for preferred tasks by strategically claiming his/her detour costs. The objective is to make suitable task assignment by maximizing the social welfare of crowdsourcing systems and protecting workers' private sensitive information. We first model the task assignment problem as a reverse auction process. We formalize the winning bid selection of reverse auction as an $n$n-to-one weighted bipartite graph matching problem with multiple 0-1 knapsack constraints. Since this problem is NP-hard, we design an approximation algorithm to select winning bids and determine corresponding payments. Based on this, a Secure Reverse Auction (SRA) protocol is proposed for this novel spatial crowdsourcing. We analyze the approximation performance of the proposed protocol and prove that it has some desired properties, including truthfulness, individual rationality, computational efficiency, and security. To the best of our knowledge, this is the first theoretically provable secure auction protocol for spatial crowdsourcing systems. In addition, we also conduct extensive simulations on a real trace to verify the performance of the proposed protocol.																	1041-4347	1558-2191				APR 1	2020	32	4					782	796		10.1109/TKDE.2019.2893240													
J								Adaptive Consistency Propagation Method for Graph Clustering	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Manifolds; Optimization; Clustering methods; Task analysis; Eigenvalues and eigenfunctions; Data mining; Adaptation models; Clustering; manifold learning; graph learning; consistency propagation		Graph clustering plays an important role in data mining. Based on an input data graph, data points are partitioned into clusters. However, most existing methods keep the data graph fixed during the clustering procedure, so they are limited to exploit the implied data manifold and highly dependent on the initial graph construction. Inspired by the recent development on manifold learning, this paper proposes an Adaptive Consistency Propagation (ACP) method for graph clustering. In order to utilize the features captured from different perspectives, we further put forward the Multi-view version of the ACP model (MACP). The main contributions are threefold: (1) the manifold structure of input data is sufficiently exploited by propagating the topological connectivities between data points from near to far; (2) the optimal graph for clustering is learned by taking graph learning as a part of the optimization procedure; and (3) the negotiation among the heterogeneous features is captured by the multi-view clustering model. Extensive experiments on real-world datasets validate the effectiveness of the proposed methods on both single- and multi-view clustering, and show their superior performance over the state-of-the-arts.																	1041-4347	1558-2191				APR 1	2020	32	4					797	802		10.1109/TKDE.2019.2936195													
J								Bayesian Networks for Data Integration in the Absence of Foreign Keys	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Bayes methods; Probabilistic logic; Data integration; Periodic structures; Education; Data models; Random variables; Bayesian networks; probabilistic data integration		In the era of open data, a single data source rarely contains all of the attributes we need for inference in specific applications. For example, a marketing department may aim to integrate retailer-specific purchase data with separate demographic data for purposes of targeted advertising - a capability not possible with either dataset alone. In this work, we address two key desiderata of an automated framework for probabilistic data integration over multiple data sources: (1) we require that each relational data source share at least one attribute with another relational data source, but we do not require these attributes to be foreign keys (e.g., attributes such as gender, age, and postal code are not foreign keys because they do not uniquely identify individuals in a data source) and (2) we require inference to be probabilistic to reflect inherent uncertainty in population-level predictions given the absence of foreign keys. While some frameworks such as Probabilistic Relational Models (PRMs) address point (2), they do not address point (1) since they rely on foreign keys to link tables. To achieve both desiderata simultaneously, we develop an automated framework to construct Bayesian networks for data integration capable of answering any probabilistic query spanning the attributes of multiple relational data sources. We demonstrate that our framework is able to closely approximate the inference of a global Bayesian network over a single relation that has been projected onto multiple local relations and further investigate properties of local relations such as the number of shared attributes and their cardinality to understand how these properties affect the quality of inference.																	1041-4347	1558-2191				APR 1	2020	32	4					803	808		10.1109/TKDE.2019.2940019													
J								Discrimination-Aware Projected Matrix Factorization	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Clustering; linear discriminant analysis; non-negative matrix factorization; subspace learning	CLASSIFICATION; COMPLETION	Non-negative Matrix Factorization (NMF) has been one of the most popular clustering techniques in machine leaning, and involves various real-world applications. Most existing works perform matrix factorization on high-dimensional data directly. However, the intrinsic data structure is always hidden within the low-dimensional subspace. And, the redundant features within the input space may affect the final result adversely. In this paper, a new unsupervised matrix factorization method, Discrimination-aware Projected Matrix Factorization (DPMF), is proposed for data clustering. The main contributions are threefold: (1) The linear discriminant analysis is jointly incorporated into the unsupervised matrix factorization framework, so the clustering can be accomplished in the discriminant subspace. (2) The manifold regularization is introduced to perceive the geometric information, and the ${\ell _{2,1}}$l2,1-norm is utilized to improve the robustness. (3) An efficient optimization algorithm is designed to solve the proposed problem with proved convergence. Experimental results on one toy dataset and eight real-world benchmarks show the effectiveness of the proposed method.																	1041-4347	1558-2191				APR 1	2020	32	4					809	814		10.1109/TKDE.2019.2936855													
J								LN-SNE: Log-Normal Distributed Stochastic Neighbor Embedding for Anomaly Detection	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Unsupervised learning; dimensionality reduction; anomaly detection; outlier detection	DIMENSIONALITY	We present a new unsupervised dimensionality reduction technique, called LN-SNE, for anomaly detection. LN-SNE generates a parametric embedding by means of Restricted Boltzmann Machines and uses a heavy-tail distribution to project data to a lower dimensional space such that dissimilarities between normal data and anomalies are preserved or strengthened. We compare LN-SNE to several benchmark dimensionality reduction methods on real datasets. The results suggest that LN-SNE for anomaly detection is less sensitive to the dimension of the latent space than the other methods and outperforms them in terms of accuracy. We empirically show that our technique scales near-linearly with respect to the number of dimensions and data size.																	1041-4347	1558-2191				APR 1	2020	32	4					815	820		10.1109/TKDE.2019.2934450													
J								Nonparametric Density Estimation Using Copula Transform, Bayesian Sequential Partitioning, and Diffusion-Based Kernel Estimator	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Estimation; Kernel; Bandwidth; Histograms; Transforms; Bayes methods; Diffusion processes; Multivariate density estimation; non-parametric; high-dimensional; Bayesian sequential partitioning; copula transform; linear diffusion	BANDWIDTH SELECTION	Non-parametric density estimation methods are more flexible than parametric methods, due to the fact that they do not assume any specific shape or structure for the data. Most non-parametric methods, like Kernel estimation, require tuning of parameters to achieve good data smoothing, a non-trivial task, even in low dimensions. In higher dimensions, sparsity of data in local neighborhoods becomes a challenge even for non-parametric methods. In this paper, we use the copula transform and two efficient non-parametric methods to develop a new method for improved non-parametric density estimation in multivariate domain. After separation of marginal and joint densities using copula transform, a diffusion-based kernel estimator is employed to estimate the marginals. Next, Bayesian sequential partitioning (BSP) is used in the joint density estimation.																	1041-4347	1558-2191				APR 1	2020	32	4					821	826		10.1109/TKDE.2019.2930052													
J								ROBUST SPEED CONTROL OF BRUSHLESS DC MOTOR BASED ON ADAPTIVE NEURO FUZZY INFERENCE SYSTEM FOR ELECTRIC MOTORCYCLE APPLICATION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Brushless direct current motor; Speed controller; Adaptive neuro fuzzy inference system	ANFIS	Electric vehicles have been widely discussed in some articles since the cost of fuel for conventional vehicles in this era is not stable and tends to increase. And also, conventional vehicles are also not fully eco-friendly and have poor efficiency. Electric vehicles, mostly, use Brushless Direct Current (BLDC) motor as the prime mover, since it has a simple structure, good performance and high efficiency. This paper presents an Adaptive Neuro Fuzzy Inference System (ANFIS) controller to control the speed of BLDC motor applied for electric motorcycle. ANFIS controller was designed and evaluated, then compared to Proportional-Integral-Derivative (PID) and Fuzzy-PID controllers. ANFIS is trained based on the data of Fuzzy-PID performances with slight modification. According to the study, ANFIS controller has better performances compared to PID and Fuzzy-PID controllers with average steady state error of 0.13% when the speed reference changes and 0.16% when the load changes. Moreover, ANFIS controller obtains 0.27 s for rise time according to 3000 rpm of speed reference, while the other controllers have longer time to reach the speed reference.																	1349-4198	1349-418X				APR	2020	16	2					415	428		10.24507/ijicic.16.02.415													
J								INTEGRATION OF SOLAR TRACKER AND MAXIMUM POWER POINT TRACKING FOR IMPROVING PHOTOVOLTAIC (PV) SYSTEM EFFICIENCY	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Photovoltaic system; Solar tracker; Maximum power point tracking; Fuzzy logic; Solar position	ALGORITHM	This paper presents a method to integrate the solar tracker and the maximum power point tracking (MPPT) to improve the photovoltaic (PV) system efficiency. The integrated system provides a closed-loop solar tracker without the sensors. Instead of using the solar sensor, the output power of the MPPT is employed as the feedback signal to the solar tracker. The solar tracker estimates the solar azimuth and elevation angles using an astronomical algorithm based on the latitude, longitude and the date-time of the local site. To improve the solar tracking accuracy, the fuzzy logic controller is employed to adjust the angle according to the power slope of maximum power with respect to the solar tracker angle. From the simulation results, the proposed method increases the PV energy by 23.23% compared to the fixed PV panel. It improves the efficiency of the existing integrated solar tracker by 0.25% based on the simulation models.																	1349-4198	1349-418X				APR	2020	16	2					429	443		10.24507/ijicic.16.02.429													
J								TWO-DEGREE-OF-FREEDOM COMPOUND CONTROL BASED ON RBF NEURAL NETWORK FOR AIR CONDITIONING TEMPERATURE CONTROL SYSTEM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Feedforward; RBF neural network; Smith predictive compensation; Incremental PID		Aiming at time delay problem of temperature control system in air-conditioned room, this paper proposes a new two-degree-of-freedom compound control approach based on RBF neural network. The new approach is mainly based on RBF neural network to adjust PID parameters. And the Smith predictor is used to eliminate the effect from lag term of the controlled object. Then, a feedforward controller is used to make the whole system have better tracking performance and anti-interference ability. The simulation results show that the dynamic performance of the new control system can be improved obviously by using the two-degree-of-freedom composite controller.																	1349-4198	1349-418X				APR	2020	16	2					445	456		10.24507/ijicic.16.02.445													
J								CONTROL OF TIME-VARYING DELAY SYSTEMS WITH UNCERTAIN PARAMETERS VIA FUZZY-MODELED PRESCRIBED PERFORMANCE CONTROL APPROACH	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Linear matrix inequality (LMI); Prescribed performance control; Takagi-Sugeno (T-S) fuzzy model; Time-varying delay systems; Uncertain parameter systems	H-INFINITY CONTROL; NONLINEAR-SYSTEMS; FEEDBACK-CONTROL; DYNAMICAL-SYSTEMS; DESIGN; NETWORK; FILTER	This paper deals with the problem of robust control for time-varying delay systems with uncertain parameters and disturbances which have the inexactly measured state via the Fuzzy-Modeled Prescribed Performance Control (F-PPC) procedure. The system models are assumed to depend on the phenomena of uncertain parameters and disturbances which are frequently encountered in most real dynamical systems as well as a time-varying delay of systems. To describe the uncertain nonlinearities with time-varying delay systems in the Takagi-Sugeno (T-S) fuzzy model, the global behavior of a nonlinear system can be simply represented using the T-S plant rule models. Then, a novel controller is computed by the linear matrix inequality (LMI) conditions. The obtained controller guarantees the L-2 -gain of the mapping from the exogenous input noise to the regulated output to be less than some prescribed value. The control design is applied to the nonlinear benchmark problems to illustrate the benefits and applicability of the proposed method. The results also show that the proposed F-PPC approach guarantees the fulfillment of both the asymptotic stability and the prescribed performance index.																	1349-4198	1349-418X				APR	2020	16	2					457	479		10.24507/ijicic.16.02.457													
J								FORCE/POSITION CONTROL STRATEGY OF 3-PRS ANKLE REHABILITATION ROBOT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Ankle rehabilitation robot; Kinematics; Force/Position control; Position control; Impedance control	PARALLEL ROBOT; DESIGN	Aiming at the control requirement of simultaneous position and force of the ankle rehabilitation robot, a prototype of the 3-PRS ankle rehabilitation robot and its force/position control strategy are proposed based on impedance control. The structural design and kinematics analysis of the 3-PRS ankle rehabilitation robot are carried out. The proposed force/position control strategy combines organically position control and impedance control. The design process of the impedance controller and its parameter is presented. The proposed control strategy is verified by simulation experiments. The influence of different impedance values on the whole impedance control system is studied based on the simulation results. Finally a force/position control strategy with real-time weight setting is further proposed, which makes the 3-PRS ankle rehabilitation robot achieve more reasonable control effect in the process of flexibility control.																	1349-4198	1349-418X				APR	2020	16	2					481	494		10.24507/ijicic.16.02.481													
J								IMPROVED ROBUST ABSOLUTE STABILITY OF TIME-DELAYED LUR'E SYSTEMS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Lur'e nonlinear system; Lyapunov-Krasovskii functional; Robust absolute stability; Time-varying delay	VARYING DELAY; CRITERIA; SYNCHRONIZATION	In this paper, we focus on the problem of the absolute and robust absolute stability for time-delayed Lur'e systems. Both the cases with time-invariant and time-varying nonlinearities are considered. By using two improved relaxed integral inequality lemmas, new delay-dependent absolute and robust absolute stability criteria are proposed via Lyapunov-Krasovskii functional (LKF) approach. The stability conditions can be expressed as convex linear matrix inequality (LMI) framework, which can be solved by using standard LMI convex optimization solvers. The criteria proposed in this paper are less conservative than some previous ones. Finally, some numerical examples are presented to show the effectiveness of the proposed approach.																	1349-4198	1349-418X				APR	2020	16	2					495	512		10.24507/ijicic.16.02.495													
J								OUTLIERS TREATMENT IN POWER CURVES USING HYBRID ARTIFICIAL INTELLIGENCE TECHNIQUE	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Outliers detection; Correction; Power curves; Artificial intelligence; Fuzzy logic; Neural networks; Hybrid technique	NORMALITY	The distribution substations power curves are often affected by outliers: very discrepant measurements of the curve standard behavior. If the presence of outliers is very large, the power utilities internal studies and analysis developed from the history of the data collected may be compromised. In order to detect and correct atypical data, a complementary module for the supervisory system was developed. In the previous work of the authors, two techniques were developed based on artificial intelligence: fuzzy logic and artificial neural networks. This work presents a hybrid technique that uses the best performances of the previous ones to reduce the relative error, increasing the correction technique performance. Furthermore, it develops an outlier detection method based on the standard behavior of the curve and the distribution of the measurements. The performances of the techniques are compared using real data from a substation over 6 years.																	1349-4198	1349-418X				APR	2020	16	2					513	525		10.24507/ijicic.16.02.513													
J								OPTIMIZATION OF HYBRID ENERGY SYSTEM BASED ON PARETO DIFFERENTIAL EVOLUTION ALGORITHM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Differential evolution algorithm; Pareto optimal solution; Hybrid energy system; Optimal allocation; Photovoltaic system; Wind turbine	GENERATION	In order to achieve the goal of energy-saving and emission reduction, a hybrid energy system has been proposed compared to the traditional diesel engine power generation, but the technology is still immature and in the process of development. Firstly, the differential evolution algorithm and Pareto optimal strategy are used to optimize the system configuration. Then the simulation analysis is carried out by MATLAB, including comparison of various hybrid energy schemes, comparison of different object number schemes and comparison with traditional diesel engine power generation systems. The simulation results show that the method has better performance in terms of cost, service life and pollutant emissions, and is superior to the traditional scheme.																	1349-4198	1349-418X				APR	2020	16	2					527	542		10.24507/ijicic.16.02.527													
J								HUMAN-ROBOT COOPERATION BASED ON VISUAL COMMUNICATION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Man-machine cooperation; Human-robot cooperation; Visual communication; Human care; Autonomous robots; Depth; Segmentation		This paper proposes a human-robot cooperative system supported by visual communication. Human,-robot cooperation is expected in various fields nowadays in order to raise efficiency, accuracy and safety of work, to name a few, production of goods, logistics, car driving, care of aged people, etc. In the human-robot cooperation, it is desirable that a human takes less part of work; whereas a robot takes a larger role with it. The idea of the present paper is that visual decision making is done by a human rather than a robot. The proposed system goes to a specified remote spot autonomously and performs objects acquisition there by communicating visually with a user. It aims at realizing 'a shopping robot' in near future and expected to be used by disadvantaged shoppers. The proposed system is presented and some experimental results are shown.																	1349-4198	1349-418X				APR	2020	16	2					543	554		10.24507/ijicic.16.02.543													
J								TOWARDS WORD SENSE DISAMBIGUATION USING MULTIPLE KERNEL SUPPORT VECTOR MACHINE	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Word sense disambiguation (WSD); Multiple kernel learning (MKL); Support vector machine (SVM); Kernel method; Natural language processing (NLP)		Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, has been a long-standing research objective for natural language processing (NLP). In this paper, we investigate the problem of combining multiple feature channels using kernel methods for the purpose of effective WSD. A straightforward method is to use a uniform combination of more adequate kernels, which are built from different types of data representations or knowledge sources. Instead of using an equal weight for all kernels in the combination, we consider the problem of integrating multiple feature channels using the state-of-the-art multiple kernel learning (MKL) approach, which can learn different weights that reflect the different importance of the feature channels for disambiguation. This approach has the advantage of the possibility to combine and select the more relevant feature channels in an elegant way. Combined with the support vector machine (SVM), this approach is demonstrated with several Senseval/Semeval disambiguation tasks.																	1349-4198	1349-418X				APR	2020	16	2					555	570		10.24507/ijicic.16.02.555													
J								PRESCRIBED-PERFORMANCE-BASED ADAPTIVE CONTROL FOR HYBRID ENERGY STORAGE SYSTEMS OF BATTERY AND SUPERCAPACITOR IN ELECTRIC VEHICLES	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Battery; Supercapacitor; Projection operator; Adaptive law; Prescribed performance control	REAL-TIME OPTIMIZATION; MANAGEMENT STRATEGY; NONLINEAR-SYSTEMS; POWER MANAGEMENT	To achieve the desired prescribed dynamic and steady-state responses, a prescribed performance control strategy is proposed in electric vehicle system, whose power system is hybrid energy storage system (HESS). The strategy also adopts adaptive control to estimate unknown parameters. Firstly, mathematical models of HESS are derived and the unknown parameters are defined. Secondly, according to the working characteristics of energy storage units (ESUs), battery and supercapacitor are responsible for supplying power to load in different situations. Then, the performance function is considered as a prescribed range for the current tracking error and the corresponding error transformation is introduced. Meanwhile, under the projection operator adaptive law, unknown parameters can be estimated and control laws can be obtained. In addition, the whole system including models and controller is proved to be stable. Finally, simulation results show the effectiveness and superiority of the proposed control strategy.																	1349-4198	1349-418X				APR	2020	16	2					571	583		10.24507/ijicic.16.02.571													
J								DETERMINATION OF MODEL STRUCTURE VIA CYCLO-STATIONARITY BASED NEURAL NETWORK	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Model structure; Output over-sampling; Cyclo-stationarity; System identification	BLIND IDENTIFICATION	Determination of model structure commonly influences the performance of system identification and model applications. It has to be performed by the data-driven methods if the priori structure information is not available, whereas the data are sometimes collected under severe experiment conditions. In this paper, a cyclo-stationarity based neural network is applied to determining the model structure through compounding information indices obtained by the output over-sampling scheme. It is illustrated that several distinct information indices on the cyclo-stationarity are detected from the experimental data. Then, different indices are compounded by a neural network to improve the determination performance. The effectiveness of the proposed approach is demonstrated through an identification experiment on a magnetic levitation system, while the performance of conventional methods degrades largely due to the severe numerical conditions.																	1349-4198	1349-418X				APR	2020	16	2					585	595		10.24507/ijicic.16.02.585													
J								A NEW FALL DETECTION METHOD BASED ON FUZZY REASONING FOR AN OMNI-DIRECTIONAL WALKING TRAINING ROBOT	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Fall detection method; Walking training robot; Fuzzy reasoning; Degree of membership		To regain and then improve the walking ability for the people with lower limb disabled, the authors have been developing a new type of omnidirectional walking training robot (WTR). This WTR can help physical therapist to conduct the walking training for lower limb disabled by performing specific training course designed by the physical therapist. To ensure the training effect, a path tracking controller was proposed to enable the WTR to precisely track the designed path. However, there is the risk of the fall while carrying out walking training by the WTR, which lead the second damage to users. In order to improve the functionality and reliability of the WTR, a fall detection method is proposed based on the fuzzy reasoning method. In detail, a posture sensor and a two-dimensional (2D) laser sensor are used to detect the body's posture and positions of two legs for user in the WTR. Then, a fuzzy knowledge base differentiating the normal walking state and one kind of falling state is set up based on the fusion information of the two sensors. Furthermore, a fall detection strategy based on the combination of upper body posture and gait information is proposed, while the user's fall state is analyzed by a novel fuzzy reasoning mechanism. Finally, the effectiveness of the proposed method is verified by a series of experiments.																	1349-4198	1349-418X				APR	2020	16	2					597	608		10.24507/ijicic.16.02.597													
J								CLASSIFICATION OF HUMAN GAIT ACCELERATION DATA USING CONVOLUTIONAL NEURAL NETWORKS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Time series data; Human motion; Machine learning; Time series imaging		The human motion analysis using wearable sensors such as accelerometers and gyroscopes is one of the important issues in ubiquitous and wearable computing. Inspired by a paper by Akiduki et al. that was released in 2018 concerning the classification of human gait motion accelerometer data, this paper attempts to classify that same data using a convolutional neural network. In the original 2018 paper, a high degree of separation was found between the data of the 13 recorded test subjects, suggesting that classification purely by looking at the motion data is possible. For the purpose of classification using the neural network, the given time series data is converted into three matrices (equivalent to image data with three channels per pixel). Using these images as input for a convolutional neural network, an accuracy of 100% was achieved in classifying the subject number from previously unseen motion data.																	1349-4198	1349-418X				APR	2020	16	2					609	619		10.24507/ijicic.16.02.609													
J								PROCESS SAFETY ENHANCEMENT OF FEEDFORWARD CONTROL USING FOUNDATION FIELDBUS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Failsafe action; Fault diagnosis; Feedforward control; Foundation Fieldbus; Function block; Measurement validity; Safety enhancement		This paper presents a practical technique in engineering phase for enhancing safety of Foundation Fieldbus (FF)-based feedforward control through propagation of measurement validity and status information in the loop using function block language to prevent hazards in the presence of a transmitter failure. The proposed technique is based on fault diagnosis of FF devices to increase the safety beyond that found in basic control loops using traditional technologies. For hybrid architecture by assigning basic and advanced function blocks to execute in H1 field instruments and H1 interface module, respectively, all possible cases for configuring parameter options not only to shut the loop down when the failure occurs but also to resume the loop to normal when the failure disappears are described. The feedforward on temperature control of H1 segment configured and operated on the Delta V integrated host is used for experimentally testing the correctness of the defined parameter options to provide function block interlocks and failsafe actions as well as fault recovery mechanisms. In addition, the Petri net model to represent the control loop behaviors for comparing the process safety enhancement in different scenarios from five cases of parameter configurations is also included.																	1349-4198	1349-418X				APR	2020	16	2					621	630		10.24507/ijicic.16.02.621													
J								ADAPTIVE SLIDING MODE GUIDANCE LAW WITH PRESCRIBED PERFORMANCE FOR INTERCEPTING MANEUVERING TARGET	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Prescribed performance; Error variable function; Sliding mode guidance law; Adaptive control	TRACKING CONTROL; FUNNEL CONTROL; CONTROL SCHEME; IMPACT; TIME; SYSTEMS	This paper describes sliding mode guidance laws with prescribed performance for maneuvering targets. The proposed guidance laws, which combine a novel error variable function with a new sliding mode surface, ensure that the line of sight (LOS) angle converges to the desired value and the LOS tracking error converges to an arbitrarily small residual set faster than some pre-designed rate. By combining the system tracking error with the prescribed performance function, a novel error variable function is designed and a new sliding mode surface is proposed. The key concept in this approach is to transform the prescribed performance constraint problem into that of the boundedness of the error variable function. The boundedness of the novel sliding mode surface ensures that the system state converges according to the prescribed performance. Additionally, this paper discusses the problem whereby the upper bound of the aggregate uncertainty, including the target information, is unavailable. An adaptive guidance law is presented for this scenario. Finally, simulation results are compared with those using other guidance laws. Numerical simulations show that the guidance laws presented in this paper achieve effective performance and can ensure the LOS angle converges to the desired value.																	1349-4198	1349-418X				APR	2020	16	2					631	648		10.24507/ijicic.16.02.631													
J								ADAPTIVE SYNCHRONIZATION OF FRACTIONAL-ORDER CHAOTIC NEURAL NETWORKS WITH UNKNOWN PARAMETERS AND TIME-VARYING DELAYS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Fractional-order chaotic neural networks; Adaptive synchronization; Parameter identification; Lyapunov direct method	PROJECTIVE SYNCHRONIZATION; STABILITY; SYSTEMS	The main concern of this paper is to address the synchronization problem of chaotic fractional-order neural networks through designing the novel adaptive control scheme. The objective of the study is to explore the importance of considering parameters uncertainty and time-varying delays. By combining the adaptive control and linear feedback with update law, a simple, analytical, and rigorous adaptive feedback scheme is derived to achieve synchronization of two coupled neural networks with time-varying delay based on the invariant principle of functional differential equations and parameter identification. Besides, the system parameters in the uncertain network can be identified in the process of synchronization. The simulation results are given to demonstrate the rationality of the theoretical analysis.																	1349-4198	1349-418X				APR	2020	16	2					649	661		10.24507/ijicic.16.02.649													
J								AN SDN-BASED WLAN SYSTEM FOR TERMINAL MOBILITY: DESIGN AND IMPLEMENTATION	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										IEEE 802.11; WLAN; Software-defined network; Fast mobility handoff	NETWORKING	Mobility support is one of the goals of wireless local area networks (WLANs). However, current 802.11-based WLAN structure cannot meet the need for flexible mobility management in high-density deployment. And for delay-sensitive services, mobility handoff in traditional 802.11 protocol will incur unacceptable interruption. In this paper, we propose an SDN-based 802.11 WLAN system and regain the control of handoff from clients in order to achieve faster mobility handoff. On data plane, three modules are designed and implemented working with openvswitch and cfg80211 subsystem in the kernel-space of access point (AP) to include 802.11 devices into SDN domain. On control plane, we implement our controller based on an SDN controller (ONOS), which runs a novel algorithm to make handoff decision. The algorithm takes channel idle evaluation and received signal strength indication (RSSI) into consideration to achieve load balancing. For southbound interface, we use Packet-in and Packet-out messages to carry wireless related information between control and data plane. Moreover, we introduce a unicast channel switch announcement (CSA) frame to speed up terminal's mobility handoff by avoiding handshake overheads in the traditional way. Our proposed system is demonstrated through a real prototype and a test scenario. The experiments results show that terminal mobility handoffs in proposed system are faster than those in the traditional way.																	1349-4198	1349-418X				APR	2020	16	2					663	679		10.24507/ijicic.16.02.663													
J								A NODE DENSITY-BASED APPROACH FOR ENERGY-EFFICIENT DATA GATHERING PROTOCOL IN WIRELESS SENSOR NETWORK ENVIRONMENTS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Energy-efficient data-gathering operations; Network lifetime; Cluster-based protocol		Network lifetime is one of the main issues of wireless sensor network (WSN) development since sensor nodes are usually powered by a battery with limited energy. This characteristic has highlighted the importance of energy-efficient protocol to minimize energy consumption during data-gathering operations in WSN environments. The cluster-based protocol is an effective way to reduce long-distance transmissions and preserve energy by partitioning WSN into some clusters. In this approach, only selected cluster head (CH) nodes will perform data gathering from the cluster node members and transmit the aggregated data to the base station (BS). In this paper, a new approach to build clusters for WSN based on sensor nodes density is proposed. This approach includes the mechanisms to estimate the optimal number of clusters, to select and rotate CH nodes and the algorithm to perform data communications among sensor nodes, CH nodes, and the base station. The proposed approach shows better performance in terms of network lifetime, energy consumption in comparison to the other two well-known WSN communication protocols in data gathering scenarios.																	1349-4198	1349-418X				APR	2020	16	2					681	700		10.24507/ijicic.16.02.681													
J								PVBTS: A NOVEL TASK SCHEDULING ALGORITHM FOR HETEROGENEOUS COMPUTING PLATFORMS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Heterogeneous computing; Task scheduling; Directed acyclic graph; Schedule length; Efficiency	GENETIC ALGORITHM	Efficient task scheduling has always been one of the most critical issues for high performance in heterogeneous computing. The heterogeneity of computation costs on a given set of processors and the communication costs among processors increase the complexity of the scheduling problem. Generally, the application consists of several tasks with dependencies. If the computation costs, task dependencies and communication costs are known a priori, the application can be represented by a static model, namely the directed acyclic graphs (DAG) model. In this paper, we proposed a novel task scheduling algorithm called penalty value based task scheduling (PVBTS) for application scheduling problem. The PVBTS algorithm dynamically determines the execution order of tasks according to the penalty value which is computed based on the heterogeneity of execution completion time on a given set of processors. In each step, the PVBTS algorithm maintains a ready list including all the independent tasks, then selects the task with the highest penalty value and maps it to a processor that gives the minimum execution completion time of the task. The PVBTS algorithm uses randomly generated task graphs and some real-world application task graphs to evaluate performance. The experimental results indicate that the PVBTS algorithm outperforms some well-known scheduling algorithms selected for the performance comparison in terms of schedule length (makespan) and efficiency.																	1349-4198	1349-418X				APR	2020	16	2					701	713		10.24507/ijicic.16.02.701													
J								TWO-DEGREE-OF-FREEDOM PIDA CONTROLLERS DESIGN OPTIMIZATION FOR LIQUID-LEVEL SYSTEM BY USING MODIFIED BAT ALGORITHM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										2DOF-PIDA controllers; Modified bat algorithm; Liquid-level system; Modern optimization		In this paper, an optimal design of the two-degree-of-freedom proportional-integral-derivative-accelerated (2DOF-PIDA) controllers for the liquid-level system based on the modern optimization by using the modified bat algorithm (MBA) is proposed. The MBA is the new modified version of the original bat algorithm (BA) developed from the echolocation behaviour of micro bats. To improve its exploration and exploitation properties, the random number drawn from a Levy-flight distribution and new loudness and pulse emission rate functions are proposed. Performance of the MBA over the BA investigated against 10 benchmark functions is presented in this paper. Results of the 2DOF-PIDA controllers designed by the proposed MBA for the liquid-level system are compared with those of the one-degree-of-freedom proportional-integral-derivative (1DOF-PID) controller designed by Ziegler-Nichols (ZN) tuning rule, 2DOF-PID designed by Araki-Taguchi (AT) tuning rule and 1DOF-PID and 1DOF-PIDA controllers designed by the MBA. As results, it was found that the liquid-level controlled system with the 2DOF-PIDA controllers designed by the MBA can provide very satisfactory responses superior to that with the 2DOF-PID controllers designed by AT and MBA and 1DOF-PID controller designed by ZN, significantly.																	1349-4198	1349-418X				APR	2020	16	2					715	732		10.24507/ijicic.16.02.715													
J								FIXED-TIME DYNAMIC SURFACE CONTROL FOR POWER SYSTEMS WITH STATCOM	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Dynamic surface control; Fixed-time stability; STATCOM; Generator excitation	NONLINEAR COORDINATED CONTROL; GENERATOR EXCITATION; IMMERSION	This paper develops a fixed-time control design for power systems with STATCOM via a dynamic surface control approach. In order to avoid the problem of "explosion of terms" inherent in backstepping algorithm, the proposed dynamic surface control strategy is developed to avoid such a problem. Based on the Lyapunov direct method, the stability of the closed-loop dynamics is proved to ensure that all trajectories of the closed-loop dynamics are semi-globally fixed-timely uniformly ultimately bounded. The effectiveness and superiority of the presented design are verified on a single-machine infinite bus power system. The simulation results exhibit that the presented control can effectively improve dynamic performances, rapidly suppress power system oscillations o f the overall closed-loop dynamics in a fixed time, and has the superior performances over a conventional dynamic surface control approach.																	1349-4198	1349-418X				APR	2020	16	2					733	748		10.24507/ijicic.16.02.733													
J								ADAPTIVE DYNAMIC SURFACE CONTROL FOR HIGHER-ORDER MODELS OF SYNCHRONOUS GENERATORS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Higher-order model; Adaptive dynamic surface control; Generator excitation control; Adaptive control	MULTIMACHINE POWER-SYSTEMS; EXCITATION CONTROL; TRANSIENT STABILIZATION; VOLTAGE REGULATION; OUTPUT-FEEDBACK; DESIGN; STABILITY	An adaptive dynamic surface controller (DSC) scheme for higher-order models of synchronous generators in the presence of unknown parameters is presented in this paper. In spite of the presence of unknown parameters in the system models, the developed control law is used to stabilize the closed-loop system and avoid the problem of "explosion of terms" inherent in backstepping approach. Further, with the help of Lyapunov stability arguments, the proposed control scheme can guarantee that all trajectories of the overall closed-loop dynamics are semi-globally uniformly ultimately bounded. The simulation results exhibit the effectiveness of the proposed control strategy. It can offer excellent dynamic performance, avoid the problem of backstepping complexity explosion, and perform better than a conventional adaptive backstepping control design.																	1349-4198	1349-418X				APR	2020	16	2					749	764		10.24507/ijicic.16.02.749													
J								STABILITY AND INERTIA RESPONSE IMPROVEMENT OF BOOST CONVERTERS INTERLACED WITH CONSTANT POWER LOADS	INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL										Boost converter; Constant power load; Dc microgrid; Dynamic response; Virtual capacitance; Virtual damping; Virtual inertia	VIRTUAL INERTIA; DC MICROGRIDS; STABILIZATION; ENHANCEMENT; IMPEDANCE	In dc microgrids, the distributed sources and loads with different electrical characteristics are typically interconnected to the main bus through power electronic converters. The existence of interfaced converters creates two major problems: 1) the load-side converters and their associated loads, usually considered as constant power loads (CPLs), introduce destabilizing effects into the system; 2) the source-side converters do not possess any inertia or damping properties but reduce the overall inertia of the system. To overcome the stability problems caused by CPLs and low inertia, this paper proposes an active damping strategy based on a linear feedback. The proposed strategy measures the inductor current implemented in source-side boost converter in order to enhance the damping of the dc microgrids with CPLs. The control input-to-output voltage transfer function of a boost converter loaded with a CPL is inherently nonlinear, unstable and a non-minimum phase system that makes its control very difficult. In addition, the synthetic inertia of de bus is enhanced by adding the virtual inertia control to the inner current control loop that is fast enough to emulate inertia and damping coefficient concept. In order to study the stability of de MG with CPLs, a comprehensive small-signal model is derived and then, an acceptable range of inertia response parameters is determined by using the system's root locus analysis. Performance of the proposed control structure is demonstrated through numerical simulations.																	1349-4198	1349-418X				APR	2020	16	2					765	782		10.24507/ijicic.16.02.765													
J								ADSCNet: asymmetric depthwise separable convolution for semantic segmentation in real-time	APPLIED INTELLIGENCE										Semantic segmentation; Dense connection; Real-time; Depthwise separable convolution	NEURAL-NETWORKS; CLASSIFICATION; FEATURES	Semantic segmentation can be considered as a per-pixel localization and classification problem, which gives a meaningful label to each pixel in an input image. Deep convolutional neural networks have made extremely successful in semantic segmentation in recent years. However, some challenges still exist. The first challenge task is that most current networks are complex and it is hard to deploy these models on mobile devices because of the limitation of computational cost and memory. Getting more contextual information from downsampled feature maps is another challenging task. To this end, we propose an asymmetric depthwise separable convolution network (ADSCNet) which is a lightweight neural network for real-time semantic segmentation. To facilitating information propagation, Dense Dilated Convolution Connections (DDCC), which connects a set of dilated convolutional layers in a dense way, is introduced in the network. Pooling operation is inserted before ADSCNet unit to cover more contextual information in prediction. Extensive experimental results validate the superior performance of our proposed method compared with other network architectures. Our approach achieves mean intersection over union (mIOU) of 67.5% on Cityscapes dataset at 76.9 frames per second.																	0924-669X	1573-7497				APR	2020	50	4					1045	1056		10.1007/s10489-019-01587-1													
J								Detecting smoky vehicles from traffic surveillance videos based on dynamic features	APPLIED INTELLIGENCE										Smoky vehicle detection; Integral projection; Local binary patterns; Optical flow; Dynamic features	CONVOLUTIONAL NEURAL-NETWORK; PARTICULATE MATTER; IMAGE; FIRE; ROAD; SEPARATION; FRAMEWORK; EXHAUST; SENSOR; SHAPE	Existing smoky vehicle detection methods are vulnerable to false alarms because of the continuous interferences from common passed vehicles and the complex characteristics of smoke. This paper presents a video smoky vehicle detection method based on dynamic features. Three groups of features, including Multi-Sequence Integral Projection (MS-IP), Center-Symmetric Local Binary Patterns on Three Orthogonal Planes (CSLBP-TOP) and Histograms of Oriented Optical Flow (HOOF), are proposed or employed to characterize dynamic features of successive Region of Interest (ROIs). More specifically, the MS-IP characterizes the diffusion and distribution information based on multiple-sequence analysis and integral projection. The CSLBP-TOP characterizes the spatiotemporal texture information by (1) combining the strengths of Shift-Invariant Feature Transform (SIFT) and LBP and (2) extending the spatial features to three-dimensional (3D) space based on three orthogonal planes (TOP). The HOOF characterizes the motion information by inducing a very characteristic optical flow profile to distinguish smoky objects and non-smoky objects in successive ROIs based on the fact that the smoke is ejected from vehicle exhaust port and then gradually spreads around. The above three groups of features are complementary, and we fuse them to increase algorithm robustness. Experiment results show that our method achieves better performances than existing methods.																	0924-669X	1573-7497				APR	2020	50	4					1057	1072		10.1007/s10489-019-01589-z													
J								Generating high quality crowd density map based on perceptual loss	APPLIED INTELLIGENCE										Crowd density estimation; Convolutional neural network; Perceptual loss; Semantic features		High quality crowd density maps preserve a large amount of spatial information of crowd distribution, which provides significant priori information for the field of crowd behavior analysis and anomaly detection. Recent work on crowd density estimation pays more attention to the accuracy of crowd counting, ignoring the quality of crowd density map estimation. Hence, in this paper, we propose an end-to-end crowd density estimation network to generate high quality crowd density map. The original pixel-level Euclidean distance loss function in the Multi-column Convolutional Neural Network (MCNN) is replaced by the perceptual loss network. By optimizing the perceptual loss function that is defined as the differences between high-level semantic features generated by a pre-trained network, high-quality map estimation can be obtained. At the same time the accuracy of crowd counting and the sensitivity to the external environment can be improved. Extensive experiments conducted on challenging datasets validate the proposed method outperforms the state-of-the-art methods in both the crowd counting accuracy and the density estimation quality.																	0924-669X	1573-7497				APR	2020	50	4					1073	1085		10.1007/s10489-019-01573-7													
J								Quality based adaptive score fusion approach for multimodal biometric system	APPLIED INTELLIGENCE										Image quality; Adaptive score fusion; Multimodal biometric	DECISION-LEVEL FUSION; LIKELIHOOD RATIO; FACE; FINGERPRINT; IRIS; EXTRACTION	Multimodal Biometric Systems are extensively employed over unimodal counterparts for user authentication in the digital world. However, the application of multimodal systems to security-critical applications is limited mainly due to non-adaptiveness of these systems to the dynamic environment and inability to distinguish between spoofing attack and the noisy input image. In order to address these issues, a multimodal biometric system, which adaptively combines the scores from individual classifiers is proposed. For this, three modalities viz. face, finger, and iris are used to extract individual classifier scores. These classifier scores are adaptively fused considering that concurrent modalities are boosted and discordant modalities are suppressed. The conflicting belief among classifiers is resolved not only to achieve optimum fusion of classifier scores but also to cater dynamic environment. The proposed quality based score fusion also distinguish between spoofing attacks and noisy inputs as well. The performance of the proposed multimodal biometric system is experimentally validated using three chimeric multimodal databases. On an average, the proposed system achieves an accuracy of 99.5%, an EER of 0.5% and also outperforms state-of-the-art methods.																	0924-669X	1573-7497				APR	2020	50	4					1086	1099		10.1007/s10489-019-01579-1													
J								Driver action recognition using deformable and dilated faster R-CNN with optimized region proposals	APPLIED INTELLIGENCE										Driver action; Deformable convolution; Dilated convolution; Attention; Region proposal optimization	DRIVING POSTURES; STILL IMAGES; ASSISTANCE; TRANSFORM; FEATURES	Distracted driver action is the main cause of road traffic crashes, which threatens the security of human life and public property. Based on the observation that cues (like the hand holding the cigarette) reveal what the driver is doing, a driver action recognition model is proposed, which is called deformable and dilated Faster R-CNN (DD-RCNN). Our approach utilizes the detection of motion-specific objects to classify driver actions exhibiting great intra-class differences and inter-class similarity. Firstly, deformable and dilated residual block are designed to extract features of action-specific RoIs that are small in size and irregular in shape (such as cigarettes and cell phones). Attention modules are embedded in the modified ResNet to reweight features in channel and spatial dimensions. Then, the region proposal optimization network (RPON) is presented to reduce the number of RoIs entering R-CNN and improves model efficiency. Lastly, the RoI pooling module is replaced with the deformable one, and the simplified R-CNN without regression layer is trained as the final classifier. Experiments show that DD-RCNN demonstrates state-of-the-art results on Kaggle-driving dataset and self-built dataset.																	0924-669X	1573-7497				APR	2020	50	4					1100	1111		10.1007/s10489-019-01603-4													
J								A hybrid fuzzy filtering-fuzzy thresholding technique for region of interest detection in noisy images	APPLIED INTELLIGENCE										Region of interest detection; Gaussian noise; Fuzzy thresholding; Fuzzy filtering; Asymmetrical triangular function	ACTIVE CONTOURS DRIVEN; IMPULSE NOISE; SEGMENTATION; ENERGY; MODEL; SUPERRESOLUTION; ALGORITHM	Noise leads to the ambiguity in regions of interest detection by corrupting the pixel information and is a vital problem in image processing domain. A novel hybrid technique based on fuzzy filtering and fuzzy thresholding is proposed here to extract the object regions accurately in presence of Gaussian noises. The proposed method is automated, does not need any parameter tuning as well does not need prior knowledge of the image or noise. An asymmetrical triangular fuzzy filter with median center coupled with a thresholding based on fuzziness minimization technique are implemented for this purpose. The fuzzy thresholding technique helps to classify the pixels with low signal-to-noise ratio (SNR) caused either due to noise or by the application of noise removal process. The proposed technique is applied in benchmark images corrupted by noises and are compared with some of the popular algorithms of object detection. The results indicate that the proposed method has superior performance in terms of peak signal-to-noise ratio (PSNR) and mean square error (MSE) value for images corrupted with Gaussian noises with standard deviation upto 1.5.																	0924-669X	1573-7497				APR	2020	50	4					1112	1132		10.1007/s10489-019-01551-z													
J								A reference points and intuitionistic fuzzy dominance based particle swarm algorithm for multi/many-objective optimization	APPLIED INTELLIGENCE										Multi; Many-objective optimization; Intuitionistic fuzzy dominance; Double search strategy; Particle swarm optimization; Reference point	MULTIOBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHMS; OPTIMALITY	Intuitionistic Fuzzy Sets is one of the most influential extension and development of Zadeh's fuzzy set theory. It has strong performance in dealing with uncertain information, while taking into account information on membership degree, non-membership degree and hesitation degree. In this paper, a new loose Pareto dominant relationship named intuitionistic fuzzy dominance is adopted to research multi/many-objective particle swarm optimization problems. Particle swarm optimization (PSO) with double search strategy is employed to update the population to enhance the exploitation and exploration capability of particle in the objective space, especially high-dimensional objective space. In addition, the uniformly distributed reference points are used to balance the convergence and diversity of the algorithm. The proposed algorithm has been compared with four recent multi-objective particle swarm optimization algorithms and four state-of-the-art many-objective evolutionary algorithms on 16 benchmark MOPs with 3, 5,8,10 and 15 objectives, respectively. The simulation results show that the proposed algorithm has better performance on most test problems.																	0924-669X	1573-7497				APR	2020	50	4					1133	1154		10.1007/s10489-019-01569-3													
J								Data mining-based approach for ontology matching problem	APPLIED INTELLIGENCE										Semantic web; Ontology matching; Matching instances; Data mining; Feature selection	ALIGNMENT; ALGORITHM	Ontology matching aims at identifying the correspondences between instances and data properties of different ontologies. The use of data mining approach in matching ontology problem is reviewed in this article. We propose DMOM (Data Mining for Ontology Matching based instances) framework to select data properties of instances efficiently. The framework exploits data mining techniques to select the most appropriate features to match ontologies. Moreover, three strategies have been investigated to select the relevant features for the matching process. The first one called exhaustive, explores the enumerate search tree randomly by generating at each iteration a subset of feature attributes, where each node is evaluated by running the matching process on its selected attributes. The second approach called statistical, it uses some statistical values to select the most relevant properties. The third one called FIM (Frequent Itemsets Mining), it explores the correlation between different properties and selects the most frequent properties describing the overall instances of the given ontology. To demonstrate the usefulness of DMOM framework, several experiments have been carried out on OAEI (Ontology Alignment Evaluation Initiative) and DBpedia ontology databases. The results show that the third strategy, FIM, outperforms the two other strategies (Exhaustive, and Statistical). The results also reveal that DMOM outperforms the state-of-the-art ontology matching approaches in terms of execution time and the quality of the matching process.																	0924-669X	1573-7497				APR	2020	50	4					1204	1221		10.1007/s10489-019-01593-3													
J								Semi-supervised dimensionality reduction via sparse locality preserving projection	APPLIED INTELLIGENCE										Semi-supervised learning; Dimensionality reduction; l(p) norm; Pseudo label		The dimensionality reduction of the unbalanced semi-supervised problem is difficult because there are too few labeled samples. In this paper, we propose a new dimensionality reduction method for the unbalanced semi-supervised problem, called sparse locality preserving projection (SLPP for short). In the past work of solving the semi-supervised dimensionality reduction problems, they either abandon some unlabeled samples or do not utilize the implicit discriminant information of unlabeled samples. While, SLPP learns the optimal projection matrix with the full use of the discriminant information and the geometric structure of the unlabeled samples. Here, we preserve the geometric structure of the rest unlabeled samples and their k-nearest neighbors after increasing the number of labeled samples by label propagation. The optimization problem of SLPP can be easily solved by a generalized eigenvalue problem. Results on various data sets from UCI machine learning repository and two hyperspectral data sets demonstrate that SLPP is superior to other conventional reduction methods.																	0924-669X	1573-7497				APR	2020	50	4					1222	1232		10.1007/s10489-019-01574-6													
J								An efficient strategy for using multifactorial optimization to solve the clustered shortest path tree problem	APPLIED INTELLIGENCE										Multifactorial evolutionary algorithm; Clustered shortest-path tree problem; Evolutionary algorithms; Genetic algorithm; Multifactorial optimization	EVOLUTIONARY MULTITASKING; ALGORITHM; NETWORKS	Arising from the need of all time for optimization of irrigation systems, distribution network and cable network, Clustered Shortest-Path Tree Problem (CluSPT) has been attracting a lot of attention and interest from the research community. On the other hand, the Multifactorial Evolutionary Algorithm (MFEA) is one of the most recently exploited realms of Evolutionary Algorithms (EAs) and its performance in solving optimization problems has been very promising. Considering these characteristics, this paper describes a new approach using the MFEA for solving the CluSPT. The MFEA has two tasks: the goal of the first task is to determine the best tree (w.r.t. cost minimization) which envelops all vertices of the CluSPT while the goal of the second task is to find the fittest solution possible for the problem. The purpose of the second task is to find good materials for implicit genetic transfer process in MFEA to improve the quality of CluSPT. To apply this new algorithm, a decoding scheme for deriving individual solutions from the unified representation in the MFEA is also introduced in this paper. Furthermore, evolutionary operators such as population initialization, crossover and mutation operators are also proposed. These operators are applicable for constructing valid solution from both sparse and complete graph. Although the proposed algorithm is slightly complicated for implementation, it can enhance ability to explore and exploit the Unified Search Space (USS). To prove this increment in performance i.e, to assess the effectiveness of the proposed algorithm and methods, the authors implemented them on both Euclidean and Non-Euclidean instances. Experiment results show that the proposed MFEA outperformed existing heuristic algorithms in most of the test cases. The impact of the proposed MFEA was analyzed and a possible influential factor that may be useful for further study was also pointed out.																	0924-669X	1573-7497				APR	2020	50	4					1233	1258		10.1007/s10489-019-01599-x													
J								Belief-peaks clustering based on fuzzy label propagation	APPLIED INTELLIGENCE										Unsupervised learning; Belief functions; Belief peaks; Label propagation; Fuzzy partition	SEMI-SUPERVISED CLASSIFICATION; C-MEANS; ALGORITHM	For unsupervised learning, we propose a new clustering method which incorporates belief peaks into a linear label propagation strategy. The proposed method aims to reveal the data structure by finding out the exact number of clusters and deriving a fuzzy partition. Firstly, the cluster centers and outliers can be identified by the improved belief metric, which makes use of the whole data distribution information so as to correctly highlight the cluster centers without the limitation of massive neighbor points. Secondly, an informative initial fuzzy cluster assignment for each remaining point is created by considering the distances between its neighbors and each cluster center, then the fuzzy label of each point will be iteratively updated by absorbing its neighbors' label information until the fuzzy partition is stable. The label propagation assignment strategy provides a valuable alternative technique with explicit convergence and linear complexity in the field of belief-peaks clustering. The effectiveness of the proposed method is tested on seven commonly used real-world datasets from the UCI Machine Learning Repository, and seven synthetic datasets in the domain of data clustering. Comparing with several state-of-the-art clustering methods, the experiments reveal that the proposed method enhanced the clustering results in terms of the exact numbers of clusters and the Adjusted Rand Index. Further, the parameter analysis experiments validate the robustness to the two tunable parameters in the proposed method.																	0924-669X	1573-7497				APR	2020	50	4					1259	1271		10.1007/s10489-019-01576-4													
J								Feature redundancy term variation for mutual information-based feature selection	APPLIED INTELLIGENCE										Machine learning; Feature selection; Information theory; Feature redundancy	RELEVANCE	Feature selection plays a critical role in many applications that are relevant to machine learning, image processing and gene expression analysis. Traditional feature selection methods intend to maximize feature dependency while minimizing feature redundancy. In previous information-theoretical-based feature selection methods, feature redundancy term is measured by the mutual information between a candidate feature and each already-selected feature or the interaction information among a candidate feature, each already-selected feature and the class. However, the larger values of the traditional feature redundancy term do not indicate the worse a candidate feature because a candidate feature can obtain large redundant information, meanwhile offering large new classification information. To address this issue, we design a new feature redundancy term that considers the relevancy between a candidate feature and the class given each already-selected feature, and a novel feature selection method named min-redundancy and max-dependency (MRMD) is proposed. To verify the effectiveness of our method, MRMD is compared to eight competitive methods on an artificial example and fifteen real-world data sets respectively. The experimental results show that our method achieves the best classification performance with respect to multiple evaluation criteria.																	0924-669X	1573-7497				APR	2020	50	4					1272	1288		10.1007/s10489-019-01597-z													
J								Multi-strategy brain storm optimization algorithm with dynamic parameters adjustment	APPLIED INTELLIGENCE										Brain storm optimization; Multi-strategy; Individual selection rules; Dynamic parameters adjustment	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION ALGORITHM; GLOBAL OPTIMIZATION; ADAPTATION	As a novel swarm intelligence optimization algorithm, brain storm optimization (BSO) has its own unique capabilities in solving optimization problems. However, the performance of traditional BSO strategy in balancing exploitation and exploration is inadequate, which reduces the convergence performance of BSO. To overcome these problems, a multi-strategy BSO with dynamic parameters adjustment (MSBSO) is presented in this paper. In MSBSO, four competitive strategies based on improved individual selection rules are designed to adapt to different search scopes, thus obtaining more diverse and effective individuals. In addition, a simple adaptive parameter that can dynamically regulate search scopes is designed as the basis for selecting strategies. The proposed MSBSO algorithm and other state-of-the-art algorithms are tested on CEC 2013 benchmark functions and CEC 2015 large scale global optimization (LSGO) benchmark functions, and the experimental results prove that the MSBSO algorithm is more competitive than other related algorithms.																	0924-669X	1573-7497				APR	2020	50	4					1289	1315		10.1007/s10489-019-01600-7													
J								Collision avoiding decentralized sorting of robotic swarm	APPLIED INTELLIGENCE										Swarm robotics; Obstacle avoidance; Sorting	NAVIGATION; AVOIDANCE; ALGORITHM	Sorting the swarm of robots is required when the robots are carrying different loads and they can not simply swap the loads. In 2016, Zhou et al. presented an interesting algorithm to sort a swarm of robots, wherein the authors made a main tree and a feedback tree to assign a topology to the robots, based on which the robots moved while arranging themselves in a sorted order. While the approach was very interesting and the results were critically analyzed by the authors, we see a critical problem that the approach did not account for collisions because of which the results can be very different. In this paper, we extend the work of Zhou et al. by enabling the robots to avoid collision by using a geometric approach called as "follow the gap" method. Together both the algorithms allow robot swarm to sort themselves in a straight line while avoiding collision simultaneously.																	0924-669X	1573-7497				APR	2020	50	4					1316	1326		10.1007/s10489-019-01602-5													
J								Unconstrained convex minimization based implicit Lagrangian twin extreme learning machine for classification (ULTELMC)	APPLIED INTELLIGENCE										Extreme learning machine; Unconstrained minimization; Smoothing approaches; Quadratic programming problem; Iterative schemes	SUPPORT VECTOR MACHINE; REGRESSION	The recently proposed twin extreme learning machine (TELM) requires solving two quadratic programming problems (QPPs) in order to find two non-parallel hypersurfaces in the feature that brings in the additional requirement of external optimization toolbox such as MOSEK. In this paper, we propose implicit Lagrangian TELM for classification via unconstrained convex minimization problem (ULTELMC) and further suggest iterative convergent schemes which eliminates the requirement of external optimization toolbox generally required in solving the quadratic programming problems (QPPs) of TELM. The solutions to the dual variables of the proposed ULTELMC are obtained using iterative schemes containing 'plus' function which is not differentiable. To overcome this shortcoming, the generalized derivative approach and smooth approximation approaches are suggested. Further, to test the performance of the proposed approaches, classification performances are compared with support vector machine (SVM), twin support vector machine (TWSVM), extreme learning machine (ELM), twin extreme learning machine (TELM) and Lagrangian extreme learning machine (LELM). Moreover, non-requirement to solve QPPs makes the iterative schemes find the solution faster as compared to the reported methods that finds the solution in dual space. Computational times required in finding the solutions are also presented for comparison.																	0924-669X	1573-7497				APR	2020	50	4					1327	1344		10.1007/s10489-019-01596-0													
J								H-infinity Containment Control of Multiagent Systems Under Event-Triggered Communication Scheduling: The Finite-Horizon Case	IEEE TRANSACTIONS ON CYBERNETICS										Event-triggered protocol; H-infinity containment control; multiagent systems (MASs); state observer	OUTPUT-FEEDBACK; CONSENSUS CONTROL	This paper investigates the finite-horizon H-infinity containment control issue for a general discrete time-varying linear multiagent systems with multileaders. All followers in such a system are driven into a convex hull spanned by multiple leaders, which can be transformed into a problem of tracking a virtual trajectory generated by these leaders. For this purpose, a local state observer is put forward to estimate the state of each agent itself. Then, the estimated state is transmitted to corresponding neighbors governing by an innovation-based event-triggered scheduling protocol. The purpose of the addressed problem is to design both an event-based distributed controller and a state observer such that a prescribed H-infinity containment index can be achieved over a given finite horizon. First, with the help of the completing the square method, a sufficient condition is established to ensure the desired H-infinity containment performance. Then, by resort to a novel nominal energy cost index combined with Moore-Penrose pseudoinverse method, the desired controller and observer parameters are obtained by solving two coupled backward recursive Riccati difference equations. Two positive scalars in proposed nominal energy cost index provide a tradeoff among the controlled tracking errors, the energy of transformed control inputs, and the precision of estimated states. Finally, a simulation example is given to illustrate the usefulness of the proposed theoretical results.																	2168-2267	2168-2275				APR	2020	50	4					1372	1382		10.1109/TCYB.2018.2885567													
J								Neural-Network-Based Adaptive Funnel Control for Servo Mechanisms With Unknown Dead-Zone	IEEE TRANSACTIONS ON CYBERNETICS										Servomotors; Backstepping; Transient analysis; Artificial neural networks; Fuzzy logic; Control design; Adaptive control; funnel function; input dead-zone; neural network (NN); predefined performance; servo mechanisms	DYNAMIC SURFACE CONTROL; OUTPUT-FEEDBACK CONTROL; MIMO NONLINEAR-SYSTEMS; DISCRETE-TIME-SYSTEMS; PRESCRIBED PERFORMANCE; TRACKING CONTROL; MOTION CONTROL; INPUT; OBSERVER	This paper proposes an adaptive funnel control (FC) scheme for servo mechanisms with an unknown dead-zone. To improve the transient and steady-state performance, a modified funnel variable, which relaxes the limitation of the original FC (e.g., systems with relative degree 1 or 2), is developed using the tracking error to replace the scaling factor. Then, by applying the error transformation method, the original error is transformed into a new error variable which is used in the controller design. By using an improved funnel function in a dynamic surface control procedure, an adaptive funnel controller is proposed to guarantee that the output error remains within a predefined funnel boundary. A novel command filter technique is introduced by using the Levant differentiator to eliminate the "explosion of complexity" problem in the conventional backstepping procedure. Neural networks are used to approximate the unknown dead-zone and unknown nonlinear functions. Comparative experiments on a turntable servo mechanism confirm the effectiveness of the devised control method.																	2168-2267	2168-2275				APR	2020	50	4					1383	1394		10.1109/TCYB.2018.2875134													
J								Complementarity in Requirements Tracing	IEEE TRANSACTIONS ON CYBERNETICS										Cybernetics; Training; Global Positioning System; Stakeholders; Environmental factors; Research and development; Maintenance engineering; Association analysis; complementarity; cybernetic enrichment; human-machine interactions; requirements tracing; traceability	THEORETICAL REPLICATION; COEFFICIENT ALPHA; STRATEGY	Complementarity between activities reveals that doing any one of them increases the returns to doing the others. In other words, complementarity leads to the synergistic effect that the whole is greater than the sum of its parts. Identifying and exploiting complementarity can benefit many cybernetic activities, where human-machine interactions are inherent and dominant. One such activity is requirements tracing that helps stakeholders to track the status of their goals. Although various kinds of support for human analysts in requirements tracing have been proposed, little is known about the nature of complementarity when different tracing practices are involved. In this paper, we explore the role of complementarity by considering together the tagging-to-trace (T2T) and learning-to-trace (L2T) activities. We present a novel approach to examining which T2T and L2T practices enhance the qualities of each other. Our approach also uncovers the environmental factors which the complementarity is sensitive to. Applying our approach to the logs of 140 analyst-tracing units offers operational insights into the rigorous detection of complementarity and shows the importance of understanding the cybernetic conditions under which the requirements tracing practices may in fact be complementary.																	2168-2267	2168-2275				APR	2020	50	4					1395	1404		10.1109/TCYB.2018.2889830													
J								Recurrent Broad Learning Systems for Time Series Prediction	IEEE TRANSACTIONS ON CYBERNETICS										Artificial neural networks; Time series analysis; Learning systems; Zinc; Predictive models; Complex systems; Feedforward systems; Broad learning systems (BLSs); prediction; neural networks (NNs); time series	NEURAL-NETWORK; RANDOMIZED ALGORITHMS; MACHINE; MULTISTEP	The broad learning system (BLS) is an emerging approach for effective and efficient modeling of complex systems. The inputs are transferred and placed in the feature nodes, and then sent into the enhancement nodes for nonlinear transformation. The structure of a BLS can be extended in a wide sense. Incremental learning algorithms are designed for fast learning in broad expansion. Based on the typical BLSs, a novel recurrent BLS (RBLS) is proposed in this paper. The nodes in the enhancement units of the BLS are recurrently connected, for the purpose of capturing the dynamic characteristics of a time series. A sparse autoencoder is used to extract the features from the input instead of the randomly initialized weights. In this way, the RBLS retains the merit of fast computing and fits for processing sequential data. Motivated by the idea of "fine-tuning" in deep learning, the weights in the RBLS can be updated by conjugate gradient methods if the prediction errors are large. We exhibit the merits of our proposed model on several chaotic time series. Experimental results substantiate the effectiveness of the RBLS. For chaotic benchmark datasets, the RBLS achieves very small errors, and for the real-world dataset, the performance is satisfactory.																	2168-2267	2168-2275				APR	2020	50	4					1405	1417		10.1109/TCYB.2018.2863020													
J								Incomplete Multiview Spectral Clustering With Adaptive Graph Learning	IEEE TRANSACTIONS ON CYBERNETICS										Co-regularization; graph learning; incomplete multiview clustering; low-rank representation	NONNEGATIVE LOW-RANK	In this paper, we propose a general framework for incomplete multiview clustering. The proposed method is the first work that exploits the graph learning and spectral clustering techniques to learn the common representation for incomplete multiview clustering. First, owing to the good performance of low-rank representation in discovering the intrinsic subspace structure of data, we adopt it to adaptively construct the graph of each view. Second, a spectral constraint is used to achieve the low-dimensional representation of each view based on the spectral clustering. Third, we further introduce a co-regularization term to learn the common representation of samples for all views, and then use the k-means to partition the data into their respective groups. An efficient iterative algorithm is provided to optimize the model. Experimental results conducted on seven incomplete multiview datasets show that the proposed method achieves the best performance in comparison with some state-of-the-art methods, which proves the effectiveness of the proposed method in incomplete multiview clustering.																	2168-2267	2168-2275				APR	2020	50	4					1418	1429		10.1109/TCYB.2018.2884715													
J								Observer Design of Discrete-Time Fuzzy Systems Based on an Alterable Weights Method	IEEE TRANSACTIONS ON CYBERNETICS										Observers; Fuzzy systems; Switches; Nonlinear systems; Computational efficiency; Approximation error; Cybernetics; Alterable weights; fuzzy systems; observer design; state estimation	NONQUADRATIC STABILIZATION CONDITIONS; NONLINEAR-SYSTEMS; STABILITY; MODELS	This paper proposes an improvement on observer design of discrete-time fuzzy systems based on an alterable weights method. Different from the recent result, a more effective ranking-based switching mechanism is developed by introducing a bank of alterable weights for the sake of making use of the size difference information of the normalized fuzzy weighting functions more freely than before. Therefore, a positive result can be provided in this paper, that is, less conservative conditions of designing feasible fuzzy observers can be obtained than those existing results, while the computational cost of designing feasible fuzzy observers is even less than the up-to-date one. Finally, two numerical examples are given to show the progressiveness of the proposed method.																	2168-2267	2168-2275				APR	2020	50	4					1430	1439		10.1109/TCYB.2018.2878419													
J								Controllability of Two-Time-Scale Discrete-Time Multiagent Systems	IEEE TRANSACTIONS ON CYBERNETICS										Controllability; Perturbation methods; Eigenvalues and eigenfunctions; Multi-agent systems; Discrete-time systems; Difference equations; Network topology; Controllability; coordination control; discrete-time; multiagent systems (MASs); singularly perturbed systems; two-timescale	SINGULAR PERTURBATIONS; CONSENSUS; NETWORKS; OBSERVABILITY; REDUCTION; LEADERS	In this paper, the controllability problem is addressed for a two-time-scale discrete-time system with multiple agents. First, the system is described by a singularly perturbed difference equation expressed on fast timescale. Then, to eliminate the singular perturbation parameter, by using the iterative method and approximate approach, the two-time-scale system is separated into slow and fast subsystems. Subsequently, some sufficient and/or necessary conditions of controllability for the systems are derived by using matrix theory. Moreover, under three special network topologies, the necessary criteria for controllability are proposed via graph theory. Finally, we give a simulation example to illustrate the effectiveness of the proposed theoretical results.																	2168-2267	2168-2275				APR	2020	50	4					1440	1449		10.1109/TCYB.2018.2884498													
J								Edge-Based Finite-Time Protocol Analysis With Final Consensus Value and Settling Time Estimations	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Multi-agent systems; Estimation; Eigenvalues and eigenfunctions; Electronic mail; Unmanned aerial vehicles; Symmetric matrices; Edge-based protocol; final consensus value; finite-time consensus; settling time estimation	2ND-ORDER MULTIAGENT SYSTEMS; DYNAMICAL-SYSTEMS; TRACKING CONTROL; LEADER; NETWORKS; AGENTS	The objective of this paper is to design the protocols with a final consensus value and settling time estimations for finite-time consensus of multiagent systems. A couple of new edge-based protocols are developed for multiple second-order nonlinear agents under bounded or Lipschitz-type nonlinear functions, respectively. The final consensus value of the multiagent system is obtained as an average expression. Further, to obtain the estimation of the finite settling time, a special Lyapunov function is constructed. Through the construction processes, both the final consensus value and the settling time are obtained. Finally, as applications, a finite-time formation controller based on the first protocol is designed for multiple mini-spacecraft, verified by simulations.																	2168-2267	2168-2275				APR	2020	50	4					1450	1459		10.1109/TCYB.2018.2872806													
J								Graph Convolutional Network Hashing	IEEE TRANSACTIONS ON CYBERNETICS										Binary codes; Convolutional codes; Semantics; Automatic generation control; Optimization; Training; Graph convolutional network (GCN); hashing; image retrieval; nearest neighbor search	QUANTIZATION	Recently, graph-based hashing that learns similarity-preserving binary codes via an affinity graph has been extensively studied for large-scale image retrieval. However, most graph-based hashing methods resort to intractable binary quadratic programs, making them unscalable to massive data. In this paper, we propose a novel graph convolutional network-based hashing framework, dubbed GCNH, which directly carries out spectral convolution operations on both an image set and an affinity graph built over the set, naturally yielding similarity-preserving binary embedding. GCNH fundamentally differs from conventional graph hashing methods which adopt an affinity graph as the only learning guidance in an objective function to pursue the binary embedding. As the core ingredient of GCNH, we introduce an intuitive asymmetric graph convolutional (AGC) layer to simultaneously convolve the anchor graph, input data, and convolutional filters. By virtue of the AGC layer, GCNH well addresses the issues of scalability and out-of-sample extension when leveraging affinity graphs for hashing. As a use case of our GCNH, we particularly study the semisupervised hashing scenario in this paper. Comprehensive image retrieval evaluations on the CIFAR-10, NUS-WIDE, and ImageNet datasets demonstrate the consistent advantages of GCNH over the state-of-the-art methods given limited labeled data.																	2168-2267	2168-2275				APR	2020	50	4					1460	1472		10.1109/TCYB.2018.2883970													
J								Adversarial Examples for Hamming Space Search	IEEE TRANSACTIONS ON CYBERNETICS										Binary codes; Perturbation methods; Semantics; Quantization (signal); Neural networks; Optimization; Adversarial examples; deep neural network (DNN); hashing; image search	IMAGE; QUANTIZATION	Due to its strong representation learning ability and its facilitation of joint learning for representation and hash codes, deep learning-to-hash has achieved promising results and is becoming increasingly popular for the large-scale approximate nearest neighbor search. However, recent studies highlight the vulnerability of deep image classifiers to adversarial examples; this also introduces profound security concerns for deep retrieval systems. Accordingly, in order to study the robustness of modern deep hashing models to adversarial perturbations, we propose hash adversary generation (HAG), a novel method of crafting adversarial examples for Hamming space search. The main goal of HAG is to generate imperceptibly perturbed examples as queries, whose nearest neighbors from a targeted hashing model are semantically irrelevant to the original queries. Extensive experiments prove that HAG can successfully craft adversarial examples with small perturbations to mislead targeted hashing models. The transferability of these perturbations under a variety of settings is also verified. Moreover, by combining heterogeneous perturbations, we further provide a simple yet effective method of constructing adversarial examples for black-box attacks.																	2168-2267	2168-2275				APR	2020	50	4					1473	1484		10.1109/TCYB.2018.2882908													
J								Event-Triggered Adaptive Control of Saturated Nonlinear Systems With Time-Varying Partial State Constraints	IEEE TRANSACTIONS ON CYBERNETICS										Nonlinear systems; Adaptive control; Time-varying systems; Actuators; System performance; Lyapunov methods; Asymmetric; event-triggered adaptive control; input saturation; partial state constraints	BARRIER LYAPUNOV FUNCTIONS; TRACKING CONTROL; H-INFINITY; STABILIZATION; CONSENSUS	This paper investigates the problem of event-triggered adaptive control for a class of nonlinear systems subject to asymmetric input saturation and time-varying partial state constraints. To facilitate analyzing the influence of asymmetric input saturation, the saturation function is converted into a linear form with respect to control input. To achieve the objective that partial states do not exceed the constraints, a more general form of Lyapunov function is offered. Different from some existing results about output/full state constraints, the proposed scheme only requires that the partial states satisfy the time-varying constraints. Moreover, an event-triggered scheme with a varying threshold is designed to reduce the communication burden. With the time-varying asymmetric barrier Lyapunov functions, a novel event-triggered control scheme is developed, which ensures that partial states are without violation of required constraints and the tracking error converges to a small neighborhood of the origin despite appearing as saturated phenomenon. Eventually, the theoretic results are confirmed by two examples.																	2168-2267	2168-2275				APR	2020	50	4					1485	1497		10.1109/TCYB.2018.2865499													
J								Prior Knowledge-Based Probabilistic Collaborative Representation for Visual Recognition	IEEE TRANSACTIONS ON CYBERNETICS										Training; Visualization; Collaboration; Probabilistic logic; Databases; Face recognition; Correlation; Collaborative representation; prior knowledge; representation-based classifier; visual recognition	FACE RECOGNITION; SPARSE; CLASSIFICATION; SCALE	Collaborative representation is an effective way to design classifiers for many practical applications. In this paper, we propose a novel classifier, called the prior knowledge-based probabilistic collaborative representation-based classifier (PKPCRC), for visual recognition. Compared with existing classifiers which use the collaborative representation strategy, the proposed PKPCRC further includes characteristics of training samples of each class as prior knowledge. Four types of prior knowledge are developed from the perspectives of image distance and representation capacity. They adaptively accommodate the contribution of each class and result in an accurate representation to classify a query sample. Experiments and comparisons on four challenging databases demonstrate that PKPCRC outperforms several state-of-the-art classifiers.																	2168-2267	2168-2275				APR	2020	50	4					1498	1508		10.1109/TCYB.2018.2880290													
J								A Novel Piecewise Affine Filtering Design for T-S Fuzzy Affine Systems Using Past Output Measurements	IEEE TRANSACTIONS ON CYBERNETICS										Memory filtering; piecewise affine (PWA) filters; piecewise fuzzy Lyapunov functions; Takagi-Sugeno (T-S) fuzzy affine systems	FEEDBACK CONTROLLER-DESIGN; DISCRETE-TIME-SYSTEMS; STABILITY ANALYSIS	This paper tackles the problem of piecewise affine memory filtering design for the discrete-time norm-bounded uncertain Takagi-Sugeno fuzzy affine systems. The objective is to design an admissible filter using past output measurements of the system, guaranteeing the asymptotic stability of the filtering error system with a given H-infinity performance index. Based on the piecewise fuzzy Lyapunov functions and the projection lemma, a new sufficient condition for H-infinity filtering performance analysis is first derived, and then the filter synthesis is carried out. It is shown that the filter gains can be obtained by solving a set of linear matrix inequalities. In addition, it is also shown that the filtering performance can be improved with the increasing number of past output measurements used in the filtering design. Finally, two examples are presented to show the advantages and effectiveness of the proposed approach.																	2168-2267	2168-2275				APR	2020	50	4					1509	1518		10.1109/TCYB.2018.2883476													
J								Cooperative Path Following Ring-Networked Under-Actuated Autonomous Surface Vehicles: Algorithms and Experimental Results	IEEE TRANSACTIONS ON CYBERNETICS										Observers; Kinematics; Surges; Topology; Technological innovation; Cybernetics; Closed loop systems; Closed curve; cooperative path following; distributed observer; extended state observer (ESO); line-of-sight; symmetric formation	MODEL-PREDICTIVE CONTROL; ADAPTIVE-CONTROL; CONTAINMENT CONTROL; OUTPUT REGULATION; TRACKING; SYSTEMS; LEADER; VESSELS; CONSENSUS; OBSERVER	This paper addresses the cooperative path following the problem of ring-networked under-actuated autonomous surface vehicles on a closed curve. A cooperative guidance law is proposed at the kinematic level such that a symmetric formation pattern is achieved. Specifically, individual guidance laws of surge speed and angular rate are developed by using a backstepping technique and a line-of-sight guidance method. Then, a coordination design is proposed to update the path variables under a ring-networked topology. The equilibrium point of the closed-loop system has been proven to be globally asymptotically stable. The result is extended to the cooperative path following the lack of sharing of a global reference velocity, and a distributed observer is designed to recover the reference velocity to each vehicle. Moreover, the cooperative path following the presence of an unknown sideslip is considered, and an extended state observer is developed to compensate for the effect of the unknown sideslip. Both simulation and experimental results are provided to illustrate the effectiveness of the proposed cooperative guidance law for the path following over a closed curve.																	2168-2267	2168-2275				APR	2020	50	4					1519	1529		10.1109/TCYB.2018.2883335													
J								Global Fixed-Time Consensus Tracking of Nonlinear Uncertain Multiagent Systems With High-Order Dynamics	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Uncertainty; Observers; Convergence; Stability analysis; Consensus; finite-time control; fixed-time control; multiagent systems; nonlinear system	DELAY SYSTEMS; STABILIZATION; STABILITY; DESIGN	In this paper, we study the consensus tracking problem for high-order nonlinear uncertain multiagent systems. By using the fixed-time control technique and the modified addition of a power integrator method, a novel distributed observer-based consensus protocol is proposed. Compared with the existing results in the literature, the proposed protocol can achieve consensus tracking in a fixed time independent of initial conditions even in the presence of unknown parameters and nonlinear uncertainties bounded by positive functions. Simulation examples are given to illustrate the effectiveness of the theoretical results.																	2168-2267	2168-2275				APR	2020	50	4					1530	1540		10.1109/TCYB.2018.2879892													
J								A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds	IEEE TRANSACTIONS ON CYBERNETICS										Biological neural networks; Insects; Visual systems; Visualization; Photoreceptors; Detectors; Cluttered backgrounds; direction selectivity; natural images; neural modeling; small target motion detection	COLLISION DETECTION; NEURONS; MODEL; ORGANIZATION; RESPONSES; DCMD	Discriminating targets moving against a cluttered background is a huge challenge, let alone detecting a target as small as one or a few pixels and tracking it in flight. In the insect's visual system, a class of specific neurons, called small target motion detectors (STMDs), have been identified as showing exquisite selectivity for small target motion. Some of the STMDs have also demonstrated direction selectivity which means these STMDs respond strongly only to their preferred motion direction. Direction selectivity is an important property of these STMD neurons which could contribute to tracking small targets such as mates in flight. However, little has been done on systematically modeling these directionally selective STMD neurons. In this paper, we propose a directionally selective STMD-based neural network for small target detection in a cluttered background. In the proposed neural network, a new correlation mechanism is introduced for direction selectivity via correlating signals relayed from two pixels. Then, a lateral inhibition mechanism is implemented on the spatial field for size selectivity of the STMD neurons. Finally, a population vector algorithm is used to encode motion direction of small targets. Extensive experiments showed that the proposed neural network not only is in accord with current biological findings, i.e., showing directional preferences but also worked reliably in detecting the small targets against cluttered backgrounds.																	2168-2267	2168-2275				APR	2020	50	4					1541	1555		10.1109/TCYB.2018.2869384													
J								Deep Network Embedding for Graph Representation Learning in Signed Networks	IEEE TRANSACTIONS ON CYBERNETICS										Laplace equations; Prediction algorithms; Clustering algorithms; Eigenvalues and eigenfunctions; Cybernetics; Machine learning; Natural language processing; Deep learning; graph representation learning; network embedding; signed network analysis; structural balance	STRUCTURAL BALANCE	Network embedding has attracted an increasing attention over the past few years. As an effective approach to solve graph mining problems, network embedding aims to learn a low-dimensional feature vector representation for each node of a given network. The vast majority of existing network embedding algorithms, however, are only designed for unsigned networks, and the signed networks containing both positive and negative links, have pretty distinct properties from the unsigned counterpart. In this paper, we propose a deep network embedding model to learn the low-dimensional node vector representations with structural balance preservation for the signed networks. The model employs a semisupervised stacked auto-encoder to reconstruct the adjacency connections of a given signed network. As the adjacency connections are overwhelmingly positive in the real-world signed networks, we impose a larger penalty to make the auto-encoder focus more on reconstructing the scarce negative links than the abundant positive links. In addition, to preserve the structural balance property of signed networks, we design the pairwise constraints to make the positively connected nodes much closer than the negatively connected nodes in the embedding space. Based on the network representations learned by the proposed model, we conduct link sign prediction and community detection in signed networks. Extensive experimental results in real-world datasets demonstrate the superiority of the proposed model over the state-of-the-art network embedding algorithms for graph representation learning in signed networks.																	2168-2267	2168-2275				APR	2020	50	4					1556	1568		10.1109/TCYB.2018.2871503													
J								Automatic Leader-Follower Persistent Formation Generation With Minimum Agent-Movement in Various Switching Topologies	IEEE TRANSACTIONS ON CYBERNETICS										Topology; Switches; Optimization; Shape; Multi-agent systems; Sensors; Control law; downward-tree; leader-follower; multiagent systems (MASs); relation-invariable persistent formation (RIPF); switching topologies	MULTIAGENT SYSTEMS; CONSENSUS; NETWORK	This paper presents the generation strategy, motion planning, and switching topologies of a distance-based leader-follower relation-invariable persistent formation (RIPF) of multiagent systems (MASs). An efficient algorithm is designed to find out if a persistent formation can be generated from a rigid graph. Derived from the properties of a rigid graph, the algorithm to generate RIPF from any initial location is presented. In order to generate different RIPFs in the switching topology, state and transition matrices are introduced. To achieve the minimum agent-movement among RIPFs, a downward-tree combinatorial optimization algorithm is presented. In the end, with the selected minimum agent-movement RIPF, a control law is designed to drive initial RIPF to desired RIPF with given distances among agents. Simulation results show the proposed generation method, control law, and downward-tree are effective to realize the desired formation.																	2168-2267	2168-2275				APR	2020	50	4					1569	1581		10.1109/TCYB.2018.2865803													
J								Edge-Based Fractional-Order Adaptive Strategies for Synchronization of Fractional-Order Coupled Networks With Reaction-Diffusion Terms	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Complex networks; Couplings; Biological neural networks; Adaptive control; Coupled neural network; edge-based adaptive law; fractional-order; reaction-diffusion; synchronization	MITTAG-LEFFLER SYNCHRONIZATION; RECURRENT NEURAL-NETWORKS; TIME-VARYING DELAYS; CLUSTER SYNCHRONIZATION; STABILITY; CHAOS; ARRAY	In this paper, spatial diffusions are introduced to fractional-order coupled networks and the problem of synchronization is investigated for fractional-order coupled neural networks with reaction-diffusion terms. First, a new fractional-order inequality is established based on the Caputo partial fractional derivative. To realize asymptotical synchronization, two types of adaptive coupling weights are considered, namely: 1) coupling weights only related to time and 2) coupling weights dependent on both time and space. For each type of coupling weights, based on local information of the node's dynamics, an edge-based fractional-order adaptive law and an edge-based fractional-order pinning adaptive scheme are proposed. Furthermore, some new analytical tools, including the method of contradiction, L'Hopital rule, and Barbalat lemma are developed to establish adaptive synchronization criteria of the addressed networks. Finally, an example with numerical simulations is provided to illustrate the validity and effectiveness of the theoretical results.																	2168-2267	2168-2275				APR	2020	50	4					1582	1594		10.1109/TCYB.2018.2879935													
J								hPSD: A Hybrid PU-Learning-Based Spammer Detection Model for Product Reviews	IEEE TRANSACTIONS ON CYBERNETICS										Feature extraction; Semisupervised learning; Data models; Motion pictures; Economics; Unsolicited electronic mail; Buildings; Positive and unlabeled dataset learning (PU-learning); semisupervised learning; spammer detection; user-product relations	SALES	Spammers, who manipulate online reviews to promote or suppress products, are flooding in online commerce. To combat this trend, there has been a great deal of research focused on detecting review spammers, most of which design diversified features and thus develop various classifiers. The widespread growth of crowdsourcing platforms has created large-scale deceptive review writers who behave more like normal users, that the way they can more easily evade detection by the classifiers that are purely based on fixed characteristics. In this paper, we propose a hybrid semisupervised learning model titled hybrid PU-learning-based spammer detection (hPSD) for spammer detection to leverage both the users' characteristics and the user-product relations. Specifically, the hPSD model can iteratively detect multitype spammers by injecting different positive samples, and allows the construction of classifiers in a semisupervised hybrid learning framework. Comprehensive experiments on movie dataset with shilling injection confirm the superior performance of hPSD over existing baseline methods. The hPSD is then utilized to detect the hidden spammers from real-life Amazon data. A set of spammers and their underlying employers (e.g., book publishers) are successfully discovered and validated. These demonstrate that hPSD meets the real-world application scenarios and can thus effectively detect the potentially deceptive review writers.																	2168-2267	2168-2275				APR	2020	50	4					1595	1606		10.1109/TCYB.2018.2877161													
J								Finite-Time Stability of Delayed Memristor-Based Fractional-Order Neural Networks	IEEE TRANSACTIONS ON CYBERNETICS										Delay; equilibrium point; finite-time stability; fractional-order; memristor; neural networks	GLOBAL EXPONENTIAL STABILITY; GRONWALL INEQUALITY; VARYING DELAYS; SYNCHRONIZATION; STABILIZATION	This paper studies one type of delayed memristor-based fractional-order neural networks (MFNNs) on the finite-time stability problem. By using the method of iteration, contracting mapping principle, the theory of differential inclusion, and set-valued mapping, a new criterion for the existence and uniqueness of the equilibrium point which is stable in finite time of considered MFNNs is established when the order a satisfies 0 < alpha < 1. Then, when 1 < alpha < 2, on the basis of generalized Gronwall inequality and Laplace transform, a sufficient condition ensuring the considered MFNNs stable in finite time is given. Ultimately, simulation examples are proposed to demonstrate the validity of the results.																	2168-2267	2168-2275				APR	2020	50	4					1607	1616		10.1109/TCYB.2018.2876901													
J								Geometric Structural Ensemble Learning for Imbalanced Problems	IEEE TRANSACTIONS ON CYBERNETICS										Basic classifier; ensemble learning; geometric structure; imbalanced problems; machine learning; relaxation techniques	RANDOM SUBSPACE; MINORITY CLASS; CLASSIFICATION	The classification on imbalanced data sets is a great challenge in machine learning. In this paper, a geometric structural ensemble (GSE) learning framework is proposed to address the issue. It is known that the traditional ensemble methods train and combine a series of basic classifiers according to various weights, which might lack the geometric meaning. Oppositely, the GSE partitions and eliminates redundant majority samples by generating hyper-sphere through the Euclidean metric and learns basic classifiers to enclose the minority samples, which achieves higher efficiency in the training process and seems easier to understand. In detail, the current weak classifier builds boundaries between the majority and the minority samples and removes the former. Then, the remaining samples are used to train the next. When the training process is done, all of the majority samples could be cleaned and the combination of all basic classifiers is obtained. To further improve the generalization, two relaxation techniques are proposed. Theoretically, the computational complexity of GSE could approach O(nd log(n(min)) log(n(maj))). The comprehensive experiments validate both the effectiveness and efficiency of GSE.																	2168-2267	2168-2275				APR	2020	50	4					1617	1629		10.1109/TCYB.2018.2877663													
J								Solving Multiobjective Constrained Trajectory Optimization Problem by an Extended Evolutionary Algorithm	IEEE TRANSACTIONS ON CYBERNETICS										Space vehicles; Trajectory optimization; Optimal control; Vehicle dynamics; Planning; Multiobjective optimal control; multiple-shooting; NSGA-III optimization; Pareto-optimal; trajectory optimization	NONDOMINATED SORTING APPROACH; PARTICLE SWARM OPTIMIZATION; LIBRATION-POINT ORBITS; GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; NSGA-II; COLLOCATION	Highly constrained trajectory optimization problems are usually difficult to solve. Due to some real-world requirements, a typical trajectory optimization model may need to be formulated containing several objectives. Because of the discontinuity or nonlinearity in the vehicle dynamics and mission objectives, it is challenging to generate a compromised trajectory that can satisfy constraints and optimize objectives. To address the multiobjective trajectory planning problem, this paper applies a specific multiple-shooting discretization technique with the newest NSGA-III optimization algorithm and constructs a new evolutionary optimal control solver. In addition, three constraint handling algorithms are incorporated in this evolutionary optimal control framework. The performance of using different constraint handling strategies is detailed and analyzed. The proposed approach is compared with other well-developed multiobjective techniques. Experimental studies demonstrate that the present method can outperform other evolutionary-based solvers investigated in this paper with respect to convergence ability and distribution of the Pareto-optimal solutions. Therefore, the present evolutionary optimal control solver is more attractive and can offer an alternative for optimizing multiobjective continuous-time trajectory optimization problems.																	2168-2267	2168-2275				APR	2020	50	4					1630	1643		10.1109/TCYB.2018.2881190													
J								Adaptive Decentralized Controller Design for a Class of Switched Interconnected Nonlinear Systems	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Actuators; Nonlinear systems; Interconnected systems; Adaptive control; Switched systems; Actuator failures; control directions; decentralized controller; switched parameter update law; switched systems	BARRIER LYAPUNOV FUNCTIONS; DYNAMIC SURFACE CONTROL; TIME-DELAY SYSTEMS; NEURAL-CONTROL; FEEDBACK SYSTEMS; STABILIZATION	This paper is concerned with the switched decentralized adaptive control design problem for switched interconnected nonlinear systems under arbitrary switching, where the actuator failures may occur infinite times and the control directions are allowed to be unknown. By introducing a Nussbaum-type function and an integrable auxiliary signal, a switched decentralized adaptive control scheme is developed to deal with the potentially infinite times of actuator failures and the unknown control directions. The basic idea is to design different parameter update laws and control laws for distinct switched subsystems. It is proved that the state variables of the resulting closed-loop system are asymptotically stable. Finally, a numerical simulation on a double-inverted pendulum model is given to verify the proposed control scheme.																	2168-2267	2168-2275				APR	2020	50	4					1644	1654		10.1109/TCYB.2018.2878578													
J								Multiview Latent Space Learning With Feature Redundancy Minimization	IEEE TRANSACTIONS ON CYBERNETICS										Dictionaries; Redundancy; Correlation; Minimization; Sparse matrices; Machine learning; Data models; Complementary information; Hilbert-Schmidt independence criterion (HSIC); latent space; multiview learning; redundancy minimization	SPARSE-REPRESENTATION; DISCRIMINATIVE DICTIONARY	Multiview learning has received extensive research interest and has demonstrated promising results in recent years. Despite the progress made, there are two significant challenges within multiview learning. First, some of the existing methods directly use original features to reconstruct data points without considering the issue of feature redundancy. Second, existing methods cannot fully exploit the complementary information across multiple views and meanwhile preserve the view-specific properties; therefore, the degraded learning performance will be generated. To address the above issues, we propose a novel multiview latent space learning framework with feature redundancy minimization. We aim to learn a latent space to mitigate the feature redundancy and use the learned representation to reconstruct every original data point. More specifically, we first project the original features from multiple views onto a latent space, and then learn a shared dictionary and view-specific dictionaries to, respectively, exploit the correlations across multiple views as well as preserve the view-specific properties. Furthermore, the Hilbert-Schmidt independence criterion is adopted as a diversity constraint to explore the complementarity of multiview representations, which further ensures the diversity from multiple views and preserves the local structure of the data in each view. Experimental results on six public datasets have demonstrated the effectiveness of our multiview learning approach against other state-of-the-art methods.																	2168-2267	2168-2275				APR	2020	50	4					1655	1668		10.1109/TCYB.2018.2883673													
J								Robot-Assisted Pedestrian Regulation Based on Deep Reinforcement Learning	IEEE TRANSACTIONS ON CYBERNETICS										Robot motion; Feature extraction; Navigation; Planning; Robot sensing systems; Collision avoidance; Deep reinforcement learning (DRL); human-robot interaction (HRI); pedestrian flow regulation	CROWD DYNAMICS; NAVIGATION; BEHAVIOR; OPTIMIZATION; SIMULATION; EVACUATION; NETWORKS; DESIGN	Pedestrian regulation can prevent crowd accidents and improve crowd safety in densely populated areas. Recent studies use mobile robots to regulate pedestrian flows for desired collective motion through the effect of passive human-robot interaction (HRI). This paper formulates a robot motion planning problem for the optimization of two merging pedestrian flows moving through a bottleneck exit. To address the challenge of feature representation of complex human motion dynamics under the effect of HRI, we propose using a deep neural network to model the mapping from the image input of pedestrian environments to the output of robot motion decisions. The robot motion planner is trained end-to-end using a deep reinforcement learning algorithm, which avoids hand-crafted feature detection and extraction, thus improving the learning capability for complex dynamic problems. Our proposed approach is validated in simulated experiments, and its performance is evaluated. The results demonstrate that the robot is able to find optimal motion decisions that maximize the pedestrian outflow in different flow conditions, and the pedestrian-accumulated outflow increases significantly compared to cases without robot regulation and with random robot motion.																	2168-2267	2168-2275				APR	2020	50	4					1669	1682		10.1109/TCYB.2018.2878977													
J								On the Parzen Kernel-Based Probability Density Function Learning Procedures Over Time-Varying Streaming Data With Applications to Pattern Classification	IEEE TRANSACTIONS ON CYBERNETICS										Kernel; Convergence; Neural networks; Bandwidth; Probability density function; Estimation; Probabilistic logic; Concept drift; convergence in probability and with probability one; online density estimation; pattern classification in nonstationary environments; time-varying environment	NEURAL-NETWORKS; DECISION TREES; ENVIRONMENTS; ENSEMBLE	In this paper, we propose a recursive variant of the Parzen kernel density estimator (KDE) to track changes of dynamic density over data streams in a nonstationary environment. In stationary environments, well-established traditional KDE techniques have nice asymptotic properties. Their existing extensions to deal with stream data are mostly based on various heuristic concepts (losing convergence properties). In this paper, we study recursive KDEs, called recursive concept drift tracking KDEs, and prove their weak (in probability) and strong (with probability one) convergence, resulting in perfect tracking properties as the sample size approaches infinity. In three theorems and subsequent examples, we show how to choose the bandwidth and learning rate of a recursive KDE in order to ensure weak and strong convergence. The simulation results illustrate the effectiveness of our algorithm both for density estimation and classification over time-varying stream data.																	2168-2267	2168-2275				APR	2020	50	4					1683	1696		10.1109/TCYB.2018.2877611													
J								Discrete Optimal Graph Clustering	IEEE TRANSACTIONS ON CYBERNETICS										Discrete label learning; information loss; optimal graph; out-of-sample	MATRIX FACTORIZATION; IMAGES	Graph-based clustering is one of the major clustering methods. Most of it works in three separate steps: 1) similarity graph construction; 2) clustering label relaxing; and 3) label discretization with k-means (KM). Such common practice has three disadvantages: 1) the predefined similarity graph is often fixed and may not be optimal for the subsequent clustering; 2) the relaxing process of cluster labels may cause significant information loss; and 3) label discretization may deviate from the real clustering result since KM is sensitive to the initialization of cluster centroids. To tackle these problems, in this paper, we propose an effective discrete optimal graph clustering framework. A structured similarity graph that is theoretically optimal for clustering performance is adaptively learned with a guidance of reasonable rank constraints. Besides, to avoid the information loss, we explicitly enforce a discrete transformation on the intermediate continuous label, which derives a tractable optimization problem with a discrete solution. Furthermore, to compensate for the unreliability of the learned labels and enhance the clustering accuracy, we design an adaptive robust module that learns the prediction function for the unseen data based on the learned discrete cluster labels. Finally, an iterative optimization strategy guaranteed with convergence is developed to directly solve the clustering results. Extensive experiments conducted on both real and synthetic datasets demonstrate the superiority of our proposed methods compared with several state-of-the-art clustering approaches.																	2168-2267	2168-2275				APR	2020	50	4					1697	1710		10.1109/TCYB.2018.2881539													
J								l(0)-Motivated Low-Rank Sparse Subspace Clustering	IEEE TRANSACTIONS ON CYBERNETICS										Alternating direction method of multipliers (ADMMs); generalization of the minimax-concave (GMC) penalty; l(0) regularization; low-rank; sparsity; subspace clustering	NONNEGATIVE MATRIX FACTORIZATION; FACE RECOGNITION; P-NORM; CONVERGENCE; ALGORITHM; REPRESENTATION; SEGMENTATION; SELECTION; REGULARIZATION; MODELS	In many applications, high-dimensional data points can be well represented by low-dimensional subspaces. To identify the subspaces, it is important to capture a global and local structure of the data which is achieved by imposing low-rank and sparseness constraints on the data representation matrix. In low-rank sparse subspace clustering (LRSSC), nuclear and l(1)-norms are used to measure rank and sparsity. However, the use of nuclear and l(1)-norms leads to an overpenalized problem and only approximates the original problem. In this paper, we propose two l(0) quasi-norm-based regularizations. First, this paper presents regularization based on multivariate generalization of minimax-concave penalty (GMC-LRSSC), which contains the global minimizers of a l(0) quasi-norm regularized objective. Afterward, we introduce the Schatten-0 (S-0) and l(0)-regularized objective and approximate the proximal map of the joint solution using a proximal average method (S-0/l(0)-LRSSC). The resulting nonconvex optimization problems are solved using an alternating direction method of multipliers with established convergence conditions of both algorithms. Results obtained on synthetic and four real-world datasets show the effectiveness of GMC-LRSSC and S-0/l(0)-LRSSC when compared to state-of-the-art methods.																	2168-2267	2168-2275				APR	2020	50	4					1711	1725		10.1109/TCYB.2018.2883566													
J								Toward Occlusion Handling in Visual Tracking via Probabilistic Finite State Machines	IEEE TRANSACTIONS ON CYBERNETICS										Target tracking; Correlation; Visualization; Partitioning algorithms; Probabilistic logic; Adaptation models; Support vector machines; Correlation filters; occlusion handling; probabilistic finite state machines; visual tracking	OBJECT TRACKING	Visual tracking has been an active research area in computer vision for decades. However, the performance of existing techniques is still challenged by various factors, such as occlusion and change in appearance of the target. In this paper, we propose a novel framework based on correlation filtering and probabilistic finite state machines (FSMs) to handle occlusion. In our tracking framework, the target is partitioned into several parts whose occlusion states are automatically detected. A set of states for the target is defined in terms of the combination of the parts' occlusion states. The probabilistic FSMs are then used to model the target's state transitions so as to reduce the effect of noise in the output response maps of correlation filters. Our target model's update strategy is adaptable online depending on the estimated state of the target. Extensive experiments have been performed on several public benchmarks and the proposed algorithm achieves competitive results against state-of-the-art techniques.																	2168-2267	2168-2275				APR	2020	50	4					1726	1738		10.1109/TCYB.2018.2884007													
J								Cooperative Fault Diagnosis for Uncertain Nonlinear Multiagent Systems Based on Adaptive Distributed Fuzzy Estimators	IEEE TRANSACTIONS ON CYBERNETICS										Fuzzy logic; Fault diagnosis; Multi-agent systems; Estimation error; Observers; Cooperative fault diagnosis; distributed estimators; multiagent systems (MASs); output feedback; undirected; directed networks	SUFFICIENT CONDITIONS; CONSENSUS; SYNCHRONIZATION; INFORMATION; NETWORK; DESIGN	This paper presents a cooperative fault diagnosis scheme for a class of uncertain nonlinear multiagent systems component and sensor faults in individual agents. Since the faulty system affects the healthy systems through interconnections, for each agent an estimator is designed to collect neighboring output estimations errors to consider its faulty effects on others, when computing its estimations for local state and faulty parameters. A new structure of distributed estimators is proposed by filtering regressor signals and sharing them among agents. Then, the sharings of signals are planned by properly constructing auxiliary graphs for undirected and directed networks. Two conditions are given to preselect estimators parameters for the convergences of the estimation errors. Unlike the existing results dealing with one common parameter with full state measurement and only for undirected graphs, this paper presents an output measurement-based approach for multiple parameters in undirected/directed networks. It shows that for the faults not providing persistent excitation in a signal agent, it is possible to estimate the faults exactly if the they excite all agents persistently. A simulation example of a group of single-link flexible-joint robots is given to verify the effectiveness of the proposed method.																	2168-2267	2168-2275				APR	2020	50	4					1739	1751		10.1109/TCYB.2018.2877101													
J								Consensus Tracking for Heterogeneous Interdependent Group Systems	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Topology; Laplace equations; Redundancy; Consensus tracking; distributed control; heterogeneous group systems; interdependent model	2ND-ORDER MULTIAGENT SYSTEMS; COMMUNICATION; PROTOCOL	This paper is concerned with the consensus tracking problem for heterogeneous interdependent group systems with fixed communication topologies. First, the interdependent model of the heterogeneous system is built from the perspective of the difference of the individual characteristic and the difference of the subgroup topology structure. A class of distributed consensus tracking control protocol is proposed for realizing the consensus tracking of the heterogeneous interdependent group system via using local information. Then, for fixed communication topologies, some corresponding sufficient conditions are given to ensure the achievement of the consensus tracking. Two parameters are defined, which denote, respectively, the proportion of interdependence individual and the redundancy of interdependence. The effects of these parameters are analyzed on the consensus tracking of group systems. Numerical simulations are provided to illustrate the effectiveness of the theoretical analysis.																	2168-2267	2168-2275				APR	2020	50	4					1752	1760		10.1109/TCYB.2018.2874972													
J								Event-Triggered Recursive Filtering for Shift-Varying Linear Repetitive Processes	IEEE TRANSACTIONS ON CYBERNETICS										2-D systems; event-triggered mechanism; linear repetitive processes (LRPs); recursive filtering	ITERATIVE LEARNING CONTROL; KALMAN FILTER; STABILITY ANALYSIS; NONLINEAR-SYSTEMS; STATE; QUANTIZATION; DESIGN	This paper addresses the recursive filtering problem for shift-varying linear repetitive processes (LRPs) with limited network resources. To reduce the resource occupancy, a novel event-triggered strategy is proposed where the concern is to broadcast those necessary measurements to update the innovation information only when certain events appear. The primary goal of this paper is to design a recursive filter rendering that, under the event-triggered communication mechanism, an upper bound (UB) on the filtering error variance is ensured and then optimized by properly determining the filter gains. As a distinct kind of 2-D systems, the LRPs are cast into a general Fornasini-Marchesini model by using the lifting technique. A new definition of the triggering-shift sequence is introduced and an event-triggered rule is then constructed for the transformed system. With the aid of mathematical induction, the filtering error variance is guaranteed to have a UB which is subsequently optimized with appropriate filter parameters via solving two series of Riccati-like difference equations. Theoretical analysis further reveals the monotonicity of the filtering performance with regard to the event-triggering threshold. Finally, an illustrative simulation is given to show the feasibility of the designed filtering scheme.																	2168-2267	2168-2275				APR	2020	50	4					1761	1770		10.1109/TCYB.2018.2881312													
J								Prespecified-Time Cluster Synchronization of Complex Networks via a Smooth Control Approach	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Complex networks; Cybernetics; Protocols; Task analysis; Particle swarm optimization; Computer science; Complex networks; prespecified time; smooth controllers; synchronization	MULTIAGENT SYSTEMS; NEURAL-NETWORKS; CONSENSUS; STABILITY; DESIGN	Most existing finite-/fixed-time synchronization control schemes are nonsmooth or discontinuous, and the settling time is estimated with conservatism. It is due to the utilization of signum function or fraction power state feedback. This brief considers the problem of prespecified-time cluster synchronization of complex networks with a smooth control protocol. The synchronization time is independent of any control parameters or any systems' initial conditions, which is actually uniformly prescribed according to task requirements without any estimations. Moreover, the cluster synchronization can maintain after the specified time, and the smooth control input can always keep uniformly bounded in an infinite time interval as well. Finally, one numerical example is provided to illustrate the effectiveness of the proposed protocol and design method.																	2168-2267	2168-2275				APR	2020	50	4					1771	1775		10.1109/TCYB.2018.2882519													
J								Projected Primal-Dual Dynamics for Distributed Constrained Nonsmooth Convex Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Convex functions; Heuristic algorithms; Linear programming; Convergence; Cybernetics; Indexes; Distributed convex optimization; multiagent networks; nonsmooth analysis; primal-dual dynamics	RECURRENT NEURAL-NETWORK; LIMITING ACTIVATION FUNCTION; ECONOMIC-DISPATCH; ALGORITHMS; SUBJECT; COORDINATION; STABILITY; CONSENSUS; SYSTEMS	A distributed nonsmooth convex optimization problem subject to a general type of constraint, including equality and inequality as well as bounded constraints, is studied in this paper for a multiagent network with a fixed and connected communication topology. To collectively solve such a complex optimization problem, primal-dual dynamics with projection operation are investigated under optimal conditions. For the nonsmooth convex optimization problem, a framework under the LaSalle's invariance principle from nonsmooth analysis is established, where the asymptotic stability of the primal-dual dynamics at an optimal solution is guaranteed. For the case where inequality and bounded constraints are not involved and the objective function is twice differentiable and strongly convex, the globally exponential convergence of the primal-dual dynamics is established. Finally, two simulations are provided to verify and visualize the theoretical results.																	2168-2267	2168-2275				APR	2020	50	4					1776	1782		10.1109/TCYB.2018.2883095													
J								Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network	NEURAL NETWORKS										Automatic summarization; Abstraction; RNN encoder-decoder; Seq2seq; Multiple timescale; Temporal hierarchy; Pointer generator	NEURAL-NETWORKS; BRAIN	In order to tackle the problem of abstractive summarization of long multi-sentence texts, it is critical to construct an efficient model, which can learn and represent multiple compositionalities better. In this paper, we introduce a temporal hierarchical pointer generator network that can represent multiple compositionalities in order to handle longer sequences of texts with a deep structure. We demonstrate how a multilayer gated recurrent neural network organizes itself with the help of an adaptive timescale in order to represent the compositions. The temporal hierarchical network is implemented with a multiple timescale architecture where the timescale of each layer is also learned during the training process through error backpropagation through time. We evaluate our proposed model using an Introduction-Abstract summarization dataset from scientific articles and the CNN/Daily Mail summarization benchmark dataset. The results illustrate that, we successfully implement a summary generation system for long texts by using the multiple timescale with adaptation concept. We also show that we have improved the summary generation system with our proposed model on the benchmark dataset. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						1	11		10.1016/j.neunet.2019.12.022													
J								Cluster stochastic synchronization of complex dynamical networks via fixed-time control scheme	NEURAL NETWORKS										Cluster synchronization; Complex dynamical networks; Stochastic perturbations; Quantization; Fixed-time technique	NEURAL-NETWORKS; LAG SYNCHRONIZATION; ADAPTIVE-CONTROL; STABILIZATION; SYSTEMS; DELAYS	By means of fixed-time (FDT) control technique, cluster stochastic synchronization of complex networks (CNs) is investigated. Quantized controller is designed to realize the synchronization of CNs within a settling time. FDT synchronization criteria are established with the help of Lyapunov functional and comparison system methods. It should be noted that the convergence of synchronization is further improved by comparing with existing FDT synchronization results. Numerical simulations are given to illustrate our results. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						12	19		10.1016/j.neunet.2019.12.019													
J								Adaptive neural tree exploiting expert nodes to classify high-dimensional data	NEURAL NETWORKS										Neural tree; Expert systems; High-dimensional features; Data complexity; Feature clustering	FEATURE-SELECTION; DECISION TREES; RBF NETWORKS; CLASSIFICATION; ALGORITHM; ENSEMBLE; CONVERGENCE; COMPLEXITY; DIAGNOSIS; DESIGN	Classification of high dimensional data suffers from curse of dimensionality and over-fitting. Neural tree is a powerful method which combines a local feature selection and recursive partitioning to solve these problems, but it leads to high depth trees in classifying high dimensional data. On the other hand, if less depth trees are used, the classification accuracy decreases or over-fitting increases. This paper introduces a novel Neural Tree exploiting Expert Nodes (NTEN) to classify high-dimensional data. It is based on a decision tree structure, whose internal nodes are expert nodes performing multidimensional splitting. Any expert node has three decision-making abilities. Firstly, they can select the most eligible neural network with respect to the data complexity. Secondly, they evaluate the over-fitting. Thirdly, they can cluster the features to jointly minimize redundancy and overlapping. To this aim, metaheuristic optimization algorithms including GA, NSGA-II, PSO and ACO are applied. Based on these concepts, any expert node splits a class when the over-fitting is low, and clusters the features when the over-fitting is high. Some theoretical results on NTEN are derived, and experiments on 35 standard data show that NTEN reaches good classification results, reduces tree depth without over-fitting and degrading accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						20	38		10.1016/j.neunet.2019.12.029													
J								Bipartite synchronization for inertia memristor-based neural networks on coopetition networks	NEURAL NETWORKS										Memristive neural networks; Bipartite synchronization; Discontinuous control; Inertia term	SAMPLED-DATA; EXPONENTIAL SYNCHRONIZATION; STABILITY ANALYSIS; SYSTEMS; CONSENSUS; DELAYS	This paper addresses the bipartite synchronization problem of coupled inertia memristor-based neural networks with both cooperative and competitive interactions. Generally, coopetition interaction networks are modeled by a signed graph, and the corresponding Laplacian matrix is different from the nonnegative graph. The coopetition networks with structural balance can reach a final state with identical magnitude but opposite sign, which is called bipartite synchronization. Additionally, an inertia system is a second-order differential system. In this paper, firstly, by using suitable variable substitutions, the inertia memristor-based neural networks (IMNNs) are transformed into the first-order differential equations. Secondly, by designing suitable discontinuous controllers, the bipartite synchronization criteria for IMNNs with or without a leader node on coopetition networks are obtained. Finally, two illustrative examples with simulations are provided to validate the effectiveness of the proposed discontinuous control strategies for achieving bipartite synchronization. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						39	49		10.1016/j.neunet.2019.11.010													
J								Exponential and adaptive synchronization of inertial complex-valued neural networks: A non-reduced order and non-separation approach	NEURAL NETWORKS										Adaptive design; Complex-valued neural network; Exponential synchronization; Inertial model	GENERAL DECAY SYNCHRONIZATION; FIXED-TIME SYNCHRONIZATION; VARYING DELAYS; LEAKAGE DELAY; STABILITY; DYNAMICS; PERIODICITY; CRITERIA	This paper mainly deals with the problem of exponential and adaptive synchronization for a type of inertial complex-valued neural networks via directly constructing Lyapunov functionals without utilizing standard reduced-order transformation for inertial neural systems and common separation approach for complex-valued systems. At first, a complex-valued feedback control scheme is designed and a nontrivial Lyapunov functional, composed of the complex-valued state variables and their derivatives, is proposed to analyze exponential synchronization. Some criteria involving multi-parameters are derived and a feasible method is provided to determine these parameters so as to clearly show how to choose control gains in practice. In addition, an adaptive control strategy in complex domain is developed to adjust control gains and asymptotic synchronization is ensured by applying the method of undeterminated coefficients in the construction of Lyapunov functional and utilizing Barbalat Lemma. Lastly, a numerical example along with simulation results is provided to support the theoretical work. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						50	59		10.1016/j.neunet.2020.01.002													
J								Robust adaptation regularization based on within-class scatter for domain adaptation	NEURAL NETWORKS										Domain adaptation; Robust loss; Within-class scatter; Joint distribution matching; Manifold regularization	FEATURE CO-REGRESSION; SPARSE REPRESENTATION; MULTIPLE SOURCES; KERNEL; FRAMEWORK; ALIGNMENT; SVM	In many practical applications, the assumption that the distributions of the data employed for training and test are identical is rarely valid, which would result in a rapid decline in performance. To address this problem, the domain adaptation strategy has been developed in recent years. In this paper, we propose a novel unsupervised domain adaptation method, referred to as Robust Adaptation Regularization based on Within-Class Scatter (WCS-RAR), to simultaneously optimize the regularized loss, the within-class scatter, the joint distribution between domains, and the manifold consistency. On the one hand, to make the model robust against outliers, we adopt an l(2,1)-norm based loss function in virtue of its row sparsity, instead of the widely-used l(2)-norm based squared loss or hinge loss function to determine the residual. On the other hand, to well preserve the structure knowledge of the source data within the same class and strengthen the discriminant ability of the classifier, we incorporate the minimum within-class scatter into the process of domain adaptation. Lastly, to efficiently solve the resulting optimization problem, we extend the form of the Representer Theorem through the kernel trick, and thus derive an elegant solution for the proposed model. The extensive comparison experiments with the state-of-the-art methods on multiple benchmark data sets demonstrate the superiority of the proposed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						60	74		10.1016/j.neunet.2020.01.009													
J								A 3D deep supervised densely network for small organs of human temporal bone segmentation in CT images	NEURAL NETWORKS										Computed tomography imaging analysis; Temporal bone; Deep supervised densely network	DROPOUT; FCN	Computed Tomography (CT) has become an important way for examining the critical anatomical organs of the human temporal bone in the diagnosis and treatment of ear diseases. Segmentation of the critical anatomical organs is an important fundamental step for the computer assistant analysis of human temporal bone CT images. However, it is challenging to segment sophisticated and small organs. To deal with this issue, a novel 3D Deep Supervised Densely Network (3D-DSD Net) is proposed in this paper. The network adopts a dense connection design and a 3D multi-pooling feature fusion strategy in the encoding stage of the 3D-Unet, and a 3D deep supervised mechanism is employed in the decoding stage. The experimental results show that our method achieved competitive performance in the CT data segmentation task of the small organs in the temporal bone. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						75	85		10.1016/j.neunet.2020.01.005													
J								Multi-context aware user-item embedding for recommendation	NEURAL NETWORKS										Recommender systems; Representation learning; Embedding-based model; Auxiliary information	GRAPH	Real recommender systems usually contain various auxiliary information. Some of the most recent works make meaningful exploration of incorporating auxiliary information into the representation model for competitive recommendation. However, learning user and item representations still faces two challenges: (1) existing works do not well address the problem of integrating multi-type auxiliary information; (2) learning representations for inactive users is still challenging due to the high sparsity of explicit user-item associations. In order to tackle these problems, in this paper, the attributed heterogeneous network and bipartite interaction network are employed to incorporate various auxiliary information and user-item associations. A joint objective function and an efficient algorithm are devised for the representation learning. Experimental results show that the proposed algorithm has significant advantages over the state-of-the-art baselines. What is remarkable is that our proposed method is demonstrated to be especially useful for dealing with low-active users in the system. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						86	94		10.1016/j.neunet.2020.01.008													
J								Differential-game for resource aware approximate optimal control of large-scale nonlinear systems with multiple players	NEURAL NETWORKS										Approximate dynamic programming; Event-driven control; Neural network control; Nonzero sum game; Optimal control	ZERO-SUM GAMES; ALGORITHM	In this paper, we propose a novel differential-game based neural network (NN) control architecture to solve an optimal control problem for a class of large-scale nonlinear systems involving N-players. We focus on optimizing the usage of the computational resources along with the system performance simultaneously. In particular, the N-players' control policies are desired to be designed such that they cooperatively optimize the large-scale system performance, and the sampling intervals for each player are desired to reduce the frequency of feedback execution. To develop a unified design framework that achieves both these objectives, we propose an optimal control problem by integrating both the design requirements, which leads to a multi-player differential-game. A solution to this problem is numerically obtained by solving the associated Hamilton-Jacobi (HJ) equation using event-driven approximate dynamic programming (E-ADP) and artificial NNs online and forward-in-time. We employ the critic neural networks to approximate the solution to the HJ equation, i.e., the optimal value function, with aperiodically available feedback information. Using the NN approximated value function, we design the control policies and the sampling schemes. Finally, the event-driven N-player system is remodeled as a hybrid dynamical system with impulsive weight update rules for analyzing its stability and convergence properties. The closed-loop practical stability of the system and Zeno free behavior of the sampling scheme are demonstrated using the Lyapunov method. Simulation results using a numerical example are also included to substantiate the analytical results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						95	108		10.1016/j.neunet.2019.12.031													
J								Medi-Care AI: Predicting medications from billing codes via robust recurrent neural networks	NEURAL NETWORKS										Billing codes; Robust recurrent neural networks; Health care data; Medication prediction		In this paper, we present an effective deep prediction framework based on robust recurrent neural networks (RNNs) to predict the likely therapeutic classes of medications a patient is taking, given a sequence of diagnostic billing codes in their record. Accurately capturing the list of medications currently taken by a given patient is extremely challenging due to undefined errors and omissions. We present a general robust framework that explicitly models the possible contamination through overtime decay mechanism on the input billing codes and noise injection into the recurrent hidden states, respectively. By doing this, billing codes are reformulated into its temporal patterns with decay rates on each medical variable, and the hidden states of RNNs are regularized by random noises which serve as dropout to improved RNNs robustness towards data variability in terms of missing values and multiple errors. The proposed method is extensively evaluated on real health care data to demonstrate its effectiveness in suggesting medication orders from contaminated values. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						109	116		10.1016/j.neunet.2020.01.001													
J								Attention-guided CNN for image denoising	NEURAL NETWORKS										Image denoising; CNN; Sparse block; Feature enhancement block; Attention block	MINIMIZATION; FRAMEWORK; SPARSE	Deep convolutional neural networks (CNNs) have attracted considerable interest in low-level computer vision. Researches are usually devoted to improving the performance via very deep CNNs. However, as the depth increases, influences of the shallow layers on deep layers are weakened. Inspired by the fact, we propose an attention-guided denoising convolutional neural network (ADNet), mainly including a sparse block (SB), a feature enhancement block (FEB), an attention block (AB) and a reconstruction block (RB) for image denoising. Specifically, the SB makes a tradeoff between performance and efficiency by using dilated and common convolutions to remove the noise. The FEB integrates global and local features information via a long path to enhance the expressive ability of the denoising model. The AB is used to finely extract the noise information hidden in the complex background, which is very effective for complex noisy images, especially real noisy images and bind denoising. Also, the FEB is integrated with the AB to improve the efficiency and reduce the complexity for training a denoising model. Finally, a RB aims to construct the clean image through the obtained noise mapping and the given noisy image. Additionally, comprehensive experiments show that the proposed ADNet performs very well in three tasks (i.e. synthetic and real noisy images, and blind denoising) in terms of both quantitative and qualitative evaluations. The code of ADNet is accessible at https://github.com/hellloxiaotian/ADNet. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						117	129		10.1016/j.neunet.2019.12.024													
J								A causal discovery algorithm based on the prior selection of leaf nodes	NEURAL NETWORKS										Causal discovery; Linear Non-Gaussian Acyclic Models; Leaf nodes; Causal order	ROBUST	In recent years, Linear Non-Gaussian Acyclic Model (LiNGAM) has been widely used for the discovery of causal network. However, solutions based on LiNGAM usually yield high computational complexity as well as unsatisfied accuracy when the data is high-dimensional or the sample size is too small. Such complexity or accuracy problems here are often originated from their prior selection of root nodes when estimating a causal ordering. Thus, a causal discovery algorithm termed as GPL algorithm (the LiNGAM algorithm of Giving Priority to Leaf-nodes) under a mild assumption is proposed in this paper. It assigns priority to leaf nodes other than root nodes. Since leaf nodes do not affect others in a structure, we can directly estimate a causal ordering in a bottom-up way without performing additional operations like data updating process. Corresponding proofs for both feasibility and superiority are offered based on the properties of leaf nodes. Aside from theoretical analyses, practical experiments are conducted on both synthetic and real-world data, which confirm that GPL algorithm outperforms the other two state-of-the-art algorithms in computational complexity and accuracy, especially when dealing with high-dimensional data (up to 200) or small sample size (down to 100 for the dimension of 70). (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						130	145		10.1016/j.neunet.2019.12.020													
J								Adaptive tracking synchronization for coupled reaction-diffusion neural networks with parameter mismatches	NEURAL NETWORKS										Tracking synchronization; Reaction-diffusion neural network; Adaptive control; Parameter mismatch	DIRICHLET BOUNDARY-CONDITIONS; TIME-VARYING DELAYS; EXPONENTIAL SYNCHRONIZATION; GLOBAL SYNCHRONIZATION; IMPULSIVE SYNCHRONIZATION; DYNAMICAL NETWORKS; PINNING CONTROL; PASSIVITY; TERMS; STRATEGIES	In this paper, tracking synchronization for coupled reaction-diffusion neural networks with parameter mismatches is investigated. For such a networked control system, only local neighbor information is used to compensate the mismatch characteristic termed as parameter mismatch, uncertainty or external disturbance. Different from the general boundedness hypothesis, the parameter mismatches are permitted to be unbounded. For the known parameter mismatches, parameter-dependent controller and parameter-independent adaptive controller are respectively designed. While for fully unknown network parameters and parameter mismatches, a distributed adaptive controller is proposed. By means of partial differential equation theories and differential inequality techniques, the tracking synchronization errors driven by these nonlinear controllers are proved to be uniformly ultimately bounded and exponentially convergent to some adjustable bounded domains. Finally, three numerical examples are given to test the effectiveness of the proposed controllers. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						146	157		10.1016/j.neunet.2019.12.025													
J								Performance boost of time-delay reservoir computing by non-resonant clock cycle	NEURAL NETWORKS										Time-delay; Reservoir computing; Clock cycle; Resonance; Memory capacity; Network representation	SYSTEMS	The time-delay-based reservoir computing setup has seen tremendous success in both experiment and simulation. It allows for the construction of large neuromorphic computing systems with only few components. However, until now the interplay of the different timescales has not been investigated thoroughly. In this manuscript, we investigate the effects of a mismatch between the time-delay and the clock cycle for a general model. Typically, these two time scales are considered to be equal. Here we show that the case of equal or resonant time-delay and clock cycle could be actively detrimental and leads to an increase of the approximation error of the reservoir. In particular, we can show that non-resonant ratios of these time scales have maximal memory capacities. We achieve this by translating the periodically driven delay-dynamical system into an equivalent network. Networks that originate from a system with resonant delay-times and clock cycles fail to utilize all of their degrees of freedom, which causes the degradation of their performance. (C) 2020 The Authors. Published by Elsevier Ltd.																	0893-6080	1879-2782				APR	2020	124						158	169		10.1016/j.neunet.2020.01.010													
J								l(2)-l(infinity) state estimation for delayed artificial neural networks under high-rate communication channels with Round-Robin protocol	NEURAL NETWORKS										l(2)-l(infinity) state estimation; Artificial neural networks; High-rate communication channel; Round-Robin protocol; Time-delays	MULTIAGENT SYSTEMS; DESIGN	In this paper, the l(2)-l(infinity) state estimation problem is addressed for a class of delayed artificial neural networks under high-rate communication channels with Round-Robin (RR) protocol. To estimate the state of the artificial neural networks, numerous sensors are deployed to measure the artificial neural networks. The sensors communicate with the remote state estimator through a shared high-rate communication channel. In the high-rate communication channel, the RR protocol is utilized to schedule the transmission sequence of the numerous sensors. The aim of this paper is to design an estimator such that, under the high-rate communication channel and the RR protocol, the exponential stability of the estimation error dynamics as well as the l(2)-l(infinity) performance constraint are ensured. First, sufficient conditions are given which guarantee the existence of the desired l(2)-l(infinity) state estimator. Then, the estimator gains are obtained by solving two sets of matrix inequalities. Finally, numerical examples are provided to verify the effectiveness of the developed l(2)-l(infinity) state estimation scheme. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						170	179		10.1016/j.neunet.2020.01.013													
J								A neurodynamic approach to nonsmooth constrained pseudoconvex optimization problem	NEURAL NETWORKS										Nonsmooth pseudoconvex optimization; Neurodynamic approach; Lyapunov function; Global convergence	RECURRENT NEURAL-NETWORK; LIMITING ACTIVATION FUNCTION; EQUALITY	This paper presents a new neurodynamic approach for solving the constrained pseudoconvex optimization problem based on more general assumptions. The proposed neural network is equipped with a hard comparator function and a piecewise linear function, which make the state solution not only stay in the feasible region, but also converge to an optimal solution of the constrained pseudoconvex optimization problem. Compared with other related existing conclusions, the neurodynamic approach here enjoys global convergence and lower dimension of the solution space. Moreover, the neurodynamic approach does not depend on some additional assumptions, such as the feasible region is bounded, the objective function is lower bounded over the feasible region or the objective function is coercive. Finally, both numerical illustrations and simulation results in support vector regression problem show the well performance and the viability of the proposed neurodynamic approach. (C) 2019 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						180	192		10.1016/j.neunet.2019.12.015													
J								FOM: Fourth-order moment based causal direction identification on the heteroscedastic data	NEURAL NETWORKS										Causal discovery; Causal direction; Heteroscedastic data; Fourth-order moment		Identification of the causal direction is a fundamental problem in many scientific research areas. The independence between the noise and the cause variable is a widely used assumption to identify the causal direction. However, such an independence assumption is usually violated due to heteroscedasticity of the real-world data. In this paper, we propose a new criterion for the causal direction identification which is robust to the heteroscedasticity of the data. In detail, the fourth-order moment of noise is proposed to measure the asymmetry between the cause and effect. A heteroscedastic Gaussian process regression-based estimation of the fourth-order moment is proposed accordingly. Under some commonly used assumptions of the causal mechanism, we theoretically show that the noise's fourth-order moment of the causal direction is smaller than that of the anti-causal direction. Experimental results on both simulated and real-world data illustrate the efficiency of the proposed approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						193	201		10.1016/j.neunet.2020.01.006													
J								EEG based multi-class seizure type classification using convolutional neural network and transfer learning	NEURAL NETWORKS										Convolution neural network; Electroencephalogram; Epilepsy; Seizure type; Support vector machine; Transfer learning	SIGNAL CLASSIFICATION; EPILEPSY DIAGNOSIS; COMPONENT ANALYSIS; ILAE COMMISSION; POSITION PAPER; LOG ENERGY; ENTROPY; RECOGNITION; MODEL	Recognition of epileptic seizure type is essential for the neurosurgeon to understand the cortical connectivity of the brain. Though automated early recognition of seizures from normal electroencephalogram (EEG) was existing, no attempts have been made towards the classification of variants of seizures. Therefore, this study attempts to classify seven variants of seizures with non-seizure EEG through the application of convolutional neural networks (CNN) and transfer learning by making use of the Temple University Hospital EEG corpus. The objective of our study is to perform a multiclass classification of epileptic seizure type, which includes simple partial, complex partial, focal non-specific, generalized non-specific, absence, tonic, and tonic-clonic, and non-seizures. The 19 channels EEG time series was converted into a spectrogram stack before feeding as input to CNN. The following two different modalities were proposed using CNN: (1) Transfer learning using pretrained network, (2) Extract image features using pretrained network and classify using the support vector machine classifier. The following ten pretrained networks were used to identify the optimal network for the proposed study: Alexnet, Vgg16, Vgg19, Squeezenet, Googlenet, Inceptionv3, Densenet201, Resnetl8, Resnet50, and Resnet101. The highest classification accuracy of 82.85% (using Googlenet) and 88.30% (using Inceptionv3) was achieved using transfer learning and extract image features approach respectively. Comparison results showed that CNN based approach outperformed conventional feature and clustering based approaches. It can be concluded that the EEG based classification of seizure type using CNN model could be used in pre-surgical evaluation for treating patients with epilepsy. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						202	212		10.1016/j.neunet.2020.01.017													
J								Directed EEG neural network analysis by LAPPS (p <= 1) Penalized sparse Granger approach	NEURAL NETWORKS										Multivariate Granger Analysis; Outliers; Motor Imagery; Sparse network	BRAIN-COMPUTER INTERFACE; LP NORM SPACE; MOTOR IMAGERY; CONNECTIVITY; CONVERGENCE; PERFORMANCE; ALGORITHMS; EXECUTION; ASYMMETRY; OPERATORS	The conventional multivariate Granger Analysis (GA) of directed interactions has been widely applied in brain network construction based on EEG recordings as well as fMRI. Nevertheless, EEG is usually inevitably contaminated by strong noise, which may cause network distortion due to the L2-norm used in GAs for directed network recovery. The Lp (p <= 1) norm has been shown to be more robust to outliers as compared to LASSO and L2-GAs. Motivated to construct the sparse brain networks under strong noise condition, we hereby introduce a new approach for GA analysis, termed LAPPS (Least Absolute LP (0<p<1) Penalized Solution). LAPPS utilizes the Ll-loss function for the residual error to alleviate the effect of outliers, and another Lp-penalty term (p=0.5) to obtain the sparse connections while suppressing the spurious linkages in the networks. The simulation results reveal that LAPPS obtained the best performance under various noise conditions. In a real EEG data test when subjects performed the left and right hand Motor Imagery (MI) for brain network estimation, LAPPS also obtained a sparse network pattern with the hub at the contralateral brain primary motor areas consistent with the physiological basis of MI. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						213	222		10.1016/j.neunet.2020.01.022													
J								Person Re-Identification with Feature Pyramid Optimization and Gradual Background Suppression	NEURAL NETWORKS										Person re-identification; End-to-end; Feature pyramid optimization; Gradual Background Suppression		Compared with face recognition, the performance of person re-identification (re-ID) is still far from practical application. Among various interferences, there are two factors seriously limiting the performance improvement, i.e., the feature discriminability determined by "external network effectiveness", and the image quality determined by "internal background clutters". Target at the "external network effectiveness" problem, feature pyramids are effective to learn discriminative features because they can learn both detailed features from high-resolution shallow layers and semantical features from low-resolution deep layers, however, it can only achieve slight improvement on re-ID tasks because of the error back propagation problem. To handle the problem and utilize the effectiveness of feature pyramids, we propose a strategy called Feature Pyramid Optimization (FPO). Instead of concatenating features directly, the selected layers are optimized independently in a top-bottom order. Target at the "internal background clutters" problem, background suppression is generally considered for removing the environmental interference and improving the image quality. Several mask-based methods are used attempting to totally remove background clutters but achieve limited promotion because of the mask sharpening effect. We propose a novel strategy, i.e., Gradual Background Suppression (GBS) to reduce the background clutters and keep the smoothness of images simultaneously. Extensive experiments have been conducted and the results demonstrate the effectiveness of both FPO and GBS. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						223	232		10.1016/j.neunet.2020.01.012													
J								Adaptive complex-valued stepsize based fast learning of complex-valued neural networks	NEURAL NETWORKS										Adaptive complex-valued stepsize; Complex-valued neural networks; Fast learning; Rotation factors; Scaling factors; Saddle points	BACKPROPAGATION ALGORITHM; HIERARCHICAL STRUCTURES; CONVERGENCE ANALYSIS; LOCAL MINIMA; SIZE	Complex-valued gradient descent algorithm is a popular tool to optimize functions of complex variables, especially for the training of complex-valued neural networks. However, the choice of suitable learning stepsize is a challenging task during the training process. In this paper, an adaptive complex-valued stepsize design method is proposed for complex-valued neural networks by generalizing the adaptable learning rate tree technique to the complex domain. The scaling and rotation factors are introduced to simultaneously adjust the amplitude and phase of complex-valued stepsize. The search range is thus expanded from half line to half plane such that better search direction is obtained at each iteration. We analyze the dynamics of the algorithm near a saddle point and find that it is very easy to escape from the saddle point to guarantee fast convergence and high accuracy. Some experimental results on function approximation and pattern classification tasks are presented to illustrate the advantages of the proposed algorithm over some previous ones. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						233	242		10.1016/j.neunet.2020.01.011													
J								Deep feature transfer learning for trusted and automated malware signature generation in private cloud environments	NEURAL NETWORKS										Deep learning; Transfer learning; Convolutional neural networks; Malware detection; Cryptojacking; Automatic signature generation	METHODOLOGY	This paper presents TrustSign, a novel, trusted automatic malware signature generation method based on high-level deep features transferred from a VGG-19 neural network model pretrained on the ImageNet dataset. While traditional automatic malware signature generation techniques rely on static or dynamic analysis of the malware's executable, our method overcomes the limitations associated with these techniques by producing signatures based on the presence of the malicious process in the volatile memory. By leveraging the cloud's virtualization technology, TrustSign analyzes the malicious process in a trusted manner, since the malware is unaware and cannot interfere with the inspection procedure. Additionally, by removing the dependency on the malware's executable, our method is fully capable of signing fileless malware as well. TrustSign's signature generation process does not require feature engineering or any additional model training, and it is done in a completely unsupervised manner, eliminating the need for a human expert. Because of this, our method has the advantage of dramatically reducing signature generation and distribution time. In fact, in this paper we rethink the typical use of deep convolutional neural networks and use the VGG-19 model as a topological feature extractor for a vastly different task from the one it was trained for. The results of our experimental evaluation demonstrate TrustSign's ability to generate signatures impervious to the process state over time. By using the signatures generated by TrustSign as input for various supervised classifiers, we achieved up to 99.5% classification accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						243	257		10.1016/j.neunet.2020.01.003													
J								Deterministic learning of hybrid Fuzzy Cognitive Maps and network reduction approaches	NEURAL NETWORKS										Fuzzy cognitive maps; Hybrid models; Inverse learning; Interpretability	OPTIMIZATION; CONVERGENCE	Hybrid artificial intelligence deals with the construction of intelligent systems by relying on both human knowledge and historical data records. In this paper, we approach this problem from a neural perspective, particularly when modeling and simulating dynamic systems. Firstly, we propose a Fuzzy Cognitive Map architecture in which experts are requested to define the interaction among the input neurons. As a second contribution, we introduce a fast and deterministic learning rule to compute the weights among input and output neurons. This parameterless learning method is based on the Moore-Penrose inverse and it can be performed in a single step. In addition, we discuss a model to determine the relevance of weights, which allows us to better understand the system. Last but not least, we introduce two calibration methods to adjust the model after the removal of potentially superfluous weights. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						258	268		10.1016/j.neunet.2020.01.019													
J								Learning deformable registration of medical images with anatomical constraints	NEURAL NETWORKS										Medical image registration; Convolutional neural networks; X-ray image analysis	CHEST RADIOGRAPHS; SEGMENTATION; MODEL	Deformable image registration is a fundamental problem in the field of medical image analysis. During the last years, we have witnessed the advent of deep learning-based image registration methods which achieve state-of-the-art performance, and drastically reduce the required computational time. However, little work has been done regarding how can we encourage our models to produce not only accurate, but also anatomically plausible results, which is still an open question in the field. In this work, we argue that incorporating anatomical priors in the form of global constraints into the learning process of these models, will further improve their performance and boost the realism of the warped images after registration. We learn global non-linear representations of image anatomy using segmentation masks, and employ them to constraint the registration process. The proposed AC-RegNet architecture is evaluated in the context of chest X-ray image registration using three different datasets, where the high anatomical variability makes the task extremely challenging. Our experiments show that the proposed anatomically constrained registration model produces more realistic and accurate results than state-of-the-art methods, demonstrating the potential of this approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						269	279		10.1016/j.neunet.2020.01.023													
J								Improved value iteration for neural-network-based stochastic optimal control design	NEURAL NETWORKS										Adaptive critic designs; Adaptive dynamic programming; Neural networks; Optimal control; Stochastic processes; Value iteration	ALGORITHMS	In this paper, a novel value iteration adaptive dynamic programming (ADP) algorithm is presented, which is called an improved value iteration ADP algorithm, to obtain the optimal policy for discrete stochastic processes. In the improved value iteration ADP algorithm, for the first time we propose a new criteria to verify whether the obtained policy is stable or not for stochastic processes. By analyzing the convergence properties of the proposed algorithm, it is shown that the iterative value functions can converge to the optimum. In addition, our algorithm allows the initial value function to be an arbitrary positive semi-definite function. Finally, two simulation examples are presented to validate the effectiveness of the developed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						280	295		10.1016/j.neunet.2020.01.004													
J								K-Anonymity inspired adversarial attack and multiple one-class classification defense	NEURAL NETWORKS										K-Anonymity; Adversarial defense; Adversarial attack; Deep SVDD; Kernel learning	IMAGE	A novel adversarial attack methodology for fooling deep neural network classifiers in image classification tasks is proposed, along with a novel defense mechanism to counter such attacks. Two concepts are introduced, namely the K-Anonymity-inspired Adversarial Attack (K-A(3)) and the Multiple Support Vector Data Description Defense (M-SVDD-D). The proposed K-A(3) introduces novel optimization criteria to standard adversarial attack methodologies, inspired by the K-Anonymity principles. Its generated adversarial examples are not only misclassified by the neural network classifier, but are uniformly spread along K different ranked output positions. The proposed M-SVDD-D consists of a deep neural architecture layer consisting of multiple non-linear one-class classifiers based on Support Vector Data Description that can be used to replace the final linear classification layer of a deep neural architecture, and an additional class verification mechanism. Its application decreases the effectiveness of adversarial attacks, by increasing the noise energy required to deceive the protected model, attributed to the introduced non-linearity. In addition, M-SVDD-D can be used to prevent adversarial attacks in black-box attack settings. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						296	307		10.1016/j.neunet.2020.01.015													
J								Effective metric learning with co-occurrence embedding for collaborative recommendations	NEURAL NETWORKS										Recommender systems; Metric learning; Cooccurrence-based embedding; Regularization; Top-n recommendations	MATRIX FACTORIZATION	In recommender systems, matrix factorization and its variants have grown up to be dominant in collaborative filtering due to their simplicity and effectiveness. In matrix factorization based methods, dot product which is actually used as a measure of distance from users to items, does not satisfy the inequality property, and thus may fail to capture the inner grained preference information and further limits the performance of recommendations. Metric learning produces distance functions that capture the essential relationships among rating data and has been successfully explored in collaborative recommendations. However, without the global statistical information of user-user pairs and item-item pairs, it makes the model easy to achieve a suboptimal metric. For this, we present a cooccurrence embedding regularized metric learning model (CRML) for collaborative recommendations. We consider the optimization problem as a multi-task learning problem which includes optimizing a primary task of metric learning and two auxiliary tasks of representation learning. In particular, we develop an effective approach for learning the embedding representations of both users and items, and then exploit the strategy of soft parameter sharing to optimize the model parameters. Empirical experiments on four datasets demonstrate that the CRML model can enhance the naive metric learning model and significantly outperforms the state-of-the-art methods in terms of accuracy of collaborative recommendations. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						308	318		10.1016/j.neunet.2020.01.021													
J								Theory of deep convolutional neural networks: Downsampling	NEURAL NETWORKS										Deep learning; Convolutional neural networks; Approximation theory; Downsampling; Filter masks	MULTILAYER FEEDFORWARD NETWORKS; APPROXIMATION; REGRESSION; ALGORITHM; BOUNDS	Establishing a solid theoretical foundation for structured deep neural networks is greatly desired due to the successful applications of deep learning in various practical domains. This paper aims at an approximation theory of deep convolutional neural networks whose structures are induced by convolutions. To overcome the difficulty in theoretical analysis of the networks with linearly increasing widths arising from convolutions, we introduce a downsampling operator to reduce the widths. We prove that the downsampled deep convolutional neural networks can be used to approximate ridge functions nicely, which hints some advantages of these structured networks in terms of approximation or modeling. We also prove that the output of any multi-layer fully-connected neural network can be realized by that of a downsampled deep convolutional neural network with free parameters of the same order, which shows that in general, the approximation ability of deep convolutional neural networks is at least as good as that of fully-connected networks. Finally, a theorem for approximating functions on Riemannian manifolds is presented, which demonstrates that deep convolutional neural networks can be used to learn manifold features of data. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						319	327		10.1016/j.neunet.2020.01.018													
J								A model for navigation in unknown environments based on a reservoir of hippocampal sequences	NEURAL NETWORKS										Hippocampus; Sequences; Reinforcement learning; Replay; Theta oscillations	PLACE-CELL SEQUENCES; PATH-INTEGRATION; PHASE PRECESSION; THETA SEQUENCES; COGNITIVE MAPS; MEMORY; NETWORK; REPLAY; EXPERIENCE; DYNAMICS	Hippocampal place cell populations are activated in sequences on multiple time scales during active behavior, resting and sleep states, suggesting that these sequences are the genuine dynamical motifs of the hippocampal circuit. Recently, prewired hippocampal place cell sequences have even been reported to correlate to future behaviors, but so far there is no explanation of what could be the computational benefits of such a mapping between intrinsic dynamical structure and external sensory inputs. Here, I propose a computational model in which a set of predefined internal sequences is used as a dynamical reservoir to construct a spatial map of a large unknown maze based on only a small number of salient landmarks. The model is based on a new variant of temporal difference learning and implements a simultaneous localization and mapping algorithm. As a result sequences during intermittent replay periods can be decoded as spatial trajectories and improve navigation performance, which supports the functional interpretation of replay to consolidate memories of motor actions. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						328	342		10.1016/j.neunet.2020.01.014													
J								A unified model of rule-set learning and selection	NEURAL NETWORKS										Connectionist model; Rule learning; Task-switching; Prefrontal cortex; Basal ganglia	COGNITIVE CONTROL; PREFRONTAL CORTEX; FRONTAL-CORTEX; NEURAL MECHANISMS; BASAL GANGLIA; ORGANIZATION; DISCOVERY; REWARD; SWITCH; COSTS	The ability to focus on relevant information and ignore irrelevant information is a fundamental part of intelligent behavior. It not only allows faster acquisition of new tasks by reducing the size of the problem space but also allows for generalizations to novel stimuli. Task-switching, task-sets, and rule-set learning are all intertwined with this ability. There are many models that attempt to individually describe these cognitive abilities. However, there are few models that try to capture the breadth of these topics in a unified model and fewer still that do it while adhering to the biological constraints imposed by the findings from the field of neuroscience. Presented here is a comprehensive model of rule-set learning and selection that can capture the learning curve results, error-type data, and transfer effects found in rule-learning studies while also replicating the reaction time data and various related effects of task-set and task-switching experiments. The model also factors in many disparate neurological findings, several of which are often disregarded by similar models. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						343	356		10.1016/j.neunet.2020.01.028													
J								A deep CNN approach to decode motor preparation of upper limbs from time-frequency maps of EEG signals at source level	NEURAL NETWORKS										Brain computer interface; Electroencephalography; Deep learning; Convolutional neural network; Time-frequency analysis; Beamforming	NEURAL-NETWORKS; BRAIN; CLASSIFICATION; IMAGERY; INTERFACES	A system that can detect the intention to move and decode the planned movement could help all those subjects that can plan motion but are unable to implement it. In this paper, motor planning activity is investigated by using electroencephalographic (EEG) signals with the aim to decode motor preparation phases. A publicly available database of 61-channels EEG signals recorded from 15 healthy subjects during the execution of different movements (elbow flexion/extension, forearm pronation/supination, hand open/close) of the right upper limb was employed to generate a dataset of EEG epochs preceding resting and movement's onset. A novel system is introduced for the classification of premovement vs resting and of premovement vs premovement epochs. For every epoch, the proposed system generates a time-frequency (TF) map of every source signal in the motor cortex, through beamforming and Continuous Wavelet Transform (CWT), then all the maps are embedded in a volume and used as input to a deep CNN. The proposed system succeeded in discriminating premovement from resting with an average accuracy of 90.3% (min 74.6%, max 100%), outperforming comparable methods in the literature, and in discriminating premovement vs premovement with an average accuracy of 62.47%. The achieved results encourage to investigate motor planning at source level in the time-frequency domain through deep learning approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						357	372		10.1016/j.neunet.2020.01.027													
J								The feature extraction of resting-state EEG signal from amnestic mild cognitive impairment with type 2 diabetes mellitus based on feature-fusion multispectral image method	NEURAL NETWORKS										Feature-fusion multispectral image; aMCI with T2DM; EEG signal; Convolutional neural network	ALZHEIMERS-DISEASE; DEMENTIA; PROGRESSION; RHYTHMS; TIME	Recently, combining feature extraction and classification method of electroencephalogram (EEG) signals has been widely used in identifying mild cognitive impairment. However, it remains unclear which feature of EEG signals is best effective in assessing amnestic mild cognitive impairment (aMCI) with type 2 diabetes mellitus (T2DM) when combining one classifier. This study proposed a novel feature extraction method of EEG signals named feature-fusion multispectral image method (FMIM) for diagnosis of aMCI with T2DM. The FMIM was integrated with convolutional neural network (CNN) to classify the processed multispectral image data. The results showed that FMIM could effectively identify aMCI with T2DM from the control group compared to existing multispectral image method (MIM), with improvements including the type and quantity of feature extraction. Meanwhile, part of the invalid calculation could be avoided during the classification process. In addition, the classification evaluation indexes were best under the combination of Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-1, and were also best under the combination of the Theta-Alphal-Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-2. Therefore, FMIM can be used as an effective feature extraction method of aMCI with T2DM, and as a valuable biomarker in clinical applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						373	382		10.1016/j.neunet.2020.01.025													
J								Universal approximation with quadratic deep networks	NEURAL NETWORKS										Deep learning; Quadratic networks; Approximation theory	COMPLEXITY; SHALLOW	Recently, deep learning has achieved huge successes in many important applications. In our previous studies, we proposed quadratic/second-order neurons and deep quadratic neural networks. In a quadratic neuron, the inner product of a vector of data and the corresponding weights in a conventional neuron is replaced with a quadratic function. The resultant quadratic neuron enjoys an enhanced expressive capability over the conventional neuron. However, how quadratic neurons improve the expressing capability of a deep quadratic network has not been studied up to now, preferably in relation to that of a conventional neural network. Specifically, we ask four basic questions in this paper: (1) for the one-hidden-layer network structure, is there any function that a quadratic network can approximate much more efficiently than a conventional network? (2) for the same multi-layer network structure, is there any function that can be expressed by a quadratic network but cannot be expressed with conventional neurons in the same structure? (3) Does a quadratic network give a new insight into universal approximation? (4) To approximate the same class of functions with the same error bound, could a quantized quadratic network have a lower number of weights than a quantized conventional network? Our main contributions are the four interconnected theorems shedding light upon these four questions and demonstrating the merits of a quadratic network in terms of expressive efficiency, unique capability, compact architecture and computational capacity respectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				APR	2020	124						383	392		10.1016/j.neunet.2020.01.007													
J								Combination of loss functions for deep text classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Loss Function; Convolutional neural network (CNN); Ensemble method; Multi-class classifier	ENSEMBLE; CORRENTROPY	Ensemble methods have shown to improve the results of statistical classifiers by combining multiple single learners into a strong one. In this paper, we explore the use of ensemble methods at the level of the objective function of a deep neural network. We propose a novel objective function that is a linear combination of single losses and integrate the proposed objective function into a deep neural network. By doing so, the weights associated with the linear combination of losses are learned by back propagation during the training stage. We study the impact of such an ensemble loss function on the state-of-the-art convolutional neural networks for text classification. We show the effectiveness of our approach through comprehensive experiments on text classification. The experimental results demonstrate a significant improvement compared with the conventional state-of-the-art methods in the literature.																	1868-8071	1868-808X				APR	2020	11	4					751	761		10.1007/s13042-019-00982-x													
J								A deep neural networks based recommendation algorithm using user and item basic data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Recommendation algorithm; Deep neural networks; Side data; Collaborative filtering; Sparsity problem; Cold-start problem	ONLINE REVIEWS; COLD START; TRUST; PREFERENCES; SIMILARITY; SYSTEMS	User basic data (e.g. user gender, user age and user ID, etc.) and item basic data (e.g. item name, item category, etc.) are important side data that can be used to enhance the performance of recommendation algorithms, whereas attempts concerning this issue are still relatively scarce. In this study, a deep neural networks based recommendation algorithm is proposed where user average rating, user basic data (user gender, user age, user occupation, user ID), item basic data (item name, item category, item ID) and item average rating are used. The main idea of the algorithm is to build a regression model for predicting user ratings based on deep neural networks. For this, according to the user data (user average rating and user basic data) and the item data (items basic data and item average rating), a user feature matrix and an item feature matrix are respectively constructed using the four types of neural network layers [i.e., embedding layer (EL), convolution layer (CL), pooling layer (PL) and fully connected layer (FCL)]. Then, based on the obtained user feature matrix and item feature matrix, a user-item feature matrix is further constructed using a FCL. On this basis, a regression model for predicting user ratings can be trained, and a recommendation list can be generated according to the predicted user ratings. To verify the effectiveness of the proposed algorithm, three experiments are conducted using the real data from the MovieLens website. The results of experiments show that the proposed algorithm not only outperforms the state-of-the-art collaborative filtering (CF) recommendation algorithms but also alleviates the data sparsity problem and cold-start problem that would occur when the state-of-the-art CF recommendation algorithms are used.																	1868-8071	1868-808X				APR	2020	11	4					763	777		10.1007/s13042-019-00981-y													
J								Cross-modal learning for material perception using deep extreme learning machine	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Cross-modal matching; Material perception; Correlation learning; Deep extreme learning machine	CANONICAL CORRELATION-ANALYSIS; TACTILE PERCEPTION; REPRESENTATIONS; RECOGNITION; NETWORK; FUSION	The material property of an object's surface is critical for the tasks of robotic manipulation or interaction with its surrounding environment. Tactile sensing can provide rich information about the material characteristics of an object's surface. Hence, it is important to convey and interpret tactile information of material properties to the users during interaction. In this paper, we propose a visual-tactile cross-modal retrieval framework to convey tactile information of surface material for perceptual estimation. In particular, we use tactile information of a new unknown surface material to retrieve perceptually similar surface from an available surface visual sample set. For the proposed framework, we develop a deep cross-modal correlation learning method, which incorporates the high-level nonlinear representation of deep extreme learning machine and class-paired correlation learning of cluster canonical correlation analysis. Experimental results on the publicly available dataset validate the effectiveness of the proposed framework and the method.																	1868-8071	1868-808X				APR	2020	11	4					813	823		10.1007/s13042-019-00962-1													
J								DeepCascade-WR: a cascading deep architecture based on weak results for time series prediction	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Time series prediction (TSP); Deep learning (DL); Extreme learning machine (ELM); Online sequential extreme learning machine (OS-ELM); Weak results	NEURAL-NETWORKS; COMPETITION; ALGORITHM	Noisy and nonstationary real-world time series predictions (TSPs) are challenging tasks. Confronted with these challenging tasks, the predictive power of traditional shallow models is commonly not satisfactory enough. While the research on deep learning (DL) has made milestone breakthrough in recent years, and DL paradigm has gradually become indispensable for accomplishing these complex tasks. In this work, a cascading deep architecture based on weak results (DeepCascade-WR) is established, which possesses deep models' marked capability of feature representation learning based on complex data. In DeepCascade-WR, weak prediction results are defined, innovating the forecasting mode of traditional TSP. The original data will be properly reconstituted with prior knowledge, generating attribute vectors with valid predictive information. DeepCascade-WR possesses online learning ability and effectively avoids the retraining problem, owing to the property of OS-ELM, one base model of DeepCascade-WR. Besides, ELM is exploited as another base model of DeepCascade-WR, therefore, DeepCascade-WR naturally inherits some valuable virtues from ELM, including faster training speed, better generalization ability and the avoidance of being fallen into local optima. Ultimately, in the empirical results, DeepCascade-WR demonstrates its superior predictive performance on five benchmark financial datasets, i.e., <^>DJI, <^>GSK, <^>HSI, JOUT, and S&P 500 Index, compared with its base learners and other state-of-the-art algorithms.																	1868-8071	1868-808X				APR	2020	11	4					825	840		10.1007/s13042-019-00994-7													
J								DeepSite: bidirectional LSTM and CNN models for predicting DNA-protein binding	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										DNA-protein binding; Deep learning; Bidirectional long short-term memory; Convolutional neural networks	NEURAL-NETWORKS; SPECIFICITIES	Transcription factors are cis-regulatory molecules that bind to specific sub-regions of DNA promoters and initiate transcription, the process that regulates the conversion of genetic information from DNA to RNA. Several computational methods have been developed to predict DNA-protein binding sites in DNA sequence using convolutional neural network (CNN). However, these techniques could indicate the dependency information of DNA sequence information in the framework of CNN. In addition, these methods are not accurate enough in prediction of the DNA-protein binding sites from the DNA sequence. In this study, we employ the bidirectional long short-term memory (BLSTM) and CNN to capture long-term dependencies between the sequence motifs in DNA, which is called DeepSite. Apart from traditional CNN, which includes six layers: input layer, BLSTM layer, CNN layer, pooling layer, full connection layer and output layer, DeepSite approach can predict DNA-protein binding sites with 87.12% sensitivity, 91.06% specificity, 89.19% accuracy and 0.783 MCC, when tested on the 690 Chip-seq experiments from ENCODE. Lastly, we conclude that our proposed method can also be applied to find DNA-protein binding sites in different DNA sequences.																	1868-8071	1868-808X				APR	2020	11	4					841	851		10.1007/s13042-019-00990-x													
J								An adversarial non-volume preserving flow model with Boltzmann priors	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Flow models; Boltzmann machines; Generative adversarial networks; Deep generative model		Flow-based generative models (flow models) are conceptually attractive due to tractability of the exact log-likelihood and the exact latent-variable inference. In order to generate sharper images and extend the Gaussian prior of Flow models to other discrete forms, we propose an adversarial non-volume preserving flow model with Boltzmann priors (ANVP) for modeling complex high-dimensional densities. In order to generate sharper images, an ANVP model introduces an adversarial regularizer into the loss function to penalize the condition that it places a high probability in regions where the training data distribution has a low density. Moreover, we show that the Gaussian prior can be extended to other forms such as the Boltzmann prior in the proposed ANVP model, and we use multi-scale transformations and Boltzmann priors to model the data distribution. The experiments show that proposed model is effective in image generation task.																	1868-8071	1868-808X				APR	2020	11	4					913	921		10.1007/s13042-019-01048-8													
J								Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Emotion recognition; Psychophysiological signals; Video streams; Multimodal features; Deep belief networks		Emotion recognition has attracted great interest. Numerous emotion recognition approaches have been proposed, most of which focus on visual, acoustic or psychophysiological information individually. Although more recent research has considered multimodal approaches, individual modalities are often combined only by simple fusion or are directly fused with deep learning networks at the feature level. In this paper, we propose an approach to training several specialist networks that employs deep learning techniques to fuse the features of individual modalities. This approach includes a multimodal deep belief network (MDBN), which optimizes and fuses unified psychophysiological features derived from the features of multiple psychophysiological signals, a bimodal deep belief network (BDBN) that focuses on representative visual features among the features of a video stream, and another BDBN that focuses on the high multimodal features in the unified features obtained from two modalities. Experiments are conducted on the BioVid Emo DB database and 80.89% accuracy is achieved, which outperforms the state-of-the-art approaches. The results demonstrate that the proposed approach can solve the problems of feature redundancy and lack of key features caused by multimodal fusion.																	1868-8071	1868-808X				APR	2020	11	4					923	934		10.1007/s13042-019-01056-8													
J								Triplanar convolution with shared 2D kernels for 3D classification and shape retrieval	COMPUTER VISION AND IMAGE UNDERSTANDING										3D vision; Medical image; Deep learning; Computer vision		Increasing the depth of Convolutional Neural Networks (CNNs) has been recognized to provide better generalization performance. However, in the case of 3D CNNs, stacking layers increases the number of learnable parameters linearly, making it more prone to learn redundant features. In this paper, we propose a novel 3D CNN structure that learns shared 2D triplanar features viewed from the three orthogonal planes, which we term S3PNet. Due to the reduced dimension of the convolutions, the proposed S3PNet is able to learn 3D representations with substantially fewer learnable parameters. Experimental evaluations show that the combination of 2D representations on the different orthogonal views learned through the S3PNet is sufficient and effective for 3D representation, with the results outperforming current methods based on fully 3D CNNs. We support this with extensive evaluations on widely used 3D data sources in computer vision: CAD models, LiDAR point clouds, RGB-D images, and 3D Computed Tomography scans. Experiments further demonstrate that S3PNet has better generalization capability for smaller training sets, and learns more of kernels with less redundancy compared to kernels learned from 3D CNNs.																	1077-3142	1090-235X				APR	2020	193								102901	10.1016/j.cviu.2019.102901													
J								Multi-exposure photomontage with hand-held cameras	COMPUTER VISION AND IMAGE UNDERSTANDING										Multi-exposure image fusion; MRF; Rough registration	IMAGE FUSION	The paper studies the image fusion from multiple images taken by hand-held cameras with different exposures. Existing methods often generate unsatisfactory results, such as blurring/ghosting artifacts due to the problematic handling of camera motions, dynamic contents, and inappropriately fusion of local regions (e.g., over or under exposed). In addition, they often require a high-quality image registration, which is hard to achieve in scenarios with large depth variations and dynamic textures, and is also time-consuming. In this paper, we propose to enable a rough registration by a single homography and combine the inputs seamlessly to hide any possible misalignment. Specifically, the method first uses a Markov Random Field (MRF) energy for the labeling of all pixels, which assigns different labels to different aligned input images. During the labeling, it chooses well-exposed regions and skips moving objects at the same time. Then, the proposed method combines a Laplacian image according to the labels and constructs the fusion result by solving the Poisson equation. Furthermore, it adds some internal constraints when solving the Poisson equation for balancing and improving fusion results. We present various challenging examples, including static/dynamic, indoor/outdoor and daytime/nighttime scenes, to demonstrate the effectiveness and practicability of the proposed method.																	1077-3142	1090-235X				APR	2020	193								102929	10.1016/j.cviu.2020.102929													
J								Learning a confidence measure in the disparity domain from O(1) features	COMPUTER VISION AND IMAGE UNDERSTANDING										Stereo matching; Confidence measure; Machine learning; Semi global matching		Depth sensing is of paramount importance for countless applications and stereo represents a popular, effective and cheap solution for this purpose. As highlighted by recent works concerned with stereo, uncertainty estimation can be a powerful cue to improve accuracy in stereo. Most confidence measures rely on features, mainly extracted from the cost volume, fed to a random forest or a convolutional neural network trained to estimate match uncertainty. In contrast, we propose a novel strategy for confidence estimation based on features computed in the disparity domain, making our proposal suited for any stereo system including COTS devices, and in constant time. We exhaustively assess the performance of our proposals, referred to as O1 and O2, on KITTI and Middlebury datasets with three popular and different stereo algorithms (CENSUS, MC-CNN and SGM), as well as a deep stereo network (PSM-Net). We also evaluate how well confidence measures generalize to different environments/datasets.																	1077-3142	1090-235X				APR	2020	193								102905	10.1016/j.cviu.2020.102905													
J								Deep code operation network for multi-label image retrieval	COMPUTER VISION AND IMAGE UNDERSTANDING										Multi-label image retrieval; Hashing; Deep learning		Deep hashing methods have been extensively studied for large-scale image search and achieved promising results in recent years. However, there are two major limitations of previous deep hashing methods for multilabel image retrieval: the first one concerns the flexibility for users to express their query intention (so-called the intention gap), and the second one concerns the exploitation of rich similarity structures of the semantic space (so-called the semantic gap). To address these issues, we propose a novel Deep Code Operation Network (CoNet), in which a user is allowed to simultaneously present multiple images instead of a single one as his/her query, and then the system triggers a series of code operators to extract the hidden relations among them. In this way, a set of new queries are automatically constructed to cover users' real complex query intention, without the need of explicitly stating them. The CoNet is trained with a newly proposed margin-adaptive triplet loss function, which effectively encourages the system to incorporate the hierarchical similarity structures of the semantic space into the learning procedure of the code operations. The whole system has an end-to-end differentiable architecture, equipped with an adversarial mechanism to further improve the quality of the final intention representation. Experimental results on four multi-label image datasets demonstrate that our method significantly improves the state-of-the-art in performing complex multi-label retrieval tasks with multiple query images.																	1077-3142	1090-235X				APR	2020	193								102916	10.1016/j.cviu.2020.102916													
J								UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking	COMPUTER VISION AND IMAGE UNDERSTANDING										Object detection; Object tracking; Benchmark; Evaluation protocol	MULTITARGET TRACKING; ROBUST; APPEARANCE; HISTOGRAMS	Effective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predefined setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traffic scenes (over 140,000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and tracking methods. Our analysis shows the complex effects of detection accuracy on MOT system performance. Based on these observations, we propose effective and informative evaluation metrics for MOT systems that consider the effect of object detection for comprehensive performance analysis.																	1077-3142	1090-235X				APR	2020	193								102907	10.1016/j.cviu.2020.102907													
J								Adversarial autoencoders for compact representations of 3D point clouds	COMPUTER VISION AND IMAGE UNDERSTANDING										Adversarial Autoencoders; Point Clouds; Deep Learning; Representation Learning; Neural Networks; Adversarial Learning		Deep generative architectures provide a way to model not only images but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for challenging tasks, including 3D points generation, reconstruction, compression, and clustering. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. Moreover, our model is capable of learning meaningful compact binary descriptors with adversarial training conducted on a latent space. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers a much broader portion of training data distribution. Finally, our quantitative evaluation shows that 3dAAE provides state-of-the-art results for 3D points clustering and 3D object retrieval.																	1077-3142	1090-235X				APR	2020	193								102921	10.1016/j.cviu.2020.102921													
J								A progressive learning framework based on single-instance annotation for weakly supervised object detection	COMPUTER VISION AND IMAGE UNDERSTANDING										Single-instance annotation; Progressive learning framework; Weakly supervised object detection; Instance mining	LOCALIZATION	Fully-supervised object detection (FSOD) and weakly-supervised object detection (WSOD) are two extremes in the field of object detection. The former relies entirely on detailed bounding-box annotations while the later discards them completely. To balance these two extremes, we propose to make use of the so-called single-instance annotations, i.e., all images that contain only a single object are labeled with the corresponding bounding-boxes. By using such instance annotations of the simplest images, we propose a progressive learning framework that integrates image-level learning, single-instance learning, and multi-instance learning into an end-to-end network. Specifically, our framework is composed of three parallel streams that share a proposal feature extractor. The first stream is supervised by image-level annotations, which provides global information of all training data for the shared feature extractor. The second stream is supervised by single-instance annotations to bridge the features learning gap between the image level and instance level. To further learn from complex images, we propose an overlap-based instance mining algorithm to mine pseudo multi-instance annotations from the detection results of the second stream, and use them to supervise the third stream. Our method achieves a trade-off between the detection accuracy and annotation cost. Extensive experiments demonstrate the effectiveness of our proposed method on the PASCAL VOC and MS-COCO dataset, implying that a few single-instance annotations can improve the detection performance of WSOD significantly (more than 10%) and reduce the average annotation cost of FSOD greatly (more than 5 times).																	1077-3142	1090-235X				APR	2020	193								102903	10.1016/j.cviu.2020.102903													
J								In praise of partially interpretable predictors	STATISTICAL ANALYSIS AND DATA MINING										mean-squared error; model average; Nadaraya-Watson; Priestley-Chao; stacking	MODEL; REGRESSION	Often there is an uninterpretable model that is statistically as good as, if not better than, a successful interpretable model. Accordingly, if one restricts attention to interpretable models, then one may sacrifice predictive power or other desirable properties. A minimal condition for an interpretable, usually parametric, model to be better than another model is that the first should have smaller mean-squared error or integrated mean-squared error. We show through a series of examples that this is often not the case and give the asymptotic forms of a variety of interpretable, partially interpretable, and noninterpretable methods. We find techniques that combine aspects of both interpretability and noninterpretability in models seem to give the best results.																	1932-1864	1932-1872				APR	2020	13	2					113	133		10.1002/sam.11450													
J								Dependability-based cluster weighting in clustering ensemble	STATISTICAL ANALYSIS AND DATA MINING										cluster dependability; clustering ensemble; consensus partition; entropy	COMBINING MULTIPLE CLUSTERINGS; CLASSIFICATION; SELECTION; IMPROVE; QUALITY; MEMORY	After observing the ensemble success in supervised learning (such as classification), it was extended into unsupervised learning. Therefore, cluster ensemble, which merges multiple basic data partitions or clusters (called as ensemble pool) into an ordinarily better clustering solution usually named as consensus partition, emerged. Any cluster ensemble method tries to optimize a particular criterion during extracting the consensus partition out of the ensemble pool. But traditional cluster ensembles consider all the pool members with the equal importance in making the consensus partition; that is to say that each basic partition or cluster participates in the cluster ensemble algorithm equivalently. Indeed, they ignore to consider any ensemble member according to its importance. But it is obvious that some clusters with more quality deserve more emphasis and some clusters with less quality deserve less emphasis during generating consensus partition. This paper proposes (a) a metric to evaluate quality of any arbitrary cluster, (b) a mechanism to project the computed quality of a cluster into a meaningful weight value, and (c) an approach to apply the weight values of the basic clusters in the cluster ensemble process. Experimental results conducted on a number of real-world standard datasets indicate that the proposed method outperforms the state of the art methods.																	1932-1864	1932-1872				APR	2020	13	2					151	164		10.1002/sam.11451													
J								Toward a Sociable and Dependable Elderly Care Robot: Design, Implementation and User Study	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Elderly care robot; Service robot; HomeMate; Sociability; Dependability; Affordance; Interaction design; Emotional design		A critical demand for innovation in elderly care, especially in delivering the quality of service to elderlies, arises as many parts of the world are rapidly in transition to an elderly society. The advancement of robotic technologies, especially, in cognitive robotics and sociable human-robot interaction offers a great opportunity for meeting such demand. This paper presents the design and implementation of a next generation of elderly care robot, named as "HomeMate," based on an innovative undertaking in its sociability and dependability with extensive user studies. The elderlies taken care of by Senior Welfare Centers are chosen as the target sector, for which the following five service scenarios are designed: infotainment, video chatting, game playing, medicine alarm and, in particular, errand services. Unlike conventional approaches, the scenarios are designed here to ensure the overall quality of service by maximizing the synergy under an elderly-caregiver-service robot ecosystem. The unique features implemented in HomeMate include 1) the principle of affordance in appearance design by matching functionality and anthropomorphism indices, 2) the sociability implemented by balancing between autonomy and user controllability as well as by integrating multimodal interactions into HomeMate avatar, and 3) the emphasis on dependability to inspire confidence on HomeMate as a trusted assistant, for which the principle of dependability is proposed and implemented with a cognitive framework. Experiments and user studies strongly support the proposed design principles and verify the dependability in performance.																	0921-0296	1573-0409				APR	2020	98	1			SI		5	17		10.1007/s10846-019-01028-8													
J								Meet Stevie: a Socially Assistive Robot Developed Through Application of a 'Design-Thinking' Approach	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Human-robot interaction; Design thinking; Mixed-methods; Ethics; Use-cases; Robot design; User-centered design; Stevie robot	SOFT; ANTHROPOMORPHISM; FABRICATION; ACCEPTANCE; GENDER	Socially assistive service robots offer a compelling means of empowering vulnerable members of society. They can perform useful custodial functions for their users, thus reducing reliance on informal and formal caregivers. The aim of this work was to develop a prototype socially assistive robot through application of user-centered design methods, adopting an approach based on the 'design thinking' philosophy. Using a range of design techniques that have seldom been applied in a robotics context, a high resolution prototype was designed, fabricated and evaluated using a mixed-methods approach. Experiments were performed with four distinct user groups (including residents and care staff at a retirement community) to explore the first impressions that the robot created, its perceived usefulness, and the potential for acceptance of the platform. Overall, the robot was positively perceived by each user group, and user feedback from testing generated insights regarding the design, usability and envisioned use-cases of the technology. These findings have value in informing future iterations of robot design, as well as providing motivation for a series of future studies.																	0921-0296	1573-0409				APR	2020	98	1			SI		39	58		10.1007/s10846-019-01051-9													
J								Exploring Low-Cost Mobile Manipulation for Elder Care Within a Community Based Setting	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Elder care robotics; Human robot interaction; Mobile manipulation	ASSISTIVE ROBOT; NURSING-HOMES; OLDER-ADULTS	This paper identifies tasks an affordable mobile manipulator service robot could do to benefit older adults' independence in a supportive apartment living facility, and a series of tests validating the highest ranked tasks. Previous deployments considered a mobile only robotic base, performing exercises through walking encouragement and hydration by water delivery, both followed by pain assessment and are briefly described. Current tests investigated the efficacy of mobile manipulation tasks by adapting a novel, low-cost telescopic robotic arm to the same mobile base, with aspects of human-robot interaction investigated through a physical interactive game with the older adults. All deployments took place at a Program of All-inclusive Care (PACE) center and interactions were evaluated by two observers, along with post-interaction surveys with the older adults. Previous work on elder care robotics is discussed. Results of the mobile manipulation deployments, along with design guidelines are presented. Future work includes the development of a new mobile manipulator capable of performing the investigated tasks with a greater level of autonomy and efficiency.																	0921-0296	1573-0409				APR	2020	98	1			SI		59	70		10.1007/s10846-019-01041-x													
J								User-Oriented Design of Active Monitoring Bedside Agent for Older Adults to Prevent Falls	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Bedside agent; Fall prevention; User-oriented design; Assistive robotics		A small bedside agent for preventing falls has been developed. It talks to a person on a bed to prevent them from getting out of bed abruptly, until a care worker arrives. This paper describes the user-oriented design process of the agent system. The development process involving users, such as nurses and caregivers, as well as older adults is described. First, hardware design, such as the outer shape, size, and function of the agent was reviewed by nurses and caregivers, mainly from a safety viewpoint. The prototype agent incorporating improvements based on their opinions was used experimentally by older adults after several review processes. Second, the software design of the agent, such as the content of voice call, was studied through multiple experiments to improve its acceptability. Lastly, the integrated model was introduced into care facilities and hospitals to investigate the practical serviceability of the system.																	0921-0296	1573-0409				APR	2020	98	1			SI		71	84		10.1007/s10846-019-01050-w													
J								Human Re-Identification with a Robot Thermal Camera Using Entropy-Based Sampling	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Service robots; Re-identification; Elderly care; Thermal camera; Occlusion; Body motion	PERSON REIDENTIFICATION	Human re-identification is an important feature of domestic service robots, in particular for elderly monitoring and assistance, because it allows them to perform personalized tasks and human-robot interactions. However vision-based re- identification systems are subject to limitations due to human pose and poor lighting conditions. This paper presents a new re-identification method for service robots using thermal images. In robotic applications, as the number and size of thermal datasets is limited, it is hard to use approaches that require huge amount of training samples. We propose a re-identification system that can work using only a small amount of data. During training, we perform entropy-based sampling to obtain a thermal dictionary for each person. Then, a symbolic representation is produced by converting each video into sequences of dictionary elements. Finally, we train a classifier using this symbolic representation and geometric distribution within the new representation domain. The experiments are performed on a new thermal dataset for human re-identification, which includes various situations of human motion, poses and occlusion, and which is made publicly available for research purposes. The proposed approach has been tested on this dataset and its improvements over standard approaches have been demonstrated.																	0921-0296	1573-0409				APR	2020	98	1			SI		85	102		10.1007/s10846-019-01026-w													
J								First-person Video Analysis for Evaluating Skill Level in the Humanitude Tender-Care Technique	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Dementia; Care; Deep neural network (DNN); Skill evaluation; Wearable system; Computer vision; First person video	CAREGIVER BURDEN; FAMILY CAREGIVERS; DEMENTIA; INTERVENTIONS; DEPRESSION	In this paper, we describe a wearable first-person video (FPV) analysis system for evaluating the skill levels of caregivers. This is a part of our project that aims to quantize and analyze the tender-care technique known as Humanitude by using wearable sensing and AI technology devices. Using our system, caregivers can evaluate and elevate their care levels by themselves. From the FPVs of care sessions taken by wearable cameras worn by caregivers, we obtained the 3D facial distance, pose and eye-contact states between caregivers and receivers by using facial landmark detection and deep neural network (DNN)-based eye contact detection. We applied statistical analysis to these features and developed algorithms that provide scores for tender-care skill. In experiments, we first evaluated the performance of our DNN-based eye contact detection by using eye contact datasets prepared from YouTube videos and FPVs that assume conversational scenes. We then performed skill evaluations by using Humanitude training scenes involving three novice caregivers, two Humanitude experts and seven middle-level students. The results showed that our eye contact detection outperformed existing methods and that our skill evaluations can estimate the care skill levels.																	0921-0296	1573-0409				APR	2020	98	1			SI		103	118		10.1007/s10846-019-01052-8													
J								Robotic Standard Development Life Cycle in Action	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Knowledge-based systems; Robotic modelling; Companion robots; Development life cycle; Standards and ethics; etc; )		Robotics is a fast-growing field which requires the efficient development of adapted standards. Hence, in this paper, we propose a development methodology to support the robot standardization effort led by international, technical, and professional associations such as the Institute of Electrical and Electronics Engineers (IEEE). Our proposed standard development life cycle is a middle-out, iterative, collaborative, and incremental approach we have successfully applied to the development of the new IEEE Ontological Standard for Ethically Driven Robotics and Automation Systems (IEEE P7007 Standard).																	0921-0296	1573-0409				APR	2020	98	1			SI		119	131		10.1007/s10846-019-01107-w													
J								Autonomous Flight Take-off in Flapping Wing Aerial Vehicles	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Aerial vehicle; Avian flyers; Flapping wing; Jumping mechanism; Leg thrust; Take-off; Unsteady aerodynamics	AERODYNAMIC MODEL; LEG; MECHANISM; FORCES	This paper presents the design and dynamic simulation of a jumping mechanism for flight initiation in flapping wing aerial vehicles to enhance their autonomous mobility. It is inspired by avian flyers that start flight by jumping using their hind limbs. A 4-bar linkage that performs the prescribed take-off maneuvers has been geometrically designed using dimensional synthesis by motion generation. The kinematic dimensions obtained thereof have been ascertained analytically. A magnetic latching solenoid actuator and its corresponding transmission mechanism has been proposed for jumping actuation due to its compact structure, and low power consumption with no heat nor electrical noise generation. After take-off, the necessity to sustain the aerial vehicle airborne by providing adequate supportive forces at low air speeds has been demonstrated by the prospect of unsteady aerodynamics. Fluid-structure interaction has been analyzed by co-simulation between ADAMS? and MATLAB?/Simulink. The designed legs give an inclined take-off of about 64 degrees and an initial speed of almost 2m/s with an angular deflection of 110 degrees between the leg segments. Flight performance indicates an increase in lift/drag ratios with in both flapping rate and forward velocity up to some limit. The results also compared favorably with those from wind tunnel tests and previous analysis of related work. They all exhibited an increase in wing efficiency with decreasing mean angles of attack as the air vehicle approaches stable flight.																	0921-0296	1573-0409				APR	2020	98	1			SI		135	152		10.1007/s10846-019-01003-3													
J								On the Accuracy of Inertial Parameter Estimation of a Free-Flying Robot While Grasping an Object	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Space robotics; Identification; Mobile manipulation; Non-linear control	SPACE ROBOTS; IDENTIFICATION	When model-based controllers are used for carrying out precise tasks, the estimation of model parameters is key for a better trajectory tracking performance. We consider the scenario of a free-flying space robot with limited actuation that has grasped an object of unknown properties. The inertial parameters - mass, centre of mass and the inertia tensor - of the robot-object system are to be determined. In our previous works, we used a parameter estimation algorithm where truncated Fourier series were used to represent both the reference excitation trajectory and the executed one. That algorithm is the focus of this paper, along with a study of the factors that could contribute to a loss of accuracy in the obtained estimates. Simulation results with the Space CoBot free-flyer robot are used to find the causes of the latter: first, a modified version of the minimum condition number criterion is used to generate feasible excitatory trajectories. The results are compared with those given by the maximum information criterion to check for better excitation. Next, an appropriate number of harmonics has to be found for the Fourier series, which will be used to fit the measured data. This is done autonomously, based on the least squares residual. Finally, the relation between input saturation while executing the excitation trajectory and the errors in the resulting parameter estimates is studied.																	0921-0296	1573-0409				APR	2020	98	1			SI		153	163		10.1007/s10846-019-01040-y													
J								Using Pre-Computed Knowledge for Goal Allocation in Multi-Agent Planning	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi agent planning; Actuation maps; Goal allocation; Robotics; Distributed planning; Path planning	SYSTEM	Many real-world robotic scenarios require performing task planning to decide courses of actions to be executed by (possibly heterogeneous) robots. A classical centralized planning approach has to find a solution inside a search space that contains every possible combination of robots and goals. This leads to inefficient solutions that do not scale well. Multi-Agent Planning (MAP) provides a new way to solve this kind of tasks efficiently. Previous works on MAP have proposed to factorize the problem to decrease the planning effort i.e. dividing the goals among the agents (robots). However, these techniques do not scale when the number of agents and goals grow. Also, in most real world scenarios with big maps, goals might not be reached by every robot so it has a computational cost associated. In this paper we propose a combination of robotics and planning techniques to alleviate and boost the computation of the goal assignment process. We use Actuation Maps (AMs). Given a map, AMs can determine the regions each agent can actuate on. Thus, specific information can be extracted to know which goals can be tackled by each agent, as well as cheaply estimating the cost of using each agent to achieve every goal. Experiments show that when information extracted from AMs is provided to a multi-agent planning algorithm, the goal assignment is significantly faster, speeding-up the planning process considerably. Experiments also show that this approach greatly outperforms classical centralized planning.																	0921-0296	1573-0409				APR	2020	98	1			SI		165	190		10.1007/s10846-019-01022-0													
J								Cleaning Tasks Knowledge Transfer Between Heterogeneous Robots: a Deep Learning Approach	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Learning from demonstration; Transfer learning; Data augmentation; Convolutional neural networks; Task parametrized Gaussian mixture models	PRIMITIVES; MIXTURE; ICUB	Nowadays, autonomous service robots are becoming an important topic in robotic research. Differently from typical industrial scenarios, with highly controlled environments, service robots must show an additional robustness to task perturbations and changes in the characteristics of their sensory feedback. In this paper, a robot is taught to perform two different cleaning tasks over a table, using a learning from demonstration paradigm. However, differently from other approaches, a convolutional neural network is used to generalize the demonstrations to different, not yet seen dirt or stain patterns on the same table using only visual feedback, and to perform cleaning movements accordingly. Robustness to robot posture and illumination changes is achieved using data augmentation techniques and camera images transformation. This robustness allows the transfer of knowledge regarding execution of cleaning tasks between heterogeneous robots operating in different environmental settings. To demonstrate the viability of the proposed approach, a network trained in Lisbon to perform cleaning tasks, using the iCub robot, is successfully employed by the DoRo robot in Peccioli, Italy.																	0921-0296	1573-0409				APR	2020	98	1			SI		191	205		10.1007/s10846-019-01072-4													
J								Deeper in BLUE Development of a roBot for Localization in Unstructured Environments	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned ground vehicle; Mobile robot; Localization; GNSS; SLAM; Low-level control; Extended Kalman filter; ROS	VISION; SYSTEM; CAR; MANIPULATION; ACCURACY; TRACTOR	Despite the progress that has been made with simulators and the existence of datasets, real experimental platforms are, and will continue to be necessary. Well-designed research platforms that produce reliable results and are easy to operate and debug make all the difference in research productivity. In this paper, we show the works that turned a stock electric cart into a research robot called BLUE. It provides a ROS interface that allows real-time control, monitoring, and adjustment of the system. We provide a quantitative performance evaluation, and a GitHub repository that contains all the information required to replicate the process.																	0921-0296	1573-0409				APR	2020	98	1			SI		207	225		10.1007/s10846-019-00983-6													
J								Improved MDS-based Localization with Non-line-of-sight RF Links	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot localization; Indoor positioning; Multidimensional scaling; Non-line-of-sight propagation; Bias estimation; Noise; Constrained optimization; ToF; RSSI	NLOS IDENTIFICATION; MITIGATION; ERROR	The performance of indoor localization techniques adopted in robot localization systems that use radio positioning is usually degraded in non-line-of-sight (NLOS) environments. In this paper, we propose a technique for estimating NLOS biases and measurement noise in distances under multidimensional scaling (MDS) based positioning with fixed nodes. An ideal matrix of pairwise distance measurements exhibits a symmetry that allows to compute mobile node positions from those of fixed ones, and then recompute exactly the fixed node positions from the earlier computed mobile node positions. In a NLOS environment, this symmetry is lost; fixed node positions can not be reproduced exactly. This work exploits the error in the recomputation of fixed node positions for the correction of NLOS biases and noise in the pairwise distances. A constrained-optimization problem is formulated to estimate biases for each measured distance and final mobile node positions under the MDS scheme. A supplementary approach is presented for special cases where the number of mobile nodes is less than 3. Experimental results show that position errors can be reduced by up to 28% for a set up of 4 fixed and 3 mobile nodes. Simulations are used to further validate the results for larger deployments of nodes.																	0921-0296	1573-0409				APR	2020	98	1			SI		227	237		10.1007/s10846-019-01021-1													
J								Filtering Infrequent Behavior in Business Process Discovery by Using the Minimum Expectation	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Business Process; Infrequent Events; Minimum Expectation; Process Mining	OUTLIER DETECTION; ANOMALY DETECTION; TIME-SERIES	The aim of process discovery is to discover process models from the process execution data stored in event logs. In the era of "Big Data," one of the key challenges is to analyze the large amounts of collected data in meaningful and scalable ways. Most process discovery algorithms assume that all the data in an event log fully comply with the process execution specification, and the process event logs are no exception. However, real event logs contain large amounts of noise and data from irrelevant infrequent behavior. The infrequent behavior or noise has a negative influence on the process discovery procedure. This article presents a technique to remove infrequent behavior from event logs by calculating the minimum expectation of the process event log. The method was evaluated in detail, and the results showed that its application in existing process discovery algorithms significantly improves the quality of the discovered process models and that it scales well to large datasets.																	1557-3958	1557-3966				APR-JUN	2020	14	2					1	15		10.4018/IJCINI.2020040101													
J								Comparisons of Speech Parameterisation Techniques for Classification of Intellectual Disability Using Machine Learning	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Classification; Intellectual Disability (ID); Linear Predictive Coding; Mel-Frequency Cepstral Coefficients; Typically Developed (TD)	CHILDREN; LANGUAGE; MFCC	Classification of intellectually disabled children through manual assessment of speech at an early age is inconsistent, subjective, time-consuming and prone to error. This study attempts to classify the children with intellectual disabilities using two speech feature extraction techniques: Linear Predictive Coding (LPC) based cepstral parameters, and Mel-frequency cepstral coefficients (MFCC). Four different classification models: k-nearest neighbour (k-NN), support vector machine (SVM), linear discriminant analysis (LDA) and radial basis function neural network (RBFNN) are employed for classification purposes. 48 speech samples of each group are taken for analysis, from subjects with a similar age and socio-economic background. The effect of the different frame length with the number of filterbanks in the MFCC and different frame length with the order in the LPC is also examined for better accuracy. The experimental outcomes show that the projected technique can be used to help speech pathologists in estimating intellectual disability at early ages.																	1557-3958	1557-3966				APR-JUN	2020	14	2					16	34		10.4018/IJCINI.2020040102													
J								Enhanced Bootstrapping Algorithm for Automatic Annotation of Tweets	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Bootstrapping; Emotion Classification; Emotions; Semantic Similarity	EMOTION	Annotations are critical in various text mining tasks such as opinion mining, sentiment analysis, word sense disambiguation. Supervised learning algorithms start with the training of the classifier and require manually annotated datasets. However, manual annotations are often subjective, biased, onerous, and burdensome to develop; therefore, there is a need for automatic annotation. Automatic annotators automatically annotate the data for creating the training set for the supervised classifier, but lack subjectivity and ignore semantics of underlying textual structures. The objective of this research is to develop scalable and semantically rich automatic annotation system while incorporating domain dependent characteristics of the annotation process. The authors devised an enhanced bootstrapping algorithm for the automatic annotation of Tweets and employed distributional semantic models (LSA and Word2Vec) to augment the novel Bootstrapping algorithm and tested the proposed algorithm on the 12,000 crowd-sourced annotated Tweets and achieved a 68.56% accuracy which is higher than the baseline accuracy.																	1557-3958	1557-3966				APR-JUN	2020	14	2					35	60		10.4018/IJCINI.2020040103													
J								Exploiting Visual Features in Financial Time Series Prediction	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Artificial Neural Networks; Extreme Gradient Boosting; Forex; Machine Learning; Machine Vision; Predictability; Quantitative Analysis; Support Vector Machine	PRICE-VOLATILITY; RISK PREMIA; FUTURES; PERFORMANCE; NETWORKS; RETURNS	The possibility to enhance prediction accuracy for foreign exchange rates was investigated in two ways: first applying an outside the box approach to modeling price graphs by exploiting their visual properties, and secondly employing the most efficient methods to detect patterns to classify the direction of movement. The approach that exploits the visual properties of price graphs which make use of density regions along with high and low values describing the shape; hence, the authors propose the name 'Finance Vision.' The data used in the predictive model consists of 1-hour past price values of 4 different currency pairs, between 2003 and 2016. Prediction performances of state-of-the-art methods; Extreme Gradient Boosting, Artificial Neural Network and Support Vector Machines are compared over the same data with the same sets of features. Results show that density based visual features contribute considerably to prediction performance.																	1557-3958	1557-3966				APR-JUN	2020	14	2					61	76		10.4018/IJCINI.2020040104													
J								MCDM Approach for Mitigation of Flooding Risks in Odisha (India) Based on Information Retrieval	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										BWM; Environmental Impacts; Flooding; MCDM; Odisha; SWARA	MULTICRITERIA APPROACH; SELECTION; SWARA; SUSTAINABILITY; METHODOLOGY	Multi-criteria decision-making (MCDM) provides a suitable platform for groups as well as promotion of the participants' role in decision processes. This also enables the development of real participatory processes essential for the successful implementation and sustainable flood management programs. The present study contributes by applying two MCDM approaches for weighting the criteria related to the environmental impacts of flooding. Moreover, an attempt was made in this study by an extensive review of literature, and consultations with experts to identify the environmental impacts of flooding in Odisha State (India). Then, the Best Worst Method (BWM) followed by the Step-Wise Weight Assessment Ratio Analysis (SWARA) method was used to rank the environmental impacts which were considered as the risk factors. The result of this study will be useful to the governance system for an effective and proper planning, and implementation of flood mitigation projects.																	1557-3958	1557-3966				APR-JUN	2020	14	2					77	91		10.4018/IJCINI.2020040105													
J								Data Analytic Techniques for Developing Decision Support System on Agrometeorological Parameters for Farmers	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Classification and Regression Trees (CART); Crop Type Prediction; Decision Support System; Smart Agriculture; Support Vector Machines (SVM)		The day-to-day lives of humans are changing remarkably due to the evolution in tools, techniques and technology across the planet. This evolution is not only impacting the growth of humans but also contributing to the growth and well-being of society and country. The domain of data analytics (DA) and internet of things (IoT) is very much facilitating this growth. But there have been only a handful of innovations and explorations in the field of agriculture, although it being the backbone and largely contributing to the gross domestic product (GDP) of a country like India. The reason for it may be profuse, such as the erratic weather conditions, improper irrigation, farmers being skeptical using modern tools and many more. But being in a developing country that has its primary focus on invention and innovation, a consensus has to be reached so that the modern tools and technologies, abet agriculture throughout the country. In our work, an attempt is made to analyze the different aspects that influences the variable outcomes in agriculture with the aid of various data analytic algorithms. Rainfall, humidity and temperature are some of the variables that determine the type of crop. Therefore, the task of prediction of crop type given these factors using decision trees and support vector machines (SVM) is implemented, and the accuracy of the models are computed. Here, more focus is given to the state of Karnataka and to its major crops. With rice, ragi and maize being some of the predominant crops, an analysis is portrayed considering the yield across the state.																	1557-3958	1557-3966				APR-JUN	2020	14	2					92	107		10.4018/IJCINI.2020040106													
J								Predictive Analytical Model for Microblogging Data Using Asset Bubble Modelling	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Apache Storm; Asset Bubble Model; Forecasting; Information Diffusion; Sentiment Analysis; Twitter Data	SOCIAL MEDIA ANALYTICS	In recent days, social media plays a significant role in the ecosystem of the big data world and its different types of information. There is an emerging need for collection, monitoring, analyzing, and visualizing the different information from various social media platforms in different domains like businesses, public administration, and others. Social media acts as the representative with numerous microblogs for analytics. Predictive analytics of such microblogs provides insights into various aspects of the real-world entities. In this article, a predictive model is proposed using the tweets generated on Twitter social media. The proposed model calculates the potential of a topic in the tweets for the prediction purposes. The experiments were conducted on tweets of the regional election in India and the results are better than the existing systems. In the future, the model can be extended for analysis of information diffusion in heterogeneous systems.																	1557-3958	1557-3966				APR-JUN	2020	14	2					108	118		10.4018/IJCINI.2020040107													
J								From motor to visually guided bimanual affordance learning	ADAPTIVE BEHAVIOR										Sensorimotor learning; affordances; goal babbling; motor equivalence; interlimb coordination; bimanual affordances	COMPUTATIONAL MODELS; KINEMATICS; BEHAVIOR; SYSTEMS	The mechanisms of how the brain orchestrates multi-limb joint action have yet to be elucidated and few computational sensorimotor (SM) learning approaches have dealt with the problem of acquiring bimanual affordances. We propose a series of bidirectional (forward/inverse) SM maps and its associated learning processes that generalize from uni- to bimanual interaction (and affordances) naturally, reinforcing the motor equivalence property. The SM maps range from a SM nature to a solely sensory one: full body control, delta SM control (through small action changes), delta sensory co-variation (how body-related perceptual cues covariate with object-related ones). We make several contributions on how these SM maps are learned: (1) Context and Behavior-Based Babbling: generalizing goal babbling to the interleaving of absolute and local goals including guidance of reflexive behaviors; (2) Event-Based Learning: learning steps are driven by visual, haptic events; and (3) Affordance Gradients: the vectorial field gradients in which an object can be manipulated. Our modeling of bimanual affordances is in line with current robotic research in forward visuomotor mappings and visual servoing, enforces the motor equivalence property, and is also consistent with neurophysiological findings like the multiplicative encoding scheme.																	1059-7123	1741-2633				APR	2020	28	2					63	78		10.1177/1059712319855836													
J								On the nature and origins of cognition as a form of motivated activity	ADAPTIVE BEHAVIOR										Cognition; motivation; dissipative adaption; ecological psychology	SELF-ORGANIZATION	A fundamental challenge for enactive theory and other radical varieties of non-representational "E cognition" is to reconceive the end-directed character of cognitive activity in naturally emergent but also experientially adequate terms. In short, it is necessary to show how cognitive activity is motivated. In this article, I present a preliminary analysis of the nature of motivation and the challenge that it presents to cognitive science. I make the case that a theory of motivation is a critical desideratum for dynamical theories of cognition, especially insofar as they understand cognition as a self-organized and "soft assembled" process. Finally, I propose that a branch of ecological psychology that conceives of cognition as a special variety of "dissipative adaptation" offers a promising framework for confronting this challenge.																	1059-7123	1741-2633				APR	2020	28	2					89	103		10.1177/1059712318824325													
J								Testing the degree of overlap for the expected value of random intervals	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Overlapping index; Interval-valued data; Random intervals; Hypothesis testing; Bootstrap approach	DISCRIMINANT-ANALYSIS; LINEAR-REGRESSION; SIMILARITY TEST; EXPECTATION; MODEL; IDENTIFICATION	Some hypothesis tests for analyzing the degree of overlap between the expected value of random intervals are provided. For this purpose, a suitable measure to quantify the overlapping grade between intervals is considered on the basis of the Szymkiewicz-Simpson coefficient defined for general sets. It can be seen as a kind of likeness index to measure the mutual information between two intervals. On one hand, an estimator for the proposed degree of overlap between intervals is provided and its strong consistency is analyzed. On the other hand, two tests are also proposed in this framework: a one-sample test to examine the degree of overlap between the expected value of a random interval and a given interval, and a two-sample test to check the degree of overlap between the expected value of two random intervals. To solve such hypothesis tests, two statistics are suggested and their limit distributions are studied.by considering both asymptotic and bootstrap techniques. Their power has been also explored by means of local alternatives. In addition, some simulation studies are carried out to investigate the behavior of the proposed approaches. Finally, the performance of the tests is also reported in a real-life application. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						1	19		10.1016/j.ijar.2019.12.012													
J								A new rule reduction and training method for extended belief rule base based on DBSCAN algorithm	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Extended belief rule base; Rule reduction; Parameter learning	EVIDENTIAL REASONING APPROACH; SAFETY ASSESSMENT MODEL; WEIGHT CALCULATION; EXPERT-SYSTEM; INFERENCE METHODOLOGY; ACTIVATION METHOD; CLASSIFICATION; OPTIMIZATION	Rule reduction is one of the focuses of numerous researches on belief-rule-based system, in some cases, too many redundant rules may be a concern to the rule-based system. Though rule reduction methods have been widely used in the belief-rule-based system, extended belief-rule-based system, which is an expansion of belief-rule-based system, still lacks methods to reduce and train rules in the extended belief rule base (EBRB). To this end, this paper proposes an EBRB reduction and training method. Based on the density-based spatial clustering applications with noise (DBSCAN) algorithm, a new EBRB reduction method is proposed, where all the rules in the EBRB will be visited and rules within the distance of the fusion threshold will be fused. Moreover, the EBRB training method using parameter learning, which uses a set of training data to train the parameters of EBRB, is also proposed to improve the accuracy of the EBRB system. Two case studies of regression and classification are used to illustrate the feasibility and efficiency of the proposed EBRB reduction and training method. Comparison results show that the proposed method can effectively downsize the EBRB and increase the accuracy of EBRB system. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						20	39		10.1016/j.ijar.2019.12.016													
J								Data meaning and knowledge discovery: Semantical aspects of information systems	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Data meaning; Information system; Rough set theory; Dominance-based rough set approach; Formal concept analysis	FORMAL CONCEPT ANALYSIS; ROUGH FUZZY-SETS; CONCEPT LATTICES; CHU SPACES; DECISION; APPROXIMATIONS; UNCERTAINTY; VAGUENESS; MODEL	Data tables provide the standard means of representation of qualitative or quantitative information about objects of interest. They also form a starting point for the task of information processing: an operation of passing from raw data or information to semantically processed knowledge. The fundamental issue here is the question about the meaning of data: What do entries in the data table actually tell us about objects? It entails another question: How should the meaning be further processed? The primary aim of the article is an attempt to answer these two questions. To this end we are going to employ conceptual scales from formal concept analysis, an important theory of data processing introduced by Rudolf Wille, and apply them to data tables so as to obtain multivalued information systems, which were introduced and developed within the conceptual framework of rough set theory by Zdzislaw Pawlak and Ewa Orlowska. Our main idea is to regard multivalued information systems as the semantics or meaning of the original tables. This idea allows us to describe and combine classical rough set theory, dominance-based rough set approach, and formal concept analysis within the single framework of multivalued information systems, which is rich enough to cope with a number of semantical nuances that may occur during the process of data analysis. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						40	57		10.1016/j.ijar.2020.01.002													
J								Belief function of Pythagorean fuzzy rough approximation space and its applications	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Pythagorean fuzzy rough set; Evidence theory; Belief function; Plausibility function; Attribute reduction	ATTRIBUTE REDUCTION; DECISION-MAKING; SETS	Rough set theory and evidence theory are two approaches to handle decision making and reduction problems of imprecise and uncertain knowledge. It is motivated that Pythagorean fuzzy set excels at describing the situation where the sum of non-membership degree and membership degree is greater than 1, and may have wider applications than intuitionistic fuzzy set. So in this paper we study probability measure of Pythagorean fuzzy sets and belief structure of Pythagorean fuzzy information systems based on rough set theory, and discuss the reduction of Pythagorean fuzzy information systems. First, we review the properties of Pythagorean fuzzy sets and the upper and lower Pythagorean fuzzy rough approximation operators on the level sets. Then using these properties, probability measure of Pythagorean fuzzy sets are constructed. And the belief and plausibility functions are studied by using the Pythagorean fuzzy rough upper and lower approximation operators. Finally, we apply the belief function to construct an attribute reduction algorithm, and an example is employed to illustrate the feasibility and validity of the algorithm. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						58	80		10.1016/j.ijar.2020.01.001													
J								Multilevel surrogate modeling approach for optimization problems with polymorphic uncertain parameters	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Surrogate model; Artificial neural network; Optimization; Polymorphic uncertainty; Reinforced concrete structure	STRUCTURAL RELIABILITY-ANALYSIS; NEURAL-NETWORKS	The solution of optimization problems with polymorphic uncertain data requires combining stochastic and non-stochastic approaches. The concept of uncertain a priori parameters and uncertain design parameters quantified by random variables and intervals is presented in this paper. Multiple runs of the nonlinear finite element model solving the structural mechanics with varying a priori and design parameters are needed to obtain a solution by means of iterative optimization algorithms (e.g. particle swarm optimization). The combination of interval analysis and Monte Carlo simulation is required for each design to be optimized. This can only be realized by substituting the nonlinear finite element model by numerically efficient surrogate models. In this paper, a multilevel strategy for neural network based surrogate modeling is presented. The deterministic finite element simulation, the stochastic analysis as well as the interval analysis are approximated by sequentially trained artificial neural networks. The approach is verified and applied to optimize the concrete cover of a reinforced concrete structure, taking the variability of material parameters and the structural load as well as construction imprecision into account. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						81	91		10.1016/j.ijar.2019.12.015													
J								A correctness result for synthesizing plans with loops in stochastic domains	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Plan and program synthesis; Stochastic domains; Loops in plans and programs; Stochastic algorithms; Planning in robotics		Finite-state controllers (FSCs), such as plans with loops, are powerful and compact representations of action selection widely used in robotics, video games and logistics. There has been steady progress on synthesizing FSCs in deterministic environments, but the algorithmic machinery needed for lifting such techniques to stochastic environments is not yet fully understood. While the derivation of FSCs has received some attention in the context of discounted expected reward measures, they are often solved approximately and/or without correctness guarantees. In essence, that makes it difficult to analyze fundamental concerns such as: do all paths terminate, and do the majority of paths reach a goal state? In this paper, we present new theoretical results on a generic technique for synthesizing FSCs in stochastic environments, allowing for highly granular specifications on termination and goal satisfaction. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						92	107		10.1016/j.ijar.2019.12.005													
J								Extending characteristic relations on an incomplete data set by the three-way decision theory	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Thinking in threes; Characteristic relation; Characteristic set; Maximal consistent block; Maximal characteristic neighborhood system; Local definability	ATTRIBUTE REDUCTION; ROUGH SETS; APPROXIMATIONS; OPERATORS	The methods of mining incomplete data based on characteristic sets or characteristic relation have been intensively studied in recent years. With the development of related research, many modifications of the definition of characteristic relation have been proposed. However, few of them can be used for decision rule induction due to the so-called definability problem. In this paper, by using the wide sense of the three-way decision theory, we extend the notions of characteristic relation and characteristic set to the systems with four types of characteristic relations and characteristic sets, respectively. Then we study the probabilistic approximations based on the extended characteristic set system. Moreover, we extend the maximal characteristic neighborhood system into four types of maximal characteristic neighborhood systems, and investigate the probabilistic approximations based on them. Finally, we generalize the definition of local definability for incomplete data processing. In particular, several existing methods based on characteristic sets are integrated into our model, and the new types of characteristic relations proposed by us seem more practical than the classical one in some aspects. In this way, our model illustrates the effectiveness and comprehensiveness of thinking in threes. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						108	121		10.1016/j.ijar.2019.12.011													
J								Accelerator for supervised neighborhood based attribute reduction	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Accelerator; Approximation quality; Attribute reduction; Conditional entropy; Supervised neighborhood relation; Neighborhood rough set	ROUGH SET MODEL; FEATURE-SELECTION; CONDITIONAL ENTROPY; DECISION-MAKING; INFORMATION; APPROXIMATION; UNCERTAINTY; GRANULATION; SYSTEMS	In neighborhood rough set, radius is a key factor. Different radii may generate different neighborhood relations for discriminating samples. Unfortunately, it is possible that two samples with different labels are regarded as indistinguishable, mainly because the neighborhood relation does not always provide satisfactory discriminating performance. Moreover, it should be noticed that the process of obtaining reducts in terms of multiple different radii is very time-consuming, mainly because different radii imply different reducts and those reducts should be searched, respectively. To solve the above problems, not only a supervised neighborhood relation is proposed for obtaining better discriminating performance, but also an accelerator is designed to speed up the process of obtaining reducts. Firstly, both intra-class radius and inter-class radius are proposed to distinguish samples. Different from the previous approaches, the labels of samples are taken into account and then this is why our approach is referred to as the supervised neighborhood based strategy. Secondly, from the viewpoint of the variation of radius, an accelerator is designed which aims to quickly obtain multiple radii based reducts. Such mechanism is based on the consideration that the reduct in terms of the previous radius may guide the process of obtaining the reduct in terms of the current radius. The experimental results over 12 UCI data sets show the following: 1) compared with the traditional and pseudolabel neighborhood based reducts, our supervised neighborhood based reducts can provide higher classification accuracies; 2) our accelerator can significantly reduce the elapsed time for obtaining reducts. This study suggests new trends for considering neighborhood rough set related topics. (C) 2019 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						122	150		10.1016/j.ijar.2019.12.013													
J								Modeling agent's conditional preferences under objective ambiguity in Dempster-Shafer theory	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Anscombe-Aumann acts; Conditional Choquet expected value; Belief and plausibility functions; Ambiguity; Conditional preferences	LOWER PROBABILITIES; EXPECTED UTILITY; BELIEF FUNCTIONS; COHERENCE; REPRESENTATION; IGNORANCE; RISK	We manage decisions under "objective" ambiguity by considering generalized Anscombe-Aumann acts, mapping states of the world to generalized lotteries on a set of consequences. A generalized lottery is modeled through a belief function on consequences, interpreted as a partially specified randomizing device. Preference relations on these acts are given by a decision maker focusing on different scenarios (conditioning events). We provide a system of axioms which are necessary and sufficient for the representability of these "conditional preferences" through a conditional functional parametrized by a unique full conditional probability P on the algebra of events and a cardinal utility function u on consequences. The model is able to manage also "unexpected" (i.e., "null") conditioning events and distinguishes between a systematically pessimistic or optimistic behavior, either referring to "objective" belief functions or their dual plausibility functions. Finally, an elicitation procedure is provided, reducing to a Quadratically Constrained Linear Program (QCLP). (C) 2020 Published by Elsevier Inc.																	0888-613X	1873-4731				APR	2020	119						151	176		10.1016/j.ijar.2019.12.019													
J								A multiple attribute decision making three-way model for intuitionistic fuzzy numbers	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decisions; Relative loss functions; Intuitionistic fuzzy numbers; Multiple attribute decision making	AGGREGATION OPERATORS; FRAMEWORK; CRITERIA; SUPPORT	In order to use three-way decision (TWD) to solve multiple attribute decision making (MADM) problems, in this article, a new TWD model with intuitionistic fuzzy numbers (IFNs) is proposed. First of all, we define the relative loss functions to demonstrate some features of loss functions in TWDs, which is the basis for future research. Then, based on the correlation between the loss functions and the IFNs, we get the relative loss functions based on IFNs. At the same time, the classification rules of the TWDs are discussed from different viewpoints, including the thresholds and their properties. Aiming at MADM problems with unreasonable values, a new integrated method of relative loss functions is established to obtain a fairer loss integration result of alternatives. In addition, considering that there are no decision attributes and only condition attributes in MADM, we use grey relational degree to calculate the condition probability. In the end, a novel TWD model is proposed to solve MADM problems with IFNs, and a practical example on selecting suppliers is used to demonstrate its effectiveness and practicability. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						177	203		10.1016/j.ijar.2019.12.020													
J								Probabilistic abstract argumentation frameworks, a possible world view	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Abstract argumentation frameworks; Probabilistic abstract argumentation frameworks; Normal forms; Probabilistic cliques; Probabilistic logic programming	LOGIC; SEMANTICS	After Dung's founding work in Abstract Argumentation Frameworks there has been a growing interest in extending the Dung's semantics in order to describe more complex or real life situations. Several of these approaches take the direction of weighted or probabilistic extensions. One of the most prominent probabilistic approaches is that of constellation Probabilistic Abstract Argumentation Frameworks. In this paper, we first make the connection of possible worlds and constellation semantics; we then introduce the probabilistic attack normal form for the constellation semantics; we furthermore prove that the probabilistic attack normal form is sufficient to represent any Probabilistic Abstract Argumentation Framework of the constellation semantics; then we illustrate its connection with Probabilistic Logic Programming and briefly present an existing implementation. The paper continues by also discussing the probabilistic argument normal form for the constellation semantics and proves its equivalent properties. Finally, this paper introduces a new probabilistic structure for the constellation semantics, namely probabilistic cliques. (C) 2019 Published by Elsevier Inc.																	0888-613X	1873-4731				APR	2020	119						204	219		10.1016/j.ijar.2019.12.006													
J								Partially observable game-theoretic agent programming in Golog	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Agent programming; Cognitive robotics; Multi-agent systems; Reasoning about actions and change		In this paper, we present the agent programming language POGTGolog (Partially Observable Game-Theoretic Golog), which integrates explicit agent programming in Golog with game-theoretic multi-agent planning in partially observable stochastic games. In this framework, we assume one team of cooperative agents acting under partial observability, where the agents may also have different initial belief states and not necessarily the same rewards. POGTGolog allows for specifying a partial control program in a high-level logical language, which is then completed by an interpreter in an optimal way. To this end, we define a formal semantics of POGTGolog programs in terms of Nash equilibria, and we then specify a POGTGolog interpreter that computes one of these Nash equilibria. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						220	241		10.1016/j.ijar.2019.12.017													
J								A rule-based framework for risk assessment in the health domain	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Depression; Falls; Formal ontology; Logical rule-based system; Missing data; Risk assessment	CARDIOVASCULAR RISK; LATE-LIFE; DEPRESSION; FALLS; EPIDEMIOLOGY; PEOPLE; METAANALYSIS; PERFORMANCE; PREVENTION; ONTOLOGY	Risk assessment is an important decision support task in many domains, including health, engineering, process management, and economy. There is a growing interest in automated methods for risk assessment. These methods should be able to process information efficiently and with little user involvement. Currently, from the scientific literature in the health domain, there is availability of evidence-based knowledge about specific risk factors. On the other hand, there is no automatic procedure to exploit this available knowledge in order to create a general risk assessment tool which can combine the available quantitative data about risk factors and their impact on the corresponding risk. We present a Framework for the Assessment of Risk of adverse Events (FARE) and its first concrete applications FRAT-up and DRAT-up, which were used for fall and depression risk assessment in older persons and validated on four and three European epidemiological datasets, respectively. FARE consists of i) a novel formal ontology called On2Risk; and ii) a logical and probabilistic rule-based model. The ontology was designed to represent qualitative and quantitative data about risks in a general, structured and machine-readable manner so that this data may be concretely exploited by risk assessment algorithms. We describe the structure of the FARE model in the form of logic and probabilistic rules. We show how when starting from machine-readable data about risk factors, like the data contained in On2Risk, an instance of the algorithm can be automatically constructed and used to estimate the risk of an adverse event. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						242	259		10.1016/j.ijar.2019.12.018													
J								On properties of a new decomposable entropy of Dempster-Shafer belief functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Shannon's entropy; Dempster-Shafer theory of belief functions; Decomposable entropy of belief functions; Compound distributions property; Conditional entropy; Strong probability consistency	MATHEMATICAL-THEORY; UNCERTAINTY; INFORMATION; AMBIGUITY	We define entropy of belief functions in the Dempster-Shafer (D-S) theory that satisfies a compound distributions property that is analogous to the property that characterizes Shannon's definitions of entropy and conditional entropy for probability mass functions. None of the existing definitions of entropy for belief functions in the D-S theory satisfy this property. We describe some important properties of our definition, and discuss its semantics as a measure of dissonance and not uncertainty. Finally, we compare our definition of entropy with some other definitions that are similar to ours in the sense that these definitions measure dissonance and not uncertainty. Published by Elsevier Inc.																	0888-613X	1873-4731				APR	2020	119						260	279		10.1016/j.ijar.2020.01.004													
J								Strategy selection under entropy measures in movement-based three-way decision	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Trisecting-acting-outcome; Three-way decision; Probabilistic movement model; Cross entropy; Conditional entropy	THEORETIC ROUGH SETS; ATTRIBUTE REDUCTION; MODEL	The trisecting-acting-outcome (TAO) model of three-way decisions comprises three components, i.e., constructing a trisection, strategies for action, and measuring the outcome. Existing research seldom involves the third component, i.e., measuring the effectiveness of the outcome. A previous study provides a strategy select method by analyzing benefits or costs in the movement-based three-way decision. However, we often choose a strategy depend on the probability or distribution of three regions instead of the benefits or costs. In this paper, we propose a probabilistic movement model of three-way decision and give a strategy selection mechanism based on information entropy. Conditional entropy and cross entropy are used to select strategies by measuring these probabilities according to the user's preferences and structural features of the three partitions. It selects the action or strategy which has the maximum probability of desirable movements based on the minimization of the conditional entropy. It also selects the action or strategy which has the least difference between actual probabilistic distributions and expected probabilistic distribution based on the minimization of the cross entropy. We give two examples to illustrate the practicality and effectiveness of the strategy selection method in the probabilistic movement model of three-way decision. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						280	291		10.1016/j.ijar.2020.01.013													
J								The three-way-in and three-way-out framework to treat and exploit ambiguity in data	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decision; Uncertainty; Ambiguity; Partial labels; Machine learning	THEORETIC ROUGH SET; DECISION; IMPRECISE	In this paper, we address ambiguity, intended as a characteristic of any data expression for which a unique meaning cannot be associated by the computational agent for either lack of information or multiple interpretations of the same configuration. In particular, we will propose and discuss ways in which a decision-support classifier can accept ambiguous data and make some (informative) value out of them for the decision maker. Towards this goal we propose a set of learning algorithms within what we call the three-way-in and three-way-out approach, that is, respectively, learning from partially labeled data and learning classifiers that can abstain. This approach is based on orthopartitions, as a common representation framework, and on three-way decisions and evidence theory, as tools to enable uncertain and approximate reasoning and inference. For both the above learning settings, we provide experimental results and comparisons with standard Machine Learning techniques, and show the advantages and promising performances of the proposed approaches on a collection of benchmarks, including a real-world medical data set. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						292	312		10.1016/j.ijar.2020.01.010													
J								The relationships between topologies and generalized rough sets	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Binary relation; Rough set; Separation axiom; Topology; Transitive closure	REFLEXIVE	Topology can be used as a tool for knowledge discovery in databases and has been studied intensively in rough set theory. Topology and binary relations can be induced from each other. In this paper, we mainly study topologies induced by arbitrary binary relations. We first consider the elementary properties of such topologies, show that the relation induced by the topology tau(R) from a relation R is exactly the transitive closure of I boolean OR R, where I is the relation of equality, and that the topology induced by the relation R-tau from a topology tau is exactly tau if and only if tau is quasi-discrete. Then we examine special topological properties of tau(R) induced by a binary relation R. In particular, we obtain equivalent descriptions of separation axioms and provide characterizations of topological properties such as compactness, second countability and connectedness. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						313	324		10.1016/j.ijar.2020.01.011													
J								Incremental reasoning in probabilistic Signal Temporal Logic	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Knowledge representation; Stream reasoning; Incremental reasoning; Probabilistic logic; Temporal logic; Runtime verification		Robot safety is of growing concern given recent developments in intelligent autonomous systems. For complex agents operating in uncertain, complex and rapidly-changing environments it is difficult to guarantee safety without imposing unrealistic assumptions and restrictions. It is therefore necessary to complement traditional formal verification with monitoring of the running system after deployment. Runtime verification can be used to monitor that an agent behaves according to a formal specification. The specification can contain safety-related requirements and assumptions about the environment, environment-agent interactions and agent-agent interactions. A key problem is the uncertain and changing nature of the environment. This necessitates requirements on how probable a certain outcome is and on predictions of future states. We propose Probabilistic Signal Temporal Logic (ProbSTL) by extending Signal Temporal Logic with a sub-language to allow statements over probabilities, observations and predictions. We further introduce and prove the correctness of the incremental stream reasoning technique progression over well-formed formulas in ProbSTL. Experimental evaluations demonstrate the applicability and benefits of ProbSTL for robot safety. (C) 2020 The Authors. Published by Elsevier Inc.																	0888-613X	1873-4731				APR	2020	119						325	352		10.1016/j.ijar.2020.01.009													
J								On distributive laws between 2-uninorms and overlap (grouping) functions	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Uninorms; 2-Uninorms; Distributivity equation; Overlap function; Grouping function	CONDITIONAL DISTRIBUTIVITY; INTERVAL OVERLAP; UNI-NULLNORMS; T-NORMS; UNINORM	Since Aczel studied the distributive law between two operations, the distributive laws among different binary aggregation functions become an interesting and vast research field. This paper is mainly devoted to solving the distributivity equations between 2-uninorms and overlap(grouping) functions, which are two classes of special aggregation functions found to possess wide applications in image processing, decision making, classification and fuzzy detection problems. We obtain the sufficient and necessary conditions of the distributivity equations between five classes of basic 2-uninorms and overlap(grouping) functions. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						353	372		10.1016/j.ijar.2020.01.008													
J								Generalized transitivity: A systematic comparison of concepts with an application to preferences in the Babington Smith model	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Stochastic transitivity; T-transitivity; Cycle-transitivity; E-transitivity; Reciprocal relations; Babington Smith model	CYCLE-TRANSITIVITY; RECIPROCAL RELATIONS; FUZZY; RANKING	Reciprocal relations are binary relations Q with entries Q(i, j) is an element of [0,1], and such that Q(i, j) + Q(j, i) = I. Relations of this kind occur quite naturally in various domains, such as preference modeling and preference learning. For example, Q(i, j) could be the fraction of voters in a population who prefer candidate i to candidate j. In the literature, various attempts have been made at generalizing the notion of transitivity to reciprocal relations. In this paper, we compare three important frameworks of generalized transitivity: g-stochastic transitivity, T-transitivity, and cycle-transitivity. To this end, we introduce E-transitivity as an even more general notion. We also use this framework to extend an existing hierarchy of different types of transitivity. As an illustration, we study transitivity properties of probabilities of pairwise preferences, which are induced as marginals of an underlying probability distribution on rankings (strict total orders) of a set of alternatives. In particular, we analyze the interesting case of the so-called Babington Smith model, a parametric family of distributions of that kind. (C) 2020 Published by Elsevier Inc.																	0888-613X	1873-4731				APR	2020	119						373	407		10.1016/j.ijar.2020.01.007													
J								A decoupled design approach for complex systems under lack-of-knowledge uncertainty	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Epistemic uncertainty; Systems engineering; Concurrent engineering; Early design phase; Crashworthiness	SPACES	This paper introduces a special approach for the design of complex systems in early development stages accounting for lack-of-knowledge uncertainty. As complex systems have to be broken down into their components by a decoupling methodology, the presented work regards so-called box-shaped solution spaces as subsets of the total set of permissible designs not violating any design constraints. Hereby, the design variables are decoupled and their design intervals are maximized. Then, each aspect related to the corresponding interval can be studied independently in a subsequent development step by different stakeholders (design groups or designers). Especially in early design phases, the consideration of uncertainty is crucial; this is not so much related to aleatoric uncertainty as probability functions are often unavailable. More important and more difficult to handle is epistemic uncertainty, i.e., lack-of-knowledge uncertainty. Here, uncertainties which occur later in the development and the facts that the current design stage does not include smaller design features and that the available models represent only coarsely the later designs are important. This paper complements prior work by providing a complete methodology for relevant uncertainties. This includes uncertainties in controllable design variables as well as in uncontrollable parameters all captured by interval arithmetic. Furthermore, it extends existing worst-case approaches by best-case approaches. The user can now base design decisions on (a) a deterministic solution space without consideration of lack-of-knowledge uncertainties, (b) the same with consideration of uncertainties in uncontrollable parameters only or controllable variables only, or (c) the most complete approach where uncertainties in both controllable variables and uncontrollable parameters are considered. The corresponding scenarios are exemplified via examples from automotive engineering (design for crashworthiness). (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				APR	2020	119						408	420		10.1016/j.ijar.2020.01.006													
J								An agent for learning new natural language commands	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Human-agent interaction; Human-computer interaction; Agents learning from humans; Natural language processing; Machine learning	TASK	Teaching via natural language is an intuitive way for end users to add functionality to a virtual assistant, enabling them to personalize their assistant with new commands without requiring the intervention of the system developer, who cannot possibly anticipate all of an end user's needs. In this paper we introduce our Learning by Instruction Agent (LIA), the first virtual assistant, for an email domain, that is capable of learning how to perform new commands taught by end users in natural language. LIA grounds the semantics of each command in terms of primitive executable procedures. When a user provides LIA with a command that it does not understand, it prompts the user to explain the command through a sequence of natural language steps. From this input, LIA learns the meaning of the new command and how to generalize the command to novel situations. For example, having been taught how to "forward an email to Alice", it can correctly understand "forward this email to Bob". We show that users that were assigned to interact with LIA completed the task quicker than users assigned to interact with a non-learning agent. These results demonstrate the potential of natural language teaching to improve the capabilities of intelligent personal assistants. We annotated 4759 natural language statements with their associated computer readable execution commands (logical forms) to form a dataset (which we publicize in this paper). We present the performance of several different parser methods on this dataset.																	1387-2532	1573-7454				APR	2020	34	1							6	10.1007/s10458-019-09425-x													
J								Strategyproof multi-item exchange under single-minded dichotomous preferences	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Housing markets; Kidney exchange; Organ exchange; Strategyproofness; Pareto optimality; Individual rationality	CHOICE	We consider multi-item exchange markets in which agents want to receive one of their target bundles of resources. The model encompasses well-studied markets for kidney exchange, lung exchange, and multi-organ exchange. We identify a general and sufficient condition called weak consistency for the exchange mechanisms to be strategyproof even if we impose any kind of distributional, diversity, or exchange cycle constraints. Within the class of weakly consistent and strategyproof mechanisms, we highlight two important ones that satisfy constrained Pareto optimality and strong individual rationality. Several results in the literature follow from our insights. We also derive impossibility results when constrained Pareto optimality is defined with respect to more permissive individual rationality requirements.																	1387-2532	1573-7454				APR	2020	34	1							3	10.1007/s10458-019-09426-w													
J								Agents teaching agents: a survey on inter-agent transfer learning	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multiagent learning; Transfer learning; Reinforcement learning		While recent work in reinforcement learning (RL) has led to agents capable of solving increasingly complex tasks, the issue of high sample complexity is still a major concern. This issue has motivated the development of additional techniques that augment RL methods in an attempt to increase task learning speed. In particular, inter-agent teaching-endowing agents with the ability to respond to instructions from others-has been responsible for many of these developments. RL agents that can leverage instruction from a more competent teacher have been shown to be able to learn tasks significantly faster than agents that cannot take advantage of such instruction. That said, the inter-agent teaching paradigm presents many new challenges due to, among other factors, differences between the agents involved in the teaching interaction. As a result, many inter-agent teaching methods work only in restricted settings and have proven difficult to generalize to new domains or scenarios. In this article, we propose two frameworks that provide a comprehensive view of the challenges associated with inter-agent teaching. We highlight state-of-the-art solutions, open problems, prospective applications, and argue that new research in this area should be developed in the context of the proposed frameworks.																	1387-2532	1573-7454				APR	2020	34	1							9	10.1007/s10458-019-09430-0													
J								Strategic negotiations for extensive-form games	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Automated negotiations; Non-zero-sum games; Extensive-form games; General game playing; Monte Carlo tree search		When studying extensive-form games it is commonly assumed that players make their decisions individually. One usually does not allow the possibility for the players to negotiate their respective strategies and formally commit themselves to future moves. As a consequence, many non-zero-sum games have been shown to have equilibrium outcomes that are suboptimal and arguably counter-intuitive. For this reason we feel there is a need to explore a new line of research in which game-playing agents are allowed to negotiate binding agreements before they make their moves. We analyze what happens under such assumptions and define a new equilibrium solution concept to capture this. We show that this new solution concept indeed yields solutions that are more efficient and, in a sense, closer to what one would expect in the real world. Furthermore, we demonstrate that our ideas are not only theoretical in nature, but can also be implemented on bounded rational agents, with a number of experiments conducted with a new algorithm that combines techniques from Automated Negotiations, (Algorithmic) Game Theory, and General Game Playing. Our algorithm, which we call Monte Carlo Negotiation Search, is an adaptation of Monte Carlo Tree Search that equips the agent with the ability to negotiate. It is completely domain-independent in the sense that it is not tailored to any specific game. It can be applied to any non-zero-sum game, provided that its rules are described in Game Description Language. We show with several experiments that it strongly outperforms non-negotiating players, and that it closely approximates the theoretically optimal outcomes, as defined by our new solution concept.																	1387-2532	1573-7454				APR	2020	34	1							2	10.1007/s10458-019-09424-y													
J								The complexity of bribery and control in group identification	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Computational social choice; Algorithms; Group identification; Bribery; Control; Complexity	APPROVAL; HARD	The goal of this paper is to analyze the complexity of constructive/destructive bribery and destructive control in the framework of group identification. Group identification applies to situations where a group of individuals determine who among them are socially qualified. We consider consent rules, the consensus-start-respecting rule, and the liberal-start-respecting rule. Each consent rule is characterized by two positive integers s and t, and the socially qualified individuals are determined as follows. If an individual qualifies herself, then she is socially qualified if and only if there are in total at least s individuals qualifying her. Otherwise, she is NOT socially qualified if and only if there are in total at least t individuals disqualifying her. The liberal (resp. consensus)-start-respecting rule determines the socially qualified individuals recursively. In the first step, all individuals qualifying themselves (resp. qualified by all individuals) are socially qualified. Then, the procedure recursively adds individuals who are not socially qualified but are qualified by at least one socially qualified individual into the set of socially qualified individuals until no one can be added this way.																	1387-2532	1573-7454				APR	2020	34	1							8	10.1007/s10458-019-09427-9													
J								Probabilistic physical search on general graphs: approximations and heuristics	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Graph search; Planning under uncertainty; Stochastic search	EXPLORATION; AGENTS	We consider an agent seeking to obtain an item, potentially available at different locations in a physical environment. The traveling costs between locations are known in advance, but there is only probabilistic knowledge regarding the possible prices of the item at any given location. Given such a setting, the problem is to find a plan that maximizes the probability of acquiring the good while minimizing both travel and purchase costs. Sample applications include agents in search-and-rescue or exploration missions, e.g., a rover on Mars seeking to mine a specific mineral. These probabilistic physical search problems have been previously studied, but we present the first approximation and heuristic algorithms for solving such problems on general graphs. We establish an interesting connection between these problems and classical graph-search problems, which led us to provide the approximation algorithms and hardness of approximation results for our settings. We further suggest several heuristics for practical use, and demonstrate their effectiveness with simulation on a real graph structure.																	1387-2532	1573-7454				APR	2020	34	1							1	10.1007/s10458-019-09423-z													
J								Crossmodal attentive skill learner: learning in Atari and beyond with audio-video inputs	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Hierarchical learning; Reinforcement learning; Multimodal learning		This paper introduces the Crossmodal Attentive Skill Learner (CASL), integrated with the recently-introduced Asynchronous Advantage Option-Critic architecture [Harb et al. in When waiting is not an option: learning options with a deliberation cost. arXiv preprint arXiv:1709.04571, 2017] to enable hierarchical reinforcement learning across multiple sensory inputs. Agents trained using our approach learn to attend to their various sensory modalities (e.g., audio, video) at the appropriate moments, thereby executing actions based on multiple sensory streams without reliance on supervisory data. We demonstrate empirically that the sensory attention mechanism anticipates and identifies useful latent features, while filtering irrelevant sensor modalities during execution. Further, we provide concrete examples in which the approach not only improves performance in a single task, but accelerates transfer to new tasks. We modify the Arcade Learning Environment [Bellemare et al. in J Artif Intell Res 47:253-279, 2013] to support audio queries (ALE-audio code available at https://github.com/shayegano/Arcade-Learning-Environment), and conduct evaluations of crossmodal learning in the Atari 2600 games H.E.R.O. and Amidar. Finally, building on the recent work of Babaeizadeh et al. [in: International conference on learning representations (ICLR), 2017], we open-source a fast hybrid CPU-GPU implementation of CASL (CASL code available at https://github.com/shayegano/CASL).																	1387-2532	1573-7454				APR	2020	34	1							16	10.1007/s10458-019-09439-5													
J								Stable outcomes in modified fractional hedonic games	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Coalition formation games; Hedonic games; Nash equilibrium; Core; Price of anarchy; Price of stability	COALITION STRUCTURE GENERATION; STABILITY	In coalition formation games self-organized coalitions are created as a result of the strategic interactions of independent agents. In this paper we assume that for each couple of agents (i, j), weight w(i, j) = w(j,i) reflects how much agents i and j benefit from belonging to the same coalition. We consider the (symmetric) modified fractional hedonic game, that is a coalition formation game in which agents' utilities are such that the total benefit of agent i belonging to a coalition (given by the sum of wi, j over all other agents j belonging to the same coalition) is averaged over all the other members of that coalition, i.e., excluding herself. Modified fractional hedonic games constitute a class of succinctly representable hedonic games. We are interested in the scenario in which agents, individually or jointly, choose to form a new coalition or to join an existing one, until a stable outcome is reached. To this aim, we consider common stability notions leading to strong Nash stable outcomes, Nash stable outcomes or core stable outcomes: we study their existence, complexity and performance, both in the case of general weights and in the case of 0-1 weights. In particular, we completely characterize the existence of the considered stable outcomes and show many tight or asymptotically tight results on the performance of these natural stable outcomes for modified fractional hedonic games, also highlighting the differences with respect to the model of fractional hedonic games, in which the total benefit of an agent in a coalition is averaged over all members of that coalition, i.e., including herself.																	1387-2532	1573-7454				APR	2020	34	1							4	10.1007/s10458-019-09431-z													
J								Solving the fair electric load shedding problem in developing countries	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Load shedding; Fairness; Constrained optimization	SUSTAINABLE DEVELOPMENT; CASCADING FAILURES; ENERGY; NIGERIA; MODELS	Often because of limitations in generation capacity of power stations, many developing countries frequently resort to disconnecting large parts of the power grid from supply, a process termed load shedding. This leaves households in disconnected parts without electricity, causing them inconvenience and discomfort. Without fairness being taken into due consideration during load shedding, some households may suffer more than others. In this paper, we solve the fair load shedding problem (FLSP) by creating solutions which connect households to supply based on some fairness criteria (i.e., to fairly connect homes to supply in terms of duration, their electricity needs, and their demand), which we model as their utilities. First, we briefly describe some state-of-art household-level load shedding heuristics which meet the first criteria. Second, we model the FLSP as a resource allocation problem, which we formulate into twoMixed Integer Programming (MIP) problems based on the Multiple Knapsack Problem. In so doing, we use the utilitarian, egalitarian and envy-freeness social welfare metrics to develop objectives and constraints that ensure our FLSP solutions results in fair allocations that consider the utilities of agents. Then, we solve the FLSP and show that our MIP models maximize the groupwise and individual utilities of agents, and minimize the differences between their pairwise utilities under a number of experiments. When taken together, our endeavour establishes a set of benchmarks for fair load shedding schemes, and provide insights for designing fair allocation solutions for other scarce resources.																	1387-2532	1573-7454				APR	2020	34	1							12	10.1007/s10458-019-09428-8													
J								Multi-objective multi-agent decision making: a utility-based analysis and survey	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multi-agent systems; Multi-objective decision making; Multi-objective optimisation criteria; Solution concepts; Reinforcement learning	PARETO EQUILIBRIA; COMPREHENSIVE SURVEY; MULTICRITERIA; OPTIMIZATION; MANAGEMENT; NETWORKS; SYSTEMS; GAMES; NEGOTIATION; PERFORMANCE	The majority of multi-agent system implementations aim to optimise agents' policies with respect to a single objective, despite the fact that many real-world problem domains are inherently multi-objective in nature. Multi-objective multi-agent systems (MOMAS) explicitly consider the possible trade-offs between conflicting objective functions. We argue that, in MOMAS, such compromises should be analysed on the basis of the utility that these compromises have for the users of a system. As is standard in multi-objective optimisation, we model the user utility using utility functions that map value or return vectors to scalar values. This approach naturally leads to two different optimisation criteria: expected scalarised returns (ESR) and scalarised expected returns (SER). We develop a new taxonomy which classifies multi-objective multi-agent decision making settings, on the basis of the reward structures, and which and how utility functions are applied. This allows us to offer a structured view of the field, to clearly delineate the current state-of-the-art in multi-objective multi-agent decision making approaches and to identify promising directions for future research. Starting from the execution phase, in which the selected policies are applied and the utility for the users is attained, we analyse which solution concepts apply to the different settings in our taxonomy. Furthermore, we define and discuss these solution concepts under both ESR and SER optimisation criteria. We conclude with a summary of our main findings and a discussion of many promising future research directions in multi-objective multi-agent systems.																	1387-2532	1573-7454				APR	2020	34	1							10	10.1007/s10458-019-09433-x													
J								Truthfulness on a budget: tradingmoney for approximation through monitoring	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Budget-feasible mechanisms; Auctions; Frugality; Set systems; Facility location; Obnoxious facility location	MECHANISMS; FRUGALITY; LOCATION; DESIGN; PRICE	Albeit a pervasive desideratum when computing in the presence of selfish agents, truthfulness typically imposes severe limitations to what can be implemented. The price of these limitations is typically paid either economically, in terms of the financial resources needed to enforce truthfulness, or algorithmically, in terms of restricting the set of implementable objective functions, which often leads to renouncing optimality and resorting to approximate allocations. In this paper, with regards to utilitarian problems, we ask two fundamental questions: (i) what is the minimum sufficient budget needed by optimal truthful mechanisms, and (ii) whether it is possible to sacrifice optimality in order to achieve truthfulness with a lower budget. To answer these questions, we connect two streams of work on mechanism design and look at monitoring-a paradigm wherein agents' actual costs are bound to their declarations. In this setting, we prove that the social cost is always a sufficient budget, even for collusion-resistant mechanisms, and, under mild conditions, also a necessary budget for a large class of utilitarian problems that encompass set system problems. Furthermore, for two well-studied problems outside of this class, namely facility location and obnoxious facility location, we draw a novel picture about the relationship between (additive) approximation and frugality. While for optimal mechanisms we prove that the social cost is always a sufficient and necessary budget for both problems, for approximate mechanisms we do have a dichotomy: for the facility location problem (i.e., agents want to be close to the facilities) we show that "good" approximations still need a budget equal to the social cost; on the contrary, for the obnoxious facility location problem (i.e. agents want to be as far away from the facilities as possible) we show that it is possible to trade approximation for frugality, thus obtaining truthfulness with a lower budget.																	1387-2532	1573-7454				APR	2020	34	1							5	10.1007/s10458-019-09435-9													
J								A collaborative agent-based traffic signal system for highly dynamic traffic conditions	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Intelligent transportation system; Multi-agent systems; Coordinated decision making	NETWORKS	In this paper we present DALI, a distributed, collaborative multi-agent traffic signal timing system (TST) for highly dynamic traffic conditions. In DALI, intersection controllers are augmented with software agents which collaboratively adapt signal timings by considering the feedback of all controller agents that may be affected by a change. The model is based on a real-world TST and will be deployed with minimal changes to the infrastructure. DALI has been validated by traffic engineers as well as through extensive simulation of the City of Richardson's traffic network, comprising 128 signalized intersections. The experimental results show that, in highly dynamic scenarios, DALI outperforms the conventional traffic system used by the city as well as a state-of-the-art reinforcement learning-based TST.																	1387-2532	1573-7454				APR	2020	34	1							17	10.1007/s10458-019-09434-w													
J								Bounds and dynamics for empirical game theoretic analysis	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Empirical games; Asymmetric games; Replicator dynamics		This paper provides several theoretical results for empirical game theory. Specifically, we introduce bounds for empirical game theoretical analysis of complexmulti-agent interactions. In doing so we provide insights in the empirical meta game showing that a Nash equilibrium of the estimated meta-game is an approximate Nash equilibrium of the true underlying meta-game. We investigate and show how many data samples are required to obtain a close enough approximation of the underlying game. Additionally, we extend the evolutionary dynamics analysis of meta-games using heuristic payoff tables (HPTs) to asymmetric games. The state-of-the-art has only considered evolutionary dynamics of symmetric HPTs in which agents have access to the same strategy sets and the payoff structure is symmetric, implying that agents are interchangeable. Finally, we carry out an empirical illustration of the generalised method in several domains, illustrating the theory and evolutionary dynamics of several versions of the AlphaGo algorithm (symmetric), the dynamics of the Colonel Blotto game played by human players on Facebook (symmetric), the dynamics of several teams of players in the capture the flag game (symmetric), and an example of a meta-game in Leduc Poker (asymmetric), generated by the policy-space response oracle multi-agent learning algorithm.																	1387-2532	1573-7454				APR	2020	34	1							7	10.1007/s10458-019-09432-y													
J								Bilevel optimization based on iterative approximation of multiple mappings	JOURNAL OF HEURISTICS										Bilevel optimization; Evolutionary algorithms; Stackelberg games; Mathematical programming	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHM; MATHEMATICAL PROGRAMS; OPTIMALITY CONDITIONS; MODEL; IMPLEMENTATION; LOCATION; DESIGN	A large number of application problems involve two levels of optimization, where one optimization task is nested inside the other. These problems are known as bilevel optimization problems and have been studied by both classical optimization community and evolutionary optimization community. Most of the solution procedures proposed until now are either computationally very expensive or applicable to only small classes of bilevel optimization problems adhering to mathematically simplifying assumptions. In this paper, we propose an evolutionary optimization method that tries to reduce the computational expense by iteratively approximating two important mappings in bilevel optimization; namely, the lower level rational reaction mapping and the lower level optimal value function mapping. The algorithm has been tested on a large number of test problems and comparisons have been performed with other algorithms. The results show the performance gain to be quite significant. To the best knowledge of the authors, a combined theory-based and population-based solution procedure utilizing mappings has not been suggested yet for bilevel problems.																	1381-1231	1572-9397				APR	2020	26	2					151	185		10.1007/s10732-019-09426-9													
J								Basic variable neighborhood search for the minimum sitting arrangement problem	JOURNAL OF HEURISTICS										Sitting arrangement problem; Variable neighborhood search; Basic variable neighborhood search; Graph embedding; Linear layout	DIVERSIFICATION; INTENSIFICATION	The minimum sitting arrangement (MinSA) problem is a linear layout problem consisting in minimizing the number of errors produced when a signed graph is embedded into a line. This problem has been previously tackled by theoretical and heuristic approaches in the literature. In this paper we present a basic variable neighborhood search (BVNS) algorithm for solving the problem. First, we introduce a novel constructive scheme based on the identification of cliques from the input graph, when only the positive edges are considered. The solutions obtained by the constructive procedure are then used as a starting point for the proposed BVNS algorithm. Efficient implementations of the several configurations of the local search procedure within the BVNS are described. The algorithmic proposal is then compared with previous approaches in the state of the art for the MinSA over different sets of referred instances. The obtained results supported by non-parametric statistical tests, indicate that BVNS can be considered as the new state-of-the-art algorithm for the MinSA.																	1381-1231	1572-9397				APR	2020	26	2					249	268		10.1007/s10732-019-09432-x													
J								Logistics optimization for a coal supply chain	JOURNAL OF HEURISTICS										Scheduling; Packing; Solver-independent modeling; Large neighborhood search; Rolling horizon; Custom search strategy; Tidal constraints	SOLVE; ALGORITHM; MINIZINC	The Hunter Valley coal export supply chain in New South Wales, Australia, is of great importance to the Australian economy. Effectively managing its logistics, however, is challenging, because it is a complex system, covering a large geographic area and comprising a rail network, three coal terminals, and a port, and has many stakeholders, e.g., mining companies, port authorities, coal terminal operators, rail infrastructure providers, and above rail operators. We develop a matheuristic logistics planning system which integrates, amongst other concerns, train scheduling, stockpile management, and vessel scheduling. Different components of the supply chain are modeled at different levels of granularity. An extensive computational study has generated insights into the bottlenecks in the logistics system, which are used to guide changes in operating policies and future investments. The planning system uses a solver-independent modeling technology. This allowed us to observe differences between the performance of constraint programming and mixed-integer programming in the context of a rolling-horizon approach, due to custom search heuristics.																	1381-1231	1572-9397				APR	2020	26	2					269	300		10.1007/s10732-019-09435-8													
J								Enhancement for human resource management in the ULD build-up process of air-cargo terminal: a strategic linkage approach	JOURNAL OF HEURISTICS										Backward removal heuristics; Linkage model; ULD build-up process; Workforce scheduling; Simulation	AIRCRAFT LINE MAINTENANCE; SIMULATION; WORKFORCE; MODELS	This study proposes a strategic linkage method with mathematical model, simulation model and heuristic approaches to an adaptive workforce scheduling problem reflecting the work site situation of the air cargo terminal. For the application of the proposed method, we first generate an initial workforce schedule using the optimization model for minimizing labor costs. The simulation model is then used to verify that the initial schedule is applicable on-site. If the initial plan does not meet the service level expressed as a delay in the departure of aircrafts, by linking optimization and simulation models, additional required personnel are determined for reducing the frequency of departure delay. But, though the experiments, we were able to confirm that the linkage of optimization and simulation alone resulted in the creation of excess manpower. To compensate for the limitation of the linkage, we develop the Backward Removal Heuristics (BRHs) algorithm. In an experiment with BRHs algorithm, we showed that BRHs could reduce the number of unnecessarily added workers based on the interaction between utilization of workers and operation congestion. With case study on the ULD build-up process of K airline air-cargo terminal, the linkage model with BRHs algorithm is tested and proven its effectiveness. The experimental results show that the workforce schedule from the linkage model increases the service level and the cost, but when applied to the BRHs algorithm, the model maintains the service level but decreases the cost.																	1381-1231	1572-9397				APR	2020	26	2					301	333		10.1007/s10732-020-09436-y													
J								Modeling the Complexity of Road Accidents Prevention: A System Dynamics Approach	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Causal; Complexity; Loop Diagrams; Non-Linear; Road Accidents; System Dynamics; Transport System	INJURIES; INCIDENTS; THINKING; DEATHS; HEALTH	Simplistic representations of traffic safety disregard the dynamic interactions between the components of the road transport system (RTS). The resultant road accident (RA) preventive measures are consequently focused almost solely on individual/team failures at the sharp end of the RTS (mainly the road users). The RTS is complex and therefore cannot be easily understood by studying the system parts in isolation. The study modeled the occurrence of road accidents in Uganda using the dynamic synthesis methodology (DSM). This article presents the work done in the first three stages of the DSM. Data was collected from various stakeholders including road users, traffic police officers, road users, and road constructors. The study focused on RA prevention by considering the linear and non-linear interactions of the variables during the pre-crash phase. Qualitative models were developed and from these, key leverage points that could possibly lower the road accident incidences demonstrating the need for a shared system wide responsibility for road safety at all levels are suggested.																	2160-9772	2160-9799				APR-JUN	2020	9	2					24	41		10.4018/IJSDA.2020040102													
J								A System Dynamics Model of Technology and Society: In the Context of a Developing Nation	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Causality; Information Society; Mobile Phones; Modelling; Policy; Social Construction Of Technology; System Dynamics		Kenya has emerged in recent times as one of the fastest growing telecom markets in the world. There is not a single, widely used paradigm which has synthesised the various schools and theories dealing with technology and society. This article argues that the issue of mobile technology on society is a complex technical and social phenomenon that needs to be understood from both ICT and social science perspectives. This study used the concept of governance socio-techno-economic systems as the theoretical framework. System dynamics are used as both the methodology and tool to model the mobile industry impact on society. The study shows that the increase in social capital intensity is an important source of the economic growth. This increase will strengthen the accelerator mechanism of the economy and creates larger multiplier effects. The increase in social capital intensity can be obtained through managing innovation processes base on the development of education and the R&D capacity of the nation.																	2160-9772	2160-9799				APR-JUN	2020	9	2					42	63		10.4018/IJSDA.2020040103													
J								Logistics Effectiveness Through Systems Thinking	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Approach; Application; Efficacy; Underlying Principle; Understanding	DYNAMICS	Various emerging concepts influence logistics management as scholars are developing the body of knowledge. So also, the progress and the multidisciplinary aspect of knowledge that has been influencing logistics management has changed the way scholars and researchers think about logistics as an arena of application. This, in turn, influences the logistics practices. There has been an incredible shift in organizations towards an inter-disciplinary approach where all functions of an organization interact towards the achievement of organizational objectives. This shift, therefore, calls for logistics to adapt to the emerging concepts in order to contribute meaningfully to the overall goals of the organizations. Hence, adopting a grounded theory approach with in-depth literature review this article endeavors to discuss the application of systems thinking the approach to logistics management.																	2160-9772	2160-9799				APR-JUN	2020	9	2					64	79		10.4018/IJSDA.2020040104													
J								Entrepreneurial innovative network and the design of socio-economic neural system	INTERNATIONAL JOURNAL OF SYSTEM DYNAMICS APPLICATIONS										Collective Decision-Making; Corporate Collaboration; Creativity; Information Laser; Innovation Network; Innovation; Neural Network; Self-Organization	CREATIVITY	This article aims to establish how an idea becomes an innovation and how creativity, collective dynamics, and information are interconnected. The results of the study showed that the emergence of innovations is closely connected with collective collaboration, and that it is impossible outside of group dynamics. The process of self-organization and collective decision-making is realized through a synergistic interaction, which then transforms into the so-called "information laser" and serves as a basis for the emergence of innovation. Both individuals (as persons and as separate entrepreneurs) are the elements of the innovation system and the actors of the artificial neural network-socioeconomic neural systems (SENS-systems). These systems act through self-organization and corporate collaboration, and the efforts of each element are amplified through the interaction with the other elements. The model of the SENS-systems can explain how the individual idea transforms into innovation and spreads throughout the world.																	2160-9772	2160-9799				APR-JUN	2020	9	2					80	102		10.4018/IJSDA.2020040105													
J								A Lagrangean based solution algorithm for the knapsack problem with setups	EXPERT SYSTEMS WITH APPLICATIONS										Knapsack problem with setups; Lagrangean relaxation		We consider the knapsack problem with setups which is a generalization of the classical knapsack problem where the items belong to families and an item can be placed in the knapsack only if its family is selected. The problem has received increasing attention by researchers because of its theoretical significance and practical applications related to resource allocation. This paper presents an algorithm based on a Lagrangean relaxation of the problem that produces solutions whose quality can be assessed automatically with the algorithm itself without ever knowing the optimal solutions. We report results of an extensive computational study which show that the method can solve near optimally very large instances of the problem with up to 500 families and 2,000.000 items in reasonable amount of time. This study shows the merit of using the Lagrangean relaxation method to solve the current problem when the constraints to relax are chosen properly. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113077	10.1016/j.eswa.2019.113077													
J								A neural network for semantic labelling of structured information	EXPERT SYSTEMS WITH APPLICATIONS										Semantic labelling; Information integration; Neural networks	TABLES; TAPON	Intelligent systems rely on rich sources of information to make informed decisions. Using information from external sources requires establishing correspondences between the information and known information classes. This can be achieved with semantic labelling, which assigns known labels to structured information by classifying it according to computed features. The existing proposals have explored different sets of features, without focusing on what classification techniques are used. In this paper we present three contributions: first, insights on architectural issues that arise when using neural networks for semantic labelling; second, a novel implementation of semantic labelling that uses a state-of-the-art neural network classifier which achieves significantly better results than other four traditional classifiers; third, a comparison of the results obtained by the former network when using different subsets of features, comparing textual features to structural ones, and domain-dependent features to domain-independent ones. The experiments were carried away with datasets from three real world sources. Our results show that there is a need to develop more semantic labelling proposals with sophisticated classification techniques and large features catalogues. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113053	10.1016/j.eswa.2019.113053													
J								Discovering generalized design knowledge using a multi-objective evolutionary algorithm with generalization operators	EXPERT SYSTEMS WITH APPLICATIONS										Design space exploration; Knowledge discovery; Feature extraction; Data mining; Multi-objective evolutionary algorithm; Adaptive operator selection	GENETIC ALGORITHM; ASSOCIATION RULES; CLASSIFICATION; OPTIMIZATION; CONSTRAINTS; PATTERNS	The early-phase design of complex systems is a challenging task, as a decision maker has to take into account the intricate relationships among different design variables. A popular way to help decision makers easily identify important design features is to use data mining. However, many of the existing algorithms output design features that are too complex (e.g., conjunction of many literals with unrelated predicates), making it difficult for a user to understand, remember, and apply these features to find better designs. In this paper, we introduce a new data mining method that extracts compact design features through knowledge generalization. The proposed method performs a search over the space of features using a multi-objective evolutionary algorithm that contains a set of generalization operators in addition to conventional evolutionary operators. Both variables and feature types are generalized by using an ontology defining a set of domain-specific concepts and relationships. Generalization leads to more compact and insightful features, as generalized knowledge encompasses wider concepts. A comparative experiment is conducted on a real-world system architecting problem to demonstrate the gain in compactness of the extracted features without significant reductions in predictive power. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113025	10.1016/j.eswa.2019.113025													
J								An efficient pattern growth approach for mining fault tolerant frequent itemsets	EXPERT SYSTEMS WITH APPLICATIONS										Fault tolerant frequent itemset mining; Frequent itemset mining; Pattern growth; Association rules mining	APPROXIMATE	Mining fault tolerant (FT) frequent itemsets from transactional databases are computationally more expensive than mining exact matching frequent itemsets. Previous algorithms mine FT frequent itemsets using Apriori heuristic. Apriori-like algorithms generate exponential number of candidate itemsets including the itemsets that do not exist in the database. These algorithms require multiple scans of database for counting the support of candidate FT itemsets. In this paper we present a novel algorithm, which mines FT frequent itemsets using frequent pattern growth approach (FT-PatternGrowth). FT-PatternGrowth adopts a divide-and-conquer technique and recursively projects transactional database into a set of smaller projected transactional databases and mines FT frequent itemsets in each projected database by exploring only locally frequent items. This mines the complete set of FT frequent itemsets and substantially reduces those candidate itemsets that do not exist in the database. FT-PatternGrowth stores the transactional database in a highly condensed much smaller data structure called frequent pattern tree (FP-tree). The support of candidate itemsets are counted directly from the FP-tree without scanning the original database multiple times. This improves the processing speed of algorithm. Our experiments on benchmark databases indicates mining FT frequent itemsets using FT-PatternGrowth is highly efficient than Apriori-like algorithms. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113046	10.1016/j.eswa.2019.113046													
J								Distracting users as per their knowledge: Combining linked open data and word embeddings to enhance history learning	EXPERT SYSTEMS WITH APPLICATIONS										Linked open data; Semantics; DBpedia SpotLight; YAGO; Word2Vec	RETRIEVAL-SYSTEM; BACKPROPAGATION; CLASSIFICATION; DBPEDIA	Organizations that preserve and promote heritage must meet the expectatives of sophisticated visitors who, far from wanting simply to be informed, desire to explore engaging and innovative technology-driven experiences which consider their particular interests and encourage them to discover more. We describe an approach based on quiz games that can be exploited in the deployment of such challenging experiences. The game consists of raising multiple-choice questions about a particular theme which is introduced by a Humanities expert through a brief narrative. Given the input text, a question and its right answer, our strategy provides the expert with a set of wrong alternatives (called distractors). These options are chosen from a (semi)automatically-built tailor-made corpus of documents by considering each player's level of knowledge on the game theme and exploiting Linked Open Data initiatives and natural language processing. On the one hand, automatic selection of distractors assists the Humanities expert to create games about very diverse topics without needing to be a specialist in all of them. On the other one, distractors are related to the right answer of each question in an appealing and meaningful way, which contributes to arouse the visitors' curiosity and their possible interest in exploring similar experiences in future visits. The work has been experimentally validated, achieving better results than a previous distractor identification strategy. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113051	10.1016/j.eswa.2019.113051													
J								Multiple writer retrieval systems based on language independent dissimilarity learning	EXPERT SYSTEMS WITH APPLICATIONS										Multi-Gradient elongated quinary patterns; Multiscale histogram of templates; Support vector machines; Training dissimilarities; Writer retrieval	FACE RECOGNITION; IDENTIFICATION; VERIFICATION; KHATT; COMBINATION; HISTOGRAM; PATTERNS; FEATURES	Retrieval based on query images supports interesting applications in handwritten document analysis, such as checking manuscripts originality, and authorship. In this respect, writer retrieval systems aim to automatically find all manuscripts belonging to the same author. Presently, we propose a new combination scheme for multiple writer retrieval systems that employ different features and dissimilarities. The proposed combination is founded on writer-independent, SVM dissimilarity learning. For experimental evaluation, three individual systems are proposed each of which, has its specific features. To develop the first system, we propose the Multiscale Histogram Of Templates (M-HOT). For the second system, we introduce the so-called Multi-Gradient Elongated Quinary Pattern (MG-EQP) as new descriptor for handwriting characterization. The third system uses the well-known Run Length Features. Retrieval tests are performed on CVL, ICDAR-2011, ICDAR-2013 and ICDAR-2017 datasets. Furthermore, to highlight the language-independence aspect, experiments are performed on KHATT dataset that contains Arabic handwritten documents. Results obtained evince the effectiveness of the proposed features as well as the combination scheme, which outperforms both individual systems and the state of the art. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113023	10.1016/j.eswa.2019.113023													
J								Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait	EXPERT SYSTEMS WITH APPLICATIONS										1D-Convnet; Parkinson; Gait; Classification; Deep learning	FEATURES; RHYTHM	Diagnosing Parkinson's disease is a complex task that requires the evaluation of several motor and non motor symptoms. During diagnosis, gait abnormalities are among the important symptoms that physicians should consider. However, gait evaluation is challenging and relies on the expertise and subjectivity of clinicians. In this context, the use of an intelligent gait analysis algorithm may assist physicians in order to facilitate the diagnosis process. This paper proposes a novel intelligent Parkinson detection system based on deep learning techniques to analyze gait information. We used 1D convolutional neural network (1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed model processes 18 1D-signals coming from foot sensors measuring the vertical ground reaction force (VGRF). The first part of the network consists of 18 parallel 1D-Convnet corresponding to system inputs. The second part is a fully connected network that connects the concatenated outputs of the 1D-Convnets to obtain a final classification. We tested our algorithm in Parkinson's detection and in the prediction of the severity of the disease with the Unified Parkinson's Disease Rating Scale (UPDRS). Our experiments demonstrate the high efficiency of the proposed method in the detection of Parkinson disease based on gait data. The proposed algorithm achieved an accuracy of 98.7%. To our knowledge, this is the state-of-the-start performance in Parkinson's gait recognition. Furthermore, we achieved an accuracy of 85.3% in Parkinson's severity prediction. To the best of our knowledge, this is the first algorithm to perform a severity prediction based on the UPDRS. These results show that the model is able to learn intrinsic characteristics from gait data and to generalize to unseen subjects, which could be helpful in a clinical diagnosis. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113075	10.1016/j.eswa.2019.113075													
J								Improving performances of Top-N recommendations with co-clustering method	EXPERT SYSTEMS WITH APPLICATIONS										Collaborative filtering; Co-clustering; Bipartite networks; Top-N recommendations		Collaborative filtering has been widely used in many applications. The typical idea is to identify preferences of users by utilizing their interaction data over the whole items. However, this is sometimes inaccurate since users might share different preferences with different sets of items. In this paper, we put forward a new recommendation method based on collaborative filtering called User-Item Community Detection based Recommendation (UICDR) method. The new parameter-free and scalable community detection method is modified from our previous work. We derive a unipartite form of bipartite modularity and put forward a new network representation. By constructing a bipartite network with user-item interaction data, we first partitions users and items into several subgroups. After getting clusters with tightly linked users and items, traditional collaborative filtering models can be trained for each cluster. The results on four real-world data sets show that, the proposed UICDR significantly improves the performances of Top-N recommendations of several traditional collaborative filtering methods. In addition, UICDR is helpful to cold-start problem. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113078	10.1016/j.eswa.2019.113078													
J								Oblique Decision Tree Ensemble via Twin Bounded SVM	EXPERT SYSTEMS WITH APPLICATIONS										Oblique; Ensemble; Decision trees; Random forest (RaF); Rotation forest (RoF); Twin bounded support vector machine (TBSVM)	SUPPORT VECTOR MACHINE; ROTATION FOREST; CLASSIFICATION; ROBUST; CLASSIFIERS	Ensemble methods with "perturb and combine" strategy have shown improved performance in the classification problems. Recently, random forest algorithm was ranked one among 179 classifiers evaluated on 121 UCI datasets. Motivated by this, we propose a new approach for the generation of oblique decision trees. At each non-leaf node, the training data samples are grouped in two categories based on the Bhattachrayya distance with randomly selected feature subset. Then, twin bounded support vector machine (TBSVM) is used to get two clustering hyperplanes such that each hyperplane is closer to data points of one group and as far as possible from the data points of other group. Based on these hyperplanes, each non-leaf node is splitted to generate the decision tree. In this paper, we used different base models like random forest (RaF), rotation forest (RoF), random sub rotation forest (RRoF) to generate the different oblique decision tree forests named as TBRaF, TBRoF and TBRRoF, respectively. In earlier oblique decision trees, like multisurface proximal support vector machine (MPSVM) based oblique decision trees, matrices are semi-positive definite and hence different regularization methods are required. However, no explicit regularization techniques need to be applied to the primal problems as the matrices in the proposed TBRaF, TBRoF and TBRRoF are positive definite. We evaluated the performance of the proposed models (TBRaF, TBRoF and TBRRoF) on 49 datasets taken from the UCI repository and on some real-world biological datasets (not in UCI). The experimental results and statistical tests conducted show that TBRaF and TBRRoF outperform other baseline methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113072	10.1016/j.eswa.2019.113072													
J								A feature transfer enabled multi-task deep learning model on medical imaging	EXPERT SYSTEMS WITH APPLICATIONS										Multi-task deep learning; Object detection; Segmentation; Classification; Medical imaging analysis	CONVOLUTIONAL NEURAL-NETWORK; CLASSIFICATION; MAMMOGRAMS; SEGMENTATION; MASS; CNN	Object detection, segmentation, and classification are three common tasks in medical image analysis. Multi-task deep learning (MTL) tackles these three tasks jointly, which provides two advantages-saving computational cost and improving robustness against overfitting. Existing multi-task deep models start with learning each task as an individual objective in parallel and then integrate the tasks at the end of the architecture with one cost function. Such architecture fails to take advantage of the combined power of the features from each individual task at an early stage of the training. In this research, we propose a new architecture, FT-MTL-Net, an MTL model enabled by feature transfer. Traditional transfer learning deals with the same or similar task (e.g., classification) from different data sources (a.k.a. domain). The underlying assumption is that the knowledge gained from various source domains may help the learning task on the target domain. Our proposed FT-MTL-Net utilizes the different tasks from the same domain. Considering that features from the tasks are different views of the domain, the combined feature maps can be well exploited using knowledge from multiple views to enhance the generalizability. To evaluate the validity of the proposed approach, FT-MTL-Net is compared with models from literature including eight classification models, four detection models, and three segmentation models using a publicly available Full Filed Digital Mammogram dataset for breast cancer diagnosis. Experimental results show that the proposed FT-MTL-Net outperforms the competing models in classification and detection and has comparable results in segmentation. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								112957	10.1016/j.eswa.2019.112957													
J								Lexicon-Grammar based open information extraction from natural language sentences in Italian	EXPERT SYSTEMS WITH APPLICATIONS										Open information extraction; Lexicon-Grammar; n-ary propositions; Natural language processing; Italian language	ACCEPTABILITY; SYSTEM	In the last decade, the quantity of readily accessible text has grown rapidly and enormously, long exceeding the capacity of humans to read and understand it. One of the most interesting strategies proposed to fulfill this need is known as Open Information Extraction (OIE). It is essentially devised to read in sentences and rapidly extract one or more domain-independent coherent propositions, each represented by a verb relation and its arguments. Even though many OIE approaches exist for English, no significant research has been conducted about OIE on Italian texts. Due to the usage of language-specific features, OIE systems operating in other languages are not directly applicable for Italian. Therefore, this paper proposes, as first contribution, a novel approach to perform OIE for Italian language, based on standard linguistic structures to analyze sentences and on a set of verbal behavior patterns to extract information from them. These patterns are built combining a solid linguistic theoretical framework, i.e. Lexicon-Grammar (LG), and distributional profiles extracted from a contemporary Italian corpus, i.e. itWaC. Starting from simple sentences, the approach is able to determine elementary tuples, then, all their permutations, by adding complements and adverbials, and, finally, n-ary propositions, by granting syntactic invariance, preserving the overall grammaticality and also respecting some syntactic constraints and selection preferences, thus approximating a first level of semantic acceptability. As second contribution of this work, a gold standard dataset for the Italian language has been built from the itWaC corpus, aimed at being widely used to enable the experimental validation of OIE solutions. It has been manually and independently labeled by four Italian native speakers with all the n-ary propositions that can be extracted, following the criteria of grammaticality and acceptability, i.e. granting syntactic well-formedness and meaningfulness in the context. Finally, the proposed approach has been experimented and quantitatively validated on this gold standard dataset, also in comparison with an indirect approach translating input sentences and output propositions from Italian to English and vice versa and embedding an OIE approach for English, as well as with an OIE system for Italian previously presented by the authors. The results obtained have shown the effectiveness of the proposed approach in generating propositions with respect to these criteria of grammaticality and acceptability. Even if the approach has been evaluated for the Italian language, it is essentially based on linguistic resources produced by LG, which exist for many languages besides Italian and a representative corpus for the language under consideration. Given these premises, it has a general basis from a methodological perspective and can be proficiently extended also to other languages. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								112954	10.1016/j.eswa.2019.112954													
J								Keystroke dynamics obfuscation using key grouping	EXPERT SYSTEMS WITH APPLICATIONS										Behavioral biometrics; Keystroke dynamics; Hierarchical clustering	AUTHENTICATION; IDENTIFICATION; BIOMETRICS	Keystroke dynamics is one of the most widely adopted identity verification techniques in remote systems. It is based on modeling users' specific patterns of typing on the keyboard. When utilized in conjunction with the commonly used passwords, the use of keystroke dynamics can dramatically increase the level of security without interfering with the user experience. However, aspects of keystroke dynamics that applied on passwords, such as processing keystroke events and storing feature vectors or user models, can expose users to identity theft and a new set of privacy risks, thus questioning the added value of keystroke dynamics. In addition, common encryption techniques will be unable to mitigate these threats, since the user's behavior changes from one session to another. In this paper, we suggest key grouping as an obfuscation method to ensure keystroke dynamics privacy. When applied on the keystroke events, the key grouping dramatically reduces the possibility of password theft. To perform the key grouping optimally, we present a novel method which produces groups that can integrated with any keystroke dynamics algorithm. Our method divides the keys into groups using hierarchical clustering with dedicated statistical heuristics algorithm. We tested our method's key grouping output on five keystroke dynamics algorithms using a public dataset and managed to show a consistent improvement of up to 7% in the AUC over other, more intuitive key groupings and random key groupings. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113091	10.1016/j.eswa.2019.113091													
J								Real-time purchase behavior recognition system based on deep learning-based object detection and tracking for an unmanned product cabinet	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Object detection; Purchase behavior recognition; Unmanned product cabinet		We propose a system to recognize purchasing behavior by detecting and tracking products in real time using only camera sensors in an unmanned product cabinet. To detect and track products in real time, we focused on the simultaneous pre-processing of videos from multiple cameras for robust product detection. After synchronizing multiple videos, unnecessary frames with relatively little information are removed based on change detection. An object score is measured on a frame-by-frame basis to select the most significant frames. Next, the target products are detected and tracked in the selected frames. Finally, the purchasing behavior of the detected product is recognized based on the tracking information. These processes were used to design an end-to-end recognition framework. The contribution of this paper is significant in that by redesigning the existing deep neural networks a real-time integrated system for a practical application was successfully realized without any bottleneck from multi-camera inputs to final object recognition process. Furthermore, the proposed object detection network shows comparable performance with the state-of-the-art methods. We performed intensive experiments to evaluate pure object detection performance as well as to evaluate various purchase/return scenarios. For example, for a basic purchase/return scenario, the proposed system achieved about 92% or more accuracy, which can be the actual level of commercialization. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113063	10.1016/j.eswa.2019.113063													
J								Customized and knowledge-centric service design model integrating case-based reasoning and TRIZ	EXPERT SYSTEMS WITH APPLICATIONS										Service design; Theory of Inventive Problem-Solving (TRIZ); Case-Based Reasoning (CBR); Shopping navigation service	SYSTEM; INNOVATION; FRAMEWORK; CBR; ONTOLOGY	Aiming at accelerating customized innovative service design, a novel knowledge-centric innovative service design (KISD) model is proposed through integrating memory-oriented-method case-based reasoning (CBR) and non-memory-oriented-method theory of inventive problem solving (TRIZ) to generate abundant ideas efficiently. Based on KISD, three key design approach phases are integrated: domain requirement acquisition (DRA), knowledge-centric resolution generation (KRG), and customized design knowledge-reasoning (CDK) phases. In the DRA phase, customer knowledge hierarchy is adopted to elicit customer requirements. In the KRG phase, CBR and TRIZ contradiction analysis are conducted and innovation principles are generated for constructing a functional knowledge hierarchy. In the CDK phase, quality function deployment analysis is conducted to evaluate and determine suitable new service functions. The case study of designing and developing a new shopping navigation service system for a shopping mall are presented to demonstrate the approach in practice. An empirical verification is conducted to verify the feasibility of the proposed approach and the satisfaction of the service. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113062	10.1016/j.eswa.2019.113062													
J								DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction	EXPERT SYSTEMS WITH APPLICATIONS										Time series prediction; Spatio-temporal relationship; Attention mechanism; Dual-stage two-phase model; Deep attention network	MODELS	Long-term prediction of multivariate time series is still an important but challenging problem. The key to solve this problem is capturing (1) the spatial correlations at the same time, (2) the spatio-temporal relationships at different times, and (3) long-term dependency of the temporal relationships between different series. Attention-based recurrent neural networks (RNN) can effectively represent and learn the dynamic spatio-temporal relationships between exogenous series and target series, but they only perform well in one-step time prediction and short-term time prediction. In this paper, inspired by human attention mechanism including the dual-stage two-phase (DSTP) model and the influence mechanism of target information and non-target information, we propose DSTP-based RNN (DSTP-RNN) and DSTP-RNN-II respectively for long-term time series prediction. Specifically, we first propose the DSTP-based structure to enhance the spatial correlations between exogenous series. The first phase produces violent but decentralized response weight, while the second phase leads to stationary and concentrated response weight. Then, we employ multiple attentions on target series to boost the long-term dependency. Finally, we study the performance of deep spatial attention mechanism and provide interpretation. Experimental results demonstrate that the present work can be successfully used to develop expert or intelligent systems for a wide range of applications, with state-of-the-art performances superior to nine baseline methods on four datasets in the fields of energy, finance, environment and medicine, respectively. Overall, the present work carries a significant value not merely in the domain of machine intelligence and deep learning, but also in the fields of many applications. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113082	10.1016/j.eswa.2019.113082													
J								Simultaneous feature selection and heterogeneity control for SVM classification: An application to mental workload assessment	EXPERT SYSTEMS WITH APPLICATIONS										Support vector machines; Feature selection; Heterogeneity control; Mental workload; Group penalty functions	SUPPORT VECTOR MACHINES; COGNITIVE LOAD; CONJOINT; STRESS; INTELLIGENCE; PERFORMANCE; ALGORITHM; APNEA; COST	In this study, an expert system is presented for analyzing the mental workload of interacting with a mobile phone while facing common daily tasks. Psychophysiological signals were collected from various devices, each characterized by a different cost and obtrusiveness. To deal with user-level signal data, a support vector machine-based feature selection approach is proposed. Given the limited person-level information available, our goal was to construct robust models by pooling population-level information across users (as a heterogeneity control). A single optimization problem that combines four objectives is proposed: model, margin maximization, feature selection, and heterogeneity control. The costs of using the devices were estimated, leading to a decision tool that allowed experiment designers to evaluate the marginal benefit of using a given device in terms of performance and its cost. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								112988	10.1016/j.eswa.2019.112988													
J								Text document summarization using word embedding	EXPERT SYSTEMS WITH APPLICATIONS										Extractive summarization; Semantic summarization; Distributional semantics; Text summarization		Automatic text summarization essentially condenses a long document into a shorter format while preserving its information content and overall meaning. It is a potential solution to the information overload. Several automatic summarizers exist in the literature capable of producing high-quality summaries, but they do not focus on preserving the underlying meaning and semantics of the text. In this paper, we capture and preserve the semantics of text as the fundamental feature for summarizing a document. We propose an automatic summarizer using the distributional semantic model to capture semantics for producing high-quality summaries. We evaluated our summarizer using ROUGE on DUC-2007 dataset and compare our results with other four different state-of-the-art summarizers. Our system outperforms the other reference summarizers leading us to the conclusion that usage of semantic as a feature for text summarization provides improved results and helps to further reduce redundancies from the input source. (C) 2019 Published by Elsevier Ltd.																	0957-4174	1873-6793				APR 1	2020	143								112958	10.1016/j.eswa.2019.112958													
J								Efficient approach for incremental weighted erasable pattern mining with list structure	EXPERT SYSTEMS WITH APPLICATIONS										Data mining; Erasable patterns; Incremental mining; Weighted conditions; Pruning techniques	FREQUENT ITEMSETS; UTILITY ITEMSETS; FAST ALGORITHM	Erasable pattern mining is one of the important fields of frequent pattern mining. It diagnoses and solves the economic problems that arise in the manufacturing industry. The real-world database is continually accumulated over time, and each item has a different importance. Therefore, if we use conventional erasable pattern mining without considering the characteristics of the real-world database, less meaningful patterns can be extracted. Also, when mining a real-world database, the algorithm must be able to process operations quickly and efficiently. In this paper, in order to meet these requirements, we propose an algorithm which is implemented as a list structure for mining erasable patterns in an incremental database with weighted condition. Compared to existing state-of-the-art mining algorithms, the proposed algorithm performs pattern pruning by applying weighted condition to a dynamic database, so it extracts fewer candidate patterns and shows fast performance. We test our algorithms and the algorithms previously presented with various real datasets and synthetic datasets and obtained results such as run time, memory usage, scalability, and accuracy tests. By analyzing and comparing these experimental results, we show that the proposed algorithm has outstanding performance. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113087	10.1016/j.eswa.2019.113087													
J								An efficient stripped cover-based accelerator for reduction of attributes in incomplete decision tables	EXPERT SYSTEMS WITH APPLICATIONS										Rough set; Attribute reduction; Stripped cover; Attribute significance measure; Incomplete data	FEATURE-SELECTION METHOD; ROUGH SET APPROACH; UNCERTAINTY MEASURES; RULE ACQUISITION; ENTROPY; SYSTEMS; APPROXIMATIONS; ALGORITHM	Attribute reduction in incomplete decision tables plays an extremely important role in machine learning, data mining, pattern recognition, especially for experts and intelligent systems. Many different reducts have been given in the rough set approach to find a promising reduct. However, efficiently extracting a reduct from large-scale incomplete data sets is time-consuming and becomes a challenging research problem. Although researchers have spent a lot of efforts for improving the computational efficiency, most existing methods have quite high complexity and focus mostly on the positive region reduct. To accelerate the attribute reduction process, we firstly introduce in this paper a new concept of stripped covers. Then, we investigate vital properties of stripped covers as well as provide attribute significance measures. By using these measures, we propose an effective and efficient heuristic algorithm framework for fast computation of popular reduct types. It is also worthwhile to mention that our algorithm has better time complexity than existing methods. Furthermore, the performance of the proposed method is experimentally demonstrated across multiple real-world datasets and compared with the main state-of-the-art methods. The results showed that our method outperforms the compared methods in the terms of obtained reduct size, computational time and classification accuracy. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113076	10.1016/j.eswa.2019.113076													
J								Real-time biomechanical modeling of the liver using Machine Learning models trained on Finite Element Method simulations	EXPERT SYSTEMS WITH APPLICATIONS										Machine learning; Finite element method; Real time; Liver; Coherent point drift; Biomechanical modeling	DEFORMATIONS; FRAMEWORK	The development of accurate real-time models of the biomechanical behavior of different organs and tissues still poses a challenge in the field of biomechanical engineering. In the case of the liver, specifically, such a model would constitute a great leap forward in the implementation of complex applications such as surgical simulators, computed-assisted surgery or guided tumor irradiation. In this work, a relatively novel approach for developing such a model is presented. It consists in the use of a machine learning algorithm, which provides real-time inference, trained on tens of thousands of simulations of the biomechanical behavior of the liver carried out by the finite element method on more than 100 different liver geometries. Considering a target accuracy threshold of 3 mm for the Euclidean Error, four different scenarios were modeled and assessed: a single liver with an arbitrary force applied (99.96% of samples within the accepted error range), a single liver with two simultaneous forces applied (99.84% samples in range), a single liver with different material properties and an arbitrary force applied (98.46% samples in range), and a much more general model capable of modeling the behavior of any liver with an arbitrary force applied (99.01% samples in range for the median liver). The results show that the Machine Learning models perform extremely well on all the scenarios, managing to keep the Mean Euclidean Error under 1 mm in all cases. Furthermore, the proposed model achieves working frequencies above 100Hz on modest hardware (with frequencies above 1000Hz being easily achievable on more powerful GPUs) thus fulfilling the real-time requirements. These results constitute a remarkable improvement in this field and may involve a prompt implementation in clinical practice. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113083	10.1016/j.eswa.2019.113083													
J								A hybrid localization model using node segmentation and improved particle swarm optimization with obstacle-awareness for wireless sensor networks	EXPERT SYSTEMS WITH APPLICATIONS										Localization; Node segmentation; Obstruction; Particle swarm optimization; Wireless Sensor Networks	ALGORITHMS	Other than energy consumption, precision is of the utmost importance in node localization. Various wireless-sensor-network applications require the accurate information of sensor nodes' locations. For instance, an enemy intrusion detection system (e.g., geo-fencing) needs accurate sensor nodes' locations to detect where intruding enemies are located. As practical examples, forest fire, landslide, and water quality monitoring systems require the early identification of root causes' exact locations before they can widely spread. In general, range-based localization techniques often yield higher accuracies because the localization estimation can be directly derived from the distance between hops and can leverage received signal strength indicator (RSSI) values but require model approximation of various hops and distances as in range-free localization techniques. However, the important factor that affects the accuracies is sensor node positioning, especially when sensor nodes (SNs) are spread across areas filled with obstructions causing less localization accuracy. Due to the diffraction caused by obstructions, the approximate distances between pairs of anchor nodes and unknown nodes using RSSI can differ substantially from the actual values. This research, therefore, aims to improve sensor node localization in situations where SNs are in areas with obstructions. We propose a novel technique, node segmentation with improved particle swarm optimization (NS-IPSO) that divides SNs into segments to improve the accuracy of the estimated distances between pairs of anchor nodes and unknown nodes. First, we determine candidate sensor nodes that could potentially be used to segment anchor nodes in the area. Such sensor nodes (STs) are those on the shortest paths between anchor nodes that appear more often than the average appearances of all sensor nodes. Then, segment nodes (SMs, sensor nodes for segmenting the anchor nodes) are selected from all the other STs based on certain specified conditions. To further improve the localization precision, we enhance the fitness function for each anchor node by taking into account the number of hops between each anchor node and unknown nodes. Furthermore, we enhance particle swarm optimization (PSO) by considering only particles that do not change positions to possibly reduce the chance of the local optimal trap. In this research, we test our proposed scheme's performance considering three forms of sensor node positioning: C-shape, H-shape, and S-shape. The simulation results show that the proposed scheme achieves higher accuracy in comparison with the recent state-of-the-art methods, i.e., hybrid discrete PSO (HDPSO), Hybrid PSO, approximate distances node localization (ADNL), the weight-search localization algorithm (WSLA), and min-max PSO techniques, particularly the situation where sensor nodes are in areas with obstacles. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113044	10.1016/j.eswa.2019.113044													
J								Merged Tree-CAT: A fast method for building precise computerized adaptive tests based on decision trees	EXPERT SYSTEMS WITH APPLICATIONS										Computerized adaptive tests; Decision trees; Linear programming	ITEM-EXPOSURE CONTROL; REGRESSION TREES; R PACKAGE; QUALITY; ABILITY	Over the last few years, there has been an increasing interest in the creation of Computerized Adaptive Tests (CATs) based on Decision Trees (DTs). Among the available methods, the Tree-CAT method has been able to demonstrate a mathematical equivalence between both techniques. However, this method has the inconvenience of requiring a high performance cluster while taking a few days to perform its computations. This article presents the Merged Tree-CAT method, which extends the Tree-CAT technique, to create CATs based on DTs in just a few seconds in a personal computer. In order to do so, the Merged Tree-CAT method controls the growth of the tree by merging those branches in which both the distribution and the estimation of the latent level are similar. The performed experiments show that the proposed method obtains estimations of the latent level which are comparable to the obtained by the state-of-the-art techniques, while drastically reducing the computational time. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113066	10.1016/j.eswa.2019.113066													
J								Video on demand recommender system for internet protocol television service based on explicit information fusion	EXPERT SYSTEMS WITH APPLICATIONS										Internet protocol television; Over-the-top; Video-on-demand; Video on demand recommender system; Data sparsity problem; Information fusion	MODEL	Internet protocol television (IPTV) provides video on demand (VOD), internet service, and real-time broadcasting to users as a service that combines broadcasting and communication technology. Among various services, the sales of VOD are profitable because VODs offer relatively strong direct revenue models in IPTV services. However, the development of a VOD recommender system for IPTV service is highly challenging owing to the lack of explicit preference information of users in an IPTV environment. Previous studies for IPTV VOD recommender systems have attempted to solve the data sparsity problem through implicit preference information; however, it is better to utilize explicit preference information to improve the performance of system. Recently, IPTV service providers have provided their own over-the-top (OTT) services such that explicit preference information of users for items can be combined. Therefore, we proposed a novel information fusion method for an IPTV VOD recommender system that integrates the explicit information of both IPTV and OTT services. In addition, we utilized the probabilistic matrix factorization, that guarantees high performance in most recommender systems, as a recommender algorithm in this study. Finally, we conducted comparative evaluations based on various metrics and validated that the information fusion of IPTV and OTT services contribute to the IPTV VOD recommender system. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113045	10.1016/j.eswa.2019.113045													
J								Reward-driven U-Net training for obstacle avoidance drone	EXPERT SYSTEMS WITH APPLICATIONS										Autonomous drone; Reward-driven training; Actor Critic networks; U-Net; Policy gradient method; Reinforcement learning		Along with the fast progress in deep learning, an autonomous drone with obstacle avoidance capability has been studied mainly by two machine learning paradigms, Le. supervised learning, and reinforcement learning. The former has some advantages since the trained network is light and fast, but it needs a large amount of data that requires laborious manual labeling. With the latter, such a drawback can be overcome as an agent learns by itself in a simulated environment, although the gap between the real and simulated one has to be minimized in the end. This study proposes a new framework where a supervised segmentation network is trained with labels made by an actor-critic network in a reward-driven manner, wherein this U-Net based network infers the next moving direction from the sequence of input images. For the actor-critic part, several recent policy gradient algorithms have been tested for controlling the drone with the continuous action space. After training in the Airsim simulation environment, the model is transferred to a Bebop drone flying in the real environment, built as a reconfigurable maze using panels and a hoop. The result suggests that our network enables the drone to navigate through the obstacles using only monocular RGB input in the trained environment as well as in the reconfigured ones without retraining. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113064	10.1016/j.eswa.2019.113064													
J								Top-K interesting preference rules mining based on MaxClique	EXPERT SYSTEMS WITH APPLICATIONS										Maxclique; Association rules; Conditional preference rules; Belief system; Interestingness measure	ASSOCIATION RULES; UTILITY ITEMSETS; PATTERNS	In order to fully considered context constraints and eliminate the redundancy of preferences in the personalized queries in the database, a Top-K conditional preference mining framework based on maximal clique is proposed. Firstly, a conditional constraint model i(+) (sic) i(-)vertical bar X is proposed, which means that users prefer i(+) to i(-) under the constraint of X. Based on this model, we design a MPM algorithm to efficiently obtain users contextual preferences by using graphic model and new pruning strategies. Then, we propose an updated belief system and the concept of common belief. The belief system is constructed by using users common preferences, and combined with Bayesian approach to acquire users' Top-K preference rules. Finally, the experiment result indicates that MPM algorithm is far more efficient than three classic or leading-edge mining algorithms in mining all rules and has comparable performance with 2 state-of-the-art Top-K methods. Moreover, the experiment results on Recall, precision and F1-Measure show that MPM algorithm comprehensively outperforms than six kinds various algorithms. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113043	10.1016/j.eswa.2019.113043													
J								Detecting malware evolution using support vector machines	EXPERT SYSTEMS WITH APPLICATIONS										Maiware; Support vector machine; Feature analysis, malware evolution		Malware families typically evolve over a period of time. Differences between malware samples within a single family can originate from various code modifications designed to evade detection, or changes that are made to alter the functionality of the malware itself. Thus, malware samples from the same family from different time periods can exhibit significantly different behavior. In this research, we apply feature ranking-based on linear support vector machine (SVM) weights-to identify changes within malware families. We analyze numerous malware families over extended periods of time. Our goal is to demonstrate that we can detect evolutionary changes within malware families using an automated and quantifiable machine learning based technique. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113022	10.1016/j.eswa.2019.113022													
J								Portfolio formation with preselection using deep learning from long-term financial data	EXPERT SYSTEMS WITH APPLICATIONS										Asset preselection; Long-term financial data; Financial forecasting; Portfolio optimisation	SUPPORT VECTOR REGRESSION; STOCK SELECTION MODEL; NEURAL-NETWORK; HYBRID MODEL; MEAN-VARIANCE; OPTIMIZATION; PREDICTION; CONSTRUCTION; FRAMEWORK; RETURNS	Portfolio theory is an important foundation for portfolio management which is a well-studied subject yet not fully conquered territory. This paper proposes a mixed method consisting of long short-term memory networks and mean-variance model for optimal portfolio formation in conjunction with the asset preselection, in which long-term dependences of financial time-series data can be captured. The experiment uses a large volume of sample data from the UK Stock Exchange 100 Index between March 1994 and March 2019. In the first stage, long short-term memory networks are used to forecast the return of assets and select assets with higher potential returns. After comparing the outcomes of the long short-term memory networks against support vector machine, random forest, deep neural networks, and autoregressive integrated moving average model, we discover that long short-term memory networks are appropriate for financial time-series forecasting, to beat the other benchmark models by a very clear margin. In the second stage, based on selected assets with higher returns, the mean-variance model is applied for portfolio optimisation. The validation of this methodology is carried out by comparing the proposed model with the other five baseline strategies, to which the proposed model clearly outperforms others in terms of the cumulative return per year, Sharpe ratio per triennium as well as average return to the risk per month of each triennium. i.e. potential returns and risks. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113042	10.1016/j.eswa.2019.113042													
J								SOINN plus , a Self-Organizing Incremental Neural Network for Unsupervised Learning from Noisy Data Streams	EXPERT SYSTEMS WITH APPLICATIONS										Continuous learning; Catastrophic forgetting; Self-organizing incremental neural network; Non-stationary data; Data stream; Concept drift	ASSOCIATIVE MEMORY; CELL STRUCTURES; ENVIRONMENTS	The goal of continuous learning is to acquire and fine-tune knowledge incrementally without erasing already existing knowledge. How to mitigate this erasure, known as catastrophic forgetting, is a grand challenge for machine learning, specifically when systems are trained on evolving data streams. Self-organizing incremental neural networks (SOINN) are a class of neural networks that are designed for continuous learning from non-stationary data. Here, we propose a novel method, SOINN+, which is fundamentally different from the previous generations of SOINN with respect to how "forgetting" is modeled. To achieve a more graceful forgetting, we developed three new concepts: idle time, trustworthiness, and unutility of a node. SOINN+ learns a topology-preserving mapping of the input data to a network structure, which reveals clusters of arbitrary shapes in streams of noisy data. Our experiments with synthetic and real-world data sets showed that SOINN+ can maintain discovered structures even under sudden and recurring concept drifts. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113069	10.1016/j.eswa.2019.113069													
J								Portfolio management via two-stage deep learning with a joint cost	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Long short-term memory; Portfolio management; Joint cost function	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; PREDICTION; SELECTION; INVESTORS; MODEL; INDEX	Portfolio management is a series of processes that maximize returns and minimize risk by allocating assets efficiently. Along with the developments in machine learning technology, it has been studied to apply machine learning methods to prediction-based portfolio management. However, such methods have a few limitations. First, they do not consider the relations between assets for the prediction. In addition, the studies commonly focus on the prediction accuracy, neglecting the construction of portfolios. Furthermore, the methods have usually been evaluated with index data, which hardly represent actual prices to buy or sell an asset. To overcome these problems, Exchange Traded Funds (ETFs) are employed for base assets for the evaluation, and we propose a two-stage deep learning framework, called Grouped-ETFs Model (GEM), with a joint cost function. The GEM is designed to learn the features of inter-asset and groups in each stage. Also, the proposed joint cost can consider relative returns for the training while the relative returns are a crucial factor to construct a portfolio. The results of a rigorous evaluation with global ETF data indicate that the proposed GEM with the joint cost outperforms the equally weighted portfolio and the ordinary deep learning model by 33.7% and 30.1%, respectively. An additional experiment using sector ETFs verifies the generality of the proposed model where the results accord with those of the previous experiment. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113041	10.1016/j.eswa.2019.113041													
J								Acoustic scene classification using deep CNN with fine-resolution feature	EXPERT SYSTEMS WITH APPLICATIONS										Acoustic scene classification; Convolutional neural network; Lateral construction; Depth-wise separable convolution; Fine-resolution convolutional neural network		Convolutional neural networks with spectrogram feature representation for acoustic scene classification are attracting more and more attentions due to its favorable performance. However, most of the existing methods are still restricted to the tradeoff between the minimum coverage area across time-frequency feature representation, i.e. time-frequency feature resolution, and the depth of CNN models. Thus, it is unfeasible to improve the performance by simply deepening networks. In this paper, fine-resolution convolutional neural network (FRCNN) is proposed to embrace the progress in very deep architecture, feature fusion and convolutional operation. Specifically, lateral construction is applied to generate a fine-resolution feature map with semantic information, and depth-wise separable convolution is utilized to reduce the number of trainable parameters. Extensive experiments demonstrate that the proposed FRCNN exhibits high performance on several metrics, with low computational complexity. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 1	2020	143								113067	10.1016/j.eswa.2019.113067													
J								Hybridizing Bees Algorithm with Firefly Algorithm for Solving Complex Continuous Functions	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Bees Algorithm; Complex Continuous Functions; Firefly Algorithm; Foraging Algorithm; HBAFA; HFBA; Hybrid Bee Firefly Algorithm; Hybrid Firefly Bee Algorithm; Optimisation; Swami Intelligence	OPTIMIZATION; SWARM	In this article, two hybrid schemes using the Bees Algorithm (BA) and the Firefly Algorithm (FA) are presented for numerical complex problem resolution. The BA is a recent population-based optimization algorithm, which tries to imitate the natural behaviour of honey bees foraging for food. The FA is a swarm intelligence technique based upon the communication behaviour and the idealized flashing features of tropical fireflies. The first approach, called the Hybrid Bee Firefly Algorithm (HBAFA), centres on improvements to the BA with FA during the local search thus increasing exploitation in each research zone. The second one, namely the Hybrid Firefly Bee Algorithm (HFBA), uses FA in the initialization step for a best exploration and detection of promising areas in research space. The performance of the novel hybrid algorithms was investigated on a set of various benchmarks and compared with standard BA, and other methods found in the literature. The results show that the proposed algorithms perform better than the Standard BA, and confirm their effectiveness in solving continuous optimization functions.																	1947-8283	1947-8291				APR-JUN	2020	11	2					27	55		10.4018/IJAMC.2020040102													
J								Evolutionary Metaheuristics to Solve Multiobjective Assignment Problem in Telecommunication Network: Multiobjective Assignment Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Assignment Problem; Choquet Integral; Efficient Solution; Evolutionary Algorithm; Multi-Criteria Decision Aid; Multiobjective Combinatorial Problem; NSGA-II; Pareto Optimal Set; SPEA2	GENETIC ALGORITHM	The authors propose a computing approach for solving a multiobjective problem in the telecommunication network field, suggested by an Algerian industrial company. The principal goal is in developing a palliative solution to overcome some generated problems existing in the current management system. A mathematical operational model has been established. The exact algorithms that solve multiobjective optimization problems are not appropriate for large scale problems. However, the application of metaheuristics approach leads perfectly to approximate the Pareto optimal set. In this paper, the authors apply a well-known multiobjective evolutionary algorithm, the Non-dominated Sorting Genetic Algorithm (NSGA-II), compare the obtained results with those generated by the Strength Pareto Evolutionary Algorithm-II (SPEA2) and propose a way to help the decision maker, who is often confronted with the choice of a final solution, to make his preferences afterward using a utility function based on a Choquet integral measure. Finally, numerical experiments are presented to validate the approach.																	1947-8283	1947-8291				APR-JUN	2020	11	2					56	76		10.4018/IJAMC.2020040103													
J								Embedded Real-Time System for Traffic Sign Recognition on ARM Processor	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Advanced Driver Assistance Systems (ADAS); ARM processor; Detection; Raspberry Pi; Real-Time; Recognition; Road Traffic Sign	IDENTIFICATION; ALGORITHMS; DESIGN	This article proposes the design of a novel hardware embedded system used for automatic real-time road sign recognition. The algorithm used was implemented in two main steps. The first step, which detects the road signs, is performed by the maximally stable extremal region method on HSV color space. The second step enables the recognition of the detected signs by using the oriented fast and rotated brief features method. The novelty of the embedded hardware system, on an ARM processor, leads to a real-time implementation of the ADAS applications. The proposed system was tested on the Belgium Traffic Sign Detection and Recognition Benchmark and on the German Traffic Signs Datasets. The proposed approach attained a high detection and recognition rate with real-world situations. The achieved results are acceptable when compared to state-of-the-art systems.																	1947-8283	1947-8291				APR-JUN	2020	11	2					77	98		10.4018/IJAMC.2020040104													
J								An Adaptive Neuro-Based Fuzzy Inference System (ANFIS) for the Prediction of Option Price: The Case of the Australian Option Market	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Adaptive Network-Based Fuzzy Inference System (ANFIS); Binomial Tree; Finite Differential; Forecasting; Monte Carlo; Option Price; Particle Swami Optimization; Soft Computing	HEDGING DERIVATIVE SECURITIES; STOCHASTIC VOLATILITY; NETWORK; MODELS; RISK; SELECTION; AMERICAN; SCHOLES; BLACK	Option price prediction has been an important issue in the finance literature within recent years. Affected by numerous factors, option price forecasting remains a challenging problem. In this study, a novel hybrid model for forecasting option price consisting of parametric and non-parametric methods is presented. This method is composed of three stages. First, the conventional option pricing methods such as Binomial Tree, Monte Carlo, and Finite Difference are used to primarily calculate the option prices. Next, the author employs an Adaptive Neuro-Fuzzy Inference System (ANFIS) in which the parameters are trained with particle swarm optimization to minimize the prediction errors associated with parametric methods. To select the best input data for the ANFIS structure, which has high mutual information associated with the future option price, the proposed method uses an entropy approach. Experimental examples with data from the Australian options market demonstrate the effectivity of the proposed hybrid model in enhancing the prediction accuracy compared to another method.																	1947-8283	1947-8291				APR-JUN	2020	11	2					99	117		10.4018/IJAMC.2020040105													
J								Face Recognition Using RLDA Method Based on Mutated Cuckoo Search Algorithm to Extract Optimal Features	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Cuckoo Search (CS); Face Recognition; Genetic Algorithm (GA); Linear Discriminant Analysis (LDA); Mutation; Regularized-LDA (R-LDA)	BACTERIAL FORAGING OPTIMIZATION; LINEAR DISCRIMINANT-ANALYSIS; EIGENFACES; FISHER	Regularized-LDA (R-LDA) is one of the most successful holistic approaches that is introduced to overcome the "small sample size" (SSS) problem of the LDA method, which is often encountered in Face Recognition (FR) tasks. R-LDA is based on reducing the high variance of principal components of the within-class scatter matrix to optimize the regularized Fisher's criterion. In this article, the authors assume that some of these components do not have significant information and they can be discarded. To this end, the authors propose CS-RLDA that uses a Cuckoo search (CS) algorithm to select the optimal eigenvectors from a within-class matrix. However, the CS algorithm has a slow convergence speed. To deal with this problem, and to create more diversity and better trade-off between exploitation and exploration around the best solutions, the authors have modified the basic cuckoo algorithm by using a mutation operator. The experimental results performed on the ORL and UMIST databases indicate that the proposed method enhances the performance of FR.																	1947-8283	1947-8291				APR-JUN	2020	11	2					118	133		10.4018/IJAMC.2020040106													
J								Multi-Layer Distributed Constraint Satisfaction for Multi-criteria Optimization Problem: Multimodal Transportation Network Planning Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Assignment Problem; Distributed Approach; Multi-Agent System; Multi-Criteria Optimization Problem; Multi-Layer; Multimodal Transportation Network; Planning Problem	COLUMN GENERATION	This article introduces a new approach to solve the multimodal transportation network planning problem (MTNP). In this problem, the commodities must be transported from an international network by at least two different transport modes. The main purpose is to identify the best multimodal transportation strategy. The present contribution focuses on efficient optimization methods to solve MTNP. This includes the assignment and the scheduling problems. The authors split the MTNP into layered. Each layer is presented by an agent. These agents interact, collaborate, and communicate together to solve the problem. This article defines MTNP as a distributed constraint satisfaction multicriteria optimization problem (DCSMOP). This latter is a description of the constraint optimization problem (COP), where variables and constraints are distributed among a set of agents. Each agent can interact with other agents to share constraints and to distribute complementary tasks. Experimental results are the proof of this work efficiently.																	1947-8283	1947-8291				APR-JUN	2020	11	2					134	155		10.4018/IJAMC.2020040107													
J								Role of Regression Models in Bridge Pier Scour Prediction	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										FFNN; MARS; Prediction; Scour Depth	CLEAR-WATER SCOUR; CONTRACTION SCOUR; NEURAL-NETWORKS; DEPTH; RUNOFF; PERFORMANCE; BEHAVIOR	Scour monitoring is an important concern in the design of any hydraulic structure. This study introduces the application of regression models in the prediction of scour depth around a bridge pier. Feedforward Neural Network (FFNN) and Multivariate Adaptive Regression Spline (MARS) models have been developed using different flow parameters. The flow parameters taken into consideration are the flow depth, flow velocity, pier diameter, and Froude's number. The FFNN models with different combinations of input parameters along with a simultaneous variation in the number of hidden neurons were developed to further increase the prediction accuracy. The best combination of hidden neurons and input parameters was selected and compared with the developed MARS model. Further, these models were compared with the selected empirical models to find out the best possible model for bridge pier scour prediction. All the developed regression models and selected empirical models were compared using standard statistical performance evaluation measures such as Root Mean Square Error (RMSE), Nash-Sutcliffe Efficiency (NSE), Mean Absolute Percentage Error (MAPE) and Percentage BIAS (PBIAS). The FFNN model developed with 4-input parameters performed better compared with other combinations of input parameters. The performance indices of all developed models show that as the input parameter increases, prediction accuracy also increases. A superior prediction accuracy was observed with FFNN model with 4-input parameters compared to MARS model and other selected empirical models.																	1947-8283	1947-8291				APR-JUN	2020	11	2					156	170		10.4018/IJAMC.2020040108													
J								Environmental Economic Power Dispatch Using Bat Algorithm with Generalized Fly and Evolutionary Boundary Constraint Handling Scheme	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										BatAlgorithm With Generalized; BatAlgorithm (BA); Environment; Evolutionary Boundary Constraint Handling Scheme; Fuel Cost; Pollutant Emission; Power Dispatch; Walk Algorithm (BAG)	COMBINED HEAT; EMISSION DISPATCH; MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; SYSTEM; MODEL; FLOW	This article intends to resolve the evolving environmental economic power dispatching problem (EED) using an enhanced version of the bat algorithm (BA) which is the Bat Algorithm with Generalized Fly (BAG). A good solution based on the Evolutionary Boundary Constraint Handling Scheme rather than the well-known absorbing technique and a good choice of the bi-objective function are provided to maintain the advantages of such algorithms on this problem. In the first stage, an individual economic power dispatch problem is considered by minimizing the fuel cost and taking into account the maximum pollutant emission. In the second stage and after weighting soft constraints satisfaction maximization and hard constraints abuse penalties, the proposed approach of the bi-objective environmental and economic load dispatch was built on a pareto function. The approach was tested on a thermal power plant with 10 generators and an IEEE30 power system of 6 generators. The results on the two datasets compared to those of other methods show that the proposed technique yields better cost and pollutant emissions.																	1947-8283	1947-8291				APR-JUN	2020	11	2					171	191		10.4018/IJAMC.2020040109													
J								Population Based Techniques for Solving the Student Project Allocation Problem	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Ant Colony Optimization; Constraint Satisfaction; Genetic Algorithm; Gravitational Search Algorithm; Student Project Allocation	ALGORITHMS; COLONY	The student project allocation problem is a well-known constraint satisfaction problem that involves assigning students to projects or supervisors based on a number of criteria. This study investigates the use of population-based strategies inspired from physical phenomena (gravitational search algorithm), evolutionary strategies (genetic algorithm), and swarm intelligence (ant colony optimization) to solve the Student Project Allocation problem for a case study from a real university. A population of solutions to the Student Project Allocation problem is represented as lists of integers, and the individuals in the population share information through population-based heuristics to find more optimal solutions. All three techniques produced satisfactory results and the adapted gravitational search algorithm for discrete variables will be useful for other constraint satisfaction problems. However, the ant colony optimization algorithm outperformed the genetic and gravitational search algorithms for finding optimal solutions to the student project allocation problem in this study.																	1947-8283	1947-8291				APR-JUN	2020	11	2					192	207		10.4018/IJAMC.2020040110													
J								Improving ant colony optimization algorithm with epsilon greedy and Levy flight	COMPLEX & INTELLIGENT SYSTEMS										Ant colony optimization; Epsilon greedy; Levy flight; Levy distribution	PARTICLE SWARM OPTIMIZATION; CONVERGENCE; SEARCH; SYSTEM; PROOF	Ant colony optimization (ACO) algorithm is a meta-heuristic and reinforcement learning algorithm, which has been widely applied to solve various optimization problems. The key to improving the performance of ACO is to effectively resolve the exploration/exploitation dilemma. Epsilon greedy is an important and widely applied policy-based exploration method in reinforcement learning and has also been employed to improve ACO algorithms as the pseudo-stochastic mechanism. Levy flight is based on Levy distribution and helps to balance searching space and speed for global optimization. Taking advantage of both epsilon greedy and Levy flight, a greedy-Levy ACO incorporating these two approaches is proposed to solve complicated combinatorial optimization problems. Specifically, it is implemented on the top of max-min ACO to solve the traveling salesman problem (TSP) problems. According to the computational experiments using standard TSPLIB instances, greedy-Levy ACO outperforms max-min ACO and other latest TSP solvers, which demonstrates the effectiveness of the proposed methodology.																	2199-4536	2198-6053															10.1007/s40747-020-00138-3		MAR 2020											
J								Interval-Valued Hesitant Fuzzy Stochastic Decision-Making Method Based on Regret Theory	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Hesitant fuzzy stochastic decision-making; Interval-valued hesitant fuzzy elements; Group satisfaction degree; Regret theory	PREFERENCE RELATIONS; PROSPECT-THEORY; SETS; ENTROPY	Different studies show that human beings are usually limited rational, their regret aversion behavior is playing an important role in the process of stochastic decision-making. However, such psychological behavior is neglected in current studies. The interval-valued hesitant fuzzy sets can more effectively depict the uncertain information than hesitant fuzzy sets. Therefore, we propose an interval-valued hesitant fuzzy stochastic decision-making approach based on group satisfaction degree and regret theory. Firstly, based on the score and variance function, a novel group satisfaction degree is defined, which can fully reflect the overall level and group divergence. Secondly, the attribute weights optimization model based on the group satisfaction degree and the deviation of attribute values is constructed to obtain the weight vector of attributes. Then, the regret value and the rejoice value are obtained by the novel regret-rejoice function, and the alternatives are ranked according to the total psychological perception values of the decision maker. Finally, we illustrated the application of the developed method with an emergency decision-making problem. Sensitivity and comparative analyses are implemented to demonstrate the superiority, stability, and validity of the proposed method based on regret theory.																	1562-2479	2199-3211				JUN	2020	22	4					1091	1103		10.1007/s40815-020-00830-z		MAR 2020											
J								A New Hesitant Fuzzy-Based Forecasting Method Integrated with Clustering and Modified Smoothing Approach	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Hesitant fuzzy set; Fuzzy c-means clustering; Exponential smoothing; Aggregation operator; Fuzzy time series	TIME-SERIES; INTERVALS; AGGREGATION; ENROLLMENTS; ALGORITHM; LENGTHS; MODELS	This study provides a new fuzzy time series method of forecasting integrated with the theory of fuzzy clustering, exponential smoothing approach and hesitant fuzzy information. In most of the existing studies, intervals with equal and unequal lengths have been used individually to handle the complex decision-making problems. This paper adopts the Fuzzy c-means (FCM) clustering approach to partition the universe of discourse into both equal and unequal space intervals to improve the prediction quality. Hesitant fuzzy sets (HFSs) are developed and an aggregation operator is applied to aggregate the hesitant information. In next process, modified smoothing approach is implemented to defuzzify the historical data. To verify and validate the effectiveness of the proposed method, it is applied to forecast the benchmark data set: enrollments of the Alabama University. Empirical results signified the improved accuracy of the proposed method based on statistical accuracy measure techniques and it reveals that proposed method generates better forecasting outputs than all existing models.																	1562-2479	2199-3211				JUN	2020	22	4					1104	1117		10.1007/s40815-020-00829-6		MAR 2020											
J								Joint Production and Pricing Strategy in Robust Model of Crowdfunding Considering Uncertain Preference	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Reward-based crowdfunding; Uncertain preference; Robust optimization method; Production and pricing; Platform participation		While our society began to recognize the importance to capture the uncertain preference of people, the existing literature has confined its research work only under a static preference framework in crowdfunding. This paper represents the attempt to incorporate people uncertain preference on product quality and attributes into crowdfunding for production and pricing. More specifically, with describe the uncertain preference of supporters on product quality with robust optimization method, the distribution of the uncertain preference in the worst cases is solved firstly and the corresponding optimal product quality is investigated. Then, according to the different preference of supporters for product attributes, the differentiated pricing strategy is considered in the second stage. Finally, the model is extended to the situation that the third-party platform is participation to illustrate the effectiveness of platform on sharing economy. The research provides the sponsor with the quality level of product production and the corresponding optimal prices for different product quality and attributes. And the results show that: (i) It is optimal for sponsor to increase the unit price of the taste product and not to decide the product quality at the early stage when the risk of investors uncertain preference on product quality is greater than a certain value; (ii) Compare with the low effort level of the platform, the price of the taste product is lower when the platform effort level is high.																	1562-2479	2199-3211				JUN	2020	22	4					1342	1355		10.1007/s40815-020-00838-5		MAR 2020											
J								A Novel Three-Way Investment Decisions Based on Decision-Theoretic Rough Sets with Hesitant Fuzzy Information	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Decision-theoretic rough sets (DTRSs); Hesitant fuzzy sets (HFSs); Three-way decisions; Investment decisions	AGGREGATION; OPERATORS	As a classic model of three-way decisions, the decision-theoretic rough sets (DTRSs) have been adopted to help make profit-based investment decisions. However, there is epistemic uncertainty in the assessment of investment projects, hence the hesitant fuzzy sets (HFSs) are appropriate tool for characterizing the revenue functions and cost functions. To get a more reasonable result in the three-way investment decisions, we introduce HFSs to DTRSs and explore a new three-way investment decision model. Firstly, we take into account the revenue and cost of DTRSs with hesitant fuzzy elements and propose a hesitant fuzzy decision-theoretic rough sets (HFDTRSs) model. Then, we calculate the revenue functions and cost functions by aggregating hesitant fuzzy elements with the Bayesian decision procedure. Considering the differences in the personalities and attitudes of decision-makers, we propose optimistic strategy and pessimistic strategy to aggregate revenue functions and cost functions. Finally, based on the score of profit maximization, we make three-way investment decisions. A case study of coal investment was used to demonstrate the proposed methods.																	1562-2479	2199-3211															10.1007/s40815-020-00836-7		MAR 2020											
J								The Swiss army knife of time series data mining: ten useful things you can do with the matrix profile and ten lines of code	DATA MINING AND KNOWLEDGE DISCOVERY										Time series; Joins; Motif discovery; Anomaly detection	CLASSIFICATION; EARTHQUAKES; RECURRENCE; FAULT; MICROEARTHQUAKES; ALGORITHM; LOCATION; VOLCANO; SYSTEM; SLIP	The recently introduced data structure, the Matrix Profile, annotates a time series by recording the location of and distance to the nearest neighbor of every subsequence. This information trivially provides answers to queries for both time series motifs and time series discords, perhaps two of the most frequently used primitives in time series data mining. One attractive feature of the Matrix Profile is that it completely divorces the high-level details of the analytics performed, from the computational "heavy lifting." The Matrix Profile can be computed using the appropriate computational paradigm for the task at hand: CPU, GPU, FPGA, distributed computing, anytime computation, incremental computation, and so forth. However, all the details of such computation can be hidden from the analyst who only needs to think about her analytical need. In this work, we expand on this philosophy and ask the following question: If we assume that we get the Matrix Profile for free, what interesting analytics can we do, writing at most ten lines of code? As we will show, the answer is surprisingly large and diverse. Our aim here is not to establish or compete with state-of-the-art results, but merely to show that we can both reproduce the results of many existing algorithms and find novel regularities in time series data collections with very little effort.																	1384-5810	1573-756X				JUL	2020	34	4					949	979		10.1007/s10618-019-00668-6		MAR 2020											
J								Localization approach of FLC and ANFIS technique for critical applications in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Location and optimization of energy; Cluster head; FLC and ANFIS algorithm	ALGORITHM; HOP	Wireless Sensor Network (WSN) is an arrangement of sensor nodes to collect environmental data and send it to Base Station (BS). The major application of the sensor networks is monitoring the physical environmental conditions. The WSN system plays a critical role to select the optimal location, distance, and degree of specific data communicating between WSN and BS. The optimally located sensor senses the data and sends it to the BS. The communicated data is in accordance with the location and its degrees of distance, both distributions in transmitting data to BS requires high energy requirement. Hence, the main challenge of WSN is to optimize the location and reduce the date communication cost. In this paper, we have put forward our try to comparison with optimization algorithm fuzzy logic control (FLC) with Adaptive Network-Based FLC Inference System (ANFIS) for localization of sensor nodes in WSN. To identify the best in optimization WSNs location by a degree of distance of each node also cost of the network analysis based proposed algorithms. The simulation has shown that the combined approach of ANFIS has better performance over current approaches in terms of security and accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-01888-1		MAR 2020											
J								A framework for automatic detection of heart diseases using dynamic deep neural activation functions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activation function; Deep neural network; Gradient descent; Conjugate gradient; Quasi Newton; Levenberg Marquardt	DIAGNOSIS	Availability of the information related to the medical tests are wide and thus the demand for advanced analysis is continuously growing. In the recent past, a number of automatic disease detection algorithms are generated. The studies focus on various life threatening diseases such as cardiovascular diseases or cervical diseases or cancerous growth in tissues or non-malignant tumours. Nevertheless, considering various reports from the health organizations and government survey reports, the researchers have highest focus on cardiovascular diseases. The major area of focus is to determine or predict the possibilities of heart diseases based on predictive analysis. It is been learnt from the parallel research outcomes that the use of neural network based analytical frameworks are highly accurate for these purposes. The majority of the research attempts have demonstrated the use of single activation function for determining the predictive value. Nonetheless, the activation functions have significantly different properties and the effective selection of the activation function can generate higher accuracy. In the other hand ineffective selection of the activation functions, may also lead to incorrect determination of the disease and disease severity. Thus for a clinical decision support system, it is very crucial to have effective activation function in the framework. Hence, this work attempts to identify the characteristics of these activation functions and provides a framework for dynamic selection of the activation function based on the nature of the dataset. Also, the training strategies for any neural model play a major role for disease detection and prediction, which is again subjected to the time complexity of the learning function. Nevertheless, this work proposes to identify the optimal deep learning function by deploying the proposed framework. The major outcome of this work demonstrates nearly 96% accuracy for disease prediction due to the dynamic selection of the activation function and establishes the novelty of the research by defining the severity of the diseases.																	1868-5137	1868-5145															10.1007/s12652-020-01883-6		MAR 2020											
J								The distributed user trace collection and storage system based on interface window tree model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										User behavior; User trace collection; Interface window tree; Unique storage; Distributed storage; Web usage		There has been an exponential growth in the variety of information and it is significantly that users' accurate information is demanded on cloud storage. Actually retrieval the exact data is incompatible for the low effectiveness and most of the methods are in black box mode of data collection and analysis. This paper proposes an approach for data collection, storage and retrieval user interaction traces on the Internet based on model of interface window tree (IWT), and each user's access trajectory is an instance of the IWT. While user visits the web site, it deals with large volume of data and user operations. These operation sequence is actually records of the user's behavior. The model provide a unified distributed storage by establishing a storage directory tree for each storage server node which is relied on the data types of structured and unstructured. The experiment results show that the system not only has good performance in providing more accurate data collection but also solves the storage problem of complex data types, and retrieves user data and access interfaces with improved effectiveness.																	1868-5137	1868-5145															10.1007/s12652-020-01897-0		MAR 2020											
J								Heuristic optimization techniques for connecting renewable distributed generators on distribution grids	NEURAL COMPUTING & APPLICATIONS										Renewable distributed power generators; Voltage level; Power losses; Operating cost; Optimization methodology; Egyptian case study; Load levels	OPTIMAL PLACEMENT; DISTRIBUTION NETWORKS; DISTRIBUTION-SYSTEMS; MULTIOBJECTIVE OPTIMIZATION; OPTIMAL ALLOCATION; LOSS MINIMIZATION; MULTIPLE DGS; ALGORITHM; SEARCH; HYBRID	Proposing a high effective objective function by utilizing optimal weighting factors plays an important role in power systems to boost the quality, attitude, and efficiency of evaluating the position and capacity of renewable distributed generators (RDGs) optimally. This research introduces a comprehensive study of different effective objective functions. A comprehensive analysis between the most modern optimization techniques, like hybrid particle swarm optimization (PSO) with Quazi-Newton, hybrid PSO with gravitational search algorithm, grasshopper optimization algorithm, moth-flame optimization, and slap-swarm algorithm, is done in order to determine the best optimizer with respect to high performance, high accuracy, and the minimum convergence time. The best prepared methodology is proposed and compared with other modern techniques to validate its performance. The suggested scheme is exercised by studying the impact of the RDGs integration for 33 and 69 nodes of IEEE distribution grids, in addition to one of the Egyptian radial distribution networks as a practical case study within 24 h at different load levels. The numerical results confirmed the importance and usefulness of incorporating electing the efficient objective function and superior optimization algorithm in the power system to achieve a successful global optimum solution to ensure the power quality through enhanced voltage levels, while minifying the system power losses and the operating prices.																	0941-0643	1433-3058				SEP	2020	32	17					14195	14225		10.1007/s00521-020-04812-y		MAR 2020											
J								A novel trigonometric operation-based q-rung orthopair fuzzy aggregation operator and its fundamental properties	NEURAL COMPUTING & APPLICATIONS										q-rung orthopair; Sine trigonometric operations; Operational laws; Aggregation operators; group decision making	DECISION-MAKING METHOD; CORRELATION-COEFFICIENT; MEAN OPERATORS; LAWS; SETS	The q-rung orthopair fuzzy sets (q-ROFSs) are a prominent idea to express the fuzzy information in the decision-making process and are the generalization of the existing intuitionistic fuzzy set and Pythagorean fuzzy set. The q-ROFSs can dynamically adapt the information by changing the parameter q >= 1based on the membership degree and therefore support more innumerable possibilities. Driven by these requisite characteristics, this paper aspires to present some sine trigonometric operations laws for q-ROFSs. The sine trigonometry function preserves the periodicity and symmetric about the origin, and hence, it satisfies the decision-maker preferences toward the evaluation of the objects. Associated with these laws, we define a series of new aggregation operators named as sine trigonometry weighted averaging and geometric operators to aggregate the q-rung orthopair fuzzy information. The fundamental relations between the proposed operators are also examined. Afterward, we present a group decision-making technique to solve the multiple attribute group decision-making problems based on proposed operators and illustrate with a numerical example to verify it. The superiors, as well as the advantages of the proposed operators, are also discussed in it. Lastly, the influence of the membership degrees on the operations has been investigated and found that when the parameter q increases from 2 to 4 and then from 4 to 7, then there is the certain change in the range of the score values.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15077	15099		10.1007/s00521-020-04859-x		MAR 2020											
J								Hybrid method for mining rules based on enhanced Apriori algorithm with sequential minimal optimization in healthcare industry	NEURAL COMPUTING & APPLICATIONS										Context ontology; Enhanced Apriori algorithm (EAA); Healthcare system; Sequential minimal optimization (SMO) regression; Wireless sensor network	WIRELESS SENSOR NETWORKS; PLATFORM; SYSTEM; MODEL	Data mining may enable healthcare organizations, with analysis of the different prospects and connection between seemingly unrelated information, to anticipate trends in the patient's medical condition and behavior. Raw data are large and heterogeneous from healthcare organizations. It needs to be collected and arranged, and its integration enables medical information systems to be integrated in a united way. Health data mining offers unlimited possibilities to evaluate numerous less obvious or secret data models utilizing common techniques for study. Association rule mining (ARM) is an effective technique for detecting the connection of the data which are the most commonly used and influential algorithms in ARM for an Apriori algorithm. However, it generates a large amount of rules and does not guarantee the efficiency and value of the knowledge created. In order to overcome this issue, an enhanced Apriori algorithm (EAA) based on the knowledge of a context ontology (EAA-SMO) methodology for sequential minimal optimization (SMO) is suggested. The simple knowledge is to establish the ideas of ontology as a hierarchical structure of the conceptual clusters of specific subjects, which comprises "similar" concepts that mean an exact category of the knowledge within the domain. There is an interesting rule for each cluster based on the correlation between the items. In addition, the rule developed is classified as a prediction model for anomaly detection based on SMO regression. The experimental analysis demonstrates the proposed method improved 2% of accuracy and minimizes the execution time by 25% when compared to semantic ontology.																	0941-0643	1433-3058															10.1007/s00521-020-04862-2		MAR 2020											
J								Artificial bee colony optimization-inspired synergetic study of fractional-order economic production quantity model	SOFT COMPUTING										Fractional-order differential equation; EPQ model; Laplace transformation; ABC algorithm	ALGORITHM; EQUATIONS; DERIVATIVES; CREDIT	Inventory control is one of the most widely recognized issues in the reality. This investigation manages the utilization of fractional derivatives and integration on an inventory control problem. The memory of a dynamical model is a highly concerned issue which is commonly neglected by the models described in terms of integer-order differential equation. The memory capturing the power of fractional derivative (in Caputo's sense) is utilized here to describe an economic production quantity model with deterioration when the demand depends on price and stock and production is stock dependent. Also, this study covers the integer-order model with the same assumptions as a memoryless model and a particular case of the fractional model. Due to the complex nature of the model, numerical optimization with the help of a modified artificial bee colony algorithm is done instead of the analytical approach of optimization. Finally, we have performed a sensitivity analysis in order to make a fruitful conclusion.																	1432-7643	1433-7479				OCT	2020	24	20					15341	15359		10.1007/s00500-020-04867-y		MAR 2020											
J								Fully convolutional networks for chip-wise defect detection employing photoluminescence images Efficient quality control in LED manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Fully convolutional networks; Deep learning; Photoluminescence images; Chip-wise prediction; Defect cluster detection; LED manufacturing; Quality control; Industrial application		Efficient quality control is inevitable in the manufacturing of light-emitting diodes (LEDs). Because defective LED chips may be traced back to different causes, a time and cost-intensive electrical and optical contact measurement is employed. Fast photoluminescence measurements, on the other hand, are commonly used to detect wafer separation damages but also hold the potential to enable an efficient detection of all kinds of defective LED chips. On a photoluminescence image, every pixel corresponds to an LED chip's brightness after photoexcitation, revealing performance information. But due to unevenly distributed brightness values and varying defect patterns, photoluminescence images are not yet employed for a comprehensive defect detection. In this work, we show that fully convolutional networks can be used for chip-wise defect detection, trained on a small data-set of photoluminescence images. Pixel-wise labels allow us to classify each and every chip as defective or not. Being measurement-based, labels are easy to procure and our experiments show that existing discrepancies between training images and labels do not hinder network training. Using weighted loss calculation, we were able to equalize our highly unbalanced class categories. Due to the consistent use of skip connections and residual shortcuts, our network is able to predict a variety of structures, from extensive defect clusters up to single defective LED chips.																	0956-5515	1572-8145															10.1007/s10845-020-01563-4		MAR 2020											
J								Classification using proximity catch digraphs	MACHINE LEARNING										Class cover problem; Delaunay tessellation; Digraph; Domination; Prototype selection; Support estimation	PULSAR CANDIDATE SELECTION; TESTING SPATIAL-PATTERNS; PROTOTYPE SELECTION; DOMINATION NUMBER; SEGREGATION; ALGORITHMS; SUPPORT; SYSTEMS; GRAPHS; SETS	We employ random geometric digraphs to construct semi-parametric classifiers. These data-random digraphs belong to parameterized random digraph families called proximity catch digraphs (PCDs). A related geometric digraph family, class cover catch digraph (CCCD), has been used to solve the class cover problem by using its approximate minimum dominating set and showed relatively good performance in the classification of imbalanced data sets. Although CCCDs have a convenient construction in Rd, finding their minimum dominating sets is NP-hard and their probabilistic behaviour is not mathematically tractable except for d=1. On the other hand, a particular family of PCDs, called proportional-edge PCDs (PE-PCDs), has mathematically tractable minimum dominating sets in Rd; however their construction in higher dimensions may be computationally demanding. More specifically, we show that the classifiers based on PE-PCDs are prototype-based classifiers such that the exact minimum number of prototypes (equivalent to minimum dominating sets) is found in polynomial time on the number of observations. We construct two types of classifiers based on PE-PCDs. One is a family of hybrid classifiers that depends on the location of the points of the training data set, and another type is a family of classifiers solely based on class covers. We assess the classification performance of our PE-PCD based classifiers by extensive Monte Carlo simulations, and compare them with that of other commonly used classifiers. We also show that, similar to CCCD classifiers, our classifiers tend to be robust to the class imbalance in classification as well.																	0885-6125	1573-0565				APR	2020	109	4					761	811		10.1007/s10994-020-05878-4		MAR 2020											
J								Ranking-based triplet loss function with intra-class mean and variance for fine-grained classification tasks	SOFT COMPUTING										Image classification; Triplet learning; Convolutional neural network		This paper proposed a deep ranking model for triplet selection to efficiently learn similarity metric from top ranked images. A modified distance criterion described in the current work leverages the intra-category variance in metric learning of a triplet network by learning a local sample structure. A multicolumn fusion architecture is used to capture different levels of variance, which when incorporated in the loss function strengthens it and optimizes the objective of the triplet networks. This enables a fine-grained classification strategy. State-of-the-art techniques use a group-sensitive triplet sampling to deal with this issue. However, these have the disadvantage of increased group sampling computations. Experiments are conducted over a variety of benchmark datasets including Model40, PatternNet, and In-Shop Clothing. The main purpose of these experiments are to verify whether the triplet learning technique can be applied over different kinds of data. Results demonstrate that the current work provides superior results in most cases. These results can further be improved with specific parameter tunings and ensembling techniques wherever applicable.																	1432-7643	1433-7479				OCT	2020	24	20					15519	15528		10.1007/s00500-020-04880-1		MAR 2020											
J								Efficient routing in UASN during the thermohaline environment condition to improve the propagation delay and throughput	SOFT COMPUTING										Routing; Directional flooding; Convex optimisation; Propagation delay; Throughput	ACOUSTIC COMMUNICATIONS; COMMUNICATION	In underwater acoustic sensor network (UASN), the challenging issues are bandwidth, higher propagation delay and heavy packet loss during data transmission. The issues can be solved through efficient routing algorithms. The existing UASN routing algorithms have larger latency in the network link and high rate of packet loss because of the salinity and temperature in the water at different depths. The salinity and temperature changes according to the depth and called as thermohaline circulation. In this paper, convex directional flooding optimisation (CDFO) algorithm improves the latency, throughput and lifetime of the nodes in the network under thermohaline condition and longshore drift from longshore current, which consist of transportation of sediments. The CDFO combines the convex optimisation and directional flooding-based routing algorithm, convex optimisation helps in identification of the hidden nodes in the network and strong communication links are established through polynomial time and semantic analysis and directional flooding algorithm reduces the packet loss and increases the network throughput. The routing protocol has implemented in ns2-AquaSim simulator and test bed for measurement of the performance metrics of the UASN.																	1432-7643	1433-7479				OCT	2020	24	20					15671	15680		10.1007/s00500-020-04895-8		MAR 2020											
J								A new non-monotonic infeasible simplex-type algorithm for Linear Programming	PEERJ COMPUTER SCIENCE										Linear programming; Simplex-type; Interior point method; Exterior point; Non-monotonic; Infeasible; Mathematical programming; Optimization	COMBINING INTERIOR-POINT; PIVOT	This paper presents a new simplex-type algorithm for Linear Programming with the following two main characteristics: (i) the algorithm computes basic solutions which are neither primal or dual feasible, nor monotonically improving and (ii) the sequence of these basic solutions is connected with a sequence of monotonically improving interior points to construct a feasible direction at each iteration. We compare the proposed algorithm with the state-of-the-art commercial CPLEX and Gurobi Primal-Simplex optimizers on a collection of 93 well known benchmarks. The results are promising, showing that the new algorithm competes versus the state-of-the-art solvers in the total number of iterations required to converge.																	2376-5992					MAR 30	2020									e265	10.7717/peerj-cs.265													
J								Towards a blockchain-based certificate authentication system in Vietnam	PEERJ COMPUTER SCIENCE										Blockchain; Distributed ledger; Blockchain taxonomy; Certification authentication; Hyperledger fabric	CHALLENGES	Anti-forgery information, transaction verification, and smart contract are functionalities of blockchain technology that can change the traditional business processes of IT applications. These functionalities increase the data transparency, and trust of users in the new application models, thus resolving many different social problems today. In this work, we take all the advantages of this technology to build a blockchain-based authentication system (called the Vietnamese Educational Certification blockchain, which stands for VECefblock) to deal with the delimitation of fake certificate issues in Vietnam. In this direction, firstly, we categorize and analyze blockchain research and application trends to make out our contributions in this domain. Our motivating factor is to curb fake certificates in Vietnam by applying the suitability of blockchain technology to the problem domain. This study proposed some blockchain-based application development principles in order to build a step by step VECefblock with the following procedures: designing overall architecture along with business processes, data mapping structure and implementing the decentralized application that can meet the specific Vietnamese requirements. To test system functionalities, we used Hyperledger Fabric as a blockchain platform that is deployed on the Amazon EC2 cloud. Through performance evaluations, we proved the operability of VECefblock in the practical deployment environment. This experiment also shows the feasibility of our proposal, thus promoting the application of blockchain technology to deal with social problems in general as well as certificate management in Vietnam.																	2376-5992					MAR 30	2020									e266	10.7717/peerj-cs.266													
J								Roadmap to distillery spent wash treatment and use of soft computing techniques	EVOLUTIONARY INTELLIGENCE										Distillery industries; Risk; Distillery spent wash; Treatments; Coagulants; Soft computing techniques	COAGULATION-FLOCCULATION TREATMENT; OBTUSIFOLIA SEED GUM; FLY-ASH KINETICS; WATER-TREATMENT; SURFACE-WATER; PROCESS DESIGN; RICE STARCH; OPTIMIZATION; REMOVAL; REUSE	Distillery industries in several regions all over the world pose a serious risk, as it generates unpleasant compounds. Under such circumstances, it seeks an effective spent wash treatment, to eliminate the contaminants. Accordingly, this paper provides a relevant review regarding the distillery spent wash treatments, associated with the proper treatments and coagulants. At first, it reviews 67 recent research papers, from which 24 papers belong to distillery spent wash treatment and remaining belongs to other treatments. Further, it extends the valuable chronological review on distillery spent wash treatment. In addition, it describes the several processes such as anaerobic treatment, aerobic treatment, nanofiltration, reverse osmosis, adsorption and electrochemical treatments, adopted to treat distillery spent wash as reported in the literature. In the same way, it analyses the usage of different types of coagulants such as natural, electro and chemical coagulants used in distillery spent wash treatment. To the next of the coagulant analysis, it checks out the performance review of entire contributions on distillery spent wash treatment. The conventional process for distillery spent wash treatment having limitations such as limited removal efficiency, high operating cost and maintenance also it needs a high detention time that eventually increases the on the whole treatment process time. The aforesaid limitation can be overcome by adopting soft computing techniques. Soft computing has been widely studied and applied in the past three decades for engineering and scientific research computing. In environmental engineering, engineers and researchers have effectively used various techniques of soft computing like fuzzy logic, artificial neural networks, adaptive neuro-fuzzy inference systems, and support vector machines which can be useful for the researchers to achieve further research on distillery spent wash treatment.																	1864-5909	1864-5917															10.1007/s12065-020-00381-0		MAR 2020											
J								DV-Hop based localization methods for additionally deployed nodes in wireless sensor network using genetic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Localization; Range free; DV-Hop; Genetic algorithm	RANGE-FREE LOCALIZATION	Localization of sensor nodes is of utmost importance as the information collected by sensor nodes may be meaningless without the knowledge of its origin. Due to limited battery life and non uniform data loaded in the network, some of the sensor nodes may exhaust their battery early and may cease their intended function. This may lead to partitioning of network, wherein some portion of network may not communicate its sensed data to the sink node. In such situation we need to deploy some additional new nodes in the network. Newly deployed nodes needs to be localized as they are unaware of their estimated location. In this paper, we have proposed a new framework to localize newly deployed nodes in a pre-localized network using GA DV-Hop algorithm. The proposed framework has been simulated for various network topologies such as Random topology, C-shaped topology and W-shaped topology. Simulation results show that our proposed localization framework has a trade off between localization error and energy consumption, i.e. one of the proposed method features high localization accuracy at the cost of high energy consumption. However, it has been observed that proposed methods requires less time for localization as compared to traditional methods.																	1868-5137	1868-5145															10.1007/s12652-020-01907-1		MAR 2020											
J								Discrete teaching-learning-based optimization algorithm for clustering in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Clustering; Energy; TLBO algorithm; Discrete optimization; Mutation	PARTICLE SWARM OPTIMIZATION; ENERGY-EFFICIENT MULTILEVEL; INTERNET; PROTOCOL; PERFORMANCE; MECHANISM; THINGS; SCHEME	Clustering is an appealing paradigm exploited to improve the lifetime and scalability of wireless sensor networks (WSNs). Considering the NP-completeness of the clustering problem, numerous meta-heuristic algorithms are provided in the literature for the clustering of WSNs. Teaching-learning-based optimization (TLBO) is an optimization algorithm employed to tackle continuous optimization problems. In this paper, a novel discrete version of the TLBO algorithm is being presented that employs the swap and mutation operators to deal with discrete solutions. Subsequently, the new-fangled algorithm was utilized to design a hierarchical energy-aware clustering scheme for the WSNs to minimize the energy usage of the sensor nodes. In addition, an energy-aware local search algorithm was provided to enhance the network lifetime by taking factors such as energy and distance into account. Extensive simulations are conducted to indicate the effectiveness of this scheme in reducing the power usage of the sensor nodes and improving the WSN lifetime.																	1868-5137	1868-5145															10.1007/s12652-020-01902-6		MAR 2020											
J								A novel bat optimized runlength networks (BORN) for an efficient classification of breast cancer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										BORN; Intelligent; Visual saliency segmentation; RF-ELM; EGAM; Associate classifiers	EXTREME LEARNING-MACHINE; FEATURES	In recent times, Medical Information and Processing deals with various methodologies utilized for the prognosis and diagnosis of various harmful diseases with the help of trending artificial intelligence and machine learning techniques. Breast cancer is one of such disease which occupies the major share in killing the millions of people especially women. Several intelligent methods were proposed for an efficient diagnosis of breast cancer, but brighter light of research is required for better diagnosis. Hence the new methodology of integrating the run length features along with the Bat optimized learning Machines-BORN has been proposed. BORN also features the most efficient visual saliency segmentation process to obtain highly efficient diagnosis. The main aim of the proposed BORN algorithm is to diagnosis the different stages of breast cancer with high accuracy and minimal error. For attaining the high accuracy, BORN has been tested with two different datasets MIAS and DDSM with different learning kernels and compared with the other intelligent algorithms such as RF-ELM, EGAM and Associate Classifiers in which accuracy of the proposed classifier is 99.5%.																	1868-5137	1868-5145															10.1007/s12652-020-01890-7		MAR 2020											
J								APT attack detection algorithm based on spatio-temporal association analysis in industrial network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Advanced persistent threat; Industrial network security; Attack detection; Association analysis; Spatio-temporal feature	MODEL	The advanced persistent threat (APT) is the foremost threat to industrial network security today, and traditional feature detection based industrial intrusion detection systems arc often unable to detect the latest APT attacks. Existing researchers believe that theft of sensitive data is one of the important goals of APT attacks. In order to accurately identify the stealing behavior of the APT attack, a spatio-temporal association analysis is proposed to detect the APT attack in industrial network, which includes association rules mining, historical data retrieval and feature classification. Firstly, FP-Growth Algorithm on APT attack's temporal features, spatial features and category features is adopted to mine the association rules. And then, the relation between APT attack's features is analyzed, and the rules are explained semantically in combination with the features of APT attack historical data retrieval method based on Bloom filter algorithm. Finally, a multi-feature spatial weighted combined SVM classification detection algorithm which is used to detect abnormal APT attack session flows. Experiments show that our proposed algorithm has a good ability to detect hidden APT attacks, and the multi-feature spatial weighted combined SVM classification detection algorithm has higher detection accuracy and lower false alarm rate than traditional single classification detection, and it is also safe for industrial control security. In addition, our proposed algorithm uses less space and could judge whether the data is in the current data set rapidly.																	1868-5137	1868-5145															10.1007/s12652-020-01840-3		MAR 2020											
J								Heuristic/meta-heuristic methods for restricted bin packing problem	JOURNAL OF HEURISTICS										Assignment; Zigzag sorting; GRASP; Simulated annealing; Bin packing	APPROXIMATION; OPTIMIZATION; ALGORITHMS	This paper addresses a special bin packing problem in which each item can only be assigned to a subset of the bins. We name this problem as the restricted bin packing problem (RBPP). This paper is designed to explore the relationships of RBPP with classic NP-complete problems, and to resolve the restrictions of assignment through heuristic and meta-heuristic algorithms. A new heuristic algorithm named 'Max-fit Based on Zigzag Sorting with Retained Feasibility' is proposed. In this heuristic algorithm, a feasibility retaining rule is constructed to assure the assignment of every item; a zigzag sorting method is designed to improve the performance of the algorithm. Our heuristic algorithm is able to generate better results in comparison with existing heuristics. Greedy Randomized Adaptive Search Procedure (GRASP) and Simulated Annealing (SA) are exploited to obtain better solutions for RBPP. A new construction method based on cliques and zigzag sorting are built for GRASP and SA. The proposed methods are shown to have higher efficiency than traditional ones through numeric examples.																	1381-1231	1572-9397				OCT	2020	26	5					637	662		10.1007/s10732-020-09444-y		MAR 2020											
J								Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications	JOURNAL ON MULTIMODAL USER INTERFACES										Sonification design; Musical expectancy; Movement perception	SELF-EFFICACY; EXERCISE	When designing movement sonifications, their effect on people's movement must be considered. Recent work has shown how real-time sonification can be designed to alter the way people move. However, the mechanisms through which these sonifications alter people's expectations of their movement is not well explained. This is especially important when considering musical sonifications, to which people bring their own associations and musical expectation, and which can, in turn, alter their perception of the sonification. This paper presents a Movement Expectation Sonification Model, based on theories of motor-feedback and expectation, to explore how musical sonification can impact the way people perceive their movement. Secondly, we present a study that validates the predictions of this model by exploring how harmonic stability within sonification interacts with contextual cues in the environment to impact movement behaviour and perceptions. We show how musical expectancy can be built to either reward or encourage movement, and how such an effect is mediated through the presence of additional cues. This model offers a way for sonification designers to create movement sonifications that not only inform movement but can be used to encourage progress and reward successes.																	1783-7677	1783-8738				JUN	2020	14	2			SI		153	166		10.1007/s12193-020-00322-2		MAR 2020											
J								Predicting hydrogen storage capacity of metal-organic frameworks using group method of data handling	NEURAL COMPUTING & APPLICATIONS										Hydrogen storage; Metal-organic framework; Prediction; GMDH-PNN	NEURAL-NETWORK; ADSORBED HYDROGEN; IONIC LIQUIDS; ADSORPTION; VISCOSITY; NANOFLUIDS; MIXTURES; PRESSURE; DENSITY; BINDING	Due to their unique properties, metal-organic frameworks have exhibited excellent performance for hydrogen storage purposes in the last decade. In this regard, model development to predict the hydrogen storage in metal-organic frameworks is of a vital importance for designing and developing of efficient processes based on these new synthetic material. The objective of the present study is to develop a new model to predict the hydrogen storage capacity in metal-organic frameworks. The group method of data handling-type polynomial neural networks is implemented as a soft computing approach for model building. As an advantage, only 40% of data points are used for model development and the rest of data (60%) are designated for testing of the model. The results show that the proposed model has reasonable accuracy in which the root mean square error for the proposed model is 0.28. The model can acceptably predict effects of surface area and pressure on hydrogen storage capacity of MOFs demonstrating good ability of the proposed model for tracing physically expected trend for hydrogen storage. Additionally, the leverage measure demonstrates that the proposed model is statistically acceptable and valid. It should be noted that an artificial neural network is also developed for comparison with GMDH-PNN model, in which the results confirm that both of the models have approximately same performance and accuracy. However, due to simple mathematical structure of GMDH-PNN, it is significantly more appropriate for engineering applications.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14851	14864		10.1007/s00521-020-04837-3		MAR 2020											
J								Prediction of attentional focus from respiration with simple feed-forward and time delay neural networks	NEURAL COMPUTING & APPLICATIONS										Attention; Respiration; Neural network; Prediction; Time delay	COMPUTATIONAL INTELLIGENCE; VARIABILITY; EEG; ALERTNESS; MODEL; ADHD	Current methods to infer an agent's state of attentional focus rely on scalp potential recordings and pupil diameter measurements, both of which are unrealistic in many real-world situations, and are also prone to movement and electrical artifacts. Being able to predict attentional performance from a simple and noninvasive measure, such as respiration, could have obvious potential benefit for simplifying measurement and improving task performance in many settings, and could also be employed clinically with attentionally compromised populations for training and rehabilitation. It has been suggested that respiration and attention comprise a neuro-physiologically coupled system, and behavioral data has indicated that attentional performance, including reaction time and reaction time variability (RTV), covary with respiratory dynamics. In the present study, we tested several neural network configurations for the prediction of attentional control state (RTV) from respiratory parameters. We observed significant predictive power derived solely from respiratory input, and conclude that a robust and portable feedback device utilizing soft computation is feasible for this purpose. We suggest specific model and data source improvements to potentially further reduce errors in prediction.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14875	14884		10.1007/s00521-020-04841-7		MAR 2020											
J								Multi-cohort intelligence algorithm for solving advanced manufacturing process problems	NEURAL COMPUTING & APPLICATIONS										Multi-cohort intelligence; Variations of cohort intelligence; Abrasive water jet machining; Electro-discharge machining; Micro-machining	PROCESS PARAMETERS; EVOLUTIONARY OPTIMIZATION; MACHINING PARAMETERS; MICRO; TAGUCHI	In recent years, several nature-inspired optimization methods have been proposed and applied on various classes of problems. The applicability of the recently developed socio-inspired optimization method referred to as multi-cohort intelligence (Multi-CI) is validated by solving real-world problems from manufacturing processes domain, viz. non-traditional manufacturing processes. The problems are minimization of surface roughness for abrasive water jet machining (AWJM), electro-discharge machining (EDM), micro-turning and micro-milling processes. Furthermore, the taper angle for the AWJM, relative electrode wear rate for EDM, burr height and burr thickness for micro-drilling, flank wear for micro-turning process, machining time for micro-milling processes were minimized. It is important to mention that for the micro-drilling and micro-milling process different tool specifications were used. In addition, for EDM the material removal rate was maximized. The performance of the algorithm has been validated by comparing the results with other variations of CI algorithm and several contemporary algorithms such as firefly algorithm, genetic algorithm, simulated annealing and particle swarm optimization. In AWJM, Multi-CI achieved 5-8% and 8-23% minimization for surface roughness and taper angle, respectively. For EDM, 47-80% maximization of material removal rate; 2-13% and 92-98% minimization of surface roughness and relative electrode wear rate, respectively, have been attained. Furthermore, for micro-turning 2% minimization of flank wear and for micro-milling, 2-6% minimization of machining time were attained. For micro-drilling, 24% and 16-34% minimization of burr height and burr thickness were attained. In addition, the performance is compared with the regression and response surface methodology approaches and experimental solutions. The analysis regarding the convergence of all the algorithms is discussed in detail. The contributions in this paper have opened up several avenues for further applicability of the Multi-CI algorithm for solving real-world problems.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15055	15075		10.1007/s00521-020-04858-y		MAR 2020											
J								A generative adversarial network with adaptive constraints for multi-focus image fusion	NEURAL COMPUTING & APPLICATIONS										Multi-focus image fusion; Adaptive weight block; Generative adversarial networks; End-to-end	TRANSFORM	In this paper, we propose a novel end-to-end model for multi-focus image fusion based on generative adversarial networks, termed as ACGAN. In our model, due to the different gradient distribution between the corresponding pixels of two source images, an adaptive weight block is proposed in our model to determine whether source pixels are focused or not based on the gradient. Under this guidance, we design a special loss function for forcing the fused image to have the same distribution as the focused regions in source images. In addition, a generator and a discriminator are trained to form a stable adversarial relationship. The generator is trained to generate a real-like fused image, which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused image from the ground truth. Finally, the fused image is very close to ground truth in probability distribution. Qualitative and quantitative experiments are conducted on publicly available datasets, and the results demonstrate the superiority of our ACGAN over the state-of-the-art, in terms of both visual effect and objective evaluation metrics.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15119	15129		10.1007/s00521-020-04863-1		MAR 2020											
J								Formula-E race strategy development using artificial neural networks and Monte Carlo tree search	NEURAL COMPUTING & APPLICATIONS										Formula-E race strategy; Energy management; Machine learning; Artificial neural networks	WHEEL TORQUE DISTRIBUTION; HYBRID ELECTRIC VEHICLE; POWER MANAGEMENT	Energy management has been one of the most important parts in electric race strategies since the Federation Internationale de l'Automobile Formula-E championships were launched in 2014. Since that time, a number of unfavorable race finishes have been witnessed due to poor energy management. Previous researches have been focused on managing the power flow between different energy sources or different energy consumers based on a fixed cycle. However, there is no published work in the literature about energy management of a full electric racing car on repeated course but with changeable settings and driving styles. Different from traditional energy management problems, the electric race strategy is more of a multi-stage decision-making problem which has a very large scale. Meanwhile, this is a time-critical task in motorsport where fast prediction tools are needed and decisions have to be made in seconds to benefit the final outcome of the race. In this study, the use of artificial neural networks (ANN) and tree search techniques is investigated as an approach to solve such a large-scale problem. ANN prediction models are developed to replace the traditional lap time simulation as a much faster performance prediction tool. Implementation of Monte Carlo tree search based on the proposed ANN fast prediction models has provided decent capability to generate decision-making solution for both pre-race planning and in-race reaction to unexpected scenarios.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15191	15207		10.1007/s00521-020-04871-1		MAR 2020											
J								Efficient machine learning algorithm for electroencephalogram modeling in brain-computer interfaces	NEURAL COMPUTING & APPLICATIONS										Brain-computer interfaces (BCIs); Machine learning; Electroencephalogram (EEG); Modeling	ARTIFICIAL-INTELLIGENCE; FACE RECOGNITION; SYSTEMS	Brain-computer interfaces (BCIs) provide the measurement of the activities of central nervous systems, and they convert the activities into artificial outputs. Currently, one of the most interesting researches in BCIs is to develop methods for decoding people's intent from the neural signals. Electroencephalogram (EEG) is one of the ideal solution that provides a portable recording system of neural signals. One of the challenges is to decode people's intended movement based on EEG. Machine learning is a powerful tools and has been used in BCIs. Due to the large-scale computations, the model training process for machine learning is not always efficient. In order to address such challenges of electroencephalogram modeling in BCIs, we present an efficient machine learning algorithm based on normal equation. First, we propose a systolic matrix multiplication of two matrices. Second, we propose a systolic matrix inversion for large matrix. Third, we propose a systolic matrix-vector multiplication. In addition, the efficiency of the machine learning algorithm is analyzed and its applications of electroencephalogram modeling in BCIs are discussed.																	0941-0643	1433-3058															10.1007/s00521-020-04861-3		MAR 2020											
J								Multi-label learning for crop leaf diseases recognition and severity estimation based on convolutional neural networks	SOFT COMPUTING										Multi-label; Crop diseases recognition; Crop diseases severity estimation; Convolutional neural network; Computer vision	REGRESSION; ALGORITHM	Crop diseases have always been a dilemma as it can cause significant diminution in both quality and quantity of agricultural yields. Thus, automatic recognition and severity estimation of crop diseases on leaves plays a crucial role in agricultural sector. In this paper, we propose a series of automatic image-based crop leaf diseases recognition and severity estimation networks, i.e., BR-CNNs, which can simultaneously recognize crop species, classify crop diseases and estimate crop diseases severity based on deep learning. BR-CNNs based on binary relevance (BR) multi-label learning algorithm and deep convolutional neural network (CNN) approaches succeed in identifying 7 crop species, 10 crop diseases types (including Healthy) and 3 crop diseases severity kinds (normal, general and serious). Compared with LP-CNNs and MLP-CNNs, the overall performance of BR-CNNs is superior. The BR-CNN based on ResNet50 achieves the best test accuracy of 86.70%, which demonstrates the feasibility and effectiveness of our network. The BR-CNN based on the light-weight NasNet also achieves excellent test accuracy of 85.28%, which can provide more possibilities for the development of mobile systems and devices.																	1432-7643	1433-7479				OCT	2020	24	20					15327	15340		10.1007/s00500-020-04866-z		MAR 2020											
J								A metaheuristic optimization model for spectral allocation in cognitive networks based on ant colony algorithm (M-ACO)	SOFT COMPUTING										Metaheuristic framework; Cognitive radio networks; Search space algorithm; Graph-cut modeling; Detection rate; Spectrum allocation	ENERGY DETECTION	Cognitive radio networks have been gaining widespread attraction among researchers especially with the increasing demand for radio frequency spectrum whose availability is quite scarce. Cognitive radio networks provide an ideal solution to allocate spectrum to users on an intelligent basis through a series of spectrum sensing and decision making. A metaheuristic soft computing framework is proposed and implemented in this research work by using powerful optimization concepts of evolutionary algorithm, namely ant colony algorithm, coupled with graph-cut modeling of given wireless network to provide the expected precision of detection. Channel characteristics have been taken as the feature vectors which are modeled as n-tuple graph to decide upon the maximization of channel allocation probability based on availability in an opportunistic basis. Exhaustive experimentations have been conducted and optimal performance justified against other benchmark algorithms.																	1432-7643	1433-7479				OCT	2020	24	20					15551	15560		10.1007/s00500-020-04882-z		MAR 2020											
J								A novel QIM-DCT based fusion approach for classification of remote sensing images via PSO and SVM models	SOFT COMPUTING										Image fusion; Quantization index modulation with discrete contourlet transform; Bayesian filter with adaptive type-2 fuzzy system; Image classification and particle swarm optimization with support vector machine	FRAME-BASED FEATURES; WAVELET TRANSFORM; ACCURACY; IRS-P6	Fusion of panchromatic and multispectral images has become a research interest for the classification of remote sensing images. The spectral and spatial resolutions of different images give better information with the aid of image classification. However, fusing pixels for various satellite images is difficult due to the nature of original image consists of complex information. Similarly, most of the existing fusion algorithms implement a unified processing over the whole part of the image, thereby leaving certain important needs out of consideration. The main aim of our proposed approach is to fuse the images by gathering all important information from multiple images with minimum errors. In this paper, we propose a novel quantization index modulation with discrete contourlet transform-based fusion approach for classification of remote sensing images (LISS IV sensor). In order to improve the image fusion performance, we eliminate certain noises (salt, pepper, and Gaussian) using Bayesian filter with Adaptive Type-2 Fuzzy System. After image fusion, we make image classification by two steps of processes including deep multi-feature extraction and feature selection. Multiple features such as spectral, shape, global and local features are extracted using Affine Transformation (0 degrees, 90 degrees, 180 degrees, and 270 degrees), and then the best set of features are chosen by mutual information and maximal information coefficients. Finally, the image is classified into seven classes using PSO and SVM namely Urban, Vegetation, Wetland, Tank, Water Area, Bare Land, and Roadways. MATLAB R2017b has been used for evaluation of the LISS IV images. Experimental results revealed that our proposed approach is very effective in terms of their classification accuracy.																	1432-7643	1433-7479				OCT	2020	24	20					15561	15576		10.1007/s00500-020-04884-x		MAR 2020											
J								Exploiting Semantics for Face Image Deblurring	INTERNATIONAL JOURNAL OF COMPUTER VISION										Face image deblurring; Semantic face parsing; Deep convolutional neural networks		In this paper, we propose an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks. As the human faces are highly structured and share unified facial components (e.g., eyes and mouths), such semantic information provides a strong prior for restoration. We incorporate face semantic labels as input priors and propose an adaptive structural loss to regularize facial local structures within an end-to-end deep convolutional neural network. Specifically, we first use a coarse deblurring network to reduce the motion blur on the input face image. We then adopt a parsing network to extract the semantic features from the coarse deblurred image. Finally, the fine deblurring network utilizes the semantic information to restore a clear face image. We train the network with perceptual and adversarial losses to generate photo-realistic results. The proposed method restores sharp images with more accurate facial features and details. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm performs favorably against the state-of-the-art methods in terms of restoration quality, face recognition and execution speed.																	0920-5691	1573-1405				JUL	2020	128	7					1829	1846		10.1007/s11263-019-01288-9		MAR 2020											
J								Bayesian network classifiers using ensembles and smoothing	KNOWLEDGE AND INFORMATION SYSTEMS										Bayesian network classifier; Ensemble learning; Probability smoothing; Hierarchical Dirichlet process; Attribute discretization		Bayesian network classifiers are, functionally, an interesting class of models, because they can be learnt out-of-core, i.e. without needing to hold the whole training data in main memory. The selective K-dependence Bayesian network classifier (SKDB) is state of the art in this class of models and has shown to rival random forest (RF) on problems with categorical data. In this paper, we introduce an ensembling technique for SKDB, called ensemble of SKDB (ESKDB). We show that ESKDB significantly outperforms RF on categorical and numerical data, as well as rivalling XGBoost. ESKDB combines three main components: (1) an effective strategy to vary the networks that is built by single classifiers (to make it an ensemble), (2) a stochastic discretization method which allows to both tackle numerical data as well as further increases the variance between different components of our ensemble and (3) a superior smoothing technique to ensure proper calibration of ESKDB's probabilities. We conduct a large set of experiments with 72 datasets to study the properties of ESKDB (through a sensitivity analysis) and show its competitiveness with the state of the art.																	0219-1377	0219-3116				SEP	2020	62	9					3457	3480		10.1007/s10115-020-01458-z		MAR 2020											
J								Selecting effective features on prediction of delay in servicing ships arriving to ports using a combination of Clonal Selection and Grey Wolf Optimization algorithms-Case study: Shahid Rajaee port in Bandar Abbas	COMPUTATIONAL INTELLIGENCE										Clonal Selection algorithm; feature selection; Grey Wolf Optimization; maritime transportation; support vector regression	SUPPORT VECTOR REGRESSION	Predicting the delay in servicing incoming ships to ports is crucial for maritime transportation. In this study, we use support vector regression (SVR) in order to accurately predict this delay for ships arriving to the terminal No. 1 of Shahid Rajaee's port in Bandar Abbas. To achieve this goal, a combination of Clonal Selection and Grey Wolf Optimization algorithms (named as CLOGWO) is used for two purposes: (i) selecting the most important features among the features that affect prediction of this delay and (ii) optimizing SVR parameters for a more accurate prediction. Performance of the proposed method was compared with Genetic Algorithm (GA), Clonal Selection (CS), Grey Wolf Optimization (GWO), and Particle Swarm Optimization (PSO) algorithms on the following metrics: correlation, rate of feature reduction, root mean square error (RMSE), and normalized RMSE (NRMSE). Evaluations on Shahid Rajaee dataset showed that the mean value of these metrics in 10 independent runs of the proposed method were 0.867, 74.45%, 0.080, and 9.02, respectively. These results and evaluations on standard datasets indicate that the proposed method provides competitive results with other evolutionary algorithms.																	0824-7935	1467-8640				AUG	2020	36	3					1140	1160		10.1111/coin.12323		MAR 2020											
J								What drives bio-art in the twenty-first century? Sources of innovations and cultural implications in bio-art/biodesign and biotechnology	AI & SOCIETY										Science and arts; Bio-art; Biodesign; Anthropocentrism; Biocentrism; Converging of art and science		Bio-art epitomizes a coalescence of art and sciences. It is an emerging contemporary artistic practice that uses a wide range of traditional artistic media interwoven with new artistic media that are biological in nature. This includes molecules, genes, cells, tissues, organs, living organisms, ecological niches, landscapes and ecosystems. In addition, bio-art expands into conceptual art using biological processes such as growth, cell division, photosynthesis and concepts of the origin of life and evolution, explaining them as new artistic media. In this time of global challenges, bio-art communicates thoughts and feelings that involve relationships between the artist, science, public and the biological organism or biological concept. This article reviews the major challenges and driving forces that contributed to emerging biology-centered art and design.																	0951-5666	1435-5655															10.1007/s00146-020-00940-0		MAR 2020											
J								AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media	AI & SOCIETY										Artificial intelligence (AI); Ethics; Media; News; Public discourse; Public policy		As artificial intelligence (AI) technologies become increasingly prominent in our daily lives, media coverage of the ethical considerations of these technologies has followed suit. Since previous research has shown that media coverage can drive public discourse about novel technologies, studying how the ethical issues of AI are portrayed in the media may lead to greater insight into the potential ramifications of this public discourse, particularly with regard to development and regulation of AI. This paper expands upon previous research by systematically analyzing and categorizing the media portrayal of the ethical issues of AI to better understand how media coverage of these issues may shape public debate about AI. Our results suggest that the media has a fairly realistic and practical focus in its coverage of the ethics of AI, but that the coverage is still shallow. A multifaceted approach to handling the social, ethical and policy issues of AI technology is needed, including increasing the accessibility of correct information to the public in the form of fact sheets and ethical value statements on trusted webpages (e.g., government agencies), collaboration and inclusion of ethics and AI experts in both research and public debate, and consistent government policies or regulatory frameworks for AI technology.																	0951-5666	1435-5655															10.1007/s00146-020-00965-5		MAR 2020											
J								A brain tumor image segmentation technique in image processing using ICA-LDA algorithm with ARHE model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ICA-LDA; Feature extraction; Brain tumor; Abnormal conditions; Adaptive median filter; ARHE model		In digital image processing, image segmentation is the key methodology which is to be used frequently. In digital image processing, noise reduction and enhancement techniques are plays as a vital role. Brain is major and major organ of the human body which is to be controlled by the nervous system. In this paper, we proposed a brain tumor image enhancement technique with the help of the ICA-LDA (independent component analysis-linear discriminate analysis algorithm with ARHE (adaptive region based histogram enhancement) model. Image fusion technique is to apply for combination of the two or more input image. In this paper, the weighted average technique is to be used for image fusion techniques. The noise reduction and enhancement techniques are to be applied in preprocessing stage. The adaptive median filter is to be used for preprocessing stage. The ARHE (adaptive region based histogram enhancement) model is to be used for enhancement present in the preprocessing stage. The feature extraction and the feature optimization have to be utilized with the ICA (independent component analysis). The LDA (linear discriminate analysis) is to be used for the classification techniques. Using this classifier which is to separate the abnormal and normal stages. When the brain tumor is denoted as abnormal case then the morphological based segmentation is to be done. The simulation and result shows the analysis of various parameters such as specificity, sensitivity, positive predictive value, negative predictive value, accuracy, precision, and recall.																	1868-5137	1868-5145															10.1007/s12652-020-01875-6		MAR 2020											
J								A novel muscle-computer interface for hand gesture recognition using depth vision	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Depth vision; Hand gesture recognition; Muscle computer interface; Clustering; Classification	CLASSIFICATION; PROTOTYPE	Muscle computer Interface (muCI), one of the widespread human-computer interfaces, has been widely adopted for the identification of hand gestures by using the electrical activity of muscles. Although multi-modal theory and machine learning algorithms have made enormous progress in muCI over the last decades, the processing of the collecting and labeling large data sets creates a high workload and leads to time-consuming implementations. In this paper, a novel muCI was developed to integrate the advantages of EMG signals and depth vision, which could be used to automatically label the cluster of EMG data collected using depth vision. A three layers hierarchical k-medoids approach was designed to extract and label the clustering feature of ten hand gestures. A multi-class linear discriminant analysis algorithm was applied to build the hand gesture classifier. The results showed that the proposed algorithm had high accuracy and the muCI performed well, which could automatically label the hand gesture in all experiments. The proposed muCI can be utilized for hand gesture recognition without labeling the data in advance and has the potential for robot manipulation and virtual reality applications.																	1868-5137	1868-5145															10.1007/s12652-020-01913-3		MAR 2020											
J								A survey on influence maximization in a social network	KNOWLEDGE AND INFORMATION SYSTEMS										Social networks; Influence maximization; Approximation algorithm; Greedy strategy	INFORMATION DIFFUSION PROBABILITIES; POSITIVE INFLUENCE; SPREAD; ALGORITHM; CENTRALITY; APPROXIMABILITY; THRESHOLDS; HARDNESS; SETS	Given a social network with diffusion probabilities as edge weights and a positive integer k, which k nodes should be chosen for initial injection of information to maximize the influence in the network? This problem is popularly known as the Social Influence Maximization Problem (SIM Problem). This is an active area of research in computational social network analysis domain, since one and half decades or so. Due to its practical importance in various domains, such as viral marketing, target advertisement and personalized recommendation, the problem has been studied in different variants, and different solution methodologies have been proposed over the years. This paper presents a survey on the progress in and around SIM Problem. At last, it discusses current research trends and future research directions as well.																	0219-1377	0219-3116				SEP	2020	62	9					3417	3455		10.1007/s10115-020-01461-4		MAR 2020											
J								Legendre Neural Network Method for Several Classes of Singularly Perturbed Differential Equations Based on Mapping and Piecewise Optimization Technology	NEURAL PROCESSING LETTERS										Extreme learning machine; Legendre polynomials; Mapping technology; Piecewise optimization technology; Singularly perturbed differential equations	VARYING SYLVESTER EQUATION; FINITE-ELEMENT-METHOD; NUMERICAL-SOLUTION; APPROXIMATIONS; CONVERGENCE; SYSTEM; MESH	In this paper, we develop a novel neural network model with mapping and piecewise optimization technology for several classes of the linear singularly perturbed initial value and boundary value differential equations with variable coefficients. First, the Legendre polynomials are selected as the activation function of the artificial neural network, the mapping technology is employed to transform the original uniform partition points and the piecewise optimization technology is used to improve the calculation accuracy. Then, the solution of the linear singularly perturbed differential equations is solved by using the extreme learning machine optimization algorithm. Finally, the numerical experiments show that the developed method can effectively improve the accuracy of the calculation.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2891	2913		10.1007/s11063-020-10232-9		MAR 2020											
J								Opening the black box: Personalizing type 2 diabetes patients based on their latent phenotype and temporal associated complication rules	COMPUTATIONAL INTELLIGENCE										diabetes associated complication rules; latent variable discovery; patient personalization; temporal phenotype; time series clustering	RETINOPATHY; MELLITUS; NEPHROPATHY; NEUROPATHY; DISEASE	It is widely considered that approximately 10% of the population suffers from type 2 diabetes. Unfortunately, the impact of this disease is underestimated. Patient's mortality often occurs due to complications caused by the disease and not the disease itself. Many techniques utilized in modeling diseases are often in the form of a "black box" where the internal workings and complexities are extremely difficult to understand, both from practitioners' and patients' perspective. In this work, we address this issue and present an informative model/pattern, known as a "latent phenotype," with an aim to capture the complexities of the associated complications' over time. We further extend this idea by using a combination of temporal association rule mining and unsupervised learning in order to find explainable subgroups of patients with more personalized prediction. Our extensive findings show how uncovering the latent phenotype aids in distinguishing the disparities among subgroups of patients based on their complications patterns. We gain insight into how best to enhance the prediction performance and reduce bias in the models applied using uncertainty in the patients' data.																	0824-7935	1467-8640															10.1111/coin.12313		MAR 2020											
J								Multi-instance iris remote authentication using private multi-class perceptron on malicious cloud server	APPLIED INTELLIGENCE										Multi-biometric systems; Correlation analysis; Fully homomorphic encryption; Contradistinguish similarity analysis; Private multi-class perceptron	FEATURE FUSION; BIOMETRICS; SYSTEMS	In recent years, biometric authentication system (BAS) has become the most promising and popular authentication system in identity management. Due to its capability to solve the limitations of unimodal systems, multi-biometric systems (MBS) have been extensively accepted in various fields. The main step in MBS is information fusion. On the other hand, directly storing the fused templates into a centralized server leads to privacy concerns. Recently, many BAS based on homomorphic encryption has been introduced to provide confidentiality for the fused templates. However, most of the existing solutions rely on an implication of the assumption that the server is "Honest-but-Curious". As a result, the compromise of such server results into entire system vulnerability. To address this, we propose a novel P rivacy P reserving (PP) multi-instance iris remote authentication system to accord with attacks at the malicious server and over the transmission channel. Our scheme uses F ully H omomorphic E ncryption (FHE) to achieve the confidentiality of the fused iris templates and polynomial factorization algorithm to achieve the integrity of the matching result. We propose a PP iris authentication system using P rivate M ulti-C lass P erceptron (PMCP) by using the properties of FHE. Moreover, we propose C ontradistinguish S imilarity A nalysis (CSA), a feature level fusion technique that minimizes the between-class correlations and maximizes the pair-wise correlations. Our method has experimented on IITD and CASIA-V3-Interval iris databases to check the effectiveness and robustness. Experimental results show that our method provides improved accuracy, and eliminates the need to trust the cloud server when compared to the state-of-the-art approaches.																	0924-669X	1573-7497				SEP	2020	50	9					2848	2866		10.1007/s10489-020-01681-9		MAR 2020											
J								Handwritten Arabic numerals recognition using convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										OCR; Deep learning; CNN; Handwritten numeral recognition; Arabic numeral	DIGITS; FEATURES; BANGLA	Numeral recognition is an important preliminary step for optical character recognition, document understanding and others. Deep learning based numeral recognition models have achieved huge popularity among the researchers around the world since last few years. Several convolutional neural network (CNN) based architectures have been proposed and many of those have achieved state-of-the-art results in numeral recognition. In this paper, we have explored CNN based architectures for handwritten Arabic numeral recognition. We have also developed a handwritten Arabic numerals dataset using various morphological operations on an existing dataset thus increasing the size of the dataset from 3000 to 72,000 images. A modification of previously proposed CNN architecture has given us an accuracy of 98.91% and our proposed architecture has produced 99.76%, which is comparable to state-of-the-art results found in the domain of handwritten Arabic numeral recognition.																	1868-5137	1868-5145															10.1007/s12652-020-01901-7		MAR 2020											
J								Multiple parametric fault diagnosis using computational intelligence techniques in linear filter circuit	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Analog circuit diagnosis; Parametric fault diagnosis; Autoencoder; Extreme learning machine (ELM)	RIDGELET NETWORK APPROACH; WAVELET TRANSFORM; ANALOG; ALGORITHM	This paper presents parametric fault diagnosis in analog circuit using machine learning algorithm. Single and multiple parametric fault diagnosis based on Simulation before test approach are considered in this study. A benchmark State Variable Filter circuit has been used as an example circuit for experiment validations. Extreme learning machine based autoencoder (ELM-AE) is proposed in this paper for feature reduction and classification. The transfer function of the benchmark circuit is simulated by performing Monte-Carlo analysis, the features are obtained to form fault dictionary for single and double faults in such way that the dictionary contains unique values for faulty and fault free configuration of the circuit. The input test patterns from the fault dictionary which are required for testing are reduced to improve the classification by autoencoder. Autoencoder is used as pre-processor to learn and reduce the features before performing classification by the extreme learning machine (ELM) algorithm. The results obtained by the proposed algorithm is compared with the other ELM empirical models like extreme learning machine, kernel extreme learning machine and an evolutionary algorithm called self-adaptive evolutionary extreme learning machine to analyze the performance of the proposed algorithm. The experimental results show that the ELM-AE contributes the higher diagnosis accuracy than other ELM models referred in this paper.																	1868-5137	1868-5145															10.1007/s12652-020-01908-0		MAR 2020											
J								From Here to 2023: Civil Drones Operations and the Setting of New Legal Rules for the European Single Sky	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned aircraft systems; GDPR; European Single Market; Design; Risk evaluation; Single European Sky; Drones; EU law		The paper deals with the legal facets of the Single European Sky strategy and the reform of today's European air traffic management network for Unmanned Aerial Systems (UAS). The aim of this strategy is to guarantee standards on safety, efficiency and environmental impact of the air traffic, so as to let drones progressively sharing it. The paper scrutinizes the ways in which the European Commission and European Aviation Safety Agency (EASA) intend to attain this end by September 2023. In addition, the paper sheds light on some of the troubles with this roadmap. First, it is all about the clarification and implementation of the regulations and guidelines set up by both EASA and the Commission. Second, problems of coordination persist among different regulatory authorities in multiple legal domains, e.g. the interaction between civil aviation law authorities and data protection authorities. Third, most solutions on a risk-based regulation, markets integration, and the design of law-abiding drones should involve the UAS sector industries. The latter are an essential factor for the adoption of new technological standards and still, the definition of such standards brings us back to the coordination problems mentioned above. Whether or not these problems on rules, standards and coordination for the governance of law-abiding drones will obstruct the agenda for a Single European Sky remains an open question. But, it seems fair to admit such problems will increasingly attract the attention of scholars over the next years.																	0921-0296	1573-0409				NOV	2020	100	2					493	503		10.1007/s10846-020-01185-1		MAR 2020											
J								A Characterization of Proximity Operators	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Proximity operator; Convex regularization; Nonconvex regularization; Subdifferential; Shrinkage operator; Social sparsity; Group sparsity	SPARSITY	We characterize proximity operators, that is to say functions that map a vector to a solution of a penalized least-squares optimization problem. Proximity operators of convex penalties have been widely studied and fully characterized by Moreau. They are also widely used in practice with nonconvex penalties such as the pseudo-norm, yet the extension of Moreau's characterization to this setting seemed to be a missing element of the literature. We characterize proximity operators of (convex or nonconvex) penalties as functions that are the subdifferential of some convex potential. This is proved as a consequence of a more general characterization of the so-called Bregman proximity operators of possibly nonconvex penalties in terms of certain convex potentials. As a side effect of our analysis, we obtain a test to verify whether a given function is the proximity operator of some penalty, or not. Many well-known shrinkage operators are indeed confirmed to be proximity operators. However, we prove that windowed Group-LASSO and persistent empirical Wiener shrinkage-two forms of a so-called social sparsity shrinkage-are generally not the proximity operator of any penalty; the exception is when they are simply weighted versions of group-sparse shrinkage with non-overlapping groups.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		773	789		10.1007/s10851-020-00951-y		MAR 2020											
J								Recommender systems with selfish users	KNOWLEDGE AND INFORMATION SYSTEMS										Recommender systems; Game theory; Optimization		Recommender systems are a fundamental component of contemporary social-media platforms and require feedback submitted from users in order to fulfill their goal. On the other hand, the raise of advocacy about user-controlled data repositories supports the selective submission of data by user through intelligent software agents residing at the user end. These agents are endowed with the task of protecting user privacy by applying a "soft filter" on personal data provided to the system. In this work, we pose the question: "how should the software agent control the user feedback submitted to a recommender system in a way that is most privacy preserving, while the user still enjoys most of the benefits of the recommender system?". We consider a set of such agents, each of which aims to protect the privacy of its serving user by submitting to the recommender system server a version of her real rating profile. The fact that issued recommendations to a user depend on the collective rating profiles by all agents gives rise to a novel game-theoretic setup that unveils the trade-off between privacy preservation of each user and the quality of recommendation they receive. Privacy is quantified through a distance metric between declared and an "initial" random rating profile; the latter is assumed to provide a "neutral" starting point for the disclosure of the real profile. We allow different users to have different perception of their privacy through a user-dependent utility function of this distance. The quality of recommendations for each user depends on submitted ratings of all users, including the ratings of the user to whom the recommendation is provided. We prove the existence of a Nash equilibrium point (NEP), and we derive conditions for that. We show that user strategies converge to the NEP after an iterative best-response strategy update sequence that involves circulation of aggregate quantities in the system and no revelation of real ratings. We also present various modes of user cooperation in rating declaration, by which users mutually benefit in terms of privacy. We evaluate and compare cooperative and selfish strategies in their performance in terms of privacy preservation and recommendation quality through real movie datasets.																	0219-1377	0219-3116				AUG	2020	62	8					3239	3262		10.1007/s10115-020-01460-5		MAR 2020											
J								Measuring time-sensitive user influence in Twitter	KNOWLEDGE AND INFORMATION SYSTEMS										Influential Twitter users; Social network analysis; Time-sensitive ranking; AHP; Knowledge discovery	SOCIAL NETWORKS; CENTRALITY; NODES	Identification of the influential users is one of the most practical analyses in social networks. The importance of this analysis stems from the fact that such users can affect their followers "/friends" viewpoints. This study aims at introducing two new indices to identify the most influential users in the Twitter social network. Four sets of features extracted from user activities, user profile, tweets, and actions performed on tweets are deployed to create the proposed indices. The available methods of detecting the most influential Twitterers either consider a limited set of features or do not accurately measure the effect of each feature. The indices proposed in this paper consider a comprehensive set of features and also provide a time-sensitive rank which can be used to measure the dynamic nature of influence. Moreover, the relative impact of each feature is computed and considered in the indices. We employ the indices to discover the influential Twitter users posting on Paris attacks in 2015, in a comprehensive analysis. The influence trend of users' tweets in a 21-day period discloses that 76% of the users do not succeed in posting a second influential tweet. Results reveal that the proposed indices can detect both the publicly recognized sources (like celebrities) and also the less known individuals which gain credit by posting several influential tweets after a specific event. We further compare the proposed indices with other available approaches.																	0219-1377	0219-3116				SEP	2020	62	9					3481	3508		10.1007/s10115-020-01459-y		MAR 2020											
J								Gradient self-weighting linear collaborative discriminant regression classification for human cognitive states classification	MACHINE VISION AND APPLICATIONS										Feature selection; Functional magnetic resonance imaging; k-nearest neighbors; Linear collaborative discriminant regression classification; Region of interest	FMRI DATA; BRAIN; NETWORKS; ENSEMBLE; ACTIVATION; RESOLUTION; SELECTION; MACHINE	In recent decades, huge volumes of data are available to inspect human brain activities for disease detection. Specifically, the functional magnetic resonance imaging (fMRI) is a powerful tool to enquire the brain functions. In fMRI, identifying the active patterns of the specific cognitive state is one of the emerging concerns for neuroscientists. The high-dimensional features make fMRI data difficult for mining and classification, because if the volume of the data space increases, then the acquired data become sparse, which leads to the "curse of dimensionality" problem. To address this concern, a new feature selection and classification methodology was proposed for classifying the human cognitive states from fMRI data. Initially, the fMRI data were collected from the StarPlus and Haxby datasets. Then, k-nearest neighbors algorithm (k-NN)-based genetic algorithm was developed to choose the optimal voxels from the active region of interests. The proposed approach selects the data to feature subsets based on k-NN algorithm, so the data volume was effectively reduced and the voxel information was maintained significantly. The most informative voxels were given as the input for gradient self-weighting that produces an optimal weight value. Respective weight value was added to the projection matrix of linear collaborative discriminant regression classification for identifying the future projection matrix that reduces the error between two individual voxels in subspace. The experimental outcome shows that the proposed methodology improved the accuracy in fMRI data classification up to 0.7-23% compared to the existing methods.																	0932-8092	1432-1769				MAR 28	2020	31	3							21	10.1007/s00138-020-01070-9													
J								Towards the Ethical Publication of Country of Origin Information (COI) in the Asylum Process	MINDS AND MACHINES										Asylum; Country of Origin Information (COI); Data ethics; Dual use research; Refugees; Human rights; Open access	OPEN ACCESS	This article addresses the question of how 'Country of Origin Information' (COI) reports-that is, research developed and used to support decision-making in the asylum process-can be published in an ethical manner. The article focuses on the risk that published COI reports could be misused and thereby harm the subjects of the reports and/or those involved in their development. It supports a situational approach to assessing data ethics when publishing COI reports, whereby COI service providers must weigh up the benefits and harms of publication based, inter alia, on the foreseeability and probability of harm due to potential misuse of the research, the public good nature of the research, and the need to balance the rights and duties of the various actors in the asylum process, including asylum seekers themselves. Although this article focuses on the specific question of 'how to publish COI reports in an ethical manner', it also intends to promote further research on data ethics in the asylum process, particularly in relation to refugees, where more foundational issues should be considered.																	0924-6495	1572-8641				JUN	2020	30	2					247	257		10.1007/s11023-020-09523-w		MAR 2020											
J								DeNNeS: deep embedded neural network expert system for detecting cyber attacks	NEURAL COMPUTING & APPLICATIONS										Cybersecurity; Deep learning; Neural network; Embedded expert system; Phishing attack detection; Malware detection; Rule extraction; Rule refinement	ANDROID MALWARE DETECTION; RULE EXTRACTION; LEARNING APPROACH; ALGORITHM; GENERATION; MODEL	With the advances in computing powers and increasing volumes of data, deep learning's emergence has helped revitalize artificial intelligence research. There is a growing trend of applying deep learning techniques to image processing, speech recognition, self-driving cars, and even health-care. Recently, several deep learning models have been employed to detect a cyber threat such as network attack, malware infiltration, or phishing website; nevertheless, they suffer from not being explainable to security experts. Security experts not only do need to detect the incoming threat but also need to know the incorporating features that cause that particular security incident. To address this issue, in this paper, we propose a deep embedded neural network expert system (DeNNeS) that extracts refined rules from a trained deep neural network (DNN) architecture to substitute the knowledge base of an expert system. The knowledge base later is used to classify an unseen security incident and inform the final user of the corresponding rule that made that inference. We consider different rule extraction scenarios, and to prove the robustness of DeNNeS, we evaluate it on two cybersecurity datasets including UCI phishing websites dataset and Android malware dataset comprising more than 4000 Android APKs from several sources. The comparison results of DeNNeS with standalone DNN, JRip, and common machine learning algorithms show that DeNNeS with the retraining uncovered samples scenario outperforms other algorithms on both datasets. Furthermore, the extracted rules approximately reproduce the accuracy of the neural network from which they are derived. DeNNeS achieves an outstanding accuracy of 97.5% and a negligible false positive rate of 1.8% about 2.4% higher and 3.5% lower than the rule learner JRip on the phishing dataset. Moreover, DeNNeS outperforms random forest (RF), which produces the highest results among decision tree (DT), support vector machine, k-nearest neighbor, and Gaussian naive Bayes. Despite smaller training data in the malware dataset, DeNNeS achieves an accuracy of 95.8% and F-1 score of 91.1%, much higher than JRip and RF.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14753	14780		10.1007/s00521-020-04830-w		MAR 2020											
J								DFPhaseFL: a robust device-free passive fingerprinting wireless localization system using CSI phase information	NEURAL COMPUTING & APPLICATIONS										Device-free indoor localization; Channel state information (CSI); Phase calibration; Transfer deep learning	KERNEL	Device-free passive wireless indoor localization is attracting great interest in recent years due to the widespread deployment of Wi-Fi devices and the numerous location-based services requirements. In this paper, we propose DFPhaseFL, the first device-free fingerprinting indoor localization system that purely uses CSI phase information. It utilizes the CSI phase information extracted from simply a single link to estimate the location of the target, neither requiring the target to wear any electronic equipment nor deploying a large number of access points and monitor devices. In DFPhaseFL, the raw CSI phases are extracted from the CSI measurements through the three antennas of the Intel WiFi Link 5300 wireless Network Interface Card (IWL 5300 NIC) firstly. Then, linear transformation and noise filtering are applied to acquire the calibrated CSI phases. Through experimental observations, we find that the calibrated CSI phase owns an unpredictable characteristic over time. Thus, it cannot be directly applied as a fingerprint. To this end, a transfer deep supervised neural network method combining deep neural network and transfer learning is proposed to obtain feature representations with both transferability and discriminability as fingerprints. Then, the DFPhaseFL system uses the SVM algorithm to obtain the estimation of the target location online. Experiment results demonstrate that the DFPhaseFL owns a better estimation precision compared with the other state of art, and maintain a stable localization accuracy for a long time without reacquiring the fingerprint database.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14909	14927		10.1007/s00521-020-04847-1		MAR 2020											
J								Research on water temperature prediction based on improved support vector regression	NEURAL COMPUTING & APPLICATIONS										Water temperature prediction; Genetic algorithm; Support vector regression (SVR); Mutual information; Solar radiation	GA-SVR; SELECTION; LAKE; LOAD	This paper presents a model for predicting the water temperature of the reservoir incorporating with solar radiation to analyze and evaluate the water temperature of large high-altitude reservoirs in western China. Through mutual information inspection, the model shows that the dependent variable has a good correlation with water temperature, and it is added to the sample feature training model. Then, the measured water temperature data in the reservoir for many years are used to establish the support vector regression (SVR) model, and genetic algorithm (GA) is introduced to optimize the parameters, so as to construct an improved support vector machine (M-GASVR). At the same time, root-mean-square error, mean absolute error, mean absolute percentage error, and Nash-Sutcliffe efficiency coefficient are used as the criteria for evaluating the performance of SVR model, ANN model, GA-SVR model, and M-GASVR model. In addition, the M-GASVR model is used to simulate the water temperature of the reservoir under different working conditions. The results show that ANN model is the worst among the four models, while GA-SVR model is better than SVR model in terms of metric, and M-GASVR model is the best. For non-stationary sequences, the prediction model M-GASVR can well predict the vertical water temperature and water temperature structure in the reservoir area. This study provides useful insights into the prediction of vertical water temperature at different depths of reservoirs.																	0941-0643	1433-3058															10.1007/s00521-020-04836-4		MAR 2020											
J								Blockchain Expansion to secure Assets with Fog Node on special Duty	SOFT COMPUTING										Blockchain; Fog; Cloud; DDoS; FNOSD (fog node on special duty); IoT	BIG DATA ANALYTICS; IOT APPLICATIONS; INTERNET; DDOS; CHALLENGES; PRIVACY; THINGS	Blockchain expansion is the high priority necessity to improve security. Hacking and other attacks are headway from system innovation and security measures for the cloud architecture. That is why the countermeasure deployed for such attacks should act in an opportune way and ought to be situated as close as possible to attacking device. As an example, DDoS attacks were not that complex as they are getting now with the new technology known as IoT. Imagine the consequences if 25 billion of IoT devices generate a huge amount of data for DDoS attacks. That is why we propose a new framework that can expand blockchain in a manner where more companies can share their resources to enhance security. So, we proposed a new and complete framework with cloud, fog, to secure configuration files with blockchain technology. Our framework considers the configuration files from SDN or NFV as an asset to secure with blockchain. By saving configuration files into blockchain, we can detect illegal changes occurred to configuration files after hacking attack. This study also focuses on expanding blockchain between the multiple service providers with ease to prevent waste of resources. This paper mainly provides opportunities for different could or companies to secure their assets by employing the power of blockchain and smart contracts.																	1432-7643	1433-7479				OCT	2020	24	20					15209	15221		10.1007/s00500-020-04857-0		MAR 2020											
