PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Improved-GWO designed FO based type-II fuzzy controller for frequency awareness of an AC microgrid under plug in electric vehicle	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Plug in electric vehicle (PEV); Improved-grey wolf optimization (I-GWO); Fractional order type-II fuzzy PID (FO-T2-FPID) controller; Microgrid (mg); Distributed generation (dg); fuel cell	AUTOMATIC-GENERATION CONTROL; ENERGY-STORAGE; OPERATION MANAGEMENT; FORECAST ENGINE; POWER-PLANT; ALGORITHM; WIND; SYSTEMS; TURBINE; MODELS	The paper proposes to use an improved Grey Wolf Optimization (GWO) optimized Fractional Order (FO) based type-II fuzzy controller for frequency regulation of an AC microgrid in presence of plug-in electric vehicles. The AC microgrid comprises various renewable-based energy sources such as wind turbine generator (WTG), photovoltaic cell (PV), microturbine (MT), aqua electrolyzer based fuel cell (FC) and diesel engine generator (DEG) along with various storage devices (ESD) like battery energy storage (BES), flywheel energy storage (FES) and electric vehicles (EV). The uncertain nature of solar, wind technologies and load demand makes the system more complicated and introduces the frequency oscillations in the system. It is highly needed to establish power balance among generation by designing an appropriate controller for frequency regulation. This paper proposes a robust fractional order type-II fuzzy PID (FO-T2-FPID) where the parameters of the FO-T2-FPID controllers are optimized by suggesting an improved GWO (I-GWO) algorithm. To demonstrate the capability of proposed I-GWO algorithm few comparative analyses have been performed over particle swarm optimization (PSO) and grey wolf optimization (GWO) algorithm. It is also observed that for the same FO-T2-FPID controller structure, the percentage improvement in the objective function (ITAE) under load uncertainty by I-GWO compared to GWO and PSO are 29.74% and 65.94% respectively. Finally, it is conferred that proposed I-GWO optimized FO-T2-FPID controller significantly improves microgrid dynamic performance under various operational conditions.																	1868-5137	1868-5145															10.1007/s12652-020-02260-z		JUN 2020											
J								Scattering coefficients and pathgain of a multilayer tissue structure using ABCD matrix method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Body area network; Multilayer structure; ABCD matrix; Scattering coefficients; Path loss	IN-BODY; ON-BODY; PROPAGATION; PERFORMANCE; ANTENNA	Numerical modeling plays an important role in the prediction of received signal strength inside the human body, in case of on-body to in-body radio wave propagation in a body area network (BAN). Since the structure of a human body is heterogeneous in nature, the Fresnel scattering coefficients could not be used to incorporate the attenuation properties of the multilayer tissue structure of the human body. This paper proposes the use of ABCD matrix method, which has been used in the analysis of multilayer dielectric slab structure, to determine the attenuation of the electro magnetic (EM) signal due to various in-body structures. In addition, variation of reflection and transmission coefficients of a four-layer body tissue structure with respect to the incident angle is presented and recommends suitable operating frequency in ultra-wide band (UWB) for BAN applications. The authors conducted measurements in body mimicking liquid phantom in UWB and included in support of their research outcome.																	1868-5137	1868-5145															10.1007/s12652-020-02244-z		JUN 2020											
J								Competitive swarm optimizer with mutated agents for finding optimal designs for nonlinear regression models with multiple interacting factors	MEMETIC COMPUTING										c-optimal design; D-optimal design; Large scale global optimization; Optimal exact design; Swarm optimization	GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; MEMETIC ALGORITHMS; SELECTION; TIME	This paper proposes a novel enhancement for competitive swarm optimizer (CSO) by mutating loser particles (agents) from the swarm to increase the swarm diversity and improve space exploration capability, namely competitive swarm optimizer with mutated agents (CSO-MA). The selection mechanism is carried out so that it does not retard the search if agents are exploring in promising areas. Simulation results show that CSO-MA has a better exploration-exploitation balance than CSO and generally outperforms CSO, which is one of the state-of-the-art metaheuristic algorithms for optimization. We show additionally that it also generally outperforms swarm based types of algorithms and an exemplary and popular non-swarm based algorithm called Cuckoo search, without requiring a lot more CPU time. We apply CSO-MA to find ac-optimal approximate design for a high-dimensional optimal design problem when other swarm algorithms were not able to. As applications, we use the CSO-MA to search various optimal designs for a series of high-dimensional statistical models. The proposed CSO-MA algorithm is a general-purpose optimizing tool and can be directly amended to find other types of optimal designs for nonlinear models, including optimal exact designs under a convex or non-convex criterion.																	1865-9284	1865-9292				SEP	2020	12	3					219	233		10.1007/s12293-020-00305-6		JUN 2020											
J								Multi-geometric Sparse Subspace Clustering	NEURAL PROCESSING LETTERS										Riemannian manifold; Sparse subspace clustering; Multi-geometric	STATISTICS; ALGORITHM	Recently, the Riemannian manifold has received special attention in unsupervised clustering since the real-world visual data usually resides on a special manifold where Euclidean geometry fails to capture. Although many clustering algorithms have been proposed, most of them use only a single geometric model to describe the data. In this paper, a multi-geometric subspace clustering model is proposed, and the subspace representation is learned together by constructing a shared affinity matrix of multi-order data. Experimental results on several different types of datasets show that the clustering performance of our proposed algorithm is better than most of subspaces algorithms.																	1370-4621	1573-773X				AUG	2020	52	1			SI		849	867		10.1007/s11063-020-10274-z		JUN 2020											
J								Semi-supervised Learning Algorithm Based on Linear Lie Group for Imbalanced Multi-class Classification	NEURAL PROCESSING LETTERS										Lie group; Lie group machine learning; Semi-supervised learning; Imbalanced data; Multi-class classification	FRAMEWORK; RECOGNITION; ENSEMBLE	In practical application, the data are imbalanced, it is difficult to find the balanced, rather skewed data is the common occurrence. This poses a severe challenge to the classification algorithm. At present, imbalanced data classification methods are mainly for binary classes designed, and it is difficult to extend them to multiple classes. In this study, we introduced Lie group machine learning and proposed a semi-supervised learning algorithm based on the linear Lie group. First, the sample set is represented by a matrix, the isomorphism(or homomorphism)-GL(n) linear Lie group of the corresponding learning system is found, and the labeled data are used to represent the object to be learned by linear Lie group. Then, according to the algebraic structure of the linear Lie group, it is marked by the group method. We performed experiments on 18 benchmark multi-class imbalanced datasets to demonstrate the performance of our proposed method and measured the performance of multi-class imbalanced data using four state-of-the-art learning algorithms (mean of accuracy, mean of f-measure, and mean of area under the curve). The experimental results demonstrate that the proposed method is effective and improves the performance.																	1370-4621	1573-773X				AUG	2020	52	1			SI		869	889		10.1007/s11063-020-10287-8		JUN 2020											
J								An efficient global representation constrained by Angular Triplet loss for vehicle re-identification	PATTERN ANALYSIS AND APPLICATIONS										Vehicle re-identification; Angular Triplet loss; Metric learning		Vehicle re-identification is becoming an increasingly important problem in modern intelligent transportation systems. Substantial results have been achieved with methods based on deep metric learning. Most of the previous works tend to design complicated neural network models or utilize extra information. In this work, we introduce a simple Angular Triplet loss on the basis of analysis of different feature representations constrained by softmax loss and triplet loss. A batch normalization layer with zero bias is adopted to pass through the embedded feature before loss calculation. Then, triplet loss is calculated in cosine metric space instead of Euclidean space. In this way, triplet loss can cooperate with softmax consistently. By unifying the metric space of these two types of losses, the proposed method achieves 77.3% and 95.9% in rank-1 on VehicleID and VeRi-776 datasets, respectively. With only global features utilized, the proposed model can be seen as an effective baseline for vehicle re-identification task.																	1433-7541	1433-755X															10.1007/s10044-020-00900-w		JUN 2020											
J								Nonparametric "anti-Bayesian" quantile-based pattern classification	PATTERN ANALYSIS AND APPLICATIONS										"Anti-Bayesian" classification; Nonparametric quantile-based method; Mixture model; Sample quantile; Kernel density estimation; Robust classification		Parametric and nonparametric pattern recognition have been studied for almost a century based on a Bayesian paradigm, which is, in turn, founded on the principles of Bayes theorem. It is well known that the accuracy of the Bayes classifier cannot be exceeded. Typically, this reduces to comparing the testing sample to mean or median of the respective distributions. Recently, Oommen and his co-authors have presented a pioneering and non-intuitive paradigm, namely that of achieving the classification by comparing the testing sample with another descriptor, which could also be quite distant from the mean. This paradigm has been termed as being "anti-Bayesian," and it essentially uses the quantiles of the distributions to achieve the pattern recognition. Such classifiers attain the optimal Bayesian accuracy for symmetric distributions even though they operate with a non-intuitive philosophy. While this paradigm has been applied in a number of domains (briefly explained in the body of this paper), its application for nonparametric domains has been limited. This paper explains, in detail, how such quantile-based classification can be extended to the nonparametric world, using both traditional and kernel-based strategies. The paper analyzes the methodology of such nonparametric schemes and their robustness. From a fundamental perspective, the paper utilizes the so-called large sample theory to derive strong asymptotic results that pertain to the equivalence between the parametric and nonparametric schemes for large samples. Apart from the new theoretical results, the paper also presents experimental results demonstrating their power. These results pertain to artificial data sets and also involve a real-life breast cancer data set obtained from the University Hospital Centre of Coimbra. The experimental results clearly confirm the power of the proposed "anti-Bayesian" procedure, especially when approached from a nonparametric perspective.																	1433-7541	1433-755X															10.1007/s10044-020-00903-7		JUN 2020											
J								Ternary tree-based structural twin support tensor machine for clustering	PATTERN ANALYSIS AND APPLICATIONS										Twin support tensor machine; Unsupervised learning; Tree-based clustering; Tensor space; Ternary decision structure	VECTOR MACHINE	Most of the real-life applications usually involve complex data, e.g., grayscale images, where information is distributed spatially in the form of two-dimensional matrices (elements of second-order tensor space). Traditional vector-based clustering models such ask-means and support vector clustering rely on low-dimensional features representations for identifying patterns and are prone to loss of useful information which is present in spatial structure of the data. To overcome this limitation, tensor-based clustering models can be utilized for identifying relevant patterns in matrix data as they take advantage of structural information present in multi-dimensional framework and reduce computational overheads as well. However, despite these numerous advantages, tensor clustering has still remained relatively unexplored research area. In this paper, we propose a novel clustering framework, termed as Ternary Tree-based Structural Least Squares Support Tensor Clustering (TT-SLSTWSTC), that builds a cluster model as a hierarchical ternary tree, where at each node non-ambiguous data are dealt separately from ambiguous data points using the proposed Ternary Structural Least Squares Support Tensor Machine (TS-LSTWSTM). The TS-LSTWSTM classifier considers the structural risk minimization of data alongside a symmetrical L2-norm loss function. Also, initialization framework based on tensork-means has been used in order to overcome the instability disseminated by random initialization. To validate the efficacy of the proposed framework, computational experiments have been performed on human activity recognition and image recognition problems. Experimental results show that our method is not only fast but yields significantly better generalization performance and is comparatively more robust in order to handle heteroscedastic noise and outliers when compared to related methods.																	1433-7541	1433-755X															10.1007/s10044-020-00902-8		JUN 2020											
J								A novel multi-attribute decision-making framework based on Z-RIM: an illustrative example of cloud service selection	SOFT COMPUTING										BWM; MDM; RIM; Z-numbers; Z-RIM	RANK REVERSAL; FUZZY TOPSIS; SUPPLY CHAIN; MODEL; MCDM; UNCERTAINTY; PERFORMANCE; STRATEGIES; ALGORITHM; NUMBERS	In the multi-attribute decision-making (MADM) problems, decision makers refer to the extreme attribute values in general when evaluating the alternatives. However, in the real world, the ideal solution may lie in somewhere between the extreme values. The recently proposed reference ideal method (RIM) is able to solve the problem rightly. This study aims at developing a novel MADM framework combining best-worst method (BWM), maximizing deviation method (MDM), and RIM under Z-number environment. In this framework, Z-number is used to depict the inherent uncertainty and reliability of information in the decision makers' judgments. And BWM and MDM are combined to determine the comprehensive attribute weights, in which BWM is utilized to obtain the subjective weights, while MDM is utilized to obtain the objective weights. In addition, Z-RIM is proposed by extending the traditional RIM under Z-number environment, which is employed for ranking the alternatives. An illustrative example of cloud service selection problem is implemented to illustrate the proposed framework. By comparison analysis, we demonstrate that Z-RIM can not only avoid rank reversal problem, but also generate reasonable results during the MADM processes.																	1432-7643	1433-7479															10.1007/s00500-020-05087-0		JUN 2020											
J								Artificial intelligent controller-based power quality improvement for microgrid integration of photovoltaic system using new cascade multilevel inverter	SOFT COMPUTING										Photovoltaic system; Artificial Intelligence; MPPT; Multilevel inverter; THD		Nowadays, grid-connected photovoltaic (PV) power system is quite popular in many countries. For grid-connected PV power system, to achieve maximum power and good power quality of the system are considered as big challenges. In order to achieve this, artificial intelligent (AI) controller-based maximum power point tracking (MPPT) algorithm has been investigated for PV system as well as a new cascade multilevel inverter (MLI) is proposed for grid integration of PV system. The proposed cascade MLI has been designed with a smaller number of power electronic switches and it can be operated at asynchronous voltage sources, which is most adaptable for PV system. This proposed inverter can reduce the total harmonic distortion (THD) at output side by means of increasing the output voltage level with this power quality of the system has been improved. The microgrid integration of proposed inverter has been controlled by using AI-based voltage source controller and this proposed system is designed and simulated in MATLAB environment at various weather conditions and loading conditions. Finally, prototype model has been developed in laboratory and evaluating its performance. The simulation results and prototype results were verified with IEEE 1547 standard that proves the proposed system effectiveness.																	1432-7643	1433-7479															10.1007/s00500-020-05120-2		JUN 2020											
J								Correlation structure between agroclimatic big data and EVOO fatty acid profile determined by GC and NMR spectra	JOURNAL OF CHEMOMETRICS										agroclimatic data; extra virgin olive oil; fatty acid profile; linear correlation; NMR	OLIVE OILS; WEATHER CONDITIONS; REGIOISOMER COMPOSITIONS; INFRARED-SPECTRA; L.; LATITUDE; PRECIPITATION; CULTIVARS; AREAS; H-1	Fatty acids are the major compounds in olive oils, and the determination of their profile is very useful in order for the authentication of the high quality of extra virgin olive oil (EVOO) and the evaluation of its purity and traceability. This work considers the estimations of saturated (SAFA), monounsaturated (MUFA) and polyunsaturated (PUFA) fatty acids obtained by gas chromatography (GC),C-13 (carbonyl, C-16, and aliphatic carbons regions), and(1)H NMR spectra. The aim of this paper is to analyze the correlation structure being between the abovementioned fatty acids estimations and some long-run agroclimatic measurements (temperature, humidity, wind speed and direction, radiation, precipitation, and evapotranspiration), which were downloaded from the official website of the Andalusian Automatic Weather Stations (AWSs). The versatile graphical possibilities of the free softwareR-projectallow the design of computational programs providing interesting conclusions about the described correlations as a function of month, variety of olive, and protected designation of origin (PDO). The statistically significant correlations could determine relevant agroclimatic information to be included, in a further research, among the explanatory variables in regression models in order to improve the estimation of fatty acid profile of EVOOs, considered as a quality parameter in their authentication.																	0886-9383	1099-128X				AUG	2020	34	8							e3273	10.1002/cem.3273		JUN 2020											
J								Near-infrared multivariate model transfer for quantification of different hydrogen bonding species in aqueous systems	JOURNAL OF CHEMOMETRICS										aqueous solution; hydrogen bond; liquid water organization; model transfer; near-infrared	LIQUID WATER; CORRELATION SPECTROSCOPY; TEMPERATURE-DEPENDENCE; CALIBRATION TRANSFER; SPECTROMETRY NIRS; CURVE RESOLUTION; RAMAN; IONS; SPECTRA; HYDRATION	It is challenging to achieve consistent quantification of hydrogen-bonded water species by fitting Gaussian bands on spectra collected from different instruments, especially when the spectral resolution is low. Therefore, this work aims to propose a near-infrared (NIR) multivariate calibration model transfer method to obtain reliable, consistent, and reproducible results from different spectroscopic instruments. A composite method of Gaussian deconvolution, multivariate curve resolution-alternating least squares (MCR-ALS), and partial least squares (PLS) was developed to quantify five water species (labeled as C1, C2, C3, C4, and C5), respectively, assigned to non-hydrogen-bonded, singly hydrogen-bonded, doubly and triply hydrogen-bonded species, distorted ice-like structures in the liquid phase, and fully hydrogen-bonded species showing ice-like structure. Primary instrument (iS50) was selected due to the highest spectral resolution and spectra (7500-5845 cm(-1)with spectral resolution of 0.5 cm(-1)) were acquired from four types of salt solutions. The developed PLS model yielded a high prediction performance when applied on two independent prediction sets withR(P)(2)higher than 0.95 and RMSEP less than 0.009 for prediction of C1-5. Model built with primary spectra was successfully transferred to spectra of secondary instrument (iN10), producing RMSEP less than 0.010. Transferred model applied on secondary instruments (NIRECO and MPA) showed that fully H-bonded species in deionized water decreased as temperature increased, while non-hydrogen-bonded and singly hydrogen-bonded demonstrated an opposite temperature dependence. Our results confirmed that it is promising to transfer the calibration model from a sophisticated spectrometer to a simple spectroscopic instrument in terms of quantification of hydrogen-bonded water species.																	0886-9383	1099-128X				SEP	2020	34	9							e3274	10.1002/cem.3274		JUN 2020											
J								Dynamic feature selection method with minimum redundancy information for linear data	APPLIED INTELLIGENCE										Feature selection; Mutual information; Conditional redundancy; Linear data	MUTUAL INFORMATION; CLASSIFICATION; DEPENDENCY; REGRESSION; RELEVANCE	Feature selection plays a fundamental role in many data mining and machine learning tasks. In this paper, we proposed a novel feature selection method, namely, Dynamic Feature Selection Method with Minimum Redundancy Information (MRIDFS). In MRIDFS, the conditional mutual information is used to calculate the relevance and the redundancy among multiple features, and a new concept, the feature-dependent redundancy ratio, was introduced. Such ratio can represent redundancy more accurately. To evaluate our method, MRIDFS is tested and compared with seven popular methods on 16 benchmark data sets. Experimental results show that MRIDFS outperforms in terms of average classification accuracy.																	0924-669X	1573-7497				NOV	2020	50	11					3660	3677		10.1007/s10489-020-01726-z		JUN 2020											
J								Large margin classifiers to generate synthetic data for imbalanced datasets	APPLIED INTELLIGENCE										Imbalanced learning; Large margin classifiers; Oversampling; Synthetic sample generation	CLASSIFICATION	In this paper we propose the development of an approach capable of improving the results obtained by classification algorithms when applied to imbalanced datasets. The method, called Incremental Synthetic Balancing Algorithm (ISBA), performs an iterative procedure based on large margin classifiers, aiming to generate synthetic samples in order to reduce the level of imbalance. In the process, we use the support vectors as the reference for the generation of new instances, allowing them to be positioned in regions with greater representativeness. Furthermore, the new samples can exceed the limits of the ones used for their generation, which enables extrapolation of the boundaries of the minority class, achieving more significant recognition of this class of interest. We present comparative experiments with other techniques, among them the SMOTE, which provide strong evidence of the applicability of the proposed approach.																	0924-669X	1573-7497				NOV	2020	50	11					3678	3694		10.1007/s10489-020-01719-y		JUN 2020											
J								Chaos Game Optimization: a novel metaheuristic algorithm	ARTIFICIAL INTELLIGENCE REVIEW										Metaheuristic; Statistical analysis; Chaos Game Optimization	IMPERIALIST COMPETITIVE ALGORITHM; SYMBIOTIC ORGANISMS SEARCH; GLOBAL OPTIMIZATION; SIZE OPTIMIZATION; KRILL HERD; EVOLUTION	In this paper, a novel metaheuristic algorithm called Chaos Game Optimization (CGO) is developed for solving optimization problems. The main concept of the CGO algorithm is based on some principles of chaos theory in which the configuration of fractals by chaos game concept and the fractals self-similarity issues are in perspective. A total number of 239 mathematical functions which are categorized into four different groups are collected to evaluate the overall performance of the presented novel algorithm. In order to evaluate the results of the CGO algorithm, three comparative analysis with different characteristics are conducted. In the first step, six different metaheuristic algorithms are selected from the literature while the minimum, mean and standard deviation values alongside the number of function evaluations for the CGO and these algorithms are calculated and compared. A complete statistical analysis is also conducted in order to provide a valid judgment about the performance of the CGO algorithm. In the second one, the results of the CGO algorithm are compared to some of the recently developed fractal- and chaos-based algorithms. Finally, the performance of the CGO algorithm is compared to some state-of-the-art algorithms in dealing with the state-of-the-art mathematical functions and one of the recent competitions on single objective real-parameter numerical optimization named "CEC 2017" is considered as numerical examples for this purpose. In addition, a computational cost analysis is also conducted for the presented algorithm. The obtained results proved that the CGO is superior compared to the other metaheuristics in most of the cases.																	0269-2821	1573-7462															10.1007/s10462-020-09867-w		JUN 2020											
J								Feasibility of spectral domain techniques for the classification of non-stationary signals	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Singular value decomposition; Framelet transform; Discrete wavelet transform; Discrete cosine transform; Sawtooth; Burst	FEATURE-EXTRACTION; TRANSFORM	Extensive research is carried out in the analysis of non stationary signals. Most of the real time signals are non-stationary in nature. In most cases, these non stationary signals are of types viz defect / non defect, normal / abnormal etc. Hence analysis refers to categorising the signals. Developing a signal processing algorithm for performing the above task is a major challenge. In Machine learning, features are extracted and classifiers are used for categorizing the signals. Feature extraction can be done in time domain, frequency domain and spectral domain. In this paper, feasibility of Singular Value Decomposition (SVD), Framelet Transforms, Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) for feature extraction is studied. These features are aggregated using statistical parameters, namely mean, skewness and kurtosis. These aggregated features are then fed to Back Propagation Network (BPN). Performance of Back Propagation Network is measured in terms of sensitivity, specificity and accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02220-7		JUN 2020											
J								Software architecture of the internet of things (IoT) for smart city, healthcare and agriculture: analysis and improvement directions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Software architecture; Smart city; Healthcare; Agriculture; Architectural paradigms	MODEL	Internet of things (IoT) enables organizations to automate the process and improves service delivery through Internet technology and transferring the data at the cloud level. IoT does not allow the use of a universal software architecture for different fields in which it is used, but needs to be adjusted according to the requirements of users. This paper presents an analysis of currently available types of software architectures of the IoT systems in the field of smart cities, healthcare, and agriculture. It provides a proposal for solutions and improvements of different software architecture types, interactions between identified software architecture elements that will provide better performance and simplicity. The novelty of the study is the analysis of different types of IoT software architecture such as: layered, service-oriented and cloud-based software architecture application in these areas of IoT. Based on the analysis, the study proposed the type of software architecture of the IoT system for the relevant area of application (smart city, healthcare, and agriculture). Specific points of research are: analysis of different types of software architecture applied in IoT systems, identification of functionalities available in IoT systems through different types of software architecture, the proposal for enhancement of the above functionalities, and proposal of software architecture that is most relevant to the IoT system of a particular area.																	1868-5137	1868-5145															10.1007/s12652-020-02197-3		JUN 2020											
J								Echo state learned compositional pattern neural networks for the early diagnosis of cancer on the internet of medical things platform	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things (IoT); Machine learning and data mining techniques; Cancer; Iterated memetic correlation features; Cognitive decision; Echo state deep learning process; Compositional pattern neural network		Recent years medical fields are facing more challenging issues while detecting disease because day by day disease are developed due to the several factors. The IoT medical device is placed in human body that captures the people's cancer related health conditions such as breast conditions, skin rashes, teeth related information and lung information is collected. The gathered data analyzed using machine learning, data mining algorithm to get the changes present in the people health condition. Features are determined from the collected information, and optimized features are chosen by the implementation of a selection method for Iterated memetic correlations. The selected features are trained using an echo state deep learning process that uses various levels of hidden layers that solve possible cognitive cancer decisions without making any errors. The new features are classified using the compositional pattern neural network with the assistance of trained features. Finally, system efficiency is assessed using experimental results such as F-measure, mean absolute error, precision, recall and abnormal pattern prediction rate.																	1868-5137	1868-5145															10.1007/s12652-020-02218-1		JUN 2020											
J								An energy and coverage sensitive approach to hierarchical data collection for mobile sink based wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network (WSN); Energy hole problem; Mobile sink (MS); Rendezvous node (RN); Coverage significance	DATA DISSEMINATION MODEL; CLUSTERING PROTOCOL; ROUTING PROTOCOL; EFFICIENT; LIFETIME; DEPLOYMENT; ALGORITHM; HYBRID; AWARE	In recent years, wireless sensor networks experience the energy hole problem as the most critical issue due to the heavy data forwarding load on the proximate sensor nodes to the sink. The best known solution found by the current state-of-the-art approaches for the energy hole problem is the Mobile Sink (MS) strategy. However, allowing the MS to visit every node for data collection incurs high data delivery latency, which may not be feasible in delay-sensitive applications. Thus, in this paper, restricted mobile sink motion is considered, where the MS halts at a limited number of locations stated as sojourn locations and all nodes disseminate their data to the nearby sojourn locations. The data dissemination to the sojourn location is achieved via a cluster-based routing protocol which aims to preserve the sensor nodes' energy to enhance the network lifetime. Furthermore, analogous to network lifetime, extending the coverage lifetime is of equal importance in many coverage sensitive applications of WSN. Thus, this article incorporates the coverage parameter to the proposed protocol in order to preserve the network coverage despite certain nodes die. Based on the sojourn locations, the proposed routing algorithm ensures that each cluster data is disseminated to the MS following the minimum hop path to limit the data delivery delay. Experimental results demonstrate the efficacy of the proposed protocol over several state-of-the-art protocols with respect to different metrics like network lifetime, coverage ratio, energy efficiency, packet delivery ratio, end-to-end delay, etc.																	1868-5137	1868-5145															10.1007/s12652-020-02176-8		JUN 2020											
J								A discrete wavelet transform and recurrent neural network based medical image compression for MRI and CT images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Discrete wavelet transform; Gravitational search algorithm; Image compression; Optimized recurrent neural network; Particle swarm optimization; Peak signal to noise ratio	WATERMARKING; ALGORITHM	Medical imaging is an active and developing field that has an impact on recognition, diagnosis and surgical planning of the disease. The image compression is introduced in the medical imaging for decreasing the redundancies to avoid storage and bandwidth related problems. In this research, Discrete Wavelet Transform and Recurrent Neural Network based compression techniques are used through brain images for achieving better compression rate with less loss. The region growing and otsu thresholding are used to separate the ROI image and Non ROI image. The DWT and RNN are used for the ROI and non ROI portions of the medical image. Additionally, the local attributes of RNN is enhanced by Gravitational Search Algorithm and Particle Swarm Optimization. The performance of this proposed method is calculated as peak signal to noise ratio, mean square error, compression ratio and space saving percentage. The existing techniques used to compare the proposed method are Fractional Order Darwinian PSO, Quasi Fractal and Oscillation Method and Burrows Wheeler Transform-move to front transform. The PSNR of the proposed method is 35.2362 dB which is high when compared to the existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02212-7		JUN 2020											
J								Low-latency cloud-fog network architecture and its load balancing strategy for medical big data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Load balancing strategy; Fog computing; Edge network equipment; Medical big data; Hybrid cloud-fog network; Improved bat algorithm; Powell local search		In order to apply fog computing to the field of medical big data, this paper proposes a low-latency hybrid cloud-fog network architecture for medical big data, which can solve the processing delay of business in cloud computing center architecture. In this architecture, edge network equipment such as routers and switches in the hospital are used to build a "fog computing" layer between the cloud server and terminals. Then, the computing service for medical data on the cloud is moved to fog equipment for processing, which reduces the processing delay of medical businesses. Besides, it reduces the computing load on cloud servers and improves the overall robustness of network. For further optimizing the processing delay of business in the above network architecture, we study the load balancing strategy in fog computing network. Due to the global search ability of bat algorithm is strong, the convergence speed is fast and it is easy to implement. Therefore, this paper uses bat algorithm to solve the optimization problem in medical big data scenario. Bat algorithm is better than genetic algorithm and particle swarm optimization on the unconstrained optimization problems. However, it also suffers from problems such as local optimization and slow convergence. To solve this problem, we utilize load balancing to initialize bat population data that improves the quality of solution for initial samples. After getting the best bats, a Powell local search is performed on them, which speeds up the convergence of our algorithm. Finally, simulation results show that the proposed hybrid cloud-fog network architecture can reduce the processing delay of medical big data and improve user experience effectively.																	1868-5137	1868-5145															10.1007/s12652-020-02245-y		JUN 2020											
J								A Binocular MSCKF-Based Visual Inertial Odometry System Using LK Optical Flow	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-state constraint Kalman filter (MSCKF); Visual inertial Odometry (VIO); Pose estimation; Robot operating system (ROS)	RECONSTRUCTION SYSTEM; TRACKING; MOTION; FILTER; SCALE	The odometry is an important part of intelligent mobile robots to achieve positioning and navigation functions. At present, the mainstream visual odometry locates only through the visual information obtained by camera sensors. Therefore, in the case of insufficient light, texture missing and camera jitter, the visual odometry is difficult to locate accurately. To solve the problem, we propose a binocular MSCKF-based visual inertial odometry system using Lucas-Kanade (LK) optical flow. Firstly, the Inertial Measurement Unit (IMU) is introduced to overcome the above problems. Moreover, LK optical flow algorithm is utilized to process the visual information obtained by the binocular camera, and MSCKF algorithm is employed to realize the fusion of visual information and inertial information, which improves the accuracy and efficiency of the visual inertial odometry system positioning. Finally, the proposed method is simulated on the European Robotics Challenge (EuRoc) dataset by Robot Operating System (ROS), and compared with two other advanced visual inertial odometry systems, ROVIO and MSCKF-mono. A large number of simulations verify that the proposed method can achieve accurate pose estimation, which is superior to the two existing advanced visual inertial odometry systems.																	0921-0296	1573-0409															10.1007/s10846-020-01222-z		JUN 2020											
J								Cooperative Path Following Control of Fixed-wing Unmanned Aerial Vehicles with Collision Avoidance	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Cooperative path following; Fixed-wing UAVs; Vector field histogram (VFH); Collision avoidance	MODEL-PREDICTIVE CONTROL; MULTIPLE UAVS; TRACKING; ROBOT	In this paper, we propose a novel curved path following scheme for multiple fixed-wing unmanned aerial vehicles (UAVs) that can achieve coordinated curved path following and handle collision avoidance simultaneously. The proposed solution is a hybrid system that combines a path following strategy and a collision avoidance method to ensure collision-free maneuvering of a group of UAVs cooperative flight. A strategy based on a virtual structure and a kinematic model is proposed to derive the cooperative curved path following. Meanwhile, a possible collision will occur when multi-UAVs flight in the form of dense formations, or there are path intersections when the formation is changed. However, all fixed-wing vehicles need to maintain a minimum airspeed and cannot stop before the collision. Therefore, there is a need for a fast and efficient method that can be implemented online, taking into account the physical limitations of the vehicle (minimum speed, turning rate, etc.) to avoid possible collision. We employ a modified vector field histogram (VFH) method to provide real-time collision avoidance, and evasive maneuvers work only if the distance is within the conflict zone. In order to verify the proposed control scheme, dense formation flight tests of small fixed-wing UAVs were implemented.																	0921-0296	1573-0409															10.1007/s10846-020-01210-3		JUN 2020											
J								A PSO-inspired architecture to hybridise multi-objective metaheuristics	MEMETIC COMPUTING										Multi-objective optimisation; Hybridisation of metaheuristics; Bi-objective spanning tree	ALGORITHM	Hybridisation is a technique that exploits and unites the best features of individual algorithms. The literature includes several hybridisation methodologies, among which there are general procedures, termed architectures, that provide generic functionalities and features for solving optimisation problems. Successful hybridisation methodologies have applied concepts of the multi-agent paradigm, such as cooperation and agent intelligence. However, there is still a lack concerning architectures for the hybridisation of multi-objective metaheuristics that fully explore these concepts. This study proposes a new architecture, namedMO-MAHM, based on concepts from Particle Swarm Optimisation, to hybridise multi-objective metaheuristics. We apply theMO-MAHMto the Bi-objective Spanning Tree Problem. Four algorithms were hybridised within theMO-MAHM: three evolutionary algorithms and a local search method. We report the results of experiments with 180 instances, analyse the behaviour of theMO-MAHM, and compare to the results produced by algorithms proposed for the Bi-objective Spanning Tree Problem.																	1865-9284	1865-9292				SEP	2020	12	3					235	249		10.1007/s12293-020-00307-4		JUN 2020											
J								A semi-supervised recurrent neural network for video salient object detection	NEURAL COMPUTING & APPLICATIONS										Video salient object detection; Recurrent neural network; Visual attention; Semi-supervised training	SEGMENTATION; TRACKING	A semi-supervised, one-dimensional recurrent neural network (RNN) approach called RVS has been proposed in this paper for video salient object detection. The proposed RVS approach involves the processing of each frame independently without explicitly considering temporal information. The RNN is trained using one-dimensional superpixel features to classify the salient object regions into salient foreground and non-salient background superpixels. Deep learning algorithms generally exhibit heavy dependence on training data size and often take extremely long time for training. On the contrary, the proposed RVS approach involves the training of an RNN using a small data which results in significant reduction in training time. The RVS approach has been extensively evaluated and its results are compared with those of several state-of-the-art methods using the public-domain VideoSeg, SegTrack v1 and SegTrack v2 benchmark video datasets. Further, the RVS approach has been tested using the authors' own video dataset and the complex DAVIS and video object segmentation datasets to evaluate the impact of motion and blur on its performance. The RVS approach delivers results superior to those of several approaches that strongly rely upon spatio-temporal features in detecting the salient objects from the video sequences.																	0941-0643	1433-3058															10.1007/s00521-020-05081-5		JUN 2020											
J								Group decision-making framework using complex Pythagorean fuzzy information	NEURAL COMPUTING & APPLICATIONS										Group decision-making; VIKOR; Complex Pythagorean fuzzy model; Ranking measure	EXTENDED VIKOR METHOD; SELECTION; MODEL; SETS	This paper contributes to the expanding literature on VIKOR, a well-known multi-criteria decision-making technique. It is abundantly used to find the compromise solution regarding some significant criteria under different models. The general VIKOR methodology implements the principle that a recommended response must be a feasible solution which is nearest to positive ideal solution having maximum group utility and minimum individual regret. In this article, we propose a new strategy to address multi-criteria group decision-making problems named complex Pythagorean fuzzy VIKOR (CPF-VIKOR) method. It is designed to handle a great deal of vagueness and hesitation which are often present in human decisions. The CPF-VIKOR method allows the linguistic terms to express individual opinions of experts about the performance of alternatives and the weights of the criteria. We combine the individual judgments of experts with the help of complex Pythagorean fuzzy weighted averaging operator. Further, we compute the ranking measure with the help of group utility and regret measures by adjusting the weight of strategy of maximum group utility within the unit interval. We sort the alternatives in an ascending order relative to group utility measure and individual regret measure. Moreover, we demonstrate our proposed method with the help of a flow chart and two numerical examples: one for the selection of the most beneficial renewable energy project in Spain and another for the selection of the most suitable logistic village location in Turkey. Finally, we present the comparative analysis of our proposed method with the CPF-TOPSIS method to prove its validity.																	0941-0643	1433-3058															10.1007/s00521-020-05100-5		JUN 2020											
J								Multi-deep features fusion for high-resolution remote sensing image scene classification	NEURAL COMPUTING & APPLICATIONS										Deep convolutional network; Feature fusion; Multi-subset feature fusion; Scene recognition; Remote sensing	CONVOLUTIONAL NEURAL-NETWORKS; LAND-USE; FEATURE-EXTRACTION; ATTENTION; MODELS	In view of the small number of categories and the relatively little amount of labeled data, it is challenging to apply the fusion of deep convolution features directly to remote sensing images. To address this issue, we propose a pyramid multi-subset feature fusion method, which can effectively fuse the deep features extracted from different pre-trained convolutional neural networks and integrate the global and local information of the deep features, thereby obtaining stronger discriminative and low-dimensional features. By introducing the idea of weighting the difference between different categories, the weight discriminant correlation analysis method is designed to make it pay more attention to those categories that are not easy to distinguish. In order to mine global and local feature information, the pyramid method is employed to divide feature fusion into several layers. Each layer divides the features into several subsets and then performs feature fusion on the corresponding feature subsets, and the number of subsets from top to bottom gradually increases. Feature fusion at the top of the pyramid obtains a global representation, while feature fusion at the bottom obtains a local detail representation. Our experiment results on three public remote sensing image data sets demonstrate that the proposed multi-deep features fusion method produces improvements over other state-of-the-art deep learning methods.																	0941-0643	1433-3058															10.1007/s00521-020-05071-7		JUN 2020											
J								SAOSA: Stable Adaptive Optimization for Stacked Auto-encoders	NEURAL PROCESSING LETTERS										Deep neural network; Stacked auto-encoder; Cost function optimization; Stable adaptive learning	NATURAL GRADIENT; STABILITY	The stacked auto-encoders are considered deep learning algorithms automatically extracting meaningful unsupervised features from the input data using a hierarcfhical learning process. The parameters are learnt layer-by-layer in each auto-encoder (AE). As optimization is one of the main components of the neural networks and auto-encoders, the learning rate is one of the crucial hyper-parameters of neural networks and AE. This issue on a large scale and especially sparse data sets is more important. In this paper, we adapt the learning rate for special AE corresponding to various components of AE networks in each stochastic gradient calculation and analyze the theoretical convergence of back-propagation learning for the proposed method. We also promote our methodology for online adaptive optimizations suitable for deep learning. We obtain promising results compared to constant learning rates on the (1) MNIST digit, (2) blogs-Gender-100 text, (3) smartphone based recognition of human activities and postural transitions time series, and (4) EEG brainwave feeling emotions time series classification tasks using a single machine.																	1370-4621	1573-773X				AUG	2020	52	1			SI		823	848		10.1007/s11063-020-10277-w		JUN 2020											
J								Deep learning-based effective fine-grained weather forecasting model	PATTERN ANALYSIS AND APPLICATIONS										Long short-term memory; Temporal convolutional networks; Weather prediction; WRF; Neural network; Time-series data analysis	DAILY MAXIMUM; PREDICTION	It is well-known that numerical weather prediction (NWP) models require considerable computer power to solve complex mathematical equations to obtain a forecast based on current weather conditions. In this article, we propose a novel lightweight data-driven weather forecasting model by exploring temporal modelling approaches of long short-term memory (LSTM) and temporal convolutional networks (TCN) and compare its performance with the existing classical machine learning approaches, statistical forecasting approaches, and a dynamic ensemble method, as well as the well-established weather research and forecasting (WRF) NWP model. More specifically Standard Regression (SR), Support Vector Regression (SVR), and Random Forest (RF) are implemented as the classical machine learning approaches, and Autoregressive Integrated Moving Average (ARIMA), Vector Auto Regression (VAR), and Vector Error Correction Model (VECM) are implemented as the statistical forecasting approaches. Furthermore, Arbitrage of Forecasting Expert (AFE) is implemented as the dynamic ensemble method in this article. Weather information is captured by time-series data and thus, we explore the state-of-art LSTM and TCN models, which is a specialised form of neural network for weather prediction. The proposed deep model consists of a number of layers that use surface weather parameters over a given period of time for weather forecasting. The proposed deep learning networks with LSTM and TCN layers are assessed in two different regressions, namely multi-input multi-output and multi-input single-output. Our experiment shows that the proposed lightweight model produces better results compared to the well-known and complex WRF model, demonstrating its potential for efficient and accurate weather forecasting up to 12 h.																	1433-7541	1433-755X															10.1007/s10044-020-00898-1		JUN 2020											
J								Extension of multi-Moora method with some q-rung orthopair fuzzy Dombi prioritized weighted aggregation operators for multi-attribute decision making	SOFT COMPUTING										q-rung orthopair fuzzy sets (q-ROFSs); Dombi operations; Arithmetic averaging operators; Geometric averaging operators; Multiple attribute decision making; MULTIMOORA method	SELECTION; NUMBER	The Dombi operators provide a flexible structure with its adjustable parameter because of Dombi generalized structure. On the other hand, priority aggregation operators play an important role in expressing the importance level of alternatives and attributes. In this study, novel Dombi prioritized aggregations are developed on q-rung orthopair fuzzy sets (q-ROFSs). The q-ROFSs include many fuzzy sets with dynamically changing q parameters. q-ROFSs include intuitionistic fuzzy sets, Pythagorean fuzzy sets and Fermatean fuzzy sets according to value of q parameter. In this study, Dombi prioritized aggregation of q-ROFSs is presented. The operators introduced are q-ROFSs Dombi prioritized weighted averaging operator (q-ROFSDPWA) and q-ROFSs Dombi prioritized weighted geometric operator (q-ROFSDPWG). We also investigate some of the properties of these operators. The proposed operators are used in MULTIMOORA method. The proposed methods with new aggregation operators are analyzed according to the q parameter of q-ROFSs and Dombi parameter on numerical example and also compared with other existing studies. It is seen that novel q-ROFSDPWA and q-ROFSDPWG aggregations give reasonable and stable results for multiple criteria decision making problem.																	1432-7643	1433-7479															10.1007/s00500-020-05091-4		JUN 2020											
J								Credit linked two-stage multi-objective transportation problem in rough and bi-rough environments	SOFT COMPUTING										Multi-objective transportation problem; Rough set; Bi-rough set; Credit period; Restricted fixed charge	FIXED-CHARGE; SUPPLY CHAIN; GENETIC ALGORITHM; MODEL; OPTIMIZATION; NETWORK	With a rapid growth of research in multi-objective transportation problem, rough and bi-rough sets are two new mathematical ideas for formulating real-world-based problems involving uncertain data. In this study, we have investigated a two-stage multi-objective transportation problem by considering credit period policy under rough and bi-rough environments. In this regard, three conflicting objective functions have been optimized simultaneously under the same restrictions. In first objective function, we have presented the minimization of transportation cost of a production house. In second objective function, total transportation cost of retailers has been minimized. But, in last one, we have maximized total profit of distributors. Besides, due to existence of different types of uncertainties in our real-life problems, in the proposed model, independent parameters (including, actual transportation cost, requirement of the retailers, and cost per unit distance) have been considered as rough in nature and dependent parameters such as demanded transportation cost and demand of the distributors have been considered as bi-rough in nature. Moreover, to convert the uncertain model into an equivalent deterministic form, a rough and bi-rough programming approach has been derived along with the expected value approach. Finally, by using these ideas, the mathematical model of our considered transportation problem has been illustrated. After that, the proposed model has been solved by applying NSGA-II algorithm (elitist non-dominated sorting genetic algorithm) with some simulated numerical data. Some sensitivity analysis associated with our proposed model has also been discussed to show the effectiveness of the model.																	1432-7643	1433-7479															10.1007/s00500-020-05066-5		JUN 2020											
J								Automatic segmentation and classification of liver tumor from CT image using feature difference and SVM based classifier-soft computing technique	SOFT COMPUTING										Liver; Tumor; SVM classifier; Difference feature; Region growing	AIDED DIAGNOSIS SYSTEM; TEXTURE ANALYSIS; TOMOGRAPHY; LESIONS; CANCER	The liver is essential for endurance and to carry out a large number of significant functions, including manufacture of indispensable proteins, and metabolism of fats and carbohydrates. The examination of CT might be employed for planning and managing the treatments for tumor in a proper way and for directing biopsies as well as other simply determined process. The Manual segmentation and Computed Axial Tomography (CT) image classification is a tedious task and time consuming process for large amount of data. Computer-Aided Diagnosis (CAD) systems take part in a fundamental role in the detection of liver disease in an early stage and therefore decrease death rate of liver cancer. In this paper an automatic CAD system is presented in three stage. In the first step, automatic liver segmentation and lesion's detection is carried out. Then, the next step is to extract features. At last, liver lesions classification into malignant and benign is done by using the novel contrast based feature-difference method. The extracted features from the lesion area with its surrounding normal liver tissue are based on intensity and texture. The lesion descriptor is obtained by considering the difference between the features of both lesion area and normal tissue of liver. Finally to categorize the liver lesions into malignant or benign a new SVM based machine learning classifier is trained on the new descriptors. The investigational outcome show hopeful improvement. Besides, the projected approach is insensitive to ranges of textures and intensity between demographics, imaging devices, and patients and settings. The classifier discriminates the tumor by comparatively high precision and offers a subsequent view to the radiologist.																	1432-7643	1433-7479															10.1007/s00500-020-05094-1		JUN 2020											
J								Genetic programming for high-dimensional imbalanced classification with a new fitness function and program reuse mechanism	SOFT COMPUTING										Genetic programming; Fitness function; Class imbalance; High dimensionality	SELECTION; ENSEMBLE; SMOTE	Genetic programming (GP) has been successfully applied to classification. However, GP may evolve biased classifiers when encountering the problem of class imbalance. These biased classifiers are often not reliable to be applied to some real-world applications. High dimensionality makes it more difficult for classifiers to effectively separate the majority class and the minority class. The use of GP to handle the joint effect of high dimensionality and class imbalance has not been heavily investigated. In this paper, we propose a GP approach to high-dimensional imbalanced classification, with the goals of increasing the classification performance as well as saving training time. To achieve this goal, a new fitness function is developed to solve the problem of class imbalance, and moreover, a strategy is proposed to reuse previous good GP individuals for improving efficiency. The proposed method is examined on ten high-dimensional imbalanced datasets. Experimental results show that, for high-dimensional imbalanced classification, the proposed method generally outperforms other GP methods and traditional classification algorithms using sampling methods to solve the problem of class imbalance.																	1432-7643	1433-7479															10.1007/s00500-020-05056-7		JUN 2020											
J								Prediction of form roughness coefficient in alluvial channels using efficient hybrid approaches	SOFT COMPUTING										ANN; Dune bedform; Bedform resistance; MLP-FFA	SUPPORT VECTOR MACHINE; BED FORM; FIREFLY ALGORITHM; LOAD; SEDIMENT; MODEL	In general, total roughness coefficient in open channels includes both grain resistance and bedform resistance. Due to the nonlinearity of the roughness coefficient, an accurate prediction of the bedform roughness is difficult. In this study, the capability of artificial neural network and multilayer perceptron (MLP) with firefly algorithm (MLP-FFA) were assessed in predicting the form resistance in channels with dune bedform. In this regard, different input combinations based on flow, bedform, and sediment characteristics were developed in order to determine the best combination. Five different experimental data series were applied to train and test the models. It was found that in predicting the form resistance, the model which took the advantages of both flow and sediment characteristics yielded to better outcomes. It was observed that the bedform characteristics led to an improvement in models accuracy. The results of the sensitivity analysis showed that the Reynolds number and the relative discharge were more effective parameters in the modeling process. Also, investigating the dune geometry (i.e., relative dune height) showed that the densimetric Froude number was the most significant variable.																	1432-7643	1433-7479															10.1007/s00500-020-05090-5		JUN 2020											
J								Uniform distribution driven adaptive differential evolution	APPLIED INTELLIGENCE										Box-constrained single objective optimization; Evolutionary optimization; Adaptive evolutionary algorithms; Reproduction operators; Differential Evolution	PARTICLE SWARM OPTIMIZATION; REDUCTION; ALGORITHM; SEARCH	Evolutionary algorithms are popular optimization tools for real-world applications due to their numerous advantages such as capability of parallel search along multiple directions by maintaining a population of candidates, invariance to certain mathematical properties (convexity, continuity and hardness) of fitness landscape and ability to handle black-box problems. However, most of the current evolutionary algorithms are loosely based on heuristics inspired by nature and lack the crucial theoretical background. Motivated by the overwhelming advantages of such optimization algorithms and the necessity for theoretical foundation, this paper presents a new evolutionary algorithm - UDE (Uniform Differential Evolution) for solving single- objective optimization problems along with a theoretical analysis of the proposed UDE algorithm. Thus, this paper formally gives insights about the features and properties of the various optimization strategies used. This method is different from traditional Differential Evolution variants as it employs a uniform probability distribution for generating new candidate solutions. UDE is further developed to obtain an adaptive evolutionary algorithm - Adaptive UDE (AUDE), which has shown to obtain significant improvements in the performance and convergence speeds compared to other algorithms on a benchmark set of 19 test problems. The source codes are available at.																	0924-669X	1573-7497				NOV	2020	50	11					3638	3659		10.1007/s10489-020-01707-2		JUN 2020											
J								Improved gait recognition through gait energy image partitioning	COMPUTATIONAL INTELLIGENCE										feature extraction; gait cycle; silhouette gait energy; gait recognition	EXTRACTION; INFORMATION; MODEL	Recently, human gait pattern has turned into an essential biometric feature to recognize an individual remotely. Gait as a feature becomes challenging owing to variation in appearance under different covariate conditions (eg, shoe, surface, haul, viewpoint and attire). The covariates may alter few fragment of gait while other fragment stay unaltered, leading to lower the probability of correct identification. To overcome such variation, an improved gait recognition strategy is proposed in this article by gait energy image partitioning and selection processing. Our method involves pre-processing of raw video for silhouette extraction, gait cycle detection, segmentation into different regions, and histogram of gradients feature extraction from selected segments. In this way, the specific features across complete gait cycles are extracted precisely. Finally, recognition is done by using K-NN. The proposed strategy has been assessed using the CASIA B gait database. Our outcomes shows a particular proposed strategy accomplishes high recognition rate and outperforms the advanced gait recognition mechanism.																	0824-7935	1467-8640				AUG	2020	36	3					1261	1274		10.1111/coin.12340		JUN 2020											
J								A transformer-based approach to irony and sarcasm detection	NEURAL COMPUTING & APPLICATIONS										Sentiment analysis; Natural language processing; Figurative language; Sarcasm; Irony; Deep learning; Transformer networks	LANGUAGE	Figurative language (FL) seems ubiquitous in all social media discussion forums and chats, posing extra challenges to sentiment analysis endeavors. Identification of FL schemas in short texts remains largely an unresolved issue in the broader field of natural language processing, mainly due to their contradictory and metaphorical meaning content. The main FL expression forms are sarcasm, irony and metaphor. In the present paper, we employ advanced deep learning methodologies to tackle the problem of identifying the aforementioned FL forms. Significantly extending our previous work (Potamias et al., in: International conference on engineering applications of neural networks, Springer, Berlin, pp 164-175, 2019), we propose a neural network methodology that builds on a recently proposed pre-trained transformer-based network architecture which is further enhanced with the employment and devise of a recurrent convolutional neural network. With this setup, data preprocessing is kept in minimum. The performance of the devised hybrid neural architecture is tested on four benchmark datasets, and contrasted with other relevant state-of-the-art methodologies and systems. Results demonstrate that the proposed methodology achieves state-of-the-art performance under all benchmark datasets, outperforming, even by a large margin, all other methodologies and published studies.																	0941-0643	1433-3058															10.1007/s00521-020-05102-3		JUN 2020											
J								Explainable artificial intelligence and machine learning: A reality rooted perspective	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										artificial intelligence; data science; explainable Artificial Intelligence; machine learning; statistics	BLACK-BOX; MODELS	As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAIcan be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Algorithmic Development > Statistics Technologies > Machine Learning																	1942-4787	1942-4795														e1368	10.1002/widm.1368		JUN 2020											
J								Multilayer Convolutional Neural Network to Filter Low Quality Content from Quora	NEURAL PROCESSING LETTERS										Quora; Deep learning; Question-answering; Convolutional neural network		Question answering (QA) websites now play a crucial role in meeting Internet users' information needs. Quora is a growing QA platform where users get quick answers to their questions from their peers. Nonetheless, it is noted that a significant number of questions remained unanswered for a long time. Questions that have long been unable to receive any answer, opinion-based, need a debate to get the answers, or a valid answer does not exist, fall underInsincerequestion group. It is therefore important to weed outInsincerequestions in order to maintain the integrity of the site. Quora have a huge number of such questions that can not be filtered manually. To overcome this problem, this paper proposes a multi-layer convolutional neural network model that helps to minimizeInsincerequestions from the website. Two embeddings were created from Quora dataset: (i) using Skipgram, and (ii) using Continuous Bag of Word model. The created embeddings and a pre-trained GloVe embedding vector were used for system development. The proposed model needs only the question text to predict the question isInsincerequestion or not and hence free from manual feature engineering. The experimental results indicated that the proposed multilayer CNN model outperforming over the earlier works by achieving the F1-score of 0.98 for the best case.																	1370-4621	1573-773X				AUG	2020	52	1			SI		805	821		10.1007/s11063-020-10284-x		JUN 2020											
J								Treant: training evasion-aware decision trees	DATA MINING AND KNOWLEDGE DISCOVERY										Adversarial machine learning; Robust learning; Decision tree ensembles	CLASSIFIERS	Despite its success and popularity, machine learning is now recognized as vulnerable toevasion attacks, i.e., carefully crafted perturbations of test inputs designed to force prediction errors. In this paper we focus on evasion attacks against decision tree ensembles, which are among the most successful predictive models for dealing with non-perceptual problems. Even though they are powerful and interpretable, decision tree ensembles have received only limited attention by the security and machine learning communities so far, leading to a sub-optimal state of the art for adversarial learning techniques. We thus proposeTreant, a novel decision tree learning algorithm that, on the basis of a formal threat model, minimizes an evasion-aware loss function at each step of the tree construction.Treantis based on two key technical ingredients:robust splittingandattack invariance, which jointly guarantee the soundness of the learning process. Experimental results on publicly available datasets show thatTreantis able to generate decision tree ensembles that are at the same time accurate and nearly insensitive to evasion attacks, outperforming state-of-the-art adversarial learning techniques.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1390	1420		10.1007/s10618-020-00694-9		JUN 2020											
J								Human activity recognition in smart environments employing margin setting algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Margin setting algorithm; Machine learning; Activity recognition; Supervised learning; Smart environment	LEARNING ALGORITHM; PREDICTION; CLASSIFIER	Human activity recognition is gaining promising attention in the research community with the recent revolution in artificial intelligence and machine learning to infer activities from the time-series sensor data. Significant progress has been made in the application of effective machine learning algorithms for pattern recognition and prediction of human activities in smart environments, such as ambient assisted living, healthcare monitoring, surveillance-based security and fitness tracking. In this paper, we propose to apply a supervised learning algorithm called margin setting algorithm (MSA) to predict the human activities. To validate the performance of MSA, we compare it with the support vector machine (SVM) and artificial neural network (ANN) and understand the activities of daily living of two residents in a smart home. The experimental results show that our proposed algorithm outperforms other state-of-the-art machine learning algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02229-y		JUN 2020											
J								Ontology-Mediated Querying with Horn Description Logics	KUNSTLICHE INTELLIGENZ										Ontology-mediated querying; Horn description logics; Fine-grained data complexity; Query-by-example; Ontology-based data access	DATA COMPLEXITY; FAMILY	An ontology-mediated query (OMQ) consists of a database query paired with an ontology. When evaluated on a database, an OMQ returns not only the answers that are already in the database, but also those answers that can be obtained via logical reasoning using rules from ontology. There are many open questions regarding the complexities of problems related to OMQs. Motivated by the use of ontologies in practice, new reasoning problems which have never been considered in the context of ontologies become relevant, since they can improve the usability of ontology enriched systems. This thesis deals with various reasoning problems that emerge from ontology-mediated querying and it investigates the computational complexity of these problems. We focus on ontologies formulated in Horn description logics, which are a popular choice for ontologies in practice. In particular, the thesis gives results regarding the data complexity of OMQ evaluation by completely classifying complexity and rewritability questions for OMQs based on an EL ontology and a conjunctive query. Furthermore, the query-by-example problem, and the expressibility and verification problem in ontology-based data access are introduced and investigated.																	0933-1875	1610-1987															10.1007/s13218-020-00674-7		JUN 2020											
J								A rest-time-based prognostic model for remaining useful life prediction of lithium-ion battery	NEURAL COMPUTING & APPLICATIONS										Lithium-ion battery; Capacity degradation; Capacity regeneration; Particle filter; Remaining useful life prediction	HEALTH ESTIMATION; CYCLE-LIFE; STATE; FRAMEWORK; PERFORMANCE; MECHANISMS; MANAGEMENT	This paper proposes a novel empirical model for the remaining useful life prediction of lithium-ion battery. The proposed model is capable of modeling both the global degradation and local degradation of lithium-ion battery aging process. The global degradation process is regarded as the ideal aging profile without any interference by regeneration phenomenon. However, the regeneration phenomenon inevitably occurs in practical usage of lithium-ion battery and affects the local degradation significantly. Therefore, we separate the local degradation process from the global degradation process to represent the local battery aging process affected by regeneration phenomenon. We unify the modeling method for the global and local degradation process by exponential functions with cleverly designing of the corresponding cycles. The particle filter framework is employed to estimate the model parameters with measurement data. The future capacity is predicted after the identification, and the remaining useful life is extracted by calculating the difference between the predicted capacity and failure threshold. Model comparisons on benchmark battery datasets have been conducted. The results demonstrate that our proposed method is capable of capturing the degradation and regeneration phenomena, and the remaining useful life prediction performance of our proposed model is better than state-of-the-art modeling methods.																	0941-0643	1433-3058															10.1007/s00521-020-05105-0		JUN 2020											
J								Improved algorithm for management of outsourced database	NEURAL COMPUTING & APPLICATIONS										Outsourced data; Storage; Verification; Security	SEARCH SCHEME; COMPUTATION; DELEGATION	In cloud storage, clients outsource data storage to save local resources. Clients lose the controllability to manage data, and cloud service providers are usually untrusted. When clients want to know the correctness of data anytime, they have to verify the data. In this paper, we focus on the validation of outsourced data. We propose a data correctness verification scheme to verify the correctness of cloud storage data. By calculating the validation data, when the user queries the data, it is easy to determine whether the cloud server returns the correct data. In addition, clients can update outsourced data. When updating the database, the clients can verify the correctness of the updated file by calculating and producing a new proof. The efficiency of data validation is very high, and the computational overhead on the client side is very low. This scheme is suitable for the resource-constrained devices, such as wearable devices. This scheme can be applied in the IoT perception layer with limited computing resources.																	0941-0643	1433-3058															10.1007/s00521-020-05047-7		JUN 2020											
J								Deep reinforcement learning for drone navigation using sensor data	NEURAL COMPUTING & APPLICATIONS										UAV; drone; Deep reinforcement learning; Deep neural network; Navigation; Safety assurance	AERIAL VEHICLES; NETWORKS	Mobile robots such as unmanned aerial vehicles (drones) can be used for surveillance, monitoring and data collection in buildings, infrastructure and environments. The importance of accurate and multifaceted monitoring is well known to identify problems early and prevent them escalating. This motivates the need for flexible, autonomous and powerful decision-making mobile robots. These systems need to be able to learn through fusing data from multiple sources. Until very recently, they have been task specific. In this paper, we describe a generic navigation algorithm that uses data from sensors on-board the drone to guide the drone to the site of the problem. In hazardous and safety-critical situations, locating problems accurately and rapidly is vital. We use the proximal policy optimisation deep reinforcement learning algorithm coupled with incremental curriculum learning and long short-term memory neural networks to implement our generic and adaptable navigation algorithm. We evaluate different configurations against a heuristic technique to demonstrate its accuracy and efficiency. Finally, we consider how safety of the drone could be assured by assessing how safely the drone would perform using our navigation algorithm in real-world scenarios.																	0941-0643	1433-3058															10.1007/s00521-020-05097-x		JUN 2020											
J								Domain knowledge graph-based research progress of knowledge representation	NEURAL COMPUTING & APPLICATIONS										Domain knowledge graph; Knowledge representation; Entity; Relationship; Property		Domain knowledge graph has become a research topic in the era of artificial intelligence. Knowledge representation is the key step to construct domain knowledge graph. There have been quite a few well-established general knowledge graphs. However, there are still gaps on the domain knowledge graph construction. The research introduces the related concepts of the knowledge representation and analyzes knowledge representation of knowledge graphs by category, which includes some classical general knowledge graphs and several typical domain knowledge graphs. The paper also discusses the development of knowledge representation in accordance with the difference of entities, relationships and properties. It also presents the unsolved problems and future research trends in the knowledge representation of domain knowledge graph study.																	0941-0643	1433-3058															10.1007/s00521-020-05057-5		JUN 2020											
J								Predicting academic performance of students in Chinese-foreign cooperation in running schools with graph convolutional network	NEURAL COMPUTING & APPLICATIONS										Graph convolutional network; Learning analytics; Chinese-foreign cooperatively run schools; Academic early alert		To improve students' performance effectively, many Chinese universities are establishing systems to predict student's academic performance and sending student's academic early alerts. With student's population of over 600,000, Chinese-foreign cooperation in running schools (CFCRS) has become one of the booming higher education forms in China. Compared with students in the non-cooperatively running programs, CFCRS students' academic performances are weaker on average. To predict the academic-at-risk students and provide efficient supports to the students, a precise and prompt academic prediction is in great need. Therefore, this research aims at representing a more efficient and accurate model to predict academic performance of CFCRS students which will be based on graph convolutional network. In this research, student's similarity is measured on academic performance by Pearson correlation coefficient. An undirected graph in which similar students are connected is conducted. Feature matrix is composed of students' previous grades. In addition, graph convolutional network is trained based on the undirected graph and feature matrix. In the experiment, it shows that this model predicts certain student's performance on certain course from the final exam results in previous semesters, which might improve learning efficiency and teaching quality. With an average accuracy of 81.5%, graph convolutional network outperforms support vector machine and random forest models.																	0941-0643	1433-3058															10.1007/s00521-020-05045-9		JUN 2020											
J								Advanced stochastic approaches for Sobol' sensitivity indices evaluation	NEURAL COMPUTING & APPLICATIONS										Multidimensional integration; Sensitivity analysis; Global sensitivity indices; Lattice rules; Digital nets; Air pollution modeling	ERROR ANALYSIS; MONTE; ALGORITHMS; COEFFICIENTS; MODELS	Sensitivity analysis is a modern promising technique for studying large systems such as ecological systems. The main idea of sensitivity analysis is to evaluate and predict (through computer simulations on large mathematical models) the measure of the sensitivity of the model's output to the perturbations of some input parameters, and it is a technique for refining the mathematical model. The main problem in the sensitivity analysis is the evaluation of total sensitivity indices. The mathematical formulation of this problem is represented by a set of multidimensional integrals. In this work, some new stochastic approaches for evaluating Sobol' sensitivity indices of the unified Danish Eulerian model have been presented. For the first time, a special type of digital nets and lattice rules are applied for multidimensional sensitivity analysis and their advantages are discussed. A comparison of accuracy of eight stochastic approaches for evaluating Sobol' sensitivity indices is performed. The obtained results will be important and useful for the surveyed scientists (physicists, chemicals, meteorologists) to make a comparative classification of the input parameters with respect to their influence on the concentration of the pollutants of interest.																	0941-0643	1433-3058															10.1007/s00521-020-05074-4		JUN 2020											
J								Effectiveness of symmetric rejection for a secure and user convenient multistage biometric system	PATTERN ANALYSIS AND APPLICATIONS										Multi-stage biometric verification; Biometrics; Serial fusion; Reject option; Symmetric rejection; Sequential probability ratio test	LEVEL FUSION; MULTIPLE CLASSIFIERS; SCORE-LEVEL; FRAMEWORK; OPTION; CLASSIFICATION	A multistage biometric verification system uses multiple biometrics and/or multiple biometric verifiers to generate a verification decision. The core of a multistage biometric verification system is reject option which allows a stage not to give a genuine/impostor decision when it is not confident enough. This paper studies the effectiveness of symmetric rejection for multistage biometric verification systems. The symmetric rejection method determines the reject region by symmetrically rejecting equal proportion of genuine and impostor scores. The applicability of a multistage biometric verification system depends on how secure and user convenient it is, which is measured by the performance-cost trade-off. This paper analyzes the performance-cost trade-off of symmetric rejection method by conducting extensive experiments. Experiments are performed on two biometric databases: (1) publicly available NIST database and (2) a keystroke database. In addition, the symmetric rejection method is empirically compared with two existing rejection methods: (1) sequential probability ratio test-based method, which uses score-fusion and (2) Marcialis et al.'s method, which does not use score fusion. Results demonstrate strong effect of symmetric rejection method on creating a secure and user convenient multistage biometric verification system.																	1433-7541	1433-755X															10.1007/s10044-020-00899-0		JUN 2020											
J								A credit-based dynamical evaluation method for the smart configuration of manufacturing services under Industrial Internet of Things	JOURNAL OF INTELLIGENT MANUFACTURING										Flexibility of manufacturing services; Industrial IoT; Referential credit value; Service scoring mechanism; Smart service configuration; Credit-based manufacturing mode; Customer satisfaction	DECISION-MAKING; EVALUATION MODEL; FUZZY ANP; SELECTION; BLOCKCHAIN; AHP; MANAGEMENT; ENTROPY; TOPSIS; SETS	Most scheduling problems are required to follow rigid metrics, such as the maximum completion time, earliest deadline first, etc., ignoring the flexibility of manufacturing services (MSs) and the effects of their historical data hidden in millions of manufacturing activities. The historical data serves as a powerful basis for describing the comprehensiveness or credibility of the MS itself with the help of Industrial IoT enabling all equipment to communicate and take preventive actions. Similar to the bank credit for individuals, MSs should also have their own referential credit values when choosing the most suitable service for a specific manufacturing task. This paper summarizes the MS attributes from six aspects with sufficient sub-attributes. The fuzzy Analytic Network Process combined with the Cross-Entropy method is employed to evaluate the credit of MSs in the complex manufacturing network system. Such service scoring mechanism (SSM) can personify a comprehensive credit evaluation of services, where, a smart service configuration mode based on credit is proposed for carrying out the supply-demand matching with the help of the data-security technology. Subsequently, a credit-based manufacturing mode is derived under SSM. Numerical examples are carried out to demonstrate the validity of the matching mode. The result may assist manufacturers to allocate their manufacturing tasks in real time in a "credit" way and make quicker decisions in exceptional circumstances, while making the chosen service truly competent enough to finish the work, so as to further improve the customer satisfaction.																	0956-5515	1572-8145															10.1007/s10845-020-01604-y		JUN 2020											
J								Design and analysis of an optimized microfluidic channel for isolation of circulating tumor cells using deterministic lateral displacement technique	COMPLEX & INTELLIGENT SYSTEMS										Circulating tumor cells; Deterministic lateral displacement; Microfluidics; COMSOL multiphysics; Bio-particles	CANCER-CELLS; SEPARATION; BLOOD	Circulating tumor cells (CTCs) are extremely scarce cells which cut off from a primary tumor and percolate into the circulation of blood flow and are, thus, critical for precise cancer detection and treatment. Deterministic lateral displacement (DLD) which exploits asymmetric splitting of laminar flow around the implanted microposts has displayed trustworthy capabilities in separating cells of varying sizes. In this research work, a microfluidic channel consisting of three symmetrically aligned inlets and outlets and embedded circular posts has been proposed which effectively separates the CTCs from lymphocytes utilizing the concept of DLD. Using a commercial software COMSOL Multiphysics 5.4, the design of the proposed microchannel has been simulated and analyzed considering an injected blood sample containing massive CTCs and slim WBCs of radii 13.5 mu m and 6 mu m, respectively. The proposed model of microchannel isolates the CTCs from WBCs at a comparatively higher sample mass flow rate of 4 x 10(-6) kg/s and Reynolds number of 8.9 thereby operating efficiently at higher throughput, and offers excellent linearity in terms of velocity magnitude, pressure, shear rate and Reynolds number. The computational analysis of the proposed microchannel reveals that it can isolate CTCs from WBCs with better separation ratio, offers higher throughput, reduces possibilities of clogging and maintains better uniformity of pressure distribution and other flow parameters when compared with existing microchannel designs. The maximum separation ratio for CTCs and WBCs has been obtained as 84% and 96%, respectively.																	2199-4536	2198-6053				OCT	2020	6	3					711	720		10.1007/s40747-020-00164-1		JUN 2020											
J								A parallel variable neighborhood search algorithm with quadratic programming for cardinality constrained portfolio optimization	KNOWLEDGE-BASED SYSTEMS										Metaheuristics; Variable neighborhood search; Asynchronous parallelization; Quadratic programming; Portfolio optimization	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; PARTICLE SWARM OPTIMIZATION; VEHICLE-ROUTING PROBLEM; BEE COLONY ALGORITHM; LOCAL SEARCH; SIMULTANEOUS PICKUP; SELECTION	Over the years, portfolio optimization remains an important decision-making strategy for investment. The most familiar and widely used approach in the field of portfolio optimization is the meanvariance framework introduced by Markowitz. Following this pioneering work, many researchers have extended this model to make it more practical and adapt to real-life problems. In this study, one of these extensions, the cardinality constrained portfolio optimization problem, is considered. Cardinality constraints transform the quadratic optimization model into the mixed-integer quadratic programming problem, which is proved to be NP-Hard, making it harder to obtain an optimal solution within a reasonable time by using exact solution methodologies. Hence, the vast majority of the researchers have taken advantage of approximate algorithms to overcome arising computational difficulties. To develop an efficient solution approach for cardinality constrained portfolio optimization, in this study, a parallel variable neighborhood search algorithm combined with quadratic programming is proposed. While the variable neighborhood search algorithm decides the combination of assets to be held in the portfolio, quadratic programming quickly calculates the proportions of assets. The performance of the proposed algorithm is tested on five well-known datasets and compared with other solution approaches in the literature. Obtained results confirm that the proposed solution approach is very efficient especially on the portfolios with low risk and highly competitive with state-of-the-art algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105944	10.1016/j.knosys.2020.105944													
J								A situation refinement model for complex event processing	KNOWLEDGE-BASED SYSTEMS										Complex event processing; Situational awareness; Situation refinement; Event enrichment; Rule learning	MINING FREQUENT ITEMSETS; DATA STREAMS; PATTERNS	Complex Event Processing (CEP) systems aim at processing large flows of events to discover situations of interest (SOI). CEP uses predefined pattern templates to detect occurrences of complex events in an event stream. CEP systems rely on domain experts to define complex patterns rules to recognize SOI. The task of identifying complex patterns faces with several challenges, such as the complexity of writing the pattern rules, and the need to acquire and process background information considering event stream's real-time constraints. Developing an efficient rule mining algorithm to fine-tune the CEP pattern to recognize SOI requires the tackling of three main obstacles. First, the CEP pattern rules must be inferred by utilizing the user's preferred context and the history of the event stream. Second, to avoid the issue of pattern complexity, the minimum number of rules must be used in the refinement process. Finally, to respond to emerging situations, the refinement task must be fulfilled in near realtime. In this work, we present a rule mining model to refine the CEP pattern rules by considering these obstacles while providing the ability to adjust the level of refinement to fit the applied scenario. This paper aims to: (1) Review the challenges associated with incorporating domain knowledge in CEP systems to improve awareness of real-world situations; (2) Present a Situation Refinement model to extract the minimal set of rules from external knowledge required to identify Situations Of Interest; (3) Demonstrate the summary update process of the event stream; and (4) Evaluate the derived rules with respect to their coverage and complexity. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105881	10.1016/j.knosys.2020.105881													
J								A novel method for sentiment classification of drug reviews using fusion of deep and machine learning techniques	KNOWLEDGE-BASED SYSTEMS										Machine learning; Deep learning; Sentiment analysis; Information fusion; 3-way decisions; Drug review	CONVOLUTIONAL NEURAL-NETWORKS; BRAIN STORM OPTIMIZATION; FEATURE-SELECTION; FUZZY ARTMAP; HYBRID MODEL; ENSEMBLE; ALGORITHMS	Nowadays, the development of new computer-based technologies has led to rapid increase in the volume of user-generated textual content on the website. Patient-written medical and health-care reviews are among the most valuable and useful textual content on social media which have not been studied extensively by researchers in the fields of natural language processing (NLP) and data mining. These reviews offer insights into the interaction of patients with doctors, treatment, and their satisfaction or frustration with the delivery of healthcare services. In this study, we propose two deep fusion models based on three-way decision theory to analyze the drug reviews. The first fusion model, 3-way fusion of one deep model with a traditional learning algorithm (3W1DT) developed using a deep learning method as a primary classifier and a traditional learning method as the secondary method that is used when the confidence of the deep method during classification of test samples is low. In the second proposed deep fusion model, 3-way fusion of three deep models with a traditional model (3W3DT), three deep and one traditional models are trained on the entire training data and each classifies the test sample individually. Then, the most confident classifier is selected to classify the test drug review. Our results on the reviews based on Drugs.com dataset show that both proposed 3W1DT and 3W3DT methods outperformed the traditional and deep learning methods by 4% and the 3W3DT outperformed 3W1DT by 2% in terms of accuracy and F1-measure. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105949	10.1016/j.knosys.2020.105949													
J								Multigranulation consensus fuzzy-rough based attribute reduction	KNOWLEDGE-BASED SYSTEMS										Multigranulation model; Fuzzy-rough set theory; Attribute reduction; Coevolutionary learning; Neonatal brain fMRIs	ACCELERATOR; IMAGE	As big data often contains a significant amount of unstructured, imprecise, and uncertain data, the fuzzy-rough-set-based attribute reduction is a valuable technique for uncertainty reasoning and data mining. However, tradition fuzzy-rough-set algorithms take all data correlations into account, which may lead to challenging requirements for computing and memory space resources. In this paper, we propose a novel Multigranulation Consensus Fuzzy-Rough Attribute Reduction (MCFR) algorithm for big data analysis, which assigns tasks to multiple nodes for parallel computing with different multigranulation weights and creates reasonable granular subpopulations for attribute reduction. A multigranulation margin model with a self-evolving consensus scheme is constructed wherein super elitists are classified into different multigranulation populations using the weighted margins' matrix to calculate the parallel positive regions. This model allows penalizing super elitists with cooperative behaviors from coarsening to refining. A cascade cross-coevolutionary multigranulation learning model utilizes a more efficient coevolutionary attribute classification with multigranulation flexible thresholds. Here, the super elitist crossover learning strategy is employed to search for elitists' chromosomes in granular subpopulations to yield better objective function values for handling dynamic big data classification tasks. Our experimental results demonstrate that MCFR can achieve a high performance in addressing uncertainty and fuzzy attribute reduction problems for big datasets with increasing noise. We also demonstrate a practical application of MCFR by using it to automatically segment fMRIs of the neonatal brain tissues at both the global and tissue levels. The results were in the good agreement with expert manual classifications, thus introducing a new method for the medical decision support problem of predicting disorders from neonatal brain fMRIs. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105945	10.1016/j.knosys.2020.105945													
J								Node-community membership diversifies community structures: An overlapping community detection algorithm based on local expansion and boundary re-checking	KNOWLEDGE-BASED SYSTEMS										Overlapping community detection; Local expansion; Boundary re-checking; Node-community membership	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; COMPLEX NETWORKS; SOCIAL NETWORKS; INFLUENTIAL SPREADERS; IDENTIFICATION; FRAMEWORK; MODEL; CUTS	Local expansion methods excel in efficiency for mining overlapping communities in real-world networks. However, two problems prevent such methods from identifying diversely structured communities. First, local expansion methods generate independent communities only. Second, local expansion methods depend heavily on quality functions. This work provides a solution for local expansion methods to identify diversely structured communities. The proposed overlapping community detection algorithm performs local expansion and boundary re-checking sub-processes in order. The local expansion process first gets a cover of the network, and then the boundary re-checking process optimizes the cover of the network resulting from the local expansion process. To solve the first problem, the proposed algorithm establishes associations between boundaries of adjacent communities via the boundary re-checking process. To solve the second problem, the proposed algorithm expands and optimizes communities based on node-community membership optimization. We compared the proposed algorithm to seven state-of-the-art algorithms by examining their performance on five groups of artificial networks and sixteen real-world networks. Experimental results showed that the proposed algorithm outperforms compared algorithms in identifying diversely structured communities. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105935	10.1016/j.knosys.2020.105935													
J								Mind your privacy: Privacy leakage through BCI applications using machine learning methods	KNOWLEDGE-BASED SYSTEMS										Privacy leakage; Brain-computer interface; BCI; EEG; BCI games	BRAIN-COMPUTER INTERFACE; EXECUTIVE FUNCTIONS; NEURAL RESPONSE; POWER; EEG; FEATURES; STIMULI	With the digitization of almost every aspect of our lives, privacy leakage in cyber space has become a pressing concern. Brain-Computer Interface (BCI) systems have become more popular in recent years and are now being used for a variety of applications. BCI data represents an individual's brain activity at a given time. Like many other kinds of data, BCI data can be utilized for malicious purposes. Electroencephalography (EEG) is one of the most popular brain activity acquisition methods of BCI applications. More specifically, BCI games, represent one of the main EEG applications. However, a malicious BCI application (e.g. game) could allow an attacker to take advantage of an unsuspecting user happily enjoying a game and record the user's brain activity; by analyzing this data, the attacker can infer private information and characteristics regarding the user, without his/her consent or awareness. This study is the first to demonstrate the ability to predict and infer meaningful personality traits and cognitive abilities by analyzing resting-state EEG (rsEEG) recordings of an individual's brain activity using a variety of machine learning methods. A comprehensive set of raw rsEEG scans, along with the dissociation level and executive function (EF) performance measures, for the 162 subjects were used in our evaluation. The best results we achieved were an accuracy of 73% for dissociation classification and less than 16% mean absolute error in predicting performance for all examined EFs. These encouraging results are better than those presented in prior research, both in terms of accuracy and data-validity and dataset size. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105932	10.1016/j.knosys.2020.105932													
J								CNN-based image recognition for topology optimization	KNOWLEDGE-BASED SYSTEMS										Convolutional neural network; GPU computing; Topology analysis; Compliance; Topology image recognition	STRUCTURAL TOPOLOGY; DESIGN; HOMOGENIZATION	Effectiveness of several currently popular topology optimization methods is closely related to the number of design variables consisted of discretized finite elements. Since the number of design variables is proportional to the number of finite element meshes, a very fine discretization needs more computational cost to carry out a finite element analysis and evaluate a compliance based objective function with the volume constraint. This paper presents a new computational method by using convolutional neural networks (CNNs) which can be substituted for the FEM process to calculate compliances. The robustness and adaptability of the proposed method are tested on a MBB (Messerschmitt-Bolkow-Blohm) beam and two cantilever beam problems. The designed CNN is trained on a 48 x 16 pixel resolution dataset taken from coarse meshes. The trained CNN can predict the information of image-based topologies composed of fine meshes. A graphics processing unit (GPU) is then used to accelerate the bulk-processing of data. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				JUN 21	2020	198								105887	10.1016/j.knosys.2020.105887													
J								Shape-based outlier detection in multivariate functional data	KNOWLEDGE-BASED SYSTEMS										Outlier detection; Anomaly detection; Multivariate functional data; functional data features; Multivariate time series; Multidimensional curve shape	DEPTH MEASURES	Multivariate functional data refer to a population of multivariate functions generated by a system involving dynamic parameters depending on continuous variables (e.g., multivariate time series). Outlier detection in such a context is a challenging problem because both the individual behavior of the parameters and the dynamic correlation between them are important. To address this problem, recent work has focused on multivariate functional depth to identify the outliers in a given dataset. However, most previous approaches fail when the outlyingness manifests itself in curve shape rather than curve magnitude. In this paper, we propose identifying outliers in multivariate functional data by a method whereby different outlying features are captured based on mapping functions from differential geometry. In this regard, we extract shape features reflecting the outlyingness of a curve with a high degree of interpretability. We conduct an experimental study on real and synthetic datasets and compare the proposed method with functional-depth-based methods. The results demonstrate that the proposed method, combined with state-of-the-art outlier detection algorithms, can outperform the functional-depth-based methods. Moreover, in contrast with the baseline methods, it is efficient regardless of the proportion of outliers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105960	10.1016/j.knosys.2020.105960													
J								Single and simultaneous fault diagnosis of gearbox via a semi-supervised and high-accuracy adversarial learning framework	KNOWLEDGE-BASED SYSTEMS										Single; Simultaneous; Fault diagnosis; Gearbox; Adversarial learning; Continuous wavelet transform	CONVOLUTIONAL NEURAL-NETWORK; CONTINUOUS WAVELET TRANSFORM; INTELLIGENT DIAGNOSIS; BEARINGS	Gearboxes are the most widely used elements for transferring speed and power in many industrial machines. High-accuracy gearbox fault diagnosis is quite significant for keeping the machine working reliably and safely. Owing to various unseen faults, it is pretty challenging to realize high-accuracy intelligent fault diagnosis of gearboxes using existing methods. In addition, existing intelligent fault diagnosis methods heavily rely on a huge number of labeled samples, and the features extraction and selection are mainly done manually. In this paper, a semi-supervised and high-accuracy adversarial learning framework for the single and simultaneous fault diagnosis of the gearbox based on Generative Adversarial Nets and time-frequency imaging is proposed. The proposed method involves two parts. In the first part, continuous wavelet transform is adopted to transform one-dimensional raw vibration signals into two-dimensional time-frequency images. In the second part, the labeled and unlabeled time-frequency images are inputted into the built adversarial learning model to realize single and simultaneous fault diagnosis of the gearbox. Finally, two case studies are implemented to verify the proposed method. The results indicate that it is higher in accuracy and fewer in training steps of achieving the highest accuracy rate than other existing intelligent fault diagnosis methods in literatures. Moreover, its performance in stability is pretty good as well. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105895	10.1016/j.knosys.2020.105895													
J								Introducing the DM-P approach for analysing the performances of real-time clinical decision support systems	KNOWLEDGE-BASED SYSTEMS										Performance analytics; Clinical decision support; Real-time alert; Defects map; p-chart; Statistical process control	PROCESS-CONTROL CHARTS; RESPIRATORY-DISTRESS-SYNDROME; ACUTE LUNG INJURY; PROTECTIVE-VENTILATION; CLUSTER-ANALYSIS; IMPROVEMENT; MODEL; METAANALYSIS; PREVENTION; PROFILES	This paper introduces the DM-P approach for assessing the performance of the alerts produced from a clinical decisions support system when there is no information to indicate a genuine observation and one that is not. The DM-P approach allows the representation of the p-charts, normally used in a statistical process control chart setting, as a defects map which is ordered according to the occurrence of similar behaviour as determined using agglomerative hierarchical clustering. The effectiveness of this new approach is demonstrated using real life data from the VILIAlert clinical decisions support system deployed in real-time in an intensive care unit to monitor patients on the ventilator and used to trigger alerts to clinicians when a patient is at risk of receiving injurious mechanical ventilation. The VILIAlert data was successfully used as a case study for the DM-P approach proving both the usefulness of the technique and its ability to identify the genuine cases of patients at risk. Such an approach also has scale up quality being designed in such a way to be able to deal with large volumes of data and can be extended to include any form of statistical process control chart not just p-charts. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105877	10.1016/j.knosys.2020.105877													
J								Direction-sensitive relation extraction using Bi-SDP attention model	KNOWLEDGE-BASED SYSTEMS										Relation extraction; Shortest dependency path; Recurrent neural network; Self-attention		Relation extraction is a crucial task of natural language processing (NLP). It plays a key role in question answering, web search, and information retrieval and so on. Previous research on this task has verified the effectiveness of using attention mechanisms, shortest dependency paths (SDP) and LSTM. However, most of these methods focus on learning a semantic representation of the whole sentence, highlighting the importance of partial words, or pruning the sentence with SDP. They ignore the lose of information in these methods, such as the dependency relation of each word and preposition words to indicate the relation direction. Besides, the SDP-based approach is prone to over-pruning. Based on the above observations, this paper presents a framework with a Bi-directional SDP (Bi-SDP) attention mechanism to tackle these challenges. The Bi-SDP is a novel representation of SDP, including original SDP and its reverse. The attention mechanism, based on Bi-SDP, builds a parallel word-level attention to capture relational semantic words and directional words. Furthermore, we explored a novel pruning strategy to minimize the length of input instance and the number of RNN cells simultaneously. Moreover, experiments are conducted on two datasets: SemEval-2010 Task 8 dataset and KBP37 dataset. Compared with the previous public models, our method can achieve better competitive performance on the SemEval-2010 Task 8 dataset and outperform existing models on the KBP37 dataset. Additionally, our experimental results also evidence that the directional prepositions in sentences are useful for relation extraction and can improve the performance of relationship with apparent physical direction. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105928	10.1016/j.knosys.2020.105928													
J								Hybrid time-aligned and context attention for time series prediction	KNOWLEDGE-BASED SYSTEMS										Time-series; Long-term prediction; Similarity search; Attention	TRAFFIC FLOW	Time series forecasting can provide more extensive data support for good decision making. Recently, many deep learning based forecasting models have been proposed, and thus the main problems are how to learn effective historical information and alleviate the influence of error propagation, especially for long-term prediction. In this paper, we present a new attention model base on LSTM encoderdecoder architecture to predict long-term time series. We define a similar scenes of time series, which include periodic pattern and time-nearest pattern, and provide a similar scenes search method. Base on this, we design a hybrid time aligned and context attention model (HTC-Attn), and the former focus on the characteristics of the alignment position, while the latter focus on the context features of specific location in similar scenes. The attention gate is designed to control the absorption degree of two different types in the prediction model. Furthermore, the proposed model use double-layer encoder-decoder structure to learn the trend term and time dependence of time series. Experimental results show that HTC-Attn can effectively maintain long-term dependence and learn detailed in single factor time series prediction tasks, and accuracy consistently outperforms the state-of-the-art baselines at least 2%. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105937	10.1016/j.knosys.2020.105937													
J								A hybrid scheme-based one-vs-all decision trees for multi-class classification tasks	KNOWLEDGE-BASED SYSTEMS										Decision tree; One-vs-all; Split criteria; Hybrid scheme; Multi-class classification	SUPPORT VECTOR MACHINES; INDUCTION	Decision tree algorithms have been proved to be a powerful and popular approach in classification tasks. However, they do not have reasonable classification performance in multi-class scenarios. In the present study, decision tree algorithms are combined with the one-vs-all (OVA) binarization technique to improve the generalization capabilities of the scheme. However, unlike previous literature that has focused on aggregation strategies, the present study is focused on the process of building base classifiers over the OVA scheme. A novel split criterion, entitled by the splitting point correction matrix (SPCM), is proposed in this regards, which can effectively deal with the unbalance problem caused by the OVA scheme. The SPCM is a kind of hybrid scheme, which integrates distribution and permutation information from the training data at each splitting point. Therefore, compared to other classical split criteria, such as the C4.5, the proposed method can make the right choice about the optimal splitting point at the root or internal nodes from multi-angle. In order to evaluate the effectiveness of the SPCM approach, extensive experiments are carried out compared to the classical and state-of-the-art methods. The experiments are performed on sixteen datasets, where the effectiveness and the accuracy of the proposed method is verified. It is concluded that the SPCM method not only has excellent classification performance but also produces a more compact decision tree. Moreover, it is found that the SPCM method has especially a considerable improvement in the depth of tree. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105922	10.1016/j.knosys.2020.105922													
J								An accelerated stochastic variance-reduced method for machine learning problems	KNOWLEDGE-BASED SYSTEMS										Stochastic optimization; Variance reduction; Hypergradient; Recursive gradient	MINI-BATCH ALGORITHMS; NONCONVEX OPTIMIZATION; GRADIENT DESCENT	Variance reduction techniques provide simple and fast algorithms for solving machine learning problems. In this paper, we present a novel stochastic variance-reduced method. The proposed method relies on the mini-batch version of stochastic recursive gradient algorithm (MB-SARAH), which updates stochastic gradient estimates by using a simple recursive scheme. However, facing the challenge of the step size sequence selection in MB-SARAH, we introduce an online step size sequence based on the hypergradient descent (HD) method, which only requires little additional computation. For the proposed method, referred to as MB-SARAH-HD, we provide a general convergence analysis and prove linear convergence for strongly convex problems in expectation. Specifically, we prove that the proposed method has sublinear convergence rate in a single outer loop. We also prove that the iteration complexity outperforms several variants of the state-of-the-art stochastic gradient descent (SGD) method under suitable conditions. Numerical experiments on standard datasets are provided to demonstrate the efficacy and superiority of our MB-SARAH-HD method over existing approaches in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105941	10.1016/j.knosys.2020.105941													
J								Identifying critical nodes in complex networks via graph convolutional networks	KNOWLEDGE-BASED SYSTEMS										Complex networks; Adjacency matrices; Critical nodes; Graph convolutional networks	INFLUENTIAL NODES; CENTRALITY; SPREADERS; IDENTIFICATION; DYNAMICS; RANKING	Critical nodes of complex networks play a crucial role in effective information spreading. There are many methods have been proposed to identify critical nodes in complex networks, ranging from centralities of nodes to diffusion-based processes. Most of them try to find what kind of structure will make the node more influential. In this paper, inspired by the concept of graph convolutional networks(GCNs), we convert the critical node identification problem in complex networks into a regression problem. Considering adjacency matrices of networks and convolutional neural networks(CNNs), a simply yet effectively method named RCNN is presented to identify critical nodes with the best spreading ability. In this approach, we can generate feature matrix for each node and use a convolutional neural network to train and predict the influence of nodes. Experimental results on nine synthetic and fifteen real networks show that under Susceptible-Infected-Recovered (SIR) model, RCNN outperforms the traditional benchmark methods on identifying critical nodes under spreading dynamic. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105893	10.1016/j.knosys.2020.105893													
J								DRaWS: A dual random-walk based sampling method to efficiently estimate distributions of degree and clique size over social networks	KNOWLEDGE-BASED SYSTEMS										Random-walk based graph sampling; Estimator; Distributions of degree and clique structures; Social networks	GENERAL FRAMEWORK	Random-walk based sampling methods have been widely employed to characterize social networks. However, existing random-walk based sampling methods cause inaccuracies in estimating the degree structure and high sampling costs in estimating clique structures. In this paper, we propose a dual random-walk based sampling method, called DRaWS by designing a dual residence of the random walker, to estimate both the distributions of degree and clique size with low costs. The key idea behind DRaWS is that it leverages the many-to-one formation between many nodes and one clique in a large graph to shorten the sampling paths and thus reduce the sampling costs greatly while reflecting the different sampling probabilities of the two types of node structures. Meanwhile, DRaWS employs the one-to-many representativeness between one node and many nodes in a clique to improve the quality of samples. Furthermore, two re-weighted estimators for DRaWS's process are proposed to estimate the two different node structures. Experimental evaluation driven by real graph datasets shows that DRaWS drastically cuts down the sampling costs of the state-of-the-art methods while increasing the accuracy when estimating both the degree and clique structural properties. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105891	10.1016/j.knosys.2020.105891													
J								Novel trajectory privacy-preserving method based on prefix tree using differential privacy	KNOWLEDGE-BASED SYSTEMS										Location service; Trajectory protection; Prefix tree; Trajectory segmentation; Differential privacy; Markov chain		Location-based services, such as DiDi and bike sharing, are becoming increasingly popular. However, the use of these services raises privacy concerns. In the past few years, differential privacy technology has been applied to location information protection. However, most existing models largely fail to resist complex background-knowledge attacks. This paper proposes a novel privacy preservation method for trajectory data. It is based on a prefix tree and uses differential privacy. It should be noted that existing methods only consider either a certain location point or the entire trajectory. In the proposed prefix tree structure, the nodes of the tree store the trajectory segments. The parameter minimum description length method is combined with the Dijkstra method to select feature trajectory points that represent the entire trajectory, thus further reducing the (computational) complexity of data processing. To protect privacy, Laplacian noise is added to the location data of trajectory segments by using differential privacy. In addition, a background and contextual information attack model is proposed, and the corresponding protection method is provided. Finally, a Markov chain is used to limit the size of the noise added to the data. The proposed algorithm is compared with related stateof-the-art algorithms on a public dataset. The results demonstrate that our algorithm can ensure not only privacy but also data availability. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUN 21	2020	198								105940	10.1016/j.knosys.2020.105940													
J								Generating adversarial examples with input significance indicator	NEUROCOMPUTING										Deep learning; Adversarial attack; Sensitivity analysis; Relevance propagation		Lots of adversarial attacks have been put forward to identify vulnerabilities in existing DNNs and improve their robustness. However, an indispensable benchmark, Jacobian-based Saliency Map Attack USMA), suffers from computational issues that prevent its application. In this paper, we propose to generate adversarial examples based on the input significance indicator, with two indicators -sensitivity and relevance- included to find the most sensitive or relevant element by backpropagating scores based on DNNs' output. Our experiment results show that the sensitivity-based attack can alleviate the heavy computational problem of JSMA and still remain the effectiveness, while the relevance-based indicator can avoid the saturation problem of gradients with a slight sacrifice of runtime, thus reducing the magnitude of the perturbations with a higher rate of attack success. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 21	2020	394						1	12		10.1016/j.neucom.2020.01.040													
J								Event-triggered guaranteed cost consensus for uncertain nonlinear multi-agent systems with time delay	NEUROCOMPUTING										Event-triggered; Multi-agent systems; Guaranteed cost; Consensus; Time delay; Uncertain	LEADER-FOLLOWING CONSENSUS; DYNAMICS	The problem of event-triggered guaranteed cost consensus for nonlinear multi-agent systems with time delay and uncertain parameters is solved in this paper. First, a distributed event-triggered strategy is presented, by which control protocol is updated only at the triggering time instants, leading to significant saving of communication resources. Second, based on the Lyapunov method and the proposed event-triggered strategy, some criteria are obtained to ensure that guaranteed cost consensus can be achieved. Moreover, it is proved that the Zeno behavior is excluded under the proposed event-triggered scheme. Third, an algorithm is given to design suitable control gains in terms of linear matrix inequalities. Finally, the validity of the proposed results in this paper is verified via numerical simulation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						13	26		10.1016/j.neucom.2020.02.003													
J								MMFNet: A multi-modality MRI fusion network for segmentation of nasopharyngeal carcinoma	NEUROCOMPUTING										Nasopharyngeal carcinoma; Segmentation; Multi-modality MRI; 3D Convolutional block attention module; Residual fusion block; Self-transfer	CLASSIFICATION; IMPACT; MODEL; CT	Segmentation of nasopharyngeal carcinoma (NPC) from Magnetic Resonance Images (MRI) is a crucial prerequisite for NPC radiotherapy. However, manual segmenting of NPC is time-consuming and labor-intensive. In addition, single-modality MRI generally cannot provide enough information for accurate delineation. Therefore, a novel multi-modality MRI fusion network (MMFNet) is proposed to complete accurate segmentation of NPC via utilizing T1, T2 and contrast-enhanced T1 of MRI. The backbone of MMFNet is designed as a multi-encoder-based network, consisting of several encoders and one decoder, where the encoders aim to capture modality-specific features and the decoder is to obtain fused features for NPC segmentation. A fusion block consisting of a 3D Convolutional Block Attention Module (3D-CBAM) and a residual fusion block (RFBlock) is presented. The 3D-CBAM recalibrates low-level features captured from modality-specific encoders to highlight both informative features and regions of interest (ROIs) whereas the RFBlock fuses re-weighted features to keep balance between fused ones and high-level features from decoder. Moreover, a training strategy named self-transfer is also proposed which utilizes pre-trained modality-specific encoders to initialize multi-encoder-based network in order to make full mining of individual information from multi-modality MRI. The proposed method based on multi-modality MRI can effectively segment NPC and its advantages are validated by extensive experiments. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						27	40		10.1016/j.neucom.2020.02.002													
J								LLR: Learning learning rates by LSTM for training neural networks	NEUROCOMPUTING										Neural networks; Learning rates; LSTM; Optimization; Gradients	SPEED; ELM	In the training process of the deep neural networks, the learning rate plays an important role in whether the training process can converge and how fast it can achieve converge. In order to ensure convergence, most of the existing optimization methods adopt a multi-stage descending small learning rate which is hand-designed. However, this method converges slowly especially in the early stage of training. Based on this, a learning rate adjustment strategy that can automatically adjust and has a faster speed of loss decline will be helpful to the training of the deep model. In this paper, a dynamic adjustment strategy of learning rate is developed based on the Long Short Term Memory(LSTM) model and the gradients of loss function. This method effectively utilizes the advantages of the LSTM model considering the multistep learning rate as a whole, and generates the learning rate of the current step based on the memory information of the previous learning rate. Three datasets and four architectures are used in the experiments. We applied the learning rate adjustment method to various optimization methods and achieved good results that our method can achieve even smaller loss under the same number of iterations. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						41	50		10.1016/j.neucom.2020.01.106													
J								Comparison of base classifiers for multi-label learning	NEUROCOMPUTING										Classification; Classifier; Experimental comparison; Multi-label; Multilabel	CLASSIFICATION	Multi-label learning methods can be categorised into algorithm adaptation, problem transformation and ensemble methods. Some of these methods depend on a base classifier and the relationship is not well understood. In this paper the sensitivity of five problem transformation and two ensemble methods to four types of classifiers is studied. Their performance across 11 benchmark datasets is measured using 16 evaluation metrics. The best classifier is shown to depend on the method: Support Vector Machines (SVM) for binary relevance, classifier chains, calibrated label ranking, quick weighted multi-label learning and RAndom k-labELsets; k-Nearest Neighbours (k-NN) and Naive Bayes (NB) for Hierarchy Of Multilabel classifiERs; and Decision Trees (DT) for ensemble of classifier chains. The statistical performance of a classifier is also found to be generally consistent across the metrics for any given method. Overall, DT and SVM have the best performance-computational time trade-off followed by k-NN and NB. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						51	60		10.1016/j.neucom.2020.01.102													
J								Driving amount based stochastic configuration network for industrial process modeling	NEUROCOMPUTING										Driving amount; Stochastic configuration network; Inequality constraints; Generalization capability; Compact structure	ALGORITHMS; APPROXIMATION	Stochastic configuration network (SCN) that randomly assigns the weights connecting the input layer and the hidden layer with an inequality constraint can achieve a fast learning speed for dealing with regression tasks. In this paper, a driving amount based SCN (DASCN) is proposed to improve the performance in terms of generalization and structure compactness, which have gained considerable attention in the industrial process. In the proposed DASCN, the driving amount incorporated into SCN is used to further improve the structural parameters, especially the output weights, of SCN. The performance of DASCN is evaluated by function approximation, four benchmark datasets and practical application in the industrial process. The simulation results indicate that the DASCN has better generalization capability and a more compact network structure compared to other methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						61	69		10.1016/j.neucom.2020.02.029													
J								A subregion division based multi-objective evolutionary algorithm for SVM training set selection	NEUROCOMPUTING										Training set selection; Multi-objective optimization; Evolutionary algorithm; SVM; Instance selection	SUPPORT VECTOR MACHINES; INSTANCE SELECTION; GENETIC ALGORITHM; CLASSIFICATION; REDUCTION; MODEL	Support vector machine (SVM) is a popular machine learning method with a solid theoretical foundation, and has shown promising performance on different classification problems. However, it suffers from an expensive training cost, which makes it not be very suitable for the application with a large-scale training set. To this end, as a data pre-processing technique, training set selection (TSS) for SVM has received much attention recently, since it can reduce the size of SVM training set without degrading the performance. In this paper, a subregion division based multi-objective evolutionary algorithm termed SDMOEA-TSS is proposed for SVM training set selection, where objective space is divided into several subregions for effectively searching good solutions. Specifically, in SDMOEA-TSS, a divided based initialization strategy is firstly suggested to initialize the population to locate in different regions of objective space. Then a subregion based evolutionary (including crossover, mutation and update) strategy is developed, which not only makes full use of individuals in each subregion for local search but also maintains the whole population's global search ability. Empirical studies on 21 public data sets demonstrate the superiority of the proposed algorithm over the state-of-the-arts in terms of both quality and diversity of the selected SVM training subset. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						70	83		10.1016/j.neucom.2020.02.028													
J								Two neural dynamics approaches for computing system of time-varying nonlinear equations	NEUROCOMPUTING										Neural dynamics; Time-varying nonlinear equations; Gradient-based neural dynamics (GND); Zeroing neural dynamics (ZND)	PARAMETER-ESTIMATION; NETWORK; STABILIZATION; OPTIMIZATION; MANIPULATORS; ALGORITHMS; DESIGN	The problem of solving time-varying nonlinear equations has received much attention in many fields of science and engineering. In this paper, firstly, considering that the classical gradient-based neural dynamics (GND) might result in nonnegligible residual error in handling time-varying nonlinear equations, an adaptive coefficient GND (AGND) model is constructed as an improvement. Besides, the secondly new designed model is the projected zeroing neural dynamics (PZND) to relieve the limitation on the available activation function, which can be of saturation and non-convexity different from that should be unbounded and convex described in the traditional zeroing neural dynamics (ZND) approach. Moreover, theoretical analyses on the AGND model and PZND model are provided to guarantee their effectiveness. Furthermore, computer simulations are conducted and analyzed to illustrate the efficacy and superiority of the two new neural dynamics models designed for online solving time-varying nonlinear equations. Finally, applications to robot manipulator motion generation and Lorenz system are provided to show the feasibility and practicability of the proposed approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						84	94		10.1016/j.neucom.2020.02.011													
J								ADCM: attention dropout convolutional module	NEUROCOMPUTING										Attention; Dropout-channel; Dropout-region; Network architecture; Convolutional module	NEURAL-NETWORKS	Network architecture design plays an important role in boosting the performance of models in various applications. In this work, we design a general and lightweight module named the attention dropout convolutional module (ADCM). It consists of two submodules, channel attention dropout (CAD) and position attention dropout (PAD), and each submodule integrates both attention and dropout mechanisms. The attention mechanism emphasizes the meaningful information and suppresses unnecessary noise. The dropout-channel in the CAD submodule filters the channel based on its channel attention, while the dropout-region in the PAD submodule filters the region consisting of the spatially correlated features according to its position attention. The two dropout methods we designed allow the baseline network to learn more robust features and further boost its performance. Finally, we deploy the ADCM in consecutive layers of classical convolutional neural networks and evaluate its performance on multiple benchmark datasets. The experimental results demonstrate that the ADCM brings significant improvements to the performance of the baseline models at negligible computational cost and with less complexity. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						95	104		10.1016/j.neucom.2020.02.007													
J								Humor detection via an internal and external neural network	NEUROCOMPUTING										Humor; Natural language processing; External attention neural network		Humor as a lubricant for daily life is frequently used in language expressions. It is usually triggered by comparisons of metaphorical scenes or misunderstandings of ambiguous words. Hence, detecting and recognizing the humor implied in text is an interesting and challenging research problem in the field of natural language processing. To understand humor, we discuss incongruity and ambiguity in detail and then propose an internal and external attention neural network (IEANN) for the humor detection task. The IEANN integrates two types of attention mechanisms to capture the incongruity and ambiguity in humor text. Meanwhile, extensive experiments are conducted on two humor datasets to test the effectiveness and robustness of our model. The experimental results show that the proposed model not only achieves state-of-the-art performance but also has better interpretability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						105	111		10.1016/j.neucom.2020.02.030													
J								A parallel vision approach to scene-specific pedestrian detection	NEUROCOMPUTING										Pedestrian detection; Specific scene; Synthetic data; Video surveillance; Parallel vision	CAMERA NETWORKS	In recent years, with the development of computing power and deep learning algorithms, pedestrian detection has made great progress. Nevertheless, once a detection model trained on generic datasets (such as PASCAL VOC and MS COCO) is applied to a specific scene, its precision is limited by the distribution gap between the generic data and the specific scene data. It is difficult to train the model for a specific scene, due to the lack of labeled data from that scene. Even though we manage to get some labeled data from a specific scene, the changing environmental conditions make the pre-trained model perform bad. In light of these issues, we propose a parallel vision approach to scene-specific pedestrian detection. Given an object detection model, it is trained via two sequential stages: (1) the model is pre-trained on augmented-reality data, to address the lack of scene-specific training data; (2) the pre-trained model is incrementally optimized with newly synthesized data as the specific scene evolves over time. On publicly available datasets, our approach leads to higher precision than the models trained on generic data. To tackle the dynamically changing scene, we further evaluate our approach on the webcam data collected from Church Street Market Place, and the results are also encouraging. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						114	126		10.1016/j.neucom.2019.03.095													
J								GAN-Based virtual-to-real image translation for urban scene semantic segmentation	NEUROCOMPUTING										Domain adaptation; Virtual-to-real image translation; Generative adversarial networks; Semantic segmentation; Deep convolutional neural networks		Semantic image segmentation requires large amounts of pixel-wise labeled training data. Creating such data generally requires labor-intensive human manual annotation. Thus, extracting training data from video games is a practical idea, and pixel-wise annotation can be automated from video games with near perfect accuracy. However, experiments show that models trained using raw video-game data cannot be directly applied to real-world scenes because of the domain shift problem. In this paper, we propose a domain-adaptive network based on CycleGAN that translates scenes from a virtual domain to a real domain in both the pixel and feature spaces. Our contributions are threefold: 1) we propose a dynamic perceptual network to improve the quality of the generated images in the feature spaces, making the translated images are more conducive to semantic segmentation; 2) we introduce a novel weighted self-regularization loss to prevent semantic changes in translated images; and 3) we design a discrimination mechanism to coordinate multiple subnetworks and improve the overall training efficiency. We devise a series of metrics to evaluate the quality of translated images during our experiments on the public GTA-V (a video game dataset, i.e., the virtual domain) and Cityscapes (a real-world dataset, i.e., the real domain) and achieved notably improved results, demonstrating the efficacy of the proposed model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						127	135		10.1016/j.neucom.2019.01.115													
J								Super-resolution using multi-channel merged convolutional network	NEUROCOMPUTING										Super-resolution; Convolutional neural network; Dense block; Feature fusion	SINGLE-IMAGE SUPERRESOLUTION	Single-image super-resolution (SISR) has been an important topic due to the demand for high-quality virtual images in the field of visual artificial intelligence. Methods based on deep learning have achieved great success based on the excellent capability of grasping complicated features of deep convolutional networks. The performance can be improved slightly but not obviously by simply widening or deepening the network. In this paper, we propose a merged convolutional network for super-resolution, which extracts more adequate details to restore high-resolution images. We used dense blocks for feature extraction to concatenate deep features with shallow features in depth. We also designed two sub-nets with distinct convolution kernels as different branches of the network, which can widen the network and improve the performance of the system. Finally, we employed sub-pixel layers to avoid feature distortion for up-sampling at the very end. Our method was evaluated using several standard benchmark datasets. The results demonstrate superior performance and good robustness compared with state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						136	145		10.1016/j.neucom.2019.04.089													
J								Multi-negative samples with Generative Adversarial Networks for image retrieval	NEUROCOMPUTING										Multi-negative samples; Generative Adversarial Networks; Virtual images; Semi-supervised; Image retrieval		The task of image retrieval has received considerable attention from the visual Al community. However, collecting large-scale labeled images for training is rather laborious and expensive. Moreover, negative samples are treated equally without considering their differences when compared with the query. To overcome those problems, in this paper, we propose to utilize generated virtual images and multiple negative samples to simultaneously learn image representations for the task of image retrieval. In our method, we first utilize the Generative Adversarial Networks in a semi-supervised fashion to produce virtual images with an adversarial loss. Second, considering the neighborhood structure within negative samples, a random sampling algorithm is proposed to effectively mining the potentially hard samples. Third, we propose a multi-negative loss function with the Kullback-Leibler divergence. Finally, by optimizing the total loss the deep neural networks are trained. Then the learned networks are further used to obtain image representations. Extensive experiments are conducted on publicly available datasets. Our model demonstrates better performances in the task of image retrieval. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						146	157		10.1016/j.neucom.2018.10.110													
J								Post-processing for intra coding through perceptual adversarial learning and progressive refinement	NEUROCOMPUTING										Compression artifacts reduction; In-loop filtering; Perceptual adversarial loss; High Efficiency Video Coding (HEVC)		Video compression is an indispensable technology in image/video communications, due to its highly desirable ability to reduce the huge data volume. However, video compression introduces complex compression artifacts, such as blocking, ringing, and blurring artifacts. To reduce this issue, post-processing techniques have been extensively studied. In this paper, we propose a novel post-processing technique using multi-level progressive refinement network, which we term MPRNet. A joint training loss function is defined to optimize MPRNet via an adversarial training approach, which can boost both subjective and objective visual quality. Furthermore, our network employs progressive refine strategy, which predicts multi-level residues in one feed-forward pass. This way of coarse-to-fine refinement manner allows our network to make trade-off between refined quality and computational complexity, which facilitates the resource-aware applications. Extensive evaluations on benchmark datasets verify the superiority of our proposed MPRNet model over the latest state-of-the-art methods, achieves 8.2%, 8.3% and 9.5% of BD-rate savings for the Y/Cb/Cr channel over the HEVC baseline respectively. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 21	2020	394						158	167		10.1016/j.neucom.2018.12.090													
J								Video intra prediction using convolutional encoder decoder network	NEUROCOMPUTING										Video coding; Intra prediction; Image inpainting; Convolutional encoder-decoder network (CED); High Efficiency Video Coding (HEVC)		Intra prediction is an effective method for video coding to remove the spatial redundancy of content. Classical intra prediction method usually creates a prediction block by extrapolating the encoded pixels surrounding the target block. However, existing methods cannot guarantee the prediction efficiency for rich textural structure, especially when weak spatial correlation exists between the target block and reference pixels. To remedy this issue, this paper proposes a novel intra prediction method via convolutional encoder-decoder network, which we term IPCED. IPCED can learn and extract the internal representation of reference blocks, and progressively generate a prediction block from this representation. IPCED is a data-driven method, which represents an improvement over hand-crafted methods, and is capable of improving the accuracy of intra prediction. Extensive experimental results demonstrate that IPCED can generate higher-quality intra prediction results, achieves 3.41%, 3.07% and 3.44% bitrate saving for the Y/Cb/Cr channel compared with HEVC baseline, which is significantly beyond existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						168	177		10.1016/j.neucom.2019.02.064													
J								A novel background subtraction algorithm based on parallel vision and Bayesian GANs	NEUROCOMPUTING										Background subtraction; Background model; Bayesian generative adversarial network; Convolutional neural networks; Parallel vision	MOVING OBJECT DETECTION; LOW-RANK; BENCHMARK	To address the challenges of change detection in the wild, we present a novel background subtraction algorithm based on parallel vision and Bayesian generative adversarial networks (GANs). First, we use the median filtering algorithm for background image extraction. Then, we build the background subtraction model by using Bayesian GANs to classify all pixels into foreground and background, and use parallel vision theory to improve the background subtraction results in complex scenes. The proposed algorithm has been evaluated on the well-known, publicly available changedetection.net dataset. Experiment results show that the proposed algorithm results in better performance than many state-of-the-art ones. In addition, our model trained on CDnet dataset can generalize very well to unseen datasets, outperforming multiple state-of-art methods. The major contribution of this work is to apply parallel vision and Bayesian GANs to solve the difficulties in background subtraction, achieving high detection accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 21	2020	394						178	200		10.1016/j.neucom.2019.04.088													
J								Gravitational search algorithm based optimized deep learning model with diverse set of features for facial expression recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Facial expression recognition; Deep belief network; Semi-supervised approach; Gravitational search algorithm; Histogram oriented gradients		Facial expression recognition (FER) is an essential part of effective human-computer interaction and serves as a helpful medium for children and patients who have problems with communication. However, most of the previous studies focus on building a FER model based on supervised and unsupervised approaches. This paper is focused on a semi-supervised deep belief network (DBN) approach to predict the facial expressions from the CK+, Oulu CASIA, MMI, and JAFFE datasets. To achieve accurate classification of the facial expressions, a gravitational search algorithm (GSA) is applied to optimize some parameters in the DBN network. The Histogram oriented gradients (HOG) and 2D-Discrete Wavelet Transform (2D-DWT) are used for feature extraction from the lip, cheek, brow, eye, and furrow patches. The unwanted information present in the image is eliminated using a feature selection approach. The feature extraction is done by the Kernel-principal component analysis to obtain higher-order correlations between input variables and detect non-linear components. The HOG features extracted from the lip patch provides the best performance for accurate facial expression classification. Finally, a comparative analysis to compare the proposed model with different machine learning techniques based on the evaluation criteria. The results demonstrate that DBN-GSA based classifier is more accurate than the rest of the classifiers.																	1868-5137	1868-5145															10.1007/s12652-020-02235-0		JUN 2020											
J								Real-time facial expression recognition for affect identification using multi-dimensional SVM	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Facial expressions; Affect; Multilayered SVM; Binary Tree SVM	SELF-ASSESSMENT MANNEQUIN; EMOTION	Automated detection of different affect states for human beings using facial expressions has attracted an increasing level of research attention for high accuracy. In general, existing facial expressions databases have been used in automated affect state detection to achieve better efficiency. However, minimal real-time experiments data has been used for detecting affect state. To efficiently automate the affect recognition process, in this work, we proposed a new classifier framework and acquired experimentally real-time dataset for training and testing phase. The participants' face images are captured and processed on the Region of Interest (ROI) to determine the feature points, and hence feature vectors are obtained. The facial feature vectors are given as an input to the proposed multiclass classifier, Multi-dimensional Support Vector Machine (MDSVM), which efficiently identifies and classifies the different affect state. The MDSVM is designed multi-dimensional, based on the two-dimensional valence-arousal emotional model. Thus, two SVMs are used, one in level-1, which classifies the input dataset into two classes based on valence, similarly the second SVM in level-2 classifies the input dataset into two categories based on arousal. The efficiency is improved by using the 8-cross-validation method. Thus, the methodology proposed in this work combines the advantage of reliable dataset acquired experimentally, significant 12 feature vectors obtained from facial expressions, prominently active multidimensional SVM, and 8-cross validation method. Experimental results illustrate the efficacy of the proposed system for accurately classifying the affect state into four categories, Relax, Sad, Angry, and Happy. The average accuracy obtained is 94.25% without k-fold cross-validation and 95.88% with 8-fold cross-validation.																	1868-5137	1868-5145															10.1007/s12652-020-02221-6		JUN 2020											
J								Bit-swapping linear feedback shift register (LFSR) for power reduction using pre-charged XOR with multiplexer technique in built in self-test	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Embedded systems; VLSI; BIST; BS-LFSR; Pre-charge; Set and reset; Embedded chip; Communication; MUX; Effective dual threshold LFSR		Testing becomes an inevitable part of the VLSI circuit. All the circuits or products must be verified before delivery. The data collected during testing is used to remove the faulty parts from the products and help to enhance the design and manufacturing process and improve the returns as well. There are several contributions made by researchers which are majorly based on Linear Feedback Shift Register (LFSR), bit swapping LFSR and dual threshold bit swapping LFSR methods. Reduction of delay, power consumptions and fault coverage are considered as major factors from the above methods. The proposed approach uses Bit Swapping-Linear Feedback Shift Register (BS-LFSR) architecture which integrates pre-charge, set-reset and gate replacement with mux techniques have alleviated the inherent drawbacks of linear feedback shift register and produced better results. Pre-charge method is reducing the delay and power consumption there by maximizing the operating frequency angle with high performance. High fault coverage is achieved through set and reset technique. Multiplexers mainly employed to minimize the delay and power consumption. BS-LFSR pattern generators are used to decrease the transition power from high switching action. These are producing an arbitrary test sequences with less power of switching by determining the pretense space between two subsequent designs. The area is reduced with the help of combinational logic. Experimental evaluation is done using ISCAS89 benchmark datasets. From the results, it is inferred that the fault coverage is increased and power consumptions are reduced.																	1868-5137	1868-5145															10.1007/s12652-020-02222-5		JUN 2020											
J								A neuro fuzzy system for incorporating ethics in the internet of things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial intelligence; Ethics; Internet of things; Neuro fuzzy system		The Internet of Things (IoT) is a promising technology for addressing the challenges of urbanisation, however ethical ramifications of introducing such pervasive technology have not been duly considered. It is assumed that smart devices are exempt from moral, religious or legal responsibilities with regard to their surroundings. Such a perspective on device functioning may lead to situations where essential human values are put at risk. We suggest that social parameters be also included within the realm of machine functioning. In this work, we propose a neuro fuzzy system (NFS) that is able to implement ethics relevant to the context of a device and also fine tune them. Initially ethical requirements are specified in terms of fuzzy ethics rules and appropriate ethical response is learned subsequently. The method enables us to address the vital question of ethical compliance of smart things so that ethical rights of people are not infringed upon. By such incorporation, we ensure that machines respect core human values paving way for human friendly technology ecosystems like ethical smart city.																	1868-5137	1868-5145															10.1007/s12652-020-02217-2		JUN 2020											
J								Some historical context for minimal cognition	ADAPTIVE BEHAVIOR										Minimally cognitive behavior; minimal cognition	DYNAMICS; SYSTEMS; DECISIONS	I provide some historical perspective on the development of the "minimally cognitive behavior" research program and its relation to "minimal cognition."																	1059-7123	1741-2633														1059712320931595	10.1177/1059712320931595		JUN 2020											
J								Log(A)G: An algebraic Non-Monotonic logic for reasoning with graded propositions	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Non-Monotonicity; Weighted logics; Uncertainty; Graded propositions; Unified framework for Non-Monotonicity	1ST-ORDER; KNOWLEDGE; BELIEF	We presentLog(A)G, a weighted algebraic non-monotonic logic for reasoning with graded beliefs.Log(A)Gis algebraic in that it is a language of only terms, some of which denote propositions and may be associated with ordered grades. The grades could be taken to represent a wide variety of phenomena including preference degrees, priority levels, trust ranks, and uncertainty measures. Reasoning inLog(A)Gis non-monotonic and may give rise to contradictions. Belief revision is, hence, an integral part of reasoning and is guided by the grades. This yields a quite expressive language providing an interesting alternative to the currently existing approaches to non-monotonicity. We show howLog(A)Gcan be utilised for modelling resource-bounded reasoning; simulating inconclusive reasoning with circular, liar-like sentences; and reasoning about information arriving over a chain of sources each with a different degree of trust. While there certainly are accounts in the literature for each of these issues, we are not aware of any single framework that accounts for them all likeLog(A)Gdoes. We also show howLog(A)Gcaptures a wide variety of non-monotonic logical formalisms. As such,Log(A)Gis a unifying framework for non-monotonicity which is flexible enough to admit a wide array of potential uses.																	1012-2443	1573-7470															10.1007/s10472-020-09697-0		JUN 2020											
J								Towards developing hybrid educational data mining model (HEDM) for efficient and accurate student performance evaluation	SOFT COMPUTING										Educational data mining (EDM); HEDM; Naive Baye's classification; J48 Classifier; Student performance; WEKA	PREDICTION	Many educational institutions use data mining for maintaining the student records, specifically academic performances, which are more significant. The academic performances of the students are to be analyzed for improving their results and also the overall results of the institutions. Moreover, the prediction of academic performance of students has been an important and developing research domain in educational data mining (EDM), in which data mining and machine learning techniques are used for deriving data from educational warehouse. With that, this paper develops a novel approach called hybrid educational data mining model (HEDM) for analyzing the student performance for effectively enhancing the educational quality for students. The proposed model evaluates the student performances based on distinctive factors that provide appropriate results. Furthermore, the model combines the efficiencies of Naive Baye's classification technique and J48 Classifier for deriving the results and categorizing the student performance in precise manner. The model is evaluated with the benchmark education dataset that is available online in the WEKA environment. The results show that the proposed model outperforms the results of existing works in evaluating student performance in EDM.																	1432-7643	1433-7479															10.1007/s00500-020-05075-4		JUN 2020											
J								An algorithmic approach to solve unbalanced triangular fuzzy transportation problems	SOFT COMPUTING										Transportation problem (TP); Fully fuzzy solution; Un-balanced fuzzy transportation problem (FTP); Triangular fuzzy number	RANKING	This paper presents problems in fuzzy transportation, which deals with fuzzy costs, fuzzy supplies, and demands of a fuzzy nature on any quantity transported. The paper deals with the minimization of the total fuzzy cost under the fuzzified decision-variables. The proposed method gives better optimum for fuzzy transportation problem (FTP) with unbalance and balance types. The method is intended to obtain a basic feasible solution (or "initial basic feasible solution") (IBFS) of unbalanced fuzzy problems with triangular fuzzy number. A new and simple heuristic approach for obtaining optimum solution of triangular fuzzy unbalanced transportation problem is proposed that reduces the number of iterations in the optimization process. A given triangular fuzzy unbalanced TP is converted into a modified triangular unbalanced FTP by increasing the fuzzy-demand/fuzzy-supply of an origin and a destination, and the same is resolved by new method. Also an illustrative numerical example is discussed for proposed method solving a triangular FTP with m, n origins and destinations, respectively.																	1432-7643	1433-7479															10.1007/s00500-020-05103-3		JUN 2020											
J								Sequential characterization of statistical epi-convergence	SOFT COMPUTING										Optimization; Epi-convergence; Statistical convergence; Epigraphs	CONVEX-SETS CONES; OPTIMIZATION; SEQUENCES; EPICONVERGENCE	Epi-convergence is used as an efficient tool in optimization theory. It finds optimal solutions in such a way that it ensures the convergence of infimum values. In some cases, some functions may not conform to an expected pattern and reduce the efficiency of optimization. Moreover, obtaining epi-limit function may fail due to disruption of these functions. For that reason, it may be necessary to use an alternative method that diminishes the effect of such functions by excluding them from consideration. In this paper, we give a sequential characterization of statistical epi-convergence which enables us to eliminate corrupted functions deviate from the majority of the data. Therefore, statistical epi-limit inferior and superior are defined. Then, we show that sequential characterization of statistical epi-convergence is not biconditional as its ordinary definition. At the end, these definitions lead the way through the conditions for statistical convergence of infimum values which is an essential property to solve optimization problems.																	1432-7643	1433-7479															10.1007/s00500-020-05092-3		JUN 2020											
J								Things that help out: designing smart wearables as partners in stress management	AI & SOCIETY										Interaction design; Stress management; Smart wearables; Chronic PTSD; Wearable technologies; Affective computing	SOCIAL SUPPORT	We propose an approach to designing smart wearables that act as partners to help people cope with stress in daily life. Our approach contributes to the developing field of smart wearables by addressing how technological capabilities can be designed to establish partnerships that consider the person, the situation, and the appropriate type of support. As such, this study also contributes to healthcare by opening up novel technology-supported routes to stress treatment and care. We present the results of a phenomenological study conducted with three war veterans who suffer from chronic posttraumatic stress disorder. We describe how their experiences of dealing with their stress informed our design approach, and discuss the implications of these results on smart wearables and stress management in general. We conclude by reflecting on the limitations of this study and directions for future work.																	0951-5666	1435-5655															10.1007/s00146-020-01003-0		JUN 2020											
J								Disengagement with ethics in robotics as a tacit form of dehumanisation	AI & SOCIETY										Roboethics; Roboticists; Dehumanisation; Lived ethics	DEHUMANIZATION; ROBOETHICS; CULTURE; FUTURE; TECHNOLOGY; PRINCIPLES; THINKING	Over the past two decades, ethical challenges related to robotics technologies have gained increasing interest among different research and non-academic communities, in particular through the field of roboethics. While the reasons to address roboethics are clear, why not to engage with ethics needs to be better understood. This paper focuses on a limited or lacking engagement with ethics that takes place within some parts of the robotics community and its implications for the conceptualisation of the human being. The underlying assumption is that the term 'ethical' essentially means 'human'. Thus, this paper discusses a working hypothesis according to which by avoiding to engage with roboethics, roboticists contribute to the tacit dehumanisation process emerging in and outside of robotics. An alternative approach includes 'lived ethics' which involves not only incorporating formal ethical approaches into the roboticists' work but also 'being' ethical and actually engaging with ethical reflection and practice.																	0951-5666	1435-5655															10.1007/s00146-020-01000-3		JUN 2020											
J								Anchor-free multi-orientation text detection in natural scene images	APPLIED INTELLIGENCE										Text detection; Natural scene image; Anchor-free; Convolutional Neural Network	LOCALIZATION	Text detection in natural scene images is a key prerequisite for computer vision tasks such as image search, blind navigation, autopilot, and multi-language translation. Existing text detection methods only detect partial region of large-scale texts and are difficult to detect small-scale texts. Aiming at this problem, an anchor-free multi-orientation text detection method is proposed. Firstly, Feature Pyramid Network (FPN) is used to combine the multiple feature layers of Convolutional Neural Network (CNN) to predict the geometric properties of text, which can be used to expand the receptive field of each pixel and thus help to detect more large-scale texts. Secondly, a new loss function independent of the scale of text is designed, which enables the pixels in the small-scale text to have a larger calculation weight, thereby facilitating the detection of small-scale texts. Finally, the results of pixel-level semantic segmentation are used to filter obviously unreasonable candidate text boxes, and at the same time improve the accuracy and recall rate of text detection. The experimental results on ICDAR 2015 and MSRA-TD500 prove the good performance of our method.																	0924-669X	1573-7497				NOV	2020	50	11					3623	3637		10.1007/s10489-020-01742-z		JUN 2020											
J								Autism AI: a New Autism Screening System Based on Artificial Intelligence	COGNITIVE COMPUTATION										Artificial intelligence; Autism spectrum disorder (SD); Cognitive computing; Deep learning; Intelligent systems; Machine learning; Medical screening	SPECTRUM QUOTIENT	Autistic spectrum disorder (ASD) is a neurodevelopment condition normally linked with substantial healthcare costs and time-consuming assessments where early detection of ASD traits can help limit the development of the condition. The existing conventional ASD screening methods contain a large number of items and are based on domain expert rules which may be criticized of being lengthy and subjective. More importantly, these methods use basic scoring functions to pinpoint to autistic traits rather intelligently learning patterns from cases and controls which can be more accurate and efficient. One promising solution to deal with the above issues and speed up ASD assessment referrals is to develop intelligent artificial intelligence screening methods that not only provide accurate pre-diagnostic classifications but also improve the efficiency and accessibility of the screening process. This paper proposes a new autism screening system that replaces the conventional scoring functions in classic screening methods with deep learning algorithms. The system is composed of a mobile application that provides the user interface capturing questionnaire data; an intelligent ASD detection web service that interfaces with a Convolutional Neural Network (CNN) trained with historical ASD cases; and a database that enables the CNN to learn new knowledge from future users of the system. The CNN classification method was evaluated against a large autism dataset consisting of adult, adolescent, child, and toddler cases and controls. The results obtained from the CNN were compared with other intelligent algorithms in which superior performance was achieved by the CNN. Particularly, the proposed CNN-based ASD classification system revealed higher accuracy, sensitivity, and specificity when compared with conventional screening methods. This indeed will be of high benefit for busy medical clinics and diagnosticians and could possibly be a new direction to change the way ASD diagnosis process is conducted in the future.																	1866-9956	1866-9964				JUL	2020	12	4					766	777		10.1007/s12559-020-09743-3		JUN 2020											
J								Evaluation of the size of time windows for the travelling salesman problem in delivery operations	COMPLEX & INTELLIGENT SYSTEMS										Operational research; Package delivery; Time windows; Travelling salesman problem	VEHICLE-ROUTING PROBLEM; ALGORITHM	A great challenge in operational research is to apply time-efficient algorithms to find the optimal solutions to the travelling salesman problem (TSP) and its many variations. The TSP with time windows (TSPTW) arises due to intense pressure for business to improve customer service. As online shopping becomes more popular, customer satisfaction increases if customers can decide when their orders are delivered to them. Customers may choose a time window, which is defined by an earliest delivery time and a latest delivery time, during which the package is delivered. Delivering packages to multiple customers is a typical TSPTW. One main challenge for a delivery business is to determine the size of the time window (i.e., the difference between the latest and earliest delivery times), which affects delivery cost and customer satisfaction. Although many previous studies investigated the TSPTW, none of those focused on the size of time windows. This study is the first that experiments with different time window sizes and determines their impact on tour duration, customer satisfaction, and solution time of the optimal delivery routes. The experiment results show that increasing the size of the time window decreases tour duration and customer satisfaction and increases solution time. Decreasing the size of the time window increases tour duration and customer satisfaction and decreases solution time. A small solution time is necessary for the scheduling of deliveries to many customers. A large solution time prevents a delivery business from delivering packages using optimal routes, which increases delivery cost and decreases customer satisfaction. The results of this study indicate that a general guideline for business is to allow customers to choose a time window size that is within the cost limit but is sufficiently small to maximize customer satisfaction and optimize delivery routes.																	2199-4536	2198-6053				OCT	2020	6	3					681	695		10.1007/s40747-020-00167-y		JUN 2020											
J								Presenting a new fuzzy system for web service selection aimed at dynamic software rejuvenation	COMPLEX & INTELLIGENT SYSTEMS										Software rejuvenation; Fuzzy system; Web services; Fault tolerance; Software aging	POLICY; TRUST; MODEL	As an effective technique to counteract software aging, software rejuvenation is applied in continuously running applications such as web service-based systems. In such systems, web services are allocated depending on the requirements of receivers and the facilities of servers. One of the challenging issues during assignment of web services is how to select the appropriate server to minimize faults. In this paper, we proposed dynamic software rejuvenation in the form of a proactive fault-tolerance technique based on fuzzy system. While including a threshold for the rejuvenation of each web service, we carried out the training phase based on the features of the service providers as well as the receivers' requirements. The results of simulations revealed that our strategy can mitigate the failure rate of web services by 45, 40, 23, and 12% in comparison with the non-fuzzy, regression-based, Markov-based, and ACOGELS-based web service rejuvenation strategies, respectively.																	2199-4536	2198-6053				OCT	2020	6	3					697	710		10.1007/s40747-020-00168-x		JUN 2020											
J								Modification of Harris hawks optimization algorithm with random distribution functions for optimum power flow problem	NEURAL COMPUTING & APPLICATIONS										Stochastic optimization; Random distribution function; OPF problem; Harris hawks	PARTICLE SWARM OPTIMIZATION; BEE COLONY ALGORITHM; SEARCH OPTIMIZATION; ECONOMIC-DISPATCH; WOLF OPTIMIZER; NONSMOOTH; EMISSION; SECURITY; NEWTON; SOLVE	Harris hawks optimization (HHO) algorithm, which is inspired from Harris hawks hunting strategy, uses uniform random numbers in the optimization process. This paper proposes modifying HHO with seven types of random distribution function definitions that are chi-square distribution, normal distribution, exponential distribution, Rayleigh distribution, Student's distribution,Fdistribution, and lognormal distribution to show effects on stochastic search-based optimization algorithm performance. The modified HHO algorithm is tested via some benchmark test functions. Results are compared with each other and with classical HHO solutions. Then, the HHO and its modified versions are applied to optimum power flow (OPF), which is an important problem for power system engineering for decades. The algorithms are applied to IEEE 30-bus test system to minimize total fuel cost of the power system, active/reactive power losses, and emission, by comparing with recent OPF researches. Considering the applicability of the proposed approach and the results achieved, one can confirm that it might be a different alternative method for solving OPF problems. One of the important results of the paper in the IEEE 30-bus test system is that the cost of fuel is calculated as 798.9105 $/h with classical HHO, while it is calculated as 798.66 $/h with the HHO modified with SD function.																	0941-0643	1433-3058															10.1007/s00521-020-05073-5		JUN 2020											
J								Multi-verse optimization algorithm- and salp swarm optimization algorithm-based optimization of multilevel inverters	NEURAL COMPUTING & APPLICATIONS										Multilevel inverters; Harmonic elimination; Multi-verse optimization algorithm; Salp swarm algorithm; Particle swarm optimization	SELECTIVE HARMONIC ELIMINATION; SWITCHING SCHEME; DC SOURCES	Renewable energy sources are installed into both distribution and transmission grids more and more with the introduction of smart grid concept. Hence, efficient usage of cascaded H-bridge multilevel inverters (MLIs) for power control applications becomes vital for sustainable electricity. Conventionally, selective harmonic elimination equations need to be solved for obtaining optimum switching angles of MLIs. The objective of this study is to obtain switching angles for MLIs to minimize total harmonic distortion. This study contributes to the solution of this problem by utilizing two recently developed intelligent optimization algorithms: multi-verse optimization algorithm and salp swarm algorithm. Moreover, well-known particle swarm optimization is utilized for MLI optimization problem. Seven-level, 11-level and 15-level MLIs are used to minimize total harmonic distortions. Simulation results with different modulation indexes for seven-, 11- and 15-level MLIs are calculated and compared in terms of the accuracy and solution quality. Numerical calculations are verified by using MATLAB/Simulink-based models.																	0941-0643	1433-3058															10.1007/s00521-020-05062-8		JUN 2020											
J								An efficient optimization approach for designing machine learning models based on genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Machine learning; Deep neural networks; Optimization; Genetic algorithm; Polymer nanocomposites; Fracture energy	FRACTURE-TOUGHNESS; COMPUTATIONAL INTELLIGENCE; NEURAL-NETWORK; NANOCOMPOSITES; PERFORMANCE; POLYMERS; ANFIS; PREDICTION	Machine learning (ML) methods have shown powerful performance in different application. Nonetheless, designing ML models remains a challenge and requires further research as most procedures adopt a trial and error strategy. In this study, we present a methodology to optimize the architecture and the feature configurations of ML models considering a supervised learning process. The proposed approach employs genetic algorithm (GA)-based integer-valued optimization for two ML models, namely deep neural networks (DNN) and adaptive neuro-fuzzy inference system (ANFIS). The selected variables in the DNN optimization problems are the number of hidden layers, their number of neurons and their activation function, while the type and the number of membership functions are the design variables in the ANFIS optimization problem. The mean squared error (MSE) between the predictions and the target outputs is minimized as the optimization fitness function. The proposed scheme is validated through a case study of computational material design. We apply the method to predict the fracture energy of polymer/nanoparticles composites (PNCs) with a database gathered from the literature. The optimized DNN model shows superior prediction accuracy compared to the classical one-hidden layer network. Also, it outperforms ANFIS with significantly lower number of generations in GA. The proposed method can be easily extended to optimize similar architecture properties of ML models in various complex systems.																	0941-0643	1433-3058															10.1007/s00521-020-05035-x		JUN 2020											
J								Neural image reconstruction using a heuristic validation mechanism	NEURAL COMPUTING & APPLICATIONS										Image reconstruction; Convolutional neural network; Heuristic algorithms	FEATURES	Image reconstruction is a mathematical process, where the image is compressed into a small representation and derived from this form. The general use of the reconstruction technique finds a place in noise removal from images obtained in medicine or other areas of life. In this paper, we propose a heuristic validation mechanism for training different types of neural networks in the problem of image reconstruction. The main idea is based on finding some important areas on image by heuristic algorithm and train network until a certain level of entropy of these areas is achieved. The mathematical model of this technique is described and supported by experimental results on different datasets with complex analysis of different heuristics. Proposed approach shows that it can reduce the average time of training process using convolutional neural networks.																	0941-0643	1433-3058															10.1007/s00521-020-05046-8		JUN 2020											
J								Complete autoencoders for classification with missing values	NEURAL COMPUTING & APPLICATIONS										Missing data; Complete deep learners; Autoencoding; Multitask learning	RECOGNITION; NETWORK	It has been demonstrated that modified denoising stacking autoencoders (MSDAEs) serve to implement high-performance missing value imputation schemes. On the other hand, complete MSDAE (CMSDAE) classifiers, which extend their inputs with target estimates from an auxiliary classifier and are layer by layer trained to recover both the observation and the target estimates, offer classification results that are better than those provided by MSDAEs. As a consequence, investigating whether CMSDAEs can improve the MSDAEs imputation processes has an obvious practical importance. In this correspondence, two types of imputation mechanisms with CMSDAEs are considered. The first is a direct procedure in which the CMSDAE output is just the target. The second mechanism is suggested by the presence of the targets in the vectors to be autoencoded, and it uses the well-known multitask learning (MTL) ideas, including the observations as a secondary task. Experimental results show that these CMSDAE structures increase the quality of the missing value imputations, in particular the MTL versions. They give the best result in 5 out of 6 missing value problems.																	0941-0643	1433-3058															10.1007/s00521-020-05066-4		JUN 2020											
J								Learning semantic and relationship joint embedding for author name disambiguation	NEURAL COMPUTING & APPLICATIONS										Author name disambiguation; Network embedding; Meta-path; Citation network		Author name disambiguation is an important research topic in the academic information retrieval community. Existing methods rely either on feature engineering on rich attributes information or on relationship information to obtain documents' similarity, but seldom consider the complementarity and the correlation between them. The feature engineering on attributes, especially on rich text information, could capture the global semantic concepts, while the relationship information could encode local structural proximity in multiple academic networks. To bridge the gap between semantic and relationship information in author name disambiguation, this paper presents a joint representation learning approach, which could encode both semantic and relationship information into a common low dimensional space. Specifically, the proposed method consists of four modules: (1) semantic embedding module; (2) relationship embedding module; (3) semantic and relationship joint embedding module; and (4) clustering module. Experimental results demonstrate that the proposed joint representation learning approach consistently outperforms the state-of-the-art methods on three benchmarks.																	0941-0643	1433-3058															10.1007/s00521-020-05088-y		JUN 2020											
J								An Improved Crow Search Algorithm for Test Data Generation Using Search-Based Mutation Testing	NEURAL PROCESSING LETTERS										Improved Crow Search Algorithm; Cauchy random number; Mutation sensitivity testing; Mothra mutation operators	SYSTEM	Automation of test data generation is of prime importance in software testing because of the high cost and time incurred in manual testing. This paper proposes an Improved Crow Search Algorithm (ICSA) to automate the generation of test suites using the concept of mutation testing by simulating the intelligent behaviour of crows and Cauchy distribution. The Crow Search Algorithm suffers from the problem of search solutions getting trapped into the local search. The ICSA attempts to enhance the exploration capabilities of the metaheuristic algorithm by utilizing the concept of Cauchy random number. The concept of Mutation Sensitivity Testing has been used for defining the fitness function for the search based approach. The fitness function used, aids in finding optimal test suite which can achieve high detection score for the Program Under Test. The empirical evaluation of the proposed approach with other popular meta-heuristics, prove the effectiveness of ICSA for test suite generation using the concepts of mutation testing.																	1370-4621	1573-773X				AUG	2020	52	1			SI		767	784		10.1007/s11063-020-10288-7		JUN 2020											
J								Intra-layer Synchronization in Duplex Networks with Time-Varying Delays and Stochastic Perturbations Under Impulsive Control	NEURAL PROCESSING LETTERS										Impulsive; Synchronization; Duplex network; Stochastic; Adaptive control	NEURAL-NETWORKS; EXPONENTIAL SYNCHRONIZATION; ADAPTIVE SYNCHRONIZATION; ANTI-SYNCHRONIZATION; MULTIAGENT SYSTEMS; COMPLEX NETWORKS; STABILITY; STABILIZATION; FINITE	This paper concerns the intra-layer synchronization in duplex networks with stochastic perturbations under impulsive control. In this paper, we proposed a dynamical system on a duplex network with impulsive control, where delay of interactions within each layer, delay of interactions between layers, and environmental noises are included. Different from Tang et al. (Sci China Technol Sci 61(12):1907-1914, 2018), we used different topology structures for the two layers of the duplex network. Different from the model of Shen and Tang (Chin Phys B 27(10):100503, 2018), delays in the node interactions within or between the layers are allowed. Besides, the environmental noises are included. Moreover, due to the superior efficiency of impulsive control, we use it to control the synchronization of the duplex network even if the dynamical system exhibits chaos phenomena. Finally, we obtain some interesting simulation results by applying our theoretic results to the Chua-Chua chaotic system to show the effectiveness of our control schemes.																	1370-4621	1573-773X				AUG	2020	52	1			SI		785	804		10.1007/s11063-020-10281-0		JUN 2020											
J								An anomaly based distributed detection system for DDoS attacks in Tier-2 ISP networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										DDoS attacks; Network security; Entropy; Information divergence; Flash events; Detection	DIVERGENCE MEASURES; DEFENSE; SERVICE	In the present computer era, the vulnerabilities inherent in the Internet architecture enable various kinds of attacks. Distributed Denial of Service (DDoS) is one of such prominent attack that is a lethal threat to Internet domain that harnesses its computing and communication resources. The increase in network traffic rates of legitimate traffic and its flow similarity with attack traffic has made the DDoS detection very difficult despite deployment of diversified defense solutions. The ISPs are bound to invest heavily to counter such problems which has a significant impact on company finances. To provide uninterrupted quality services to the end users, ISPs needs to deploy a distributed solution for timely detection and discrimination of attack and behaviorally similar flash events (FE) traffic. Such distributed defense systems can be deployed at source-end, intermediate network-end or at the victim-end location. Since the volume of traffic to be analyzed is very large, the detection accuracy and low computational complexity of the proposed defense solution is always a challenging problem. This paper proposes an ISP level distributed, collaborative and automated (D-CAD) defense system for detecting DDoS attacks and FEs, and has the capability to effectively distinguishing the two. Additionally, D-CAD defense system is also capable of categorizing FE traffic and has low computational complexity. The proposed system is validated in novel software defined networks (SDN) using Mininet emulator. The results show that D-CAD defense system outperformed its existing counterparts on various detection system evaluation metrics.																	1868-5137	1868-5145															10.1007/s12652-020-02208-3		JUN 2020											
J								Modality-specific matrix factorization hashing for cross-modal retrieval	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cross-modal retrieval; Matrix factorization; Common semantic representations; Modality-specific; Alignment		Cross-modal retrieval has been attracted attentively in the past years. Recently, the collective matrix factorization was proposed to learn the common representations for cross-modal retrieval based on assumption that the pairwise data from different modalities should have the same common semantic representations. However, this unified common representation could inherently sacrifice the modality-specific representations for each modality because the distributions and representations of different modalities are inconsistent. To mitigate this problem, in this paper, we propose Modality-specific Matrix Factorization Hashing (MsMFH) via alignment, which learns the modality-specific semantic representation for each modality and then aligns the representations via the correlation information. Specifically, we factorize the original feature representations into individual latent semantic representations, and then align the distributions of individual latent semantic representations via an orthogonal transformation. Then, we embed the class label into the hash codes learning via latent semantic space, and obtain hash codes directly by an efficient optimization with a closed solution. Extensive experimental results on three public datasets demonstrate that the proposed method outperforms to many existing cross-modal hashing methods up to 3% in term of mean average precision (mAP).																	1868-5137	1868-5145															10.1007/s12652-020-02177-7		JUN 2020											
J								Low-delay single holoscopic 3D computer-generated image to multiview images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Holoscopic 3D imaging; Macrolens array; Low delay; Image disparity; Viewpoint image; Multiview image; Autostereoscopic display		Due to the nature of holoscopic 3D (H3D) imaging technology, H3D cameras can capture more angular information than their conventional 2D counterparts. This is mainly attributed to the macrolens array which captures the 3D scene with slightly different viewing angles and generates holoscopic elemental images based on fly's eyes imaging concept. However, this advantage comes at the cost of decreasing the spatial resolution in the reconstructed images. On the other hand, the consumer market is looking to find an efficient multiview capturing solution for the commercially available autostereoscopic displays. The autostereoscopic display provides multiple viewers with the ability to simultaneously enjoy a 3D viewing experience without the need for wearing 3D display glasses. This paper proposes a low-delay content adaptation framework for converting a single holoscopic 3D computer-generated image into multiple viewpoint images. Furthermore, it investigates the effects of varying interpolation step sizes on the converted multiview images using the nearest neighbour and bicubic sampling interpolation techniques. In addition, it evaluates the effects of changing the macrolens array size, using the proposed framework, on the perceived visual quality both objectively and subjectively. The experimental work is conducted on computer-generated H3D images with different macrolens sizes. The experimental results show that the proposed content adaptation framework can be used to capture multiple viewpoint images to be visualised on autostereoscopic displays.																	1861-8200	1861-8219															10.1007/s11554-020-00991-y		JUN 2020											
J								Obtaining costly unverifiable valuations from a single agent	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Information disclosure; Mechanism design; Asymmetric information; Principal-agent problem	PROPER SCORING RULES; REVENUE MAXIMIZATION; COMMON VALUE; TRUTH SERUM; GAS LEASES; INFORMATION; DISCLOSURE; OIL; INCENTIVES; AUCTIONS	A principal needs to elicit the true value of an object she owns from an agent who has a unique ability to compute this information. The principal cannot verify the correctness of the information, so she must incentivize the agent to report truthfully. Previous works coped with this unverifiability by employing two or more information agents and awarding them according to the correlation between their reports. We show that, in a common value setting, the principal can elicit the true information even from a single information agent, and even when computing the value is costly for the agent. Moreover, the principal's expense is only slightly higher than the cost of computing the value. For this purpose we provide three alternative mechanisms, all providing the same above guarantee, highlighting the advantages and disadvantages in each. Extensions of the basic mechanism include adaptations for cases such as when the principal and the agent value the object differently, when the object is divisible and when the agent's cost of computation is unknown. Finally, we deal with the case where delivering the information to the principal incurs a cost. Here we show that substantial savings can be obtained in a multi-object setting.																	1387-2532	1573-7454				JUN 19	2020	34	2							46	10.1007/s10458-020-09469-4													
J								Specification testing of agent-based simulation using property-based testing	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Agent-based simulation testing; Code testing; Test driven development; Model specification	FRAMEWORK	The importance of Agent-Based Simulation (ABS) as scientific method to generate data for scientific models in general and for informed policy decisions in particular has been widely recognised. However, the important technique of code testing of implementations like unit testing has not generated much research interested so far. As a possible solution, in previous work we have explored the conceptual use ofproperty-based testing. In this code testing method, model specifications and invariants are expressed directly in code and tested throughautomatedandrandomisedtest data generation. This paper expands on our previous work and explores how to use property-based testing on a technical level to encode and test specifications of ABS. As use case the simple agent-based SIR model is used, where it is shown how to test agent behaviour, transition probabilities and model invariants. The outcome are specifications expressed directly in code, which relate whole classes of random input to expected classes of output. During test execution, random test data is generated automatically, potentially covering the equivalent of thousands of unit tests, run within seconds on modern hardware. This makes property-based testing in the context of ABS strictly more powerful than unit testing, as it is a much more natural fit due to its stochastic nature.																	1387-2532	1573-7454				JUN 19	2020	34	2							47	10.1007/s10458-020-09473-8													
J								Multi-criteria decision-making method with double risk parameters in interval-valued intuitionistic fuzzy environments	COMPLEX & INTELLIGENT SYSTEMS										Multi-criteria decision-making; Risk attitudes; Interval-valued intuitionistic fuzzy numbers; Score function; Mentality parameter	LINEAR-PROGRAMMING METHODOLOGY; PREFERENCE RELATIONS; GEOMETRIC APPROACH; ACCURACY FUNCTION; SCORE FUNCTION; TOPSIS; CONSISTENCY; ALGORITHM; CONSENSUS; NUMBERS	In the multi-criteria decision-making (MCDM) process, decision-makers with different risk attitudes may have different decision results. To address this issue and present decision-makers' mentality, this paper introduces two mentality parameters. These parameters reflect the decision-makers' risk attitudes in determining the membership and non-membership degrees of the evaluation information. In addition, the parameters demonstrate the risk attitude in terms of the hesitancy degree under interval-valued intuitionistic fuzzy information. Then, a new score function of interval-valued intuitionistic fuzzy numbers (IVIFNs) is proposed that uses the introduced mentality parameters. Meanwhile, certain properties of the proposed score function are discussed. Furthermore, the weighted comprehensive score value of IVIFNs is introduced, and an MCDM method is developed in an interval-valued intuitionistic fuzzy environment. Finally, a numerical example and comparative analyses are provided to illustrate the feasibility and effectiveness of the proposed method.																	2199-4536	2198-6053				OCT	2020	6	3					669	679		10.1007/s40747-020-00165-0		JUN 2020											
J								Use of neural network based DVR for the reduction of power quality issues in composite micro-grid	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										DVR; Fuzzy logic controller; Genetic algorithm; Neural network controller (NN controller); Power quality		Most of today's every power system networks deals with a serious scenario of Power quality. This problem had ever more observed with the advent of practical hardware, in which it will carry out serious impact in the eminence of power supply in to the micro-grid networks. Matter by reason of power quality, it is produced and critical to a non-common normal current and voltage re-emergence. It takes with reference to the distress of sophisticated electronic gadgets and renewable vigor sources. The predominant matter is centers to the voltage enlargements and dips. In this paper the instigator introduces a precise method for the bulge of voltages and dips. In the direction of revise the topic pointed out above for modified power types of electronic cog is embraces. Solitary, dynamic voltage restorer (DVR), is a leading progressed control hardware that can be utilized in micro-grids. If any uncertainty produced in the micro-grid DVR act as most favorably to ease the power quality problems. This job clarifies the MATLAB consequences of a dynamic voltage restorer (DVR) demonstrating with investigation. Particular form of controller's similar to PI and NN controller like (proportional integral and neural network) is utilized in this DVR system. In this system procedure, DVR NN controller actualizes modified through the conventional PID-controller can be used examine a builded Composite micro grid system with Renewable sources. The newly developed controller is faster than traditional process initiated controller. By the use of MATLAB replication tools, presentation of the system shall be considered. To find the performance of the suggested NN based DVR which is simulated for the power loss reduction, power factor correction, and active and reactive power flow control. From the simulation results, NN based DVR is better compared to PI based DVR.																	1868-5137	1868-5145															10.1007/s12652-020-02203-8		JUN 2020											
J								Resolution for bounded-splitting jobs scheduling problem on a single machine in available time-windows	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Single machine scheduling; Available time-windows; Splitting-job; MILP model; Approximating algorithms	DETERIORATING JOBS; SHOP; COMPLEXITY; DEADLINES; MINIMIZE; DATES	This paper studies a single machine scheduling problem which machine-availability constraint is imposed. The jobs are splittable into sub-jobs and the size of each sub-job has a common lower bound. The objective aims to find an optimal scheduling solution that minimizes the maximum completion time for the whole set jobs. The scheduling problem was proved to be strongly NP-hard by a reduction from 3-Partition problem. In this paper, a general mixed-integer linear mathematical model is constructed based on some structurally optimal properties in the literature. Besides that we propose some effective heuristics concerned about assignment strategy such as assignment heuristic, heuristic based on shortest/longest processing time rules, heuristic based on max flow resolution, matching and assignment approach. In order to improve solution quality, we also propose to apply a combination between these proposed heuristics and metaheuristics such as tabu search and genetic algorithm. In addition, in the hope of achieving the better solutions in acceptable time, we introduce another approach called exact for subset-jobs matheuristic which is combined between mathematical programming and heuristic derived from single-attribute priority rule. Experimental results show the performance of proposed approximating approaches in comparing between them and the exact method based on the MILP model which is implemented by CPLEX solver. Among proposed effective algorithms, the matheuristic showed the superiority in terms of solution quality and execution time.																	1868-5137	1868-5145															10.1007/s12652-020-02162-0		JUN 2020											
J								In-depth exploration of attribute information for person re-identification	APPLIED INTELLIGENCE										Person re-identification; Attribute; Identity recognition network; Attribute recognition network	DEEP; PERFORMANCE	Pedestrian's attribute information plays an important role in person re-identification (re-ID) for its complementary to pedestrian's identity labels. However, there are few methods to utilize attribute information, which limits the development of re-ID community. In this paper, we analyze the effect of attribute information on re-ID to obtain both qualitative and quantitative results, indicating the potential for in-depth exploration of attribute information. On this basis, we propose an Identity Recognition Network (IRN) and an Attribute Recognition Network (ARN). IRN enhances the attention to pedestrian's local information while identifying pedestrians' identity. ARN calculates the attribute similarity among pedestrians accurately to promote the identification of IRN. The combination of them makes deep exploration of attribute information and is easy to implement. The experimental results on two large-scale re-ID benchmarks demonstrate the effectiveness of our method, which is on par with the state-of-the-art. In the DukeMTMC-reID dataset, mAP (rank-1) accuracy is improved from 58.4 (78.3) % to 66.4 (82.7) % for ResNet-50. In the Market1501 dataset, mAP (rank-1) accuracy is improved from 75.8 (90.5) % to 79.5 (92.8) % for ResNet-50.																	0924-669X	1573-7497				NOV	2020	50	11					3607	3622		10.1007/s10489-020-01752-x		JUN 2020											
J								Deep learning approach for facial age classification: a survey of the state-of-the-art	ARTIFICIAL INTELLIGENCE REVIEW										Age estimation; Convolutional neural network; Deep learning; Facial aging	FACE RECOGNITION; MODEL	Age estimation using face images is an exciting and challenging task. The traits from the face images are used to determine age, gender, ethnic background, and emotion of people. Among this set of traits, age estimation can be valuable in several potential real-time applications. The traditional hand-crafted methods relied-on for age estimation, cannot correctly estimate the age. The availability of huge datasets for training and an increase in computational power has made deep learning with convolutional neural network a better method for age estimation; convolutional neural network will learn discriminative feature descriptors directly from image pixels. Several convolutional neural net work approaches have been proposed by many of the researchers, and these have made a significant impact on the results and performances of age estimation systems. In this paper, we present a thorough study of the state-of-the-art deep learning techniques which estimate age from human faces. We discuss the popular convolutional neural network architectures used for age estimation, presents a critical analysis of the performance of some deep learning models on popular facial aging datasets, and study the standard evaluation metrics used for performance evaluations. Finally, we try to analyze the main aspects that can increase the performance of the age estimation system in future.																	0269-2821	1573-7462															10.1007/s10462-020-09855-0		JUN 2020											
J								Dual generalized Bonferroni mean operators based on 2-dimensional uncertain linguistic information and their applications in multi-attribute decision making	ARTIFICIAL INTELLIGENCE REVIEW										Bonferroni mean (BM); 2-dimensional uncertain linguistic variables; Multi-attribute decision making	AGGREGATION OPERATORS; ELECTRE-III; SELECTION; FRAMEWORK; MODELS	The dual generalized Bonferroni mean operator is a further extension of the generalized Bonferroni mean operator which can take the interrelationship of different numbers of attributes into account by changing the embedded parameter. The 2-dimensional uncertain linguistic variable (2DULV) adds a second dimensional uncertain linguistic variable (ULV) to express the reliability of the assessment information in first dimensional information, which is more rational and accurate than the ULV. In this paper, for combining the advantages of them, we propose the dual generalized weighted Bonferroni mean operator for 2DULVs (2DULDGWBM) and the dual generalized weighted Bonferroni geometric mean operator for 2DULVs (2DULDGWBGM). In addition, we explore several particular cases and some rational characters of them. Further, a new approach is introduced to handle multi-attribute decision making problems in the environment of 2DULVs by the proposed operators. Finally, we utilize several illustrative examples to testify the validity and superiority of this new method by comparing with several other methods.																	0269-2821	1573-7462															10.1007/s10462-020-09857-y		JUN 2020											
J								A robust extension of VIKOR method for bipolar fuzzy sets using connection numbers of SPA theory based metric spaces	ARTIFICIAL INTELLIGENCE REVIEW										BFS; Connection numbers (CNs); CN-metric spaces; Properties of CN-metric; Extended VIKOR; MAGDM	DECISION-MAKING; REPRESENTATION; ENTROPY; TOPSIS; MAGDM	The purpose of this study is to introduce an innovative multi-attribute group decision making (MAGDM) based on bipolar fuzzy set (BFS) by unifying" VIseKriterijumska Optimizacija I Kompromisno Rasenje (VIKOR)" method. The VIKOR method is considered to be a useful MAGDM method, specifically in conditions where an expert is unable to determine his choice correctly at the initiation of designing a system. The method of VIKOR is suitable for problems containing conflicting attributes, with an assumption that compromising is admissible for conflict decision, the expert wishes a solution very near to the best, and the different alternatives or choices are processed according to all developed attributes. The theory of set pair analysis is a state-of-the-art uncertainty theory which consists of three factors, including "identity degree", "discrepancy degree", and "contrary degree" of connection numbers (CNs) and coincidence with many existing theories dealing with vagueness in the given information. Consequently, inspired by this, in the present study, we make an effort to improve the theory of data measurement by introducing some metric spaces using CNs of BFSs. In this research paper, we extend VIKOR method in the context of CNs based metrics, which are obtained form bipolar fuzzy numbers (BFNs). Firstly, we develop CNs of BFNs as well as metric spaces based on CNs. We also discuss some interesting properties of proposed metric spaces. Secondly, we develop VIKOR method using CNs based metrics to handle an MAGDM problem under bipolar fuzzy type information. The predominance of proposed metric spaces is also studied by the means of examples. Furthermore, we demonstrate the efficiency of the extended VIKOR method by solving a numerical example, sensitivity analysis and a detailed comparison with some existing approaches.																	0269-2821	1573-7462															10.1007/s10462-020-09859-w		JUN 2020											
J								Lung diseases identification method based on capsule neural network	EVOLUTIONARY INTELLIGENCE										Capsule neural network; Image preprocessing; Data enhancement; Lung diseases identification		In the automatic identification of lesions, aiming at the misdiagnosis of the lung nodule size, shape, blood vessels and other lung tissues on CT images of pneumonia, this paper proposes a method for identifying lung diseases based on capsule neural networks. Considering that the texture features of lung CT images contain important medical information, this article first combines gray average, entropy, fractal dimension and fractal intercept to form feature vectors as texture features, and introduces context models to obtain context information. The LBG algorithm was used to achieve context quantization, and the CT images of lungs were preprocessed. Then, for the problem of less CT image data of lungs and fewer features of lung diseases in this paper, the existing images are enhanced and the data set is expanded. Finally, in the recognition process of pneumonia, the characteristic pose information will affect the recognition. In order to solve this problem, this paper uses a capsule neural network to identify and detect lung CT images. The application results show that the accuracy rate of identifying lung diseases by capsule neural network is 98.7%. Compared with traditional convolutional networks, capsule neural networks can better identify lung diseases.																	1864-5909	1864-5917															10.1007/s12065-020-00408-6		JUN 2020											
J								Dragonfly-based swarm system model for node identification in ultra-reliable low-latency communication	NEURAL COMPUTING & APPLICATIONS										Bit error rate; Dragonfly; Latency; Node identification; Reliability; Throughput; Ultra-reliable low-latency communication	SINE COSINE ALGORITHM; MATHEMATICAL-THEORY; OPTIMIZATION; NETWORKS; EVOLUTIONARY; EFFICIENCY; SEARCH; SCHEME; TESTS	Latency and reliability are essential parameters for enabling ultra-reliable low-latency communication (URLLC). Therefore, an approach for node identification that satisfies the requirements of latency and reliability for URLLC based on the formation of swarms by dragonflies, called dragonfly node identification algorithm (DNIA), is proposed. This method maps bio-natural systems and legacy communication into metrics of URLLC, i.e., latency and reliability, for node identification. A performance analysis demonstrates that the new paradigm for mapping the metrics, i.e., latency and reliability, in terms of nodes (food source) and noise (predators) provides another dimension for URLLC. A comparative analysis proves that DNIA demonstrates significant impact on the improvement of latency, reliability, packet loss rate, as well as throughput. The robustness and efficiency of the proposed DNIA are evaluated using statistical analysis, convergence rate analysis, Wilcoxon test, Friedman rank test, and analysis of variance on classical as well as modern IEEE Congress on Evolutionary Computation 2014 benchmark functions. Moreover, simulation results show that DNIA outperforms other bioinspired optimization algorithms in terms of cumulative distributive function and average node identification errors. The conflicting objectives in the tradeoff between low latency and high reliability in URLLC are discussed on a Pareto front, which shows the improved and accurate approximation for DNIA on a true Pareto front. Further, DNIA is benchmarked against standard functions on the Pareto front, providing significantly superior results in terms of coverage as well as convergence.																	0941-0643	1433-3058															10.1007/s00521-020-05056-6		JUN 2020											
J								E-mail classification with machine learning and word embeddings for improved customer support	NEURAL COMPUTING & APPLICATIONS										E-mail classification; Machine learning; Long short-term memory; Natural language processing		Classifying e-mails into distinct labels can have a great impact on customer support. By using machine learning to label e-mails, the system can set up queues containing e-mails of a specific category. This enables support personnel to handle request quicker and more easily by selecting a queue that match their expertise. This study aims to improve a manually defined rule-based algorithm, currently implemented at a large telecom company, by using machine learning. The proposed model should have higher F-1-score and classification rate. Integrating or migrating from a manually defined rule-based model to a machine learning model should also reduce the administrative and maintenance work. It should also make the model more flexible. By using the frameworks, TensorFlow, Scikit-learn and Gensim, the authors conduct a number of experiments to test the performance of several common machine learning algorithms, text-representations, word embeddings to investigate how they work together. A long short-term memory network showed best classification performance with an F-1-score of 0.91. The authors conclude that long short-term memory networks outperform other non-sequential models such as support vector machines and AdaBoost when predicting labels for e-mails. Further, the study also presents a Web-based interface that were implemented around the LSTM network, which can classify e-mails into 33 different labels.																	0941-0643	1433-3058															10.1007/s00521-020-05058-4		JUN 2020											
J								Simultaneous segmentation of the optic disc and fovea in retinal images using evolutionary algorithms	NEURAL COMPUTING & APPLICATIONS										Evolutionary algorithm; Differential evolution; Optic disc; Fovea; Segmentation; Retinal image	DIGITAL FUNDUS IMAGES; DIFFERENTIAL EVOLUTION; AUTOMATIC DETECTION; BLOOD-VESSELS; NERVE HEAD; LOCALIZATION; OPTIMIZATION; BOUNDARY; ENTROPY	In this work, we present a new methodology to simultaneously segment anatomical structures in medical images. Additionally, this methodology is instantiated in a method that is used to simultaneously segment the optic disc (OD) and fovea in retinal images. The OD and fovea are important anatomical structures that must be previously identified in any imagebased computer-aided diagnosis system dedicated to diagnosing retinal pathologies that cause vision problems. Basically, the simultaneous segmentation method uses an OD-fovea model and an evolutionary algorithm. On the one hand, the model is built using the intra-structure relational knowledge, associated with each structure, and the inter-structure relational knowledge existing between both and other retinal structures. On the other hand, the evolutionary algorithm (differential evolution) allows us to automatically adjust the instance parameters that best approximate the OD-fovea model in a given retinal image. The method is evaluated in the MESSIDOR public database. Compared with other recent segmentation methods in the related literature, competitive segmentation results are achieved. In particular, a sensitivity and specificity of 0.9072 and 0.9995 are respectively obtained for the OD. Considering a success when the distance between the detected and actual center is less than or equal to eta times the OD radius, the success rates obtained for the fovea are 97.3% and 99.0% for eta = 1/ and eta = 1, respectively. The segmentation average time per image is 29.35 s.																	0941-0643	1433-3058															10.1007/s00521-020-05060-w		JUN 2020											
J								Performance evaluation of manufacturing collaborative logistics based on BP neural network and rough set	NEURAL COMPUTING & APPLICATIONS										BP neural network; Rough set; Manufacturing; Collaborative logistics; Performance evaluation	ENTREPRENEURIAL ORIENTATION	The collaborative logistics in manufacturing industry has a greater impact on its operation effect, and there are many hidden factors. In order to improve the performance evaluation of manufacturing collaborative logistics, this study builds a combined performance evaluation model based on BP neural network and rough set. Moreover, this study uses the rough set attribute reduction theory to screen and optimize the evaluation indicators to obtain the key performance indicator set, and then uses BP neural network to predict and evaluate the key performance indicator data, which greatly reduces the number of training times and shortens the learning time. In addition, in this study, a case analysis was used to solve the performance evaluation model of manufacturing collaborative logistics based on rough set and BP neural network, and corresponding strategies were given. The research results show that the method proposed in this paper has certain effects.																	0941-0643	1433-3058															10.1007/s00521-020-05099-9		JUN 2020											
J								Particle swarm optimisation with opposition learning-based strategy: an efficient optimisation algorithm for day-ahead scheduling and reconfiguration in active distribution systems	SOFT COMPUTING										Metaheuristics; Particle swarm optimisation; Electric distribution systems; Reconfiguration	DISTRIBUTION NETWORK RECONFIGURATION; VARYING ACCELERATION COEFFICIENTS; RADIAL-DISTRIBUTION SYSTEMS; POWER LOSS MINIMIZATION; PSO; RELIABILITY; REDUCTION; DSTATCOM; VELOCITY; COST	In operation of active electric distribution networks, optimal configuration and schedule of distributed generation and reactive power resources are determined. This represents a formidable multi-modal constrained optimisation problem with discrete decision variables. Metaheuristics are the most common approaches for solving this problem. However, due to its multi-modal nature, metaheuristics commonly converge prematurely into local optima and cannot find near-global solutions. In this research, a new particle swarm optimisation (PSO) variant is put forward for finding optimal configuration and schedule of distributed generation and reactive power resources in distribution systems including both dispatchable and renewable distributed energy resources. In the proposed PSO variant, opposition-based learning concept is incorporated into PSO which reduces premature convergence probability through enhancement of swarm leaders. The results of the proposed opposition-based PSO in IEEE 69 bus system indicate its outperformance over conventional PSO, time-varying acceleration coefficient PSO, fractal optimisation algorithm and evolutionary programming.																	1432-7643	1433-7479															10.1007/s00500-020-05093-2		JUN 2020											
J								Cooperative Manipulation of an Unknown Object via Omnidirectional Unmanned Aerial Vehicles	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Aerial robotics; Cooperative impedance control; Cooperative transportation	IMPEDANCE CONTROL; TRANSPORT	This paper addresses the problem of cooperative manipulation and transportation of large and/or heavy unknown objects via a team of aerial robots. In particular, the chosen aerial platform is an omnidirectional quadrotor, which, by exploiting two additional actuators, has the capability to produce an omnidirectional total thrust by changing the orientation of the plane of the rotors. Since it is assumed that the object geometry and dynamic parameters are not known, the devised strategy includes a first stage in which the robots cooperatively estimate the object parameters and a second stage in which such parameters are adopted in an impedance control, aimed at limiting both the contact wrenches, due to the object/environment interaction, and the internal wrenches, due to the robot/object interaction. More in detail, two admittance filters are designed to determine the reference trajectory for the object (external impedance) and for the aerial vehicles (internal impedance). Finally, a simulation case study is developed to provide further insights on the proposed approach.																	0921-0296	1573-0409															10.1007/s10846-020-01213-0		JUN 2020											
J								Stochastic Distance Transform: Theory, Algorithms and Applications	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Distance transform; Discrete random set; Robust distance; Image segmentation; Deterministic algorithm; Monte-Carlo method	SETS	Distance transforms (DTs) are standard tools in image analysis, with applications in image registration and segmentation. The DT is based on extremal (minimal) distance values and is therefore highly sensitive to noise. We present astochastic distance transform(SDT) based ondiscrete random sets, in which a model of element-wise probability is utilized and the SDT is computed as the first moment of the distance distribution to the random set. We present two methods for computing the SDT and analyze them w.r.t. accuracy and complexity. Further, we propose a method, utilizing kernel density estimation, for estimating probability functions and associated random sets to use with the SDT. We evaluate the accuracy of the SDT and the proposed framework on images of thin line structures and disks corrupted by salt and pepper noise and observe excellent performance. We also insert the SDT into a segmentation framework and apply it to overlapping objects, where it provides substantially improved performance over previous methods. Finally, we evaluate the SDT and observe very good performance, on simulated images from localization microscopy, a state-of-the-art super-resolution microscopy technique which yields highly spatially localized but noisy point-clouds.																	0924-9907	1573-7683				JUN	2020	62	5			SI		751	769		10.1007/s10851-020-00964-7		JUN 2020											
J								Joint low-rank project embedding and optimal mean principal component analysis	IET IMAGE PROCESSING										principal component analysis; iterative methods; matrix algebra; data structures; unsupervised learning; discriminative structure; self-representation coefficient weight matrix; low-dimensional embedding subspace; LRPE-OMPCA; intrinsic manifold structure; efficient optimal projection matrix; global geometric information; unsupervised dimensionality reduction approach; optimal mean principal component analysis; joint low-rank project embedding; global data structure; effective iterative algorithm; image databases	DIMENSIONALITY REDUCTION; ALGORITHM; CLASSIFICATION; RECOGNITION; EIGENFACES	Principal component analysis (PCA) is the most widely used unsupervised dimensionality reduction approach. A number of variants of PCA have been proposed to improve the robustness of the algorithm. However, the existing methods either cannot select the useful features consistently or is still sensitive to outliers. In order to reveal the intrinsic manifold structure and preserve the global structure of data, it is needed to learn more efficient optimal projection matrix for sample sets with outliers. To this end, the authors propose a novel PCA, named low-rank project embedding and optimal mean principal component analysis (abbreviated as LRPE-OMPCA), which can learn the optimal mean and the optimal projection matrix and preserve the global geometric information and discriminative structure captured by the self-representation coefficient weight matrix into the low-dimensional embedding subspace. Thus, not only can the proposed method further reduce the influence of outliers but also can discard the useless features, which effectively improve the robustness of the method. An effective iterative algorithm to solve the LRPE-OMPCA is designed. Experimental results on several image databases illustrate the robustness and effectiveness of the proposed method.																	1751-9659	1751-9667				JUN 19	2020	14	8					1457	1466		10.1049/iet-ipr.2019.1027													
J								Single image rain removal with reusing original input squeeze-and-excitation network	IET IMAGE PROCESSING										image denoising; image reconstruction; image restoration; learning (artificial intelligence); image texture; image enhancement; rain; video signal processing; image representation; single image rain removal; input squeeze; -excitation network; network architecture; representational power; -excitation block; network connection; original input; ROI connection; texture details; rain removal performance; synthetic images; real-world images		In this study, the authors propose a novel network architecture to address the problem of removing rain streaks from single images. To strengthen the representational power of the network, they adopt the squeeze-and-excitation block in the network. Furthermore, they propose a new network connection called reusing original input (ROI). The ROI connection reuses the original input of the network and can provide more texture details of the background. These details can be useful for the restoration of the image after removing the rain streaks. Batch normalisation is applied to further improve the rain removal performance of the network. Despite the fact that the network is trained on synthetic data, experimental results show that the proposed network has a comparable performance on both synthetic images and real-world images to the state-of-the-art methods.																	1751-9659	1751-9667				JUN 19	2020	14	8					1467	1474		10.1049/iet-ipr.2019.0716													
J								Fast algorithm for large-scale subspace clustering by LRR	IET IMAGE PROCESSING											LOW-RANK REPRESENTATION; SEGMENTATION	Low-rank representation (LRR) and its variants have been proved to be powerful tools for handling subspace clustering problems. Most of these methods involve a sub-problem of computing the singular value decomposition of an n x n matrix, which leads to a computation complexity of O(n(3)). Obviously, when n is large, it will be time consuming. To address this problem, the authors introduce a fast solution, which reformulates the large-scale problem to an equal form with smaller size. Thus, the proposed method remarkably reduces the computation complexity by solving a small-scale problem. Theoretical analysis proves the efficiency of the proposed model. Furthermore, we extend LRR to a general model by using Schatten p-norm instead of nuclear norm and present a fast algorithm to solve large-scale problem. Experiments on MNIST and Caltech101 databse illustrate the equivalence of the proposed algorithm and the original LRR solver. Experimental results show that the proposed algorithm is remarkably faster than traditional LRR algorithm, especially in the case of large sample number.																	1751-9659	1751-9667				JUN 19	2020	14	8					1475	1480		10.1049/iet-ipr.2018.6596													
J								Pulmonary nodule risk classification in adenocarcinoma from CT images using deep CNN with scale transfer module	IET IMAGE PROCESSING										lung; learning (artificial intelligence); cancer; biological organs; medical image processing; image classification; patient diagnosis; neural nets; feature extraction; computerised tomography; pulmonary nodule risk classification; CT images; deep CNN; scale transfer module; lung cancer; clinical treatment decision; early diagnosis; lung adenocarcinoma; imaging studies; deep convolutional neural network; named STM-Net; different resolution images; computed tomography database; Zhongshan Hospital Fudan University; lung adenocarcinomas risk; minimally invasive adenocarcinoma; authors; risk stage prediction; classification accuracy; pulmonary nodules classification; physicians diagnosis pulmonary nodules risk classification; early-stage	LUNG-CANCER; PREDICTION; TUMORS; SIZE	Pulmonary nodules risk classification in adenocarcinoma is essential for early detection of lung cancer and clinical treatment decision. Improving the level of early diagnosis and the identification of small lung adenocarcinoma has been always an important topic for imaging studies. In this study, the authors propose a deep convolutional neural network (CNN) with scale-transfer module (STM) and incorporate multi-feature fusion operation, named STM-Net. This network can amplify small targets and adapt to different resolution images. The evaluation data were obtained from the computed tomography (CT) database provided by Zhongshan Hospital Fudan University (ZSDB). All data have a pathological label and their lung adenocarcinomas risk are classified into four categories: atypical adenomatous hyperplasia, adenocarcinoma in situ, minimally invasive adenocarcinoma, and invasive adenocarcinoma. The authors' deep learning network STM-Net was trained and tested for the risk stage prediction. The accuracy and the average area under the receiver operating characteristic curve achieved by their method are 95.455% and 0.987 for the ZSDB dataset. The experimental results show that STM-Net largely boosts classification accuracy on the pulmonary nodules classification compared with state-of-the-art approaches. The proposed method will be an effective auxiliary to help physicians diagnosis pulmonary nodules risk classification in adenocarcinoma in early-stage.																	1751-9659	1751-9667				JUN 19	2020	14	8					1481	1489		10.1049/iet-ipr.2019.0248													
J								Deep detector classifier (DeepDC) for moving objects segmentation and classification in video surveillance	IET IMAGE PROCESSING										image classification; image motion analysis; video surveillance; object detection; image segmentation; image sequences; learning (artificial intelligence); video signal processing; video surveillance; moving objects; video sequences; unsupervised anomaly discovery framework; generative adversarial networks; deep detector classifier; spatial context; temporal context; foreground object segmentation; generative models; segmented objects; videos surveillance; missing data imputation task	BACKGROUND SUBTRACTION; NEURAL-NETWORK; FEATURES; TRACKING; IMAGE	In this study, the authors present a new approach to segment and classify moving objects in video sequences by combining an unsupervised anomaly discovery framework called DeepSphere and generative adversarial networks. The proposed deep detector classifier employs and validates DeepSphere, which aims mainly to identify the anomalous cases in the spatial and temporal context in order to perform foreground objects segmentation. For post-processing, some morphological operations are considered to better segment and extract the desired objects. Finally, they take advantage of the power of generative models, which recognise the problem of semi-supervised learning as a specific missing data imputation task in order to classify the segmented objects. They evaluate the method with multiple datasets and the results confirm the effectiveness of the proposed approach, which achieves superior performance over the state-of-the-art methods having the capabilities of segmenting and classifying moving objects from videos surveillance.																	1751-9659	1751-9667				JUN 19	2020	14	8					1490	1501		10.1049/iet-ipr.2019.0769													
J								Object detection based on RGC mask R-CNN	IET IMAGE PROCESSING										neural nets; learning (artificial intelligence); feature extraction; image classification; object detection; image coding; computer vision; image representation; object detection; RGC mask R-CNN; Mask Region-Convolution Neural Network based methods; high quality samples; detection performance; improved Mask R-CNN-based method; ResNet Group Cascade Mask R-CNN; feature pyramid network neck; bounding box head; high-quality samples		Object detection is a crucial topic in computer vision. Mask Region-Convolution Neural Network (R-CNN) based methods, wherein a large intersection over union (IoU) threshold is chosen for high quality samples, have often been employed for object detection. However, the detection performance of such methods deteriorates when samples are reduced. To address this, the authors propose an improved Mask R-CNN-based method: the ResNet Group Cascade (RGC) Mask R-CNN. First, they compared ResNet with different layers, finding that ResNeXt-101-64 x 4d is superior to other backbone networks. Secondly, during the training of the test model, the performance of Mask R-CNN suffered from a small batch processing scale, resulting in inaccurately calculated mean and variance; thus, group normalisation was added to the backbone, feature pyramid network neck and bounding box head of the network. Finally, the higher the intersection of Mask R-CNN than the threshold, the easier it is to obtain high-quality samples. However, blindly selecting a high threshold leads to sample reduction and overfitting. Thus, a proposed cascade network configuration with three IoU thresholds was utilised in the process of model training. The model was trained and tested on the COCO and PASCAL VOC07 datasets. Their proposed algorithm demonstrated superior performance compared to that of the Mask R-CNN.																	1751-9659	1751-9667				JUN 19	2020	14	8					1502	1508		10.1049/iet-ipr.2019.0057													
J								Optical flow refinement using iterative propagation under colour, proximity and flow reliability constraints	IET IMAGE PROCESSING										motion estimation; image motion analysis; reliability; image sequences; optical flow refinement; iterative propagation; flow reliability constraints; motion estimation; flow reliability constraint; proximity constraint; colour constraint; Kanade-Lucas-Tomasi method; global reliability measures; colour local homogeneousness; motion map; KLT residuals; temporal evolution; optical flow local variance		This study proposes a strategy to refine optical flow based on the estimated reliability maps. These maps are firstly estimated a posteriori after the motion estimation by the well-known Kanade-Lucas-Tomasi (KLT). With two new defined criteria based, respectively, on the optical flow local variance and the temporal evolution of the KLT residuals, a global refinement of the motion map is then carried out through two stages under the control of the reliability measures and the colour local homogeneousness. According to the experiments performed on the Middlebury dataset, the authors' reliability measures prove to be a good indicator for the quality of the estimation. Indeed, the correction process increases the global reliability measures and reduces the global errors in a significant way. The experiments show that the quality is higher than classical estimation methods and ranked at 88/168 on Middlebury website.																	1751-9659	1751-9667				JUN 19	2020	14	8					1509	1519		10.1049/iet-ipr.2019.0370													
J								Segmentation of the pulmonary nodule and the attached vessels in the CT scan of the chest using morphological features and topological skeleton of the nodule	IET IMAGE PROCESSING										image enhancement; computerised tomography; medical image processing; lung; feature extraction; image segmentation; morphological feature; segmentation framework; automatic segmentation; curvature features; nodule area; Hessian-based vesselness enhancement map; segmented nodule; pulmonary nodule; CT scan; chest; computer-aided diagnosis systems; malignant nodules; baseline computed tomography scan	AUTOMATIC LUNG SEGMENTATION; NEURAL-NETWORKS; CANCER; CLASSIFICATION; TORTUOSITY; ALGORITHM; SYSTEM	Nowadays, the proficiency of Computer-Aided Diagnosis systems for early diagnosis of malignant nodules in baseline Computed Tomography (CT) scan of chest crucially depends on the authenticity of the segmented nodule. In this study, the authors introduce a new morphological feature called solidity radius (SR). They then employ this feature in the new segmentation framework for the automatic segmentation of nodule and the attached vessels around the seed point on the nodule, delineated by an expert. In the framework, they extract the SR and the curvature features and employ them to determine the candidate pixels of the nodule. They then use the convex-hull image of the candidate pixels to surround the nodule area. Afterward, using the region growing on the Hessian-based vesselness enhancement map, the attached vessels are labelled. Finally, they apply the traditional solidity feature of the segmented nodule and the pattern of the related skeleton to prune the false positive pieces. They validate the introduced approach on two datasets, including 56 and 481 CTs (containing 1205 nodules). They show the proficiency of their SR-based approach compared to the state-of-the-art methods with average Dice Similarity Coefficients of 77.98 and 77.47% for the two datasets, respectively.																	1751-9659	1751-9667				JUN 19	2020	14	8					1520	1528		10.1049/iet-ipr.2019.1054													
J								Adaptive Gaussian notch filter for removing periodic noise from digital images	IET IMAGE PROCESSING										optical filters; Fourier transforms; filtering theory; notch filters; image restoration; image denoising; noisy peak positions; associated noisy areas; identified noisy peaks; identified noisy peak areas; notch filters; peak signal-to-noise ratio; AGNF; periodic noise; adaptive Gaussian notch filter; digital images; Fourier transform domain; Moire pattern noises; uncorrupted images; noisy functions; easily distinguishable conjugate peaks	FREQUENCY-DOMAIN FILTER; RESTORATION	Periodic noise corrupts digital images during acquisition and transmission stages by adding repetitive patterns. This study introduces a new adaptive Gaussian notch filter (AGNF) in Fourier transform domain for effectively restoring images contaminated with periodic, quasi-periodic and Moire pattern noises. Since periodic noises are sinusoidal functions added to the uncorrupted images, Fourier transform of images make these noisy functions to concentrate as easily distinguishable conjugate peaks in frequency domain. The proposed AGNF effectively identifies the noisy peak positions and adaptively quantifies the associated noisy areas by analysing the ratio of averages of frequencies from adaptively varying neighbourhoods. These identified noisy peaks are then diffused by Gaussian notch filter of adaptively varying sizes. The proposed filter ensures maximum diffusion of identified noisy peak areas by maintaining the minimum frequency values from the outputs of overlapping notch filters. Visual and quantitative experimental analysis of the proposed algorithm with mean absolute error, peak signal-to-noise ratio, mean structural similarity index measure and computation time reveals that AGNF is better in restoring images contaminated with periodic noises when compared to other methods.																	1751-9659	1751-9667				JUN 19	2020	14	8					1529	1538		10.1049/iet-ipr.2018.5707													
J								Image interpolation with adaptive k-nearest neighbours search and random non-linear regression	IET IMAGE PROCESSING										image reconstruction; regression analysis; image resolution; interpolation; nearest neighbour methods; search problems; random processes; image patch pairs; nonlinear regression model; extreme learning machine; test images; image datasets; reconstructed images; nearest neighbours search; image interpolation methods; input image patch; nonlinear mapping; high-resolution image patches; training image patches; normalised Gaussian similarity; nearest neighbour searching scheme	EXTREME LEARNING-MACHINE; FACE IMAGE; SUPERRESOLUTION	Learning-based image interpolation methods have been proved to be effective in image interpolation. In this study, the authors propose an accurate image interpolation with adaptive k-nearest neighbour searching and non-linear regression. The proposed method aims to find k-nearest neighbours of the input image patch and use them to learn the non-linear mapping between low-resolution and high-resolution image patches. To be specific, they first divide the training image patches into many subspaces, then they utilise an adaptive robust and precise k nearest neighbour searching scheme with proposed normalised Gaussian similarity to find the k nearest neighbours in the matched subspace. The selected k image patch pairs are then used to learn the non-linear regression model through an extreme learning machine. Furthermore, the proposed interpolation method is a cascade framework that consists of two stages. Stage 2 takes the results of Stage 1 as input to further improve the performance. Extensive experimental results on commonly used test images and image datasets indicate that their proposed algorithm obtains competitive performance against the state-of-the-art methods both in terms of objective evaluation values and the subjective effect of reconstructed images.																	1751-9659	1751-9667				JUN 19	2020	14	8					1539	1548		10.1049/iet-ipr.2019.1591													
J								Hierarchical morphological graph signal multi-layer decomposition for editing applications	IET IMAGE PROCESSING										image enhancement; filtering theory; mathematical morphology; image texture; graph theory; image colour analysis; morphological processing; editing applications; hierarchical morphological graph signal multilayer decomposition; editing signals; 2D colour images; 3D coloured meshes; state-of-the-art editing; morphological filtering operators	EMPIRICAL MODE DECOMPOSITION; IMAGE QUALITY ASSESSMENT; SCALE-SPACE; REPRESENTATION; EQUATIONS; FREQUENCY; DISTANCE; FILTERS	The authors address the problem of editing signals such as 2D colour images or 3D coloured meshes that are represented under the general framework of graph signals. As state-of-the-art editing approaches decompose an image into several layers in order to manipulate them, they propose a hierarchical multi-layer decomposition of graph signals that relies on morphological filtering. Since morphological filtering operators require a complete lattice, a dedicated approach for the morphological processing of vectorial data on graphs is used. By iterating the application of morphological filterings of decreasing sizes, the graph signal is decomposed into several detail layers, each capturing a given detail level. Editing applications such as abstraction, sharpness enhancement and tone mapping are shown to illustrate the benefits of the proposed approach.																	1751-9659	1751-9667				JUN 19	2020	14	8					1549	1560		10.1049/iet-ipr.2019.0576													
J								Measuring photography aesthetics with deep CNNs	IET IMAGE PROCESSING										photography; backpropagation; computer vision; art; image classification; convolutional neural nets; deep CNNs; automatic photo aesthetic assessment algorithms; photo quality enhancement; personal photo assistance; deep learning-based techniques; computer vision; aesthetic value; artistic quality; aesthetic measure; individual aesthetic attributes; multitask deep convolutional neural network; visualisation technique; aesthetic feature vector; photography aesthetics measurement; loss function; mathematical representation; gradient backpropagation; aesthetic attributes; relative foreground position; general aesthetic score prediction; human-interpretability enhancement		In spite of the recent advancements of deep learning based techniques, automatic photo aesthetic assessment still remains a challenging computer vision task. Existing approaches used to focus on providing a single aesthetic score or category ("good" or "bad") of photograph, rather than quantifying "goodness" or "badness". The existing algorithms often ignore the importance of different attributes contributing to the artistic quality of the photograph. To obtain the human-interpretability of aesthetic score of photo, we advocate learning the aesthetic attributes alongwith the prediction of the general aesthetic score. We propose a multi-task deep CNN, that collectively learns aesthetic attributes alongwith a general aesthetic score for the photograph. To understand the mathematical representation of the attributes in the proposed model, a visualization technique is proposed using back propagation of gradients. These visualization of attributes correspond to the location of objects in the images in order to find out which part of an image "triggers" the classification outcome, thus providing the insights about the model's understanding of these attributes. This paper proposes an aesthetic feature vector based on the relative foreground position of the object in the image. The proposed aesthetic features outperform the state-of-art methods especially for Rule of Thirds attribute.																	1751-9659	1751-9667				JUN 19	2020	14	8					1561	1570		10.1049/iet-ipr.2019.1300													
J								Classification of dry age-related macular degeneration and diabetic macular oedema from optical coherence tomography images using dictionary learning	IET IMAGE PROCESSING										feature extraction; vision defects; image classification; diseases; image segmentation; eye; biomedical optical imaging; medical image processing; optical tomography; dry age-related macular degeneration; diabetic macular oedema; optical coherence tomography images; AMD; DME; vision loss; developed countries; retinal layer structure; diseases; automatic classification; classification algorithm; retinal layer segmentation; normal region identifications; abnormal region identifications; local intensity gradients; dictionary learning-based classifiers; normal subjects; normal OCT images; edge directions; extracted features	DME	Age-related Macular Degeneration (AMD) and Diabetic Macular Edema (DME) are the major causes of vision loss in developed countries. Alteration of retinal layer structure and appearance of exudates are the most significant signs of these diseases. In this paper, with the aim of automatic classification of DME, AMD, and normal subjects using Optical Coherence Tomography (OCT) images, a dictionary-learning based classification is proposed. The two important issues intended in this approach are avoiding retinal layer segmentation and attempting to mimic the authors' understanding based on normal and abnormal region identifications, considering that the signs of diseases appear in a small fraction of B-Scans. The histogram of oriented gradients feature descriptor was utilized to characterize the distribution of local intensity gradients and edge directions. To capture the structure of extracted features, different dictionary learning-based classifiers are employed. The dataset consists of 45 subjects: 15 patients with AMD, 15 patients with DME, and 15 normal subjects. The proposed classifier leads to an accuracy of 95.13, 100.00, and 100.00% for DME, AMD, and normal OCT images, respectively, only by considering 4% of all B-Scans of a volume, which outperforms the state-of-the-art methods.																	1751-9659	1751-9667				JUN 19	2020	14	8					1571	1579		10.1049/iet-ipr.2018.6186													
J								Pavement crack detection network based on pyramid structure and attention mechanism	IET IMAGE PROCESSING										learning (artificial intelligence); object detection; crack detection; roads; structural engineering computing; image recognition; cracks; maintenance engineering; convolutional neural nets; intelligent transportation systems; pavement crack detection network; pyramid structure; attention mechanism; automatic pavement crack detection; surrounding pavement; spatial-channel combinational attention module; crack features; CRACK500 dataset; pavement crack datasets; encoder-decoder network; road maintenance; encoder-decoder architecture; end-to-end trainable deep convolution neural network; complex topology structures; pooling operation; intelligent transportation system		Automatic detection of pavement crack is an important task for conducting road maintenance. However, as an important part of the intelligent transportation system, automatic pavement crack detection is challenging due to the poor continuity of cracks, the different width of cracks, and the low contrast between cracks and the surrounding pavement. This study proposes a novel pavement crack detection method based on an end-to-end trainable deep convolution neural network. The authors build the network using the encoder-decoder architecture and adopt a pyramid module to exploit global context information for the complex topology structures of cracks. Moreover, they introduce a spatial-channel combinational attention module into the encoder-decoder network for refining crack features. Further, the dilated convolution is used to reduce the loss of crack details due to the pooling operation in the encoder network. In addition, they introduce a lovasz hinge loss function, which is suitable for small objects. They train the authors' network on the CRACK500 dataset and evaluate it on three pavement crack datasets. Among the methods they compare, their method can achieve the best experimental results.																	1751-9659	1751-9667				JUN 19	2020	14	8					1580	1586		10.1049/iet-ipr.2019.0973													
J								Single image super-resolution based on sparse representation using dictionaries trained with input image patches	IET IMAGE PROCESSING										singular value decomposition; iterative methods; image reconstruction; interpolation; image resolution; unsupervised learning; sparse representation; input image patches; equal size patches; high resolution dictionary; low resolution version; estimated SR image; difference image; reconstructed SR image; initial SR image; single image super-resolution; K-SVD; LR-HR dictionaries		In this study, an efficient self-learning method for image super-resolution (SR) is presented. In the proposed algorithm, the input image is divided into equal size patches. Using these patches, a dictionary is learned based on K-SVD, referred to as high resolution (HR) dictionary. Then, by down-sampling, the columns of the dictionary, called atoms, a low resolution (LR) version of the dictionary is obtained. An initial estimate of the SR image is constructed using the bicubic interpolation on the input image. Then in an iterative algorithm, the difference between the down-sampled version of the estimated SR image and the input image is obtained. This difference image, which includes reconstructed details is enlarged using sparse representation and LR/HR dictionaries. The enlarged detail is added to the latest reconstructed SR image. This process gradually improves the quality of the initial SR image. After several iterations, the reconstructed image is an SR version of the input image. Experimental results confirm that the proposed method performance is promising.																	1751-9659	1751-9667				JUN 19	2020	14	8					1587	1593		10.1049/iet-ipr.2019.0129													
J								Quantitative assessment of capabilities of colour models for pen ink discrimination in handwritten documents	IET IMAGE PROCESSING											INVESTIGATE; BLUE	The addition of new words in handwritten documents such as bank cheques, bills, and notes is considered as common crime. Such immoral activities on handwritten documents have a bad effect on the victim in terms of mental and financial loss. For facilitating an impartial judicial process, it is important to differentiate between the used pen inks. Earlier, image processing and pattern recognition-based techniques for pen ink differentiation gained attention among researchers due to their non-destructive nature. Existing techniques use various colour models to represent the colour of pen inks. Thus, it is crucial to assess the capabilities of various colour models for differentiating pen inks in handwritten documents. In this paper, we propose a histogram distance based quantitative assessment technique for suitable colour model identification for differentiating pen inks. Seven blue and seven black pen ink samples are acquired on 112 cheque leaves. The k-means binarisation is used to identify pen ink pixels. YCbCr colour model has been identified as the best colour model for this task. The statistical features of the ink colours in the identified colour model representation are extracted and a multi-layer perceptron (MLP) classifier validates the capability of the identified colour model for pen ink discrimination.																	1751-9659	1751-9667				JUN 19	2020	14	8					1594	1604		10.1049/iet-ipr.2018.6616													
J								Improved compound image segmentation using automatic pixel block classification with SVM	IET IMAGE PROCESSING										image coding; image classification; feature extraction; pattern classification; image segmentation; pattern clustering; edge detection; discrete wavelet transforms; support vector machines; block classification accuracy; improved compound image segmentation; automatic pixel block classification; powerpoint images; textual regions; graphical regions; pictorial regions; smooth regions; computer screen images necessitates accurate classification; input compound image; nonoverlapping blocks; block-level edge features; block neighbourhood information; trained SVM model; method performs automatic block classification	COMPRESSION; COEFFICIENT	Computer screen images such as wallpaper, web pages and powerpoint images are compound images. As these images contain a mixture of textual, graphical, pictorial and smooth regions, compression of computer screen images necessitates accurate classification and segmentation of these regions. In this study, an improved compound image segmentation using automatic block classification with support vector machine (SVM) is presented. First, the input compound image is divided into several non-overlapping blocks, and then the statistical features embedded in each block are mined after applying discrete wavelet transform. Then, the SVM model is trained effectively by using the informative samples of fuzzy c-means clustering, which takes block-level edge features and block neighbourhood information as input. Finally, the compound image is classified into two classes such as text/graphics and picture/background with the trained SVM model. Experimental results show that the proposed method performs automatic block classification with high accuracy. As the proposed classifier uses both structural and contextual information as features, block classification accuracy has been improved to a great extent. Hence, the proposed method has made similar to 6.2% improvement in block classification accuracy while comparing with existing approaches.																	1751-9659	1751-9667				JUN 19	2020	14	8					1605	1613		10.1049/iet-ipr.2018.6523													
J								NSST and vector-valued C-V model based image segmentation algorithm	IET IMAGE PROCESSING										transforms; image segmentation; image fusion; nonoverlapping regions; unsupervised image segmentation methods; automatic thresholding; image processing; image segmentation algorithm; NSST; vector-valued image; vector-valued Chan-Vese model; vector-valued C-V model; nonsubsampled shearlet transform; visual effects; diffusion process	CONTOURLET TRANSFORM; ACTIVE CONTOURS; SET	Image segmentation is a process of partitioning an image into non-overlapping regions. Existing unsupervised image segmentation methods include level set, automatic thresholding and region-based CV mode and so on. However, image segmentation as a key technology in the field of image processing has not been solved indeed, especially for images with complex texture. For this reason, the authors proposed a novel image segmentation algorithm based on NSST and the vector-valued Chan-Vese (C-V) model. First, they obtained a multi-scale representation by exploiting the non-subsampled shearlet transform (NSST) to extract multi-dimensional data in the image. Afterwards, they gave the vector-valued C-V model, and applied it to all subbands of NSST, which are treated as a vector-valued image. By comparing with other class methods, the experimental results show that the proposed method has better visual effects and lower error rates. But at the same time, it is a little time consuming. The proposed method is reasonable and effective, by taking full advantages of each subband's directional information during its diffusion process, compared with traditional C-V model.																	1751-9659	1751-9667				JUN 19	2020	14	8					1614	1620		10.1049/iet-ipr.2018.5027													
J								Object counting method based on dual attention network	IET IMAGE PROCESSING										feature extraction; image processing; learning (artificial intelligence); traffic engineering computing; regression analysis; image representation; object counting method; dual attention network; spatial attention mechanism; pyramid structure; atrous pyramid attention module; precise dense multiscale features; global attention feature module; feature representation; channel attention mechanism; designed modules		The challenging problem that the authors solved in this study is to precisely estimate the number of objects in an image. Combining the spatial attention mechanism and pyramid structure, a novel atrous pyramid attention module is introduced to extract precise dense multi-scale features for object counting. Also, a global attention feature module is designed to enhance the ability of the network to learn feature representation based on channel attention mechanism. Combining the proposed atrous pyramid attention module and global attention feature module, a novel object counting method based on a dual attention network is established in this study. The experiments on public vehicle counting dataset including TRANCOS and crowd counting dataset including Mall and Shanghitech_A datasets demonstrate the proposed method achieves competitive performance, and the ablation study verifies the structure rationality of the designed modules.																	1751-9659	1751-9667				JUN 19	2020	14	8					1621	1627		10.1049/iet-ipr.2019.0465													
J								Automatic aircraft extraction using video matting and frame registration	IET IMAGE PROCESSING										feature extraction; image sequences; object detection; image colour analysis; image segmentation; medical image processing; image processing; object tracking; video signal processing; image registration; automatic aircraft extraction; video matting; video sequence; object tracking algorithm; computational complexity; given ROI; nonrigid image registration; alpha mattes; KNN matting algorithm; static region; final alpha matte; different videos; high quality alpha maps	SHIP CLASSIFICATION; IMAGE REGISTRATION; SAR; RECOGNITION; NETWORKS; TRACKER; CNN	This study presents a technique to automatically extract the aircraft from video sequences. The technique incorporates object tracking algorithm to separate the region of interest (ROI), i.e. movement of aircraft, from video sequence, to reduce the computational complexity. Trimaps are generated automatically for given ROI using non-rigid image registration to obtain the prior information of the object and alpha mattes are estimated for each frame using KNN matting algorithm. However, the alpha matte of static region is estimated only once, that is combined with the alpha matte of moving region to generate final alpha matte. Simulations performed on different videos show that the proposed technique not only reduces the computational complexity, but also gives competitive results by generating high quality alpha maps.																	1751-9659	1751-9667				JUN 19	2020	14	8					1628	1635		10.1049/iet-ipr.2019.0067													
J								Visual object tracking via iterative ant particle filtering	IET IMAGE PROCESSING										computer vision; Kalman filters; object tracking; particle filtering (numerical methods); nonlinear filters; ant colony optimisation; iterative methods; particle impoverishment problem; iterative unscented Kalman filter; particle generation; target object; visual object tracking; iterative ant particle filtering; ant colony optimisation-based; ACO-based iterative PF; posterior distribution; locality sensitive histogram; illumination variation	ESTIMATOR; MATRIX	Visual object tracking remains a challenging task in computer vision although important progress has been made in the past decades. Particle filter (PF) is now a standard framework for solving non-linear/non-Gaussian problems, especially in visual object tracking. This study proposes an ant colony optimisation (ACO)-based iterative PF for object tracking. In the proposed method, the basic idea of ACO is used to simulate the behaviour of a particle moving toward the posterior distribution. Such idea is incorporated into the particle filtering framework in order to overcome the well-known particle impoverishment problem. An iterative unscented Kalman filter is used to design a proposal distribution for particle generation in order to generate better predicted sample states. For the likelihood model, the authors adopt the locality sensitive histogram to model the appearance of the target object, which can better handle the illumination variation during tracking. The experimental results demonstrate that the proposed tracker shows better performance than the other tracking methods.																	1751-9659	1751-9667				JUN 19	2020	14	8					1636	1644		10.1049/iet-ipr.2019.0967													
J								Pain intensity recognition via multi-scale deep network	IET IMAGE PROCESSING										image classification; face recognition; feature extraction; pattern classification; video signal processing; learning (artificial intelligence); image sequences; emotion recognition; pain intensity recognition; multiscale deep network; basic facial expression recognition; adjacent intensity levels; face shape recognition; facial pain intensities; tree classifier including multiscale classifiers	EXPRESSION	Similar to the basic facial expression recognition, one challenge for pain intensity recognition is some individual characteristics, e.g. face shapes, may cause great diversities in the same emotion. So it is usually very difficult to distinguish two adjacent intensity levels of pain expression as each intensity has a large variation. In this study, a coarse-to-fine combination method is proposed for pain intensity recognition. The results of multi-scale outputs from multiple base deep network are combined in a probabilistic way for improving the discrimination between visually similar adjacent levels. A two-layer tree classifier is proposed in a multi-task framework for pain intensity recognition as well as face shape recognition, replacing the planar Softmax classifier in each base deep network. In the first layer of tree classifier, multi-scale classifiers are constructed for recognizing facial pain intensities and the conventional classifiers are constructed for face shape recognition in the second layer. Finally, the tree classifier including multi-scale classifiers and conventional classifiers is jointly optimised during the training phase and only high level classifiers are used for recognising pain intensities in the test phase. The extensive experiments on UNBC shoulder pain dataset show the proposed method gets promising results in pain intensity recognition.																	1751-9659	1751-9667				JUN 19	2020	14	8					1645	1652		10.1049/iet-ipr.2019.1448													
J								FuSENet: fused squeeze-and-excitation network for spectral-spatial hyperspectral image classification	IET IMAGE PROCESSING										geophysical image processing; learning (artificial intelligence); neural nets; image classification; feature extraction; hyperspectral imaging; SENet; CNN; squeeze operation; global pooling; max pooling; excitation operation; fused output; classification experiments; benchmark hyperspectral image datasets; FuSENet method; fused squeeze; -excitation network; spectral-spatial hyperspectral image classification; deep learning-based approaches; hand-extracted feature-based methods; convolutional neural network; deep learning architecture; residual network	NEURAL-NETWORK; CNN	Deep learning-based approaches have become very prominent in recent years due to its outstanding performance as compared to the hand-extracted feature-based methods. Convolutional neural network (CNN) is a type of deep learning architecture to deal with the image/video data. Residual network and squeeze and excitation network (SENet) are among recent developments in CNN for image classification. However, the performance of SENet depends on the squeeze operation done by global pooling, which sometimes may lead to poor performance. In this study, the authors propose a bilinear fusion mechanism over different types of squeeze operation such as global pooling and max pooling. The excitation operation is performed using the fused output of squeeze operation. They used to model the proposed fused SENet with the residual unit and name it as FuSENet. Here the classification experiments are performed over benchmark hyperspectral image datasets. The experimental results confirm the superiority of the proposed FuSENet method with respect to the state-of-the-art methods. The source code of the complete system is made publicly available at https://github.com/swalpa/FuSENet.																	1751-9659	1751-9667				JUN 19	2020	14	8					1653	1661		10.1049/iet-ipr.2019.1462													
J								SODNet: small object detection using deconvolutional neural network	IET IMAGE PROCESSING										learning (artificial intelligence); image classification; medical image processing; diseases; biomedical optical imaging; feature extraction; object detection; eye; convolutional neural nets; Retinopathy Online Challenge dataset; softmax layer; microaneurysm; convolutional layers; deconvolution layers; nonMA; retinal fundus image; diabetic retinopathy; CNN; convolution neural network; deconvolutional neural network; object detection	DIABETIC MACULAR EDEMA; MAJOR RISK-FACTORS; MICROANEURYSM DETECTION; RETINAL MICROANEURYSMS; GLOBAL PREVALENCE; RETINOPATHY; SYSTEM	Convolution neural network (CNN) is an efficient technique to detect objects in various kinds of images, especially for microaneurysm (MA) of diabetic retinopathy in retinal fundus image. This study proposes a deconvolutional neural network to accurately discriminate MA from non-MA. The deconvolution, instead of pooling operation, is embedded into the CNN to recover the erased details of feature maps of convolutional layers. Three types of images are collected for training and predicting. Furthermore, the extracted features are fed into the fully-connected layers to classify using a softmax layer. Experimental results demonstrate that the proposed method can achieve significant sensitivity and accuracy on multiple public datasets, in comparison to the state-of-the-art. For Retinopathy Online Challenge dataset, the sensitivity and accuracy are improved up to 0.798 and 0.986, respectively.																	1751-9659	1751-9667				JUN 19	2020	14	8					1662	1669		10.1049/iet-ipr.2019.0833													
J								Brain-Computer Interface System Based on P300 Processing with Convolutional Neural Network, Novel Speller, and Low Number of Electrodes	COGNITIVE COMPUTATION										P300 wave; Event-related potential; Brain-computer interface; EEG signal processing; Convolutional neural networks	BCI; PEOPLE; ICA	The P300 wave has been successfully employed to develop brain-computer interfaces (BCI) for speller applications. However, methods to analyze the P300 require computers with high processing capability because they are computationally complex and require many electrodes. Therefore, this paper proposes a novel BCI speller system based on the P300 wave that employs a few electrodes and a processing method aimed to design ubiquitous and embedded applications. The experiments were developed with a dataset generated by our BCI data acquisition system. The BCI speller developed requires five electrodes for data acquisition, and the visual interface is an improved Donchin speller. Our BCI includes a novel processing method composed of the following modules: preprocessing, signal averaging, low computational cost convolutional neural network, and character prediction. The network has two feature extraction sections, a fully connected layer and a SoftMax layer. According to the results, the proposed BCI speller has an accuracy of 96% using just five electrodes, and it is similar to the best BCI for P300 analysis described in the literature. The processing time makes the system practical for online applications since the processing method has a low computational burden and the acquisition system has the lowest number of electrodes for P300 analysis reported in the literature. Considering the low computational burden, the low number of electrodes required, and the accuracy achieved, we conclude that our proposed BCI speller may be considered as one of the best spellers based on P300.																	1866-9956	1866-9964															10.1007/s12559-020-09744-2		JUN 2020											
J								Secured storage and disease prediction of E-health data in cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Security; Two fish encryption; Cloud; E-health; Optimization	ISSUES; DIAGNOSIS	By applying novel way of cloud computing, the computing resources and services are delivered. That can raise the benefit health care research, and alter the face of health information technology. The E-health applications have capable to process the trivial and non-trivial connections among the different sensor signals and big data, better conception of diseases. Cloud computing are controls better advantages but it has also control main security challenges. To raise the data security in cloud computing the two fish encryption algorithm was suggested by us. At first the sensitive data's are separated into the multiples of data by applying Multi kernel support vector machine (MKSVM) classification algorithm in our article. By applying optimal two fish encryption algorithm, and second the segmented sensitive data are encrypted. Then the encrypted data are saved in the cloud service provider. After the encryption process, the user must be chosen an optimal key. The optimal key will be inquired by the cloud server. For that, we are also suggested an optimization algorithm called as Modified whale optimization algorithm (MWO). After all the verification process only the user found the data from the cloud server. This will increase the security of data cloud computing environment. The performance of our suggested method is computed in terms of accuracy, time and memory utilization. The introduced method is implemented in Cloud sim with JAVA. Our techniques and algorithms are outperforms equated with existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02205-6		JUN 2020											
J								Boosting binary masks for multi-domain learning through affine transformations	MACHINE VISION AND APPLICATIONS										Multi-domain learning; Multi-task learning; Quantized neural networks	ALGORITHMS	In this work, we present a new, algorithm for multi-domain learning. Given a pretrained architecture and a set of visual domains received sequentially, the goal of multi-domain learning is to produce a single model performing a task in all the domains together. Recent works showed how we can address this problem by masking the internal weights of a given original convnet through learned binary variables. In this work, we provide a general formulation of binary mask-based models for multi-domain learning by affine transformations of the original network parameters. Our formulation obtains significantly higher levels of adaptation to new domains, achieving performances comparable to domain-specific models while requiring slightly more than 1 bit per network parameter per additional domain. Experiments on two popular benchmarks showcase the power of our approach, achieving performances close to state-of-the-art methods on the Visual Decathlon Challenge.																	0932-8092	1432-1769				JUN 18	2020	31	6							42	10.1007/s00138-020-01090-5													
J								Bayesian combined neural network for traffic volume short-term forecasting at adjacent intersections	NEURAL COMPUTING & APPLICATIONS										Traffic volume; Intersection; Forecasting; Bayesian; Neural network; MSE; Regression	PREDICTION; MODELS	Undoubtedly, one of the greatest issues nowadays is congestion. To face such problem, forecasting of traffic is required. Bayesian combined neural network (BCNN) is applied to four different locations in Kuwait (Cairo Street, Riyadh Street, Maghreb Road and Istiqlal Road) to predict the short-term traffic volume at the middle section due to traffic flow from adjacent intersections. All data were collected for a period of 1 week over 15-min observation intervals using loop detectors. In addition to time-series responses and regression plots, mean square error (MSE) has been used to validate the network performance after data normalization. In comparison with MSE andRvalues, both values were slightly less precise during weekdays compared to weekends. After standardizing, the average MSE during weekdays was 0.003468 and regression (R) was 0.98113 for the four streets. For weekends model, the average MSE was 0.003563 and regression (R) was 0.97374 for the four streets. Istiqlal Street weekday model was the best model that fits the information among all the four models; as it has the smallest MSE value equivalent to 0.0010087 and the highestRvalue of 0.9959. BCNN model has achieved outstanding prediction performance with great potential to be generalized for various locations at different times of the day. These results can allow transportation planners to forecast traffic congestions and take prior measures to avoid them. Further modeling can assist in studying factors causing intersection congestions.																	0941-0643	1433-3058															10.1007/s00521-020-05115-y		JUN 2020											
J								Improve relation extraction with dual attention-guided graph convolutional networks	NEURAL COMPUTING & APPLICATIONS										Graph neural network; Relation extraction; Attention mechanism; Multihop relational reasoning		To better learn the dependency relationship between nodes, we address the relationship extraction task by capturing rich contextual dependencies based on the attention mechanism, and using distributional reinforcement learning to generate optimal relation information representation. This method is called Dual Attention Graph Convolutional Network (DAGCN), to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of GCN, which model the semantic interdependencies in spatial and relational dimensions, respectively. The position attention module selectively aggregates the feature at each position by a weighted sum of the features at all positions of nodes internal features. Meanwhile, the relation attention module selectively emphasizes interdependent node relations by integrating associated features among all nodes. We sum the outputs of the two attention modules and use reinforcement learning to predict the classification of nodes relationship to further improve feature representation which contributes to more precise extraction results. The results on the TACRED and SemEval datasets show that the model can obtain more useful information for relational extraction tasks, and achieve better performances on various evaluation indexes.																	0941-0643	1433-3058															10.1007/s00521-020-05087-z		JUN 2020											
J								Crop growth stage estimation prior to canopy closure using deep learning algorithms	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Transfer learning; Cereal growth stage; SVM classifier	MACHINE VISION; PRECISION AGRICULTURE; CLASSIFICATION; PARAMETERS; GUIDANCE; SYSTEMS	Growth stage determination plays an important role in yield prediction and cereal husbandry decision-making. Conventionally, crop growth stage determination is performed manually by means of visual inspection. This paper investigates wheat and barley growth stage estimation by classification of proximal images using convolutional neural networks (ConvNets). A dataset consisting of 138,000 images captured prior to the crop canopy closure stage was acquired from 4 sites (7 different fields) in Ireland. The dataset includes images of 12 growth stages of wheat and 11 growth stages of barley captured for a number of crop varieties, seed rates and brightness levels. A camera was held at 2 m from the ground and two camera poses were used-downward-looking and declined to 45 degrees below the horizon. Classification was carried out using three different machine learning approaches: (1) a 5-layer ConvNet model, including three convolutional layers, which was trained from scratch on our crop dataset; (2) transfer learning based on a VGG19 network pre-trained on ImageNet with an additional four fully connected layers, and (3) a support vector machine with conventional feature extraction. The classification accuracies of the aforementioned models were found to be (1) 91.1-94.2% for the ConvNet model, (2) 99.7-100% for the transfer learning model and (3) 63.6-65.1% for the SVM. For both crops, the best accuracy was obtained using the 45 degrees camera pose and the transfer learning ConvNet model. For the growth stage classification task, the transfer learning ConvNet has the advantage of significantly reduced training time when compared with the built-from-scratch ConvNet model.																	0941-0643	1433-3058															10.1007/s00521-020-05064-6		JUN 2020											
J								A novel multi-objective modified symbiotic organisms search algorithm for optimal allocation of distributed generation in radial distribution system	NEURAL COMPUTING & APPLICATIONS										Distributed generation; Multi-objective optimization; Non-dominated sorting; Symbiotic organisms search	PARTICLE SWARM OPTIMIZATION; LEARNING BASED OPTIMIZATION; OPTIMAL PLACEMENT; GENETIC ALGORITHM; BENEFIT MAXIMIZATION; OPTIMAL LOCATION; CAPACITOR BANKS; DG ALLOCATION; CHAOS; HYBRID	This article presents a novel optimization technique to allocate distributed generation (DG) units optimally in radial distribution system (RDS). Three renewable type DG units (such as wind turbine, solar photovoltaic and biomass system) have been integrated in the RDS. In this regard, an optimization problem is formulated considering multiple technical and economic objectives of the DG planning. A new metaheuristic, namely multi-objective modified symbiotic organisms search (MOMSOS), is proposed to solve this optimal DG allocation problem. A chaos-based cross-over operator is introduced in the parasitism phase of the proposed MOMSOS to enhance diversity in the population. The proposed MOMSOS is equipped with hierarchical non-dominated sorting technique which is superior to the existing fast non-dominated sorting strategy in terms of computational complexity. An adaptive penalty function is utilized for constraint handling. The proposed algorithm is tested on some CEC 2009 benchmark test problems to ensure its global optimization capability. Furthermore, performance of the proposed MOMSOS is validated on 69-node RDS and the obtained results are compared with other well-established multi-objective optimization algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-05080-6		JUN 2020											
J								Algorithms for solving assembly sequence planning problems	NEURAL COMPUTING & APPLICATIONS										Assembly sequence planning; Particle swarm optimization algorithm; Various population strategy; Hybrid algorithm	GENETIC ALGORITHM; OPTIMIZATION	Assembly sequence planning is one of the key issues in DFA and computer-aided assembly process planning research for concurrent engineering. The purpose of this paper is to solve the problem of insufficient individual intelligence in evolutionary algorithms for assembly sequence planning, and a evolutionary algorithm for assembly sequence planning is designed. In this paper, the particle swarm optimization (PSO) algorithm is used to optimize the hybrid assembly sequence planning and assembly line balance problems. According to the assembly sequence problem, the number of assembly tool changes and the number of assembly orientation changes are transformed into the operation time of the assembly line. At the same time, the transportation of heavy parts in the assembly balance problem is considered. Then, by extracting the connection relationship and information of the parts, the disassembly method is used to inversely obtain the disassembly support matrix, and then, it is used to obtain the priority relationship diagram of the assembly operation tasks that indicate the order constraints of the job tasks on the assembly line. Aiming at the shortcoming that particle swarm optimization algorithm is easy to fall into local optimum, a various population strategy is adopted to shorten the evolution stagnation time, improve the evolution efficiency of particle swarm optimization algorithm, and enhance the optimization ability of the algorithm. Combined with the three evaluation indicators of assembly geometric feasibility, assembly process continuity, and assembly tool change times, a fitness function is constructed to achieve multi-objective optimization. Finally, experiments show that the multi-agent evolutionary algorithm is incorporated into the planning process to obtain an accurate solution through the various population strategy-particle swarm optimization algorithm, which proves the feasibility of the compound algorithm and has better performance in solving assembly sequence planning problems.																	0941-0643	1433-3058															10.1007/s00521-020-05048-6		JUN 2020											
J								Construction of the knowledge service model of a port supply chain enterprise in a big data environment	NEURAL COMPUTING & APPLICATIONS										Port supply chain; Knowledge service; Noumenon; Knowledge fusion	CHALLENGES	In the context of the rapid development of big data and artificial intelligence, knowledge service theory and big data technology are applied to build a smart port supply chain knowledge service model. This model provides a personalized, intelligent, and diversified knowledge-based service system platform solution to port supply chain enterprises, helping to realize port supply chain transformation and upgrading and intelligent integrated operations. This paper analyses and summarizes the research status on knowledge service demand and port supply chain knowledge service during the development and operation of the port supply chain and applies big data and artificial intelligence technologies such as knowledge matching, knowledge fusion, and natural language processing. A port supply chain knowledge service model including knowledge acquisition, knowledge organization and knowledge service modules is constructed. The ontology method is used to construct the ontology knowledge base of the port supply chain, and based on this, computational reasoning experiments are performed. The experiments show that ontology technology demonstrates effectiveness and superiority in constructing a knowledge service system model for the port supply chain in terms of knowledge representation and knowledge reasoning.																	0941-0643	1433-3058															10.1007/s00521-020-05044-w		JUN 2020											
J								From MCDA to Fuzzy MCDA: violation of basic axiom and how to fix it	NEURAL COMPUTING & APPLICATIONS										Fuzzy multi-criteria decision analysis; Ranking of fuzzy numbers; Overestimation; Transformation methods; Fuzzy TOPSIS; Fuzzy MAVT; Distinguishable fuzzy numbers	MULTICRITERIA DECISION-MAKING; REASONABLE PROPERTIES; RANKING; NUMBERS; TOPSIS	The use of fuzzy numbers (FNs) for managing uncertainty in multi-criteria decision analysis (MCDA) demands a thorough exploring multi-criteria decision problem under fuzzy environment. Fuzzy MCDA (FMCDA) model implies comparison, choice or ranking alternatives based on assessing corresponding functions with subsequent ranking of FNs. Despite the wide use of FMCDA in recent decades, the effect of the violation of axioms for fuzzy ranking methods on FMCDA models has not been explored yet. This paper aims at demonstrating the violation of the basic MCDA axiom, associated with ranking of dominating and dominated in Pareto alternatives, by fuzzy TOPSIS and fuzzy MAVT models as an example. The suggestion to implement FMCDA models in applications without violation of the basic axiom is elicited based on the use of distinguishable fuzzy numbers.																	0941-0643	1433-3058															10.1007/s00521-020-05053-9		JUN 2020											
J								Content-based image retrieval using feature weighting and C-means clustering in a multi-label classification framework	PATTERN ANALYSIS AND APPLICATIONS										CBIR; Multi-label; Feature weighting; C-means clustering; KNN classification	COLOR	In this paper, a novel learning algorithm based on feature weighting is proposed to improve the performance of image classification or retrieval systems in a multi-label framework. The goal is to exploit maximally the beneficial properties of each feature in the system. Since each feature can separate more effectively some of the image classes, it is hypothesized that the weights of various features at some states can be traded off against each other. The training phase of the suggested algorithm is performed in two stages: (1) The input images are clustered using a supervised C-means method iteratively; (2) image features are weighted using a local feature weighting method in each cluster. These weights are determined by considering the importance of each feature in minimizing the classification error on each cluster. In the testing phase, the cluster corresponding to the query is found first. Then, the most similar images are retrieved in the multi-label framework using the feature weights assigned to that cluster. Experimental results on three well-known, public and international image datasets demonstrate that our proposed method leads to significant performance gains over existing methods.																	1433-7541	1433-755X															10.1007/s10044-020-00887-4		JUN 2020											
J								Boundary-connection deletion strategy based method for community detection in complex networks	APPLIED INTELLIGENCE										Complex network; Community detection; Hierarchical community structure; Clustering; Boundary connection deletion strategy	CLUSTERING-ALGORITHM; OPTIMIZATION; MODULARITY	Community detection in complex networks is a difficult problem. Up to now, there is no very effective method to solve it. Recently, many community detection algorithms based on edge removal have been proposed. However, these edge removal methods often delete many key connections within communities and weaken (or destroy) the community structure of the network. This will make the network communities more difficult to be identified and reduce the accuracy and stability of the algorithm. This paper proposed a boundary connection deletion based community detection algorithm. Different from other algorithms, our algorithm focuses on identifying and removing the boundary connections between network modules. This can enhance the network community structure and get high quality network modules. With high performance, our algorithm can detect the optimal and hierarchical community structure in weighted networks simultaneously. In order to verify the effectiveness of our algorithm, the stability and robustness of our algorithm were firstly analyzed. Then a series of experiments had been done on the real-world and synthetic networks. The real-world networks include Zachary's karate club network, dolphin social network, American college foot-ball network, PolBooks network,Les Miserablescharacter network, and the coauthorship network of scientists; The synthetic networks include GN benchmark and LFR benckmark. Two indices NMI andModularityQwere used to compare our algorithm with the recently proposed algorithms, including meta-LPAm+, Srinivas and Rajendran's model, IDPM, CFCDs, EDCD, CNM, and CDASS. Experimental results show that our algorithm has better performance than these algorithms.																	0924-669X	1573-7497				NOV	2020	50	11					3570	3589		10.1007/s10489-020-01762-9		JUN 2020											
J								Reinforcement learning algorithm for non-stationary environments	APPLIED INTELLIGENCE										Markov decision processes; Reinforcement learning; Non-Stationary environments; Change detection	CLASSIFIERS	Reinforcement learning (RL) methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traffic signal control, robotic applications, etc., one often encounters situations with non-stationary environments, and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward accrued when the underlying model of the environment changes over time. To achieve this, we first adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment effectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the effectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem, and a traffic signal control problem.																	0924-669X	1573-7497				NOV	2020	50	11					3590	3606		10.1007/s10489-020-01758-5		JUN 2020											
J								Observability index optimization of robot calibration based on multiple identification spaces	AUTONOMOUS ROBOTS										Robot calibration; Observability index; Multiple identification spaces; Hybrid sensors; Uncertainty analysis	OPTIMAL MEASUREMENT CONFIGURATIONS; KINEMATIC CALIBRATION; MODEL; PARAMETERS; SELECTION; SENSORS; DESIGN; SYSTEM; SET	A calibration method is proposed for six-DoF serial robot based on multiple identification spaces consisting of two subspaces in which the orientations of joint 3 and poses of end-effector are measured simultaneously using hybrid sensors. The rotational geometric errors with higher sensitivities are identified in the first space while the rest are identified in the second. Compared with single identification space used in traditional methods, the number of geometric errors to be identified is reduced in each subspace. Thus the identification vectors corresponding to the geometric errors belonging to identification models can be better spaced. Simulation results show that the observability indices and identifiability are further improved by using the multiple identification spaces. Experimental results are also obtained from a six-DoF serial robot with laser tracker and IMUs to verify the identification accuracy improvement. Uncertainty analysis of each identification results is also provided.																	0929-5593	1573-7527				JUL	2020	44	6					1029	1046		10.1007/s10514-020-09920-1		JUN 2020											
J								A velocity control strategy for collision avoidance of autonomous agricultural vehicles	AUTONOMOUS ROBOTS										Collision avoidance; Velocity control; Vehicle safety; Agricultural vehicle	OBSTACLE DETECTION; NAVIGATION; ROBOTS	Collision avoidance ability is very important for autonomous agricultural vehicles, but the influence of different obstacles in agricultural environment is rarely taken into account. In this paper, a velocity control strategy for collision avoidance was proposed to adjust the velocity of autonomous agricultural vehicles according to the movement state and dangerous degree of the obstacles and the distance between the obstacles and the vehicles, thus to improve intelligence and safety of the vehicles. The control strategy involved two steps: collision prediction in dynamic environments with an improved obstacle space-time grid map, and velocity generator for collision avoidance with a cloud model. Simulations were conducted on the obstacle collision prediction and the designed cloud generator for velocity control respectively. Simulation results show that the proposed strategy can effectively predict collision with anti-disturbance ability for threat-free obstacles and rapid and accurate velocity output. And it realizes the real-time operation in dynamic environments with an average time of 0.2 s to predict collision. Additionally, field experiments including five trial schemes were performed to test the proposed velocity control strategy on an agricultural robot, where a haystack, a tractor and walking persons were regarded as static or dynamic obstacles. The results of the field experiments show that the proposed velocity control strategy has strong feasibility and effectiveness.																	0929-5593	1573-7527				JUL	2020	44	6					1047	1063		10.1007/s10514-020-09924-x		JUN 2020											
J								Limacon inspired artificial bee colony algorithm for numerical optimization	EVOLUTIONARY INTELLIGENCE										Artificial bee colony algorithm; Swarm intelligence; Limacon curve; Local search; Optimization		The artificial bee colony algorithm (ABCA) has established itself as a signature algorithm in the area of swarm intelligence based algorithms. The hybridization of the local search technique enhances the exploitation capability in the search process of the global optimization strategies. In this article, an effective local search technique that is designed by taking inspiration by Limacon curve, is incorporated in ABCA and the designed strategy is named Limacon inspired ABC (LABC) algorithm. The exploitation capability of the LABC strategy is tested over 18 complex benchmark optimization problems. The test results are compared with similar state-of-art algorithms and statistical analysis shows the LABC can be considered an effective variant of the ABC algorithms to solve the complex optimization problems.																	1864-5909	1864-5917															10.1007/s12065-020-00430-8		JUN 2020											
J								Model-based Persian calligraphy synthesis via learning to transfer templates to personal styles	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Persian Calligraphy Synthesis; Handwriting Synthesis; Generative Models; Neural Networks; Machine Learning		Current software tools for computer generation of Persian calligraphy can be mostly described as conventional fonts and typesetting software, which basically neglect the 'variations' of real calligraphy performed by hand, in terms of personalization to different calligraphers' styles, as well as their statistical characteristics. In this paper, we address the problem of natural-looking Persian calligraphy synthesis via a machine learning based approach, at the level of subwords. Given images of samples written by a calligrapher, we train a parametric model to imitate the style. The core idea is to make use of templates (fonts) as a source of background knowledge, and learn a probabilistic mapping from them to personal styles of calligraphers, which is posed as transformation of attributed graphs using neural networks with sliding windows. This can be understood as adding 'naturalness' to a Persian calligraphy font, in essence. We report both objective and subjective evaluations, including the model performance in writer (calligrapher) identification task and Visual Turing Test. The results of the latter suggest that humans are unable to distinguish the calligraphy synthesized by our approach from real calligraphy in many cases.																	1433-2833	1433-2825				SEP	2020	23	3					183	203		10.1007/s10032-020-00353-1		JUN 2020											
J								Regularising LSTM classifier by transfer learning for detecting misogynistic tweets with small training set	KNOWLEDGE AND INFORMATION SYSTEMS										Misogynistic tweet; Transfer learning; LSTM; Small dataset; Overfitting		Supervised machine learning methods depend highly on the quality of the training dataset and the underlying model. In particular, neural network models, that have shown great success in dealing with natural language problems, require a large dataset to learn a vast number of parameters. However, it is not always easy to build a large (labelled) dataset. For example, due to the complex nature of tweets and the manual labour involved, it is hard to create a large Twitter data set with the misogynistic label. In this paper, we propose to regularise a long short-term memory (LSTM) classifier using a pretrained LSTM-based language model (LM) to build an accurate classification model with a small training set. We explain transfer learning (TL) with a Bayesian interpretation and show that TL can be viewed as an uncertainty regularisation technique in Bayesian inference. We show that a LM pre-trained on a sequence of general to task-specific domain datasets can be used to regularise a LSTM classifier effectively when a small training dataset is available. Empirical analysis with two small Twitter datasets reveals that an LSTM model trained in this way can outperform the state-of-the-art classification models.																	0219-1377	0219-3116				OCT	2020	62	10					4029	4054		10.1007/s10115-020-01481-0		JUN 2020											
J								Li-Function Activated Zhang Neural Network for Online Solution of Time-Varying Linear Matrix Inequality	NEURAL PROCESSING LETTERS										Zhang neural network; Li activation function; Finite-time convergence; Theoretical results; Time-varying linear matrix inequality	SYLVESTER EQUATION; STABILITY ANALYSIS; CONVERGENCE; SYSTEMS	In the previous work, a typical recurrent neural network termed Zhang neural network (ZNN) has been developed for various time-varying problems solving. Based on the previous work, by exploiting a special activation function (i.e., Li activation function), the resultant ZNN model is presented and investigated in this paper for online solution of time-varying linear matrix inequality (TVLMI). For such a Li-function activated ZNN (LFAZNN) model, theoretical results are provided to show its excellent computational performance on solving the TVLMI. That is, the presented LFAZNN model has the property of finite-time convergence. Comparative simulation results with two illustrative examples further substantiate the efficacy of the presented LFAZNN model for TVLMI solving.																	1370-4621	1573-773X				AUG	2020	52	1			SI		713	726		10.1007/s11063-020-10291-y		JUN 2020											
J								A New Supervised Clustering Framework Using Multi Discriminative Parts and Expectation-Maximization Approach for a Fine-Grained Animal Breed Classification (SC-MPEM)	NEURAL PROCESSING LETTERS										Expectation-maximization; Fine-grained classification; Multi-part CNN; Snapshot serengeti; Supervised clustering		Fine-grained image classification is active research in the field of computer vision. Specifically, animal breed classification is an arduous task due to the challenges in camera traps images like occlusion, camouflage, poor illumination, pose variation, etc. In this paper, we propose a fine-grained animal breed classification model using supervised clustering based on Multi Part-Convolutional Neural Network (MP-CNN) and Expectation-Maximization (EM) clustering. The proposed model follows a straightforward pipeline that combines the deep feature extraction using the CNN pre-trained on ImageNet and classifies unsupervised data using EM clustering. Further, we also propose a multi discriminative part selection and detection for the precise classification of animal breeds without using bounding box and annotations on both training and testing phases. The model is tested on several benchmark datasets for animals, including the largest camera trap Snapshot Serengeti dataset and has achieved a cumulative accuracy of 98.4%. The results from the proposed model strengthen the belief that supervised training of deep CNN on a large and versatile dataset, extracts better features than most of the traditional approaches, even for the unsupervised tasks.																	1370-4621	1573-773X				AUG	2020	52	1			SI		727	766		10.1007/s11063-020-10246-3		JUN 2020											
J								GEP-basedclassifiers with drift-detection	EXPERT SYSTEMS										classification algorithms; concept drift detection; ensemble classifiers; gene expression programming	CLASSIFICATION RULES; ALGORITHM	In the paper, we propose two gene expression programming (GEP)-based ensemble classifiers with different drift detection mechanisms. In the related work section, we briefly review GEP as a classification tool, incremental classifiers, and concept drift detectors. Next, the structure of our two-level GEP ensemble with metagenes is described. Further on, two integrated classifiers with drift detection algorithm and Wilcoxon rank sum test drift detector are proposed. The approach is validated in the computational experiment in which several real-life and artificial datasets with concept drift have been used. Experiment confirmed that the proposed approach can be competitive to existing solutions. In the conclusion section, we briefly outline directions for future research.																	0266-4720	1468-0394														e12571	10.1111/exsy.12571		JUN 2020											
J								An application of interval differential equation on a production inventory model with interval-valued demand via center-radius optimization technique and particle swarm optimization	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										AQPSO; GQPSO; interval differential equation; interval-valued demand; trade credit policy; WQPSO	PRICE-DEPENDENT DEMAND; DYNAMIC PARAMETER ADAPTATION; VARIABLE DEMAND; PERMISSIBLE DELAY; ORDERING POLICY; LEAD TIME; DESIGN; COST; ALGORITHM; SYSTEM	Due to the fluctuation of market economy and uncertainty of customers' demand, it is quite difficult to develop an appropriate inventory model under uncertain situations. To overcome this difficulty, a food production model with preservation technology and credit-linked demand under default risk of capital in uncertain environment is developed with the help of parametric approach and interval mathematics. In this proposed model, all the inventory parameters, including demand rate, production rate and deterioration rate are considered as interval-valued. Because of the consideration of demand rate, production rate as well as deterioration rate as interval-valued, to represent the proposed model mathematically, the interval differential equations have been used. Solving these differential equations by using parametric approach, all the cost components and the corresponding average profit are obtained in the form of intervals. Therefore, the optimization problem of this model becomes interval-optimization problem. Then, to solve the interval-optimization problem, the center-radius optimization technique is established with the help of interval order relations. With the help of this technique, the interval-optimization problem is converted into crisp problem and then it is solved numerically by using different variants (Gaussian Quantum-behaved Particle Swarm Optimization, Weighted Quantum-behaved Particle Swarm Optimization, and Adaptive Quantum-behaved Particle Swarm Optimization) of Quantum-behaved Particle Swarm Optimization technique. Some real-life problems are considered and solved to justify the validity of the proposed model. The same model is also analyzed in crisp environment to verify the result of interval environment. Finally, the sensitivity analyses of both crisp and interval environments are performed separately with respect to the different system parameters.																	0884-8173	1098-111X				AUG	2020	35	8					1280	1326		10.1002/int.22254		JUN 2020											
J								Traceability of soybeans produced in Argentina based on their trace element profiles	JOURNAL OF CHEMOMETRICS										class-modeling techniques; geographical origin; soybean grains	FOOD; IDENTIFICATION; VERIFICATION; RECOGNITION	Soybean (Glycine max(L.) Merril) is a popular foodstuff and crop plant, used in human and animal food. In this work, multielement analysis of soybean grains samples in combination with chemometric tools was used to classify the geographical origins. For this purpose, 120 samples from three provinces of Argentina were analyzed for a panel of 20 trace elements by inductively coupled plasma mass spectrometry. First, we used principal component analysis for exploratory analysis. Then, supervised classification techniques such as support vector machine (SMV) discriminant analysis (SVM-DA), random forest,k-nearest neighbors, and class-modeling techniques such as soft independent modeling of class analogy (SIMCA), potential functions, and one-class SVM were applied as tools to establish a model of origin of samples. The performance of the techniques was compared using global indexes. Among all the models tested, SVM and SIMCA showed the highest percentages in terms of prediction ability in cross-validation with average values of 99.3% for SVM-DA and a median value of balanced accuracy of 96.0%, 91.7%, and 88.3% for the three origins using SIMCA. Results suggested that the developed methodology by chemometric techniques is robust and reliable for the geographical classification of soybean samples from Argentina.																	0886-9383	1099-128X														e3252	10.1002/cem.3252		JUN 2020											
J								Multi-expert multi-criteria decision making based on the likelihoods of interval type-2 trapezoidal fuzzy preference relations	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-expert multi-criteria decision making; Preference relations; Interval type-2 trapezoidal fuzzy sets; Bonferroni aggregation operator	SUPPLIER SELECTION; TOPSIS METHODS; SETS; DEMATEL; AHP; MODEL	Interval type-2 trapezoidal fuzzy sets, as a particular form of interval type-2 fuzzy sets, can precisely characterize the subjective assessments and qualitative evaluations of a group of experts. In this paper, a novel likelihood-based interval type-2 trapezoidal fuzzy multi-expert multi-criteria decision-making approach is proposed. To do so, the concepts of likelihood-based performance index, likelihood-based comprehensive evaluation value, and signed distance-based evaluation value are adopted. The interval type-2 trapezoidal fuzzy Bonferroni aggregation operator is utilized to construct the likelihood-based interval type-2 trapezoidal fuzzy preference relations. Then, the consistent lower and upper likelihoods are adopted to enhance the efficiency of the group decision making framework. The proposed multi-expert decision making approach works well when there is high degree of fluctuations in the number of criteria and experts. The practicability and feasibility of the proposed approach are validated by applications to four cases. Several comparative analyses are conducted to authenticate the dominancy of the proposed method over conventional interval type-2 trapezoidal fuzzy multi-criteria decision-making approaches.																	1868-8071	1868-808X				DEC	2020	11	12					2719	2741		10.1007/s13042-020-01148-w		JUN 2020											
J								Weight convergence analysis of DV-hop localization algorithm with GA	SOFT COMPUTING										DV-hop; Convergence analyses; Genetic algorithm (GA); Positional precision; Mathematical weight model	WIRELESS SENSOR NETWORKS; RANGE-FREE LOCALIZATION; TECHNOLOGY	The distance vector-hop (DV-hop) is a typical localization algorithm. It estimates sensor nodes location through detecting the hop count between nodes. To enhance the positional precision, the weight is used to estimate position, and the conventional wisdom is that the more hop counts are, the smaller value of weight will be. However, there has been no clear mathematical model among positioning error, hop count, and weight. This paper constructs a mathematical model between the weights and hops and analyzes the convergence of this model. Finally, the genetic algorithm is used to solve this mathematical weighted DV-hop (MW-GADV-hop) positioning model, the simulation results illustrate that the model construction is logical, and the positioning error of the model converges to 1/4R.																	1432-7643	1433-7479															10.1007/s00500-020-05088-z		JUN 2020											
J								Model precision in partial least squares with discriminant analysis: A case study in document forgery through crossing lines	JOURNAL OF CHEMOMETRICS										blue pen inks; document forgery; iPhone; parameters of merit; printer	PRINCIPAL COMPONENT ANALYSIS; BLUE PEN INKS; NONDESTRUCTIVE IDENTIFICATION; PLS-DA; SPECTROSCOPY; SEQUENCE	Document frauds can occur by the misuse of blank documents by persons who are trusted by the signer or by the adulteration of an official document. The identification of forgery documents by crossing lines requires methods with better accuracy and precision in nondestructive ways. In this sense, this work presented a methodology by applying the partial least squares with discriminant analysis (PLS-DA) chemometric tool to digital images obtained from a smartphone for crossing lines analysis in two situations: documents that were printed and then signed (the correct mode-Situation 1) and documents that were signed and then printed (fraudulent documents-Situation 2), and by employing blue pen inks types ballpoint, rollerball, gel, and felt-tip. PLS-DA models presented a correct classification of all pen types in both situations and presenting sensitivity and specificity equal to 1. Robustness, evaluated by change the printer brand, model, and ink application mode, showed no influence for gel pens. This model was validated by precision estimation at levels of repeatability, intermediary, and reproducibility showing comparable results for the three levels considering the reproducibility with an iPhone 7. Precision at the reproducibility level with iPhone Xs presented the lower value and probably was more effective due to the camera system.																	0886-9383	1099-128X														e3265	10.1002/cem.3265		JUN 2020											
J								Gaze Control of a Robotic Head for Realistic Interaction With Humans	FRONTIERS IN NEUROROBOTICS										gaze control; gaze engagement; HRI; humanoid robot; robotic head; ROS; competitive network; computer vision	SYSTEM; LOCALIZATION; RECOGNITION; ATTENTION	When there is an interaction between a robot and a person, gaze control is very important for face-to-face communication. However, when a robot interacts with several people, neurorobotics plays an important role to determine the person to look at and those to pay attention to among the others. There are several factors which can influence the decision: who is speaking, who he/she is speaking to, where people are looking, if the user wants to attract attention, etc. This article presents a novel method to decide who to pay attention to when a robot interacts with several people. The proposed method is based on a competitive network that receives different stimuli (look, speak, pose, hoard conversation, habituation, etc.) that compete with each other to decide who to pay attention to. The dynamic nature of this neural network allows a smooth transition in the focus of attention to a significant change in stimuli. A conversation is created between different participants, replicating human behavior in the robot. The method deals with the problem of several interlocutors appearing and disappearing from the visual field of the robot. A robotic head has been designed and built and a virtual agent projected on the robot's face display has been integrated with the gaze control. Different experiments have been carried out with that robotic head integrated into a ROS architecture model. The work presents the analysis of the method, how the system has been integrated with the robotic head and the experiments and results obtained.																	1662-5218					JUN 17	2020	14								34	10.3389/fnbot.2020.00034													
J								Dual exploration strategies using artificial spiking neural networks in a robotic learning task	ADAPTIVE BEHAVIOR										Spiking neural networks; neurorobotic; exploration behaviors; spatial learning	TIMING-DEPENDENT PLASTICITY; OPEN-FIELD; ANIMAL PERSONALITY; THIGMOTAXIS; COGNITION; NEURONS; TEMPERAMENT; PERFORMANCE; BEHAVIOR; CIRCUIT	Spatial information can be valuable, but new environments may be perceived as risky and thus often evoke fear responses and risk-averse exploration strategies such as thigmotaxis or wall-following behavior. Individual differences in risk-taking (boldness) and thigmotaxis have been reported in natural taxa, which may benefit their survival. In neurorobotic, the common approach is to reproduce cognitive phenomena with multiple levels of bio-inspiration into robotic scenarios. Since autonomous robots may benefit from these different behaviors in exploration tasks, this study aims at simulating two exploration strategies in a virtual robot controlled by a spiking neural network. The experimental context consists in a visual learning task solved through an operant conditioning procedure. Results suggest that the proposed neural architecture sustains both behaviors, switching from one to the other by external cues. This original bio-inspired model could be used as a first step toward further investigations of neurorobotic personality modulated by learning and complex exploration contexts.																	1059-7123	1741-2633														1059712320924744	10.1177/1059712320924744		JUN 2020											
J								GrasNet: A Simple Grassmannian Network for Image Set Classification	NEURAL PROCESSING LETTERS										Grassmann manifold; Image set classification; Deep learning; PCA		Representing image sets on the Grassmann manifold has been widely used in visual classification tasks, and the existing Grassmannian learning methods have shown powerful ability in feature representation. In order to develop the ideology of conventional deep learning to the Grassmann manifold, we devise a simple Grassmann manifold feature learning network (GrasNet) in this paper, which provides a new way for image set classification. For the proposed GrasNet, we design a fully mapping layer to transform the input Grassmannian data into more appropriate representations. In view of the consistency of the data space, orthonormal maintaining layer is exploited to normalize the input matrices to form a valid Grassmann manifold. To perform Grassmannian computing on the resulting Grassmann manifold-valued features, we also introduce a projection mapping layer. For the sake of further reducing the dimensionality and redundancy of the learned geometric features, we devise a projection pooling layer. The log-map layer is finally adopted to embed the resulting data manifold into a tangent space via Riemannian matrix logarithm map, such that the Euclidean computations apply. To learn the multistage connection weights for the proposed GrasNet, we utilize the Principal Component Analysis (PCA) algorithm rather than the complex Riemannian matrix backpropagation optimizer, which makes it be built and trained extremely easy and efficient. We evaluate our model on three different visual classification tasks: face recognition, object categorization and cell identification, respectively. Extensive classification results verify its feasibility and effectiveness.																	1370-4621	1573-773X				AUG	2020	52	1			SI		693	711		10.1007/s11063-020-10276-x		JUN 2020											
J								Parameter-Free Extreme Learning Machine for Imbalanced Classification	NEURAL PROCESSING LETTERS										Parameter-free; Extreme learning machine; Class imbalance problem; G-mean		Imbalanced data distribution is a common problem in classification situations, that is the number of samples in different categories varies greatly, thus increasing the classification difficulty. Although many methods have been used for the imbalanced data classification, there are still problems with low classification accuracy in minority class and adding additional parameter settings. In order to increase minority classification accuracy in imbalanced problem, this paper proposes a parameter-free weighting learning mechanism based on extreme learning machine and sample loss values to balance the number of samples in each training step. The proposed method mainly includes two aspects: the sample weight learning process based on the sample losses; the sample selection process and weight update process according to the constraint function and iterations. Experimental results on twelve datasets from the KEEL repository show that the proposed method could achieve more balanced and accurate results than other compared methods in this work.																	1370-4621	1573-773X															10.1007/s11063-020-10282-z		JUN 2020											
J								Scalable attack on graph data by injecting vicious nodes	DATA MINING AND KNOWLEDGE DISCOVERY										Graph Convolution Networks; Vicious Nodes; Scalable Attack		Recent studies have shown that graph convolution networks (GCNs) are vulnerable to carefully designed attacks, which aim to cause misclassification of a specific node on the graph with unnoticeable perturbations. However, a vast majority of existing works cannot handle large-scale graphs because of their high time complexity. Additionally, existing works mainly focus on manipulating existing nodes on the graph, while in practice, attackers usually do not have the privilege to modify information of existing nodes. In this paper, we develop a more scalable framework named Approximate Fast Gradient Sign Method which considers a more practical attack scenario where adversaries can only inject new vicious nodes to the graph while having no control over the original graph. Methodologically, we provide an approximation strategy to linearize the model we attack and then derive an approximate closed-from solution with a lower time cost. To have a fair comparison with existing attack methods that manipulate the original graph, we adapt them to the new attack scenario by injecting vicious nodes. Empirical experimental results show that our proposed attack method can significantly reduce the classification accuracy of GCNs and is much faster than existing methods without jeopardizing the attack performance. We have open-sourced the code of our method https://github.com/wangjhgithub/AFGSM.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1363	1389		10.1007/s10618-020-00696-7		JUN 2020											
J								Energy and spectrum aware unequal clustering with deep learning based primary user classification in cognitive radio sensor networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Cognitive radio sensor network; Deep learning; Unequal clustering; Energy consumption; Channel assignment; Routing	POWER ALLOCATION; EFFICIENT; PROTOCOL; SCHEME	The problem of energy efficiency in cognitive radio sensor networks (CRSN) is mainly caused by the limited energy of sensor nodes and other channel-related operations for data transmission. The unequal clustering method should be considered for balancing the energy consumption among the cluster heads (CHs) for prolonging the network lifetime. The CH selection should consider the number of accessible free channels for efficient channel assignment. To improve fairness, the channel assignment problem should consider energy consumption among the cluster members. Furthermore, the relay metric for the selection of the best next-hop should consider the stability of the link for improving the transmission time. The CH rotation for cluster maintenance should be energy and spectrum aware. With regard to the above objectives, this paper proposes an energy and spectrum aware unequal clustering (ESAUC) protocol that jointly overcomes the limitations of energy and spectrum for maximizing the lifetime of CRSN. Our proposed ESAUC protocol improves fairness by achieving residual energy balance among the sensor nodes and enhances the network lifetime by reducing the overall energy consumption. Deep Belief Networks algorithm is exploited to predict the spectrum holes. ESAUC improves the stability of the cluster by optimally adjusting the number of common channels. ESAUC uses a CogAODV based routing mechanism to perform inter-cluster forwarding. Simulation results show that the proposed scheme outperforms the existing CRSN clustering algorithms in terms of residual energy, Network Lifetime, secondary user-primary user Interference Ratio, Route Discovery Frequency, throughput, Packet Delivery Ratio, and end-to-end delay.																	1868-8071	1868-808X															10.1007/s13042-020-01154-y		JUN 2020											
J								Deep learning-based hybrid dynamic biased track (DL-HDBT) routing for under water acoustic sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Routing; Deep learning; High dynamic biases; Throughput; Underwater acoustic sensor networks; Deep neural network; Tracking	UNDERWATER; LOCALIZATION	In underwater acoustic sensor networks (UASN), the main challenging issues are bandwidth, higher propagation delay, and heavy packet loss during data transmission. The issues can be solved through efficient routing algorithms. Due to the complexity and variability of the underwater acoustic environment, the underwater acoustic sensor network has the characteristics of fluidity, sparse deployment, and energy limitation, which brings certain challenges to underwater positioning technology. Aiming at the scenario that the node redundancy in the underwater acoustic sensor network leads to low positioning efficiency, this paper has proposed the deep learning-high dynamic biased track (DL-HDBT) algorithm. The DL-HDBT combines the deep learning and hybrid dynamic biased tracking algorithm. Deep learning (DL) helps in the identification of the best relay nodes in the network and traffic-congested nodes are tracked using a high dynamic bias track. The routing protocol has been implemented in the ns2-AqaSim simulator and testbed for measurement of the performance metrics of the UASN. The simulation results showed that the novel routing method throughput has increased by 17%, 35%, and 57% when compared with SUN, VBF and DF method. It can effectively improve the throughput of nodes, balance positioning performance as well as energy use efficiency, and optimize the positioning result of UWASN.																	1868-5137	1868-5145															10.1007/s12652-020-02165-x		JUN 2020											
J								Interval valued trapezoidal neutrosophic set: multi-attribute decision making for prioritization of non-functional requirements	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Non-functional requirements (NFRs); Multi criteria decision making (MCDM); Multi attribute decision making (MADM); Neutrosophic set; Interval valued fuzzy set (IVFS); Interval valued intuitionistic fuzzy set (IVIFS); Interval valued neutrosophic set; Trapezoidal neutrosophic set; Interval valued trapezoidal neutrosophic set (IVTrNS); Interval valued trapezoidal neutrosophic number (IVTrNN); Interval valued trapezoidal neutrosophic weighted arithmetic averaging operator (IVTrNWAA)	AGGREGATION OPERATORS; FUZZY-SETS; NUMBER	The paper discusses the trapezoidal fuzzy number (TrFN); interval-valued intuitionistic fuzzy number (IVIFN); neutrosophic set and its operational laws; and, trapezoidal neutrosophic set (TrNS) and its operational laws. Based on the combination of IVIFN and TrNS, an interval valued trapezoidal neutrosophic set (IVTrNS) is proposed followed by its operational laws. The paper also presents the score and accuracy functions for the proposed interval valued trapezoidal neutrosophic number (IVTrNN). Then, an interval valued trapezoidal neutrosophic weighted arithmetic averaging (IVTrNWAA) operator is introduced to combine the trapezoidal information which is neutrosophic and in the unit interval of real numbers. Finally, a method is developed to handle the problems in the multi attribute decision making (MADM) environment using IVTrNWAA operator followed by a numerical example of NFRs prioritization to illustrate the relevance of the developed method.																	1868-5137	1868-5145															10.1007/s12652-020-02130-8		JUN 2020											
J								Multi linear adaptive sequence transmission based code division multiple access using multicast neighbor data communication in wireless network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CDMA; Energy aware routing; Adaptive topology; Multicasting; Wireless network	SENSOR; POWER	Data transmission over wireless networks become increasing the energy levels which has the attention of both the routing data and the better communication in network. Due to lacking of improper network signals and the effluence it causes to the energy consumption in wireless network. With the progressive advancement in CDMA based radio communication channels are used to make better communication. This allow the multicasting techniques covers through transmitters to send he data in a single communication channel. To propose a Multi linear adaptive sequence transmission (MAST-CDMA) used to sense the channels for sequence linearity to transfer the data to make decisions without traffic constraints on periodic times. The routing impacts of packets over the network transmitters are based on the Multicast neighbor data communication (MNDC).The adaptive technique reduce the energy consumption over delay occurred on bandwidth consideration through transmitter in single channel communication. The newly communication intent are neighbor routing which is examined by traffic flow analyze the transmission consumption to improve the energy level constraints to produce improved network efficiency.																	1868-5137	1868-5145															10.1007/s12652-020-02207-4		JUN 2020											
J								A two-step combined algorithm based on NARX neural network and the subsequent prediction of the residues improves prediction accuracy of the greenhouse gases concentrations	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; NARX; Residues; Greenhouse gases	TIME-SERIES PREDICTION; MODEL; PERFORMANCE; EMISSIONS; INDEX	This paper compares the most applicable models for the forecasting of time series. Models based on artificial neural networks nonlinear autoregressive neural network with external input (NARX), Elman's neural network, and vector autoregression model were implemented. A special algorithm based on the analysis and prediction of the model residuals, which significantly improves the accuracy of the forecast, was proposed. For the prediction, we used the data of the main greenhouse gases methane (CH4) and water vapor (H2O) concentrations measured in summer 2016 in the surface layer of the atmospheric air on the Arctic island Belyy, Russia. The time interval of 192 h was chosen. The time interval was characterized by the significant daily variations in the CH(4)concentration; the H2O concentrations did not have a pronounced trend. Values corresponding to the first 168 h of the interval were used for ANN training, and then, concentrations were predicted for the next 24 h. The accuracy of the prediction was determined by the set of errors and indices. The NARX model was more accurate than models based on Elman network and the VAR. The accuracy gain was from 16.5 to 40% for the models, which predicted the CH(4)concentration, and from 21 to 58% for the models, which predicted the H2O concentration. The application of the proposed combined approach made it possible to increase the accuracy of the base model from 1.5 to 20% for the CH(4)and from 4 to 20% for the H2O (depending on the corresponding errors and indices). The presented Taylor diagram was also showed the advantage of the proposed approach.																	0941-0643	1433-3058															10.1007/s00521-020-04995-4		JUN 2020											
J								A study of similarity measures through the paradigm of measurement theory: the fuzzy case	SOFT COMPUTING										Fuzzy similarity measures; Comparative similarity; Boundary axioms; Monotonicity axioms; Independence axioms; Representability by fuzzy similarity measures	DISSIMILARITY MEASURES; SETS; FAMILY	We extend to fuzzy similarity measures the study made for classical ones in a companion paper (Coletti and Bouchon-Meunier in Soft Comput 23:6827-6845, 2019). Using a classic method of measurement theory introduced by Tversky, we establish necessary and sufficient conditions for the existence of a particular class of fuzzy similarity measures, representing a binary relation among pairs of objects which expresses the idea of "no more similar than". In this fuzzy context, the axioms are strictly dependent on the combination operators chosen to compute the union and the intersection.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11223	11250		10.1007/s00500-020-05054-9		JUN 2020											
J								ICM-BTD: improved classification model for brain tumor diagnosis using discrete wavelet transform-based feature extraction and SVM classifier	SOFT COMPUTING										Brain tumor diagnosis; MRI brain images; Discrete wavelet transform; Support vector machine; Classification; Segmentation	SEGMENTATION; TEXTURE	In medical image processing, the detection, classification and segmentation of the tumor region from MRI scans accurately are very complicated, significant and time-consuming process. When there is a scenario occurs to handle with large amount of images for tumor diagnosis, there is need of an efficient and adaptive classification model to handle with the anomalous structures of human brains. The MRI brain images show the typical internal brain structure and hence help scholars and medical practitioners in accurate disease diagnosis. With that note, this paper develops a model called improved classification model for brain tumor diagnosis for appropriate classification of tumor images from input MRI images. Initially, filtering techniques are applied for preprocessing the acquired scan images and feature extraction is done with gray-level co-occurrence matrix and discrete wavelet transform equations, which produces more precise results. And, classification is done with the technique called support vector machine, in which the binary classifications are effectively done. The proposed model is evaluated under simulation, and the obtained results outperform the results of traditional brain tumor detection process based on precision, recall and processing time.																	1432-7643	1433-7479															10.1007/s00500-020-05096-z		JUN 2020											
J								The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation	AI & SOCIETY										Artificial intelligence; China; Cyber warfare; Digital ethics; Economic growth; Governance; Innovation; International competition; New Generation Artificial Intelligence Development Plan; Policy; Privacy; Social governance	HEALTH-CARE; GOVERNANCE; LANDSCAPE; RULE; LAW	In July 2017, China's State Council released the country's strategy for developing artificial intelligence (AI), entitled 'New Generation Artificial Intelligence Development Plan' ((sic)). This strategy outlined China's aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China's AI policies or have assessed the country's technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China's AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China's AI policy by bringing together debates and analyses of a wide array of policy documents.																	0951-5666	1435-5655															10.1007/s00146-020-00992-2		JUN 2020											
J								From demonstrations to task-space specifications. Using causal analysis to extract rule parameterization from demonstrations	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Human-robot interaction; Robot learning; Explainability		Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work, we show that it is possible to learn generative models for distinct user behavioural types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use these models to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable models. An advantage of structuring generative models in this way is that we can extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We further parameterize these specifications through constraint optimization in order to find a safety envelope under which motion planning can be performed. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8-99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.																	1387-2532	1573-7454				JUN 17	2020	34	2							45	10.1007/s10458-020-09471-w													
J								Why Should We Add Early Exits to Neural Networks?	COGNITIVE COMPUTATION										Deep learning; Conditional computation; Early exit; Fog computing; Distributed optimization	OPTIMIZATION; INTELLIGENCE	Deep neural networks are generally designed as a stack of differentiable layers, in which a prediction is obtained only after running the full stack. Recently, some contributions have proposed techniques to endow the networks withearly exits, allowing to obtain predictions at intermediate points of the stack. These multi-output networks have a number of advantages, including (i) significant reductions of the inference time, (ii) reduced tendency to overfitting and vanishing gradients, and (iii) capability of being distributed over multi-tier computation platforms. In addition, they connect to the wider themes of biological plausibility and layered cognitive reasoning. In this paper, we provide a comprehensive introduction to this family of neural networks, by describing in a unified fashion the way these architectures can be designed, trained, and actually deployed in time-constrained scenarios. We also describe in-depth their application scenarios in 5G and Fog computing environments, as long as some of the open research questions connected to them.																	1866-9956	1866-9964				SEP	2020	12	5					954	966		10.1007/s12559-020-09734-4		JUN 2020											
J								Efficient signal selection using supervised learning model for enhanced state restoration	COMPUTATIONAL INTELLIGENCE										combinational gates; controllability; observability; pre-silicon verification; state restoration and trace buffer	VISIBILITY ENHANCEMENT; DATA-ACQUISITION; SILICON	The post-silicon validation and debug is the most important task in the contemporary integrated circuit design methodology. The vital problem prevailing in this system is that it has limited observability and controllability due to the minimum number of storage space in the trace buffer. This tends to select the signals prudently in order to maximize state reconstruction. In the reported works, to select and to restore the signals efficiently it is categorized into two types like low simulation with high-quality technique and high simulation with low-quality technique. In this work, a node-based combinational gate signal selection algorithm is proposed based on machine learning method that maximizes the state restoration capability. A significant improvement (80%) has made to achieve adequate simulation time with the high-quality associated with the state-of-the-art of supplementary methods.																	0824-7935	1467-8640															10.1111/coin.12344		JUN 2020											
J								The hub location's method for solving optimal control problems	EVOLUTIONARY INTELLIGENCE										Nonlinear optimal control; Uncapacitated single allocation p-hub median problem; Metaheuristic algorithm	VARIABLE NEIGHBORHOOD SEARCH; DYNAMIC OPTIMIZATION PROBLEMS; GENETIC ALGORITHM; DESIGN	In this paper, the p-hub median's algorithm is modified to find an approximate solution of bounded continuous-time nonlinear optimal control problems (NOCP). For this purpose, after presenting the similarities between uncapacitated single allocation p-Hub median problem (USApHMP) which is a special case of the well-studied hub location problem and a discretized form of NOCP, an improvement on sequential general variable neighborhood search algorithm (SGVNS) is proposed. This algorithm that denoted as new-sequential general variable neighborhood search (N-SGVNS) is based on three local searches that use efficient neighborhood interchange. In addition, comparing the NOCP with the USApHMP has led us to introduce a suitable method to find an approximate solution for NOCP. Our results of the implementation of the N-SGVNS on the real-world data set for the USApHMP show the high accuracy of the obtained solutions. Because of the proposed algorithm is able to solve the USApHMP successfully, we hoped that this method would also be successful for NOCP. Solving of benchmark NOCP and then comparing the results of the N-SGVNS with recently developed algorithms suggest that the presented strategy may provide an outstanding framework for designing cost-effective control functions.																	1864-5909	1864-5917															10.1007/s12065-020-00437-1		JUN 2020											
J								Formalizing Bachmair and Ganzinger's Ordered Resolution Prover	JOURNAL OF AUTOMATED REASONING										Resolution calculus; Automatic theorem provers; Proof assistants		We present an Isabelle/HOL formalization of the first half of Bachmair and Ganzinger's chapter on resolution theorem proving, culminating with a refutationally complete first-order prover based on ordered resolution with literal selection. We developed general infrastructure and methodology that can form the basis of completeness proofs for related calculi, including superposition. Our work clarifies fine points in the chapter, emphasizing the value of formal proofs in the field of automated reasoning.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1169	1195		10.1007/s10817-020-09561-0		JUN 2020											
J								Compliant Manipulation Method for a Nursing Robot Based on Physical Structure of Human Limb	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Nursing robot; Compliant manipulation; Optimal manipulative force; Human limb structure	EXOSKELETON; FRICTION	Nursing robots can be a good substitute to do nursing works for severely disabled patients who have completely lost the ability of movement. In order to improve the safety and comfort of human-robot interaction, we propose a novel compliant manipulation method for a nursing robot based on physical structure of human limb, which is applicable to severely disabled patients and can reduce the high performance requirements of sensors in existing human-robot interaction systems. The method aims to obtain the optimal manipulative force in static and dynamic manipulations. In the aspect of static calculation, static equilibrium conditions are used to obtain optimal static manipulative force. And in the aspect of dynamic calculation, lagrangian method is used to obtain optimal dynamic manipulative force based on predefined movement trajectory. Adams simulations and experiments are executed by manipulating limb model to conduct rehabilitation movements. Experimental results show that the proposed method can provide compliant manipulation method for a nursing robot.																	0921-0296	1573-0409															10.1007/s10846-020-01221-0		JUN 2020											
J								WatchNet plus plus : efficient and accurate depth-based network for detecting people attacks and intrusion	MACHINE VISION AND APPLICATIONS										Video surveillance; People detection; Convolutional network; Deep learning	HEAD-SHOULDER DETECTION	We present an efficient and accurate people detection approach based on deep learning to detect people attacks and intrusion in video surveillance scenarios Unlike other approaches using background segmentation and pre-processing techniques, which are not able to distinguish people from other elements in the scene, we propose WatchNet++ that is a depth-based and sequential network that localizes people in top-view depth images by predicting human body joints and pairwise connections (links) such as head and shoulders. WatchNet++ comprises a set of prediction stages and up-sampling operations that progressively refine the predictions of joints and links, leading to more accurate localization results. In order to train the network with varied and abundant data, we also present a large synthetic dataset of depth images with human models that is used to pre-train the network model. Subsequently, domain adaptation to real data is done via fine-tuning using a real dataset of depth images with people performing attacks and intrusion. An extensive evaluation of the proposed approach is conducted for the detection of attacks in airlocks and the counting of people in indoors and outdoors, showing high detection scores and efficiency. The network runs at 10 and 28 FPS using CPU and GPU, respectively.																	0932-8092	1432-1769				JUN 17	2020	31	6							41	10.1007/s00138-020-01089-y													
J								Cost-sensitive sample shifting in feature space	PATTERN ANALYSIS AND APPLICATIONS										Cost-sensitive learning; Resampling; Feature space; SVM	ENSEMBLE METHOD; CLASSIFICATION; REDUCTION	The asymmetry of different misclassification costs is a common problem in many realistic applications. As one of the most familiar preprocessing methods, cost-sensitive resampling has drawn great attention due to its easy-implemented and universal properties. However, current methods mainly concentrate on changing the amount of the training set, which will alter the original distribution shapes and lead to the classifiers be over-fitted or unstable. For this case, a new method named cost-sensitive kernel shifting is proposed. The training data are remapped from the input space to the feature space by a particular kernel function, in which a distance metric is defined. Then the outliers are eliminated and the informative samples, including border and edge samples are selected due to the neighbor and geometrical information in the mapped space. Thirdly the positions of all the selected samples in the feature space are shifted. A moving step length is defined in proportion to both the ratio and different of the misclassification costs. In all steps only the kernel matrix is needed to be reshaped due to the kernel trick. Experiments on both synthetic and public datasets verify the effectiveness of the proposed methods.																	1433-7541	1433-755X				NOV	2020	23	4					1689	1707		10.1007/s10044-020-00890-9		JUN 2020											
J								A linear space adjustment by mapping data into an intermediate space and keeping low level data structures	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Space Adjustment; optimisation; structure Maintenance; classification	DOMAIN ADAPTATION; CLASSIFICATION	One of the most important assumptions in machine learning tasks is the fact that training data points and test data points are extracted from the same distribution. However, this paper assumes the situation in which this fact does no longer hold. Therefore, a task named space adjustment, through which the distribution of the data points in the training-data space and the distribution of the data points in the test-data space become identical, is inevitable. Hereby, authors propose a linear mapping for the space adjustment task in the paper. It considers four approaches for preserving localities among data samples during the space adjustment. Each approach is defined based on a different locality concept. Considering all locality concepts in an objective function, authors transform the space adjustment into an optimisation problem. The paper proposes to optimise the corresponding objective function by an iterative approach. Empirical study shows that the proposed method outperforms the baseline methods. To do experiments, authors employ a large number of real-world datasets.																	0952-813X	1362-3079															10.1080/0952813X.2020.1764634		JUN 2020											
J								TEASER: early and accurate time series classification	DATA MINING AND KNOWLEDGE DISCOVERY										Time series; Early classification; Accurate; Framework	CONFIDENCE	Early time series classification (eTSC) is the problem of classifying a time series after as few measurements as possible with the highest possible accuracy. The most critical issue of any eTSC method is to decide when enough data of a time series has been seen to take a decision: Waiting for more data points usually makes the classification problem easier but delays the time in which a classification is made; in contrast, earlier classification has to cope with less input data, often leading to inferior accuracy. The state-of-the-art eTSC methods compute a fixed optimal decision time assuming that every times series has the same defined start time (like turning on a machine). However, in many real-life applications measurements start at arbitrary times (like measuring heartbeats of a patient), implying that the best time for taking a decision varies widely between time series. We present TEASER, a novel algorithm that models eTSC as a two-tier classification problem: In the first tier, a classifier periodically assesses the incoming time series to compute class probabilities. However, these class probabilities are only used as output label if a second-tier classifier decides that the predicted label is reliable enough, which can happen after a different number of measurements. In an evaluation using 45 benchmark datasets, TEASER is two to three times earlier at predictions than its competitors while reaching the same or an even higher classification accuracy. We further show TEASER's superior performance using real-life use cases, namely energy monitoring, and gait detection.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1336	1362		10.1007/s10618-020-00690-z		JUN 2020											
J								A group decision making with probability linguistic preference relations based on nonlinear optimization model and fuzzy cooperative games	FUZZY OPTIMIZATION AND DECISION MAKING										PLPR; PLTS; PLE; Consistency; Cooperative game	CONSISTENCY MEASURES; TERM SETS	The aim of the paper is to solve the group decision making problems which contain inconsistent probabilistic linguistic preference relations (PLPRs) and unknown expert weights. When the PLPRs are inconsistent, there are contradictories in the preference relations expressed by the experts. The evaluation value with contradictory information will bring out an incorrect consequence in decision making. Hence, this paper develops a novel consistency measure method to gauge the consistency level of PLPRs. Moreover, a nonlinear optimization model is newly constructed to optimize the inconsistent PLPRs. The proposed methods overcome the limitations in the existing methods and ameliorate the interpretation and complexities of inconsistency PLPRs revise strategies. Additionally, a weighting method using fuzzy cooperative games with PLPRs is put forward to derive the weight vector of experts. It helps to balance the deviations between the individual PLPRs and the group PLRP. At last, a numerical example illustration for physician selection is put forward to demonstrate the effectiveness of the proposed model and its practical applicability. The comparative analysis gives deep insights into the rationality of the proposed model.																	1568-4539	1573-2908															10.1007/s10700-020-09329-6		JUN 2020											
J								Optimization of Grasping Efficiency of a Robot Used for Sorting Construction and Demolition Waste	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Construction and demolition waste; sorting robot; grasping strategy; mathematical models; robot kinematics parameters	CLASSIFICATION	The recycling of construction and demolition waste (CDW) remains an urgent problem to be solved. In the industry, raw CDW needs to be manually sorted. To achieve high efficiency and avoid the risks of manual sorting, a sorting robot can be designed to grasp and sort CDW on a conveyor belt. But dynamic grasping on the conveyor belt is a challenge. We collected location information with a three-dimensional camera and then evaluated the method of dynamic robotic grasping. This paper discusses the grasping strategy of rough processed CDW on the conveyor belt, and implements the function of grasping and sorting on the recycling line. Furthermore, two new mathematical models for a robotic locating system are established, the accuracy of the model is tested with Matlab, and the selected model is applied to actual working conditions to verify the sorting accuracy. Finally, the robot kinematics parameters are optimized to improve the sorting efficiency through experiments in a real system, and it was concluded that when the conveyor speed was kept at around 0.25 m center dot s(-1), better sorting results could be achieved. Increasing the speed and shortening the acceleration/deceleration time would reach the maximum efficiency when the load would allow it. Currently, the sorting efficiency reached approximately 2000 pieces per hour, showing a high accuracy.																	1476-8186	1751-8520				OCT	2020	17	5					691	700		10.1007/s11633-020-1237-0		JUN 2020											
J								Two Polynomial Time Graph Labeling Algorithms Optimizing Max-Norm-Based Objective Functions	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Energy minimization; Pixel labeling; Minimum cut; NP-hard	SEGMENTATION	Many problems in applied computer science can be expressed in a graph setting and solved by finding an appropriate vertex labeling of the associated graph. It is also common to identify the term "appropriate labeling" with a labeling that optimizes some application-motivated objective function. The goal of this work is to present two algorithms that, for the objective functions in a general format motivated by image processing tasks, find such optimal labelings. Specifically, we consider a problem of finding an optimal binary labeling for the objective function defined as the max-norm over a set oflocal costsof a form that naturally appears in image processing. It is well known that for a limited subclass of such problems, globally optimal solutions can be found viawatershed cuts, that is, by the cuts associated with the optimal spanning forests of a graph. Here, we propose two new algorithms for optimizing a broader class of such problems. The first algorithm, that works for all considered objective functions, returns a globally optimal labeling in quadratic time with respect to the size of the graph (i.e., the number of its vertices and edges) or, for an image associated graph, the size of the image. The second algorithm is more efficient, with quasi-linear time complexity, and returns a globally optimal labeling provided that the objective function satisfies certain given conditions. These conditions are analogous to the submodularity conditions encountered in max-flow/min-cut optimization, where the objective function is defined as sum of all local costs. We will also consider a refinement of the max-norm measure, defined in terms of the lexicographical order, and examine the algorithms that could find minimal labelings with respect to this refined measure.																	0924-9907	1573-7683				JUN	2020	62	5			SI		737	750		10.1007/s10851-020-00963-8		JUN 2020											
J								Complexity of planning for connected agents	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Artifical intelligence; Multi-agent systems; Planning; Computational complexity	MULTIROBOT EXPLORATION; COVERAGE; INTRACTABILITY	We study a variant of the multi-agent path finding (MAPF) problem in which the group of agents are required to stay connected with a supervising base station throughout the execution. In addition, we consider the problem of covering an area with the same connectivity constraint. We show that both problems are PSPACE-complete on directed and undirected topological graphs while checking the existence of a bounded plan is NP-complete when the bound is given in unary (and PSPACE-hard when the encoding is in binary). Moreover, we identify a realistic class of topological graphs on which the decision problem falls in NLOGSPACE although the bounded versions remain NP-complete for unary encoding.																	1387-2532	1573-7454				JUN 16	2020	34	2							44	10.1007/s10458-020-09468-5													
J								Runtime revision of sanctions in normative multi-agent systems	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Multiagent systems; Norm revision; Norm enforcement		To achieve system-level properties of a multiagent system, the behavior of individual agents should be controlled and coordinated. One way to control agents without limiting their autonomy is to enforce norms by means of sanctions. The dynamicity and unpredictability of the agents' interactions in uncertain environments, however, make it hard for designers to specify norms that will guarantee the achievement of the system-level objectives in every operating context. In this paper, we propose a runtime mechanism for the automated revision of norms by altering their sanctions. We use a Bayesian Network to learn, from system execution data, the relationship between the obedience/violation of the norms and the achievement of the system-level objectives. By combining the knowledge acquired at runtime with an estimation of the preferences of rational agents, we devise heuristic strategies that automatically revise the sanctions of the enforced norms. We evaluate our heuristics using a traffic simulator and we show that our mechanism is able to quickly identify optimal revisions of the initially enforced norms.																	1387-2532	1573-7454				JUN 16	2020	34	2							43	10.1007/s10458-020-09465-8													
J								Comparison of novelty detection methods for multispectral images in rover-based planetary exploration missions	DATA MINING AND KNOWLEDGE DISCOVERY										Novelty detection; Unsupervised learning; Space exploration	OUTLIER DETECTION; ANOMALY DETECTION; RX-ALGORITHM; CLASSIFICATION; ERROR; CALIBRATION; FRAMEWORK; LEARN	Science teams for rover-based planetary exploration missions like the Mars Science Laboratory Curiosity rover have limited time for analyzing new data before making decisions about follow-up observations. There is a need for systems that can rapidly and intelligently extract information from planetary instrument datasets and focus attention on the most promising or novel observations. Several novelty detection methods have been explored in prior work for three-channel color images and non-image datasets, but few have considered multispectral or hyperspectral image datasets for the purpose of scientific discovery. We compared the performance of four novelty detection methods-Reed Xiaoli (RX) detectors, principal component analysis (PCA), autoencoders, and generative adversarial networks (GANs)-and the ability of each method to provide explanatory visualizations to help scientists understand and trust predictions made by the system. We show that pixel-wise RX and autoencoders trained with structural similarity (SSIM) loss can detect morphological novelties that are not detected by PCA, GANs, and mean squared error autoencoders, but that the latter methods are better suited for detecting spectral novelties-i.e., the best method for a given setting depends on the type of novelties that are sought. Additionally, we find that autoencoders provide the most useful explanatory visualizations for enabling users to understand and trust model detections, and that existing GAN approaches to novelty detection may be limited in this respect.																	1384-5810	1573-756X				NOV	2020	34	6					1642	1675		10.1007/s10618-020-00697-6		JUN 2020											
J								A better exploration strategy in Grey Wolf Optimizer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Swarm intelligence; Grey wolf optimizer; Explorative equation; Opposition-based learning (OBL); Exploration and exploitation	POWER DISPATCH; ALGORITHM	The Grey Wolf Optimizer (GWO) is a recently developed population-based meta-heuristics algorithm that mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Although, GWO has shown very good results on several real-life applications but still it suffers from some issues like, the low exploration and slow convergence rate. Therefore in this paper, an improved grey wolf optimizer is proposed to modify the exploration as well as exploitation abilities of the classical GWO. This improvement is performed by using the explorative equation and opposition-based learning (OBL). The validation of the proposed modification is done on a set of 23 standard benchmark test problems using statistical, diversity and convergence analysis. The experimental results on test problems confirm that the efficiency of the proposed algorithm is better than other considered metaheuristic algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02153-1		JUN 2020											
J								Support vector machine and simple recurrent network based automatic sleep stage classification of fuzzy kernel	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy kernel SVM; Simple recurrent network; EEG; Classification; Sleep stage	NEURAL-NETWORKS; APPROXIMATION	Recently, sleep disorder is taken as a serious issue in people living. Normally people cerebrum passes through variety of static physiological steps or changes for the duration of sleep. Biomedical signal such as EEG, ECG, EOG and EMG setup and signals used to recognize sleep disorders. This work proposes better technique that can be designed to discriminate the stages of sleep which can help physicians to do an analysis and examination of related sleep disorders. In order to identify a modification inside brain, EEG signal partitioned with 5 frequency bands: delta, theta, alpha, beta and gamma. After signal acquisition, Band pass filter is applied to discriminate the input EEG signal of F-pz-C(z)electrodes into frequency bands. Statistical specific features are extracted from distinctiveness impression of EEG signal. Then classification is required for classifying the sleep stages automatically with fuzzy kernel support vector machine and simple recurrent network (SRN). In SRN, statistical features were extracted and allocate 30 s period to 5 possible levels in sleep; wakefulness, Non Rapid Eye Movement Sleep Stage 1 (NREMSS 1), NREMSS 2, NREMSS 3 and NREMSS 4, Rapid Eye Movement Sleep Stage (REMSS). These signal acquired from sleep-EDF repository from PhysioBank (PB) used to validate our proposed scheme. Simple recurrent network classification performance rate is found as 90.2% than that of other new classifiers such as feed forward neural network (FNN) and probabilistic neural network (PNN) next it was compared and results are experimented in proposed work.																	1868-5137	1868-5145															10.1007/s12652-020-02188-4		JUN 2020											
J								A modified inductor and control models of three phase Vienna rectifier topology using particle swarm optimization algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Switched inductor; PSO; Proportional gain; Integral gain; Power factor; THD	PWM	A three phase Vienna rectifier topology is proposed with a modified inductor and control models. The presented switched inductor structure works during the active switch turn on and turned off condition. So, voltage gain of the converter will be improved and the stress across the active switch due to voltage will be reduced. Here, (PSO) Particle Swarm Optimization algorithm-based control model is utilized to optimize the gain parameters of bidirectional active switch with required switching frequency. By using the capacitor voltage and the switched inductor current for gain adjustment through the light load and full load condition. So, the proposed topology reduces the (THD) Total Harmonic Distortion present in the inductor current and improves the power factor at both the continuous and the discontinuous conduction mode. The proposed circuit is applied using MATLAB/simulink and analyzed by comparing with feed forward compensation method and implemented in hardware using Field Programmable Gate Array (FPGA).																	1868-5137	1868-5145															10.1007/s12652-020-02179-5		JUN 2020											
J								Modelling of F3I based feature selection approach for PCOS classification and prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PCOS; Endocrine disorder; ANN; Naive bayes; Follicles	POLYCYSTIC-OVARY-SYNDROME; MENOPAUSE; WOMEN; RISK	In medical field, PCOS is an endocrine based disorder which affects women with reproductive age. The symptoms associated with PCOS have been mainly encountered for the women with the age range of 25-35 years. The image representation based on the pixel has turns to be more essential for improving the performance of the computer system. Estimating the size, feature selection and classification, object recognition are certain critical crisis where image processing can be implemented. Initially, the noise associated with the image will be eliminated using effectual adaptive histogram equalization, features related to PCOS are considered and features are extracted. After noise removal, features have to be selected from those images before providing the images to classifier. To carry out this process, an approach termed as Furious flies has been proposed here. So as to address the drawbacks of conventional Firefly algorithm, this work proposes an effectual and a novel approach known as furious flies for feature identification which considers three diverse stages known as attraction based ROI selection, selection for follicle identification and follicle identification. Finally, classification can be performed using Naive Bayesian classifier and artificial neural networks. However, using this we can determine the positivity and negativity of the PCOS in earlier stage based on the measurement. Accuracy of the proposed model is 98.63%, precision and specificity is 100%, F-measure is 68.76%, Recall is 55% respectively. The outcomes of this technique are efficient in contrast to prevailing methods. The simulation was carried out using MATLAB environment.																	1868-5137	1868-5145															10.1007/s12652-020-02199-1		JUN 2020											
J								Internet of Things and artificial intelligence enable energy efficiency	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy management; Machine learning; Intelligent systems	CONSUMPTION; MANAGEMENT; SYSTEMS; HOME; NETWORKS	In smart environments, there is an increasing demand for scalable and autonomous management systems. In this regard, energy efficiency hands out challenging aspects, for both home and business usages. Scalability in energy management systems is particularly difficult in those industry sector where power consumption of branches located in remote areas need to be monitored. Being autonomous requires that behavioural rules are automatically extracted from consumption data and applied to the system. Best practices for the specific energy configuration should be devised to achieve optimal energy efficiency. Best practices should also be revised and applied without human intervention against topology changes. In this paper, the Internet of Things paradigm and machine learning techniques are exploited to (1) define a novel system architecture for centralised energy efficiency in distributed sub-networks of electric appliances, (2) extract behavioural rules, identify best practices and detect device types. A system architecture tailored for autonomous energy efficiency has interesting applications in smart industry-where energy managers may effortlessly monitor and optimally setup a large number of sparse divisions-and smart home-where impaired people may avoid energy waste through an autonomous system that can be employed by the users as a delegate for decision making.																	1868-5137	1868-5145															10.1007/s12652-020-02151-3		JUN 2020											
J								An improved artificial bee colony algorithm based on mean best-guided approach for continuous optimization problems and real brain MRI images segmentation	NEURAL COMPUTING & APPLICATIONS										Optimization algorithms; Artificial bee colony; Continuous optimization problems; Search behavior; MRI images	PERFORMANCE	The artificial bee colony (ABC) algorithm is a relatively new algorithm inspired by nature and has been shown to be efficient in contrast to other optimization algorithms. Nonetheless, ABC has some similar drawbacks to the optimization algorithms in terms of the unbalanced search behavior. The original ABC algorithm shows strong exploration capability with ineffective exploitation due to the unbalanced search model. In this paper, a new ABC algorithm called MeanABC is introduced to achieve the search behavior balance via a modified search equation based on the information of the mean of the previous best solutions. To evaluate the performance of the proposed algorithm, experiments were divided into two parts: First, the proposed algorithm was tested on a comprehensive set of 14 benchmark functions. The results show that the proposed MeanABC enhances the performance of the original ABC in terms of faster global convergence speed, solution quality, and better robustness when compared to other ABC variants. Secondly, the proposed algorithm was applied as a hybrid with the FCM algorithm as a segmentation technique to a set of 20 volumes of real brain MRI images with 20 images for each volume. All of these images have several characteristics, levels of difficulty, and cover different domains. The obtained results are promising, especially when the performance of the proposed algorithm was compared to other state-of-the-art segmentation techniques.																	0941-0643	1433-3058															10.1007/s00521-020-05118-9		JUN 2020											
J								Attention deep residual networks for MR image analysis	NEURAL COMPUTING & APPLICATIONS										Deep residual networks; MR image segmentation; Channel attention; Convergence		Prostate diseases often occur in men. For further clinical treatment and diagnosis, we need to do accurate segmentation on prostate. There are already many methods that concentrate on solving the problem of automatic prostate MR image segmentation. However, the design of some hyperparameters of these methods is migrated from the models that are used for nature images which do not consider the difference between medical image and nature image. Besides, there is trend that researchers are likely to use deeper and more complicated networks to achieve high accuracy. The improvement is limited with surging parameters, computations, training time, and inference time. In this paper, we propose an efficient attention residual U-Net to segment the prostate MR image. We analyze the property of prostate MR image and fine-tune the architecture of U-Net. To accelerate the convergence of our method, residual connection and channel attention are added to our network. A set of experiments suggest our method can achieve a similar accuracy of state of the art with less parameters, less computations, shorter training time, and shorter inference time.																	0941-0643	1433-3058															10.1007/s00521-020-05083-3		JUN 2020											
J								An effective method to solve the problem of electric distribution network reconfiguration considering distributed generations for energy loss reduction	NEURAL COMPUTING & APPLICATIONS										Network reconfiguration; Distributed generator; Energy loss; Time-varying load; Pathfinder algorithm	POWER LOSS MINIMIZATION; DISTRIBUTION-SYSTEMS; CUCKOO SEARCH; OPTIMIZATION; ALGORITHM; LOAD	This paper proposes an effective network reconfiguration (NR) method in the presence of distributed generations (DGs) for energy loss. The proposed method uses average load and average power of DGs instead of the load and DGs' generation curves. For finding the optimal network configuration, pathfinder algorithm (PFA) is used to solve the NR problem. The effectiveness of the proposed method has been validated on two distribution network systems without and with DGs placement. The obtained results show that the proposed method has a good ability to determine the optimal configuration similar to the method based on the graphs of loads and DGs with much shorter calculated time and PFA can reach optimal solution with a much higher success rate and better obtained solution compared with particle swarm optimization and sunflower optimization algorithms. As a result, the proposed method is an effective and reliable method for solving the NR problem for energy loss reduction considering time-varying condition of loads and DGs.																	0941-0643	1433-3058															10.1007/s00521-020-05092-2		JUN 2020											
J								SSNET: an improved deep hybrid network for hyperspectral image classification	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks (CNN); Hyperspectral image classification; 3D-CNN; 2D-CNN; Spatial pyramid pooling (SPP)	SPECTRAL-SPATIAL CLASSIFICATION; FRAMEWORK	Classification is one of the most important task in hyperspectral image processing. In the last few decades, several classification techniques have been introduced. However, most of them could not efficiently extract features from hyperspectral images (HSI). A novel deep learning framework is proposed in this paper which efficiently utilises convolutional neural network (CNN) and spatial pyramid pooling (SPP) for extracting both the spectral-spatial features for classification. The proposed hybrid framework uses principal component analysis (PCA), 3D-CNN, 2D-CNN and SPP. The proposed CNN-based model is applied on three benchmark hyperspectral datasets, and subsequently the performance is compared with state-of-the-art methods in the same field. The obtained results reveal the superiority of the proposed model in effectively classifying HSI.																	0941-0643	1433-3058															10.1007/s00521-020-05069-1		JUN 2020											
J								An improved bat optimization algorithm to solve the tasks scheduling problem in open shop	NEURAL COMPUTING & APPLICATIONS										Open shop; Scheduling; Makespan; Bat algorithm; Heuristic functions	HYBRID GENETIC ALGORITHM; ANT COLONY OPTIMIZATION; EVOLUTIONARY ALGORITHM; HEURISTICS; SEARCH	The open shop scheduling problem involves a set of activities that should be run on a limited set of machines. The purpose of scheduling open shops problem is to provide a timetable for implementation of the entire operation so that the total execution time is reduced. The tasks scheduling problem in open shops is important in many applications due to the arbitrariness of the processing sequence of each job and lack of a prioritization of its operations. This is an NP-hard problem and obtaining an optimal solution to this problem requires a high time complexity. Therefore, heuristic techniques are used to solve these problems. In this paper, we investigate the tasks scheduling problem in open shops using the Bat Algorithm (BA) based on ColReuse and substitution meta-heuristic functions. The heuristic functions are designed to increase the rate of convergence to the optimal solution. To evaluate the performance of the proposed algorithm, standard open shop benchmarks were used. The results obtained in each benchmark are compared with those of the previous methods. Finally, after analyzing the results, it was found that the proposed BA had a better performance and was able to generate the best solution in all cases.																	0941-0643	1433-3058															10.1007/s00521-020-05055-7		JUN 2020											
J								Social movie recommender system based on deep autoencoder network using Twitter data	NEURAL COMPUTING & APPLICATIONS										Social recommender system; Deep learning; Deep autoencoder network; Collaborative filtering; Content-based filtering; Social influence		Recommender systems attempt to provide effective suggestions to each user based on their interests and behaviors. These recommendations usually match the personal user preferences and assist them in the decision-making process. With the ever-expanding growth of information on the web, online education systems, e-commerce, and, eventually, the emergence of social networks, the necessity of developing such systems is unavoidable. Collaborative filtering and content-based filtering are among the most important techniques used in recommender systems. Meanwhile, with the significant advances in deep learning in recent years, the use of this technology has been widely observed in recommender systems. In this study, a hybrid social recommender system utilizing a deep autoencoder network is introduced. The proposed approach employs collaborative and content-based filtering, as well as users' social influence. The social influence of each user is calculated based on his/her social characteristics and behaviors on Twitter. For the evaluation purpose, the required datasets have been collected from MovieTweetings and Open Movie Database. The evaluation results show that the accuracy and effectiveness of the proposed approach have been improved compared to the other state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-05085-1		JUN 2020											
J								Intuitionistic 2-tuple linguistic aggregation information based on Einstein operations and their applications in group decision making	ARTIFICIAL INTELLIGENCE REVIEW										2-tuple linguistic sets; Intuitionistic 2-tuple linguistic sets; Einstein operations; Intuitionistic 2-tuple linguistic Einstein weighted averaging operator; Intuitionistic 2-tuple linguistic Einstein weighted geometric operator; Multi-criteria group decision making	TERM SETS; OPERATORS; MODEL; SELECTION	The linguistic information can be expressed as a 2-tuple of a linguistic variable and a real number in an interval [-1/2, 1/2). The intuitionistic 2-tuple linguistic (I2TL) set accurately deals with the imprecise and unpredictable information in those decision-making problems where experts prefer the degree of membership and non-membership values in the form of 2-tuple. The existing approaches used for the aggregation operations of I2TL sets are extremely complicated. This work aims to develop new aggregation operations for I2TL sets using Einstein operations. We present intuitionistic 2-tuple linguistic Einstein weighted averaging (I2TLEWA), and intuitionistic 2-tuple linguistic Einstein weighted geometric (I2TLEWG) operators. We also discuss their properties and relationship between them. Moreover, we numerically test the feasibility and significance of our proposed operators by solving a multi-criteria group decision making (MCGDM) problem. Finally, we do a comparative analysis with another method to give insights on our designed operators for I2TL sets.																	0269-2821	1573-7462				AUG	2020	53	6					4625	4650		10.1007/s10462-020-09856-z		JUN 2020											
J								A group-key-based sensitive attribute protection in cloud storage using modified random Fibonacci cryptography	COMPLEX & INTELLIGENT SYSTEMS										Attribute group; Modified random Fibonacci cryptography; Group key; Sensitive attribute	ACCESS-CONTROL; SECURE	Cloud computing is an eminent technology for providing a data storage facility with efficient storage, maintenance, management and remote backups. Hence, user data are shifted from customary storage to cloud storage. In this transfer, the sensitive attributes are also shifted to cloud storage with high-end security. Current security techniques are processed with high encryption time and provide identical security of entire data with single key dependent. These processes are taking high computational time and leaks entire information if the key is hacked. The proposed Group Key Based Attribute Encryption using Modified Random Fibonacci Cryptographic (MRFC) technique rectifies these issues. Instead of machine learning technique, data owner preference-based attributes segregation is used to divide an input dataset into sensitive and non-sensitive attribute groups. Based on inter-organization usage and data owner's willingness, sensitive attribute is divided into 'n + 1 ' subgroups and each subgroup is encrypted by 'n + 1' group keys. The encrypted sensitive subgroups are merged with non-sensitive attributes and uploaded into a private cloud. The novelties of this paper are, (1) data owner preferred sensitive attribute classification instead of machine learning algorithms, (2) sensitive attribute encryption instead of entire attributes, (3) To reduce encryption time without compromising data owner privacy, (4) To decrypt and access the required subgroup instead of the entire attribute. Our experimental results show that, the proposed method takes minimal processing time, better classification accuracy and minimal memory space with high security to selected attributes as compared to existing classification and security techniques. Hence, sensitive data security and privacy is achieved with minimal processing cost.																	2199-4536	2198-6053															10.1007/s40747-020-00162-3		JUN 2020											
J								A search space optimization method for fuzzy access control auditing	KNOWLEDGE AND INFORMATION SYSTEMS										Optimization; Access control; Fuzzy inference systems; Security and privacy protection	LOGIC	As data become an increasingly important asset for organizations, so does the access control policies that protect aforesaid data. Many subjects (public, researchers, etc.) are interested in accessing these data, leading to the desire for simple access control. However, some scenarios use vague concepts, such as the "researcher's expertise", when making access control decisions. Therefore, access control models based on fuzzy logic have been proposed to handle these scenarios. However, subject attributes can change between access requests and are processed in non-trivial ways by these models to reach a decision. This makes it difficult to audit the capabilities of subjects and their permissions over resources, and consequently, the number of application scenarios naturally suffers. Hence, the contribution of this paper lies in proposing an optimized auditing algorithm that allows fuzzy policies to be validated before being used. An assessment is also carried out to validate the method and its effectiveness.																	0219-1377	0219-3116				OCT	2020	62	10					3973	3994		10.1007/s10115-020-01480-1		JUN 2020											
J								Integrating researchers' scientific production information through Ogmios	KNOWLEDGE AND INFORMATION SYSTEMS										Research information system; Data extraction; Knowledge engineering; Author disambiguation; Research of research	DATA EXTRACTION; TOOL	Nowadays, many R&I institutions are presently implementing mechanisms to measure and rate their scientific production so as to comply with current legislation and to support research management and decision making. In many cases, they rely on the implementation of current research information systems (CRIS). This is a challenging task that often requires major human intervention and supervision to manually include all scientific production, projects, patents, etc., in the system. In this paper, we present Ogmios, a system that aims to reduce the time and effort of this process. Ogmios is a CRIS that automatically extracts and combines information from different sources, such as academic social networks or academic search engines. This redundancy helps to reduce potential errors. Additionally, Ogmios relies on other sources, such as online subscription-based scientific citation indexing services, to add metadata to information collected for ranking purposes. We have assessed the performance of this system with a sample of 216 researchers from the University of Malaga; 815 profiles were retrieved and validated with an accuracy of over 90% in profile detection. The main contribution of this work is Ogmios's autonomous capacity to retrieve and combine all necessary information on scientific profiles and production from different data sources and, also, its adaptability to any university or research institution.																	0219-1377	0219-3116				NOV	2020	62	11					4199	4222		10.1007/s10115-020-01479-8		JUN 2020											
J								Exploring the financial indicators to improve the pattern recognition of economic data based on machine learning	NEURAL COMPUTING & APPLICATIONS										Machine learning; Economic data; Data mining; Pattern recognition		Various economic data in the financial market need to be pattern-recognized to improve the efficiency of economic data pattern recognition, further improve the accuracy of economic-related decisions, and promote stable economic development. Based on machine learning technology, this study establishes a statistical model by establishing a multiple regression model to extract financial indicators that have significant effects on the financing trade of listed companies. Moreover, this study provides a preliminary empirical model for judging whether a company conducts financing trade based on some company's financial indicators and uses data to verify the consistency of the model. In addition, this study conducts research and demonstration of the algorithm model of this research through empirical research. The research results show that the model shows high reliability and validity in accurately identifying whether the enterprise has the characteristics of conducting financing trade.																	0941-0643	1433-3058															10.1007/s00521-020-05094-0		JUN 2020											
J								Segmentation of spinal cord from computed tomography images based on level set method with Gaussian kernel	SOFT COMPUTING										Spinal cord; Vertebra; Disc; Sagittal view and axial view		In the human body, organs segmentation is the most imperative issues in therapeutic applications. The challenges are connected with medicinal image segmentation and low complexity between required organ and incorporating tissues. There exist a wide range of methodologies for how a segmentation problem can be comprehended. These methods want to have a spot specific region of individual bones. The particular part remains a test for spinal cord segmentation. As a result of the beforehand expressed downsides of the current spinal cord segmentation procedures, this paper proposes a modified spatial fuzzyCclustering with level set segmentation method to incorporate Neumann Boundary Condition, a third function, called by the level set evolution. Neumann Boundary Condition is utilized to specify the normal derivative of the function present on any surface. The proposed method gives better results of segmentation of the spinal cord organs. The execution of the proposed method proves its superiority in term of accuracy as compared with the other methods.																	1432-7643	1433-7479															10.1007/s00500-020-05113-1		JUN 2020											
J								Parameter estimation and optimization of multi-objective capacitated stochastic transportation problem for gamma distribution	COMPLEX & INTELLIGENT SYSTEMS										Multi-objective optimization; Capacitated transportation problem; Stochastic programming; Interval type-2 fuzzy number; Gamma distribution; Maximum likelihood estimation	MAXIMUM-LIKELIHOOD-ESTIMATION; PROGRAMMING-PROBLEMS; FIXED-CHARGE; MODEL; RELIABILITY	The transportation problem in real life is an uncertain problem with multi-objective decision-making. In particular, by considering the conflicting objectives/criteria such as transportation costs, transportation time, discount costs, labour costs, damage costs, decision maker searches for the best transportation set-up to find out the optimum shipment quantity subject to certain capacity restrictions on each route. In this paper, capacitated stochastic transportation problem is formulated as a multi-objective optimization model along with some capacitated restrictions on the route. In the formulated problem, we assume that parameters of the supply and demand constraints' follow gamma distribution, which is handled by the chance constrained programming approach and the maximum likelihood estimation approach has been used to assess the probabilistic distributions of the unknown parameters with a specified probability level. Furthermore, some of the objective function's coefficients are consider as ambiguous in nature. The ambiguity in the formulated problem has been presented by interval type 2 fuzzy parameter and converted into the deterministic form using an expected value function approach. A case study on transportation illustrates the computational procedure.																	2199-4536	2198-6053				OCT	2020	6	3					651	667		10.1007/s40747-020-00156-1		JUN 2020											
J								A swarm optimization-based search algorithm for the quadratic knapsack problem with conflict Graphs	EXPERT SYSTEMS WITH APPLICATIONS										Knapsack; Optimization; Population; Particle swarm	LOCAL SEARCH	The knapsack problem arises in a variety of real world applications such as railway stations, flexible manufacturing systems, multimedia, cryptography and hydrological studies. In this paper, a special case of the knapsack problem is tackled: the quadratic knapsack problem with conflict graphs. This problem is solved by using a population-based search algorithm, which is inspired from the binary particle swarm optimization combined with a quick and efficient local search. The particle swarm optimization generates a population of particles while the local search procedure tries either to repair the infeasibility of each binary solution or to improve its quality. The performance of the proposed method is evaluated on a set of benchmark instances taken from the literature (containing medium and large-scale instances), where its achieved results are compared to those published in the literature containing the bounds realized with GLPK, Cplex and those achieved by more recent methods. The proposed method remains competitive, where encouraging results have been obtained. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				JUN 15	2020	148								113224	10.1016/j.eswa.2020.113224													
J								On the moral status of social robots: considering the consciousness criterion	AI & SOCIETY										Moral status; Moral rights; Machine rights; Consciousness criterion; Social robots	ETHICS; RIGHTS	While philosophers have been debating for decades on whether different entities-including severely disabled human beings, embryos, animals, objects of nature, and even works of art-can legitimately be considered as having moral status, this question has gained a new dimension in the wake of artificial intelligence (AI). One of the more imminent concerns in the context of AI is that of the moral rights and status of social robots, such as robotic caregivers and artificial companions, that are built to interact with human beings. In recent years, some approaches to moral consideration have been proposed that would include social robots as proper objects of moral concern, even though it seems unlikely that these machines are conscious beings. In the present paper, I argue against these approaches by advocating the "consciousness criterion," which proposes phenomenal consciousness as a necessary condition for accrediting moral status. First, I explain why it is generally supposed that consciousness underlies the morally relevant properties (such as sentience) and then, I respond to some of the common objections against this view. Then, I examine three inclusive alternative approaches to moral consideration that could accommodate social robots and point out why they are ultimately implausible. Finally, I conclude that social robots should not be regarded as proper objects of moral concern unless and until they become capable of having conscious experience. While that does not entail that they should be excluded from our moral reasoning and decision-making altogether, it does suggest that humans do not owe direct moral duties to them.																	0951-5666	1435-5655															10.1007/s00146-020-01002-1		JUN 2020											
J								Mix and Match Networks: Cross-Modal Alignment for Zero-Pair Image-to-Image Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image-to-image translation; Multi-domain; Multi-modal; Feature alignment; Mix and match networks; Zero-pair translation; Semantic segmentation; Depth estimation; Deep learning		This paper addresses the problem of inferring unseen cross-modal image-to-image translations between multiple modalities. We assume that only some of the pairwise translations have been seen (i.e. trained) and infer the remaining unseen translations (where training pairs are not available). We propose mix and match networks, an approach where multiple encoders and decoders are aligned in such a way that the desired translation can be obtained by simply cascading the source encoder and the target decoder, even when they have not interacted during the training stage (i.e. unseen). The main challenge lies in the alignment of the latent representations at the bottlenecks of encoder-decoder pairs. We propose an architecture with several tools to encourage alignment, including autoencoders and robust side information and latent consistency losses. We show the benefits of our approach in terms of effectiveness and scalability compared with other pairwise image-to-image translation approaches. We also propose zero-pair cross-modal image translation, a challenging setting where the objective is inferring semantic segmentation from depth (and vice-versa) without explicit segmentation-depth pairs, and only from two (disjoint) segmentation-RGB and depth-RGB training sets. We observe that a certain part of the shared information between unseen modalities might not be reachable, so we further propose a variant that leverages pseudo-pairs which allows us to exploit this shared information between the unseen modalities.																	0920-5691	1573-1405				DEC	2020	128	12					2849	2872		10.1007/s11263-020-01340-z		JUN 2020											
J								Adaptive Robust Control and Optimal Design for Fuzzy Unmanned Helicopter Tail Reduction	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy set theory; Unmanned helicopter tail reduction system; Adaptive robust control; Fuzzy performance index; Optimal design	OBSERVER; SYSTEMS; OPTIMIZATION; PERFORMANCE; LOGIC	This paper develops an adaptive robust control scheme combined with optimal design for the unmanned helicopter tail reduction (UHTR) system. The dynamical model of the UHTR system with uncertainties is established. The uncertainties are assumed to be bounded and expressed by fuzzy set theory. With this prerequisite, an adaptive robust controller is proposed to drive the system to meet the trajectory requirements approximately. The adaptive law with leakage and dead zone is performance based and flexibly adjustable. By means of Lyapunov stability theorem, the UHTR system is both uniform bounded and uniform ultimate bounded with the proposed controller. The control scheme is deterministic rather than fuzzy if-then rules-based control. Moreover, the fuzzy performance index which contains the steady-state system performance and control cost is presented. In this way, the optimal design problem can be replaced by minimizing the performance index. Overall, the resulting control scheme can guarantee deterministic performance of the UHTR system and minimize the fuzzy performance index simultaneously.																	1562-2479	2199-3211				JUL	2020	22	5					1400	1415		10.1007/s40815-020-00870-5		JUN 2020											
J								Observer-Based Adaptive Fuzzy Control for Intelligent Ship Autopilot with Input Saturation	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive fuzzy control; Autopilot; Finite-time control; Input saturation; Output feedback	NONLINEAR-SYSTEMS; DESIGN; STABILIZATION	In this paper, the design problem of intelligent ship autopilot with unmeasured yaw rate and input saturation is investigated based on adaptive fuzzy control. First, an adaptive fuzzy output feedback controller is proposed, then, to improve the convergence speed of the tracking error in the sense of finite time, a finite-time adaptive fuzzy output feedback controller is also designed to improve the control performance. By employing fuzzy logic system (FLS) to estimate the unknown nonlinear function, an adaptive fuzzy state observer is designed to estimate the unmeasured state. An adaptation auxiliary signal for input saturation is also established to compensate the mismatch between the controller signal and the actuator signal. Combining state observer model with backstepping method, two different adaptive fuzzy output-feedback controllers with composite parameter adaptive laws are constructed, respectively. Based on Lyapunov theory, it is proved that all signals in the closed-loop systems are bounded. Finally, the simulation results are conducted to demonstrate the effectiveness of the proposed schemes.																	1562-2479	2199-3211				JUL	2020	22	5					1416	1429		10.1007/s40815-020-00880-3		JUN 2020											
J								Pythagorean Fuzzy Entropy and Its Application in Multiple-Criteria Decision-Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Pythagorean fuzzy set; Fuzzy entropy; Overall entropy	DEFINITION; WEIGHTS; SETS	In this paper, a Pythagorean fuzzy decision-making method based on overall entropy is presented. First, a new definition is proposed for fuzzy entropy for any given Pythagorean fuzzy set (PFS). The proposed definition is based on the relationship between the fuzziness contained in the given PFS and the distance from a point to a line on a projection plane. Some related properties are introduced. Second, the overall entropy of the PFS is determined based on fuzzy entropy and the degree of hesitancy; proofs are presented to formalize some related properties. Third, an entropy weight formula is provided that is based on overall entropy, and a Pythagorean fuzzy decision-making method is developed on this basis. Finally, the effectiveness and practicability of the proposed methods are illustrated by an example and three comparative analyses.																	1562-2479	2199-3211				JUL	2020	22	5					1552	1564		10.1007/s40815-020-00877-y		JUN 2020											
J								Cyclic Pursuit-Fuzzy PD Control Method for Multi-agent Formation Control in 3D Space	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multi-agent formation; Cyclic pursuit algorithm; Reconfiguration control; Fuzzy control	TARGET TRACKING	This paper proposes a novel cooperative control method based on cyclic pursuit algorithm and fuzzy control idea, to accomplish reconfiguration of multi-agent formation in 3D space. The idea is to use nonlinear cyclic pursuit control within the formation's longitudinal motion plane, and use fuzzy control-based piecewise proportional derivative (PD) controller for cooperative control in the normal direction of motion. It is called cyclic pursuit-fuzzy PD control method. The control process is divided into different fuzzy sets, for which different control coefficients are set. The paper will present two control strategies to comparatively analyze the performances of control method. As the results demonstrate, the proposed method improves the performance and accuracy of formation reconfiguration control, avoids the inter-member collisions and enhances the system stability.																	1562-2479	2199-3211															10.1007/s40815-020-00892-z		JUN 2020											
J								Vibration control of semi-active suspension system using PID controller with advanced firefly algorithm and particle swarm optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Advanced firefly algorithm; Particle swarm optimization; PID controller; Semi-active; Vehicle comfort	BEE COLONY ALGORITHM; FUZZY-LOGIC	Magnetorheological (MR) damper control for semi-active system is one of the areas of interest investigated to improve the ride comfort and stability of vehicle performance. Many types of controllers used to control the semi-active MR damper have recently been investigated by previous researchers. It is found that the improper design of control scheme has led to an unpredictable optimum target force. Therefore, this study aims to investigate an intelligent optimizer called advanced firefly algorithm (AFA) to compute the proportional-integral-derivative (PID) controller for semi-active suspension system. The performance of the PID controller with the AFA tuning was investigated and compared to the original FA technique and other conventional and intelligent optimizers as well as non-PID controller namely as heuristic method, particle swarm optimization (PSO) and Skyhook controller. A MATLAB Simulation environment was used to generate the simulation model of semi-active suspension system complete will all control elements. The study of the controllers has shown a significant improvement as the proposed PID-AFA is capable of reducing the amplitude of the sprung acceleration and body acceleration responses up to 56.5% and 67.1%, respectively compared to PID-HEURISTIC, PID-FA, PID-PSO, Skyhook and passive systems.																	1868-5137	1868-5145															10.1007/s12652-020-02158-w		JUN 2020											
J								Automated query classification based web service similarity technique using machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Query classification; Web service similarity; Indexed storage; One hot encoding		With the tremendous growth of the internet, services provided through the internet are increasing day by day. For the adaption of web service techniques, several standards like ebXML, SOAP, WSDL, UDDI, and BPEL etc. are proposed and approved by W3C. Most of the web services are operating as a query-response model. User has to submit query according to the standard adapted, and services are supporting natural language queries nowadays. The given inputs are processed by web services server can find few similarities in sentence like nouns. The keyword for nouns is filtered accurately and saved in the list as table for each domain. Same time input query words are stored in the domain. The words stored in the domain is matched with the given input queries, later used to find the similarity between the queries In this paper, an automated technique for finding web service similarity based on query classification proposed. The proposed method adapted machine learning approach called KNN, and the data maintained in a hash indexed storage tables. As a result, the relationships between the input query and stored database have been showed in precision, recall, F1-Score and Support.																	1868-5137	1868-5145															10.1007/s12652-020-02186-6		JUN 2020											
J								Efficient optimal resource allocation for profit maximization in software defined network approach to improve quality of service in cloud environments	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software defined network (SDR); QoS; Resource scheduling; Resource allocation; Profit maximizing; CERRA; MHSA; Cloud computing		Software-defined networking (SDR) technology is an approach to network management that enables dynamic, programmatically efficient network configuration in order to improve network performance and monitoring making it more like cloud computing than traditional network management. Cloud Resource scheduling is used to schedule the workload-based customer request. Here, cost effective resources allocation is introduced based on arriving request and cluster allocation. The profit maximizing scheme aims is to provide probabilistic guarantee against the resource overloading and migration. In this work, proposed software defined approach namely Modified Heuristic Search (MHS) Algorithm is proposed to achieve the cost-effective resources allocation in distributing computing environment to improve the Quality of Service in Cloud environment and its applications. To achieve the profit maximization, Cost Effective Reliable Resource allocation (CERRA) algorithm is utilized to measure the effective cluster selection in MHSA which includes a fitness function for selecting the arriving cloud requests to earn profit. Speed, transfer rate and energy are measured and compared with the existing method to analysis the resource allocation system.																	1868-5137	1868-5145															10.1007/s12652-020-02192-8		JUN 2020											
J								Three-tier IoT-edge-cloud (3T-IEC) architectural paradigm for real-time event recommendation in event-based social networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender system (RS); Event based social networks (EBSNs); Real-time event recommendation; Internet of things (IoT); Social influence; Context-aware; Edge computing; Multiple-criteria decision making (MCDM); Cold-start problem	AWARE; SYSTEM	With the popularization of internet, event-based social networks (EBSNs) have experienced recognition among people for planning and communicating social events. Due to plethora of events occurring over EBSNs and time varying user interests, different recommendation techniques have been employed to make suitable events suggestions to participants. The state of the art event recommendation methods have not explored the importance of real-time data while generating event recommendations. However, we strongly believe that performance of event recommendation in EBSN can significantly be improved if multiple parameters captured through IoT devices may be considered. In this work, a real-time event recommendation problem which involves monitoring user's current location, present road traffic, and weather conditions is addressed that adopts instant event recommendation. To address this problem, a novel three-tier IoT-edge-cloud based solution for real-time context-aware event recommendation problem named as 3T-IEC has been proposed. The 3T-IEC introduces edge computing layer where IoT data is processed for deriving contextual IoT-based location information along with event recommendation generator in the cloud layer. Further, contextual information such as user's current location, weather and temporal feasibility has been applied to filter the events. Furthermore, group, category, and economic influences are modeled on filtered events to rank them with help of multiple criteria decision making method. Moreover, personalized weights on influential factors are also learned by using distance method. For practical realization, an android based mobile application-SpotEvent has been developed. Furthermore, the qualitative and quantitative analysis of the 3T-IEC is performed on two real-world datasets acquired from Meetup. The results clearly indicate that recommendation quality of proposed system is better, when compared to its variants and other baseline methods such as VSM (260%), SVD (144%), CAER (335%), Skyline (100%) and SoCast* (16%).																	1868-5137	1868-5145															10.1007/s12652-020-02202-9		JUN 2020											
J								Optimizing appointment scheduling for out patients and income analysis for hospitals using big data predictive analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data analytics; Healthcare; Medical informatics; Out-patient scheduling; Big data; Predictive analytics		Predictive Analytics is aided mainly by Big Data Technology tools. Predictive Analytics identifies meaningful patterns, and Big Data technologies like Hadoop and Hive help analyze and transform the results to assist in Business Intelligence. While maximization of the outpatient processes is plentiful, the help of Hadoop and Python can help support decision making. Efficiently scheduling out-patients in clinics can better utilize time and space and bring the hospital more money, resulting in better patient satisfaction. In our work, we focus on both Big Data and Predictive technologies to create an effective appointment-scheduling platform for outpatients. We use Random Forest Regressor (RFR) to design a recommendation system that helps us predict the rating of a doctor based on the waiting time in outpatient clinics. Hadoop's MapReduce, a useful tool for processing large datasets, is used to schedule the patients to the best practitioner for treatment according to their specialization and rating. Besides, based on the assignment, it would calculate the income generated by a doctor in a hospital and perform an analysis using Hive. Finally, we compare the performance of various recommendation systems. We then compare the scheduling algorithms using different data sets varying in sizes and find that the elapsed time for mapping and scheduling patients invariably remains the same irrespective of the size of the data. Finally, an analysis of the income generated evaluated.																	1868-5137	1868-5145															10.1007/s12652-020-02118-4		JUN 2020											
J								Dynamic service migration and resource management for vehicular clouds	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Vehicular ad hoc network; Vehicular cloud; Migration; Resource management; Roadside unit	VIRTUAL MACHINE PLACEMENT; SCHEDULING ALGORITHMS; COMMUNICATION; AVOIDANCE	Vehicular cloud computing has received huge attention in business and scientific communities, which integrates two emerging fields, namely cloud computing and vehicular ad hoc networks. It acts as a data center by using the underutilized resources of the networked vehicles. Moreover, many studies suggest these vehicles as potential candidates for hosting virtual machines (VMs). As a result, a VM can be set up using the vehicular resources, and it can be transferred from one vehicle to another vehicle to continue its execution, under some circumstances. It enables the vehicular environment to provide services to the user requests, that are submitted to the cloud. However, the mapping of such requests to the VMs (or hosted vehicles) and service migration is very much challenging, and not well-studied in the literature. In this paper, we propose a dynamic service migration algorithm for vehicular clouds. The algorithm consists of three phases, estimation, assignment and migration. The performance is carried out through simulation runs using two scenarios of six datasets, and compared with three well-known algorithms, namely vehicular VM migration-uniform, round robin and mobility and destination workload aware migration using four performance measures. The comparison results followed by statistical validation usingTtest show the superiority of the proposed algorithm over the existing algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02166-w		JUN 2020											
J								Computer aided innovation method for detection and classification of cervical cancer using ANFIS classifier	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cervical cancer; Image classification; Tumour; Fuzzy rules; Neural network		Early detection of cervical tumour is very important to minimise deaths due to cervical cancer. Further it provides a deep insight into the anatomical information of the normal and abnormal cervix and helps in planning for a good treatment well in advance. Numerous techniques are used to detect malignancy through image segmentation. One such segmentation technique is discussed here. The proposed technique uses Artificial Neural Network Fuzzy Inference system (ANFIS) and watershed segmentation techniques for image classification and processing and compares the results with known techniques. A comprehensive set of fuzzy rules was used in the experiment to classify abnormal images to the corresponding malignancy. The experiment shows that the proposed technique is feasible and provides greater accuracy in detection of tumour types.																	1868-5137	1868-5145															10.1007/s12652-020-02191-9		JUN 2020											
J								An efficient recognition system for preserving ancient historical documents of English characters	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Character recognition; HDLA 2011 dataset; Bounding box method; Local binary patterns; SPM classifier	REPRESENTATION; CLASSIFICATION; SEGMENTATION; RETRIEVAL; DECISION	The clusters of historical documents are of great importance in terms of cultural and scientific. In order to access the documents, originality should be maintained. So conversion of digital form is highly required for recognition. While converting, those documents may be due to poor quality, overlapping of characters, complex background and so on. In this paper, an efficient system for recognizing English characters from degraded historical document images is proposed. Initially, modified Adaptive Thresholding based binarization process is performed to eliminate the noise content in the input image. The characters are segmented through the rectangular bounding box method. Then Local binary pattern (LBP) algorithm is enforced to extricate the features of each characters. Finally, Spatial Pyramid Matching (SPM) classifier is used for texture classification. HDLA 2011 dataset is employed to validate the proposed method. The proposed method achieves 94.6% recognition accuracy and 0.34 s computation time for Lucida Black-letter font. This method also outperforms better than the existing recognition techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02201-w		JUN 2020											
J								Design and implementation of an autonomous EGR cooling system using deep neural network prediction to reduce NOx emission and fuel consumption of diesel engine	NEURAL COMPUTING & APPLICATIONS										Autonomous EGR cooling system; NOx; BSFC; Deep neural networks; Backpropagation; Optimal tuned PID controller	EXHAUST-GAS RECIRCULATION; IN-CYLINDER; PERFORMANCE; COMBUSTION; LP	This study includes the design of an autonomous exhaust gas recirculation (EGR) cooling system and implementation of the system on diesel engine by using deep neural network (DNN)-based controller. The NOx formation and BSFC were optimized as output variables considering engine speed, load, EGR ratio and the exhaust gas temperature entering the intake manifold as input variables using deep and traditional NN modelling method. The DNN modelling method has stronger prediction capability than traditional NN and can deal with the modelling problem in the complex ICE data. The developed NN models were compared with the relative error analysis to verify the effectiveness of the deep NN modelling method. The activation function, the number of neurons, learning rate and backpropagation optimization method were considered as algorithm performance parameters for the design of optimum network by using the searching method.R-2, MSE, MAE and RAE are used to evaluate the output performance of the deep and traditional NN model. Then, the autonomous and independent EGR cooling system is designed apart from the main cooling system of the engine with electric pump and fan components, unlike conventional systems. PID controllers are designed for these components, and the parameters of the controller are determined by pattern search optimization method. In this way, the exhaust gas temperature entering the intake manifold and EGR ratio are set to the desired value according to the BSFC and NOx output parameters under different operating conditions of the engine. Experimental results showed that the designed DNN controller-based autonomous EGR cooling system outperforms compared to the conventional system in terms of reducing the NOx and BSFC, with a 5.751% and 2.997% reduction, respectively.																	0941-0643	1433-3058															10.1007/s00521-020-05104-1		JUN 2020											
J								Comparison of fuzzy inference algorithms for stream flow prediction	NEURAL COMPUTING & APPLICATIONS										Fuzzy logic; Flood forecasting; Gaussian membership function; Mamdani fuzzy inference system; Nash-Sutcliffe model efficiency; Sugeno fuzzy inference system	LOGIC; RAINFALL; RESOLUTION; SYSTEM; MODEL; SENSITIVITY; DISCHARGE; FORECAST; BASIN	Fuzzy logic is, inter alia, a simple and flexible approach of modelling that can be used in river basins where adequate hydrological data are unavailable. In order to improve the real-time forecasting of floods, this paper proposes a Takagi-Sugeno fuzzy inference system termed as flood model Sugeno. A total of 12 input parameters were used to develop two fuzzy flood models-Mamdani and Sugeno. Whereas Sugeno FIS performed exceptionally well in predicting the river discharge, the Mamdani FIS failed to deliver the accurate results. The river Jhelum flowing through the Kashmir Valley in the northern Himalayas, India, was hit in September 2014 by a major flood and has been chosen as the case study to apply the fuzzy flood models. With a total of 24 rules in the rule base and five levels of linguistic variables, the flood model Sugeno predicted the river discharge with Nash-Sutcliffe model efficiency of 0.887, coefficient of correlation (R-2) of 90.74%, mean square error of 0.00122, root mean square error of 0.0349, mean absolute error of 0.0139 and combined accuracy of 0.0466. The efficiencies of the developed model show acceptable levels according to the tested performance indicators implying the potential of establishing a flood forecasting system using the developed model.																	0941-0643	1433-3058															10.1007/s00521-020-05098-w		JUN 2020											
J								Hybrid multi-objective opposite-learning evolutionary algorithm for integrated production and maintenance scheduling with energy consideration	NEURAL COMPUTING & APPLICATIONS										Energy efficient; Serial production line; Multi-objective scheduling; Preventive maintenance; Production machines	OPTIMIZATION ALGORITHM; SWARM OPTIMIZATION; POWER-CONSUMPTION; 2-STAGE; MINIMIZATION; IMPERFECT	While conventional scheduling researches take production efficiency, cost and quality as objectives, increasingly serious ecological problems and energy shortage have turned scholars' attention to energy-efficient scheduling. Meanwhile, maintenance activities are of great importance to equipment availability and production continuity. This paper addresses an energy-efficient multi-objective scheduling problem of a serial production line (SPSP) integrating production and maintenance, with the criteria of minimizing the makespan and the total energy consumption simultaneously. The impact of integrating preventive maintenance (PM) is analyzed, and the better interval mode is picked out. An energy-saving strategy combining shutdown windows of PM and energy-saving windows on idle machines is designed to cut down the total energy consumption. In order to tackle this NP-hard problem, a hybrid multi-objective opposite-based learning evolutionary algorithm (HMOLEA) is developed, in which the opposite-based learning and IGD indicator-based evolutionary mechanism are combined together and a special rank assignment is designed to improve the computational efficiency. Furthermore, a self-adaptive weighted mutation operator is fused into the framework of HMOLEA to enhance the exploration of the algorithm. Extensive computational experiments are carried out to verify the effectiveness and superiority of HMOLEA in solving the SPSP.																	0941-0643	1433-3058															10.1007/s00521-020-05075-3		JUN 2020											
J								Margin setting algorithm for pattern classification via spheres	PATTERN ANALYSIS AND APPLICATIONS										Sphere-based classification; Margin setting; Hypersphere; Multi-sphere decision boundary		Margin setting algorithm (MSA) is a new sphere-based classification algorithm. It employs an artificial immune system approach to construct a number of hyperspheres that cover each class of a given set of data. To gain insights into the classification performance of MSA, it is the first work to analyze two important fundamental problems of MSA as a sphere-based classifier. First, single sphere or multiple spheres are needed to achieve good classification performance in MSA? This problem was presented as sphere analysis, which was experimentally carried out on simulation data sets using Monte Carlo method. The results demonstrated that MSA employs a multiple-sphere strategy instead of one-sphere strategy as its decision boundaries. This strategy allows MSA to achieve lower probabilities of classification error rate. Second, how to adapt the location and size of the hypersphere to achieve good classification performance? This problem was presented as adaption analysis, which was experimentally carried out on real-world data sets compared to the support vector machine and the artificial neural network. The results demonstrated that MSA employs an artificial immune system approach to optimize the locations of the hyperspheres and to shrink the radius of the hypersphere in a certain range using margin as an algorithm parameter. Overall, computational results indicate the advantages of MSA in classification performance.																	1433-7541	1433-755X				NOV	2020	23	4					1677	1688		10.1007/s10044-020-00888-3		JUN 2020											
J								A boosting Self-Training Framework based on Instance Generation with Natural Neighbors forKNearest Neighbor	APPLIED INTELLIGENCE										Semi-supervised learning (SSL); Semi-supervised classification (SSC); Self-training; Boosting; Instance generation; Natural neighbors	SEMI; CLASSIFICATION; SELECTION; REGRESSION; NETWORKS; SEARCH; MODEL; TREES	The semi-supervised self-training method is one of the successful methodologies of semi-supervised classification. The mislabeling is the most challenging issue in self-training methods and the ensemble learning is one of the common techniques for dealing with the mislabeling. Specifically, the ensemble learning can solve or alleviate the mislabeling by constructing an ensemble classifier to improve prediction accuracy in the self-training process. However, most ensemble learning methods may not perform well in self-training methods because it is difficult for ensemble learning methods to train an effective ensemble classifier with a small number of labeled data. Inspired by the successful boosting methods, we introduce a new boosting self-training framework based on instance generation with natural neighbors (BoostSTIG) in this paper. BoostSTIG is compatible with most boosting methods and self-training methods. It can use most boosting methods to solve or alleviate the mislabeling of existing self-training methods by improving the prediction accuracy in the self-training process. Besides, an instance generation with natural neighbors is proposed to enlarge initial labeled data in BoostSTIG, which makes boosting methods more suitable for self-training methods. In experiments, we apply the BoostSTIG framework to 2 self-training methods and 4 boosting methods, and then validate BoostSTIG by comparing some state-of-the-art technologies on real data sets. Intensive experiments show that BoostSTIG can improve the performance of tested self-training methods and train an effectiveknearest neighbor.																	0924-669X	1573-7497				NOV	2020	50	11					3535	3553		10.1007/s10489-020-01732-1		JUN 2020											
J								Investigating the use of an ensemble of evolutionary algorithms for letter identification in tremulous medieval handwriting	EVOLUTIONARY INTELLIGENCE										Tremor; Classification; Ensemble; Evolutionary; Genetic algorithm		Ensemble classifiers are known for performing good generalization from simpler and less accurate classifiers. Ensembles have the ability to use the variety in classification patterns of the smaller classifiers in order to make better predictions. However, to create an ensemble it is necessary to determine how the component classifiers should be combined to generate the final predictions. One way to do this is to search different combinations of classifiers with evolutionary algorithms, which are largely employed when the objective is to find a structure that serves for some purpose. In this work, an investigation is carried about the use of ensembles obtained via evolutionary algorithm for identifying individual letters in tremulous medieval writing and to differentiate between scribes. The aim of this research is to use this process as the first step towards classifying the tremor type with more accuracy. The ensembles are obtained through evolutionary search of trees that aggregate the output of base classifiers, which are neural networks trained prior to the ensemble search. The misclassification patterns of the base classifiers are analysed in order to determine how much better an ensemble of those classifiers can be than its components. The best ensembles have their misclassification patterns compared to those of their component classifiers. The results obtained suggest interesting methods for letter (up to 96% accuracy) and user classification (up to 88% accuracy) in an offline scenario.																	1864-5909	1864-5917															10.1007/s12065-020-00427-3		JUN 2020											
J								A graph neural network method for distributed anomaly detection in IoT	EVOLVING SYSTEMS										IoT cybersecurity; Graph inherent anomaly detection framework; Graph neural networks; DDoS attack detection; Decentralized detection; Synergistic detection; Multi-agent detection	DEEP LEARNING APPROACH; SVM	Recent IoT proliferation has undeniably affected the way organizational activities and business procedures take place within several IoT domains such as smart manufacturing, food supply chain, intelligent transportation systems, medical care infrastructures etc. The number of the interconnected edge devices has dramatically increased, creating a huge volume of transferred data susceptible to leakage, modification or disruption, ultimately affecting the security level, robustness and QoS of the attacked IoT ecosystem. In an attempt to prevent or mitigate network abnormalities while accommodating the cohesiveness among the involved entities, modeling their interrelations and incorporating their structural, content and temporal attributes, graph-based anomaly detection solutions have been repeatedly adopted. In this article we propose, a multi-agent system, with each agent implementing a Graph Neural Network, in order to exploit the collaborative and cooperative nature of intelligent agents for anomaly detection. To this end, against the propagating nature of cyber-attacks such as the Distributed Denial-of-Service (DDoS), we propose a distributed detection scheme, which aims to monitor efficiently the entire network infrastructure. To fulfill this task, we consider employing monitors on active network nodes such as IoT devices, SDN forwarders, Fog Nodes, achieving localization of anomaly detection, distribution of allocated resources such as the bandwidth and power consumption and higher accuracy results. In order to facilitate the training, testing and evaluation activities of the Graph Neural Network algorithm, we create simulated datasets of network flows of various normal and abnormal distributions, out of which we extract essential structural and content features to be passed to neighbouring agents.																	1868-6478	1868-6486															10.1007/s12530-020-09347-0		JUN 2020											
J								An n-state switching PSO algorithm for scalable optimization	SOFT COMPUTING										Particle swarm optimization; Evolutionary factor; Large-scale optimization; scalability	PARTICLE SWARM OPTIMIZATION; NETWORKS	Particle swarm optimization (PSO) is an optimization method that is most widely used to solve a number of problems in various fields such as engineering, economics and computer systems. However, due to its scalability and unsatisfying performance particularly for large-scale optimization problems; numerous PSO variants have been suggested so far, in the literature. This paper also proposes a new variant of the canonical PSO algorithm ('N-state switching PSO-NS-SPSO') that uses the evolutionary factor information to update particles velocities and, therefore, further enhance its performance. The evolutionary factor is derived by using the population distribution and the mean distance of each particle from the global best. The population distribution and the mean distance are determined through Euclidean distance. Moreover, algorithmic parameters such as inertia weight, and acceleration coefficients are assigned appropriate values atNstages (derived from exploration, exploitation, convergence and jumping out states) that improves the search efficiency and convergence speed. The proposed algorithm is applied to 12 widely used mathematical benchmark functions that demonstrate its best performance in terms of minimum evaluation error, fast convergence and low computational time. Besides these, seven high-dimensional functions and few other algorithms for large-scale optimization were considered to test the scalability of NS-SPSO algorithm. Our comparative results show that NS-SPSO performs well on low-dimensional problems and is promising for solving large-scale optimization problems. Furthermore, the proposed NS-PSO algorithm almost outperforms its closest rivals for various benchmarks.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11297	11314		10.1007/s00500-020-05069-2		JUN 2020											
J								Global optimization method with dual Lipschitz constant estimates for problems with non-convex constraints	SOFT COMPUTING										Global optimization; Multiextremal problems; Non-convex constraints; Lipschitz constant estimates; Dimensionality reduction; Numerical methods	ALGORITHM; PARTITIONS; MAXIMUM; SET	This paper considers the constrained global optimization problems, in which the functions are of the "black-box" type and satisfy the Lipschitz condition. The algorithms for solving the problems of this class require the use of adequate estimates of the a priori unknown Lipschitz constants for the problem functions. A novel approach presented in this paper is based on a simultaneous use of two estimates of the Lipschitz constant: an overestimated and an underestimated one. The upper estimate provides the global convergence, whereas the lower one reduces the number of trials necessary to find the global optimizer with the required accuracy. The considered algorithm for solving the constrained problems does not use the ideas of the penalty function method; each constraint of the problem is accounted for separately. The convergence conditions of the proposed algorithm are formulated in the corresponding theorem. The results of the numerical experiments on a series of multiextremal problems with non-convex constraints demonstrating the efficiency of the proposed scheme of dual Lipschitz constant estimates are presented.																	1432-7643	1433-7479				AUG	2020	24	16					11853	11865		10.1007/s00500-020-05078-1		JUN 2020											
J								Identifying ground truth in opinion spam: an empirical survey based on review psychology	APPLIED INTELLIGENCE										Opinion spam; Review spam; Ground truths; Review psychology; Crowdsourcing spammer; Expert spammer	WORD-OF-MOUTH; PRODUCT; AVAILABILITY; PERCEPTIONS; CONSUMERS; SELF; FAKE	Because it is very harmful, opinion spam, especially that involving untruthful reviews, has attracted much attention in the last decade. However, the lack of annotations, i.e., the ground truth problem, still serves as the key challenge. It is difficult because spammers always deliberately forge their reviews, which cannot be distinguished even by field experts. Considering the obvious intention of spammers, i.e., to promote or demote an items reputation, the opportunity exists to label them by considering crowd psychology. To date, several studies have applied, verified, and presented helpful evidence, including prior, empirical, heuristic, and simulative pseudo truths. In this paper, after investigating both authentic and deceptive reviewers' diverse motives, we survey state-of-the-art truth by considering two classical roles, e.g., crowdsourcing and expert spammers. For each role, several topics related to spam attacks either with or without disguising and possible outliers are highlighted. Comparison analyses led to some interesting conclusions: 1) data on professional spammers are more challenging to collect and less reliable than data on crowdsourcing spammers; 2) most linguistic evidences are less reliable than behavioral footprints; 3) abnormal activities are as trustworthy as spamming objectives, while they hardly need any extra support, such as the user profile; and 4) the top reliable facts requiring acceptable effort aredeviation,burstiness,grouped spamming,deviation over the threshold,review distribution,opinion proportionandspam cost. Moreover, we introduce several promising directions for future research. In general, this survey may shed light on new angles that can be used to understand review spam and to improve the performance of any anti-spam platforms.																	0924-669X	1573-7497				NOV	2020	50	11					3554	3569		10.1007/s10489-020-01764-7		JUN 2020											
J								Weighted H-infinity Performance Analysis of Nonlinear Stochastic Switched Systems: A Mode-Dependent Average Dwell Time Method	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Stochastic switched systems; Nonlinear systems; Weighted H-infinity performance; T-S fuzzy approach; Mode-dependent average dwell time	LINEAR-SYSTEMS; STABILIZATION; STABILITY; DESIGN	In this paper, the issues about weighted H-infinity performance analysis and H-infinity control for the stochastic switched nonlinear systems (SSNSs) with multiplicative noise are investigated. The mode-dependent average dwell time (MDADT) method is used to deal with the switching in different modes. Firstly, we get a sufficient condition to show that the considered system with all stable subsystems achieves the exponential stability in mean square sense and a specified weighted H-infinity performance, which is extended to the case that both stable and unstable subsystems coexist in terms of second-order Hamilton-Jacobi inequalities (HJIs). Then, by using Takagi-Sugeno (T-S) fuzzy approach, the H-infinity controller for SSNSs is designed via solving a set of linear matrix inequalities (LMIs). Finally, an example is supplied to illustrate the effectiveness of our results.																	1562-2479	2199-3211				JUL	2020	22	5					1454	1467		10.1007/s40815-020-00864-3		JUN 2020											
J								Evaluation of Two-Stage Networks Based on Average Efficiency Using DEA and DEA-R with Fuzzy Data	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Two-stage network; Fuzzy data; Data envelopment analysis (DEA); DEA-R; Average efficiency	DATA ENVELOPMENT ANALYSIS; ANALYSIS MODELS; RATIO DATA; INEFFICIENCY; PERFORMANCE; INDUSTRY; TOPSIS; SCALE; COST	The present paper proposes a number of models for calculating the average efficiency of two-stage networks using DEA and DEA-R with fuzzy data. If the input, intermediate, and output parameters are available in a two-stage network, DEA and DEA-R models can be used to compute the efficiency. When evaluating decision-making units (DMUs) in two-stage network DEA, the respective programming models are fractional. Meanwhile, in DEA-R, the proposed programming models are linear. Although, it is necessary that the output-to-input ratios (output-orientation) or vice versa (input orientation) be defined and available. Furthermore, DEA-R models can also evaluate DMUs with a network structure when only ratio data are available. Generally, using fuzzy data is necessary for an accurate evaluation of organizations with a two-stage network structure. Therefore, in the present article, using the alpha-cut approach, an average efficiency model is proposed for the first and second stages of a network structure. At the end, a comparison is made between the mean efficiency scores of a number of airlines by considering fuzzy data in two-stage network DEA and DEA-R.																	1562-2479	2199-3211				JUL	2020	22	5					1665	1678		10.1007/s40815-020-00896-9		JUN 2020											
J								A personalized recommendation algorithm based on large-scale real micro-blog data	NEURAL COMPUTING & APPLICATIONS										Personalized recommendation; K-means; User model; Word2vec	USERS	With the arrival of the big data era, the amount of micro-blog users and texts is constantly increasing, and research on personalized recommendation algorithm for micro-blog texts is becoming more and more urgent. In consideration of the impact of user's interests, trust transfer, time factor and social network, we proposed a new method for personalized recommendation. The method is based on community discovery, and recommends personalized micro-blog texts for users with the improved user model, which can use the social network of micro-blog platform effectively and optimize the utility function for micro-blog recommendation. Firstly, we used a multidimensional vector to represent the stereoscopic user model. Secondly, we proposed the improved k-means algorithm to extract the local community of users, which was also used to get the recommend micro-blog texts. Finally, the top-n micro-blog contents sorted by the effect function were recommended. We used a large number of real data to verify the algorithm proposed in this paper, and compared our method with some existing algorithms.																	0941-0643	1433-3058				AUG	2020	32	15					11245	11252		10.1007/s00521-020-05042-y		JUN 2020											
J								Data-driven network layer security detection model and simulation for the Internet of Things based on an artificial immune system	NEURAL COMPUTING & APPLICATIONS										Internet-of-Things security; Artificial immune system; Computational experiments		Traditional network security detection models are trained offline using attack samples of known types. Although such models have high detection rates for known attack types, they cannot identify new attack types in the network layer. At present, these detection systems have the disadvantages of slow system construction and a high cost of model updating. Facing the increasing expansion of networks and endless attacks, these detection systems lack self-adaptability and expansibility, so it is difficult to detect complex and changeable attack events in networks. In this paper, the integrated use of immunology theory, complex adaptive system theory, and computational experiment technology is proposed to develop an Internet network layer security detection model based on an artificial immune system as an improvement over existing models of Internet network layer security. On the basis of testing knowledge, when a new type of attack is encountered, the online detection and learning process enables the dynamic extension of the network security detection model. Experimental calculations and an example analysis are presented to verify the scientific validity and feasibility of the model.																	0941-0643	1433-3058															10.1007/s00521-020-05049-5		JUN 2020											
J								Driven by machine learning to intelligent damage recognition of terminal optical components	NEURAL COMPUTING & APPLICATIONS										Machine learning; Terminal optics; Damage identification; Infrared nondestructive testing	FOM	In order to realize the terminal optical element online detection system in the Shenguang III system, each optical element in each terminal optical component in the target room is detected. The research on the optical damage of terminal optical components focuses on the search for damage points, the extraction of damage information, and the classification of damage types. In addition, damage classification and identification of terminal optical components are performed through machine learning, and infrared nondestructive testing is used as technical support to improve the identification model and reduce the complexity of the spectral model. After studying the preprocessing and dimensionality reduction methods of near-infrared spectroscopy, this paper compares the effects of different preprocessing methods and screening feature methods and combines different modeling methods to conduct experiments. The research results show that the method proposed in this paper has certain effects.																	0941-0643	1433-3058															10.1007/s00521-020-05051-x		JUN 2020											
J								Genome annotation across species using deep convolutional neural networks	PEERJ COMPUTER SCIENCE										Transcription start sites; Promoters; Genome annotation; Deep learning; DNA motifs; Sequence evolution; Unbalanced datasets	SEQUENCE; DNA	Application of deep neural network is a rapidly expanding field now reaching many disciplines including genomics. In particular, convolutional neural networks have been exploited for identifying the functional role of short genomic sequences. These approaches rely on gathering large sets of sequences with known functional role, extracting those sequences from whole-genome-annotations. These sets are then split into learning, test and validation sets in order to train the networks. While the obtained networks perform well on validation sets, they often perform poorly when applied on whole genomes in which the ratio of positive over negative examples can be very different than in the training set. We here address this issue by assessing the genomewide performance of networks trained with sets exhibiting different ratios of positive to negative examples. As a case study, we use sequences encompassing gene starts from the RefGene database as positive examples and random genomic sequences as negative examples. We then demonstrate that models trained using data from one organism can be used to predict gene-start sites in a related species, when using training sets providing good genome-wide performance. This cross-species application of convolutional neural networks provides a new way to annotate any genome from existing high-quality annotations in a related reference species. It also provides a way to determine whether the sequence motifs recognised by chromatin-associated proteins in different species are conserved or not.																	2376-5992					JUN 15	2020									e278	10.7717/peerj-cs.278													
J								Identifying multiscale spatio-temporal patterns in human mobility using manifold learning	PEERJ COMPUTER SCIENCE										Complex systems; Manifold learning; Human mobility; Dimension reduction; Prediction; Geographic information science; Multiscale; Emergence; Wavelet; Networks	EARLY-WARNING SIGNALS; PREDICTABILITY; DISPLACEMENT; CHALLENGES; MIGRATION	When, where and how people move is a fundamental part of how human societies organize around every-day needs as well as how people adapt to risks, such as economic scarcity or instability, and natural disasters. Our ability to characterize and predict the diversity of human mobility patterns has been greatly expanded by the availability of Call Detail Records (CDR) from mobile phone cellular networks. The size and richness of these datasets is at the same time a blessing and a curse: while there is great opportunity to extract useful information from these datasets, it remains a challenge to do so in a meaningful way. In particular, human mobility is multiscale, meaning a diversity of patterns of mobility occur simultaneously, which vary according to timing, magnitude and spatial extent. To identify and characterize the main spatio-temporal scales and patterns of human mobility we examined CDR data from the Orange mobile network in Senegal using a new form of spectral graph wavelets, an approach from manifold learning. This unsupervised analysis reduces the dimensionality of the data to reveal seasonal changes in human mobility, as well as mobility patterns associated with large-scale but short-term religious events. The novel insight into human mobility patterns afforded by manifold learning methods like spectral graph wavelets have clear applications for urban planning, infrastructure design as well as hazard risk management, especially as climate change alters the biophysical landscape on which people work and live, leading to new patterns of human migration around the world.																	2376-5992					JUN 15	2020									e276	10.7717/peerj-cs.276													
J								A Novel Objective Grouping Evolutionary Algorithm for Many-Objective Optimization Problems	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Many-objective evolutionary algorithm; objective grouping; interdependence relation; dual selection mechanism	CONFLICT INFORMATION; GENETIC ALGORITHM; REDUCTION	The thorniest difficulties for multi-objective evolutionary algorithms (MOEAs) handling many-objective optimization problems (MaOPs) are the inefficiency of selection operators and high computational cost. To alleviate such difficulties and simplify the MaOPs, objective reduction algorithms have been proposed to remove the redundant objectives during the search process. However, those algorithms can only be applicable to specific problems with redundant objectives. Worse still, the Pareto solutions obtained by reduced objective set may not be the Pareto solutions of the original MaOPs. In this paper, we present a novel objective grouping evolutionary algorithm (OGEA) for general MaOPs. First, by dividing original objective set into several overlapping lower-dimensional subsets in terms of interdependence correlation information, we aim to separate the MaOPs into a number of sub-problems so that each of them can be able to preserve as much dominance structure in the original objective set as possible. Subsequently, we employ the nondominated sorting genetic algorithm II (NSGA-II) to generate Pareto solutions. Besides, instead of nondominated sorting on the whole population, a novel dual selection mechanism is proposed to choose individuals either having high ranks in subspaces or locating sparse region in the objective space for better proximity and diversity. Finally, we compare the proposed strategy with the other two classical space partition methods on benchmark DTLZ5 (I, M), DTLZ2 and a practical engineering problem. Numerical results show the proposed objective grouping algorithm can preserve more dominance structure in original objective set and achieve better quality of Pareto solutions.																	0218-0014	1793-6381				JUN 15	2020	34	6							2059018	10.1142/S0218001420590181													
J								Chaotic Time Series Prediction Using a Novel Echo State Network Model with Input Reconstruction, Bayesian Ridge Regression and Independent Component Analysis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Echo State Network; phase-space reconstruction; Bayesian Ridge Regression; independent component analysis; time series prediction	OPTIMIZATION; MACHINE	This paper presents a novel Echo State Network (ESN) model for chaotic time series prediction, which consists of three steps including input reconstruction, dimensionality reduction and regression. First, phase-space reconstruction is used to reconstruct the original 'attractor' of the input time series. Then, Independent Component Analysis (ICA) is used to identify independent components, reduce dimensionality and overcome multicollinearity problem of the reconstructed input matrix. Finally, Bayesian Ridge Regression provides accurate predictions thanks to its regularization effect to avoid over-fitting and its robustness to noise owing to its probabilistic strategy. Our experimental results show that our model significantly outperforms other ESN models in predicting both artificial and real-world chaotic time series.																	0218-0014	1793-6381				JUN 15	2020	34	6							2051008	10.1142/S0218001420510088													
J								Research on Data Acquisition Algorithms Based on Image Processing and Artificial Intelligence	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Number recognition; pre-processing; tilt correction; feature extraction; improved BP neural network	CHARACTER-RECOGNITION; MODEL; REPRESENTATION	At present, image recognition processing technology has been playing a decisive role in the field of pattern recognition, of which automatic recognition of bank notes is an important research topic. Due to the limitation of the size of bill layout and printing method, many invoice layouts are not clear, skewed or distorted, and even there are irregular handwritten signature contents, which lead to the problem of recognition of digital characters on bill surface. In this regard, this paper proposes a data acquisition and recognition algorithm based on improved BP neural network for ticket number identification, which is based on the theory of image processing and recognition, combined with improved bill information recognition technology. First, in the pre-processing stage of bill image, denoising and graying of bill image are processed. After binarization of bill image, the tilt detection method based on Bresenham integer algorithm is used to correct the tilted bill image. Secondly, character localization and feature extraction are carried out for par characters, and the target background is separated from the interference background in order to extract the desired target characters. Finally, the improved BP neural network-based bill digit data acquisition and recognition algorithm is used to realize the classification and recognition of bill characters. The experimental results show that the improved method has better classification and recognition effect than other data acquisition and recognition algorithms.																	0218-0014	1793-6381				JUN 15	2020	34	6							2054016	10.1142/S0218001420540166													
J								Infrared Small Target Detection Based on Morphology and SUSAN Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Infrared image; morphological filtering; top-hat transform; SUSAN detection		Infrared small target detection is one of the key techniques in infrared imaging guidance system. The technology of infrared small target detection still needs to be further studied to improve the detection performance. This paper combines the high-pass filtering characteristics of morphological top-hat transform with SUSAN algorithm, and proposes a small infrared target detection method based on morphology and SUSAN algorithm. This method uses top-hat transform to detect the high-frequency region in infrared image, and filters out the low-frequency region in the image to implement the preliminary background suppression of infrared image. Then the SUSAN algorithm is used to detect small targets in the image after background suppression. The proposed method is applied to the single infrared image which is acquired by the infrared guidance system in the process of detecting and tracking the target under specific conditions. The experimental results show that the method is effective and can detect infrared small targets under different background.																	0218-0014	1793-6381				JUN 15	2020	34	6							2050013	10.1142/S0218001420500135													
J								Research on License Plate Image Segmentation and Intelligent Character Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										License plate; image segmentation; character recognition; feature fusion		With the accumulation of people's wealth and the improvement of purchasing power, more and more people are buying cars as a means of travel. Walking and cycling of the past have now become a car trip. License plate recognition technology is especially important in intelligent transportation systems. It has been widely used in large shopping malls or supermarket parking lots, highway toll stations, speeding violation supervision and other fields. However, the accuracy and efficiency of license plate image recognition are insufficient. To solve the above problems, we propose a license plate character recognition method based on local HOG and layered LBP feature fusion from the perspectives of image pre-processing, license location, characters' segmentation and recognition. First, pre-processing the license image area by highlighting the license plate image; then, the license plate is positioned based on wavelet decomposition and brightness moment; next the tilted license plate image is corrected, the license plate frame is adjusted, and characterization is performed by using the improved projection method based on the fact that the projection of the character is a single peak or a double peak. Finally, the local HOG and hierarchical LBP feature fusion methods are used to identify the license characters. The results show that the license plate's character recognition rate of the proposed method reaches 99.71%, and the time taken is small. This not only improves the character recognition rate, but also saves recognition time. The results show that the method has important practical significance in license plates' recognition.																	0218-0014	1793-6381				JUN 15	2020	34	6							2050014	10.1142/S0218001420500147													
J								Study on the Concrete in Chloride Environment Based on Electrochemical Impedance Spectroscopy	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Concrete; water-cement ratios; chloride ion; electrochemical impedance spectroscopy		In this paper, the effect of water-cement ratio and chloride ions on the concrete meso-structure was studied. Three kinds of concrete cubes with different water-cement ratios were immersed in fresh water and salt water, respectively. Then, the Electrochemical Impedance Spectroscopy (EIS) analysis of various test cubes were carried out by using electrochemical workstation. The results show that the salt water can improve electric double layer capacitance in the test cubes with the same water-cement ratio, but it can reduce some other parameters such as resistance of pore solution, resistance to transfer the hydrated electron, coefficient of diffusion impedance of concreter, which shows that the chloride ions diffused into the concrete in salt water and increase the ionic concentration in pore solution and C-S-H gel. However, the phase angle index is constant whether in fresh water or salt water, which shows chloride ions cannot affect the concrete meso-structure even though they can improve the ion concentration of pore structure. For the concrete test cubes which has different water-cement ratio in salt water, with the reduction of water-cement ratio, the electric double-layer capacitance of concrete remains unchanged, which indicates when the water-cement ratio becomes smaller, the porosity becomes lower, and the internal structure of concrete becomes denser.																	0218-0014	1793-6381				JUN 15	2020	34	6							2059017	10.1142/S021800142059017X													
J								Linear Predictive Coefficients-Based Feature to Identify Top-Seven Spoken Languages	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Automatic language identification; line spectral pair; ensemble learning	IDENTIFICATION	Speech recognition in multilingual scenario is not trivial in the case when multiple languages are used in one conversation. Language must be identified before we process speech recognition as such tools are language-dependent. We present a language identification system (or AI tool) to distinguish top-seven world languages namely Chinese, Spanish, English, Hindi, Arabic, Bangla and Portuguese [G. F. Simons and C. D. Fennig (eds.), Ethnologue: Laguage of the Americas and the Pacific, Twentieth Edn. (SIL Internatinal, 2017)]. The system uses linear predictive coefficients-based feature, i.e. the line spectral pair-grade ratio (LSP-GR) feature, and ensemble learning for classification. Experiments were performed on more than 200 h of real-world YouTube data and the highest possible accuracy of 96.95% was received. The results can be compared with other machine learning classifiers.																	0218-0014	1793-6381				JUN 15	2020	34	6							2058006	10.1142/S0218001420580069													
J								Bearing Operating State Evaluation Based on Improved HMM	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Bearing; state evaluation; HMM; Fault Localization Matrix; Monitoring Index Vector; Genetic algorithm	HIDDEN MARKOV MODEL	With the development of industry, the fault diagnosis requirements for rolling bearings are getting higher and higher. This paper aims to develop low-complexity solutions for bearing fault diagnosis. In this paper, we use wavelet decomposition to obtain gesture Monitoring Index Vector (MIVs), after this, an improved Hidden Markov Model (HMM) algorithm was proposed for bearing fault diagnosis, in which we apply the Genetic Algorithm (GA) to avoid the convergence to local optimum, thus improving the recognition performance. The experimental results on 11 groups of test datasets demonstrate that the proposed algorithm (GAHMM) can achieve a higher average recognition rate of 93%, 87%, 87%, 93%, 93%, 97%, 100%, 97%, 97%, 100%, 97%.																	0218-0014	1793-6381				JUN 15	2020	34	6							2059016	10.1142/S0218001420590168													
J								Vehicle License Plate Recognition Based on Wavelet Transform and Vertical Edge Matching	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Wavelet transform; vertical edge matching; vehicle license plate recognition; neural network		With the improvement of our country's economic level and quality of life, the numbers and scales of highway networks and motor vehicles are constantly expanding, which makes the current road traffic burden more and more serious. As an important means of traffic automation management, license plate recognition (LPR) technology plays an important role in traffic surveillance and control. However, the recognition rate and accuracy of the traditional license plate recognition methods still need to be improved. In the case of poor surrounding environment, it is prone to localization failure, vehicle license plate recognition errors or unrecognizable phenomena. Wavelet transform, as another landmark signal processing method after Fourier transform, has been widely used in the field of image processing. In China, the number of horizontal lines is usually larger than that of vertical lines. If the two vertical boundaries of the license plate can be detected successfully, the four angles of the license plate can be determined efficiently to complete the license plate positioning. In view of the advantages of wavelet transform technology and the characteristics of vehicle license plate, in this paper, a vehicle license plate recognition algorithm based on wavelet transform and vertical edge matching is proposed. The edge of the license plate is detected by wavelet transform technology, and then the license plate is located by vertical edge matching technology. After the location is realized, the characters are segmented by vertical projection method and the characters are recognized by improved BP neural network algorithm. The experimental results show that the proposed vehicle license plate recognition algorithm based on wavelet transform and vertical edge matching performs well in algorithm performance, which provides a good reference for the development of vehicle license plate recognition system.																	0218-0014	1793-6381				JUN 15	2020	34	6							2050016	10.1142/S0218001420500160													
J								Image Feature Extraction and Object Recognition Based on Vision Neural Mechanism	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Neurological vision; image processing; image feature extraction; object recognition	TEXTURE CLASSIFICATION	As an important branch of artificial intelligence, computer vision plays a huge role in the rapid development of artificial intelligence. From a biological point of view, in the acquisition and processing of information, vision is much more important than hearing, touch, etc., because 70% of the human cerebral cortex is processing visual information. Therefore, advances in computer vision technology are critical to the development of artificial intelligence that is designed to allow machines to think and handle things like humans. The acquisition and processing of visual information has always been the focus of computer vision research, and it is also difficult. The main problem of traditional computer vision technology in the processing of visual information is that the extracted image features are less discriminative, the generalization ability of image features in complex background scenes is insufficient, and the recognition ability on object recognition is poor. In response to these problems, based on the visual neural mechanism, this paper establishes an appropriate computer model for the neuronal cells in the human primary visual cortex, models the recognition response mechanism of the visual ventral system, and performs image feature extraction on the training samples. And object recognition. The results show that compared with the traditional methods, the proposed method effectively improves the discrimination of image features, and the image features extracted under complex background scenes have good generalization ability. On this basis, the training samples can be effectively recognized. The results show that the model based on the visual neural mechanism, the recognition of the edge, orientation and contour of the training sample show the advantages of the biological vision mechanism in object recognition.																	0218-0014	1793-6381				JUN 15	2020	34	6							2054017	10.1142/S0218001420540178													
J								Application Research of KNN Algorithm Based on Clustering in Big Data Talent Demand Information Classification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Clustering; KNN algorithm; artificial intelligence; information classification	FEATURE-SELECTION	With the growth of massive data in the current mobile Internet, network recruitment is gradually growing into a new recruitment channel. How to effectively mine available information in the massive network recruitment data has become the technical bottleneck of current education and social supply and demand development. The renewal of talent demand information is carried out every day, which produces a large amount of text data. How to manage these talents' demand information reasonably becomes more and more important. Artificial classification is time-consuming and laborious, which is unrealistic naturally. Therefore, using automatic text categorization technology to classify and manage this information becomes particularly important. To break through the bottleneck of this technology, a heuristic KNN text categorization algorithm based on ABC (artificial bee colony) is proposed to adjust the weight of features, and the similarity between test observation and training observation is measured by using the method of fuzzy distance measurement. Firstly, the recruitment information is segmented and feature selection and noise data elimination are carried out by using term frequency-inverse document frequency (TF-IDF) algorithm and AP (affinity propagation) clustering algorithm. Finally, the text information is classified by using KNN algorithm combined with heuristic search and fuzzy distance measurement. The experimental results show that this method effectively solves the problem of poor stability and low classification accuracy of traditional KNN algorithm in text categorization method for talent demand.																	0218-0014	1793-6381				JUN 15	2020	34	6							2050015	10.1142/S0218001420500159													
J								Research on Digital Camouflage Pattern Generation Algorithm Based on Adversarial Autoencoder Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Camouflage design; adversarial autoencoder; main color extraction; spatial color blending		In the past, most of the digital camouflage used textural features to extract the configuration features of spots in gray images, unable to effectively utilize the position relationship between color information. In order to overcome this shortcoming, a new digital camouflage pattern design model was proposed based on the model of adversarial autoencoder network. Firstly, the complexity and performance of several main color extraction algorithms were analyzed and compared, and combined with AFK-MC2 algorithm and color similarity coefficient, a fast camouflage main color clustering method was proposed. Then a deep convolution adversarial autoencoder network was designed to extract and describe the configuration features of the spots in background pattern. In order to diffuse pixel spot and achieve the effect of spatial color blending, a morphological processing algorithm was proposed to process the generated camouflage patterns. Finally, two sets of grassland and woodland datasets were established, respectively. The influence of the number of latent variables of network on the training process was tested on the dataset, and the number of camouflage feature descriptions was determined to be greater than or equal to 10. In order to verify the effectiveness of the generated camouflage, the spots in background region and target region were randomly selected, and the Euclidean distance between the feature parameters of these spots was calculated. Both the visual and experimental results demonstrate that the generated spots have high fusion with the background.																	0218-0014	1793-6381				JUN 15	2020	34	6							2050017	10.1142/S0218001420500172													
J								Research on Sensitivity of Speckle Center Coordinate Values by Contour and Background Noise and Elimination Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Speckle contour; algorithm; ellipse fitting; edge and background noise	SEGMENTATION; DIAMETER; TUMOR	The accuracy of measuring the target object displacement is greatly influenced by the offset of central coordinate value in a laser speckle contour (CCVLSC) due to the defects obtained in background or on measuring surface, as a measuring combination of both monocular vision and laser speckle is used. In this paper, the theoretical principle of displacement measurement is first presented by a combination of monocular vision and laser speckle. Then, a model between object displacement and CCVLSC (particularly, Y coordinate value) is derived. Finally, a denoising algorithm with competitive protection of contour effective points is proposed, on the basis of effects of noises coming from background and contour edge on CCVLSC. The algorithm includes ellipse fitting to laser speckle contour, calculating offsets between all contour points and the fitted eclipse, eliminating noise points with higher deviation (generally about 5% of all contour points) by using competitive strategy, ellipse refitting, and recalculating and re-eliminating until the deviation is below a specified threshold. It is shown that the algorithm can not only eliminate the fixed noise points in each round but also protect the number of effective points to the greatest extent. Finally, the feasibility of the algorithm is verified by two ways. One is an ideal data validation. It proves that the algorithm can guarantee the convergence towards the ideal center coordinate value. Another is an experimental verification. An experimental system is built up based on the relationship between object displacement and Y coordinate value of CCVLSC for obtaining relevant dada. It is shown by the comparison between predictions and experimental data that the algorithm has a better robustness and a higher accuracy of distance measurement than other typical algorithms.																	0218-0014	1793-6381				JUN 15	2020	34	6							2055014	10.1142/S0218001420550149													
J								Research on Data Mining Algorithm Based on Pattern Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Data mining technology; pattern recognition; artificial intelligence; support vector machine		With the advent of the era of big data, people are eager to extract valuable knowledge from the rapidly expanding data, so that they can more effectively use these massive storage data. The traditional data processing technology can only achieve basic functions such as data query and statistics, and cannot achieve the goal of extracting the knowledge existing in the data to predict the future trend. Therefore, along with the rapid development of database technology and the rapid improvement of computer's computing power, data mining (DM) came into existence. Research on DM algorithms includes knowledge of various fields such as database, statistics, pattern recognition and artificial intelligence. Pattern recognition mainly extracts features of known data samples. The DM algorithm using pattern recognition technology is a better method to obtain effective information from massive data, thus providing decision support, and has a good application prospect. Support vector machine (SVM) is a new pattern recognition algorithm proposed in recent years, which avoids dimension disaster by dimensioning and linearization. Based on this, this paper studies the DM algorithm based on pattern recognition, and proposes a DM algorithm based on SVM. The algorithm divides the vector of the SV set into two different types and iterates through multiple iterations to obtain a classifier that converges to the final result. Finally, through the cross-validation simulation experiment, the results show that the DM algorithm based on pattern recognition can effectively reduce the training time and solve the mining problem of massive data. The results show that the algorithm has certain rationality and feasibility.																	0218-0014	1793-6381				JUN 15	2020	34	6							2059015	10.1142/S0218001420590156													
J								Improved Damage Characteristics Identification Method of Concrete CT Images Based on Region Convolutional Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Faster R-CNN; concrete CT images; feature pyramid network (FPN); ROI align	X-RAY CT	The detection of internal damage characteristics of concrete is an important aspect of damage evolution mechanism in concrete meso-structure. In this paper, the improved Faster R-CNN is used to detect the porosity and cracks in concrete CT images. Based on the Faster R-CNN, ResNet-101 and ResNet-50 are used as the main framework. Feature pyramid network (FPN) and ROI Align are introduced to improve the performance of the model. FPN can generate high quality feature maps. ROI Align solves the region mismatch caused by the quantization operation. Experiments show that the detection accuracy of ResNet-101 + FPN + ROI Align reaches 87.08%, which is 4.74 higher than that of ResNet-101. The detection accuracy of ResNet-50 + FPN + ROI Align reached 81.36%, which is 3.12% points higher than ResNet-50. These two improved algorithms are slower than the original algorithm for the detection time of a single picture. An effective method is provided to analyze concrete meso-damage evolution through the research.																	0218-0014	1793-6381				JUN 15	2020	34	6							2054018	10.1142/S021800142054018X													
J								Simulation for response surface in the HPLC optimization method development using artificial intelligence models: A data-driven approach	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Artificial intelligence; Validation; HPLC; Amiloride; Methyclothiazide; Retention factor	NEURAL-NETWORK; LIQUID-CHROMATOGRAPHY; RETENTION; PREDICTION; REGRESSION; ALGORITHM	In this paper, three different data-driven algorithms were employed including two nonlinear models (Artificial neural network (ANN) and Adaptive neuro-fuzzy inference system (ANFIS)) and a classical linear model (Multilinear regression analysis (MLR)) for the simulation of response surface for methyclothiazide (M) and amiloride (A) considered as (K'or k) modeling in HPCL using pH and composition of mobile phase (methanol) as the corresponding input variables. The experimental and simulated results were evaluated based on five different performance efficiency criteria namely; determination coefficient (R-2), root mean square error (RMSE), correlation coefficient (R), mean square error (MSE) and mean absolute percentage error (MAPE). The obtained results demonstrated the promising ability of ANN and ANFIS over MLR models with average R-values of 0.95 in both training and testing phases. The results also indicated that, with regard to the percentage error, ANN and ANFIS models outperformed the MLR model and increased the accuracy up to 6% and 8%, respectively for K' (M) simulation, while for K' (A), ANFIS increased the accuracy up to 5% and 4% for MLR and ANN, respectively. The overall results proved the reliability of artificial intelligence models (ANN and ANFIS) for the simulation of response surface optimization method.																	0169-7439	1873-3239				JUN 15	2020	201								104007	10.1016/j.chemolab.2020.104007													
J								Multivariate limit of detection for non-linear sensor arrays	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Detection limit; Figures of merit; Detection threshold; Ion-selective electrodes; Sensor array	ION-SELECTIVE ELECTRODES; IUPAC RECOMMENDATIONS; MEMBRANE-ELECTRODES; ANALYTICAL FIGURES; QUANTIFICATION; CALIBRATION; MERIT; UNCERTAINTY; REGRESSION; UNIVARIATE	With the increased development of low-cost and miniature devices, sensors are increasingly being deployed as arrays of redundant sensors. However, little work has been done characterizing properties of these arrays. Here, we develop and test a Bayesian algorithm for estimating the limit of detection of sensor arrays. The algorithm is applicable for single sensors as well as sensor arrays, and works by reducing a vector in the signal domain to a univariate response in the measurand domain. We show that the new algorithm can reproduce results from a benchmark algorithm for single sensors, and then demonstrate the benefit of adding additional sensors to an array. Then, we provide guidelines that achieve numerical stability while minimising computational cost. Finally, we provide a real-world example using an array of ion-selective electrodes measuring carbonate in seawater. This application demonstrates how incorporation of a set of individual low-quality sensors into an array leads to a substantially reduced LOD that clearly meets the demands of the application.																	0169-7439	1873-3239				JUN 15	2020	201								104016	10.1016/j.chemolab.2020.104016													
J								Monitoring wine fermentation deviations using an ATR-MIR spectrometer and MSPC charts	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Wine; Alcoholic fermentation; ATR-MIR; MSPC; Process analytical technologies (PAT); Quality control	PROCESS ANALYTICAL TECHNOLOGY; ATTENUATED TOTAL REFLECTANCE; INFRARED-SPECTROSCOPY; STATE; NIR	Despite the winemaker's efforts, deviations such as bacterial spoilage can occur during wine alcoholic fermentation resulting in economic losses and low quality wines. When a deviation is suspected, samples are usually sent to an oenological laboratory for the off-line analysis of specific quality control parameters. The use of ATR-MIR as a fast analytical tool to monitor the fermentation process could be very useful, as getting real-time information of the process allows making readjustments before the process ends. In this study, we aimed at detecting white wine spoilage during alcoholic fermentation due to the action of lactic bacteria using a portable ATR-MIR instrument and MSPC charts. A total of 33 small-scale alcoholic fermentations were conducted (25 in normal operation conditions (NOC) and 8 simulating a bacterial spoilage with the addition of lactic bacteria (MLF)) to evaluate the capability of the MSPC charts to detect deviations from NOC. MSPC control charts were developed based on Q residuals and Hotelling's T-2 statistics. Time-wise unfolding was applied to the original three-way data to build different PCA models, obtaining very satisfactory results: MLF samples were detected before the end of alcoholic fermentation in the Q residuals charts after 80 hours and Hotelling T-2 chart could also differentiate the samples after 100 hours.																	0169-7439	1873-3239				JUN 15	2020	201								104011	10.1016/j.chemolab.2020.104011													
J								Chemometrics modelling of temporal changes of ozone half hourly concentrations in different monitoring stations	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Chemometrics; Multivariate curve resolution; MCR-ALS; Multilinear modelling; Ozone pollution; Environmental monitoring and modelling	PRINCIPAL COMPONENT ANALYSIS; CURVE RESOLUTION; MCR-ALS; TROPOSPHERIC OZONE; POLLUTION PATTERNS; LEAST-SQUARES; PERIOD; CATALONIA; EXTENSION; SPAIN	Principal Component Analysis and Multivariate Curve Resolution (MCR) chemometrics soft modelling approaches are proposed for the simultaneous analysis of the half hourly tropospheric ozone concentrations over a continuous period of six years at three different monitoring stations. The proposed approach is based on the direct multilinear modelling of experimental data without needing any prior physicochemical, geographical nor meteorological assumptions. By application of the proposed modelling approach, a summarized description of the changes of ozone concentrations due to a reduced number of contributions is achieved. Each one of these contributions is described by a set of diurnal, seasonal, year and location profiles whose interpretation is then possible using the available knowledge. The use of the proposed chemometrics approach can be generalized for the analysis and interpretation of environmental big data sets generated in continuous on-line monitoring of multiple environmental parameters over long time periods and for a large number of monitoring stations.																	0169-7439	1873-3239				JUN 15	2020	201								104015	10.1016/j.chemolab.2020.104015													
J								Comparison of the performance of multiclass classifiers in chemical data: Addressing the problem of overfitting with the permutation test	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Pattern recognition; Glass; Wine; Overfitting; Permutation test	ACCURACY; SELECTION; MACHINE; MODELS; KAPPA	The objective of this work was to apply different pattern recognition techniques in datasets-i.e., the Glass Identification Dataset and the Wine Quality Dataset-commonly used as a chemometric study of cases. In this paper, three types of different classification models were used. The first type was based on discriminant analysis and other linear classification models such as Linear Discriminant Analysis (LDA), Regularized Discriminant Analysis (RDA), Mixture Discriminant Analysis (MDA), and Partial Least Squares Discriminant Analysis (PLS-DA). The second type was based on nonlinear classification models such as Artificial Neural Networks (ANN), Support Vector Machine (SVM) with a radial kernel function, k-Nearest Neighbors (k-NN), Naive Bayes (NB), and Learning Vector Quantization (LVQ). The last type was based on classification trees and rule-based models such as Classification and Regression Tree (CART), Bagging, Random Forest (RF), C5.0, and Generalized Boosted Machine (GBM). The obtained results outperformed the classification concerning works previously published in the literature. The computational experiments show that the LVQ was the one method able to classify all three datasets correctly. The permutation tests were applied to evaluate the occurrences of the overfitting problem. The results showed that the overfitting problem was absent, which was confirmed by the pairwise Wilcoxon signed-rank test.																	0169-7439	1873-3239				JUN 15	2020	201								104013	10.1016/j.chemolab.2020.104013													
J								Prediction of nanofluids viscosity using random forest (RF) approach	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Relative viscosity; Nanofluid; Nanoparticle; RF; SVR; MLP	WATER-BASED NANOFLUIDS; THERMAL-CONDUCTIVITY ENHANCEMENT; ARTIFICIAL NEURAL-NETWORK; GLYCOL-BASED NANOFLUIDS; ETHYLENE-GLYCOL; HEAT-TRANSFER; PARTICLE-SIZE; RHEOLOGICAL BEHAVIOR; RELATIVE VISCOSITY; DYNAMIC VISCOSITY	Accurate estimation of viscosity, one of the most important thermo-physical properties of nanofluids, is essential in heat transfer fluid applications in many industries. In this paper, for the first time, the random forest (RF), a robust artificial intelligence method is utilized to accurately estimate the viscosity of Newtonian nanofluids. To develop the model five input parameters were used, namely the temperature, solid volume fraction, viscosity of the base fluid, nanoparticle size, and density of nanoparticle. Further, 2890 datasets were collected from 50 references representing a wide range of experimental settings. The model's predictive performance was assessed against those of a multilayer perceptron (MLP) model, a support vector regression (SVR) and various classical and empirical models. The models' performance were analyzed using various statistical performance indicators and graphical plots. Performance criteria assessment, using the testing dataset, showed that the RF model provided the best prediction of the viscosity of nanofluids (R = 0.989, RMSE = 0.139, MAPE = 4.758%) in comparison to those of the MLP (R = 0.915, RMSE = 0.377, MPE = 16.194%) and the SVR (R = 0.941, RMSE = 0.315, MAPE = 7.895%). Moreover, a sensitivity analysis demonstrated that the volume fraction and density of nanoparticles were the most and second most significant factors affecting the viscosity of nanofluid, respectively.																	0169-7439	1873-3239				JUN 15	2020	201								104010	10.1016/j.chemolab.2020.104010													
J								Domain adaptive partial least squares regression	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multivariate calibration; Domain adaptive; Partial least squares regression; Hilbert-schmidt independence criterion; Calibration transfer	CALIBRATION TRANSFER; SPECTRA; NIR; STANDARDIZATION; MAINTENANCE; SELECTION; MIXTURES; PLS	In practical applications, the problem of training- and test-samples from different distributions is often encountered, such as instruments or external environmental factors change when measuring the data. Therefore, a multivariate calibration model established, based on the training set needs to be adaptive to meet the requirements of test samples from different domains. Extracting domain adaptive latent variables is an effective method to address such issues, but related studies about making multivariate calibration models adaptive to different domains in unsupervised methods are rarely reported. In this paper, a domain adaptive partial least squares regression is proposed, which uses the Hilbert-Schmidt independence criterion to evaluate the independence of the extracted latent variables and domain labels. In both the original space and reproducing kernel Hilbert space, the proposed method can obtain a closed form of the domain adaptive projection vector, which ensures the high efficiency of the model calculation. The validity of the proposed method is verified by several simulation data sets and real near-infrared spectral data sets. The experimental results show that the proposed method has good universally for unsupervised regression calibration transfer.																	0169-7439	1873-3239				JUN 15	2020	201								103986	10.1016/j.chemolab.2020.103986													
J								Noise level penalizing robust Gaussian process regression for NIR spectroscopy quantitative analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Noise level penalizing; Near-infrared spectroscopy; Quantitative analysis; Robust Gaussian process	LEAST-SQUARES REGRESSION; INTERNAL QUALITIES; SELECTION; PLS	In Near-infrared (NIR) spectroscopy qualitative analysis, noise caused data quality problem has been a bottleneck to further enhance the prediction accuracy. Appropriate preprocessing methods can reduce the influence of noise; and robust models have higher tolerance for noise disturbance. However, these methods treat all the wavelengths equally. In fact, the spectra at different wavelengths may have highly different level of noise. This paper presents a new noise-level-penalizing robust Gaussian process (NLP-RGP) regression for NIR spectroscopy quantitative analysis. The novel noise level penalizing mechanism penalize the spectra features according to their noise level, i.e., encourage the model to prefer the less noisy features over high noisy features. Gaussian process (GP) is a nonparametric machine learning method based on kernel and Bayesian inference framework; with a noise model of heavy-tailed distribution, robust Gaussian process can handle the abnormal sample data better. Experiments were taken on the determination of the total soluble solids content of navel oranges based on their surface NIR spectra. The NLP-GP outperforms the robust Gaussian process model and least squares support vector machines (LS-SVM), the state of art method. Moreover, the NLP-RGP performs even better than the NLP-GP, achieving the best prediction accuracy among all the models. This demonstrates the effectiveness of noise level penalizing mechanism, and the noise level penalizing mechanism and robust mechanism of Gaussian process can be integrated together well.																	0169-7439	1873-3239				JUN 15	2020	201								104014	10.1016/j.chemolab.2020.104014													
J								Chemometrics characterization of The Llobregat river dissolved organic matter	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										River water; Dissolved organic matter (DOM); UV spectrophotometry; Excitation-emission matrix (EEM); Molecular fluorescence spectroscopy; Multivariate Curve Resolution-Alternating Least Squares (MCR-ALS)	MULTIVARIATE CURVE RESOLUTION; PARALLEL FACTOR-ANALYSIS; EMISSION MATRIX FLUORESCENCE; EXCITATION; MARINE; TOOL; ENVIRONMENTS; SPECTROSCOPY; WATERS; CDOM	The potential use of multivariate chemometric methods for the investigation of the dissolved organic matter (DOM) sources in surface river waters monitored by UV absorbance and excitation-emission fluorescence spectroscopy is shown. The Multivariate Curve Resolution-Alternating Least Squares (MCR-ALS) method is applied to the analysis of the excitation-emission multidimensional data matrices acquired at ten different river sampling sites along the basin of the Llobregat river (located at the north-west of the Iberian Peninsula), from its origin to its mouth, during three different monitoring campaigns. The results obtained by MCR-ALS provided information about the compositional changes of the different DOM sources (i.e. humic- , fulvic- or protein-like sources) present in the investigated river samples, about their site distribution along the river basin and about their changes in three different monitoring campaigns. Eight different MCR-ALS components were resolved describing several types of DOM, with different excitation and emission spectra and with distinct distribution profiles along the river sampling sites. A comparative study of each single sampling site describing the changes in their DOM composition during the three monitoring campaigns was also performed.																	0169-7439	1873-3239				JUN 15	2020	201								104018	10.1016/j.chemolab.2020.104018													
J								Impact of the pretreatment of ATR-FTIR signals on the figures of merit when PLS is used	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Pretreatment data; ATR-FTIR; PLS; Oxybenzone; Desirability function; Figures of merit	SPECTROSCOPY	The efficiency of the analytical methods based on vibrational spectroscopy has been widely verified in a high number of publications. In addition, it has been recognized that the pretreatment of the original signals is absolutely necessary to obtain enough quality in the subsequent classification and/or regression models. In fact, an inappropriate pretreatment makes the results worse. It is also impossible to give "a priori" rules that guarantee the adequacy of a pretreatment for specific data. The effect of the pretreatments is evaluated through their impact on the quality of the classification and/or regression models built from them due to the double dependence (on the data and on the purpose of the analysis). The effect of the pretreatment has been evaluated using partial least squares regression (PLSR) in some works and the root mean squares in prediction or in cross-validation has been always used as a criterion to evaluate the regression in all these cases. However, it seems appropriate to use quality criteria of the calibration of the analytical method through the figures of merit the significance of the regression, the absence of constant or proportional bias, the residual standard deviation, the mean of the absolute values of the relative errors and the capability of detection. In this work, the use of these analytical criteria in a desirability function is proposed for the first time with calibration data of oxybenzone obtained by ATR-FTIR and PLSR. This desirability function enables to choose the best pretreatment among the 39 possibilities studied. In addition, it is shown that the same optimum is not obtained if the minimum of RMSEC_CV is considered as a criterion.																	0169-7439	1873-3239				JUN 15	2020	201								104006	10.1016/j.chemolab.2020.104006													
J								Chemometrics in comprehensive two-dimensional liquid chromatography: A study of the data structure and its multilinear behavior	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Comprehensive; Two-dimensional liquid chromatography; Multilinear; Multiset; Multivariate curve resolution	MULTIVARIATE CURVE RESOLUTION; TANDEM MASS-SPECTROMETRY; DIODE-ARRAY; MCR-ALS; SEPARATION; QUANTIFICATION; STRATEGIES; EXTRACT; ACID	Comprehensive multidimensional chromatographic techniques, such as GCxGC coupled to FID or MS and LC x LC coupled to UV or MS, have gained popularity in recent years. From the analytical perspective, these techniques allow obtaining higher peak capacities and resolution power, as well as adding selectivity from the second orthogonal dimension. From the chemometric point of view, these multidimensional techniques generate highly complex datasets, which present several challenges for their analysis. On the one hand, the selection of the appropriate chemometric data analysis tool requires the understanding of the underlying data structure and its multilinear behavior. On the other hand, peak resolution in complex samples is still a challenge, because of their possible overlapping in one or two chromatographic dimensions despite the increased resolution power. In this work, a comprehensive two-dimensional liquid chromatography method hyphenated simultaneously to PDA and MS detectors was employed for the analysis of a mixture of 31 pharmaceutical compounds. Chemometric evaluation of the obtained two-dimensional chromatograms focuses on two different goals. First, the assessment of the multilinear behavior of the high-dimensional data for each of the two detection modes (LC x LC-UV and LC x LC-MS) and, also, for the multiset data obtained by fusion of the data coming from both detectors. In addition, the chemometric resolution of peaks of overlapping compounds was evaluated using the multivariate curve resolution alternating least squares (MCR-ALS) method. Finally, the advantages of data fusion from UV and MS detectors were discussed, such as the increased ability for compound identification.																	0169-7439	1873-3239				JUN 15	2020	201								104009	10.1016/j.chemolab.2020.104009													
J								Determination of pure alcohols surface tension using Artificial Intelligence methods	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Artificial intelligence methods; Neural networks; Support vector machine; Surface tension; Alcohols	SHUFFLED COMPLEX EVOLUTION; BINARY-MIXTURES; NEURAL-NETWORK; APPLICABILITY DOMAIN; INTERFACIAL-TENSIONS; TEMPERATURE; OPTIMIZATION; PREDICTION; DENSITY; MODEL	Reliable Artificial Intelligence methods are developed here for the determination of the surface tension of alcohols in a wide range of temperatures. A total amount of 3063 data was considered for 149 alcohols. Three methods were applied: multilayer perceptron neural network, radial basis neural network and, least-squares support-vector machine (LSSVM). Three different optimization algorithms were used for the multilayer perceptron and the support-vector machine, whereas only one was needed in the case of the radial basis neural network. So, seven different Artificial Intelligence methods were considered, trained and tested. Combination of 9 different input properties, their inverses, and their natural logarithms were studied, and the final input set chosen includes 12 of them. The obtained results were widely studied by using both graphical and common statistical tools. The applicability domain was obtained and different sensitivity analyses were performed. The method giving the lowest averaged deviations is the LSSVM optimized with Cuckoo Optimization Algorithm, the Absolute Average Relative Deviation being 0.61%.																	0169-7439	1873-3239				JUN 15	2020	201								104008	10.1016/j.chemolab.2020.104008													
J								Optimization and comparison of models for prediction of soluble solids content in apple by online Vis/NIR transmission coupled with diameter correction method	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Apple; Soluble solids content; Diameter correction method; Online detection; Fruit orientation	NEAR-INFRARED SPECTROSCOPY; VARIABLE SELECTION; NONINVASIVE DETERMINATION; NIR SPECTROSCOPY; SUGAR CONTENT; RANDOM FROG; SPECTRA; QUALITY; FRUIT; COMPENSATION	The online system can achieve high efficiency towards fruit quality determination in postharvest period. Thus, developing online and nondestructive technology for inspecting and grading fruit is meaningful and profitable in the existing robotic sorting systems. In this study, the effect of fruit diameter differences on online prediction of soluble solids content (SSC) of 'Fuji' apples based on visible and near-infrared (Vis/NIR) spectroscopy was studied. Partial least square (PLS) regression was employed to establish calibration models based on three wavelength regions (675-1025, 710-980, 750-1025 nm) and two fruit orientations (stem-calyx axis vertical with stem upward (T1) and stem-calyx axis horizontal with stem towards light source (T2)), respectively. A novel diameter correction method was proposed to reduce the effect of fruit diameter differences on original spectra. Combined with pretreatment and effective wavelength (EWs) selection methods, models were optimized and compared to determine the best calibration strategy. Diffuse transmission spectra in 710-980 nm and diameter correction method with calculated attenuation coefficient were testified much better than other corresponding regions and correction methods, respectively. Baseline offset correction (BOC) and 7-point Savitzky-Golay smoothing (SGS) of pretreatments and competitive adaptive reweighted sampling (CARS) of EWs selection methods were proved to be outstanding among other methods. 59 and 63 EWs achieved the best detection accuracies with correlation coefficient of prediction (r(p)) and root mean square error of prediction (RMSEP) of 0.92 and 0.50 degrees Brix, 0.89 and 0.56 degrees Brix for T1 and T2, respectively. The overall results indicated that online Vis/NIR transmission spectra after BOC and 7-SGS with proposed diameter correction method can make the variation of fruit diameters a small interference for SSC determination, and CARS-PLS would be effective to simplify models and promote computing efficiency to make this nondestructive detection technique promisingly applied.																	0169-7439	1873-3239				JUN 15	2020	201								104017	10.1016/j.chemolab.2020.104017													
J								Stacking ensemble based deep neural networks modeling for effective epileptic seizure detection	EXPERT SYSTEMS WITH APPLICATIONS										Epileptic seizure; Electroencephalography signals; Stacking approach; Deep neural networks; K-fold cross-validation; Performance improvement	EEG SIGNALS; FEATURE-EXTRACTION; CLASSIFICATION; TRANSFORM	Electroencephalography signals obtained from the brain's electrical activity are commonly used for the diagnosis of neurological diseases. These signals indicate the electrical activity in the brain and contain information about the brain. Epilepsy, one of the most important diseases in the brain, manifests itself as a result of abnormal pathological oscillating activity of a group of neurons in the brain. Automated systems that employed the electroencephalography signals are being developed for the assessment and diagnosis of epileptic seizures. The aim of this study is to focus on the effectiveness of stacking ensemble approach based model for predicting whether there is epileptic seizure or not. So, this study enables the readers and researchers to examine the proposed stacking ensemble model. The benchmark clinical dataset provided by Bonn University was used to assess the proposed model. Comparative experiments were conducted by utilizing the proposed model and the base deep neural networks model to show the effectiveness of the proposed model for seizure detection. Experiments show that the proposed model is proven to be competitive to base DNN model. The results indicate that the performance of the epileptic seizure detection by the stacking ensemble based deep neural networks model is high; especially the average accuracy value of 97.17%. Also, its average sensitivity with 93.11% is superior to the base DNN model. Thus, it can be said that the proposed model can be included in an expert system or decision support system. In this context, this system would be precious for the clinical diagnosis and treatment of epilepsy. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113239	10.1016/j.eswa.2020.113239													
J								A feature selection algorithm for intrusion detection system based on Pigeon Inspired Optimizer	EXPERT SYSTEMS WITH APPLICATIONS										Feature selection; Intrusion Detection System; KDDCUP; Pigeon Inspired Optimizer	SWARM OPTIMIZATION; COLONY	Feature selection plays a vital role in building machine learning models. Irrelevant features in data affect the accuracy of the model and increase the training time needed to build the model. Feature selection is an important process to build Intrusion Detection System (IDS). In this paper, a wrapper feature selection algorithm for IDS is proposed. This algorithm uses the pigeon inspired optimizer to utilize the selection process. A new method to binarize a continuous pigeon inspired optimizer is proposed and compared to the traditional way for binarizing continuous swarm intelligent algorithms. The proposed algorithm was evaluated using three popular datasets: KDDCUP99, NLS-KDD and UNSW-NB15. The proposed algorithm outperformed several feature selection algorithms from state-of-the-art related works in terms of TPR, FPR, accuracy, and F-score. Also, the proposed cosine similarity method for binarizing the algorithm has a faster convergence than the sigmoid method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113249	10.1016/j.eswa.2020.113249													
J								Accelerating the Miller-Tucker-Zemlin model for the asymmetric traveling salesman problem	EXPERT SYSTEMS WITH APPLICATIONS										Traveling salesman; Asymmetric traveling salesman problem (ATSP); Miller-Tucker-Zemlin (MTZ); Subtour elimination constraints (SECs)	ALGORITHM; SEARCH; CONSTRAINTS	In this article, we present an easy to implement algorithmic approach that improves the computational performance of the Miller-Tucker-Zemlin (MTZ) model for the asymmetric traveling salesman problem (ATSP) by efficiently generating valid inequalities from fractional solutions. Computational experiments show that the proposed approach enhances considerably the performance of MTZ-based formulations reported in the literature. By adding facet-defining inequalities of the underline ATSP-polytope, the number of nodes in the branch-and-bound tree is drastically reduced, and the convergence of the MTZ-type formulations is accelerated. We also extend this idea to solve the multiple asymmetric traveling salesman problem (mATSP). This approach can help practitioners to solve real-life problems to near optimality using a standard optimization solver and may be useful to solve a variety of routing problems that use MTZ-type of subtour elimination constraints. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113229	10.1016/j.eswa.2020.113229													
J								Nonnegative matrix factorization for link prediction in directed complex networks using PageRank and asymmetric link clustering information	EXPERT SYSTEMS WITH APPLICATIONS										Link prediction; Nonnegative matrix factorization; PageRank; Asymmetric link clustering	MISSING LINKS; RECONSTRUCTION	The aim of link prediction is to predict missing links in current networks or new links in future networks. Almost all the existing directed link prediction algorithms only take into account the links direction formation but ignored the abundant network topological information such as local and global structures. Therefore, how to preserve both local and global structure information is an important issue for directed link prediction. To solve this problem, in this paper, we are motivated to propose a novel Nonnegative Matrix Factorization via Asymmetric link clustering and PageRank model, namely NMF-AP. Specifically, we utilize the PageRank algorithm to calculate the influence score of the node, which captures the global network structure information. While we employ the asymmetric link clustering method to calculate the link clustering coefficient score, which preserves the local network structure information. By jointly optimizing them in the nonnegative matrix factorization model, our model can preserve both the local and global information at the same time. Besides, we provide an effective the multiplicative updating rules to learn the parameter of NMF-AP. Extensive experiments are conducted on ten real-world directed networks, experiment results demonstrate that the method NMF-AP outperforms state-of-the-art link prediction methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113290	10.1016/j.eswa.2020.113290													
J								The damped oscillator model (DOM) and its application in the prediction of emotion development of online public opinions	EXPERT SYSTEMS WITH APPLICATIONS										Online public opinion; Opinion dynamics; Social networks; Emotion simulation; Mass-Spring-Damper system	SOCIAL MEDIA; NETWORK ANALYSIS; EXPERT-SYSTEM; PROPAGATION; GOVERNANCE; ALGORITHM; TOPOLOGY; DYNAMICS; CRISIS; LEADER	Online public opinions refer to the attitude of the public towards certain events or topics in social media and have informative significance for social governance and the formulation of public policies. Previous studies have proved the important role played by online public opinions in expert systems from which the prediction of online public opinions has drawn increasing attention from scholars. Though the development of public opinion events is largely driven by the emotions of users, few studies have regarded the issues of emotion development forecasting, and existing emotional dynamic models are relatively simple, not real-time predicting, and with no considerations of people's self-emotion-changing mechanisms. In order to fill this gap, this paper proposes a Damped Oscillator Model (DOM). Compared to existing models, the proposed model has two main merits: first, the Mass-Spring-Damper system in the physics area is used to measure the self-decaying process of human emotion which has not been included in previous opinion dynamics models; second, a self-emotional adaptation mechanism is introduced and final states with no obviously established opinions can be reached, which is common in real online public opinion cases. Simulation experiments are conducted to discuss the impacts of critical parameters contained in the model on opinion dynamics. Two real online public opinion events were studied using the proposed model, with critical parameters extracted by the particle swarm optimization algorithm. The predictive results have outperformed several previous well-known models. With its real-time predictive capacity, this model can provide substantial auxiliary support to the expert and intelligent system for making wise decisions in advance, especially during fast developing public opinion crises. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113268	10.1016/j.eswa.2020.113268													
J								The evolution of knowledge navigator model: The construction and application of KNM 2.0	EXPERT SYSTEMS WITH APPLICATIONS										KM maturity model; Knowledge navigator model; Big data; KM performance	INDUSTRIAL BIG DATA; MANAGEMENT PERFORMANCE; INTEGRATING TALENT; MATURITY MODEL; STRATEGIES; INNOVATION; BUSINESS; SYSTEMS; CHALLENGES; CUSTOMER	The knowledge management (KM) maturity model provides a framework against which both old and new KM initiatives can be assessed to determine whether they are capable of generating new knowledge. Knowledge Navigator Model (KNM) proposed in 20 09 (Hsieh et al., 2009) has been promoted in Taiwan to assist organizations to evaluate their KM status, and also aided to be a diffusion platform for government, academic and practice to exchange their KM experience. However, for the past 10 years, the industrial environment has been constantly changing that has brought the developing of KM practice. This study described the evolution of KNM to become KNM 2.0 in terms of construction and application. Qualitative and quantitative research methods were conducted to construct the proposed three modules of KNM 2.0. A 139 cases survey was employed and the results reveal the applicability of KNM 2.0. Our research contributes to the body of concepts on KM maturity models that could evaluate the readiness of service-oriented knowledge economy, big data and smart factory, and strategic KM performance. The proposed KNM 2.0, with these features which are novel and rapidly expanding fields, has been developed as an online Knowledge Management Evaluation (KM evaluation) website available for the practice to use to better understand their overall value brought by contemporary KM. Moving forward, by using KNM 2.0 to continually collect the data from the industries in Taiwan, it is expected to obtain more information about the practical KM that keeps developing with the trends, and also keep playing the role to distribute the contemporary concepts of KM. The KM experience could be referenced and the methodology could be applied to other countries. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113209	10.1016/j.eswa.2020.113209													
J								Multi-objective flexibility-complexity trade-off problem in batch production systems using fuzzy goal programming	EXPERT SYSTEMS WITH APPLICATIONS										Multi-objective optimization; Fuzzy goal programming; Multi-choice goal programming; Type-II fuzzy sets; Production system; Production flexibility-complexity	MANUFACTURING FLEXIBILITY; ASPIRATION LEVELS; MODEL	Several factors change the flexibility and the complexity of production systems. The flexibility of the production system means to meet the changing needs of the customers. As flexibility increases the complexity also increases. In this paper, a multi-objective linear programming is proposed to model the trade-off-of the complexity and flexibility of a batch production system (BPS). Strategic, tactical and operational decision variables have been considered. Seven objective functions of the proposed model are assumed as flexibility and complexity. Sixteen tactical decision variables are defined to determine the level of the dimensions of flexibilities and complexities. Thirty-four operational decision variables are defined to tune the shop-floor operations. Several sets of constraints considering aspects of flexibility and complexity as well as the conditions of batch production systems have been considered. As the achievement to the objective functions is not possible simultaneously, and there is no unique and concise relation between these objective functions in a typical batch production system so, a fuzzy goal programming (FGP) approach is proposed to solve the model. Moreover, goal programming (GP), Fuzzy GP, multi-choice goal programming (MCGP) and fuzzy MCGP are proposed and used to compare the performance of solution procedures. The superior solution approach among GP, MCGP, FGP, and FMCGP is FMCGP which concurrently considers several aspiration levels for objective functions, maximization of the achievement level of objective functions, and satisfying uncertain preference of fuzzy objectives. An evolutionary algorithm, called non-dominated sorting genetic algorithm (NSGA-II) and a random weighted version of FMCGP are customized to regenerate several non-dominated designs for flexibility-complexity trade-off-problem in BPS. The results are promising and the proposed model is capable to set the strategic, tactical and operational variables of a BPS. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113266	10.1016/j.eswa.2020.113266													
J								Discrete symbiotic organisms search method for solving large-scale time-cost trade-off problem in construction scheduling	EXPERT SYSTEMS WITH APPLICATIONS										Large-scale construction project; Deadline constraint; Time-cost trade-off; Discrete symbiotic organisms search	GENETIC ALGORITHM; OPTIMIZATION ALGORITHM	Construction projects are becoming increasingly larger and more complex in terms of size and cost. An optimization tool is necessary for the construction management system to develop the desired construction schedule to save time and cost. However, only a few efforts have been made to deal with the time-cost trade-off problem (TCTP) in the large-scale construction projects, and the existing optimization methods are slightly limited by the trouble of parameter tuning. As TCTP is known to be an NP-hard problem, this paper aims to introduce a new variant of Symbiotic Organisms Search (SOS) algorithm that does not contain control parameters, called DSOS (Discrete Symbiotic Organisms Search) which generates the parasite organism using a heuristic rule based on the network levels. This enhancement helps to improve the exploration phase and avoid premature stagnation. Performances are evaluated on project instances with different numbers of activities varying from 180 to 6300, as well as nine newly generated project instances with 720 activities but different network structures. The obtained results show a good performance of DSOS in terms of robustness and deviation from optimum in comparison with other meta-heuristics and variants of DSOS without using the heuristic rule. The good performance implies that DSOS is sufficient to serve as an effective tool to generate an optimized construction schedule. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				JUN 15	2020	148								113230	10.1016/j.eswa.2020.113230													
J								Predicting sentence-level polarity labels of financial news using abnormal stock returns	EXPERT SYSTEMS WITH APPLICATIONS										Financial news; Expert systems; Natural language processing; Multi-instance learning; Decision-making	INFORMATION-CONTENT; TEXTUAL ANALYSIS; SENTIMENT; MARKET; DISCLOSURE	Expert systems for automatic processing of financial news commonly operate at the document-level by counting positive and negative term-frequencies. This, however, limits their usefulness for investors and financial practitioners seeking specific positive and negative information on a more fine-grained level. For this purpose, this paper develops a novel machine learning approach for the prediction of sentence-level polarity labels in financial news. The method uses distributed text representations in combination with multi-instance learning to transfer information from the document-level to the sentence-level. This has two key advantages: (1) it captures semantic information of the textual data and thereby prevents the loss of information caused by bag-of-words approaches; (2) it is solely trained based on historic stock market reactions following the publication of news items without the need for any kind of manual labeling. Our experiments on a manually-labeled dataset of sentences from financial news yield a predictive accuracy of up to 71.20%, exceeding the performance of alternative approaches significantly by at least 5.10 percentage points. Hence, the proposed approach provides accurate decision support for investors and may assist investor relations departments in communicating their messages as intended. Furthermore, it presents promising avenues for future research aiming at studying communication patterns in financial news. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113223	10.1016/j.eswa.2020.113223													
J								Dimensionality reduction for multi-criteria problems: An application to the decommissioning of oil and gas installations	EXPERT SYSTEMS WITH APPLICATIONS										Oil and gas; Decommissioning; Dimensionality reduction; Feature selection; Machine learning; Multi-criteria decision analysis	DECISION-ANALYSIS; NEURAL-NETWORK; OFFSHORE OIL; CLASSIFICATION; SELECTION; SUPPORT; MANAGEMENT; INDUSTRY; SYSTEMS; IMPACTS	This paper is motivated by decommissioning studies in the field of oil and gas, which comprise a very large number of installations and are of interest to a large number of stakeholders. Generally, the problem gives rise to complicated multi-criteria decision aid tools that rely upon the costly evaluation of multiple criteria for every piece of equipment. We propose the use of machine learning techniques to reduce the number of criteria by feature selection, thereby reducing the number of required evaluations and producing a simplified decision aid tool with no sacrifice in performance. In addition, we also propose the use of machine learning to explore the patterns of the multi-criteria decision aid tool in a training set. Hence, we predict the outcome of the analysis for the remaining pieces of equipment, effectively replacing the multi-criteria analysis by the computational intelligence acquired from running it in the training set. Computational experiments illustrate the effectiveness of the proposed approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113236	10.1016/j.eswa.2020.113236													
J								Aspect-based sentiment analysis using adaptive aspect-based lexicons	EXPERT SYSTEMS WITH APPLICATIONS										Aspect-based; Sentiment analysis; Opinion mining; Lexicon generation; Genetic algorithm	PRODUCT FEATURE-EXTRACTION; MACHINE; NETWORK	Reviews expressed in e-commerce websites have formed an important source of information for both consumers and enterprises. Text sentiment analysis approaches aim to detect the sentiments of written reviews in order to achieve a better understanding of public opinion towards entities. Aspect-based sentiment analysis deals with capturing sentiments expressed towards each aspect of entities. A common approach in sentiment analysis problems is to take advantage of lexicons to generate features for classification of reviews. Existing aspect-based approaches fail to properly adapt general lexicons to the context of aspect-based datasets which results in reduced performance. To address this problem, this paper proposes extensions of two lexicon generation methods for aspect-based problems; one using statistical methods, and another using a genetic algorithm presented in our previous works. The aforementioned lexicons are then fused with prominent static lexicons to classify the aspects in reviews; this outperforms our previous works according to the t-test results (with a p-value of less than 0.001). Experimental results indicate that the proposed approach outperforms baseline methods in aspect-based polarity classification on Bing Liu's customer review datasets and improves precision, recall and F-measure by 6.0, 1.0, and 7.4 percentage points respectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113234	10.1016/j.eswa.2020.113234													
J								GNM: GridCell navigational model	EXPERT SYSTEMS WITH APPLICATIONS										Gridcell; Navigation; Exploration; Robotic; Computational model; Path-planning	REAL-TIME; SIMULTANEOUS LOCALIZATION; SPATIAL REPRESENTATION; PATH-INTEGRATION; CELLS; SEARCH; CODE; MAP	It has been shown that grid cell firing patterns in the medial entorhinal cortex, can be used as a mapping reference for spatial navigation in mice and other mammalian species. In this paper, we propose a novel computational model for patterns of grid cells and combine it with a mechanism to tune the weights of cells, which we use to create a decision-making process for robot navigation. The method is used as an unsupervised method for uninformed online search with unknown goal positions and unknown environments, such as finding the exit out of a maze or help a robot to find its way in a jungle where there is no clue about the exit. The results of this approach in simulated and real environments show superior algorithmic steps over current search methods. In addition, the typical size of the memory can be reduced without compromising completeness of the method. Our results show that the number of steps is stable in terms of variations in memory allocations. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113217	10.1016/j.eswa.2020.113217													
J								Learning alternative ways of performing a task	EXPERT SYSTEMS WITH APPLICATIONS										Task learning; Inductive learning; Process mining; Identifying strategies	ACTIVITY RECOGNITION; ASSISTANCE; DATASET; VIDEO; HOME	A common way of learning to perform a task is to observe how it is carried out by experts. However, it is well known that for most tasks there is no unique way to perform them. This is especially noticeable the more complex the task is because factors such as the skill or the know-how of the expert may well affect the way she solves the task. In addition, learning from experts also suffers of having a small set of training examples generally coming from several experts (since experts are usually a limited and expensive resource), being all of them positive examples (i.e. examples that represent successful executions of the task). Traditional machine learning techniques are not useful in such scenarios, as they require extensive training data. Starting from very few executions of the task presented as activity sequences, we introduce a novel inductive approach for learning multiple models, with each one representing an alternative strategy of performing a task. By an iterative process based on generalisation and specialisation, we learn the underlying patterns that capture the different styles of performing a task exhibited by the examples. We illustrate our approach on two common activity recognition tasks: a surgical skills training task and a cooking domain. We evaluate the inferred models with respect to two metrics that measure how well the models represent the examples and capture the different forms of executing a task showed by the examples. We compare our results with the traditional process mining approach and show that a small set of meaningful examples is enough to obtain patterns that capture the different strategies that are followed to solve the tasks. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113263	10.1016/j.eswa.2020.113263													
J								Developing a deep learning framework with two-stage feature selection for multivariate financial time series forecasting	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Multivariate financial time series; Forecasting; Feature selection; Multi-objective optimization	NEURAL-NETWORKS; STOCK; SYSTEM; ALGORITHM; PREDICTION; PRICES; MODEL	Intelligent financial forecasting modeling plays an important role in facilitating investment-related decision-making activities in financial markets. However, accurate multivariate financial time series forecasting remains a challenge due to its complex nonlinear pattern. Aiming to fill the gap in the field, a novel forecasting framework, based on a two-stage feature selection model, deep learning model, and error correction model, is presented in this study, aiming at effectively capturing the nonlinearity inherent in multivariate financial time series. Concretely, the proposed two-stage feature selection model is utilized to determine the optimal feature set to further improve the generalization of the proposed deep learning model based on three deep learning units. Meanwhile, the error correction model is used to correct the forecasts and improve the accuracy further. To validate the performance of the forecasting framework, the case studies and the corresponding sensitivity analysis are carried out, consequently demonstrating its superiority, as compared to 16 benchmarks considered. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113237	10.1016/j.eswa.2020.113237													
J								Group-of-features relevance in multinomial kernel logistic regression and application to human interaction recognition	EXPERT SYSTEMS WITH APPLICATIONS										Human interaction recognition (HIR); Group-of-features relevance; Multinomial kernel logistic regression (MKLR)	GROUP LASSO; SELECTION; SEGMENTATION; MODEL	We propose an approach for human interaction recognition (HIR) in videos using multinomial kernel logistic regression with group-of-features relevance (GFR-MKLR). Our approach couples kernel and group sparsity modelling to ensure highly precise interaction classification. The group structure in GFR-MKLR is chosen to reflect a representation of interactions at the level of gestures, which ensures more robustness to intra-class variability due to occlusions and changes in subject appearance, body size and viewpoint. The groups consist of motion features extracted from tracking interacting persons joints over time. We encode group sparsity in GFR-MKLR through relevance weights reflecting each group (gesture) discrimination capability between different interaction categories. These weights are automatically estimated during GFR-MKLR training using gradient descent minimisation. Our model is computationally efficient and can be trained on a small training dataset while maintaining a good generalization and interpretation capabilities. Experiments on the well-known UT-Interaction dataset have demonstrated the performance of our approach by comparison with state-of-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113247	10.1016/j.eswa.2020.113247													
J								Applying Dempster-Shafer theory for developing a flexible, accurate and interpretable classifier	EXPERT SYSTEMS WITH APPLICATIONS										Supervised learning; Expert systems; Gradient descent; Dempster-Shafer theory; Interpretability	RULE	Two approaches have traditionally been identified for developing artificial intelligence systems supporting decision-making: Machine Learning, which applies general techniques based on statistical analysis and optimization methods to extract information from a large amount of data looking for possible relations among them, and Expert Systems, which codify experts knowledge in rules, which are then applied to a specific situation. One of the main advantages of the first approach is its greater accuracy and wider generality for the application of the methods developed which can be used in various scenarios. By contrast, expert systems are usually more restricted and often applicable only to the domain for which they were originally developed. However, the machine learning approach requires the availability of large chunks of data, and it is much more complicated to interpret the results of the statistical methods to obtain some explanation of why the system decides, classifies, or evaluates a situation in a certain way. This issue may become very important in areas such as medicine, where it is relevant to know why the system recommends a certain treatment or diagnoses a certain illness. Likewise, in the financial sector, it might be legally required to explain that a decision to reject the granting of a mortgage loan to a person is not due to discriminatory causes such as gender or race. In order to be able to have interpretability and extract knowledge of available data we developed a classification method based on Dempster-Shafer's Plausibility Theory. Mass assignment functions (MAF) must be established to apply this theory and they assign a weight or probability to all subsets of the possible outcomes, given the presence of a certain fact on a decision scenario. Thus MAF assignments encode expert knowledge. The method learns optimal values for the weights of each MAF using the Gradient Descent method. The presented method allows combination of MAF which have been generated by the method itself or defined by an expert with those that are derived from a set of available data. The developed method was first applied to controlled scenarios and traditional data sets to ensure that classifications and explanations are correct. Results show that the model can classify with an accuracy which is comparable to other statistical classification methods, being also able to extract the most important decision rules from the data. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113262	10.1016/j.eswa.2020.113262													
J								Development of an integrated decision making model for location selection of logistics centers in the Spanish autonomous communities	EXPERT SYSTEMS WITH APPLICATIONS										Logistics; Rough set theory; DEA; FUCOM; CoCoSo	FUZZY; TOPSIS; DEMATEL; ANP	Logistics centers are those areas where all the national and international logistics and transportation operations are managed and directed to various business operations. One of the essential elements of an urban development system is to identify the appropriate location for a logistics center. In practice, the issue of evaluating and selecting the most suitable geographical area for a logistics center is considered as a complex decision making problem that can be well formulated through analytical and mathematical models. An exhaustive review of literature indicates that no concrete study has still proposed any integrated evaluation approach for logistics center selection. Thus, this paper aims in developing a two-stage decision making model to find out the most preferred zone in the autonomous communities of Spain for establishment of logistics centers. In the first stage, the considered communities are compared based on five evaluation criteria using data envelopment analysis (DEA) to identify the efficient and inefficient alternatives. In the second stage, a model is designed to evaluate the performance of the efficient communities using rough full consistency (R-FUCOM) and combined compromise solution (R-CoCoSo) methods. The adopted model allows capturing the uncertainty and vagueness in the decision makers' judgments as involved in the evaluation process with the use of rough set theory (RST). The R-FUCOM method is utilized to obtain the optimal weights of the criteria, while R-CoCoSo method is finally used to rank the efficient communities. In addition, sensitivity analyses are performed to validate the robustness of the derived results. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113208	10.1016/j.eswa.2020.113208													
J								Discriminative dimensionality reduction for sensor drift compensation in electronic nose: A robust, low-rank, and sparse representation method	EXPERT SYSTEMS WITH APPLICATIONS										Sensor drift, electronic nose; Dimensionality reduction; Domain adaptation; Transfer learning; Low-rank and sparse representation	CALIBRATION TRANSFER; GENERAL FRAMEWORK; GAS; SIGNAL; CLASSIFICATION; RECOGNITION; SYSTEM	Sensor drift, which is a critical issue in the field of sensor measurements, has plagued the sensor community in the past several decades. How to tackle the sensor drift problem using expert and intelligent systems has gained increasing attention. Most sensor drift compensation methods ignore the sparse and low-rank characteristics of sensor signals. In this paper, we propose a discriminative dimensionality reduction method for sensor drift compensation in the electronic nose. The proposed method consists of four major components. (1) The distribution discrepancy between source data and target data is alleviated via projecting all the data into a common subspace. (2) A sparse and low-rank reconstruction coefficient matrix is employed to preserve the global and local structures of sensor signals. (3) An error matrix is introduced to deal with outliers. (4) The source data label information is taken into consideration to avoid overlapping of samples with different labels in the common subspace. The formulated minimization problem with constraints can be solved in an iterative manner. The effectiveness of the proposed method has been verified by conducting experiments on two sensor drift datasets. The proposed method may provide new insights into the gas sensor drift compensation systems or other expert and intelligent systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113238	10.1016/j.eswa.2020.113238													
J								Characterizing basal-like triple negative breast cancer using gene expression analysis: A data mining approach	EXPERT SYSTEMS WITH APPLICATIONS										Gene expression; Basal-like breast cancer; Triple-negative breast cancer; Data mining	SUPPORT VECTOR MACHINES; FEATURE-SELECTION; NEURAL-NETWORKS; MICROARRAY DATA; CELL-MIGRATION; SURVIVABILITY; ASSOCIATION; IMPROVEMENT; PROGNOSIS; DIAGNOSIS	Triple-negative breast cancer (TNBC) is characterized by the absence of expression of the estrogen receptor, progesterone receptor and human epidermal growth factor receptor 2 (HER2). Therefore, TNBC is unresponsive to targeted hormonal therapies, which limits treatment options to nonselective chemotherapeutic agents. Basal-like breast cancers (BLBCs) represent a subset of about 70% of TNBCs, more frequently affecting younger patients, being more prevalent in African-American women and significantly more aggressive than tumors of other molecular subtypes, with high rates of proliferation and extremely poor clinical outcomes. Proper classification of BLBCs using current pathological tools has been a major challenge. Although TNBCs have many BLBC characteristics, the relationship between clinically defined TNBC and the gene expression profile of BLBC is not fully examined. The purpose of this study is to process publicly-available TNBC gene expression datasets generated by Affymetrix gene chips and define a set of genes, or gene signature, that can classify TNBC samples between BLBC and Non-BLBC subtypes. We used over 3500 breast cancer gene expression profiles from several individual publicly available datasets and extracted Affymetrix gene expression data for 580 TNBC cases. Several popular data mining methods along with dimensionality reduction and feature selection techniques were applied to the resultant dataset to build predictive models to understand molecular characteristics and mechanisms associated with BLBCs and to classify them more accurately according to important features extracted through microarray data analysis of BLBC and Non-BLBC cases. Our result can lead to proper identification and diagnosis of BLBCs, which can potentially direct clinical implications by dictating the most effective therapy. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113253	10.1016/j.eswa.2020.113253													
J								Group teaching optimization algorithm: A novel metaheuristic method for solving global optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Group teaching; Swarm intelligence; Global optimization; Engineering design	PARTICLE SWARM OPTIMIZATION; LEARNING-BASED OPTIMIZATION; ENGINEERING OPTIMIZATION; DIFFERENTIAL EVOLUTION; CUCKOO SEARCH	In last 30 years, many metaheuristic algorithms have been developed to solve optimization problems. However, most existing metaheuristic algorithms have extra control parameters except the essential population size and stopping criterion. Considering different characteristics of different optimization problems, how to adjust these extra control parameters is a great challenge for these algorithms in solving different optimization problems. In order to address this challenge, a new metaheuristic algorithm called group teaching optimization algorithm (GTOA) is presented in this paper. The proposed GTOA is inspired by group teaching mechanism. To adapt group teaching to be suitable for using as an optimization technique, without loss of generality, four simple rules are first defined. Then a group teaching model is built under the guide of the four rules, which consists of teacher allocation phase, ability grouping phase, teacher phase and student phase. Note that GTOA needs only the essential population size and stopping criterion without extra control parameters, which has great potential to be used widely. GTOA is first examined over 28 well-known unconstrained benchmark problems and the optimization results are compared with nine state-of-the-art algorithms. Experimental results show the superior performance of the proposed GTOA for these problems in terms of solution quality, convergence speed and stability. Furthermore, GTOA is used to solve four constrained engineering design optimization problems in the real world. Simulation results demonstrate the proposed GTOA can find better solutions with faster speed compared with the reported optimizers. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUN 15	2020	148								113246	10.1016/j.eswa.2020.113246													
J								Extreme Learning Machine for Supervised Classification with Self-paced Learning	NEURAL PROCESSING LETTERS										Classification; Extreme learning machine; Self-paced learning; Accuracy		The extreme learning machine (ELM), a typical machine learning algorithm based on feedforward neural network, has been widely used in classification, clustering, regression and feature learning. However, the traditional ELM learns all samples at once, and sample weights of traditional methods are defined before the learning process and they will not change during the learning process. So, its performance is vulnerable to noisy data and outliers, finding a way to solve this problem is meaningful. In this work, we propose a model of self-paced ELM named SP-ELM for binary classification and multi-classification originated from the self-paced learning paradigm. Concretely, the algorithm takes the importance of samples into account according to the loss of predicted value and real value, and it establishes the model from the simple samples to complex samples. By setting certain restrictions, the influence of complex data on the model is reduced. Four different self-paced regularization terms are adopted in the paper to select the instances. Experimental results demonstrate the effectiveness and of the proposed method by comparing it with other improved ELMs.																	1370-4621	1573-773X															10.1007/s11063-020-10286-9		JUN 2020											
J								Gaussian hybrid fuzzy clustering and radial basis neural network for automatic brain tumor classification in MRI images	EVOLUTIONARY INTELLIGENCE										MRI images; Brain tumor; Segmentation; Fuzzy c-means clustering; Radial basis neural network classifier	SEGMENTATION; ALGORITHM; SELECTION	Magnetic Resonance Imaging (MRI) is an emerging research area, employed extensively in radiology for the diagnosis of various neurological diseases. Here, as the classification of MRI gains significant importance for the tumor diagnosis, various techniques are developed in the literature for the automatic classification of MRI images. In this paper, a clustering scheme, namely gaussian hybrid fuzzy clustering (GHFC) is developed by hybridizing the fuzzy c-means (FCM) clustering and sparse FCM along with the Gaussian function for the segmentation purpose. After segmenting the image, the suitable features are extracted from the image and given to the Exponential cuckoo based Radial Basis Neural Network (Exponential cuckoo based RBNN) classifier. The features serve as the training information for the Exponential cuckoo based RBNN classifier, and it finally detects the training class. Simulation of the proposed work is done using BRATS and SIMBRATS databases, and the results are compared with several state-of-art techniques. Simulation results depict that the proposed GHFC along with the RBNN classifier achieved improved accuracy and mean squared error results with the values of 0.8952, and 0.0074, respectively, for the BRATS dataset and 0.8719 and 0.0036, for the SIMBRATS dataset.																	1864-5909	1864-5917															10.1007/s12065-020-00433-5		JUN 2020											
J								Experimental and comparative analysis of various solar PV module technologies using module level inverter topologies at south-India for the context-aware application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Power optimizers; Polycrystalline; Mono crystalline; Roof-top PV system	PERFORMANCE; MONOCRYSTALLINE	The PV module structure has observed many improvements in the structure design ever since the days of its origin, so that the performance and durability of PV Module is improved over the period of years. A PV module has a multi-layered structure-an anodized aluminum frame ensures rigidity to the module by coupling together all the components tightly. As solar energy is growing, the demand for electricity is reduced and it is the most prominent technology in the upcoming generations. The solar energy is used to convert the irradiation (i.e. light) into electricity. At present, the most promising energy source in India is the solar energy and the government is also taking major policies to implement the solar PV system. For electrical performance evaluation, an online monitoring tool was used, along with module temperature detector (resistance temperature detector) and solar irradiance detector (silicon reference cell). The temperature and irradiance data make it easier for the conversion of the measured I-V curve to standard test conditions (STC), so that these can then be compared with the name plate data of the module and the average degradation rates (difference of name plate data and measured data divided by the number of years the module was installed in field). Also, at some sites, the site was also equipped with tools like Multi-meter, ambient temperature and humidity sensor tilt angle meter (to measure the inclination angle of the module with respect to horizontal) and compass (to know the direction in which the modules are facing). In this study, we have reviewed a 2.2 kW solar PV system for the context-aware application with eight different PV modules for three different seasons namely summer, rainy and autumn seasons prevailing in south India from January to December. The system was monitored regularly for 24 h using monitoring portal. Sensors are being used to monitor the Coimbatore site's irradiance, temperature and wind conditions and calculate performance ratio (PR). Results from the study shows that the power generation from 2.2 kW solar PV systems has higher performance in summer when compared to any other seasons in Coimbatore. From the above reliability study the best performed panel technology is Interdigited Back Contact under different conditions with different data analysis.																	1868-5137	1868-5145															10.1007/s12652-020-02193-7		JUN 2020											
J								Gene encoder: a feature selection technique through unsupervised deep learning-based clustering for large gene expression data	NEURAL COMPUTING & APPLICATIONS										Deep learning; Gene expression; Clustering; Unsupervised learning; Genetic algorithm	CANCER; CLASSIFICATION; FRAMEWORK; MODEL	Cancer is a severe condition of uncontrolled cell division that results in a tumor formation that spreads to other tissues of the body. Therefore, the development of new medication and treatment methods for this is in demand. Classification of microarray data plays a vital role in handling such situations. The relevant gene selection is an important step for the classification of microarray data. This work presents gene encoder, an unsupervised two-stage feature selection technique for the cancer samples' classification. The first stage aggregates three filter methods, namely principal component analysis, correlation, and spectral-based feature selection techniques. Next, the genetic algorithm is used, which evaluates the chromosome utilizing the autoencoder-based clustering. The resultant feature subset is used for the classification task. Three classifiers, namely support vector machine,k-nearest neighbors, and random forest, are used in this work to avoid the dependency on any one classifier. Six benchmark gene expression datasets are used for the performance evaluation, and a comparison is made with four state-of-the-art related algorithms. Three sets of experiments are carried out to evaluate the proposed method. These experiments are for the evaluation of the selected features based on sample-based clustering, adjusting optimal parameters, and for selecting better performing classifier. The comparison is based on accuracy, recall, false positive rate, precision,F-measure, and entropy. The obtained results suggest better performance of the current proposal.																	0941-0643	1433-3058															10.1007/s00521-020-05101-4		JUN 2020											
J								Deep face recognition with clustering based domain adaptation	NEUROCOMPUTING										Face recognition; Unsupervised domain adaptation; Pseudo-label; Face clustering	REPRESENTATION	Despite great progress in face recognition tasks achieved by deep convolution neural networks (CNNs), these models often face challenges in real world tasks where training images gathered from Internet are different from test images because of different lighting condition, pose and image quality. These factors increase domain discrepancy between training (source domain) and testing (target domain) database and make the learnt models degenerate in application. Meanwhile, due to lack of labeled target data, directly fine-tuning the pre-learnt models becomes intractable and impractical. In this paper, we propose a new clustering-based domain adaptation method designed for face recognition task in which the source and target domain do not share any classes. Our method effectively learns the discriminative target feature by aligning the feature domain globally, and, at the meantime, distinguishing the target clusters locally. Specifically, it first learns a more reliable representation for clustering by minimizing global domain discrepancy to reduce domain gaps, and then applies simplified spectral clustering method to generate pseudo-labels in the domain-invariant feature space, and finally learns discriminative target representation. Comprehensive experiments on widely-used GBU, IJB-A/B/C and RFW databases clearly demonstrate the effectiveness of our newly proposed approach. State-of-the-art performance of GBU data set is achieved by only unsupervised adaptation from the target training data. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						1	14		10.1016/j.neucom.2020.02.005													
J								Adaptive iterative learning consensus control for second-order multi-agent systems with unknown control gains	NEUROCOMPUTING										Unknown control gains; Multi-agent systems; Adaptive consensus control; Iterative learning; Neural networks	NONLINEAR-SYSTEMS; CONTROL DIRECTIONS; DESIGN; ALGORITHMS; LEADERLESS	We respond to the consensus control algorithms for second-order non-linear MAS with unknown control gains in the finite time interval by adopting iterative learning control (ILC) methods in this paper. Compared to the current findings, the control gains in the proposed method are unknown functions with unknown and non-identical signs. The topology graph between the follower agents is an undirected connected graph. By referencing the Nussbaum-type function and the neural networks, the adaptive control algorithms are intended to cope with the consensus control between the agents on the limited time interval. Simulation results illustrate the efficacy of the raised algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						15	26		10.1016/j.neucom.2020.01.108													
J								Unsupervised domain adaptation with adversarial learning for mass detection in mammogram	NEUROCOMPUTING										Mammography; Mass detection; Domain adaptation; Adversarial learning		Many medical image datasets have been collected without proper annotations for deep learning training. In this paper, we propose a novel unsupervised domain adaptation framework with adversarial learning to minimize the annotation efforts. Our framework employs a task specific network, i.e., fully convolutional network (FCN), for spatial density prediction. Moreover, we employ a domain discriminator, in which adversarial learning is adopted to align the less-annotated target domain features with the well-annotated source domain features in the feature space. We further propose a novel training strategy for the adversarial learning by coupling data from source and target domains and alternating the subnet updates. We employ the public CBIS-DDSM dataset as the source domain, and perform two sets of experiments on two target domains (i.e., the public INbreast dataset and a self-collected dataset), respectively. Experimental results suggest consistent and comparable performance improvement over the state-of-the-art methods. Our proposed training strategy is also proved to converge much faster. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						27	37		10.1016/j.neucom.2020.01.099													
J								Cooperative output regulation of heterogeneous linear multi-agent systems via fully distributed event-triggered adaptive control	NEUROCOMPUTING										Cooperative output regulation; Fully distributed control; Multi-agent systems; Adaptive event-triggered control	TIME; COMMUNICATION; CONSENSUS; PROTOCOLS	In this paper, we discuss the cooperative output regulation problem for a class of heterogeneous linear multi-agent systems under undirected communication topological graph. First, an adaptive event-triggered protocol is introduced to achieve the consensus problem for leader-following system. Then, two fully distributed event-triggered adaptive control protocols are designed. That is, the state feedback and dynamic output feedback controllers, which does not need global information about network topology of system. Under the proposed two control protocols, the cooperative output regulation problem can be solved and no Zeno behavior happens for each agent. Finally, a numerical simulation is used to corroborate the validity of the conclusions under two control laws. Moreover, a comparative experiment is carried out to highlight the advantages of the control protocol designed in this paper. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						38	45		10.1016/j.neucom.2019.12.047													
J								Event-triggered adaptive consensus tracking control for non-affine multi-agent systems	NEUROCOMPUTING										Multi-agent systems; Dynamic surface control; Event-triggered control; Error-transformation-based design	NONLINEAR-SYSTEMS; NEURAL-CONTROL	This paper presents the event-triggered consensus control problem for a class of multi-agent systems (MASs) in the non-affine form. An error-transformation-based design method is presented in the nonaffine MASs. The radial basis function neural networks (RBFNNs) are used to approximate the unknown nonlinear functions. The dynamic surface control (DSC) technique is adopted to obtain parameters of the virtual controller through a first-order filter, which can make the calculation easier. An event-triggered control strategy is proposed to reduce communication frequency. Furthermore, it is further shown that the consensus tracking errors are semi-globally uniformly ultimately bounded (SGUUB). Finally, simulation results illustrate the effectiveness of the designed controller. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						46	53		10.1016/j.neucom.2020.02.023													
J								MFENet: Multi-level feature enhancement network for real-time semantic segmentation	NEUROCOMPUTING										Real-time semantic segmentation; Feature enhancement		Accuracy is of vital importance for real-time semantic segmentation. However, modern methods often weaken high-level or low-level feature extraction to promote inference speed, thereby resulting in poor accuracy. In this paper, we present a Multi-level Feature Enhancement Network (MFENet) to enhance the feature extraction of each level in backbone. This approach can achieve high performance while maintaining high inference speed. We first rely on a Spatial and Edge Extraction Module with the Laplace Operator to improve the edge information extraction of low-level features. Next, we design a Context Boost Module to increase the context information inside each object of high-level features. Finally, we introduce the Selective Refinement Module to selectively combine the information from these two modules. Our network attained precise real-time segmentation results on Cityscapes, CamVid and COCO-Stuff datasets. More specifically, the architecture achieved 76.7% Mean IoU on the Cityscapes test dataset with 12.5 GFLOPS and a speed of 47 FPS on one NVIDIA Titan Xp card, which is more accurate than existing real-time methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						54	65		10.1016/j.neucom.2020.02.019													
J								HMM-based H-infinity state estimation for memristive jumping neural networks subject to fading channel	NEUROCOMPUTING										Hidden markov model; Fading channels; Markov jump memristive neural networks; H-infinity state estimation	SYSTEMS SUBJECT; EXPONENTIAL STABILITY; VARYING SYSTEMS; SYNCHRONIZATION; DISCRETE	In this paper, the H(infinity )state estimation problem of Markov jump memristive neural networks with fading channels is concerned by virtue of the hidden Markov model approach. The measurement transmission between the sensor and the filter is completed through fading channels which are described by a modified Rice fading model. The objective of the paper is to design a memristive filter such that, in the presence of fading channels, the effect of external disturbances on the error system is attenuated at a certain level and quantified by the H-infinity-norm in the mean square sense. By employing a mode-dependent Lyapunov-Krasovskii functional, some sufficient conditions are obtained to determine the gain matrices of the filter. Finally, a numerical example is provided to demonstrate the effectiveness of the main results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						66	75		10.1016/j.neucom.2020.02.016													
J								Penetrating the influence of regularizations on neural network based on information bottleneck theory	NEUROCOMPUTING										Neural network; Regularization; Information bottleneck theory; Human brain sensory system		Regularization is a very effective algorithm to solve overfitting problem in neural network, which improves the generalization ability of the model. However, their working mechanisms and the impact on the model performance have not been fully explored. In this paper, we study and analyze them using information bottleneck theory and one theory from human brain sensory system. We propose a metric to characterise the encoding length of hidden layers, named as AEntry value. Then, we implement extensive experiments on MNIST and FashionMNIST datasets with several commonly used regularization algorithms, and calculate the corresponding AEntry values. We analyze these results and obtain three conclusions. (1) The introduction of regularization influences the encoding of relative features with prediction task in neural network. The early stopping technique avoids introducing unrelated information with the task into the model by stopping the training process as an appropriate iterations. Laplace, Gaussian and Sparse Response regularizations compress the related representation and improve the performance of neural network by introducing the prior information into the model. In contrast, Dropout, Batch Normalization, and Layer Normalization increase the encoding length of features by adopting redundant representation to improve the performance. (2) The encoding of neural network does not satisfy the data processing inequality of information theory, which is mainly caused by redundant coding of extracted features. (3) The overfitting is caused by introducing irrelative information with the target. These results can give us insight into building more efficient regularization algorithm to improve the performance of neural network model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						76	82		10.1016/j.neucom.2020.02.009													
J								Principled reward shaping for reinforcement learning via lyapunov stability theory	NEUROCOMPUTING										Reinforcement learning; Principled reward shaping; Lyapunov stability theory; Stochastic approximation		Reinforcement learning (RL) suffers from the designation in reward function and the large computational iterating steps until convergence. How to accelerate the training process in RL plays a vital role. In this paper, we proposed a Lyapunov function based approach to shape the reward function which can effectively accelerate the training. Furthermore, the shaped reward function leads to convergence guarantee via stochastic approximation, an invariant optimality condition using Bellman Equation and an asymptotical unbiased policy. Moreover, sufficient RL benchmarks have been experimented to demonstrate the effectiveness of our proposed method. It has been verified that our proposed method substantially accelerates the convergence process as well as improves the performance in terms of a higher accumulated reward. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						83	90		10.1016/j.neucom.2020.02.008													
J								Correlation minimizing replay memory in temporal-difference reinforcement learning	NEUROCOMPUTING										Reinforcement learning; Temporal-difference learning; Replay memory; Artificial neural networks		Online reinforcement learning agents are now able to process an increasing amount of data which makes their approximation and compression into value functions a more demanding task. To improve approximation, thus the learning process itself, it has been proposed to select randomly a mini-batch of the past experiences that are stored in the replay memory buffer to be replayed at each learning step. In this work, we present an algorithm that classifies and samples the experiences into separate contextual memory buffers using an unsupervised learning technique. This allows each new experience to be associated to a mini-batch of the past experiences that are not from the same contextual buffer as the current one, thus further reducing the correlation between experiences. Experimental results show that the correlation minimizing sampling improves over Q-learning algorithms with uniform sampling, and that a significant improvement can be observed when coupled with the sampling methods that prioritize on the experience temporal difference error. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						91	100		10.1016/j.neucom.2020.02.004													
J								Sparse filtering based domain adaptation for mechanical fault diagnosis	NEUROCOMPUTING										Domain adaptation; Sparse filtering; Maximum mean discrepancy; Fault diagnosis	DEEP NEURAL-NETWORKS; FEATURE-EXTRACTION; KERNEL; MODEL	Recently, machine learning has achieved considerable success in the field of mechanical fault diagnosis. Nevertheless, in many real-world applications, the original vibration data usually collected under different work conditions which lead to large distribution divergences. As a result, the performances of many machine learning methods may drop dramatically. To overcome this deficiency, domain adaptation is introduced by adapting the regression model or classifier trained in the source domain for use in the distinct but related target domain. Particularly, a novel sparse filtering based domain adaptation approach (SFDA) is proposed for the mechanical fault diagnosis. Comparing with the previous researches, two main contributions of SFDA are concluded as follows: (1) the domain adaptation is applied to the sparse filtering algorithm. (2) The l1-norm and l2-norm are employed to the maximum mean discrepancy (MMD). (3) SFDA is easy to be implemented, and high classification accuracy can be obtained. The bearing and gear dataset are utilized to testify the validity and reliability of SFDA. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						101	111		10.1016/j.neucom.2020.02.049													
J								An artificial bee colony-based kernel ridge regression for automobile insurance fraud identification	NEUROCOMPUTING										Kernel ridge regression; Artificial bee colony algorithm; Global optimization ability; Automobile insurance fraud	EXTREME LEARNING-MACHINE; SHOP SCHEDULING PROBLEM; OPTIMIZATION; ALGORITHM; CLASSIFICATION	Automobile insurance fraud is gradually spreading in its global scope, and mining automobile insurance fraud is of increasing interest to society. There are few studies on the application of Kernel Ridge Regression (KRR) to insurance fraud. KRR has shown its good generalization performance in many fields. The performance of KRR depends on the parameters. KRR requires fewer parameters. In this paper, an Artificial Bee Colony (ABC) algorithm-based Kernel Ridge Regression, called KRR-ABC, is proposed for automobile insurance fraud detection. The global optimization ability of the ABC algorithm is used to optimize the parameter combination of KRR. It can quickly find the global optimal value without traversing all the grid points and improve the generalization ability of the model. The parallel computing is also used to enhance the computing speed of KRR-ABC algorithm. The performance of the KRR-ABC model is evaluated on eight benchmark data sets and compared with other methods. The experiment results show that the KRR-ABC model has faster run time and better generation performance. The KRR-ABC model is applied to detect automobile insurance fraud and the fraud rules are obtained. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 14	2020	393						115	125		10.1016/j.neucom.2017.12.072													
J								A scalable smart router architecture with intelligent adaptive routing and fault-tolerant management	NEUROCOMPUTING										Intelligent; Smart; Switch; Adaptive; Fault-tolerant	PERFORMANCE; INTERCONNECT; NETWORK	High-radix routers that switch and route packets across large-scale racks become increasingly important and can crucially determine the latency and bandwidth of the interconnection network for supercomputers and data centers. With the omnipresence of big data analysis there is a pressing need for scalable and self-diagnosing routers to construct more high-performance and reliable interconnect networks. This paper proposes an optimized aggregated-tile router micro-architecture with the reconfigurable multi-level routing and intelligent adaptive routing schemes to reduce the hardware requirement and optimize the router performance. The experiment results show that our approach achieves 98% throughput and maintains the same delay performance even better than YARC router while providing up to 40-50% reduction in memory consumption as well as global wire complexity. Moreover, by aggregating a built-in CPU and the intellectual network management structure, our router runs heuristic search algorithms to automatically reconstruct route tables for failure links or routers without any intervention of control plane, thereby implementing the automatical fault-tolerance and fault-recovery effects when maximizing the network performance. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 14	2020	393						126	141		10.1016/j.neucom.2017.12.073													
J								COB method with online learning for object tracking	NEUROCOMPUTING										Object tracking; CNN; PLK optical flow; Adaptive; Bayesian method	VISUAL TRACKING	Object tracking is a problem about semi-supervised learning with insufficient data set. In the field of military navigation and security of public life, it is widely used to take the place of human beings. In this paper, we come up with a new algorithm based on Bayesian, CNN and PLK optical flow, which is called COB method, for object tracking problems. With the idea of track-by-detect, we cascade CNN after PLK optical flow and integrate them in a Bayesian method. Most importantly our method is proposed with an adaptive integrating method to reduce the influence of over-fitting. The integrator also introduces the competition mechanism between tracker and detector, so that the algorithm is able to update the classifier with online learning. Besides, the regularization of deep learning is used to solve the blind spots of classifier. The experimental results show that the algorithm is more robust than the previous work. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						142	155		10.1016/j.neucom.2019.01.116													
J								Fuzzy approach in modeling static and fatigue strength of composite materials and structures	NEUROCOMPUTING										Fuzzy approach; Composite materials; Composite structures; Fatigue behavior; Experimental investigations; First-ply-failure; Fatigue life	SET ANALYSIS; DAMAGE; DESIGN; FRACTURE; PLATES	For composite materials and structures static strength, fatigue damage and durability demonstrate a scatter factor of results larger than for isotropic materials. To characterize it the fuzzy set approach is proposed. Two different mechanical descriptions of fatigue life are used in order to describe the uncertainty and randomness of parameters characterizing the fatigue damage and finally the fatigue durability. The theoretical predictions representing the lower and upper bounds of a fatigue life are compared with experimental data. In general, the present analysis shows that the fuzzy set description allows us to take into account much more parameters than classical deterministic or statistical methods. The scatter of the results is significantly dependent on the form of the used approximations of fatigue lives. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						156	164		10.1016/j.neucom.2018.12.094													
J								1.2 Watt Classification of 3D Voxel Based Point-clouds using a CNN on a Neural Compute Stick	NEUROCOMPUTING										3D Convolutional Neural Network; 3D object classification; Embedded systems; Low-power		With the recent surge in popularity of Convolutional Neural Networks (CNNs), motivated by their significant performance in many classification and related tasks, a new challenge now needs to be addressed: how to accommodate CNNs in mobile devices, such as drones, smartphones, and similar low-power devices? In order to tackle this challenge we exploit the Vision Processing Unit (VPU) that combines dedicated CNN hardware blocks and very low power requirement. The lack of readily available training data and memory requirements are two of the factors hindering the training and accuracy performance of 3D CNNs. In this paper, we propose a method for generating synthetic 3D point-clouds from realistic CAD scene models to enrich the training process for volumetric CNNs. Furthermore, an efficient 3D volumetric object representation Volumetric Accelerator format (VOLA) is employed. VOLA is a sexaquaternary (power-of-four subdivision) tree-based representation which allows for significant memory saving for volumetric data. Multiple CNN models were trained and pruning techniques for the weights were applied to the trained 3D Volumetric Network in order to remove almost 70% of the parameters and outperform the existing state-of-the-art networks. The top performing and efficient model was ported to the Movidius (TM) Neural Compute Stick (NCS). After deployment on the NCS, it takes 11 ms (similar to 90 frames per second) to perform inference on each input volume, with a reported power requirement of 1.2W, which leads to 75.75 inferences per second per Watt. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						165	174		10.1016/j.neucom.2018.10.114													
J								Construction of non-convex fuzzy sets and its application	NEUROCOMPUTING										Non-convex fuzzy sets; Parametric qualitative fuzzy set (PQ FS); Fuzzy logic system; Fuzzy logic controller	REDUCTION STRATEGIES; SYSTEMS	Although non-convex fuzzy set (FS) has the high potential of great performance in data modeling and controlling, it is seldom used and discussed because the lack of linguistic explanation and normative construction way. To address this problem, we propose a method named "parametric qualitative fuzzy set (PQ FS) plus choice strategy" for the construction and linguistic explanation of non-convex FS, in which PQ FS is a collection of convex FSs with special structure, and choice strategy is an approach to choose convex FSs from PQ FS. Based on this method, a non-convex FS is obtained as the trajectory of a collection of convex FS by choosing specific convex FS under specific situation. Thus, the linguistic explanation of non-convex FS is obtained: using non-convex FSs to represent linguistic variables does not violate the routine of using convex FSs, because it shows that the linguistic variable is just represented by different convex FS at different situation. Theorems are shown to demonstrate that the "PQ FS plus choice strategy" can effectively construct a non-convex FS. Furthermore, "Why a fuzzy logic system (FLS) adopting non-convex FSs may have a higher approximation capability" is discussed by introducing a parametric qualitative FLS (PQ FLS) that is compared with a typical Mamdani FLS as a function approximator. This indicates that non-convex FSs can approximate more extrema in a given universe with smaller partition numbers or fewer rules than convex FSs. Finally, the experimental results verify that a PQ FLS designed with the proposed non-convex FS construction method can outperform traditional convex fuzzy logic controllers (FLCs). Meanwhile, using parallel computing in the model training phase of PQ FLSs can reduce the calculation time compared to single-thread mode. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						175	186		10.1016/j.neucom.2018.10.111													
J								Non-unique decision differential entropy-based feature selection	NEUROCOMPUTING										Non-unique decision; Differentiation entropy; Feature selection	NEAREST-NEIGHBOR CLASSIFICATION; DIMENSIONALITY REDUCTION; ROUGH SETS; FUZZY; RISK	Feature selection plays an important role in reducing irrelevant and redundant features, while retaining the underlying semantics of selected ones. An effective feature selection method is expected to result in a significantly reduced subset of the original features without sacrificing the quality of problem-solving (e.g., classification). In this paper, a non-unique decision measure is proposed that captures the degree of a given feature subset being relevant to different categories. This helps to represent the uncertainty information in the boundary region of a granular model, such as rough sets or fuzzy-rough sets in an efficient manner. Based on this measure, the paper further introduce a differentiation entropy as an evaluator of feature subsets to implement a novel feature selection algorithm. The resulting feature selection method is capable of dealing with either nominal or real-valued data. Experimental results on both benchmark data sets and a real application problem demonstrate that the features selected by the proposed approach outperform those attained by state-of-the-art feature selection techniques, in terms of both the size of feature reduction and the classification accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						187	193		10.1016/j.neucom.2018.10.112													
J								Mining moving object gathering pattern based on Resilient Distributed Datasets and R-tree index	NEUROCOMPUTING										Gathering pattern; Resilient Distributed Datasets; MongoDB cluster; Cloud service		It is important to mine moving object gathering pattern, but the traditional algorithms cannot effectively analyze the trajectory data with mass scale. A parallel mining algorithm based on Resilient Distributed Datasets (RDD-Gathering) and R-tree index is proposed to discover the gathering pattern in the massive trajectory data, and the mining functions are provided as cloud services on Spark. MongoDB cluster is constructed to store massive trajectory data, R-tree index structure is used to store the trajectory data, and the RDD-Gathering algorithm interface is encapsulated with RESTful cloud service. The RDD-Gathering algorithm is implemented based on Apache Spark and performances are better than normal methods in real trajectory dataset testing. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						194	202		10.1016/j.neucom.2018.09.107													
J								A tourist walk approach for internal and external outlier detection	NEUROCOMPUTING										Outlier; Internal outlier; Tourist walk; Memory size; Critical memory size; Attractor; Crossing-attractor		Outlier detection is a fundamental task for knowledge discovery in data mining, especially in the Big Data era. It aims to detect data items that deviate from the general pattern of a given data set. In this paper, we present a new outlier detection technique using tourist walks starting from each data sample and varying the memory size. Specifically, a data sample gets a higher outlier score if it participates in few tourist walk attractors, while it gets a low score if it participates in a large number of attractors. Experimental results on artificial and real data sets show good performance of the proposed method. In comparison to classical outlier detection methods, the proposed one shows the following salient features: (1) It finds out outliers by identifying the structure of the input data set instead of considering only physical features, such as distance, similarity or density. (2) It can detect not only external outliers as classical methods do, but also internal outliers staying among various normal data groups. (3) By varying the memory size, the tourist walks can characterize both local and global structures of the data set. (4) A parallel implementation is quite convenient due to the nature of large amount of independent walking of the algorithm. (5) The proposed method is a deterministic technique. Therefore, only one run is sufficient, in contrast to stochastic techniques, which require many runs. Moreover, in this work, we find, for the first time, that tourist walks can generate complex attractors in various crossing shapes. Such complex attractors reveal data structures in more details. Consequently, it can improve the outlier detection performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						203	213		10.1016/j.neucom.2018.10.113													
J								A hybrid improved kernel LDA and PNN algorithm for efficient face recognition	NEUROCOMPUTING										Dimension reduction; Face recognition; Feature extraction; Kernel discriminant analysis; Probabilistic neural network	OPTIMIZATION; SPMV; GPU	This paper proposes a hybrid approach to face recognition based on a combination of probabilistic neural networks (PNNs) and improved kernel linear discriminant analysis (IKLDA). The dimensions of a sample's features are first of all reduced, whilst retaining its relevant information, A PNN method is then adopted to solve face recognition problems. The proposed IKLDA+PNN method not only improves the overall computing efficiency, but also its precision. Face recognition experiments conducted on the ORL, YALE and AR datasets, which contain a wide variety of facial expressions, facial details, and degrees of scale, were used to validate the feasibility of the IKLDA+PNN method. The results showed that it can obtain an average recognition accuracy of 97.22%, 83.8% and 99.12%, across the three datasets, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						214	222		10.1016/j.neucom.2019.01.117													
J								State-of-the-art quantum computing simulators: Features, optimizations, and improvements for D-GM	NEUROCOMPUTING										Quantum computing; Quantum computing simulation; Parallel processing	SPMV	Quantum computing is strongly limited by physical implementations of quantum computers. Currently, development of quantum computing algorithms is carried out by analytic or simulation procedures while quantum computers are not widely available. Although quantum computing simulation is parallel by nature, spatial and temporal complexity are major performance hazards. The exponential increase in memory and global access to quantum states in simulations limit not only the number of qubits but also quantum transformations. Considering these scenarios, six quantum simulators are studied, in order to find out their main features and simulation results considering implementations classified in single/multiple processor and accelerator architectures. The main strategies used to improve performance in these architectures provide relevant parameters for the development of the new extension of the Distributed Geometric Machine environment proposed in this paper. Shor's and Grover's algorithms are simulated and compared to previous D-GM results and to LIQUi vertical bar >'s simulator, showing improvements as relative speedups up to 22.2 x in relation to the previous version and up to 910.46 x when compared to LIQUi vertical bar >. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						223	233		10.1016/j.neucom.2019.01.118													
J								An intelligent clustering algorithm for high-dimensional multiview data in big data applications	NEUROCOMPUTING										Big data; Clustering; High dimension multiview data; Optimization; Spark	PARTICLE SWARM OPTIMIZATION; EFFICIENT	There are many high-dimensional multiview data in various big data applications. It is very difficult to deal with those high-dimensional multiview data for the classic clustering algorithms, which consider all features of data with equal relevance. To tackle this challenging problem, this paper aims at proposing a novel intelligent weighting k-means clustering (IWKM) algorithm based on swarm intelligence. Firstly, the degree of coupling between clusters is presented in the model of clustering to enlarge the dissimilarity of clusters. Various weights of views and features are used in the weighting distance function to determine the clusters of objects. Secondly, to eliminate the sensitivity of initial cluster centers, swarm intelligence is utilized to find initial cluster centers, weights of views, and weights of features by a global search. Lastly, a precise perturbation is proposed to improve optimization performance of swarm intelligence. To verify the performance of clustering for high-dimensional multiview data, the experiments were performed by the evaluation metrics of Rand Index, Jaccard Coefficient and Folkes Russe in five big data applications on the two different computational platforms of apache spark and single node. The experimental results show that IWKM is effective and efficient in clustering of high-dimensional multiview data, and can obtain better performance than the other 5 kinds of approaches in these complicated data sets with more views and higher dimensions on apache spark and single node. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						234	244		10.1016/j.neucom.2018.12.093													
J								An efficient method for pruning redundant negative and positive association rules	NEUROCOMPUTING										Correlation coefficient; Multiple minimum confidence; Negative association rules; Redundant negative association rules	GPU; FREQUENT	One of the most important problems that occur when mining positive and negative association rules (PNARs) is that the number of mined PNARs is usually large, which increases the difficulties that users retrieve decision-information. Those methods that prune redundant positive association rules (PARs) don't work when negative association rules (NARs) are taken into consideration. So in this paper, we first analyze what kinds of PNARs are redundant and then propose a novel method called LOGIC by using logical reasoning to prune redundant PNARs. In addition, we combine correlation coefficient and multiple minimum confidences (mc) to ensure that the mined PNARs are strongly correlated and their number can be flexibly controlled. Experimental results show that our method can prune up to 81.6% redundant PNARs. To the best of our knowledge, LOGIC is the first method to prune redundant PNARs simultaneously. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						245	258		10.1016/j.neucom.2018.09.108													
J								An efficient parallel implementation for training supervised optimum-path forest classifiers	NEUROCOMPUTING										Optimum-path forest; Parallel algorithms; Graph algorithms	COMPUTING METHOD; CLASSIFICATION; SPMV; OPTIMIZATION	In this work, we propose and analyze parallel training algorithms for the Optimum-Path Forest (OPF) classifier. We start with a naive parallelization approach where, following traditional sequential training that considers the supervised OPF, a priority queue is used to store the best samples at each learning iteration. The proposed approach replaces the priority queue with an array and a linear search aiming at using a parallel-friendly data structure. We show that this approach leads to less competition among threads, thus yielding a more temporal and spatial locality. Additionally, we show how the use of vectorization in distance calculations affects the overall speedup and also provide directions on the situations one can benefit from that. The experiments are carried out on five public datasets with a different number of samples and features on architectures with distinct levels of parallelism. On average, the proposed approach provides speedups of up to 11.8 x and 26 x in a 24-core Intel and 64-core AMD processors, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 14	2020	393						259	268		10.1016/j.neucom.2018.10.115													
J								Mining of multiple ailments correlated to diabetes mellitus	EVOLUTIONARY INTELLIGENCE										Diabetes; Disease correlation; Prediction; Machine learning; Fuzzy modeling	DISEASE PREDICTION	Efficient and user friendly database technologies have enabled the digitization of information pertaining to the medical domain. This has not only eased the smooth record manipulation but also attracted man a researchers to explore certain challenges to solve through implementation of data mining tools and techniques. Among the nature of ailments, the information related to diabetes mellitus (DM) are found to be the maximally digitized. This has provided a challenging but buzzing platform for the researchers to do in-depth analysis and present modern edge solutions which can lead to early diagnosis of the fatal ailment. There arise numerous side-effects to a human body when it is affected by DM. These multiple ailments attack a human body with the direct or indirect influence of DM and it's corresponding drug intake. Thus, there has been a demand for a generic scheme which can predict the likeliness of certain multiple ailments that a DM patient is supposed to be attacked by in near future. In this work, a suitable scheme has been proposed in the same direction. This scheme provides a viable platform where the probabilities of multiple ailments for a DM patient can be computed. The proposed scheme also provides the probabilities of occurrence of individual ailment as well as the probabilities of occurrence of certain combination of the ailments. Occurrence of three of the major ailment are being computed in this work. These are retinal disorder, kidney malfunction, and heart disease. A Fuzzy logic strategy has been used for matching several disease constraints and produce a decisive outcome. Certain number of novel heuristic functions are presented which take these outputs and provide a probabilistically accurate prediction of occurrences of the said ailments. Suitable experimental evaluation have been made with proper data inputs. The proposed scheme has also been compared with competent schemes. An overall rates of accuracy of 97% is calculated based on a k-fold cross validation performance metric.																	1864-5909	1864-5917															10.1007/s12065-020-00432-6		JUN 2020											
J								Hybrid Grey Wolf: Bald Eagle search optimized support vector regression for traffic flow forecasting	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Traffic flow; Forecasting; Support vector regression; Grey wolf optimization; Bald eagle search	PREDICTION	In this digital interconnected era, Intelligent Transportation System (ITS) bridges the gap between communication and transportation engineering in a smarter way, thereby facilitating the trespassers and travellers with forecasting of traffic and broadcasting of traffic incidents, and infotainment data. Automatic prediction of congestion and traffic flow at one point is a challenging task. Although many machine learning algorithms exist for prediction, the selection of appropriate parameters of algorithms had a great impact on the accuracy of prediction. Hybrid combination of Grey Wolf Optimization (GWO) with new emerging Bald Eagle Search (BES) Optimization algorithm has been proposed to optimize the parameters of Support Vector regression to predict the traffic flow. This hybrid SVR-GWO-BES, has been applied to real-time traffic data of the open-source Performance Measurement system dataset and Indian road traffic, which has been proven to be better than existing methodologies.																	1868-5137	1868-5145															10.1007/s12652-020-02182-w		JUN 2020											
J								A precise feature extraction method for shock wave signal with improved CEEMD-HHT	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Improved CEEMD; HHT; Shockwave overpressure; Feature extraction; Energy spectrum	EMPIRICAL MODE DECOMPOSITION; SEMANTIC SEGMENTATION	Efficient extraction of feature parameters is the key to evaluating weapon damage performance. At present, many classical feature extraction algorithms have the problem that the extraction cannot meet the actual needs. A precise feature extraction method based on improved complementary ensemble empirical mode decomposition (CEEMD) with Hilbert-Huang Transform (HHT) was proposed in this paper to solve problems such as large noise and difficulties in extracting features of shockwave overpressure signals in complex test environment. We introduced CEEMD to decompose original explosion shockwave signals and adopted wavelet packet threshold de-noising to extract useful information from noisy high-frequency intrinsic mode functions (IMFs). The correlation coefficient algorithm is introduced to remove unrelated IMFs. In addition, we performed reconstruction of original signals to extract true time-course feature and utilized Hilbert-Huang Transform (HHT) to achieve precise extraction of instantaneous feature and energy spectrum of the various IMFs. The improved CEEMD-HHT is a precise method for shock wave signal analysis. It not only effectively removes noise, but also retains effective high-frequency information without losing useful information. Additionally, it overcomes the problems of mode mixing in empirical mode decomposition (EMD), and has the advantages of feature extraction with high accuracy and self-adaptation. The effectiveness of the proposed method is demonstrated by 2 groups of experimental data, and it precisely extracts instantaneous feature and energy spectrum of shockwave overpressure signal, which provide new theoretical basis for the evaluation of weapon damage.																	1868-5137	1868-5145															10.1007/s12652-020-02204-7		JUN 2020											
J								A scalable semantic framework for IoT healthcare applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Simulated activities; Middleware; Ontologies; Epidemic detection	HUMAN ACTIVITY RECOGNITION; ANOMALY DETECTION; DISEASE ONTOLOGY; GASTROENTERITIS; HOME; INFRASTRUCTURE; KNOWLEDGE; INTERNET; BEHAVIOR; SYSTEM	IoT-based systems for early epidemic detection have not been investigated yet in the research. The state-of-the art in sensor technology and activity recognition makes it possible to automatically detect activities of daily living (ADL). Semantic reasoning over ADLs can discover anomalies and symptoms for disorders, hence diseases and epidemics. However, semantic reasoning is computationally rather expensive and therefore unusable for real-time monitoring in large scale applications, like early epidemic detection. To overcome this limitation, this paper proposes a new scalable semantic framework based on several semantic reasoning techniques that are distributed over a semantic middleware. To reduce the number of events to process during the semantic reasoning, a complex event processing (CEP) engine is used to detect abnormal events in ADL and to generate the associated symptom indicators. To demonstrate real-time detection and scalability, the proposed framework integrates a new extension of ADLSim, a discrete event simulator that simulates long-term sequences of ADL.																	1868-5137	1868-5145															10.1007/s12652-020-02136-2		JUN 2020											
J								A hybrid Harris Hawks optimization algorithm with simulated annealing for feature selection	ARTIFICIAL INTELLIGENCE REVIEW										Feature selection; Harris Hawks algorithm; k-nearest neighbor; Classification; Data dimensionality	PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZATION; SEARCH ALGORITHM; GENETIC ALGORITHM; STRATEGY; EMAIL	The significant growth of modern technology and smart systems has left a massive production of big data. Not only are the dimensional problems that face the big data, but there are also other emerging problems such as redundancy, irrelevance, or noise of the features. Therefore, feature selection (FS) has become an urgent need to search for the optimal subset of features. This paper presents a hybrid version of the Harris Hawks Optimization algorithm based on Bitwise operations and Simulated Annealing (HHOBSA) to solve the FS problem for classification purposes using wrapper methods. Two bitwise operations (AND bitwise operation and OR bitwise operation) can randomly transfer the most informative features from the best solution to the others in the populations to raise their qualities. The Simulate Annealing (SA) boosts the performance of the HHOBSA algorithm and helps to flee from the local optima. A standard wrapper method K-nearest neighbors with Euclidean distance metric works as an evaluator for the new solutions. A comparison between HHOBSA and other state-of-the-art algorithms is presented based on 24 standard datasets and 19 artificial datasets and their dimension sizes can reach up to thousands. The artificial datasets help to study the effects of different dimensions of data, noise ratios, and the size of samples on the FS process. We employ several performance measures, including classification accuracy, fitness values, size of selected features, and computational time. We conduct two statistical significance tests of HHOBSA like paired-samples T and Wilcoxon signed ranks. The proposed algorithm presented superior results compared to other algorithms.																	0269-2821	1573-7462															10.1007/s10462-020-09860-3		JUN 2020											
J								End-of-life product disassembly with priority-based extraction of dangerous parts	JOURNAL OF INTELLIGENT MANUFACTURING										Combinatorial optimization; Disassembly line balancing; Electronic waste; Evolutionary computation; Genetic algorithm; Metaheuristic; Multi-objective optimization; Product reuse and recycling; Risk; Safety; Selective disassembly; Sustainability	OPTIMIZATION; ALGORITHM	The amount of electronic waste generated in the world is impressive. The USA alone yearly throw away 9.4 million tons of electronic devices: only 12.5% is recycled. One way to reduce this massive impact on the environment is to disassemble these devices with the aim of reusing and recycling as many parts as possible. Disassembling end-of-life products is a complex industrial process that may pose workers at risk because some parts of the product may contain dangerous materials. It is thus crucial to design efficient, sustainable and secure disassembly lines. This paper presents a multi-objective formulation of the Disassembly Line Balancing Problem (DLBP) which promotes efficiency and includes a new objective that increases the level of safety. The efficiency is guaranteed by balancing the idle times of the workstations, and by maximizing the profit and the level of feasibility of a disassembly sequence, which means disassembling the product as much as possible. Safety is maximized by extracting each dangerous part with a priority that is higher the more dangerous the part is. The most dangerous parts can thus be quickly removed from the product, thereby eliminating the exposure to the greatest risks. The disassembly continues with the execution of the tasks that remove the parts that are gradually less dangerous. Along with the DLBP formulation, this paper presents a genetic algorithm purposely designed to solve the problem. Two real-world case studies are discussed which entail the disassembly of a TV monitor and an air conditioner.																	0956-5515	1572-8145															10.1007/s10845-020-01592-z		JUN 2020											
J								Preventive maintenance scheduling optimization based on opportunistic production-maintenance synchronization	JOURNAL OF INTELLIGENT MANUFACTURING										Complex manufacturing system; Preventive maintenance; Production-maintenance synchronization; Extreme learning machine; Ant colony optimization	EXTREME LEARNING-MACHINE; GROUPING STRATEGY; SYSTEMS	Equipment maintenance is momentous for improving production efficiency, how to integrate maintenance into production to address uncertain problems has attracted considerable attention. This paper addresses a novel approach for integrating preventive maintenance (PM) into production planning of a complex manufacturing system based on availability and cost. The proposed approach relies on two phases: firstly, this study predicts required capacity of each machine through extreme learning machine algorithm. Based on analyzing historical data, the opportunistic periods are calculated for implementing PM tasks to have less impact on production and obtain the PM interval and duration. Secondly, this study obtains the scheduling planning and the least number of maintenance personnel through an improved ant colony optimization algorithm. Finally, the feasibility and benefits of this approach are investigated based on empirical study by using historical data from real manufacturing execution system and equipment maintenance system. Experimental results demonstrate the effectiveness of proposed approach, reduce personnel number while guarantee the maintenance tasks. Therefore, the proposed approach is beneficial to improve the company's production efficiency.																	0956-5515	1572-8145															10.1007/s10845-020-01588-9		JUN 2020											
J								S3D-CNN: skeleton-based 3D consecutive-low-pooling neural network for fall detection	APPLIED INTELLIGENCE										Fall detection; Optimized skeleton representation; Depth video; Pose estimation; 3D-CLP network	REAL-TIME; NAVIGATION; SYSTEM	Most existing deep-learning-based fall detection methods use either 2D neural network without considering movement representation sequences, or whole sequences instead of only those in the fall period. These characteristics result in inaccurate extraction of human action features and failure to detect falls due to background interferences or activity representation beyond the fall period. To alleviate these problems, a skeleton-based 3D consecutive-low-pooling neural network (S3D-CNN) for fall detection is proposed in this paper. In the S3D-CNN, an activity feature clustering selector is designed to extract the skeleton representation in depth videos using pose estimation algorithm and form optimized skeleton sequence of fall period. A 3D consecutive-low-pooling (3D-CLP) neural network is proposed to process these representation sequences by improving network in terms of layer number, pooling kernel size, and single input frame number. The proposed method is evaluated on public and self-collected datasets respectively, outperforming the existing methods.																	0924-669X	1573-7497				OCT	2020	50	10					3521	3534		10.1007/s10489-020-01751-y		JUN 2020											
J								Content based video retrieval system based on multimodal feature grouping by KFCM clustering algorithm to promote human-computer interaction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human-computer interaction; Content based video retrieval (CBVR); Video analysis; Feature extraction; Significant frame detection	IMAGE; MOTION; TEXT	Content Based Video Retrieval (CBVR) is so popular these days, because of the increased utilization of video based analytical systems. Video based analytics is quite effective than image analysis, as a series of actions are captured by the video. This ends up with better decision making ability. The CBVR systems play an important role in boosting the human-computer interaction. This paper presents a multimodal CBVR that takes both the visual and audio information into account for retrieving relevant videos to the user. Two modules are employed by this work to deal with video and audio data. The video data is processed to detect the significant frame from shots and is achieved by Lion Optimization Algorithm (LOA). The features are extracted from the visual data and with respect to the audio data, MHEC and LPCC features are extracted. The extracted features are clustered by Kernelized Fuzzy C Mean (KFCM) algorithm. Finally, the feature database is formed and is utilized in the query matching process during the testing phase. The performance of the proposed work is tested in terms of precision, recall, F-measure and time consumption rates. The proposed CBVR system proves better performance than the existing approaches and is evident through attained results.																	1868-5137	1868-5145															10.1007/s12652-020-02190-w		JUN 2020											
J								Computation-Aware Adaptive Planning and Scheduling for Safe Unmanned Airborne Operations	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Self-triggered control; Reachability analysis; Scheduling and replanning; Unmanned aerial vehicles	COLLISION-AVOIDANCE; UAV; CONTROLLER	Modern unmanned aerial vehicles (UAVs) rely on high-frequency periodic sensor measurements in order to safely operate in cluttered environments with both static and dynamic obstacles. However, periodic sensor checking operations are time and computation consuming and they are often not needed, especially in situations where the UAV can operate without violating the safety constraint (e.g., in uncluttered free space). In this paper, we introduce a computation-aware framework that limits sensor checking and replanning operations to instances in which such operations could be necessary. To this end, we propose an approach that utilizes reachability analysis to capture the future states of a UAV operating under the effects of noise and disturbance and performs self-triggered scheduling for sensor monitoring and replanning operations while guaranteeing safety. The replanning operation is further relaxed by performing an online reachable tube shrinking. This approach is supplemented with an online speed adaptation policy based on the curvature of the planned path to minimize deviation from the desired trajectory due to complex system dynamics and controller limitations. The proposed technique is validated with both simulations and experiments focusing on a quadrotor motion planning operation in environments consisting of both static and dynamic obstacles.																	0921-0296	1573-0409				NOV	2020	100	2					575	596		10.1007/s10846-020-01192-2		JUN 2020											
J								Application on traffic flow prediction of machine learning in intelligent transportation	NEURAL COMPUTING & APPLICATIONS										Machine learning; Intelligent transportation; Support vector regression (SVR); Traffic Flow prediction	RESOURCE-ALLOCATION; SMART CITIES; BIG DATA; NETWORKS	With the development of human society, the shortcomings of the existing transportation system become increasingly prominent, so people hope to use advanced technology to achieve intelligent transportation. However, the recognition rate of most methods of detecting video vehicles is too low and the process is complicated. This paper uses machine learning theory to design a variety of pattern classifiers, including Adaboost, SVM, RF, and SVR algorithms, to classify vehicles. Support vector regression (SVR) is a support vector regression algorithm based on the basic principles of support vector machine (SVM) and then generalized to the regression problem. This paper proposes a short-term traffic flow prediction model based on SVR and optimizes SVM parameters to form an improved SVR short-term traffic flow prediction model. It can be obtained from experiments that the classification error rate of support vector regression (SVR) is the lowest (3.22%). According to the prediction of morning and night peak hours, this paper concludes that the MAPE of SVR is reduced by 19.94% and 42.86%, respectively, and the RMSE is reduced by 29.71% and 47.22%, respectively. Experiments show that the improved algorithm can obtain the optimal parameter combination of SVR faster and better and can effectively improve the accuracy of traffic flow prediction. The target tracking pedestrian counting method proposed in this paper has significantly improved the counting accuracy. The calculation of HOG features can be further expanded, such as the selection of neighborhoods when calculating HOG features, and finally a more efficient pedestrian counting framework is implemented.																	0941-0643	1433-3058															10.1007/s00521-020-05002-6		JUN 2020											
J								Detection of abnormal brain in MRI via improved AlexNet and ELM optimized by chaotic bat algorithm	NEURAL COMPUTING & APPLICATIONS										AlexNet; Magnetic resonance image; Deep learning; Extreme learning machine; Computer-aided diagnosis	ORIENTED GENETIC ALGORITHM; EXTREME LEARNING-MACHINE; CLASSIFICATION; IMAGES; TUMOR	Computer-aided diagnosis system is becoming a more and more important tool in clinical treatment, which can provide a verification of the doctors' decisions. In this paper, we proposed a novel abnormal brain detection method for magnetic resonance image. Firstly, a pre-trained AlexNet was modified with batch normalization layers and trained on our brain images. Then, the last several layers were replaced with an extreme learning machine. A searching method was proposed to find the best number of layers to be replaced. Finally, the extreme learning machine was optimized by chaotic bat algorithm to obtain better classification performance. Experiment results based on 5 x hold-out validation revealed that our method achieved state-of-the-art performance.																	0941-0643	1433-3058															10.1007/s00521-020-05082-4		JUN 2020											
J								Format-aware learn&fuzz: deep test data generation for efficient fuzzing	NEURAL COMPUTING & APPLICATIONS										Test data generation; File format fuzzing; Code coverage; Neural language model; Recurrent neural network; Deep learning		Appropriate test data are a crucial factor to succeed in fuzz testing. Most of the real-world applications, however, accept complex structure inputs containing data surrounded by meta-data which is processed in several stages comprising of the parsing and rendering (execution). The complex structure of some input files makes it difficult to generate efficient test data automatically. The success of deep learning to cope with complex tasks, specifically generative tasks, has motivated us to exploit it in the context of test data generation for complicated structures such as PDF files. In this respect, a neural language model (NLM) based on deep recurrent neural networks (RNNs) is used to learn the structure of complex inputs. To target both the parsing and rendering steps of the software under test (SUT), our approach generates new test data while distinguishing between data and meta-data that significantly improve the input fuzzing. To assess the proposed approach, we have developed a modular file format fuzzer,IUST-DeepFuzz. Our experimental results demonstrate the relatively high coverage of MuPDF code by our proposed fuzzer,IUST-DeepFuzz, in comparison with the state-of-the-art tools such as learn&fuzz, AFL, Augmented-AFL, and random fuzzing. In summary, our experiments with many deep learning models revealed the fact that the simpler the deep learning models applied to generate test data, the higher the code coverage of the software under test will be.																	0941-0643	1433-3058															10.1007/s00521-020-05039-7		JUN 2020											
J								Deep learning controller for nonlinear system based on Lyapunov stability criterion	NEURAL COMPUTING & APPLICATIONS										Lyapunov stability; Neural network; Deep learning controller; Nonlinear system; Restricted Boltzmann machine	ADAPTIVE FUZZY CONTROLLER; NEURAL-NETWORK CONTROLLER; DYNAMICAL-SYSTEMS; PID CONTROLLER; BELIEF NETWORK; DC MOTOR; IDENTIFICATION; FEEDFORWARD; POWER	For the current paper, the technique of feed-forward neural network deep learning controller (FFNNDLC) for the nonlinear systems is proposed. The FFNNDLC combines the features of the multilayer feed-forward neural network (FFNN) and restricted Boltzmann machine (RBM). The RBM is a very important part for the deep learning controller, and it is applied in order to initialize a multilayer FFNN by performing unsupervised pretraining, where all the weights are equally zero. The weight laws for the proposed network are developed by Lyapunov stability method. The proposed controller is mainly compared with FFNN controller (FFNNC) and other controllers, where all the weights values for all the designed controllers are equally zero. The proposed FFNNDLC is able to respond the effect of the system uncertainties and external disturbances compared with other existing schemes as shown in simulation results section. To show the ability of the proposed controller to deal with a real system, it is implemented practically using an ARDUNIO DUE kit microcontroller for controlling an electromechanical system. It is proved that the proposed FFNNDLC is faster than other FFNNCs in which the parameters are learned using the backpropagation method. Besides, it is able to deal with the changes in both the disturbance and the system parameters.																	0941-0643	1433-3058															10.1007/s00521-020-05077-1		JUN 2020											
J								An ensemble method to improve prediction of earthquake-induced soil liquefaction: a multi-dataset study	NEURAL COMPUTING & APPLICATIONS										Soil liquefaction; Earthquake; Machine learning; Classifier ensemble; Genetic algorithm; Prediction	UNCONFINED COMPRESSIVE STRENGTH; SUPPORT VECTOR MACHINES	Evaluation of earthquake-induced liquefaction potential is crucial in the design phase of construction projects. Although several machine learning models achieve good prediction accuracy on their particular datasets, they may not perform well in other liquefaction datasets. To address this issue, we proposed a novel hybrid classifier ensemble to improve generalizability by combining the predictions of seven base classifiers using the weighted voting method. The applied base classifiers include back propagation neural network, support vector machine, decision tree, k-nearest neighbours, logistic regression, multiple linear regression and naive Bayes. The hyperparameters and weights of the base classifiers were tuned using the genetic algorithm. To verify the robustness of the classifier ensemble, its performance was tested on three datasets collected from previous published researches. The results show that the proposed classifier ensemble outperforms the base classifiers in terms of a variety of performance metrics including accuracy, Kappa, precision, recall, F1 score, AUC and ROC on the three datasets. In addition, the importance of influencing variables was achieved by the classifier ensemble on the three datasets to facilitate the future data collecting work. This robust ensemble method can be extended to solve other classification problems in civil engineering.																	0941-0643	1433-3058															10.1007/s00521-020-05084-2		JUN 2020											
J								Multilayer analysis of population diversity in grammatical evolution for symbolic regression	SOFT COMPUTING										Grammatical evolution; Symbolic regression; Population diversity	PHENOTYPE; GENOTYPE	In this paper, we analyze the population diversity of grammatical evolution (GE) on multiple levels of genetic information: chromosome diversity, expression diversity, and output diversity. Thereby, we use a tree-similarity metric from tree-based GP literature to determine similarity of expression trees generated in GE. The similarity of outputs is determined via their correlation. We track the pairwise similarities for all individuals within a generation on all three levels and track the distribution of similarity values over generations. We demonstrate the analysis method using four symbolic regression problem instances and find that the visualization highlights some issues that can occur when using GE such as: large groups of individuals with highly similar outputs, a high fraction of trees with constant outputs, or short and highly similar trees in the early stages of the GE run. Especially in the early phases of GE, we see that a large subset of the population represents equivalent expressions. In early stages, rather short expressions are produced leaving large parts of the chromosome unexpressed. More complex expressions can be derived only after GE has successfully evolved well-working beginnings of chromosomes.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11283	11295		10.1007/s00500-020-05062-9		JUN 2020											
J								Granulation of ecological networks under fuzzy soft environment	SOFT COMPUTING										Fuzzy soft graphs; Fuzzy information; Indiscernibility partition; Granulation of ecological networks; Algorithms	SETS; MODEL; HYPERGRAPH; VIEW	The main idea of this work is the exploration of granular structures by applying the hybrid models of fuzzy soft sets and fuzzy soft graphs to discuss the indiscernibility partition of set of universe. The information granulation is examined by applying fuzzy soft theory, and the corresponding behavior of granules is reviewed. This article proposes a novel technique of formation of granular structures using fuzzy soft graphs and defines the fuzzy soft granules. Two degree-based models are introduced to explore the abstraction of these granular structure. Then, we use these two degree-based models to granulate the certain relationships between different species in an ecological system. Further, we develop and implement some algorithms of our proposed models to granulate the underconsideration networks. Finally, a comprehensive comparison of our proposed model with other existing techniques is presented to prove the applicability and effectiveness of fuzzy soft granulation.																	1432-7643	1433-7479				AUG	2020	24	16					11867	11892		10.1007/s00500-020-05083-4		JUN 2020											
J								MoSSE: a novel hybrid multi-objective meta-heuristic algorithm for engineering design problems	SOFT COMPUTING										Spotted Hyena optimizer; Salp Swarm Algorithm; Emperor Penguin Optimizer; Multi-objective optimization; Engineering design problems	EVOLUTIONARY ALGORITHMS; OPTIMIZATION ALGORITHM; GENETIC ALGORITHM	This paper introduces a novel hybrid optimization algorithm calledMoSSEby combining the features of Multi-objective Spotted Hyena Optimizer (MOSHO), Salp Swarm Algorithm (SSA), and Emperor Penguin Optimizer (EPO).MoSSEusesMOSHO'ssearching capabilities to effectively discover the search space,SSA'sleading and selection process to achieve the fittest global solution with quicker convergence technique, andEPO'seffective mover technique for better adjustment of the next solution. The algorithm is tested on tenIEEE CEC-9standard test functions and compared with seven well-known multi-objective optimization algorithms according to their performance. The experimental results show thatMoSSEprovides highly competitive outcomes in terms of convergence speed, searchability, and accuracy. Statistical testing is also performed onIEEE CEC-9test functions. Four performance metrics (i.e., Hypervolume, Delta(p), Spread, and Epsilon) are used to validate the searching capability of the proposed algorithm.MoSSEis further applied to welded beam, multi-disk clutch brake, pressure vessel, 25-bar truss design problems to test its effectiveness. The findings show the utility of the proposed algorithm to resolve the real-life complex multi-objective optimization problems.																	1432-7643	1433-7479															10.1007/s00500-020-05046-9		JUN 2020											
J								A belief degree-based uncertain scheme for a bi-objective two-stage green supply chain network design problem with direct shipment	SOFT COMPUTING										Green supply chain; Supply chain network design problem; Uncertainty theory; Multi-objective optimization	TRANSPORTATION PROBLEM; PROGRAMMING APPROACH; FUZZY; OPTIMIZATION; SELECTION; MODELS; PERFORMANCE; DECISIONS; LAYOUT	In the lack of historical data for an uncertain event, the belief degree-based uncertainty becomes more applicable than other types of uncertainty like fuzzy theory, stochastic programming, etc. This study focuses on an uncertain bi-objective two-stage supply chain network design problem. The problem consists of plants, depots, and customers with cost and environmental impacts (CO(2)emission) where direct shipment between plants and customers is allowed. As such network could be designed for the first time in a geographical region, such problem is modeled in a belief degree-based uncertain environment. This is almost the first study on belief degree-based uncertain supply chain network design problem with environmental impacts and direct shipment. Three approaches of expected value model, chance-constrained model, and their combination are applied to convert the proposed uncertain problem to its crisp form. The obtained crisp forms are solved by two multi-objective optimization approaches of goal programming (GP) and global criterion method (GCM). An extensive computational study with various test problems is performed to study the performance of the crisp models and the solution approaches. As result, the obtained crisp formulations are highly sensitive to the changes in the cost parameters' values, and the GP performs better than the GCM from the solution quality point of view.																	1432-7643	1433-7479															10.1007/s00500-020-05085-2		JUN 2020											
J								Mining interesting actionable patterns for web service composition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Interesting patterns; Semantic measures; Utility mining; Web service; Web service composition; XML	DISCOVERY; ALGORITHM	Web services are the way of integrating the web related applications using the XML, SOAP, WSDL and UDDI protocols in a standardized manner. Web service composition is the primary task of composing variety of different services on composite applications. Effective web service composition is not an easy task. This paper proposes a novel framework for extracting the interesting actionable patterns for effective web services for composition. This algorithm utilizes utility based data mining for extracting high utility actionable patterns for web service composition. Experimental results show that HUI-Miner approach outperforms well in terms of running time efficiency for our framework.																	1868-5137	1868-5145															10.1007/s12652-020-02187-5		JUN 2020											
J								Brain tumour classification using saliency driven nonlinear diffusion and deep learning with convolutional neural networks (CNN)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Edge detection; Minimum barrier distance; Saliency; Non-linear diffusion; CNN classification; ADAM optimization	SEGMENTATION	Experts notice and classify various Regions of Interest (ROI) manually for identification, analysis, and development of a treatment. To overcome errors and discrepancies of the data in this state, automated analysis is utilized. A novel method for the classification of MRI brain tumor is proposed in this paper using the saliency driven image representation and CNN based classification with optimization. Initially, the preprocessing on MRI images is carried out using canny edge detection algorithm followed by saliency driven image representation using modified minimum barrier distance and nonlinear diffusion at multiple level. Finally, feature extraction and image classification is carried out by CNN and the optimization by ADAM optimizer. The implementation is carried out and the results are evaluated which outperforms the earlier methods.																	1868-5137	1868-5145															10.1007/s12652-020-02200-x		JUN 2020											
J								Android malware detection method based on bytecode image	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Convolutional neural network; Malware; Android; Binary data; Bytecode	APPS	Traditional machine learning based malware detection methods often use decompiling techniques or dynamic monitoring techniques to extract the feature representation of malware. This procedure is time consuming and strongly depends on the skills of experts. In addition, malware can be packed or encrypted to evade the analysis of decompiling tools. To solve this issue, we propose a static detection method based on deep learning. We directly extract bytecode file from Android APK file, and convert the bytecode file into a two-dimensional bytecode matrix, then use the deep learning algorithm, convolution neural network (CNN), to train a detection model and apply it to classify malware. CNN can automatically learn features of bytecode file which can be used to recognize malware. The proposed detection model avoids the procedure for analyzing malware features and designing the feature representation of malware. The experimental results show the proposed method is effective to detect malware, especially malware encrypted using polymorphic techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02196-4		JUN 2020											
J								Automatic epileptic seizure recognition using reliefF feature selection and long short term memory classifier	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fast empirical mode decomposition; Independent component analysis; Long short-term memory; reliefF feature selection	PHASE-SPACE REPRESENTATION; APPROXIMATE ENTROPY; WAVELET TRANSFORM; LINE LENGTH; EEG; NETWORK; SYSTEM; IDENTIFICATION	Electroencephalogram (EEG) signal based epileptic seizure recognition is an emerging technique that efficiently identifies the non-stationary progresses of brain activities. Usually, the epilepsy is recognized by clinicians on the basis of visual observation of EEG signals that normally consumes more time and also sensitive to noise. In this research, a new supervised system is proposed for automatic epileptic seizure detection. Initially, the signals are collected from Temple University Hospital (TUH), Bern-Barcelona EEG (BB-EEG), Bonn University EEG (BU-EEG) and simulated database. Then, fast empirical mode decomposition (FEMD) and feature extraction (combination of entropy, frequency, auto-regressive, and statistical features) are employed for extracting the features from collected data. Besides, reliefF approach is used to lessen the number of extracted features by obtaining a set of active feature vectors that completely solves the "curse of dimensionality" issue. After feature selection, a supervised classifier [long short-term memory (LSTM)] is utilized for classifying the epileptic seizure classes. Experimental phase demonstrates that the proposed work efficiently classifies the epileptic seizure classes by means of miss rate, specificity, false discovery rate (FDR), false omission rate (FOR), sensitivity, and accuracy. From the experimental result, the proposed work improves the classification accuracy upto 0.6-16% related to the existing works.																	1868-5137	1868-5145															10.1007/s12652-020-02185-7		JUN 2020											
J								An optimal artificial neural network based big data application for heart disease diagnosis and classification model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data; Healthcare; Heart disease; Apache spark; Prediction	HEALTH; INTERNET; SYSTEM; THINGS	At present days, world is facing several issues like irregular distribution of medicinal resources, new chronic diseases, and the raising operating cost. The way of combining recent technologies into the medical system will helps to significantly resolve the problems. This study introduces a big health application system based on optimal artificial neural network (OANN) for heart disease diagnosis, which is considered as a deadliest disease in all over the globe. The proposed OANN includes a set of two main processes namely, distance based misclassified instance removal (DBMIR) and teaching and learning based optimization (TLBO) algorithm for ANN, called (TLBO-ANN). The proposed model is developed using a Big Data framework like Apache Spark. The presented OANN model operates on two phases, namely offline prediction and online prediction. During the offline prediction stage, the benchmark heart disease dataset will be used to train a model and performs testing. Similarly, at the online prediction stage, the real time data will be streamed into Apache Spark model and the filtered data will be diagnosed by the use of trained model to obtain the prediction results. The performance of the presented OANN model has been tested using a benchmark heart disease dataset from UCI repository. A comprehensive experimental result analysis clearly verified the better outcome of the OANN model over the compared methods. The proposed method is found to be an effective tool to analyze big data based heart disease prediction model to satisfy the need of increasing number of heart patients.																	1868-5137	1868-5145															10.1007/s12652-020-02181-x		JUN 2020											
J								Attack classification using feature selection techniques: a comparative study	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intrusion detection; Feature selection; Machine learning; Feature extraction; Classification; NSL-KDD dataset	INTRUSION DETECTION SYSTEM; DESIGN	The goal of securing a network is to protect the information flowing through the network and to ensure the security of intellectual as well as sensitive data for the underlying application. To accomplish this goal, security mechanism such as Intrusion Detection System (IDS) is used, that analyzes the network traffic and extract useful information for inspection. It identifies various patterns and signatures from the data and use them as features for attack detection and classification. Various Machine Learning (ML) techniques are used to design IDS for attack detection and classification. All the features captured from the network packets do not contribute in detecting or classifying attack. Therefore, the objective of our research work is to study the effect of various feature selection techniques on the performance of IDS. Feature selection techniques select relevant features and group them into subsets. This paper implements Chi-Square, Information Gain (IG), and Recursive Feature Elimination (RFE) feature selection techniques with ML classifiers namely Support Vector Machine, Naive Bayes, Decision Tree Classifier, Random Forest Classifier, k-nearest neighbours, Logistic Regression, and Artificial Neural Networks. The methods are experimented on NSL-KDD dataset and comparative analysis of results is presented.																	1868-5137	1868-5145															10.1007/s12652-020-02167-9		JUN 2020											
J								SybilWatch: a novel approach to detect Sybil attack in IoT based smart health care	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things (IoT); Smart health; Sybil attack	INTERNET; THINGS; ARCHITECTURE; SCHEME	Internet of things (IoT) in health care is gaining popularity in the field of research to improve quality in smart health care systems and applications. However, security and privacy of Smart Health (S-Health) data are the challenging issues due to Sybil attacks. Sybil attack is one of the most common attacks where a malicious node uses morphed identities to generate Sybil nodes. Sybil nodes can acquire an authorized node identity and misbehaves by affecting its routing information, incurs interruption on communication line and storage. One of the IoT based smart health methodology is Privacy-Aware Smart Health (PASH), in which policy hiding is used to protect the privacy of users. The major issues in PASH is expensive to implement in S-Health applications, also it does not deal with attribute revocation and node traceability. To addresses these issues, a novel SybilWatch Enhanced Privacy-Aware Smart Health (E-PASH) approach is proposed in this paper. This approach has three major phases such as initialization phase, secure communication and Sybil node detection. A Lightweight Encryption Algorithm (LEA) is used to transmit SHRs (Smart Health Record) in encrypted form using prime order grouping. A novel BlueTits Detection (BTD) algorithm is used in detection phase where cluster head gathers the recent activities of the suspicious user, and based on the gathered parameters (Master key and Last One-Time Authentication), the cluster head declares it as a Sybil node. As soon as Sybil node is detected, revised revocation list is shared with active users. The proposed approach is less expensive compared to the existing approach, it also supports attribute revocation and node traceability which are the major setbacks is PASH. Simulation results and comparison analysis shows that proposed SybilWatch is efficient and cost effective compared to the existing approach, the proposed approach yields high detection rate of 99.7% and also false positive rate is reduced to 1% in the smart health systems, which is better compared to the existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02189-3		JUN 2020											
J								Research on intelligent algorithm for alerting vehicle impact based on multi-agent deep reinforcement learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-agent; DQN; Active protection algorithm; Vehicle impact; Python	NEURAL-NETWORKS; GAME-THEORY; REGULARIZATION	In previous work, we designed a smart wearable device and algorithm for early warning of dangerous vehicles for pedestrians. The algorithm uses the fuzzy comprehensive evaluation as the framework, and calculates the risk degree using the vehicle, environment, and pedestrian status as evaluation indicators. Then according to the different environment, the back-propagation neural network (BP neural network) is used to assign dynamic weights to the indicators. Finally, the overall risk is obtained by calculating the product of the risk degree vector and the weight vector. However, due to the narrowness of human experience and the insufficient learning samples, the neural network overfits on the training set and performs poorly on the test set. In the current work, we try to apply reinforcement learning (RL) and multi-agent reinforcement learning (MARL) to drive agents to generate samples, and use this to replace learning samples generated by expert experience. In addition, the deep Q-learning network (DQN) mechanism is applied in the agent training method to avoid the curse of dimensionality, and finally multi-agent deep reinforcement learning (MDRL) is applied. Different from the previous algorithm, this algorithm integrates the fuzzy calculation process that must be completed on MATLAB into the neural network during training, and thus can greatly improve the calculation speed. Through the simulation experiments using python, it proves that this method can improve the accuracy of the algorithm up to about 96% and perform the best delay level.																	1868-5137	1868-5145															10.1007/s12652-020-02198-2		JUN 2020											
J								A novel incomplete sparse least square optimized regression model for abdominal mass detection in ultrasound images	EVOLUTIONARY INTELLIGENCE										Abdominal mass diagnosis; US image; AGLOH; ISLSR model; Performance measures	BREAST-CANCER DETECTION; COMPUTERIZED DETECTION; CLASSIFICATION; SEGMENTATION; PERFORMANCE; PREDICTION; MACHINE; LESIONS; DOMAIN; TUMORS	One of the recent ways of checking the internal organs such as kidneys, gallbladder, liver, and spleen is using an abdominal US image. US image is more familiar for its unique characteristics like radiation-free, safe, cheaper and faster. However, the thing to be considered is that US images cannot grant a precise view of affected regions. More particularly the unprocessed US images comprise a lot of embedded noises. Thus the digital processing is a promising result for improving the quality of US images. This paper intends to propose a novel model for abdominal masses detection with US images. This detection model comprises two phases: feature extraction as well as Classification. In the feature extraction process, texture features are extracted from the US image by AGLOH. Then in the classification stage, the optimized ISLSR model is used to detect whether the mass is present in the abdomen or not, where the coefficient matrix is optimally tuned using a new hybrid Lion and Whale Optimization algorithm. The performance analysis of the proposed method is compared with existing techniques such as GLCM-SVM, GLCM-NN, GLCM-LDCA, AGLOH-SVM, AGLOH-NN, AGLOH-LDCA, AGLOH-ISLSR-LA, and AGLOH-ISLSR-WOA. The performance of the developed method is analyzed in terms of both positive as well as negative measures: the positive measures include accuracy, sensitivity, specificity, and precision, NPV, F(1)Score, and MCC. The negative measures include FPR, FNR, and FDR, and the efficiency of the proposed model is proved.																	1864-5909	1864-5917															10.1007/s12065-020-00431-7		JUN 2020											
J								A new Pareto multi-objective sine cosine algorithm for performance enhancement of radial distribution network by optimal allocation of distributed generators	EVOLUTIONARY INTELLIGENCE										Distributed generation; Pareto front; Non-dominated solutions; Power loss; Voltage stability	KRILL HERD ALGORITHM; PARTICLE SWARM OPTIMIZATION; VOLTAGE STABILITY; OPTIMAL PLACEMENT; DG PLACEMENT; OPTIMAL LOCATION; DISTRIBUTION-SYSTEMS; SEARCH ALGORITHM; MULTIPLE DGS; LOAD	The integration of distributed generators (DGs) is considered to be one of the best cost-effective techniques to improve the efficiency of power distribution systems in the recent deregulation caused by continuous load demand and transmission system contingency. In this perspective, a new multi-objective sine cosine algorithm is proposed for optimal DG allocation in radial distribution systems with minimization of total active power loss, maximization of voltage stability index, minimization of annual energy loss costs as well as pollutant gas emissions without violating the system and DG operating constraints. The proposed algorithm is enhanced by incorporating exponential variation of the conversion parameter and the self-adapting levy mutation to increase its performance during different iteration phases. The contradictory relationships among the objectives motivate the authors to generate an optimal Pareto set in order to help the network operators in taking fast appropriate decisions. The proposed approach is successfully applied to 33-bus and 69-bus distribution systems under four practical load conditions and is evaluated in different two-objective and three-objective optimization cases. The effectiveness of the algorithm is confirmed by comparing the results against other well-known multi-objective algorithms, namely, strength Pareto evolutionary algorithm 2, non-dominated sorting genetic algorithm II and multi-objective particle swarm optimization. The quality of Pareto fronts from different multi-objective algorithms is compared in terms of certain performance indicators, such as generational distance, spacing metric and spread metric (Delta), and its statistical significance is verified by performing Wilcoxon signed rank test.																	1864-5909	1864-5917															10.1007/s12065-020-00428-2		JUN 2020											
J								FPGA implementation of HOOFR bucketing extractor-based real-time embedded SLAM applications	JOURNAL OF REAL-TIME IMAGE PROCESSING										Features extraction; Parallel image processing; FPGA implementation; Embedded systems		Feature extraction is an important vision task in many applications like simultaneous localization and mapping (SLAM). In the recent computing systems, FPGA-based acceleration have presented a strong competition to GPU-based acceleration due to its high computation capabilities and lower energy consumption. In this paper, we present a high-level synthesis implementation on a SoC-FPGA of a feature extraction algorithm dedicated for SLAM applications. We choose HOOFR extraction algorithm which provides a robust performance but requires a significant computation on embedded CPU. Our system is dedicated for SLAM applications so that we also integrated bucketing detection method in order to have a homogeneous distribution of keypoints in the image. Moreover, instead of optimizing performance by simplifying the original algorithm as in many other researches, we respected the complexity of HOOFR extractor and have parallelized the processing operations. The design has been validated on an Intel Arria 10 SoC-FPGA with a throughput of 54 fps at 1226 x 370 pixels (handling 1750 features) or 14 fps at 1920 x 1080 pixels (handling 6929 features).																	1861-8200	1861-8219															10.1007/s11554-020-00986-9		JUN 2020											
J								Improving robot's perception of uncertain spatial descriptors in navigational instructions by evaluating influential gesture notions	JOURNAL ON MULTIMODAL USER INTERFACES										Understanding uncertain descriptors; Human-robot interactions; Human-friendly robotics; Multimodal interaction; service robotics	FUZZY VOICE COMMANDS; LANGUAGE	Human-friendly interactive features are preferred for service robots used in emerging areas of robotic applications such as caretaking, health care, assistance, education and entertainment since they are intended to be operated by non-expert users. Humans prefer to use voice instructions, responses, and suggestions in their daily interactions. Such voice instructions and responses often include uncertain spatial descriptors such as "little" and "far", which have no definitive quantitative meaning. Service robots involve direct interactions with human users through voice communication. Therefore, the ability to effectively quantify the meaning of such uncertain spatial descriptors is necessary for human-friendly service robots. This paper proposes a novel method to quantify the uncertain spatial descriptors in navigational instructions based on the current environmental setting and the influential notions conveyed by the pointing gestures that accompany voice instructions. The uncertain spatial descriptors are quantified by a fuzzy inference system that evaluates the spatial parameters of the current environment and the influential notions conveyed by pointing gestures, if available. According to the obtained experimental results, the proposed method is capable of improving the quantification ability of uncertain spatial descriptors by robots.																	1783-7677	1783-8738															10.1007/s12193-020-00328-w		JUN 2020											
J								CovidSens: a vision on reliable social sensing for COVID-19	ARTIFICIAL INTELLIGENCE REVIEW										Social sensing; COVID-19; Coronavirus; Disease tracking; Real-time; Information distillation	TRUTH DISCOVERY; MEDIA; CLASSIFICATION; PATTERNS; INTERNET	With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it has becoming inherently important to disseminate accurate and timely information about the disease. Due to the ubiquity of Internet connectivity and smart devices, social sensing is emerging as a dynamic AI-driven sensing paradigm to extract real-time observations from online users. In this paper, we propose CovidSens, a vision of social sensing-based risk alert systems to spontaneously obtain and analyze social data to infer the state of the COVID-19 propagation. CovidSens can actively help to keep the general public informed about the COVID-19 spread and identify risk-prone areas by inferring future propagation patterns. The CovidSens concept is motivated by three observations: (1) people have been actively sharing their state of health and experience of the COVID-19 via online social media, (2) official warning channels and news agencies are relatively slower than people reporting their observations and experiences about COVID-19 on social media, and (3) online users are frequently equipped with substantially capable mobile devices that are able to perform non-trivial on-device computation for data processing and analytics. We envision an unprecedented opportunity to leverage the posts generated by the ordinary people to build a real-time sensing and analytic system for gathering and circulating vital information of the COVID-19 propagation. Specifically, the vision of CovidSens attempts to answer the questions: How to distill reliable information about the COVID-19 with the coexistence of prevailing rumors and misinformation in the social media? How to inform the general public about the latest state of the spread timely and effectively, and alert them to remain prepared? How to leverage the computational power on the edge devices (e.g., smartphones, IoT devices, UAVs) to construct fully integrated edge-based social sensing platforms for rapid detection of the COVID-19 spread? In this vision paper, we discuss the roles of CovidSens and identify the potential challenges in developing reliable social sensing-based risk alert systems. We envision that approaches originating from multiple disciplines (e.g., AI, estimation theory, machine learning, constrained optimization) can be effective in addressing the challenges. Finally, we outline a few research directions for future work in CovidSens.																	0269-2821	1573-7462															10.1007/s10462-020-09852-3		JUN 2020											
J								Graph-based topic models for trajectory clustering in crowd videos	MACHINE VISION AND APPLICATIONS										Clustering; Crowd videos; Graph; Manifold embedding; Topic modeling	TRACKING	Probabilistic topic modelings, such as latent Dirichlet allocation (LDA) and correlated topic models (CTM), have recently emerged as powerful statistical tools for processing video content. They share an important property, i.e., using a common set of topics to model all data. However, such property can be too restrictive for modeling complex visual data such as crowd scenes where multiple fields of heterogeneous data jointly provide rich information about objects and events. This paper proposes graph-based extensions of LDA and CTM, referred to as GLDA and GCTM, to learn and analyze motion patterns by trajectory clustering in a highly cluttered and crowded environment. Unlike previous works that relied on a scene prior, we apply a spatio-temporal graph to uncover the spatial and temporal coherence between the trajectories of crowd motion during the learning process. The presented models advance the conventional approaches by integrating a manifold-based clustering as initialization and iterative statistical inference as optimization. The output of GLDA and GCTM are mid-level features that represent the motion patterns used later to generate trajectory clusters. Experiments on three different datasets show the effectiveness of the approaches in trajectory clustering and crowd motion modeling.																	0932-8092	1432-1769				JUN 12	2020	31	5							39	10.1007/s00138-020-01092-3													
J								I-ME: iterative model evolution for learning from weakly labeled images and videos	MACHINE VISION AND APPLICATIONS										Weakly labeled data; Discriminative exemplars; Iterative noisy data elimination; Learning from large-scale noisy data; Activity recognition		A significant bottleneck in building large-scale systems for image and video categorization is the requirement of labeled data. Manual labeling effort could be overcome by using the massive amount of web data. However, this type of data is collected through searching on the category names and is likely to inherit noise. In this study, (1) the primary objective is to improve utilizing weakly labeled data without any manual intervention. To this end, (2) we introduce a simple but effective method called "Iterative Model Evolution (I-ME)" where the goal is to discover representative instances by eliminating the irrelevant items so that the purified set can be directly used in training a model. In I-ME, (3) the elimination is done by leveraging the scores of two logistic regressors where the models are learned through iterations. We first apply our method for (4) recognizing complex human activities in images and videos and then a large-scale noisy web dataset, Clothing1M. (5) Our results are comparable to or better than the presented baselines on benchmark video datasets UCF-101, ActivityNet, FCVID and image dataset Action40. Through purifying with I-ME, we come up with only 40% of the noisy Clothing1M and we train the DNN with less but more representative training data without changing the network structure. (6) The success of I-ME over utilizing deep features supports that there is still room for improvement in exploiting large-scale weakly labeled data through mining to discover a smaller but more distinctive subset without increasing the complexity of the process.																	0932-8092	1432-1769				JUN 12	2020	31	5							40	10.1007/s00138-020-01079-0													
J								A spatially explicit agent-based model of central place foraging theory and its explanatory power for hunter-gatherers settlement patterns formation processes	ADAPTIVE BEHAVIOR										Agent-based model; hunter-gatherers; central place foraging; mobility; settlement choice	MARGINAL VALUE; MOBILITY; PROTOCOL; DISTANCE; ENERGY; CHOICE	The behavioural ecological approach to anthropology states that the density and distribution of resources determines optimal patterns of resource use and also sets its constraints to grouping, mobility and settlement choice. Central place foraging (CPF) models have been used for analyzing foraging behaviours of hunter-gatherers and drawing a causal link from the volume of available resources in the environment to the mobility decisions of hunter-gatherers. In this study, we propose a spatially explicit agent-based CPF model. We explore its potential for explaining the formation of settlement patterns and test its robustness to the configuration of space. Building on a model assuming homogeneous energy distributions, we had to add several new parameters and an adaptation mechanism for foragers to predict the length of their stay, together with a heterogeneous environment configuration. The validation of the model shows that the spatially explicit CPF is generally robust to spatial configuration of energy resources. The total volume of energy has a significant effect on constraining sedentism as predicted by aspatial model and thus can be used on different environmental conditions. Still the spatial autocorrelation of resource distribution has a linear effect on optimal mobility decisions and needs to be considered in predictive models. The effect on settlement location choice is not substantial and is more determined by other characteristics of settlement location. This limits the CPF models in analyzing settlement pattern formation processes.																	1059-7123	1741-2633				OCT	2020	28	5			SI		377	397	1059712320922915	10.1177/1059712320922915		JUN 2020											
J								Multi-objective particle swarm optimization with random immigrants	COMPLEX & INTELLIGENT SYSTEMS										Metaheuristics; Multi-objective optimization; Particle swarm optimization; Random immigrants	GENETIC ALGORITHMS; COMBINATION	Complex problems of the current business world need new approaches and new computational algorithms for solution. Majority of the issues need analysis from different angles, and hence, multi-objective solutions are more widely used. One of the recently well-accepted computational algorithms is Multi-objective Particle Swarm Optimization (MOPSO). This is an easily implemented and high time performance nature-inspired approach; however, the best solutions are not found for archiving, solution updating, and fast convergence problems faced in certain cases. This study investigates the previously proposed solutions for creating diversity in using MOPSO and proposes using random immigrants approach. Application of the proposed solution is tested in four different sets using Generational Distance, Spacing, Error Ratio, and Run Time performance measures. The achieved results are statistically tested against mutation-based diversity for all four performance metrics. Advantages of this new approach will support the metaheuristic researchers.																	2199-4536	2198-6053				OCT	2020	6	3					635	650		10.1007/s40747-020-00159-y		JUN 2020											
J								An estimation of distribution algorithm with branch-and-bound based knowledge for robotic assembly line balancing	COMPLEX & INTELLIGENT SYSTEMS										Estimation of distribution algorithm; Sampling mechanism; Branch-and-bound based knowledge; Problem-specific local search; Robotic assembly line balancing	MODEL; CARBON	Robotic assembly lines are widely used in manufacturing industries. The robotic assembly line balancing (RALB) problem aims to balance the workloads among different workstations and optimize the assembly line efficiency. This paper addresses a particular type of RALB problem, which minimizes the assembly line cycle time by determining the task and robot assignment in each workstation under precedence constraints. To solve the problem, we present an effective hybrid algorithm fusing the estimation of distribution algorithm and branch-and-bound (B&B) based knowledge. A problem-specific probability model is designed to describe the probabilities of each task being assigned to different workstations. Based on the probability model, an incremental learning method is developed and a sampling mechanism with B&B based knowledge is proposed to generate new feasible solutions. The fuse of B&B based knowledge is able to reduce the search space of EDA while focusing the search on the promising area. To enhance the exploitation ability, a problem-specific local search is developed based on the critical workstation to further improve the quality of elite solutions. The computational complexity of the proposed algorithm is analyzed, and the effectiveness of the B&B based knowledge and the problem-specific local search is demonstrated through numerical experiments. Moreover, the performance of the proposed algorithm is compared with existing algorithms on a set of widely-used benchmark instances. Comparative results demonstrate the effectiveness and efficiency of the proposed algorithm.																	2199-4536	2198-6053															10.1007/s40747-020-00166-z		JUN 2020											
J								Concurrent optimization of process parameters and product design variables for near net shape manufacturing processes	JOURNAL OF INTELLIGENT MANUFACTURING										Manufacturing optimization; Process optimization; Design optimization; Near net shape; Genetic algorithm; Machining parameters optimization	OPTIMAL MACHINING PARAMETERS; EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; CUTTING PARAMETERS; SWARM OPTIMIZATION; TURNING OPERATIONS; METHODOLOGY; SYSTEM; STOCK	This paper presents a new systematic approach to the optimization of both design and manufacturing variables across a multi-step production process. The approach assumes a generic manufacturing process in which an initial near net shape (NNS) process is followed by a limited number of finishing operations. In this context the optimisation problem becomes a multi-variable problem in which the aim is to optimize by minimizing cost (or time) and improving technological performances (e.g. turning force). To enable such computation a methodology, named conditional design optimization (CoDeO) is proposed which allows the modelling and simultaneous optimization of process parameters and product design (geometric variables), using single or multi-criteria optimization strategies. After investigation of CoDeO's requirements, evolutionary algorithms, in particular Genetic Algorithms, are identified as the most suitable for overall NNS manufacturing chain optimization The CoDeO methodology is tested using an industrial case study that details a process chain composed of casting and machining processes. For the specific case study presented the optimized process resulted in cost savings of 22% (corresponding to equivalent machining time savings) and a 10% component weight reduction.																	0956-5515	1572-8145															10.1007/s10845-020-01593-y		JUN 2020											
J								Metaheuristics for the template design problem: encoding, symmetry and hybridisation	JOURNAL OF INTELLIGENT MANUFACTURING										Template design problem; Symmetry breaking; Optimisation; Problem formulation; Metaheuristics; Memetic algorithm	COOPERATIVE STRATEGIES; SEARCH; BREAKING; ALGORITHMS	The template design problem (TDP) is a hard combinatorial problem with a high number of symmetries which makes solving it more complicated. A number of techniques have been proposed in the literature to optimise its resolution, ranging from complete methods to stochastic ones. However, although metaheuristics are considered efficient methods that can find enough-quality solutions at a reasonable computational cost, these techniques have not proven to be truly efficient enough to deal with this problem. This paper explores and analyses a wide range of metaheuristics to tackle the problem with the aim of assessing their suitability for finding template designs. We tackle the problem using a wide set of metaheuristics whose implementation is guided by a number of issues such as problem formulation, solution encoding, the symmetrical nature of the problem, and distinct forms of hybridisation. For the TDP, we also propose a slot-based alternative problem formulation (distinct to other slot-based proposals), which represents another option other than the classical variation-based formulation of the problem. An empirical analysis, assessing the performance of all the metaheuristics (i.e., basic, integrative and collaborative algorithms working on different search spaces and with/without symmetry breaking) shows that some of our proposals can be considered the state-of-the-art when they are applied to specific problem instances.																	0956-5515	1572-8145															10.1007/s10845-020-01587-w		JUN 2020											
J								A novel PCA-whale optimization-based deep neural network model for classification of tomato plant diseases using GPU	JOURNAL OF REAL-TIME IMAGE PROCESSING										Tomato leaf disease classification; Deep neural networks; Grid search hyperparameter tuning; Principal component analysis; Whale optimization algorithm	SYSTEM; SDN	The human population is growing at a very rapid scale. With this progressive growth, it is extremely important to ensure that healthy food is available for the survival of the inhabitants of this planet. Also, the economy of developing countries is highly dependent on agricultural production. The overall economic balance gets affected if there is a variance in the demand and supply of food or agricultural products. Diseases in plants are a great threat to the yield of the crops thereby causing famines and economy slow down. Our present study focuses on applying machine learning model for classifying tomato disease image dataset to proactively take necessary steps to combat such agricultural crisis. In this work, the dataset is collected from publicly available plant-village dataset. The significant features are extracted from the dataset using the hybrid-principal component analysis-Whale optimization algorithm. Further the extracted data are fed into a deep neural network for classification of tomato diseases. The proposed model is then evaluated with the classical machine learning techniques to establish the superiority in terms of accuracy and loss rate metrics.																	1861-8200	1861-8219															10.1007/s11554-020-00987-8		JUN 2020											
J								GC-NET for classification of glaucoma in the retinal fundus image	MACHINE VISION AND APPLICATIONS										Glaucoma; Deep learning; Computer-aided diagnosis; Retinal fundus image	DIAGNOSIS; FEATURES; TEXTURE	Glaucoma is the second-most dominant cause for irreversible blindness, resulting in damage to the optic nerve. Ophthalmologist diagnoses this disease using a retinal examination of the dilated pupil. Since the diagnosis is a manual and laborious procedure, an automated approach for faster diagnosis is desirable. Convolutional neural networks (CNN) could allow automation of the diagnosis procedure due to their self-learning capabilities. This paper presents a deep learning-based glaucoma classification network (GC-NET) for classifying a retinal image as glaucomatous or non-glaucomatous. The proposed GC-NET has been tested on RIM-One and Drishti datasets. Our experimental results showed that GC-NET achieves accuracy of 97.51%, sensitivity of 98.78% and specificity of 96.20% with 0.81 true positive (Tp), 0.03 false positive (Fp), 0.76 true negative (Tn) and 0.01 false negative (Fn) which outperforms state of the art. Thus, the proposed approach could be very useful for initial screening of glaucoma patients.																	0932-8092	1432-1769				JUN 12	2020	31	5							38	10.1007/s00138-020-01091-4													
J								Advances in computer-human interaction for detecting facial expression using dual tree multi band wavelet transform and Gaussian mixture model	NEURAL COMPUTING & APPLICATIONS										Facial expression; Emotion recognition; Dual-tree M-band wavelet transform (DTMBWT); Gaussian mixture model (GMM); Energy features and entropy features	RECOGNITION	In human communication, facial expressions play an important role, which carries enough information about human emotions. Last two decades, it becomes a very active research area in pattern recognition and computer vision. In this type of recognition, there is a drawback of how to extract the features because of its dynamic nature of facial structures, which are extracted from the facial images and to predict the level of difficulties in the extraction of the facial expressions. In this research, an efficient approach for emotion or facial expression analysis based on dual-tree M-band wavelet transform (DTMBWT) and Gaussian mixture model (GMM) is presented. Different facial expressions are represented by DTMBWT at various decomposition levels from one to six. From the representations, DTMBWT energy and entropy features are extracted as features for the corresponding facial expression. These features are analyzed for the recognition using GMM classifier by varying the number of Gaussians used. Japanese female facial expression database which contains seven facial expressions; happy, sad, angry, fear, neutral, surprise and disgust are employed for the evaluation. Results show that the framework provides 98.14% accuracy using fourth-level decomposition, which is considerably high.																	0941-0643	1433-3058															10.1007/s00521-020-05037-9		JUN 2020											
J								Memory augmented hyper-heuristic framework to solve multi-disciplinary problems inspired by cognitive problem solving skills	NEURAL COMPUTING & APPLICATIONS										Transfer learning; Regression; Unsupervised learning; Discretization; K-means clustering; Hyper-heuristic framework; Deja Vu	REGRESSION	This paper proposes a new framework, named Deja-Vu+, which is an extension of Deja Vu framework, a classic study on hyper-heuristic framework with 2R (Record and Recall) modules. Deja-Vu+ has the ability to handle two other domains, namely regression and unsupervised learning. The extension examines the strength of Deja-Vu+ for solving regression and unsupervised learning tasks. The regression problems are treated here as multiclass classification tasks, and unsupervised learning tasks are considered as clustering problems. The proposed framework is tested on a number of regression and unsupervised learning benchmark problems and has shown promising results to handle regression as classification. The framework attains an overall average accuracy of 70% for regression and clustering data sets. Deja-Vu+ is knowledge-rich hyper-heuristic framework, which is capable enough to transfer knowledge successfully. This knowledge transfer improves the performance of learning by avoiding the extensive heuristic search process. Our experimental results show that using previously attained knowledge to reduce the computational effort is beneficial in solving multi-disciplinary machine learning problems.																	0941-0643	1433-3058															10.1007/s00521-020-05016-0		JUN 2020											
J								Neural network and multi-objective optimization of confined flow characteristics on circular cylinder in standing double vortex region	NEURAL COMPUTING & APPLICATIONS										Reynolds number; ANOVA; Neural network; Flow characteristics	VISCOUS-FLOW; TAGUCHI; NOISE	The unsteady state and isothermal two dimensional numerical computations were carried out using Ansys Fluent-18 between the Reynolds number ranges 10 to 50. The blockage ratios (Domain height to the circular cylinder diameter) range 1.54-112. The flow characteristics such as drag coefficients and length of recirculation are optimized and correlated as a function of various Reynolds numbers at different blockage ratios. Gradual decrease in blockage ratio which means the increase in blockage effect postponed the flow separation, transition and reduces the length of recirculation and also makes the flow steady. In this study optimum flow characteristics exist at maximum blockage ratio, i.e. with minimum blockage effect and maximum Reynolds number. The artificial neural networks model proved to predict values of the total drag coefficient (R-2 = 0.979) and length of recirculation (R-2 = 0.992) closer to simulated data at 95% (alpha = 0.05) confident interval.																	0941-0643	1433-3058															10.1007/s00521-020-05079-z		JUN 2020											
J								State theory on bounded hyper EQ-algebras	SOFT COMPUTING										Hyper EQ-algebra; Sup-Bosbach state; Sup-Riecan state; Hyper congruence; Quotient hyper EQ-algebra		In a hyper structure ( X, star), x star y is a non-empty subset of X. For a state s, s(x star y) need not be well defined. In this paper, by defining s * (x star y) = sup{s(z) vertical bar z epsilon x star y}, we introduce notions of sup-Bosbach states, state-morphisms and sup-Riecan states on a bounded hyper EQ-algebra and discuss the related properties. The states on bounded hyper EQ-algebras are the generalization of states on EQ-algebras. Then we discuss the relations among sup-Bosbach states, state-morphisms and sup-Riecan states on bounded hyper EQ-algebras. By giving a counter example, we show that a sup-Bosbach state may not be a sup-Riecan state on a hyper EQ-algebra. We give conditions in which each sup-Bosbach state becomes a sup-Riecan state on bounded hyper EQ-algebras. Moreover, we introduce several kinds of congruences on bounded hyper EQ-algebras, by which we construct the quotient hyper EQ-algebras. By use of the state s on a bounded hyper EQ-algebra H, we set up a state (s) over bar on the quotient hyper EQ-algebra H/theta. We also give the condition, by which a bounded hyper EQ-algebra admits a sup-Bosbach state.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11199	11211		10.1007/s00500-020-05039-8		JUN 2020											
J								Grammatically uniform population initialization for grammar-guided genetic programming	SOFT COMPUTING										Grammar-guided genetic programming; Initialization; Genotypic uniformity; Stochastic context-free grammar	ALGORITHM; DIVERSITY	The initial population distribution is an essential issue in evolutionary computation performance. Population initialization methods for grammar-guided genetic programming have some difficulties generating a representative sample of the search space, which negatively affects the overall evolutionary process. This paper presents a grammatically uniform population initialization method to address this issue by improving the initial population uniformity: the equiprobability of obtaining any individual of the search space defined by the context-free grammar. The proposed initialization method assigns and updates probabilities dynamically to the production rules of the grammar to pursue uniformity and includes a code bloat control mechanism. We have conducted empirical experiments to compare the proposed algorithm with a standard initialization approach very often used in grammar-guided genetic programming. The results report that the proposed initialization method approximates very well a uniform distribution of the individuals in the search space. Moreover, the overall evolutionary process that takes place after the population initialization performs better in terms of convergence speed and quality of the final solutions achieved when the proposed method generates the initial population than when the usual approach does. The results also show that these performance differences are more significant when the experiments involve large search spaces.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11265	11282		10.1007/s00500-020-05061-w		JUN 2020											
J								Nested AdaBoost procedure for classification and multi-class nonlinear discriminant analysis	SOFT COMPUTING										PCA; Ranking PCA components; Separating hyperplanes; Ensemble methods; AdaBoost; Face image analysis	FEATURE-SELECTION; DIMENSIONALITY REDUCTION; MUTUAL INFORMATION; VALIDATION; ALGORITHMS; RELEVANCE; PCANET; IMAGE; FACES	AdaBoost methods find an accurate classifier by combining moderate learners that can be computed using traditional techniques based, for instance, on separating hyperplanes. Recently, we proposed a strategy to compute each moderate learner using a linear ensemble of weak classifiers that are built through the kernel support vector machine (KSVM) hypersurface geometry. In this way, we apply AdaBoost procedure in a nested loop: Each iteration of the inner loop boosts weak classifiers to a moderate one while the outer loop combines the moderate classifiers to build the global decision rule. In this paper, we explore this methodology in two ways: (a) For classification in principal component analysis (PCA) spaces; (b) For multi-class nonlinear discriminant PCA, named MNDPCA. Up to the best of our knowledge, the former is a new AdaBoost-based classification technique. Besides, in this paper we study the influence of kernel types for MNDPCA in order to set a near optimum configuration for feature selection and ranking in PCA subspaces. We compare the proposed methodologies with counterpart ones using facial expressions of the Radboud Faces database and Karolinska Directed Emotional Faces (KDEF) image database. Our experimental results have shown that MNDPCA outperforms counterpart techniques for selecting PCA features in the Radboud database while it performs close to the best technique for KDEF images. Moreover, the proposed classifier achieves outstanding recognition rates if compared with the literature techniques.																	1432-7643	1433-7479															10.1007/s00500-020-05045-w		JUN 2020											
J								Multimodal image-to-image translation between domains with high internal variability	SOFT COMPUTING										GANs; Image translation; High internal variability; Catastrophic forgetting; Generator regulating		Multimodal image-to-image translation based on generative adversarial networks (GANs) shows suboptimal performance in the visual domains with high internal variability, e.g., translation from multiple breeds of cats to multiple breeds of dogs. To alleviate this problem, we recast the training procedure as modeling distinct distributions which are observed sequentially, for example, when different classes are encountered over time. As a result, the discriminator may forget about the previous target distributions, known as catastrophic forgetting, leading to non-/slow convergence. Through experimental observation, we found that the discriminator does not always forget the previously learned distributions during training. Therefore, we propose a novel generator regulating GAN (GR-GAN). The proposed method encourages the discriminator to teach the generator more effectively when it remembers more of the previously learned distributions, while discouraging the discriminator to guide the generator when catastrophic forgetting happens on the discriminator. Both qualitative and quantitative results show that the proposed method is significantly superior to the state-of-the-art methods in handling the image data that are with high variability.																	1432-7643	1433-7479															10.1007/s00500-020-05073-6		JUN 2020											
J								Least absolute deviations estimation for uncertain autoregressive model	SOFT COMPUTING										Confidence interval; Mean absolute error; LAD; Uncertain autoregressive; Uncertain variable		To predict future values based on imprecisely observed values, uncertain time series has been proposed, and the least-squares method has been presented to estimate the unknown parameters of uncertain autoregressive models. This paper considers the least absolute deviations estimation of uncertain autoregressive model, and a minimization problem is derived to calculate the unknown parameters in the uncertain autoregressive model. Finally, some numerical examples are given to illustrate the robustness of the least absolute deviations estimation compared with the least-squares estimation.																	1432-7643	1433-7479															10.1007/s00500-020-05079-0		JUN 2020											
J								Are you sure? Prediction revision in automateddecision-making	EXPERT SYSTEMS										experiment; explainable ML; interpretability; prediction revision	DECISION-MAKING; FORECASTS; ADVICE	With the rapid improvements in machine learning and deep learning, decisions made by automated decision support systems (DSS) will increase. Besides the accuracy of predictions, their explainability becomes more important. The algorithms can construct complex mathematical prediction models. This causes insecurity to the predictions. The insecurity rises the need for equipping the algorithms with explanations. To examine how users trust automated DSS, an experiment was conducted. Our research aim is to examine how participants supported by an DSS revise their initial prediction by four varying approaches (treatments) in a between-subject design study. The four treatments differ in the degree of explainability to understand the predictions of the system. First we used an interpretable regression model, second a Random Forest (considered to be a black box [BB]), third the BB with a local explanation and last the BB with a global explanation. We noticed that all participants improved their predictions after receiving an advice whether it was a complete BB or an BB with an explanation. The major finding was that interpretable models were not incorporated more in the decision process than BB models or BB models with explanations.																	0266-4720	1468-0394															10.1111/exsy.12577		JUN 2020											
J								Two-dimensional discrete feature based spatial attention CapsNet For sEMG signal recognition	APPLIED INTELLIGENCE										Two-Dimensional discrete feature; Spatial attention; Capsule network; sEMG signal	VISUAL-ATTENTION; SELECTION; REPRESENTATION; NETWORKS	Deep learning frameworks(such as deep convolutional networks) require data to have a regular shape. However, discrete features extracted from heterogeneous data cannot be collected in a regular shape to convolute. In this article, a Two-Dimensional Discrete Feature Based Spatial Attention CapsNet(TDACAPS) is proposed to convert one-dimensional discrete features into two-dimensional structured data through Cartesian Product for surface electromyogram(sEMG) signal recognition. sEMG signal varies from person to person is the main signal source of prosthetic control. Our model transforms multi-angle discrete features into structured data to find the inherent law of sEMG signal. Due to uneven information distribution of structured data, this model combines capsule network with attention mechanism to place emphasis on abundant information regions and reduce ancillary information loss. Extensive experiments show our model yields an improvement for sEMG signal recognition of almost 3% than capsule network and other neural networks under different conditions. Our attention mechanism that employs overlapping pooling to search feature map weight is preferable to the squeeze-and-excitation module, convolutional block attention module and others. Moreover, we validate that our model has great expansibility on Wine Quality Dataset and Breast Cancer Wisconsin.																	0924-669X	1573-7497				OCT	2020	50	10					3503	3520		10.1007/s10489-020-01725-0		JUN 2020											
J								Machine learning based customer sentiment analysis for recommending shoppers, shops based on customers' review	COMPLEX & INTELLIGENT SYSTEMS										Big data; Big data analytics; Machine learning; Feature extraction; Clustering and classification; Accuracy		Big data analytics plays a major role in various industries using computing applications such as E-commerce and real-time shopping. Big data are used for promoting products and provide better connectivity between retailers and shoppers. Nowadays, people always use online promotions to know about best shops for buying better products. This shopping experience and opinion about the shopper's shop can be observed by the customer-experience shared across social media platforms. A new customer when searching a shop needs information about manufacturing date (MRD) and manufacturing price (MRP), offers, quality, and suggestions which can only be provided by the previous customer experience. The MRP and MRD are already available in the product cover or label. Several approaches have been used for predicting the product details but not providing accurate information. This paper is motivated towards applying Machine Learning algorithms for learning, analysing and classifying the product information and the shop information based on the customer experience. The product data with customer reviews is collected from benchmark Unified computing system (UCS) which is a server for data based computer product lined up for evaluating hardware, support to visualization, software management. From the results and comparison, it has been found that machine learning algorithms outperform than other approaches. The proposed HRS system has higher values of MAPE which is 96% and accuracy is nearly 98% when compared to other existing techniques. Mean absolute error of proposed HRS system is nearly 0.6 which states that the performance of the system is significantly effective.																	2199-4536	2198-6053				OCT	2020	6	3					621	634		10.1007/s40747-020-00155-2		JUN 2020											
J								Hardware-Centric AutoML for Mixed-Precision Quantization	INTERNATIONAL JOURNAL OF COMPUTER VISION										Model quantization; Mixed-precision; Automated ML; Hardware		Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to supportflexible bitwidth(1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off accuracy, latency, energy, and model size, which is both time-consuming and usually sub-optimal. There are plenty of specialized hardware accelerators for neural networks, but little research has been done to design specialized neural networks optimized for a particular hardware accelerator. The latter is demanding given the much longer design cycle of silicon than neural nets. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate the direct feedback signals to the RL agent. Compared with other conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95 x and the energy consumption by 1.9 xwith negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2035	2048		10.1007/s11263-020-01339-6		JUN 2020											
J								SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters	INTERNATIONAL JOURNAL OF COMPUTER VISION										GAN; Image translation; Facial expression synthesis; Speech synthesis; Blendshape models; Action units; 3DMM fitting; Relativistic discriminator; Emotionet; 4DFAB; LRW		Image-to-image (i2i) translation is the dense regression problem of learning how to transform an input image into an output using aligned image pairs. Remarkable progress has been made in i2i translation with the advent of deep convolutional neural networks and particular using the learning paradigm of generative adversarial networks (GANs). In the absence of paired images, i2i translation is tackled with one or multiple domain transformations (i.e., CycleGAN, StarGAN etc.). In this paper, we study the problem of image-to-image translation, under a set of continuous parameters that correspond to a model describing a physical process. In particular, we propose the SliderGAN which transforms an input face image into a new one according to the continuous values of a statistical blendshape model of facial motion. We show that it is possible to edit a facial image according to expression and speech blendshapes, using sliders that control the continuous values of the blendshape model. This provides much more flexibility in various tasks, including but not limited to face editing, expression transfer and face neutralisation, comparing to models based on discrete expressions or action units.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2629	2650		10.1007/s11263-020-01338-7		JUN 2020											
J								A network attack path prediction method using attack graph	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network attack path; Wireless network; Attack path map; Abnormal information human invasion; Yebeisi reasoning; Detection method	NEURAL-NETWORK; ALGORITHM	The prediction of intrusion intention of abnormal information in wireless network can effectively guarantee the security and stability of network. Traditional methods describe the relationship between different types of attacks. When building the model, only the path of the network nodes involved in the current attack behavior is considered, so the vulnerability of the network can not be analyzed in detail. Then, a network attack node path detection model based on attack graph is proposed. Firstly, according to the theory of attack graph, the network attack graph is defined, the right state of attacker is detected, the connection matrix of network is obtained, and the formal description of vulnerability, attack effect and attack premise is obtained. Then, the attack path graph is used to describe the transfer relationship between nodes, map the process of the attack from one host or vulnerability to the next host or vulnerability, and give the shortest path to achieve the attack intention. Further obtain the maximum possibility of intrusion under each attack path of the network, and build a network attack node path detection model based on the detection results. The experimental results show that the proposed model has high accuracy and effectively improves the efficiency of network security analysis.																	1868-5137	1868-5145															10.1007/s12652-020-02206-5		JUN 2020											
J								Vowel priority lip matching scheme and similarity evaluation model based on humanoid robot Ren-Xin	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Lip matching; Vowel priority; Similarity evaluation; Humanoid robot; Manhattan distance	SPEECH; GENERATION	At present, the significance of humanoid robots dramatically increased while this kind of robots rarely enters human life because of its immature development. The lip shape of humanoid robots is crucial in the speech process since it makes humanoid robots look like real humans. Many studies show that vowels are the essential elements of pronunciation in all languages in the world. Based on the traditional research of viseme, we increased the priority of the smooth transition of lip between vowels and propose a lip matching scheme based on vowel priority. Additionally, we also designed a similarity evaluation model based on the Manhattan distance by using computer vision lip features, which quantifies the lip shape similarity between 0-1 provides an effective recommendation of evaluation standard. Surprisingly, this model successfully compensates the disadvantages of lip shape similarity evaluation criteria in this field. We applied this lip-matching scheme to Ren-Xin humanoid robot and performed robot teaching experiments as well as a similarity comparison experiment of 20 sentences with two males and two females and the robot. Notably, all the experiments have achieved excellent results.																	1868-5137	1868-5145															10.1007/s12652-020-02175-9		JUN 2020											
J								Resilience and technological diversity in smart homes A graph-theoretic approach to modeling IoT systems with integrated heterogeneous networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart home; Science of security; Graph analysis; Modeling; Network resilience; Future networks; Internet of Things (IoT)		This article introduces our abstract modeling strategy to represent the general features and topology of the kinds of integrated and technologically diverse networks that feature in IoT systems. We begin with smart home networks. We generate instances of our model and analyze their graph-theoretic properties with an emphasis on the resilience of critical services and connections to the Global Internet. In addition to considering the network connectivity graph of nodes and links in the model, we explain ourtechnology interdependence graphtechniques.Technology interdependence graphsallow us to illuminate critical interactions in multi-technology systems such as smart homes. Using relatively simple examples we show how our approach permits the exploration of the resilience properties of various instances of smart systems involving complex technological interdependency. We describe a practical way of approaching the graphs of systems with a wide variety of integrated technologies and we discuss properties such as connectedness and other metrics. This approach can serve as the basis for tackling the challenge of designing resilient IoT-based smart-cities from the point of view of network topologies. We also study smart home resilience through path redundancy and heterogeneity of network technologies with graph centrality metrics.																	1868-5137	1868-5145															10.1007/s12652-020-02095-8		JUN 2020											
J								Rewriting Approaches for Ontology-Mediated Query Answering	KUNSTLICHE INTELLIGENZ												A most promising approach to answering queries in ontology-based data access (OBDA) is through query rewriting. In this paper we present novel rewriting approaches for several extensions of OBDA. The goal is to understand their relative expressiveness and to pave the way for efficient query answering algorithms.																	0933-1875	1610-1987															10.1007/s13218-020-00671-w		JUN 2020											
J								Reliable H-proportional to guaranteed cost control for uncertain switched fuzzy stochastic systems with multiple time-varying delays and intermittent actuator and sensor faults	NEURAL COMPUTING & APPLICATIONS										Switched Takagi-Sugeno (T-S) fuzzy stochastic systems; H-proportional to control; Guaranteed cost control; Intermittent faults (IFs); Fault-tolerant control (FTC)	NONLINEAR-SYSTEMS; TOLERANT CONTROL; INFINITY CONTROL; LINEAR-SYSTEMS; DESIGN; JUMP	In this paper, the issue of delay-dependent passive fault-tolerant control and optimal guaranteed cost control is considered for multiple time-varying delayed switched Takagi-Sugeno (T-S) fuzzy stochastic systems against intermittent actuator and sensor faults, model uncertainties, external disturbances and measurement noise. This is the first attempt to implement H-proportional to guaranteed cost control for switched T-S fuzzy stochastic systems with intermittent actuator and sensor faults which allows more kinds of faults. Thus, the method has a wider application range. A dynamic output feedback controller is designed. Then, a closed-loop system is constructed. Stability results of reliable H-proportional to guaranteed cost control are given via piecewise fuzzy Lyapunov function in terms of linear matrix inequalities. Slack matrices are not required at all. The conservatism of this paper is relatively low. At last, the effectiveness of the addressed approach is presented by explanatory examples.																	0941-0643	1433-3058															10.1007/s00521-020-05007-1		JUN 2020											
J								The explicit solution of fuzzy singular differential equations using fuzzy Drazin inverse matrix	SOFT COMPUTING										Horizontal membership functions; RDM arithmetic; Fuzzy singular differential equations; Granular order derivative; Fuzzy Drazin inverse matrix	HORIZONTAL MEMBERSHIP FUNCTION; CONTROLLED SYSTEM	This paper aims to provide a fuzzy solution for fuzzy singular differential equations (FSDEs) in which the coefficient matrices and/or initial conditions are considered as fuzzy matrices and/or numbers. In addition, the fuzzy derivative is in the sense of the granular derivative. To achieve the aim, some new concepts such as the rank and index of fuzzy matrices, and granular inverse of a nonsingular fuzzy matrix are presented. Moreover, a fuzzy matrix called fuzzy Drazin inverse matrix is defined which has a pivotal role of a fuzzy inverse matrix of a singular fuzzy matrix. Furthermore, two approaches for finding the fuzzy Drazin inverse matrix are given. In order to present the solution of FSDEs, a definition of nth order granular derivative is presented. This paper closes with some examples showing the approach is capable of finding the solution of FSDEs.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11251	11264		10.1007/s00500-020-05055-8		JUN 2020											
J								Evolutionary optimization of image processing for cell detection in microscopy images	SOFT COMPUTING										Evolutionary algorithms; Image processing; Computer vision; Heuristic optimization; Machine learning		In this paper, we present a new evolution-based algorithm that optimizes cell detection image processing workflows in a self-adaptive fashion. We use evolution strategies to optimize the parameters for all steps of the image processing pipeline and improve cell detection results. The algorithm reliably produces good cell detection results without the need for extensive domain knowledge. Our algorithm also needs no labeled data to produce good cell detection results compared to the state-of-the-art neural network approaches. Furthermore, the algorithm can easily be adapted to different applications by modifying the processing steps in the pipeline and has high scalability since it supports multithreading and computation on graphical processing units (GPUs).																	1432-7643	1433-7479															10.1007/s00500-020-05033-0		JUN 2020											
J								Safe global optimization of expensive noisy black-box functions in the delta-Lipschitz framework	SOFT COMPUTING										Safe global optimization; Expensive black-box functions; Noise; Lipschitz condition; Machine learning	ALGORITHMS WORKING; CONVERGENCE RATE; SEARCH	In this paper, the problem of safe global maximization (it should not be confused with robust optimization) of expensive noisy black-box functions satisfying the Lipschitz condition is considered. The notion "safe" means that the objective function f (x) during optimization should not violate a "safety" threshold, for instance, a certain a priori given value h in a maximization problem. Thus, any new function evaluation (possibly corrupted by noise) must be performed at "safe points" only, namely, at points y for which it is known that the objective function f (y) > h. The main difficulty here consists in the fact that the used optimization algorithm should ensure that the safety constraint will be satisfied at a point y before evaluation of f (y) will be executed. Thus, it is required both to determine the safe region Omega within the search domain D and to find the global maximum within Omega. An additional difficulty consists in the fact that these problems should be solved in the presence of the noise. This paper starts with a theoretical study of the problem, and it is shown that even though the objective function f (x) satisfies the Lipschitz condition, traditional Lipschitz minorants and majorants cannot be used due to the presence of the noise. Then, a delta-Lipschitz framework and two algorithms using it are proposed to solve the safe global maximization problem. The first method determines the safe area within the search domain, and the second one executes the global maximization over the found safe region. For both methods, a number of theoretical results related to their functioning and convergence is established. Finally, numerical experiments confirming the reliability of the proposed procedures are performed.																	1432-7643	1433-7479															10.1007/s00500-020-05030-3		JUN 2020											
J								Cortical-Inspired Wilson-Cowan-Type Equations for Orientation-Dependent Contrast Perception Modelling	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Wilson-Cowan equations; Primary visual cortex; Orientation-dependent modelling; Contrast perception; Variational modelling; Geometrical optical illusions	FUNCTIONAL ARCHITECTURE; POGGENDORFF ILLUSION; SPATIAL-FREQUENCY; REPRESENTATION	We consider the evolution model proposed in Bertalmio (Front Comput Neurosci 8:71, 2014), Bertalmio et al. (IEEE Trans Image Process 16(4):1058-1072, 2007) to describe illusory contrast perception phenomena induced by surrounding orientations. Firstly, we highlight its analogies and differences with the widely used Wilson-Cowan equations (Wilson and Cowan in BioPhys J 12(1):1-24, 1972), mainly in terms of efficient representation properties. Then, in order to explicitly encode local directional information, we exploit the model of the primary visual cortex (V1) proposed in Citti and Sarti (J Math Imaging Vis 24(3):307-326, 2006) and largely used over the last years for several image processing problems (Duits and Franken in Q Appl Math 68(2):255-292, 2010; Prandi and Gauthier in A semidiscrete version of the Petitot model as a plausible model for anthropomorphic image reconstruction and pattern recognition. SpringerBriefs in Mathematics, Springer, Cham, 2017; Franceschiello et al. in J Math Imaging Vis 60(1):94-108, 2018). The resulting model is thus defined in the space of positions and orientation, and it is capable of describing assimilation and contrast visual bias at the same time. We report several numerical tests showing the ability of the model to reproduce, in particular, orientation-dependent phenomena such as grating induction and a modified version of the Poggendorff illusion. For this latter example, we empirically show the existence of a set of threshold parameters differentiating from inpainting to perception-type reconstructions and describing long-range connectivity between different hypercolumns in V1.																	0924-9907	1573-7683															10.1007/s10851-020-00960-x		JUN 2020											
J								Western culture correct guidance and penetration teaching and its multi-dimensional training mode	SOFT COMPUTING										English teaching; Training mode; Western culture	MOTIVATION	Chinese college English teaching inevitably involves western culture, but actually great differences exist between Chinese culture and western culture. Therefore, how to establish the scientific outlook on western culture has long been a thorny issue for Chinese college English teachers. Commencing with an illustration of the necessity and importance of correct guidance and penetration western culture in Chinese college English teaching, this paper deeply analyzes the problems and shortcomings of present Chinese college English teaching in correct guidance and penetration for western culture and puts forward effective strategies of western culture introduction and penetration in Chinese college English teaching. And the paper aims to provide a useful reference for further improving the actual result of Chinese college English teaching and effectively avoiding cultural conflict between East and West.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11149	11158		10.1007/s00500-020-05004-5		JUN 2020											
J								Mathematical modeling and chemical conduct considering non-Newtonian nanofluid by utilizing heat flux features	SOFT COMPUTING										MHD Oldroyd-B nanofluids; Variable stretched surface; Thermal radiation; Cattaneo-Christov heat and mass flux; BVP4C	NATURAL-CONVECTION; MAGNETIC-FIELD; FLOW; NANOPARTICLES; IMPACT	In the current era of nanotechnology, the design of biological devices requires efficient transmission of energy among the components. Computational studies are always prerequisite for general models. In view of this importance, this paper investigated the characterizations regarding the steady flow of 2D Oldroyd-B nanofluid in the presence of thermal radiation (nonlinear) over a radially stretched sheet. Buongiorno revised nanofluid relation of the nanomaterial is instigated in precise modeling. For the mechanism of mass transmission, we utilized features of constructive-destructive prescription. As a result of modeling, the raised PDEs are converted into ODEs by appropriate transformations. The numeric scheme BVP4C is utilized for the solutions. The physical variables of the assumed flow pattern are shown graphically with the significance of the involved parameter. Besides, the prescribed investigation explored the significant impact by involved physical parameters along with the declining conduct influenced by chemical reaction parameters on the considered model. Moreover, the numerical outcomes of surface drag force and the transfer rate of heat-mass are tabulated for numerous sets of physical parameters. The improvement of these results is guaranteed by comparing it with existing techniques.																	1432-7643	1433-7479				AUG	2020	24	16					11829	11839		10.1007/s00500-020-05072-7		JUN 2020											
J								On fundamental isomorphism theorems in soft subgroups	SOFT COMPUTING										Soft sets; Soft subgroups; Normal soft subgroups; Homomorphism of soft subgroups; Quotient-terms	SETS	Molodsov initiated a novel concept of soft set theory, which is a completely new approach for modeling vagueness and uncertainty, which there is no limited condition to description of objects and is free from the difficulties affecting existing methods. This makes the theory very convenient and easy to apply in practice. After the pioneering work of Molodsov, there has been a great effort to obtain soft set analogues of classical theories. Among other fields, a progressive developments are made in the field of algebraic structure. To extend the soft set in group theory, many researchers introduced the notions of soft subgroup and investigated its applications in group theory and decision making. In this paper, by using the soft sets and their duality, we introduce new concepts on the soft sets, which are called soft quotient subgroup and quotient dual soft subgroup. We then derive their algebraic properties and, in sequel, investigate the fundamental isomorphism theorems in soft subgroups analogous to the group theory.																	1432-7643	1433-7479				AUG	2020	24	16					11841	11851		10.1007/s00500-020-05074-5		JUN 2020											
J								A comprehensive study of PAPR reduction techniques: design of DSLM-CT joint reduction technique for advanced waveform	SOFT COMPUTING										5G network; Peak power; FBMC; NOMA	NONORTHOGONAL MULTIPLE-ACCESS; LOW-COMPLEXITY; 5G NETWORKS; OFDM; SCHEME; COMMUNICATION; TRANSMITTERS; SIGNALS; NOMA; PEAK	One of the challenges in rollout of 5G is utilizing the radio communication which is not accessible by today's radio system. To achieve a high data rate, it is necessary to make 5G networks compatible with advanced waveform. In this correspondence, we discussed advanced waveform technique non-orthogonal multiple access (NOMA) and filter bank multicarrier (FBMC) for 5G network. The design of advanced form technique compatible with advanced wireless communication is very important to fulfill the vision the 5G. Peak average to power ratio (PAPR) is viewed as a significant issue in actualizing NOMA and FBMC framework. PAPR reduction design for FBMC has been presented in the writing survey, yet PAPR decrease methods in NOMA are not investigated up until this point. In that regard are different downsides of minimization techniques introduced in the review. In the present investigation, we have inspected and analyzed the presence of the reduction systems and proposed a hybrid strategy (DSLM-CT) based on discrete selective mapping (DSLM) and circular transformation technique. Further, several parameters are discussed and analyzed. At long last, it is reasoned that the exhibition of the proposed hybrid technique is better than the existed minimization methods. Additionally, it is also observed that the implementation of proposed technique reduces the power consumption of solid state power amplifier.																	1432-7643	1433-7479				AUG	2020	24	16					11893	11907		10.1007/s00500-020-05086-1		JUN 2020											
J								Multiple attribute group decision making based on 2-dimension linguistic intuitionistic fuzzy aggregation operators	SOFT COMPUTING										Multiple attribute decision making; Linguistic intuitionistic fuzzy numbers; Linguistic variables; 2-dimension linguistic variables; 2-dimension linguistic intuitionistic fuzzy variables	MULTICRITERIA; SETS; INFORMATION; ENTROPY; DIVERGENCE; CONSENSUS	The 2-dimension linguistic variables (2-DLVs) add a subjective evaluation on the reliability of the evaluation results provided by decision makers, so 2-DLVs are very useful tools for describing uncertain or fuzzy information. This work extends the idea of 2-DLVs by introducing 2-dimension linguistic intuitionistic fuzzy variables (2-DLIFVs) in which 1 class and 2 class information describe in the form of linguistic intuitionistic fuzzy numbers. The paper defines some operational laws, score, and accuracy functions for 2-DLIFVs. Further, we develop some arithmetic and geometric aggregation operators for aggregating 2-DLIF information and prove a number of valuable properties associated with them. Using the proposed aggregation operators, an approach for multiple attribute group decision making with 2-DLIF information is formulated. Finally, an illustrated example is given to verify and prove the validity of the developed method. The computed results are also compared with the existing results.																	1432-7643	1433-7479				NOV	2020	24	22					17377	17400		10.1007/s00500-020-05026-z		JUN 2020											
J								Analysis of slump and surge phenomenon in Chinese stock market based on sequence alignment method	SOFT COMPUTING										Sequence alignment; Stock market; Financial time series; Correlation; Scoring matrix	VOLATILITY	The method of sequence alignment in bioinformatics is introduced into financial time series analysis, which can capture the large-scale features of the variables, suppress the noise and reveal the implicit mode of the system from different angles without strict assumptions. Based on existing methods of sequence alignment, we propose a new scoring matrix construction method for financial sequence alignment, a purpose-oriented matrix. This method can be used to extract the feature segments of the sequence. An empirical study of the stocks of Shanghai Stock Exchange and Shenzhen Stock Exchange is implemented with this new method. The results reflect the relationship between the two stock markets under the conditions of boom or bust and confirm the feasibility and effectiveness of introducing the method into the financial analysis.																	1432-7643	1433-7479															10.1007/s00500-020-05076-3		JUN 2020											
J								Evolutionary operators for the Hamiltonian completion problem	SOFT COMPUTING										Hamiltonian completion; Evolutionary algorithm; Evolutionary operator; Crossover; Mutation; Traveling salesman	PATH COMPLETION; GENETIC ALGORITHM; LINEAR ALGORITHM; NUMBER; GRAPH; COORDINATION; SEARCH	This paper deals with evolutionary algorithms for solving the Hamiltonian completion problem. More precisely, the paper is concerned with a collection of crossover and mutation operators, which mostly originate from the traveling salesman problem, but have further on been modified or customized for Hamiltonian completion. The considered crossovers and mutations are tested on a set of randomly generated problem instances. The obtained experimental results clearly show that the behavior and relative ranking of the operators within the Hamiltonian completion environment are different than within the traveling salesman environment. Moreover, it is shown that our modified or custom-designed operator variants accomplish much better results for Hamiltonian completion than the standard variants.																	1432-7643	1433-7479															10.1007/s00500-020-05063-8		JUN 2020											
J								Applications of accelerated computational methods for quasi-nonexpansive operators to optimization problems	SOFT COMPUTING										Convex optimization; Fixed-point set; Nonexpansive; Proximal gradient method; Fixed-point algorithm; S-iteration method; Maximal monotone operator; Lasso problem	ITERATIVE ALGORITHMS; CONVERGENCE RATE; MANN ITERATIONS; PROJECTION; POLYNOMIOGRAPHY; MINIMIZATION; SETS	This paper studies the convergence rates of two accelerated computational methods without assuming nonexpansivity of the underlying operators with convex and affine domains in infinite-dimensional Hilbert spaces. One method is a noninertial method, and its convergence rate is estimated as R-T,R-{xn}(n) = o(1/root n) in worst case. The other is an inertial method, and its convergence rate is estimated as R-T,R-{yn}(n) = o(1/root n) under practical conditions. Then, we apply our results to give newresults on convergence rates for solving generalized split common fixed-point problems for the class of demimetric operators. We also apply our results to variational inclusion problems and convex optimization problems. Our results significantly improve and/or develop previously discussed fixed-point problems and splitting problems and related algorithms. To demonstrate the applicability of our methods, we provide numerical examples for comparisons and numerical experiments on regression problems for publicly available high-dimensional real datasets taken from different application domains.																	1432-7643	1433-7479															10.1007/s00500-020-05038-9		JUN 2020											
J								Global-best optimization of ANN trained by PSO using the non-extensive cross-entropy with Gaussian gain	SOFT COMPUTING										ANN-PSO; Artificial neural network; Particle swarm optimization; IOPSO-BPA; Non-extensive cross-entropy with Gaussian gain	COLONY OPTIMIZATION; ALGORITHM; PREDICTION	An efficient optimization of network weights has been the primary goal of the artificial neural network (ANN) research community since decades. The aim of every optimization problem is to minimize the network cost which is some form of error function between the desired and the actual network outputs, during the training phase. The conventional gradient-based optimization algorithms like backpropagation are likely to get trapped in local minima and are sensitive to choices of initial weights. The evolutionary algorithms have proved their usefulness in introducing randomness into the optimization procedure, since they work on a global search strategy and induce a globally minimum solution for the network weights. In this paper, we particularly focus on ANN trained by Particle Swarm Optimization (ANN-PSO), in which the local-best and global-best particle positions represent possible solutions to the set of network weights. The global-best position of the swarm, which corresponds to the minimum cost function over time, is determined in our work by minimizing a new non-extensive cross-entropy error cost function. The non-extensive cross-entropy is derived from the non-extensive entropy with Gaussian gain that has proven to give minimum values for regular textures containing periodic information represented by uneven probability distributions. The new cross-entropy is defined, and its utility for optimizing the network weights to a globally minimum solution is analyzed in this paper. Extensive experimentation on two different versions: the baseline ANN-PSO and one of its most recent variants IOPSO-BPA, on benchmark datasets from the UCI repository, with comparisons to the state of the art, validates the efficacy of our method.																	1432-7643	1433-7479															10.1007/s00500-020-05080-7		JUN 2020											
J								Category-preserving binary feature learning and binary codebook learning for finger vein recognition	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Finger vein recognition; Manifold structure; Category preservation; Binary feature learning; Codebook learning	EXTRACTION; PATTERNS	Local binary feature learning has attracted a lot of researches in image recognition due to its vital effectiveness. Generally, in the traditional local feature learning methods, a projection is learned to map the patches of image into binary features and then a codebook is generated by clustering the binary features withK-means clustering. However, these local feature learning methods, such as compact binary face descriptor and discriminative binary descriptor, ignore the category specific distributions of the original features during the feature learning process and use the real-valued clustering approach to generate the codebook, the discriminant of feature is degraded and the merits of binary feature are lost. To tack these problems, in this paper, we propose a novel category-preserving binary feature learning and binary codebook leaning (CPBFL-BCL) method for finger vein recognition. In CPBFL-BCL, the discrimination of learned binary features is generated by criteria of fisher discriminant analysis and category manifold preserving regularity during the feature learning process, and a novel binary clustering method based onK-means clustering is designed to generate binary codebook. Experimental results on recognition and retrieval tasks using two public finger vein databases are presented and demonstrate the effectiveness and efficiency of the proposed method over the state-of-the-art finger vein methods and a finger vein retrieval method.																	1868-8071	1868-808X				NOV	2020	11	11					2573	2586		10.1007/s13042-020-01143-1		JUN 2020											
J								Multi-agent active information gathering in discrete and continuous-state decentralized POMDPs by policy graph improvement	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Planning under uncertainty; Decentralized POMDP; Information gathering; Active perception	OPTIMIZATION; EXPLORATION	Decentralized policies for information gathering are required when multiple autonomous agents are deployed to collect data about a phenomenon of interest when constant communication cannot be assumed. This is common in tasks involving information gathering with multiple independently operating sensor devices that may operate over large physical distances, such as unmanned aerial vehicles, or in communication limited environments such as in the case of autonomous underwater vehicles. In this paper, we frame the information gathering task as a general decentralized partially observable Markov decision process (Dec-POMDP). The Dec-POMDP is a principled model for co-operative decentralized multi-agent decision-making. An optimal solution of a Dec-POMDP is a set of local policies, one for each agent, which maximizes the expected sum of rewards over time. In contrast to most prior work on Dec-POMDPs, we set the reward as a non-linear function of the agents' state information, for example the negative Shannon entropy. We argue that such reward functions are well-suited for decentralized information gathering problems. We prove that if the reward function is convex, then the finite-horizon value function of the Dec-POMDP is also convex. We propose the first heuristic anytime algorithm for information gathering Dec-POMDPs, and empirically prove its effectiveness by solving discrete problems an order of magnitude larger than previous state-of-the-art. We also propose an extension to continuous-state problems with finite action and observation spaces by employing particle filtering. The effectiveness of the proposed algorithms is verified in domains such as decentralized target tracking, scientific survey planning, and signal source localization.																	1387-2532	1573-7454				JUN 10	2020	34	2							42	10.1007/s10458-020-09467-6													
J								Hierarchical graph attention networks for semi-supervised node classification	APPLIED INTELLIGENCE										Graph convolutional networks; Hierarchical representation; Semi-supervisded		Recently, there has been a promising tendency to generalize convolutional neural networks (CNNs) to graph domain. However, most of the methods cannot obtain adequate global information due to their shallow structures. In this paper, we address this challenge by proposing a hierarchical graph attention network (HGAT) for semi-supervised node classification. This network employs a hierarchical mechanism for the learning of node features. Thus, more information can be effectively obtained of the node features by iteratively using coarsening and refining operations on different hierarchical levels. Moreover, HGAT combines with the attention mechanism in the input and prediction layer. It can assign different weights to different nodes in a neighborhood, which helps to improve accuracy. Experiment results demonstrate that state-of-the-art performance was achieved by our method, not only on Cora, Citeseer, and Pubmed citation datasets, but also on the simplified NELL knowledge graph dataset. The sensitive analysis further verifies that HGAT can capture global structure information by increasing the receptive field, as well as the effective transfer of node features.																	0924-669X	1573-7497				OCT	2020	50	10					3441	3451		10.1007/s10489-020-01729-w		JUN 2020											
J								UWFP-Outlier: an efficient frequent-pattern-based outlier detection method for uncertain weighted data streams	APPLIED INTELLIGENCE										Outlier detection; Weighted frequent pattern mining; Deviation indices; Uncertain weighted data streams	FRAMEWORK	In this paper, we propose an efficient frequent-pattern-based outlier detection method, namely, UWFP-Outlier, for identifying the implicit outliers from uncertain weighted data streams. For reducing the time cost of the UWFP-Outlier method, in the weighted frequent pattern mining phase, we introduce the concepts of themaximal weightandmaximal probabilityto form a compact anti-monotonic property, thereby reducing the scale of potential extensible patterns. For accurately detecting the outliers, in the outlier detection phase, we design two deviation indices to measure the deviation degree of each transaction in the uncertain weighted data streams by considering more factors that may influence its deviation degree; then, the transactions which have large deviation degrees are judged as outliers. The experimental results indicate that the proposed UWFP-Outlier method can accurately detect the outliers from uncertain weighted data streams with a lower time cost.																	0924-669X	1573-7497				OCT	2020	50	10					3452	3470		10.1007/s10489-020-01718-z		JUN 2020											
J								A new hybrid stability measure for feature selection	APPLIED INTELLIGENCE										Feature evaluation and selection; Gene selection; Stability measure; Similarity-based stability; Frequency-based stability	MICROARRAY; REPRODUCIBILITY	Feature Selection (FS) algorithms are applied in bioinformatics applications to identify the disease causing genes. Performance of such algorithms is measured in terms of accuracy of the model and stability of FS algorithms. Stability evaluates the identical replication of feature sets obtained after every execution. Recently research has shown that a stability measure must satisfy set of properties like, fully defined, monotonicity, boundedness, deterministic maximum stability, and correction for chance. Among the existing stability measures, only Nogueira's frequency based stability measure satisfies all the required properties. However, frequency based stability measures fail to discriminate among the cases when overall frequency of features are same. In order to address this issue, the paper proposes a hybrid similarity based stability measure which satisfies all the desirable properties, as mentioned earlier. The proposed stability measure is unique as it is the first similarity based stability measure that satisfies all the required properties. Also, all these essential properties are mathematically established. Further, the paper also proposes a combination of frequency based and similarity based measure which preserves all the aspects of both the approaches. The work presented also analyzes the stability performance of LASSO and Elastic Net, using synthetic and microarray gene expression datasets. Elastic Net depicts higher stability and selection of relevant features.																	0924-669X	1573-7497				OCT	2020	50	10					3471	3486		10.1007/s10489-020-01731-2		JUN 2020											
J								Maximum likelihood-based influence maximization in social networks	APPLIED INTELLIGENCE										Influence Maximization; Independent Cascade Model; Maximum Likelihood	SCALABLE INFLUENCE MAXIMIZATION; NODES; DIFFUSION	Influence Maximization (IM) is an important issue in network analyzing which widely occurs in social networks. The IM problem aims to detect the top-kinfluential seed nodes that can maximize the influence spread. Although a lot of studies have been performed, a novel algorithm with a better balance between time-consumption and guaranteed performance is still needed. In this work, we present a novel algorithm called MLIM for the IM problem, which adopts maximum likelihood-based scheme under the Independent Cascade(IC) model. We construct thumbnails of the social network and calculate theL-value for each vertex using the maximum likelihood criterion. A greedy algorithm is proposed to sequentially choose the seeds with the smallestL-value. Empirical results on real-world networks have proved that the proposed method can provide a wider influence spreading while obtaining lower time consumption.																	0924-669X	1573-7497				OCT	2020	50	10					3487	3502		10.1007/s10489-020-01747-8		JUN 2020											
J								VTAAN: Visual Tracking with Attentive Adversarial Network	COGNITIVE COMPUTATION										Visual tracking; Adversarial learning; Attention module	OBJECT TRACKING	Existing tracking methods might suffer from the performance degradation due to insufficient positive samples. A typical network structure is proposed to enrich positive samples by generating masks during the tracking process. Although this structure has achieved good results, it ignores the drift problem that occurs when the tracked object is very similar to the surrounding objects. This problem is particularly significant when background interference exists and similar objects appear. To handle this problem, in this paper, we propose a novel attentive adversarial network for visual tracking. Inspired by human visual cognitive system, we propose to employ an attention mechanism to focus on each region differing the target object from the background. At the same time, we use a variant of the cross entropy (CE) function to deal with the class imbalance problem. Our network shows favorable performance compared with state-of-the-art methods on existing tracking benchmark datasets. We conclude that our novel attentive adversarial network not only enriches positive samples in the feature space but also prevents the similarity drift problem.																	1866-9956	1866-9964															10.1007/s12559-020-09727-3		JUN 2020											
J								Moving object detection based on unified model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Camera jitter; Dynamic background; Moving object detection; Non-maximum suppression and unified model (Yolov3-improved non-maximum suppression (INMS))		Moving object detection is an essential step in several computer visions like salient object detection, visual object tracking, and video surveillance etc. Many existing methods have a drawback of low efficiency in the challenging scenes like dynamic background, camera jitter, and bad weather images. In this research, Unified model (Yolov3-Improved Non-Maximum Suppression (INMS)) method is proposed to increases the performance in moving object detection. The Change Detection net (CDNET) dataset was trained and also COCO 2014 and PascalVOC data sets were applied to analyse the performance of the developed model. The experimental analysis shows that the developed method has higher efficiency in detecting objects in camera jitter and dynamic background scene. The performance evaluation has been done using the precision, recall, IoU and the accuracy metrics for the proposed model. The results show that the developed model has the ability to effectively identify multiple objects in the dynamic background, while the existing method has the capacity to identify only single object.																	1868-5137	1868-5145															10.1007/s12652-020-02170-0		JUN 2020											
J								Integrating HSICBFO and FWSMOTE algorithm-prediction through risk factors in cervical cancer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cervical carcinoma (CC); Minority class; Data imputation; Measures; Classification; Risk factor	IMBALANCED DATA; GENE SELECTION; NEURAL-NETWORK; CLASSIFICATION; PROFILES; SMOTE	The prominent objective of cervical carcinoma (CC) prediction lies in the optimal feature selection and balanced data. The problem of majority and minority class samples are solved in the proposed work. The objective of the work lies in solving imbalanced data distribution, and of risk factor validation in cervical cancer prediction. Feature Weighted Synthetic Minority Oversampling Technique (FWSMOTE) algorithm solves the minority class issues. The missing data imputation is performed by the Mode and Median Missing Data imputation. For optimal feature selection, Hilbert-Schmidt Independence Criterion with Bacteria Forage Optimization (HSICBFO) algorithm is implemented. Ensemble Support Vector Machine with Interpolation classifier is used for cancer prediction. Various measures are deployed to analyze the performance of the proposed classifier and produces 94.77%, 93.38%, 93.86%, 94.07%, 93.60% and 93.62% for precision, recall, specificity, F-Measure, accuracy and G-mean that helps in identifying the risk level of cervical carcinoma development and guidance for further diagnosis.																	1868-5137	1868-5145															10.1007/s12652-020-02194-6		JUN 2020											
J								GRBF-NN based ambient aware realtime adaptive communication in DVB-S2	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Real-time; GRBF-NN; MODCOD; DVB-S2; DE; FRBS; Adaptive power; QoS	SATELLITE; 2ND-GENERATION; PROPAGATION; ALLOCATION; MODULATION; NETWORKS	Adaptive communication is among the hottest areas of research in almost all types of modern communication systems like cellular, Wi-Fi and broadcast systems. In this technique, various radio parameters like modulation symbol, code rate and transmit power etc. are customized according to the erratic channel state information (CSI) on the link. By optimal link utilization, it is proven that the technique is promising in terms of enhanced data rates and quality of service (QoS) even in poor channel conditions. Digital video broadcast-second generation (DVB-S2) has a built-in support for adaptive coding and modulation. Researchers have employed soft-computing and evolutionary algorithms to find the appropriate MODCOD (modulation/code) and power vector for the next transmission interval to combat channel hostilities and to fulfil the QoS demand. However, such algorithms take significant time to tapper-off, hence the solution may not be feasible for the real-time environments. To overcome this issue, in this paper, a real time, dynamic MODCOD and power allocation technique using a Gaussian radial basis function neural network (GRBF-NN), is proposed. Once sufficiently trained, the GRBF-NN can suggest the appropriate MODCOD and power vector for the next transmission interval based on the CSI and QoS demands per TV channel. The example data is generated by a fuzzy rule based system and differential evolution algorithm to suggest MODCOD and power vector, respectively. From the simulation results it is evident that the proposed scheme is swift and promising in terms of link utilization and QoS compared to the schemes in the literature. A spectral efficiency of 6 bits/s/Hz is achieved compared to previous approaches with 4.5 bits/s/Hz and 4.2 bits/s/Hz, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-02174-w		JUN 2020											
J								FCM clustering and FLS based CH selection to enhance sustainability of wireless sensor networks for environmental monitoring applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster head (CH); Fuzzy-c-means (FCM); Fuzzy logic system (FLS); Stability; Sustainability; Wireless sensor networks (WSNs)	FUZZY-LOGIC; OPTIMIZATION ALGORITHMS; ENERGY EFFICIENCY; INTERNET; DEPLOYMENT; LIFETIME; THINGS	Wireless Sensor Networks (WSNs) are the physical monitoring infrastructure of the Internet of Things (IoT) technology. For IoT based monitoring systems, the WSNs need to be sustainable with a maximum number of alive nodes so that the monitoring is effective. The sensor nodes are battery-driven and hence energy efficiency is one of the major challenges. Based on the clustering methods and the selection of Cluster Head (CH), the energy consumption of the sensor nodes can be minimized. In this research work, a clustering protocol based on fuzzy techniques is proposed to improve the stability and sustainability of WSN. Fuzzy techniques are used to tackle uncertainties occurring in wireless sensor networks. Clusters are formed based on Fuzzy-c-means (FCM) algorithm. The aim is to group the nodes properly so as to reduce intra-cluster communication distances. The CHs are then selected based on the Fuzzy Logic System (FLS). The performance of the proposed protocol is observed for an increase in the coverage area and node density. The proposed protocol is also analyzed for different sink locations. Due to better network stability and sustainability, the proposed protocol can be used for large scale IoT based monitoring systems.																	1868-5137	1868-5145															10.1007/s12652-020-02159-9		JUN 2020											
J								Design and implementation of fuzzy priority deadline job scheduling algorithm in heterogeneous grid computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy logic; Task scheduling; FPDSA; EDF; IDSA; PDSA	SEARCH ALGORITHM; TASKS	In heterogeneous distributed computing system, huge amount of attention is attracted by task scheduling process in recent days. Scheduling of task is an important problem, where, overall completion of task is completed within a shortest achievable time by scheduling dissimilar task to target processors. This research work concentrates on designing and implementing a fuzzy priority deadline based task scheduling algorithm (FPDSA) having a fuzzy deadline limitation to competent job execution. This proposed FPDSA algorithm is compared with conventional task scheduling techniques like,Improvised Deadline Scheduling Algorithm (IDSA), Earliest Deadline First (EDF) and Prioritized Based Deadline Scheduling Algorithm with respect to Average Actual Execution (AAE) and amount of Non-Delayedand Delayed Jobs. For 4000 tasks, proposed algorithm achieves 7.45%, 27.94% and 30.84% less AAE than IDSA, EDF, and PDSA and for same number of tasks, computational results by proposed FPDSA for non-delayed tasks are 0.32%, 2.17%, and 1.70% higher than IDSA, EDF, and PDSA. This enhances proposed FPDSA's performance when compared to present scheduling algorithms and illustratesFPDSA is more appropriate scheduling technique forgrid system.																	1868-5137	1868-5145															10.1007/s12652-020-02171-z		JUN 2020											
J								A sensor deployment approach for target coverage problem in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Sensor deployment; Particle swarm optimization; Fuzzy logic	PARTICLE SWARM OPTIMIZATION; ENERGY-EFFICIENT COVERAGE; LIFETIME	Sensor deployment is a fundamental issue in Wireless Sensor Networks (WSNs). Studies on the coverage problem are concentrated on deploying sensors to cover determined areas or a set of determined points (targets). On the other hand, the sensor network should have a lifetime long enough to fulfill the application requirements. Therefore, prolonging the network lifetime must be taken into account in designing a solution for a WSN that covers determined points. The problem of determining the location of sensor nodes such that the targets are covered and the network lifetime is maximized is called the sensor deployment problem. So far, various approaches, such as heuristics and approximation algorithms, have been used to solve the sensor deployment problem in WSNs. In this paper, we use two improved versions of the Particle Swarm Optimization (PSO) algorithm to solve the sensor deployment problem. The first one is cooperative PSO and the second one is improved cooperative PSO using fuzzy logic. The simulation results show that the algorithms proposed in this paper solve the target coverage problem with a longer network lifetime compared to the other similar algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02195-5		JUN 2020											
J								Deep convolutional neural network aided optimization for cold spray 3D simulation based on molecular dynamics	JOURNAL OF INTELLIGENT MANUFACTURING										Molecular dynamics; Convolutional neural network; Optimization; Cold spray; Boosted decision trees	PARTICLES; BEHAVIOR; IMPACT; TECHNOLOGY; SUBSTRATE; SPH	This study proposed a deep convolutional neural network (DCNN) aided optimization (DCNNAO) method to improve the quality of deposition during the cold spray process which was simulated by molecular dynamics (MD). The idea of the DCNNAO is to extract the value of the objective function from the MD simulation snapshots directly by DCNN aided image process technique. Considering the huge memory requirement for MD result files, the main superiority of DCNNAO is to reduce the memory requirement and improve the efficiency of the optimization process by using a contour image (several hundred kilobytes) as the input instead of an MD result file (several hundred gigabytes). To complete this strategy, a Python script is written to generate required snapshots from result files automatically. Moreover, three boosted decision trees based optimization methods including surrogate optimization and heuristic algorithms are also implemented for comparison study. A detailed optimization result demonstrates that all the above methods can obtain an acceptable solution. The comparison is also given for an informed selection of them based on the trade-off between efficiency and accuracy.																	0956-5515	1572-8145															10.1007/s10845-020-01599-6		JUN 2020											
J								An empirical analysis of binary transformation strategies and base algorithms for multi-label learning	MACHINE LEARNING										Multi-label learning; Binary transformation; Comparison of strategies; Base algorithms; Empirical analysis	CLASSIFIERS; DATASETS; MODELS	Investigating strategies that are able to efficiently deal with multi-label classification tasks is a current research topic in machine learning. Many methods have been proposed, making the selection of the most suitable strategy a challenging issue. From this premise, this paper presents an extensive empirical analysis of the binary transformation strategies and base algorithms for multi-label learning. This subset of strategies uses the one-versus-all approach to transform the original data, generating one binary data set per label, upon which any binary base algorithm can be applied. Considering that the influence of the base algorithm on the predictive performance obtained by the strategies has not been considered in depth by many empirical studies, we investigated the influence of distinct base algorithms on the performance of several strategies. Thus, this study covers a family of multi-label strategies using a diversified range of base algorithms, exploring their relationship over different perspectives. This finding has significant implications concerning the methodology of evaluation adopted in multi-label experiments containing binary transformation strategies, given that multiple base algorithms should be considered. Despite these improvements in strategy and base algorithms, for many data sets, a large number of labels, mainly those less frequent, were either never predicted, or always misclassified. We conclude the experimental analysis by recommending strategies and base algorithms in accordance with different performance criteria.																	0885-6125	1573-0565				AUG	2020	109	8					1509	1563		10.1007/s10994-020-05879-3		JUN 2020											
J								Automated design of error-resilient and hardware-efficient deep neural networks	NEURAL COMPUTING & APPLICATIONS										Neural network hardware; Error resilience; Hardware faults; Neural architecture search; Multi-objective optimization; AutoML	FAULT-TOLERANCE	Applying deep neural networks (DNNs) in mobile and safety-critical systems, such as autonomous vehicles, demands a reliable and efficient execution on hardware. The design of the neural architecture has a large influence on the achievable efficiency and bit error resilience of the network on hardware. Since there are numerous design choices for the architecture of DNNs, with partially opposing effects on the preferred characteristics (such as small error rates at low latency), multi-objective optimization strategies are necessary. In this paper, we develop an evolutionary optimization technique for the automated design of hardware-optimized DNN architectures. For this purpose, we derive a set of inexpensively computable objective functions, which enable the fast evaluation of DNN architectures with respect to their hardware efficiency and error resilience. We observe a strong correlation between predicted error resilience and actual measurements obtained from fault injection simulations. Furthermore, we analyze two different quantization schemes for efficient DNN computation and find one providing a significantly higher error resilience compared to the other. Finally, a comparison of the architectures provided by our algorithm with the popular MobileNetV2 and NASNet-A models reveals an up to seven times improved bit error resilience of our models. We are the first to combine error resilience, efficiency, and performance optimization in a neural architecture search framework.																	0941-0643	1433-3058															10.1007/s00521-020-04969-6		JUN 2020											
J								Projection wavelet weighted twin support vector regression for OFDM system channel estimation	ARTIFICIAL INTELLIGENCE REVIEW										Twin support vector regression; Wavelet transform; Unconstrained minimization; Channel estimation; OFDM	MACHINE	In this paper, an efficient projection wavelet weighted twin support vector regression (PWWTSVR) based orthogonal frequency division multiplexing system (OFDM) system channel estimation algorithm is proposed. Most Channel estimation algorithms for OFDM systems are based on the linear assumption of channel model. In the proposed algorithm, the OFDM system channel is consumed to be nonlinear and fading in both time and frequency domains. The PWWTSVR utilizes pilot signals to estimate response of nonlinear wireless channel, which is the main work area of SVR. Projection axis in optimal objective function of PWWRSVR is sought to minimize the variance of the projected points due to the utilization of a priori information of training data. Different from traditional support vector regression algorithm, training samples in different positions in the proposed PWWTSVR model are given different penalty weights determined by the wavelet transform. The weights are applied to both the quadratic empirical risk term and the first-degree empirical risk term to reduce the influence of outliers. The final regressor can avoid the overfitting problem to a certain extent and yield great generalization ability for channel estimation. The results of numerical experiments show that the propose algorithm has better performance compared to the conventional pilot-aided channel estimation methods.																	0269-2821	1573-7462															10.1007/s10462-020-09853-2		JUN 2020											
J								A deep contractive autoencoder for solving multiclass classification problems	EVOLUTIONARY INTELLIGENCE										Deep auto encoder; Contractive auto encoder; Feature reduction; Classification; MNIST variants	ALGORITHM; NETWORK	Contractive auto encoder (CAE) is on of the most robust variant of standard Auto Encoder (AE). The major drawback associated with the conventional CAE is its higher reconstruction error during encoding and decoding process of input features to the network. This drawback in the operational procedure of CAE leads to its incapability of going into finer details present in the input features by missing the information worth consideration. Resultantly, the features extracted by CAE lack the true representation of all the input features and the classifier fails in solving classification problems efficiently. In this work, an improved variant of CAE is proposed based on layered architecture following feed forward mechanism named as deep CAE. In the proposed architecture, the normal CAEs are arranged in layers and inside each layer, the process of encoding and decoding take place. The features obtained from the previous CAE are given as inputs to the next CAE. Each CAE in all layers are responsible for reducing the reconstruction error thus resulting in obtaining the informative features. The feature set obtained from the last CAE is given as input to the softmax classifier for classification. The performance and efficiency of the proposed model has been tested on five MNIST variant-datasets. The results have been compared with standard SAE, DAE, RBM, SCAE, ScatNet and PCANet in term of training error, testing error and execution time. The results revealed that the proposed model outperform the aforementioned models.																	1864-5909	1864-5917															10.1007/s12065-020-00424-6		JUN 2020											
J								Distributed matrix factorization based on fast optimization for implicit feedback recommendation	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Personalized recommendation; Collaborative filtering; User and item recommendation; Fast optimization; Distributed matrix factorization; Spark		In big data scenarios, matrix factorization (MF) is widely used in recommendation systems as it can offer high accuracy and scalability. However, when using MF to process large-scale implicit feedback data, the following two problems arise. One is that it is difficult to effectively obtain negative feedback information, which causes relatively poor recommendation accuracy. The other is that the limited resources of a single machine make the model training inefficient, and in particular, the acquisition of negative feedback information further increases the time complexity of model training. In order to solve the above two problems, we first propose a user-activity and item-popularity weighted matrix factorization (UIWMF) recommendation algorithm, which assigns every missing data different weight based on user activity and item popularity, gets negative feedback information more realistically, and leads to better recommendation accuracy. Meanwhile, in order to reduce the additional computational overhead caused by the weight strategy, we develop a fast optimization strategy to enhance the efficiency. In order to break the resource constraints of a single machine, we propose a distributed UIWMF (DUIWMF) algorithm based on Spark, which adopts an efficient parallel learning algorithm to train the model and utilizes cached in-block and out-block information to effectively reduce the communication overhead in a distributed environment. We conduct experiments on three public datasets, and the experimental results demonstrate that, comparing with the baseline MF methods, DUIWMF model has comparable performance in terms of recommendation accuracy and model training efficiency.																	0925-9902	1573-7675															10.1007/s10844-020-00601-0		JUN 2020											
J								Teaching NICO How to Grasp: An Empirical Study on Crossmodal Social Interaction as a Key Factor for Robots Learning From Humans	FRONTIERS IN NEUROROBOTICS										crossmodal learning; developmental robotics; neurocognitive models; human-robot interaction; visuomotor learning		To overcome novel challenges in complex domestic environments, humanoid robots can learn from human teachers. We propose that the capability for social interaction should be a key factor in this teaching process and benefits both the subjective experience of the human user and the learning process itself. To support our hypothesis, we present a Human-Robot Interaction study on human-assisted visuomotor learning with the robot NICO, the Neuro-Inspired COmpanion, a child-sized humanoid. NICO is a flexible, social platform with sensing and manipulation abilities. We give a detailed description of NICO's design and a comprehensive overview of studies that use or evaluate NICO. To engage in social interaction, NICO can express stylized facial expressions and utter speech via an Embodied Dialogue System. NICO is characterized in particular by combining these social interaction capabilities with the abilities for human-like object manipulation and crossmodal perception. In the presented study, NICO acquires visuomotor grasping skills by interacting with its environment. In contrast to methods like motor babbling, the learning process is, in part, supported by a human teacher. To begin the learning process, an object is placed into NICO's hand, and if this object is accidentally dropped, the human assistant has to recover it. The study is conducted with 24 participants with little or no prior experience with robots. In therobot-guidedexperimental condition, assistance is actively requested by NICO via the Embodied Dialogue System. In thehuman-guidedcondition, instructions are given by a human experimenter, while NICO remains silent. Evaluation using established questionnaires like Godspeed, Mind Perception, and Uncanny Valley Indices, along with a structured interview and video analysis of the interaction, show that the robot's active requests for assistance foster the participant's engagement and benefit the learning process. This result supports the hypothesis that the ability for social interaction is a key factor for companion robots that learn with the help of non-expert teachers, as these robots become capable of communicating active requests or questions that are vital to their learning process. We also show how the design of NICO both enables and is driven by this approach.																	1662-5218					JUN 9	2020	14								28	10.3389/fnbot.2020.00028													
J								Exploring Perception Uncertainty for Emotion Recognition in Dyadic Conversation and Music Listening	COGNITIVE COMPUTATION										Emotion prediction; Multi-task learning; Perception uncertainty modelling; Dynamic learning	OF-THE-ART; SPEECH; DEEP	Predicting emotions automatically is an active field of research in affective computing. Considering the property of the individual's subjectivity, the label of an emotional instance is usually created based on opinions from multiple annotators. That is, the labelled instance is often accompanied with the corresponding inter-rater disagreement information, which we call here the perception uncertainty. Such uncertainty information, as shown in previous studies, can provide supplementary information for better recognition performance in such a subjective task. In this paper, we propose a multi-task learning framework to leverage the knowledge of perception uncertainty to ameliorate the prediction performance. In particular, in our novel framework, the perception uncertainty is exploited in an explicit manner to manipulate an initial prediction dynamically, in contrast to merely estimating the emotional state and perception uncertainty simultaneously, as done in a conventional multi-task learning framework. To evaluate the feasibility and effectiveness of the proposed method, we perform extensive experiments for time- and value-continuous emotion predictions in audiovisual conversation and music listening scenarios. Compared with other state-of-the-art approaches, our approach yields remarkable performance improvements in both datasets. The obtained results indicate that integrating the perception uncertainty information can enhance the learning process.																	1866-9956	1866-9964															10.1007/s12559-019-09694-4		JUN 2020											
J								Efficient mining of the most significant patterns with permutation testing	DATA MINING AND KNOWLEDGE DISCOVERY										Statistical pattern mining; Hypothesis testing; Top-kpatterns	STATISTICAL SIGNIFICANCE	The extraction of patterns displaying significant association with a class label is a key data mining task with wide application in many domains. We introduce and study a variant of the problem that requires to mine the top-kstatistically significant patterns, thus providing tight control on the number of patterns reported in output. We developTopKWY, the first algorithm to mine the top-ksignificant patterns while rigorously controlling the family-wise error rate of the output, and provide theoretical evidence of its effectiveness.TopKWYcrucially relies on a novel strategy to explore statistically significant patterns and on several key implementation choices, which may be of independent interest. Our extensive experimental evaluation shows thatTopKWYenables the extraction of the most significant patterns from large datasets which could not be analyzed by the state-of-the-art. In addition,TopKWYimproves over the state-of-the-art even for the extraction ofallsignificant patterns.																	1384-5810	1573-756X				JUL	2020	34	4					1201	1234		10.1007/s10618-020-00687-8		JUN 2020											
J								Algorithmic Fairness in Mortgage Lending: from Absolute Conditions to Relational Trade-offs	MINDS AND MACHINES										Algorithmic fairness; Mortgage discrimination; Fairness trade-offs; Machine learning; Technology ethics	DISCRIMINATION	To address the rising concern that algorithmic decision-making may reinforce discriminatory biases, researchers have proposed many notions of fairness and corresponding mathematical formalizations. Each of these notions is often presented as a one-size-fits-all, absolute condition; however, in reality, the practical and ethical trade-offs are unavoidable and more complex. We introduce a new approach that considers fairness-not as a binary, absolute mathematical condition-but rather, as a relational notion in comparison to alternative decisionmaking processes. Using US mortgage lending as an example use case, we discuss the ethical foundations of each definition of fairness and demonstrate that our proposed methodology more closely captures the ethical trade-offs of the decision-maker, as well as forcing a more explicit representation of which values and objectives are prioritised.																	0924-6495	1572-8641															10.1007/s11023-020-09529-4		JUN 2020											
J								Performance analysis of ultra-dense heterogeneous network switching technology based on region awareness Bayesian decision	SOFT COMPUTING										Region awareness; Bayesian decision; Ultra-dense heterogeneous network; Handover; 5G	VERTICAL HANDOVER DECISION; ALGORITHM; FRAMEWORK	The network switching is one of the most important techniques, which keeps continuous communication between two users. A variety of approaches and strategies (such as fuzzy logic control, neural network, smart algorithm etc.) have been proposed to confront this problem. These approaches and strategies play an important role in reducing delays, decreasing drop call rates, and improving QoS during switching. However, the existing techniques and strategies often apply to some special scenarios, such as between WLAN and WiFi (or WiMAX, or 3G, or UTMS and LTE). Facing the ultra-dense heterogeneous network in the 5G communication system, this brings great difficulties to the switching, especially how to properly select a service network. Whether the existing methods and strategies are feasible remains to be studied. For solving the switching in a complication networks environment, a novel switching way is proposed in this paper. We adopt the technology of regional awareness and combine with Bayes' decision strategy to explore the switching of ultra-dense heterogeneous network. This way effectively solves the difficult problem of selecting a service network in the convention. Finally, we analyze the err probability of the proposed way. The experimental results show that our scheme can properly select the switched network in the 5G system, and the probability of the handover error is the lowest, which ensures the rationality and effectiveness of the network handover. Therefore, the proposed way in this paper is feasible.																	1432-7643	1433-7479															10.1007/s00500-020-05077-2		JUN 2020											
J								The datafication of the worldview	AI & SOCIETY										Data; Worldview; Technological imaginaries; Material hermeneutics; Second digital turn		The goal of this article is twofold. First, it aims at sketching the outlines of material hermeneutics as a three-level analysis of technological artefacts. In the first section, we introduce Erwin Panofsky's three levels of interpretation of an artwork, and we propose to import this approach in the field of philosophy of technology. Second, the rest of the article focuses on the third level, with a specific attention towards big data and algorithms of artificial intelligence. The thesis is that these new technologies are not only radically transforming our interactions with the world, or our modes of production and consumption, but also our worldview. In the second section, we rely on Panofsky'sGothic Architecture and Scholasticismto describe the Scholastic "mental habit" or worldview and its principles. In the third section, we confront this worldview with the mechanistic and informationistic worldviews. Our contribution consists in arguing that (1) despite the differences, the Scholastic, mechanistic, and informationistic worldviews are part of the same logical and causal order that dominated Western epistemology, and (2) today we are facing the appearance of a new worldview that we call "data worldview". Examples from design, architecture, and visualization of knowledge will be set all along the article.																	0951-5666	1435-5655															10.1007/s00146-020-00989-x		JUN 2020											
J								Query set centered sparse projection learning for set based image classification	APPLIED INTELLIGENCE										Query set; Sparse projection learning; Set based image classification; Discriminate subspace learning	COLLABORATIVE REPRESENTATION; RECOGNITION; MACHINE	Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employingl(2,1)norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set withl(2,1)norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.																	0924-669X	1573-7497				OCT	2020	50	10					3400	3411		10.1007/s10489-020-01730-3		JUN 2020											
J								A survey on automatic image annotation	APPLIED INTELLIGENCE										Computer vision; Image annotation; Tag assignment; Image retrieval	MAXIMUM-LIKELIHOOD-ESTIMATION; MACHINE TRANSLATION; MODEL; SHAPE	Automatic image annotation is a crucial area in computer vision, which plays a significant role in image retrieval, image description, and so on. Along with the internet technique developing, there are numerous images posted on the web, resulting in the fact that it is a challenge to annotate images only by humans. Hence, many computer vision researchers are interested in automatic image annotation and make a great effort in optimizing its performance. Automatic image annotation is a task that assigns several tags in a limited vocabulary to describe an image. There are many algorithms proposed to tackle this problem and all achieve great performance. In this paper, we review seven algorithms for automatic image annotation and evaluate these algorithms leveraging different image features, such as color histograms and Gist descriptor. Our goal is to provide insights into the automatic image annotation. A lot of comprehensive experiments, which are based on Corel5K, IAPR TC-12, and ESP Game datasets, are designed to compare the performance of these algorithms. We also compare the performance of traditional algorithms employing deep learning features. Considering that not all associated labels are annotated by human annotators, we leverage the DIA metrics on IAPR TC-12 and ESP Game datasets.																	0924-669X	1573-7497				OCT	2020	50	10					3412	3428		10.1007/s10489-020-01696-2		JUN 2020											
J								Incremental small sphere and large margin for online recognition of communication jamming	APPLIED INTELLIGENCE										Anti-jamming; Small sphere and large margin; Support vector data description; Support vector machine; Online recognition	SUPPORT VECTOR MACHINES	In the anti-jamming field of radio communication, the problem of online and multiclass jamming recognition is fundamental to implement reasonable anti-jamming measures. The incremental small sphere and large margin (IncSSLM) is proposed, this model can learn the compact boundary for own communication signals and known jamming, which relieves the open-set problem of radio data. Meanwhile it can also update the model of classifier in real time, which avoids the large memory requirement for vast jamming data and saving much time for training. The core of proposed method is the small sphere and large margin (SSLM) approach, which makes the spherical area as compact as possible, like support vector data description (SVDD), and also makes the margin between them as far as possible, like support vector machine (SVM). In other words, it can minimize intra-class divergence and maximize inter-class space. Therefore, there is a significant enhancement of recognition performance when compared with open classifiers such as SVM, and considerable superiority of training efficiency when compared with the canonical SSLM algorithm. Numerical experiments based on synthetic data, practical complex feature data of high-resolution range profile (HRRP), and jamming data of radio communication demonstrate that IncSSLM is efficient and promising for multiple and online recognition of vase and open-set radio jamming.																	0924-669X	1573-7497				OCT	2020	50	10					3429	3440		10.1007/s10489-020-01717-0		JUN 2020											
J								Overview and methods of correlation filter algorithms in object tracking	COMPLEX & INTELLIGENT SYSTEMS										Artificial intelligence; Object tracking; Correlation filter algorithms; Dataset; Template update strategy	VISUAL TRACKING	An important area of computer vision is real-time object tracking, which is now widely used in intelligent transportation and smart industry technologies. Although the correlation filter object tracking methods have a good real-time tracking effect, it still faces many challenges such as scale variation, occlusion, and boundary effects. Many scholars have continuously improved existing methods for better efficiency and tracking performance in some aspects. To provide a comprehensive understanding of the background, key technologies and algorithms of single object tracking, this article focuses on the correlation filter-based object tracking algorithms. Specifically, the background and current advancement of the object tracking methodologies, as well as the presentation of the main datasets are introduced. All kinds of methods are summarized to present tracking results in various vision problems, and a visual tracking method based on reliability is observed.																	2199-4536	2198-6053															10.1007/s40747-020-00161-4		JUN 2020											
J								Grey wolf optimizer for optimal sizing of hybrid wind and solar renewable energy system	COMPUTATIONAL INTELLIGENCE										grey wolf optimization; hybrid renewable energy; nature inspired algorithm; optimization	SIZE OPTIMIZATION; ALGORITHM; METHODOLOGY; PERFORMANCE; DELIVERY; STORAGE; MODELS; COST	By taking facts such as oil depletion, increasing number of population and energy demand into account, alternative electric generation scheme called renewable energy has entered into a new phase. These new energy sources are environmentally clean, exhaustible and friendly with affordable cost, and high reliability. Nowadays, energy generators such as photovoltaic (PV), wind turbine (WT), and geothermal energies are among the commonly used renewable sources. In this article, grey wolf optimization (GWO) methodology is proposed for minimizing the total annual cost of hybrid of wind and solar renewable energy system. Here, determining the optimal number of solar panels, WTs, and batteries which can satisfy the desired load is the main objective of this research. The obtained result shows that the proposed methodology finds optimal solution of sizing of the hybrid system with relatively lower total annual cost and fast convergence rate. To check whether the obtained result was feasible, GWO results are compared with the results of PSO, iteration method and by the work of other scholars in literature. Here the superior capabilities of GWO algorithm have been seen. It is hoped that this research would be beneficial and can be benchmark for researchers of the field.																	0824-7935	1467-8640															10.1111/coin.12349		JUN 2020											
J								Chaotic multi verse optimizer based fuzzy logic controller for frequency control of microgrids	EVOLUTIONARY INTELLIGENCE										Load frequency control (LFC); Microgrid; Renewable energy sources (RESs); Multi verse optimizer (MVO); PID controller; Fuzzy logic	SEARCH ALGORITHM; CONTROL STRATEGY; PID CONTROLLER; DESIGN; GRIDS	This paper represents an arrangement of fuzzy PD and derivative filter based PID controller (FPIDF) with its optimally designed membership functions and is employed for the load frequency control (LFC) of isolated multi- microgrids. The performance of a recently developed multi verse optimizer (MVO) algorithm is improved by introducing chaotic map appropriately in the algorithm. The improved algorithm, named as CMVO algorithm is applied to tune the proportional integral derivative (PID) controller gains for the LFC of microgrid. The superiority of the proposed CMVO algorithm is examined by its superior performance compared to MVO PID, recently published IAYA PID, JAYA PID and GA PID in the microgrid system. Further, CMVO based FPIDF with its optimized membership position (FPIDF-OM) outperforms compared to CMVO based FPIDF, IJAYA tuned FPID and FPD/PI-PD controller in microgrid system. Better dynamic performances are found with the proposed technique for stochastic type wind power, solar irradiance and load variations in the microgrid compared with FPIDF and PID. The robustness of the proposed technique is established by the sensitivity analysis. The proposed design approach has been extended to a microgrid model including biogas turbine generator, biodiesel engine generator and aqua electrolyser along with fuel cell unit. Finally, OPAL-RT based hardware-in-the-loop simulation of proposed techniques has been done.																	1864-5909	1864-5917															10.1007/s12065-020-00405-9		JUN 2020											
J								Supervised machine learning approach to predict qualitative software product	EVOLUTIONARY INTELLIGENCE										Software product quality; Machine learning; Cost estimation; Defect prediction; Reusability		Software development process (SDP) is a framework imposed on software product development and is a multi-stage process wherein a wide range of tasks and activities pan out in each stage. Each stage requires careful observations to improve productivity, quality, etc. to ease the process of development. During each stage, problems surface likes constraint of on-time completion, proper utilization of available resources and appropriate traceability of work progress, etc. and may lead to reiteration due to the defects spotted during testing and then, results into the negative walk-through due to unsatisfactory outcomes. Working on such defects can help to take a step towards the proper steering of activities and thus to improve the expected performance of the software product. Handpicking the proper notable features of SDP and then analyzing their nature towards the outcome can greatly help in getting a reliable software product by meeting the expected objectives. This paper proposed supervised Machine Learning (ML) models for the predictions of better SDP, particularly focusing on cost estimation, defect prediction, and reusability. The experimental studies were conducted on the primary data, and the evaluation reveals the model suitability in terms of efficiency and effectiveness for SDP prediction (accuracy of cost estimation: 65%, defect prediction: 93% and reusability: 82%).																	1864-5909	1864-5917															10.1007/s12065-020-00434-4		JUN 2020											
J								Probabilistic linguistic multi-criteria decision-making based on double information under imperfect conditions	FUZZY OPTIMIZATION AND DECISION MAKING										Probabilistic linguistic term set; Projection; Normalization model; Decision-making method; Data hiding	TERM SETS	In this paper, we study four projection-based normalization models and a decision-making method for probabilistic linguistic multi-criteria decision-making problems, in which the assessment information about an alternative with respect to a criterion is incomplete and the criteria weight values are not precisely known but the ranges are available. To apply the projection to the probabilistic linguistic environment, we propose the equivalent expression forms of the probabilistic linguistic term sets, and then the equivalent transformation functions between the probabilistic linguistic term set and its associated vector are presented to realize the conversion between the operations on the probabilistic linguistic term sets and the operations on their associated vectors. Next, the projection formulas of the probabilistic linguistic term sets are introduced to build different normalization models for different types of uncertain probabilistic linguistic multi-criteria decision-making problems. After that, a new deviation degree formula is proposed to account for the rationality and validity of the normalization models from the theoretical perspective. Finally, the probabilistic linguistic two-step method is used to determine the criteria weights values and rank the alternatives, and the validity of these projection-based normalization models and our proposed decision-making method are illustrated by a case about the performance assessment of data hiding techniques.																	1568-4539	1573-2908															10.1007/s10700-020-09325-w		JUN 2020											
J								Gradient Shape Model	INTERNATIONAL JOURNAL OF COMPUTER VISION										Facial landmark localization; Face alignment; Constrained local model; CLM	ACTIVE APPEARANCE MODELS	For years, the so-called Constrained Local Model (CLM) and its variants have been the gold standard in face alignment tasks. The CLM combines an ensemble of local feature detectors whose locations are regularized by a shape model. Fitting such a model typically consists of an exhaustive local search using the detectors and a global optimization that finds the CLM's parameters that jointly maximize all the responses. However, one major drawback of CLMs is the inefficiency of the local search, which relies on a large amount of expensive convolutions. This paper introduces the Gradient Shape Model (GSM), a novel approach that addresses this limitation. We are able to align a similar CLM model without the need for any convolutions at all. We also use true analytical gradient and Hessian matrices, which are easy to compute, instead of their approximations. Our formulation is very general, allowing an optional 3D shape term to be seamlessly included. Additionally, we expand the GSM formulation through a cascade regression framework. This revised technique allows a substantially reduction in the complexity/dimensionality of the data term, making it possible to compute a denser, more accurate, regression step per cascade level. Experiments in several standard datasets show that our proposed models perform faster than state-of-the-art CLMs and better than recent cascade regression approaches.																	0920-5691	1573-1405				DEC	2020	128	12					2828	2848		10.1007/s11263-020-01341-y		JUN 2020											
J								Classification Rule Mining Algorithm Combining Intuitionistic Fuzzy Rough Sets and Genetic Algorithm	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Large-scale intuitionistic fuzzy information system; Intuitionistic fuzzy classification rule base; Completeness; Interaction; Compatibility; Multi-objective optimization; Genetic algorithm	SYSTEMS; PREDICTION; BASE; REDUCTION; MODEL	This paper has proposed a classification rule base mining algorithm combining the genetic algorithm and intuitionistic fuzzy-rough set for large-scale intuitionistic fuzzy information system. The algorithm has proposed innovatively the definitions and measurement metrics of completeness, interaction and compatibility describing the whole rule base, and constructed a multi-objective optimization model to optimize the population size of data sample, and used the intuitionistic fuzzy-rough set to reduce the attribute set of fuzzy information system, and used intuitionistic fuzzy similar class to extract large-scale intuitionistic fuzzy information system rules, and obtained an optimal rule base with the minimal size, configuration, generation time and storage space. A threshold control mechanism is used to evaluate the completeness, interaction and correlation of rule population and improves the robustness and flexibility of sample population optimization and rule base generation. The algorithm is verified by the real aircraft health data sets. The algorithm is validated by similar mature and effective algorithms in accuracy, time complexity using real data and has good robustness and adaptability to different size large-scale fuzzy information system.																	1562-2479	2199-3211				JUL	2020	22	5					1694	1715		10.1007/s40815-020-00849-2		JUN 2020											
J								Smart city routing using GIS & VANET system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										GIS; Smart city; Urban areas; Routing; Real-time; VANET architecture	SHORTEST PATHS; NETWORKS; SEARCH	Cities are built by various structures like some of them with various spatial, cultural, and ecologically defined as a system. A Spatial System means the area that can be measured as various GPS (Geographical Positioning System) point systems. These systems are further divided as housing areas, shopping malls, theatres, party halls and market area and also traffic congested areas where the density of the traffic is high. These high end traffic related areas are governed by the government with the usage of various traffic related activities like arrangements or alterations whenever the traffic flow is very high. In that case our system predicts the routing strategy or control the traffic congestion using the GIS and VANET architecture. This paper holds the information about the traffic routing strategy using GIS and the prediction of the traffic congestion using VANET.																	1868-5137	1868-5145															10.1007/s12652-020-02148-y		JUN 2020											
J								Preserving mobile commerce IoT data using light weight SIMON block cipher cryptographic paradigm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; FFBN; Data security; Lightweight SIMON block cipher; Cryptography	SECURITY; CLOUD	Internet of Things (IoT) data security is one of the critical ideas in data security as the data to be transferred should be made secure. In the field of mobile commerce is the high effect that it will have on a few parts of the regular life and behavior of potential clients. In this present examination, we focused to build up the IoT security in the field of mobile commerce utilizing new cryptographic strategies. Fundamentally the sensitive data are classified from the entire dataset to enhance the accuracy utilizing Feed Forward Back Propagation Algorithm (FFBN). At that point the security of sensitive data is upgraded by utilizing Light Weight Cryptography (LWC) which encodes the input sensitive data called encryption. To enhance the data privacy and secrecy, an imaginative security model is proposed for example Lightweight SIMON block cipher. This will encrypt the data alongside optimal key selection; this LWC upgrades the m commerce data security level in cloud. For key optimization, a meta-heuristic algorithm called Crow Search Algorithm (CSA) is introduced. The proposed SIMON-CSA accomplishes least time to generate key value to decrypt the data.																	1868-5137	1868-5145															10.1007/s12652-020-02173-x		JUN 2020											
J								Formalizing the LLL Basis Reduction Algorithm and the LLL Factorization Algorithm in Isabelle/HOL	JOURNAL OF AUTOMATED REASONING										Certified algorithm; Complexity verification; Lattices; Polynomial factorization; Shortest vector problem; Verified LLL implementation	FACTORING POLYNOMIALS	The LLL basis reduction algorithm was the first polynomial-time algorithm to compute a reduced basis of a given lattice, and hence also a short vector in the lattice. It approximates an NP-hard problem where the approximation quality solely depends on the dimension of the lattice, but not the lattice itself. The algorithm has applications in number theory, computer algebra and cryptography. In this paper, we provide an implementation of the LLL algorithm. Both its soundness and its polynomial running-time have been verified using Isabelle/HOL. Our implementation is nearly as fast as an implementation in a commercial computer algebra system, and its efficiency can be further increased by connecting it with fast untrusted lattice reduction algorithms and certifying their output. We additionally integrate one application of LLL, namely a verified factorization algorithm for univariate integer polynomials which runs in polynomial time.																	0168-7433	1573-0670				JUN	2020	64	5			SI		827	856		10.1007/s10817-020-09552-1		JUN 2020											
J								An Optimized Framework for Plane-Probing Algorithms	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Digital plane recognition; Normal estimation; Plane-probing algorithm	ROBUST NORMAL ESTIMATION; NORMAL VECTOR; SURFACE; RECOGNITION	A plane-probing algorithm computes the normal vector of a digital plane from a starting point and a predicate "Is a point x in the digital plane?". This predicate is used to probe the digital plane as locally as possible and decide on the fly the next points to consider. However, several existing plane-probing algorithms return the correct normal vector only for some specific starting points and an approximation otherwise, e.g., the H- and R-algorithm proposed in Lachaud et al. (J Math Imaging Vis 59(1):23-39, 2017). In this paper, we present a general framework for these plane-probing algorithms that provides a way of retrieving the correct normal vector from any starting point, while keeping their main features. There are O(omega log omega) calls to the predicate in the worst-case scenario, where omega is the thickness of the underlying digital plane, but far fewer calls are experimentally observed on average. In the context of digital surface analysis, the resulting algorithm is expected to be of great interest for normal estimation and shape reconstruction.																	0924-9907	1573-7683				JUN	2020	62	5			SI		718	736		10.1007/s10851-020-00965-6		JUN 2020											
J								Investigating low-delay deep learning-based cultural image reconstruction	JOURNAL OF REAL-TIME IMAGE PROCESSING										Digital heritage; Image reconstruction; Low-delay reconstruction; Image inpainting; Deep learning; Image clustering	RESOLUTION	Numerous cultural assets host a great historical and moral value, but due to their degradation, this value is heavily affected as their attractiveness is lost. One of the solutions that most heritage organizations and museums currently choose is to leverage the knowledge of art and history experts in addition to curators to recover and restore the damaged assets. This process is labor-intensive, expensive and more often results in just an assumption over the damaged or missing region. In this work, we tackle the issue of completing missing regions in artwork through advanced deep learning and image reconstruction (inpainting) techniques. Following our analysis of different image completion and reconstruction approaches, we noticed that these methods suffer from various limitations such as lengthy processing times and hard generalization when trained with multiple visual contexts. Most of the existing learning-based image completion and reconstruction techniques are trained on large datasets with the objective of retrieving the original data distribution of the training samples. However, this distribution becomes more complex when the training data is diverse making the training process difficult and the reconstruction inefficient. Through this paper, we present a clustering-based low-delay image completion and reconstruction approach which combines supervised and unsupervised learning to address the highlighted issues. We compare our technique to the current state of the art using a real-world dataset of artwork collected from various cultural institutions. Our approach is evaluated using statistical methods and a surveyed audience to better interpret our results objectively and subjectively.																	1861-8200	1861-8219															10.1007/s11554-020-00975-y		JUN 2020											
J								An optimization study based on Dijkstra algorithm for a network with trapezoidal picture fuzzy numbers	NEURAL COMPUTING & APPLICATIONS										Picture fuzzy number; Trapezoidal picture fuzzy number; Score function; Shortest path problem; Dijkstra algorithm	SHORTEST-PATH PROBLEM; AGGREGATION OPERATORS; SETS	Path finding models attempt to provide efficient approaches for finding shortest paths in networks. A well-known shortest path algorithm is the Dijkstra algorithm. This paper redesigns it in order to tackle situations in which the parameters of the networks may be uncertain. To be precise, we allow that the parameters take the form of special picture fuzzy numbers. We use this concept so that it can flexibly fit the vague character of subjective decisions. The main contributions of this article are fourfold: (i) The trapezoidal picture fuzzy number along with its graphical representation and operational laws is defined. (ii) The comparison of trapezoidal picture fuzzy numbers on the basis of their expected values is proposed in terms of their score and accuracy functions. (iii) Based on these elements, we put forward an adapted form of the Dijkstra algorithm that works out a picture fuzzy shortest path problem, where the costs associated with the arcs are captured by trapezoidal picture fuzzy numbers. Also, a pseudocode for the application of our solution is provided. (iv) The proposed algorithm is numerically evaluated on a transmission network to prove its practicality and efficiency. Finally, a comparative analysis of our proposed method with the fuzzy Dijkstra algorithm is presented to support its cogency.																	0941-0643	1433-3058															10.1007/s00521-020-05034-y		JUN 2020											
J								A Style-Specific Music Composition Neural Network	NEURAL PROCESSING LETTERS										Automatic composition; Reinforcement learning; Actor-critic network; Music rules		Automatic music composition could dramatically decrease music production costs, lower the threshold for the non-professionals to compose as well as improve the efficiency of music creation. In this paper, we proposed an intelligent music composition neutral network to automatically generate a specific style of music. The advantage of our model is the innovative structure: we obtained the music sequence through an actor's long short term memory, then fixed the probability of sequence by a reward-based procedure which serves as feedback to improve the performance of music composition. The music theoretical rule is introduced to constrain the style of generated music. We also utilized a subjective validation in experiment to guarantee the superiority of our model compared with state-of-the-art works.																	1370-4621	1573-773X															10.1007/s11063-020-10241-8		JUN 2020											
J								DRFS: Detecting Risk Factor of Stroke Disease from Social Media Using Machine Learning Techniques	NEURAL PROCESSING LETTERS										Stroke disease symptoms identification; Social media content mining; Natural language processing; Spectral Clustering; PNN (probabilistic neural network); Association rule mining	CARDIOVASCULAR RISK; FRAMINGHAM; CLASSIFICATION; SCORE	In general humans are said to be social animals. In the huge expanded internet, it's really difficult to detect and find out useful information about a medical illness. In anticipation of more definitive studies of a causal organization between stroke risk and social network, It would be suitable to help social individuals to detect the risk of stroke. In this work, a DRFS methodology is proposed to find out the various symptoms associated with the stroke disease and preventive measures of a stroke disease from the social media content. We have defined an architecture for clustering tweets based on the content using Spectral Clustering an iterative fashion. The class label detection is furnished with the use of highest TF-IDF value words. The resultant clusters obtained as the output of spectral clustering is prearranged as input to the Probability Neural Network (PNN) to get the suitable class labels and their probabilities. Find Frequent word set using support count measure from the group of clusters for identify the risk factors of stroke. We found that the anticipated approach is able to recognize new symptoms and causes that are not listed in the World Health Organization (WHO), Mayo Clinic and National Health Survey (NHS). It is marked that they get associated with precise outcomes portray real statistics. This type of experiments will empower health organization, doctors and Government segments to keep track of stroke diseases. Experimental results shows the causes preventive measures, high and low risk factors of stroke diseases.																	1370-4621	1573-773X															10.1007/s11063-020-10279-8		JUN 2020											
J								Gabriel graph-based connectivity and density for internal validity of clustering	PROGRESS IN ARTIFICIAL INTELLIGENCE										Cluster validity index; Density; Connectivity; Arbitrarily shaped clusters; Gabriel graph	INDEX; VALIDATION	Clustering has an important role in data mining field. However, there is a large variety of clustering algorithms and each could generate quite different results depending on input parameters. In the research literature, several cluster validity indices have been proposed to evaluate clustering results and find the partition that best fits the input dataset. However, these validity indices may fail to achieve satisfactory results, especially in case of clusters with arbitrary shapes. In this paper, we propose a new cluster validity index fordensity-based, arbitrarily shaped clusters. Our new index is based on the density and connectivity relations extracted among the data points, based on the proximity graph, Gabriel graph. The incorporation of the connectivity and density relations allows achieving the best clustering results in the case of clusters with any shape, size or density. The experimental results on synthetic and real datasets, using the well-known neighborhood-based clustering (NBC) algorithm and the DBSCAN (density-based spatial clustering of applications with noise) algorithm, illustrate the superiority of the proposed index over some classical and recent indices and show its effectiveness for the evaluation of clustering algorithms and the selection of their appropriate parameters.																	2192-6352	2192-6360				SEP	2020	9	3					221	238		10.1007/s13748-020-00209-z		JUN 2020											
J								Secure third-party data clustering using SecureCL, phi-data and multi-user order preserving encryption	EXPERT SYSTEMS										cryptography; data mining; nearest neighbour; privacy; unsupervised learning		Secure collaborative data clustering using SecureCL is presented. SecureCL is founded on the concept of phi-data implemented using Super Secure Chain Distance Matrices and encrypted using Multi-User Order Preserving Encryption. The advantage offered, unlike comparable systems, is that SecureCL does not require any user participation once the phi-data proxy has been encrypted; it does not require recourse to Secure Multi-Party Computation protocols or 'secret sharing' mechanisms. The utility of SecureCL is illustrated using Nearest Neighbour Clustering and Density-Based Spatial Clustering of Applications with Noise, although it can be applied to any data clustering algorithm that involves distance comparison. The reported experiments demonstrate that SecureCL can produce securely cluster configurations comparable to those produced using standard, non-encrypted, approaches without entailing any significant computational overhead, thus indicating its suitability in the context of Data Mining as a Service.																	0266-4720	1468-0394														e12581	10.1111/exsy.12581		JUN 2020											
J								MR plot: A big data tool for distinguishing distributions	STATISTICAL ANALYSIS AND DATA MINING										Bayes risk; concentration measures; distributional plots; mean residual plot; survival entropy; taxi trip time	INFORMATION	Big data enables reliable estimation of continuous probability density, cumulative distribution, survival, hazard rate, and mean residual functions (MRFs). We illustrate that plot of the MRF provides the best resolution for distinguishing between distributions. At each point, the MRF gives the mean excess of the data beyond the threshold. Graph of the empirical MRF, called here the MR plot, provides an effective visualization tool. A variety of theoretical and data driven examples illustrate that MR plots of big data preserve the shape of the MRF and complex models require bigger data. The MRF is an optimal predictor of the excess of the random variable. With a suitable prior, the expected MRF gives the Bayes risk in the form of the entropy functional of the survival function, called here the survival entropy. We show that the survival entropy is dominated by the standard deviation (SD) and the equality between the two measures characterizes the exponential distribution. The empirical survival entropy provides a data concentration statistic which is strongly consistent, easy to compute, and less sensitive than the SD to heavy tailed data. An application uses the New York City Taxi database with millions of trip times to illustrate the MR plot as a powerful tool for distinguishing distributions.																	1932-1864	1932-1872				AUG	2020	13	4					405	418		10.1002/sam.11464		JUN 2020											
J								Feature selection inspired by human intelligence for improving classification accuracy of cancer types	COMPUTATIONAL INTELLIGENCE										minimum redundancy maximum relevance; feature selection; teaching-learning based optimization; classification	LEARNING-BASED OPTIMIZATION; MICROARRAY DATA; GENE SELECTION; ALGORITHM; INFORMATION	Feature selection is an essential task to predict clinical risk and biomarkers from the gene expression data. For practical matters, to choose the significant genes, researchers have been addressed several classical feature selection problems over the past decades for subsequent classification of genomics datasets with large ambient dimensionality but a small number of observations. To overcome high dimensionality and overfitting issues, in this paper, we developed a new gene selection technique by combination of minimum redundancy maximum relevance (mRMR) and teaching learning-based optimization for accurate cancer prediction. Firstly, in the proposed approach, mRMR is applied to find the most discriminative genes from the original feature sets, and then a precise teaching learning-based optimization with opposition-based learning approach further refines the reduced feature set that can contribute to identifying the type of cancers. In addition, a new activation function is also investigated for effective gene selection, which is applied to convert continuous to binary search space. Support vector machine (SVM) is used as a fitness function in the proposed method to select relevant features that can help to estimate the predictive accuracy and classify cancer accurately. Attempts have made to increase the performance of SVM classifier by tuning penalty factor, kernel parameter, and tube size parameter with the help of proposed method. In order to testify computational efficiency of proposed algorithm, we have collected six gene expression datasets. Experimental results demonstrated that proposed method by utilizing SVM with Radial Basis Function kernel function is able to significantly reduce the irrelevant genes and outperform the conventional wrapper methods in terms of accuracy and model interpretation.																	0824-7935	1467-8640															10.1111/coin.12341		JUN 2020											
J								Bio art	AI & SOCIETY										Experimental poetry; Holography; Holopoetry; Digital poetry; Telepresence; Telepresence art; Bio art	AGROBACTERIUM	In 1997, I introduced the concept and the phrase "bio art", originally in relation to my artwork "Time Capsule" (1997) (Patricia Decia, "Artista poe a vida em risco" and "Bioarte," Folha de Sao Paulo, October 10, 1997.). This work approached the problem of wet interfaces and human hosting of digital memory through the implantation of a microchip. The work consisted of a microchip implant, seven sepia-toned photographs, a live television broadcast, a webcast, interactive telerobotic webscanning of the implant, a remote database intervention, and additional display elements, including an X-ray of the implant. While "bio art" is applicable to a large gamut of in vivo works that employ biological media, made by myself and others, in 1998, I started to employ the more focused term "transgenic art" (Eduardo Kac. "Transgenic Art", Leonardo Electronic Almanac 6, no. 11 (1998). Republished in Ars Electronica '99-Life Science, ed. Gerfried Stocker and Christine Schopf (Vienna, New York: Springer, 1999), 289-296.) to describe a new art form based on the use of genetic engineering to create unique living beings. Art that manipulates or creates life must be pursued with great care, with acknowledgment of the complex issues it raises and, above all, with a commitment to respect, nurture, and love the life created. I have been creating and exhibiting a series of transgenic artworks since 1999. I have also been creating bio art that is not transgenic. The implications of this ongoing body of work have particular esthetic and social ramifications, crossing several disciplines and providing material for further reflection and dialog. What follows is an overview of theses works, the issues they evoke, and the debates they have elicited.																	0951-5666	1435-5655															10.1007/s00146-020-00958-4		JUN 2020											
J								Concurrent service access and management framework for user-centric future internet of things in smart cities	COMPLEX & INTELLIGENT SYSTEMS										Access management; Convolution neural network; IoT; Request concurrency; Service allocation	BIG DATA; IOT; ARCHITECTURE; OPERATIONS; MODEL; EDGE	Future Internet of Things (FIoT) is a service concentric distributed architecture that is used by the smart city users for information sharing and access. The design of FIoT focuses in achieving reliable service and response to the growing user demands through different interoperability features. In this manuscript, concurrent service access and management framework is introduced to improve the swiftness in user concentric request processing. Based on the availability of the services and the density of the users, the concurrency in information access is provided to the users in a reliable manner. The framework incorporates convolution neural learning process in linear and differential manner for improving the access and service usage rates of the requesting users. The access sessions are differentiated for the accessible and offloaded requests to the available service providers based on the learning instances. The proposed framework is assessed using the metrics access rate, service usage rate, access delay, time lag, and failure ratio.																	2199-4536	2198-6053															10.1007/s40747-020-00160-5		JUN 2020											
J								A Fuzzy Segmentation Method to Learn Classification of Mitosis	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Attention mechanism; Recurrent neural network; Image segmentation; Mitosis detection of breast cancer		Mitotic counts are widely used as a metric for cellular proliferation for prognosis and to determine the aggressiveness of individual cancers. This study presents a less labor-intensive method to count mitotic cells in breast cell sections. The proposed algorithm involves two phases: candidate segmentation and detection. During candidate segmentation, images are filtered through a blue ratio threshold to remove unnecessary background information and to increase the color difference between targets and non-targets for an entire digitized image. A fuzzy candidate segmentation method is used to adaptively determine threshold values in order to dichotomize gray-level images and distinguish the images of mitotic candidates from the background. The thresholding scheme integrates the spatial characteristics' distribution in a histogram to determine an intensity threshold for the processed image, in order to filter insignificant information. During the detection phase, a two-class classification uses an attention mechanism that is realized by a set of fully connected neural networks, instead of convolutional layers, which decreases the computational cost. The validation test using ICPR2012 competition datasets shows that the proposed model outperforms current state-of-art techniques, in terms of the metrics, Accuracy, F-1-score, and Precision and Recall.																	1562-2479	2199-3211				JUL	2020	22	5					1653	1664		10.1007/s40815-020-00868-z		JUN 2020											
J								Attentive convolutional gated recurrent network: a contextual model to sentiment analysis	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Sentiment analysis; Convolutional neural network; Recurrent neural network; Attention mechanism; Contextual features		Considering contextual features is a key issue in sentiment analysis. Existing approaches including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) lack the ability to account and prioritize informative contextual features that are necessary for better sentiment interpretation. CNNs present limited capability since they are required to be very deep, which can lead to the gradient vanishing whereas, RNNs fail because they sequentially process input sequences. Furthermore, the two approaches treat all words equally. In this paper, we suggest a novel approach named attentive convolutional gated recurrent network (ACGRN) that alleviates the above issues for sentiment analysis. The motivation behind ACGRN is to avoid the vanishing gradient caused by deep CNN via applying a shallow-and-wide CNN that learns local contextual features. Afterwards, to solve the problem caused by the sequential structure of RNN and prioritizing informative contextual information, we use a novel prior knowledge attention based bidirectional gated recurrent unit (ATBiGRU). Prior knowledge ATBiGRU captures global contextual features with a strong focus on the previous hidden states that carry more valuable information to the current time step. The experimental results show that ACGRN significantly outperforms the baseline models over six small and large real-world datasets for the sentiment classification task.																	1868-8071	1868-808X				DEC	2020	11	12					2637	2651		10.1007/s13042-020-01135-1		JUN 2020											
J								An effective deep learning features based integrated framework for iris detection and recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Iris recognition; Classification; Segmentation; Deep learning; Mask R-CNN	BIOMETRICS	In recent years, Iris recognition has emerged as an important and trustworthy biometric model to recognize humans. The application of automatic iris recognition models find useful in different fields namely border control, citizen confirmation, and criminal to commercial products. This paper introduces an effective deep learning (DL) based integrated model for precise iris detection, segmentation and recognition. The projected model involves different stages namely preprocessing, detection, segmentation and recognition. Initially, preprocessing of images takes place to improve the quality of the input image using Black Hat filtering, Median filtering and Gamma Correction. Then, Hough Circle Transform model is applied to localize the region of interest, i.e. iris in an effective way. Afterwards, Mask region proposal network with convolution neural network (R-CNN) with Inception v2 model is applied for trustworthy iris recognition and segmentation i.e., recognizing iris/non-iris pixels. For validating the results of the presented model, a detailed simulation takes place using a benchmark CASIA-Iris Thousand dataset and the results are validated interms of detection accuracy. The attained simulation outcome depicted that the projected technique shows maximum recognition accuracy of 99.14% which is superior to other methods such as UniNet.V2, AlexNet, VGGNet, Inception, ResNet and DenseNet models in a significant way.																	1868-5137	1868-5145															10.1007/s12652-020-02172-y		JUN 2020											
J								An improved hybridized deep structured model for accurate video event recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Video event recognition; Deep structure model; Convolution neural network; Conditional random field; Semantic level	KNOWLEDGE	Video event recognition plays an important role in the various research fields particularly in surveillance detection system. In the existing system it is done by deep hierarchical context model which utilizes several contextual data which contains various context information at level of feature, semantic and priority for the video recognition process. However, this research method might perform low with increased volume of videos and might be failed to predict the events accurately with less interrelation contextual features. The standstill challenges are solved via improved hybridized deep structured model. The three primary features of contextual data are to discriminate resultant neighborhood. Here hybrid textual perceptual descriptor and concept-based attribute extraction is performed for accurate recognition of video events. These extracted interaction context features are grouped by using improved K means algorithm. In addition, improved deep structured model that combines convolutional neural networks and conditional random fields are developed for learning middle level representations and mingle the bottom feature level, mid-semantic and top-level meanings for the identification of incidents. This proposed research method is evaluated by using VIRAT data set whose simulation analysis is performed using Matlab simulation toolkit. The overall evaluation of the proposed research method proves that the suggested method can provide better output in terms of accurate recognition of events.																	1868-5137	1868-5145															10.1007/s12652-020-02157-x		JUN 2020											
J								Internet of Health Things (IoHT) for personalized health care using integrated edge-fog-cloud network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Health monitoring; Edge-fog-cloud network; Mobility prediction; Internet of Health Things (IoHT)	FRAMEWORK; SYSTEM	This paper proposes a mobile healthcare framework based on edge-fog-cloud collaborative network. It uses edge and fog devices for parameterized health monitoring, and cloud for further health data analysis in case of abnormal health status. The continuous location change of users is a critical issue, and the connection interruption and delay in delivering health related data may be fatal in case of emergency. In this direction, in the proposed framework, mobility information of the users is considered and the users' mobility pattern detection is performed inside the cloud for advising the user regarding nearby health centre. From the theoretical analysis, it is observed that the proposed framework reduces the delay and energy consumption of user device by similar to 28% and similar to 27% respectively than the cloud only health care model. The proposed healthcare framework has been implemented in the laboratory and health data of few student volunteers are analyzed to predict their health status. The experimental analysis also shows that the proposed mobility prediction model has better precision, recall value and time- efficiency than the existing models.																	1868-5137	1868-5145															10.1007/s12652-020-02113-9		JUN 2020											
J								Hexagonality as a New Shape-Based Descriptor of Object	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Shape; Hexagonality measure; Measuring orientation; Shape elongation; Object classification	PATTERN-RECOGNITION; MOMENT INVARIANTS; CLASSIFICATION; ORIENTATION; RETRIEVAL	In this paper, we define a new shape-based measure which evaluates how much a given shape is hexagonal. Such an introduced measure ranges through the interval (0, 1] and reaches the maximal possible value 1 if and only if the shape considered is a hexagon. The new measure is also invariant with respect to rotation, translation and scaling transformations. A number of experiments, performed on both synthetic and real image data, are shown in order to confirm theoretical observations and illustrate the behavior of the new measure. The new hexagonality measure also provides several useful side results whose theoretical properties are discussed and experimentally evaluated. As side results, we obtain a new method that computes the shape orientation as the direction which optimizes the new hexagonality measure and a new shape elongation measure which computes the elongation of a given shape as the ratio of the lengths of the longer and shorter semi-axis of the appropriate associated hexagon. Several experiments relating to three well-known image datasets, such as MPEG-7 CE-1, Swedish Leaf, and Galaxy Zoo datasets, are also provided to illustrate effectiveness and benefits of the new introduced shape measures.																	0924-9907	1573-7683				OCT	2020	62	8					1136	1158		10.1007/s10851-020-00966-5		JUN 2020											
J								Complexity reduction of test zonal search for fast motion estimation in uni-prediction of High Efficiency Video Coding	JOURNAL OF REAL-TIME IMAGE PROCESSING										Motion estimation; Adaptive search range; Motion estimation termination; High Efficiency Video Coding	BI-PREDICTION; ALGORITHM; RANGE	The complexity of motion estimation in the High Efficiency Video Coding (HEVC) standard is very high as it uses a number of prediction block sizes. The test zonal search (TZS) mechanism is used as motion estimation algorithm in the fast search mode of HM encoder, the reference software for HEVC. In this paper, we present schemes for reducing the complexity of the TZS algorithm for uni-prediction in HEVC. The proposed mechanisms help in reducing complexity of grid, raster and refinement search stages of TZS. The performance of the proposed mechanisms is tested independently and also combined in the fast search mode of HM-16.18. The motion estimation time and the number of search points in the combined algorithm are reduced by 68.14% and 77.10% with a BD-rate of 0.30% and a BD-PSNR of -0.007 dB in comparison with the original fast search mode in the low-delay P main profile. The proposed complexity reduction schemes are compared with those adopted in other recently proposed fast motion estimation algorithms and the performance of the proposed scheme is found to be superior in terms of the reduction in motion estimation time.																	1861-8200	1861-8219															10.1007/s11554-020-00983-y		JUN 2020											
J								Two-stream FCNs to balance content and style for style transfer	MACHINE VISION AND APPLICATIONS										Style transfer; Fully convolutional network; Convolution neural network; Two-stream		Style transfer is to render given image contents in given styles, and it has an important role in both computer vision fundamental research and industrial applications. Following the success of deep learning-based approaches, this problem has been re-launched recently, but still remains a difficult task because of trade-off between preserving contents and faithful rendering of styles. Indeed, how well-balanced content and style are is crucial in evaluating the quality of stylized images. In this paper, we propose an end-to-end two-stream fully convolutional networks (FCNs) aiming at balancing the contributions of the content and the style in rendered images. Our proposed network consists of the encoder and decoder parts. The encoder part utilizes a FCN for content and a FCN for style where the two FCNs have feature injections and are independently trained to preserve the semantic content and to learn the faithful style representation in each. The semantic content feature and the style representation feature are then concatenated adaptively and fed into the decoder to generate style-transferred (stylized) images. In order to train our proposed network, we employ a loss network, the pre-trained VGG-16, to compute content loss and style loss, both of which are efficiently used for the feature injection as well as the feature concatenation. Our intensive experiments show that our proposed model generates more balanced stylized images in content and style than state-of-the-art methods. Moreover, our proposed network achieves efficiency in speed.																	0932-8092	1432-1769				JUN 8	2020	31	5							37	10.1007/s00138-020-01086-1													
J								Combining hyper-heuristics to evolve ensembles of priority rules for on-line scheduling	NATURAL COMPUTING										One machine scheduling; Hyper-heuristic; Priority rules; Ensemble learning; Evolutionary algorithms	DISPATCHING RULES; ALGORITHM; MACHINES	Combining metaheuristics is a common technique that may produce high quality solutions to complex problems. In this paper, we propose a combination of Genetic Programming (GP) and Genetic Algorithm (GA) to obtain ensembles of priority rules to solve a scheduling problem, denoted (1,Cap(t)parallel to Sigma T-i), on-line. In this problem, a set of jobs must be scheduled on a single machine whose capacity varies over time. The proposed approach interleaves GP and GA so that a GP is in charge of evolving single priority rules and a GA is executed after each iteration of the GP to evolve ensembles from the rules produced by the GP in this iteration, at the same time as the GP evolves the next generation of rules. Therefore, the ensembles are obtained in an anytime fashion. In the experimental study, we compare the proposed approach to a previous one in which the GP was firstly run to evolve a large pool of candidate priority rules, and then the GA was run to obtain ensembles from that pool of rules. The results of this study revealed that the ensembles produced by the interleaved combination of GP and GA are better than those obtained by the sequential combination of GP and GA. So, these results, together with the ensembles being available earlier, make this approach more appropriate to the on-line requirements of the scheduling problem.																	1567-7818	1572-9796															10.1007/s11047-020-09793-4		JUN 2020											
J								Online meta-learning firewall to prevent phishing attacks	NEURAL COMPUTING & APPLICATIONS										Meta-learning; Intelligent firewall; Phishing attacks; Long short-term memory; Self-adjusting memory; k-nearest neighbor		Phishing is the most well-known act of deceiving the Internet users, in which the 'perpetrator' plays a credible entity. This is done by misusing the inadequate protection provided by electronic tools, and by exploiting the ignorance of the user-object, in order to illegally obtain personal data, such as sensitive private information and passwords. This research proposes the online meta-learning firewall to prevent phishing attacks. It is a highly innovative and fully automated active safety tool that uses a long short-term memory meta-learner algorithm. This method can learn to efficiently classify using a small number of samples. At the same time, it can converge with a fairly small number of steps. The proposed system is an improvement on the k-nearest neighbor with self-adjusting memory algorithm, which is inspired by the model of short and long-term memory. The purpose of the system is to understand the nature of an unknown situation and to classify it, based on the most relevant characteristics that come directly from the unknown environment.																	0941-0643	1433-3058															10.1007/s00521-020-05041-z		JUN 2020											
J								A Novel Technique for Segmentation of High Resolution Remote Sensing Images Based on Neural Networks	NEURAL PROCESSING LETTERS										Remote sensing images; High-resolution; Images segmentation; Artificial intelligence; Neural networks	ARCHITECTURE	Remote sensing images have become one of the most important imaging resources recently. Thus, it is important to develop high-performance techniques to process and manipulate these images. On the other hand, image processing techniques are enhanced spatially based on neural networks. Deep learning is one of the most important techniques in use for computer vision tasks and has been deployed successfully to solve many tasks. But when dealing with remote sensing images, the deep learning method faces two main problems: the underfitting problem, because of the small amount of learning data and the unbalanced receptive field problem, because of the structural stereotype of the remote sensing images. In this paper, we propose to use a complex-valued neural network to segment high-resolution remote sensing images. The proposed network can deal with the problems of remote sensing images by using an ensemble of Complex-Valued Auto-Encoder. Based on an adaptive clustering technique, this network can be used to solve the multi-label segmentation problem of remote sensing images. The proposed method achieves state-of-the-art performance when evaluated on the ISPRS 2D dataset.																	1370-4621	1573-773X				AUG	2020	52	1			SI		679	692		10.1007/s11063-020-10280-1		JUN 2020											
J								Securing Blockchain Transactions Using Quantum Teleportation and Quantum Digital Signature	NEURAL PROCESSING LETTERS										Blockchain; Quantum entanglement; Quantum digital signatures; Quantum teleportation; Cryptocurrencies		Blockchain is the new disruptive technology which is gaining momentum in several domains of real-world applications such as Bitcoin-the most well-known example, and primarily in the financial sector. Distributed ledger system which is the main feature of blockchain technology protected across harmful updates using cryptographic techniques, e.g. hashing and digital signatures, is spread over the network so that no one is the owner of the ledger. This poses a significant challenge in the areas of performance, security, and privacy using this emerging technology. We propose an algorithm to provide a secure mechanism for performing transactions on the Blockchain network by adopting quantum digital signatures (QDS) and quantum teleportation phenomenon of quantum computing. The proposed algorithm uses key pairs to generate private keys and corresponding public keys; the quantum digital signatures are used to sign the message which are then distributed over the blockchain network using the teleportation phenomenon. The security of the keys is dependent on the fundamental principles of quantum mechanics that doesn't allow forging. The Einstein-Podolsky-Rosen (EPR) pair of particles is used to communicate the quantum information via a quantum channel for teleportation, which is generated by operating Bell measurements with corresponding EPR pairs. The original state of the particle (sender) is destroyed once the information is transported to the other particle (receiver) and the QDS scheme also validates the qubits received. The twofold validation process thereby provides high-level security in the transactions.																	1370-4621	1573-773X															10.1007/s11063-020-10272-1		JUN 2020											
J								Volumetric analysis framework for accurate segmentation and classification (VAF-ASC) of lung tumor from CT images	SOFT COMPUTING										Volumetric analysis; Tumor nodule metastasis (TNM); Segmentation; Classification; k-Means clustering	NODULES	Lung tumor can be typically stated as the abnormal cell growth in lungs that may cause severe threat to patient health, since lung is a significant organ which comprises associated network of blood veins and lymphatic canals. The earlier detection and classification of lung tumor creates a greater impact on increasing the survival rate of patients. For analysis, the Computed Tomography (CT) lung images are broadly used, since it gives information about the various lung regions. The prediction of tumor contour, position, and volume plays an imperative role in accurate segmentation and classification of tumor cells. This will aid in successful tumor stage detection and treatment phases. With that concern, this paper develops a Volumetric Analysis Framework for Accurate Segmentation and Classification of lung tumors. The volumetric analysis framework comprises the estimation of length, thickness, and height of the detected tumor cell for achieving precised results. Though there are many models for tumor detection from 2D CT inputs, it is very important to develop a method for lung nodule separation from noisy background. For that, this paper connectivity and locality features of the lung image pixels. Moreover, morphological processing techniques are incorporated for removing the additional noises and airways. Tumor segmentation has been accomplished by the k-means clustering approach. Tumor Nodule Metastasis classification based-volumetric analysis is performed for accurate results. The Volumetric Analysis Framework provides better results with respect to factors such as accuracy rate of tumor diagnosis, reduced computation time, and appropriate tumor stage classification.																	1432-7643	1433-7479															10.1007/s00500-020-05081-6		JUN 2020											
J								Differential evolution and ACO based global optimal feature selection with fuzzy rough set for cancer data classification	SOFT COMPUTING										Differential evolution; Ant colony optimization; Fuzzy rough set; Decision tree; Feature selection	KRILL HERD ALGORITHM; GENETIC ALGORITHM; OPTIMIZATION; COLONY; PREDICTION; ENSEMBLE; STRATEGY; IMPROVE	In the biomedical research field, feature selection plays the predominant role in prediction of diseases. The main objective of this paper is to predict cancer from microarray gene expression data by proposing two feature selection algorithms, namely (1) differential evolution with fuzzy rough set feature selection and (2) ant colony optimization with fuzzy rough set feature selection algorithms, which solve the multi-objective optimization problems. The first algorithm represents the hybridization of differential evolution and fuzzy rough set and aims to select the global optimal features by applying the fuzzy rough evaluation function as the fitness function. The second algorithm, i.e., hybridization of ant colony optimization and fuzzy rough set, selects global optimal features by applying the fuzzy rough evaluation function as the fitness function. The performance of proposed two features selection algorithms is evaluated with various classification metrics, which are computed from decision tree classifier using tenfold cross-validation. Five datasets are applied to analyze the performance of the feature selection algorithms. The datasets used are diffuse large B cell lymphoma, breast cancer, Leukemia and small round blue-cell tumors which are cancer datasets. In addition, a non-medical dataset, namely Gisette, is also used to demonstrate the generalization capability of the proposed algorithms. The metrics used for comparison are, namely, accuracy, precision, recall, f-measure, specificity, processing time and receiver operating characteristics. The performance comparison evidenced improved performances for the proposed algorithms. Similar to the hybridization of differential evolution and ant colony optimization with fuzzy rough set, particle swarm optimization can be extended in the future.																	1432-7643	1433-7479															10.1007/s00500-020-05070-9		JUN 2020											
J								Simultaneous localization and mapping of medical burn areas based on binocular vision and capsule networks	SOFT COMPUTING										Binocular vision; Camera calibration; Capsule network; Medical image; Burn area	RECOGNITION	Accurate evaluation of burn degree is a key step in the treatment of burn patients. The body surface area of burn area is the main basis to evaluate the degree of burn. To estimate the burn area timely and accurately is the basis of providing correct infusion volume for patients and determining further treatment measures. Therefore, it is necessary to study a fast and effective method to calculate the area of human body burn. For large-area burn patients, accurate fluid replenishment in shock period plays an important role in the maintenance of vital signs and wound healing, and the estimation of burn body surface area is the basis for calculating fluid replenishment in shock period. As an important branch of computer vision, binocular stereo vision has penetrated into many fields of production and life, which is a hot topic in computer application. Binocular stereo vision technology is based on the theory of parallax, which uses binocular camera to collect the left and right views of the measured objects. The paper proposes the binocular vision uses stereo matching algorithm to calculate the position deviation between two images, so as to obtain the 3D geometric information of the object. Based on this, this paper uses binocular vision technology and capsule network model to build a medical burn area evaluation model. The experimental results show that the method proposed in this paper can effectively locate the burn area and image processing.																	1432-7643	1433-7479															10.1007/s00500-020-05067-4		JUN 2020											
J								Short term electric load forecasting using hybrid algorithm for smart cities	APPLIED INTELLIGENCE										Smart cities; Short term load forecasting; Modified grasshopper optimization algorithm; Locally weighted support vector regression; Parameters optimization	GRASSHOPPER OPTIMIZATION ALGORITHM; NEURAL-NETWORKS; REGRESSION; MODEL	Many day-to-day operation decisions in a smart city need short term load forecasting (STLF) of its customers. STLF is a challenging task because the forecasting accuracy is affected by external factors whose relationships are usually complex and nonlinear. In this paper, a novel hybrid forecasting algorithm is proposed. The proposed hybrid forecasting method is based on locally weighted support vector regression (LWSVR) and the modified grasshopper optimization algorithm (MGOA). Obtaining the appropriate values of LWSVR parameters is vital to achieving satisfactory forecasting accuracy. Therefore, the MGOA is proposed in this paper to optimally select the LWSVR's parameters. The proposed MGOA can be derived by presenting two modifications on the conventional GOA in which the chaotic initialization and the sigmoid decreasing criterion are employed to treat the drawbacks of the conventional GOA. Then the hybrid LWSVR-MGOA method is used to solve the STLF problem. The performance of the proposed LWSVR-MGOA method is assessed using six different real-world datasets. The results reveal that the proposed forecasting method gives a much better forecasting performance in comparison with some published forecasting methods in all cases.																	0924-669X	1573-7497				OCT	2020	50	10					3379	3399		10.1007/s10489-020-01728-x		JUN 2020											
J								Domain problem-solving expert identification in community question answering	EXPERT SYSTEMS										expectation-maximization algorithm; expert finding; mixture model; Q&A community		Question-Answering (Q&A) services provide internet users with platforms to exchange knowledge and ideas. The development of Q&A sites, or Community Question Answering (CQA), mainly depends on the high-quality content continuously contributed by users with high-level expertise, who can be recognized as experts. Expert finding is an important task for the authorities of Q&A communities to encourage commitment. In a highly competitive market environment, CQA managers have to take measures to retain and nurture users, especially superior contributors. However, current expertise scoring techniques adopted in CQA often give much credit to very active users and fail to identify real experts. This study aims to develop a robust and practical expert identification framework for Q&A communities, by combining well-designed expertise scoring technique and probabilistic clustering model. With regard to expert identification, a numerical metric of users' expertise is developed as the optimal expert finding strategy, and a clustering algorithm based on Gaussian-Gamma mixture model (GGMM) is proposed to efficiently distinguish experts from nonexperts. In the experiments, the proposed method is applied to real-world datasets collected from subcommunities of Stack Exchange Q&A networks. Results obtained from comparative experiments show that our method achieves better performance than the state-of-the-art methods and demonstrate the effectiveness of the proposed framework. The analysis shows that the framework which combines the proposed expertise scoring technique and Gaussian-Gamma mixture clustering model is capable of detecting excellent domain problem-solving experts who exhibit both domain interest and expertise.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12582	10.1111/exsy.12582		JUN 2020											
J								Evaluation of multiple linear regression applied to impedimetric sensing	JOURNAL OF CHEMOMETRICS										catechol; electrochemical impedance spectroscopy; hydroquinone; MLR; multivariate calibration	ELECTROCHEMICAL IMPEDANCE SPECTROSCOPY; STRIPPING VOLTAMMETRY; ELECTRODES; CORROSION; GRAPHITE; CATECHOL; FUEL	In this work, the application of multiple linear regression (MLR) on the development of electroanalytical methods based on electrochemical impedance spectroscopy (EIS) data was investigated. This approach was proposed considering that more than one element of the equivalent electrical circuit fitted to impedance spectra could keep linear relationship with the analyte concentration and be used in a multivariate calibration model, rather than using only one element of the circuit in a univariate regression, the benchmark procedure in the context of impedimetric (bio)sensing. First, MLR was evaluated in the individual determination of the redox probe ferrocyanide in aqueous solutions, followed by the determination of catechol and hydroquinone in tap water samples. MLR produced better predictions than univariate regression when more complex electrochemical systems (catechol and hydroquinone) and more complex samples (tap water) were analyzed. The determinations of hydroquinone and catechol in the direct analysis of spiked tap water samples were successful, with apparent recoveries ranging from 94% to 136%.																	0886-9383	1099-128X														e3271	10.1002/cem.3271		JUN 2020											
J								Intention classification in multiturn dialogue systems with key sentences mining	COMPUTATIONAL INTELLIGENCE										key sentence; keywords extraction; multiturn dialogue; text classification; weight computation	NETWORK	The multiturn dialogue system has been prevalently used in e-commerce websites and modern information systems, which significantly improves the efficiency of problem solving and further promotes the service quality. In a multiturn dialogue system, the problem of intention classification is a core task, as the intention of a customer is the basis of subsequent problems handling. However, traditional related methods are unsuitable for the classification of multiturn dialogues. Because traditional methods do not distinguish the importance of each sentence and concatenate all sentences in the text, which is likely to generate a model with low prediction accuracy. In this paper, we propose a method of multiturn dialogue classification based on key sentences mining. We design a keywords extraction algorithm, mining key sentences from the dialogue text. We propose an algorithm finishing the computation of the weights of each sentence. According to the sentence weight and the sentence vector, the dialogue text is transformed to a dialogue vector. The dialogue text is classified by a classifier, and the input is the dialogue vector. We conducted sufficient experiments on a real-world dataset, evaluating the performance of the proposed method. The experimental results show that our method outperforms the related methods on a series of evaluation metrics.																	0824-7935	1467-8640															10.1111/coin.12345		JUN 2020											
J								Physical education teaching for saving energy in basketball sports athletics using Hidden Markov and Motion Model	COMPUTATIONAL INTELLIGENCE										basketball; energy saving; Hidden Markov model; motion model; physical education		A new trend of schooling is characterized by long-term learning and driven by technological, social, and cultural developments. This trend means that physical education (PE) and sports science must be strengthened. Although PE and sports are practical activities, specialists can make use of modern teaching technologies. Basketball is intended to develop the skills and understanding of movement and protection and its ability to take use of an active and healthy lifestyle in a variety of activities. Therefore, a survey suggested that, energy is valuable part in PE, especially more energy is increased by playing basketball. Hence this study concentrates on Hidden Markov hybridised with Motion Model (HM-HMM), to save energy through the habit of playing basketball. The secret of HM-HMM is computer evaluation system, particularly useful for the calculation of mastery of the academic knowledge of a collection of information points in pathways in PE in colleges to approximate and infer difficulties and unknown properties according to the observed variables. This article introduces a motion model which is more practical based on studies for player movement to save energy.																	0824-7935	1467-8640															10.1111/coin.12343		JUN 2020											
J								Objective identification of local spatial structure for material characterization	STATISTICAL ANALYSIS AND DATA MINING										atom probe tomography; clustering; cold spots; cross-correlation; global association; hot spots; local association; multiple testing	ATOM-PROBE TOMOGRAPHY; MULTIVARIATE STATISTICAL-ANALYSIS; FALSE DISCOVERY RATE; MORANS I; DISTRIBUTIONS; ASSOCIATION	Objective tools for characterizing materials at the atomic level are often difficult to develop because of the size or structure of the data. Atom probe tomography (APT) is a measurement tool that maps the location and type of atoms in materials in three-dimensions (3D), producing data sets with potentially billions of observations. In this work, we present a set of spatial statistics methods developed to test the null hypotheses of no global spatial association; no local spatial association; and no local spatial cross-correlation and apply these for the first time to APT data. The empirical and modeled covariogram and Moran's I can be used to study the global structure of a spatially referenced atomic element. The local indicator of spatial association (LISA) identifies volumes where high levels of values (hot spots) or low levels of values (cold spots) of elemental clustering exist. The local indicator of spatial cross-correlation (LISC) reports where simultaneously high levels or low levels of two atomic elements occur. For each test statistic at each location, an associated p-value is produced that can be used to weigh the evidence in favor of spatial clustering. The size of APT data sets presents some challenges, so the effect of weight functions and neighborhood selection on the computation and significance of the test statistics are discussed, and the issue of multiple statistical testing is also considered. These methods are illustrated using an APT data set with atomic percentages reported in voxels binned to 1 nm(3).																	1932-1864	1932-1872				AUG	2020	13	4					377	393		10.1002/sam.11462		JUN 2020											
J								Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces	ADAPTIVE BEHAVIOR										Adaptive models; predictive models; episodic memory; memory consolidation; intrinsic motivation; robotics	SYSTEMS; MODELS	This work presents an architecture that generates curiosity-driven goal-directed exploration behaviours for an image sensor of a microfarming robot. A combination of deep neural networks for offline unsupervised learning of low-dimensional features from images and of online learning of shallow neural networks representing the inverse and forward kinematics of the system have been used. The artificial curiosity system assigns interest values to a set of pre-defined goals and drives the exploration towards those that are expected to maximise the learning progress. We propose the integration of an episodic memory in intrinsic motivation systems to face catastrophic forgetting issues, typically experienced when performing online updates of artificial neural networks. Our results show that adopting an episodic memory system not only prevents the computational models from quickly forgetting knowledge that has been previously acquired but also provides new avenues for modulating the balance between plasticity and stability of the models.																	1059-7123	1741-2633														1059712320922916	10.1177/1059712320922916		JUN 2020											
J								Accurate real-time visual SLAM combining building models and GPS for mobile robot	JOURNAL OF REAL-TIME IMAGE PROCESSING										Robot localization; Building models; Multimodal fusion; Graph optimization	ROBUST; VERSATILE; TRACKING	This paper presents a novel 7 DOF (i.e., orientation, translation, and scale) visual simultaneous localization and mapping (vSLAM) system for mobile robots in outdoor environments. In the front end of this vSLAM system, a fast initialization method is designed for different vSLAM backbones, which upgrades the accuracy of trajectory and reconstruction of vSLAM with an absolute scale computed from depth maps generated by building blocks. In the back end of this vSLAM, we propose a nonlinear optimization mechanism throughout which multimodal data are combined for more robust optimization. The modality of building blocks in optimization can improve the tracking accuracy and the scale estimation. By integrating the pose estimated from visual information and the position received through GPS, the optimization further alleviates the drift. The experimental results prove that the proposed method is extremely suitable for outer AR application for outdoor environments, because our method has superior initialization performance, runs in real time, and achieves real scale, higher accuracy, and robustness.																	1861-8200	1861-8219															10.1007/s11554-020-00989-6		JUN 2020											
J								Evaluating causes of algorithmic bias in juvenile criminal recidivism	ARTIFICIAL INTELLIGENCE AND LAW										Criminal recidivism; Machine learning; Algorithmic fairness; Risk assessment; Criminal justice; Automated decision making	RISK-ASSESSMENT; YOUNG OFFENDERS; SAVRY; ASSESSMENTS; YLS/CMI	In this paper we investigate risk prediction of criminal re-offense among juvenile defendants using general-purpose machine learning (ML) algorithms. We show that in our dataset, containing hundreds of cases, ML models achieve better predictive power than a structured professional risk assessment tool, the Structured Assessment of Violence Risk in Youth (SAVRY), at the expense of not satisfying relevant group fairness metrics that SAVRY does satisfy. We explore in more detail two possible causes of this algorithmic bias that are related to biases in the data with respect to two protected groups, foreigners and women. In particular, we look at (1) the differences in the prevalence of re-offense between protected groups and (2) the influence of protected group or correlated features in the prediction. Our experiments show that both can lead to disparity between groups on the considered group fairness metrics. We observe that methods to mitigate the influence of either cause do not guarantee fair outcomes. An analysis of feature importance using LIME, a machine learning interpretability method, shows that some mitigation methods can shift the set of features that ML techniques rely on away from demographics and criminal history which are highly correlated with sensitive features.																	0924-8463	1572-8382															10.1007/s10506-020-09268-y		JUN 2020											
J								A joint hybrid corona based opportunistic routing design with quasi mobile sink for IoT based wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Routing; Sink; Opportunistic; Corona; Low power; Fuzzy logic; Lossy links; WSN; IoT	LOAD DISTRIBUTION; PROTOCOL; DISSEMINATION	The resource constrained wireless sensor embedded devices are deployed in the edge of the Internet of Things (IoT) system for smart monitoring and control of large scale Wireless Sensor Network (WSN) applications. A joint hybrid corona based Opportunistic Routing (OR) with path-constrained Quasi-Mobile Sink (QMS) is designed to address the challenges for (i) providing long term sustainable operation and scalability of WSN (ii) also counteract the hot-spot problem near the sink. This hybrid routing design adopted opportunistic mode of forwarding as it's first and last resort to increase transmission reliability, despite the time varying lossy radio links. The eligible relay-set based on the corona level it resides with respect to sink, collaborates for packet forwarding in a fully distributed online manner during the opportunistic mode. The priority order of relay(s) are determined on the fly based on multi-metrics fuzzy decision logic for timer based coordination and adopts a cross layered differentiated back off strategy for distributed priority based contention. The routing design switches to unicast mode of forwarding via the most trusted relay(s) for subsequent transmissions to increase the energy efficiency under stable link conditions. Simulation results shows that hybrid OR design achieves high end to end packet delivery ratio and minimizes the average energy consumed per node in comparison with existing routing protocol designs. The joint routing design with QMS is found to scale well and prevents the unbalanced energy consumption by minimizing the maximum energy dissipation and normalized energy consumption per packet compared to the Static Sink (SS) and Uncontrolled Mobility Model based Sink (UMM-S).																	1868-5137	1868-5145															10.1007/s12652-020-02116-6		JUN 2020											
J								The Multiple Steiner TSP with order constraints: complexity and optimization algorithms	SOFT COMPUTING										Steiner TSP; Order constraints; Integer linear programming; Branch-and-Price algorithm; Branch-and-Cut algorithm	TRAVELING SALESMAN PROBLEM; CUT ALGORITHM; FORMULATIONS; BRANCH	We consider a variant of the Travelling Salesman Problem (TSP), the Multiple Steiner TSP with Order constraints (MSTSPO). Consider a weighted undirected graph and a set of salesmen, and each salesman is associated with a set of compulsory vertices to visit, called terminals. The MSTSPO consists in finding a minimum-cost subgraph containing for each salesman a tour going in a specified order through its terminals. Along with its importance from a theoretical point of view, the problem is also challenging in practice since it has applications in telecommunication networks. We show that the problem is NP-hard even for a single salesman and propose integer programming formulations. We then devise both Branch-and-Cut and Branch-and-Price algorithms to solve the problem. The extensive computational results are presented, showing the efficiency of our algorithms.																	1432-7643	1433-7479															10.1007/s00500-020-05043-y		JUN 2020											
J								The scale -invariant space for attention layer in neural network	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						1	10															
J								Target transfer Q -learning and its convergence analysis	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						11	22															
J								Integrating adaptive moving window and just -in -time learning paradigms for soft -sensor design	NEUROCOMPUTING											PARTIAL LEAST-SQUARES; LOCALLY WEIGHTED REGRESSION; QUALITY PREDICTION; DATA STREAMS; PLS; MODEL; ENSEMBLE; FUZZY; IDENTIFICATION; SYSTEM																		0925-2312	1872-8286				JUN 7	2020	392						23	37															
J								Fine-grained facial expression analysis using dimensional emotion model	NEUROCOMPUTING											RECOGNITION; DATABASE; PERCEPTION; UNIVERSALITY; SPECIFICITY; EIGENFACES; JUDGMENTS; VALENCE; AROUSAL; 3D																		0925-2312	1872-8286				JUN 7	2020	392						38	49															
J								Multi -column point -CNN for sketch segmentation	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						50	59															
J								Attributes guided facial image completion	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						60	69															
J								Group consensus control for discrete -time heterogeneous multi -agent systems with time delays	NEUROCOMPUTING											NETWORKED PREDICTIVE CONTROL; VARYING NONLINEAR-SYSTEMS; MISSING MEASUREMENTS; SUBJECT; STATE																		0925-2312	1872-8286				JUN 7	2020	392						70	85															
J								Exponential synchronization of multiple impulsive discrete -time memristor-based neural networks with stochastic perturbations and time -varying delays ?	NEUROCOMPUTING											STABILITY; STABILIZATION																		0925-2312	1872-8286				JUN 7	2020	392						86	97															
J								Multi -level feature fusion based Locality -Constrained Spatial Transformer network for video crowd counting	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						98	107															
J								Multimodal image registration using histogram of oriented gradient distance and data -driven grey wolf optimizer	NEUROCOMPUTING											SATELLITE IMAGES; ALGORITHM; PERFORMANCE; FRAMEWORK; SIFT																		0925-2312	1872-8286				JUN 7	2020	392						108	120															
J								Convolutional neural network based diagnosis of bone pathologies of proximal humerus	NEUROCOMPUTING											CLASSIFICATION; INSTABILITY; ULTRASOUND; TUMORS																		0925-2312	1872-8286				JUN 7	2020	392						124	131															
J								Deep learning for ultrasound image caption generation based on object detection	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						132	141															
J								Non -contact heart rate detection by combining empirical mode decomposition and permutation entropy under non -cooperative face shake	NEUROCOMPUTING																													0925-2312	1872-8286				JUN 7	2020	392						142	152															
J								Heart sounds classification using a novel 1-D convolutional neural network with extremely low parameter consumption	NEUROCOMPUTING											FEATURES; SPECTROGRAM; SELECTION																		0925-2312	1872-8286				JUN 7	2020	392						153	159															
J								Dynamic MRI reconstruction exploiting blind compressed sensing combined transform learning regularization	NEUROCOMPUTING										Dynamic MRI; BCS; Image reconstruction; Transform learning regularization; Sparsity priors	LOW-RANK; SPARSIFYING TRANSFORMS; CONVERGENCE GUARANTEES; UNDERSAMPLED (K; T)-SPACE	The goal of dynamic magnetic resonance imaging (dynamic MRI) is to visualize tissue properties and their local changes over time that are traceable in the MR signal. Compressed sensing enables the accurate recovery of images from highly under-sampled measurements by exploiting the sparsity of the images or image patches in a transform domain or dictionary. In this work, we focus on blind compressed sensing (BCS), where the underlying sparse signal model is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the unknown model from highly under-sampled measurements. Specifically, in our model, the patches of the under-sampled images are approximately sparse in a transform domain. Transform learning that combines wavelet and gradient sparsity is considered as regularization in our model for dynamic MR images. The original complex problem is decomposed into several simpler subproblems, then each of the subproblems is efficiently solved with a variable splitting iterative scheme. The results of numerous experiments show that the proposed algorithm outperforms the state-of-the-art compressed sensing MRI algorithms and yields better reconstructions results. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 7	2020	392						160	167		10.1016/j.neucom.2018.12.087													
J								Multi-label transfer learning for the early diagnosis of breast cancer	NEUROCOMPUTING										Computer-aided diagnosis; Breast cancer; Multi-label classification; Convolutional neural network; Transfer learning; Fine-tuning	COMPUTER-AIDED DETECTION; NEURAL-NETWORKS; CLUSTERED MICROCALCIFICATIONS; MAMMOGRAPHY-LEXICON; DATA SYSTEM; CLASSIFICATION; BIOPSY; VARIABILITY; GRADIENT; TEXTURE	Early diagnosis of breast cancer, when it is small and has not spread, can make the disease easier to treat which increases the patient's chances of survival. The recent proposed methods for the early diagnosis of breast cancer, and while showing great success in achieving this goal, rely on one of the indicators in the mammogram to diagnose the patient's condition. Whether it is identifying differences in shapes and patterns of the findings (i.e. masses, calcifications,.etc.) or assessing the breast density as a risk indicator, these Computer-aided Diagnosis (CAD) systems by using single-label classification, fail to exploit the intrinsic useful correlation information among data from correlated domains. Rather than learning to identify the disease based on one of the indicators, we propose the joint learning of the tasks using multi-label image classification. Furthermore, we introduce a new fine-tuning strategy for using transfer learning, that takes advantage of the end-to-end image representation learning when adapting the pre-trained Convolutional Neural Network (CNN) to the new task. We also propose a customized label decision scheme, adapted to this problem, which estimates the optimal confidence for each visual concept. We demonstrate the effectiveness of our approach on four benchmark datasets, CBISDDSM, BCDR, INBreast and MIAS, obtaining better results compared to other commonly used baselines. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 7	2020	392						168	180		10.1016/j.neucom.2019.01.112													
J								Automated hepatobiliary toxicity prediction after liver stereotactic body radiation therapy with deep learning -based portal vein segmentation	NEUROCOMPUTING										Toxicity prediction; Primary liver cancer; SBRT; Deep learning	PHASE-I; RADIOTHERAPY; HEAD; TRIALS; ORGANS; TUMORS	Purpose: To develop a framework for automated prediction of hepatobiliary (HB) toxicity after liver stereotactic body radiation therapy (SBRT). Materials and methods: A newly recognized toxicity type, named central or HB liver toxicity, had been reported, manifestation of which strongly correlates with the dose delivered to portal vein (PV) during SBRT. We propose a novel framework for automated HB toxicity prediction by combining deep learningbased auto-segmentation, PV anatomy analysis and the previously reported HB toxicity model. For validation of the framework, an IBR approved representative database of 72 patients treated with SBRT from primary (37) and metastatic (35) liver cancer was assembled. Each case included a pre-treatment CT, manual segmentations of tumor and PV, approved treatment plan, and the record of acute and late posttreatment toxicities. Performance of the developed framework was evaluated by quantitative comparison against manual predictions of HB toxicity, as well as post-treatment toxicity follow-ups. Results: The manual and automated predictions of HB toxicity were in agreement for 94% cases using either V-BED10 30 = 45 cc or V-BED10 40 = 37 cc dosimetric predictors. When compared to post-treatment follow-ups for primary liver cancer, the proposed automated framework made 86% and 83% correct predictions in comparison to 83% and 80% correct manual predictions using V(BED10)30 = 45 cc or V(BED10)40 >= 37 cc, respectively. Conclusion: The proposed framework automates the HB toxicity prediction with the accuracy similar to manual analysis-based HB toxicity prediction. The strategy is quite general and extendable to the automated prediction of toxicities of other organs. (c) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 7	2020	392						181	188		10.1016/j.neucom.2018.11.112													
J								Robust brain extraction tool for CT head images	NEUROCOMPUTING										Convolutional neural network; Deep learning; Image segmentation; Skull stripping; Brain extraction; Computed tomography	SEGMENTATION; MRI	Extracting brain parenchyma from computed tomography (CT) images of the head is an important prerequisite step in a number of image processing applications, as it improves the computational speed and accuracy of quantitative analyses and image co-registration. In this study, we present a robust method based on fully convolutional neural networks (CNN) to remove non-brain tissues from head CT scans in a computationally efficient manner. The method includes an encoding part, which has sequential convolutional filters that produce feature representation of the input image in low dimensional space, and a decoding part, which consists of convolutional filters that reconstruct the input image from the reduced representation. We trained several CNN models on 122 volumetric head CT scans and tested our models on 22 withheld volumetric CT head scans based on two experts' manual brain segmentation. The performance of our best CNN model on the test set is: Dice Coefficient = 0.998 +/- 0.001 (mean +/- standard deviation), recall = 0.999 +/- 0.001, precision = 0.998 +/- 0.001, and accuracy = 1. Our method extracts complete volumetric brain from head CT images in about 2 s which is substantially faster than currently available methods. To the best of our knowledge, this is the first study using CNN to perform brain extraction from CT images. In conclusion, the proposed approach based on CNN provides accurate extraction of brain tissue from head CT images in a computationally efficient manner. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 7	2020	392						189	195		10.1016/j.neucom.2018.12.085													
J								A fully convolutional network feature descriptor: Application to left ventricle motion estimation based on graph matching in short-axis MRI	NEUROCOMPUTING										Deep learning; Feature descriptor; Ventricle motion estimation; Graph matching; Correspondence	DEFORMATION RECOVERY; CARDIAC MOTION; CINE-MRI; MODELS; SEGMENTATION; ALGORITHM; HEART	Cardiac diseases cause abnormal motion dynamics over the cardiac cycle, and can, therefore, be diagnosed by analyzing the myocardial motion of the left ventricle (LV). In this paper, a feature point descriptor using fully convolutional neural networks (FCNs) is proposed and applied to cardiac motion estimation based on graph matching. A fully convolutional network is trained to predict endocardial contours and extract features of points from short-axis cine magnetic resonance (MR) images. An LV graph is constructed using the extracted point features, and a convex graph matching cost function is defined to estimate the point correspondence between images in two given phases. The sparsity and double stochastic constraints are introduced into the cost function, which is optimized iteratively by the alternating direction method of multipliers (ADMM). Finally, the transformation using compact supported radial basis functions with sparsity constraint is employed to estimate the dense displacement field between two cardiac images in two phases based on the correspondence relationship. The performance of the proposed method was evaluated on two public cardiac databases, and the experimental results show that the FCN feature descriptor outperforms traditional feature descriptors in estimating the correspondence between endocardial contours of the LV. For LV motion estimation, the proposed method provides more accurate motion fields than existing graph matching algorithms. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 7	2020	392						196	208		10.1016/j.neucom.2018.10.101													
