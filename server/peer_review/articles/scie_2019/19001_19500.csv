PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								A hybrid representation-based simile component extraction	NEURAL COMPUTING & APPLICATIONS										Simile component; Concept; Character		Simile, a special type of metaphor, can help people to express their ideas more clearly. Simile component extraction is to extract tenors and vehicles from sentences. This task has a realistic significance since it is useful for building cognitive knowledge base. With the development of deep neural networks, researchers begin to apply neural models to component extraction. Simile components should be in cross-domain. According to our observations, words in cross-domain always have different concepts. Thus, concept is important when identifying whether two words are simile components or not. However, existing models do not integrate concept into their models. It is difficult for these models to identify the concept of a word. What's more, corpus about simile component extraction is limited. There are a number of rare words or unseen words, and the representations of these words are always not proper enough. Exiting models can hardly extract simile components accurately when there are low-frequency words in sentences. To solve these problems, we propose a hybrid representation-based component extraction (HRCE) model. Each word in HRCE is represented in three different levels: word level, concept level and character level. Concept representations (representations in concept level) can help HRCE to identify the words in cross-domain more accurately. Moreover, with the help of character representations (representations in character levels), HRCE can represent the meaning of a word more properly since words are consisted of characters and these characters can partly represent the meaning of words. We conduct experiments to compare the performance between HRCE and existing models. The experiment results show that HRCE significantly outperforms current models.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14655	14665		10.1007/s00521-020-04818-6		MAR 2020											
J								Modeling of UCS value of stabilized pond ashes using adaptive neuro-fuzzy inference system and artificial neural network	SOFT COMPUTING										Unconfined compressive strength; Adaptive neuro-fuzzy inference system; Artificial neural network	UNIAXIAL COMPRESSIVE STRENGTH; BEARING RATIO; ANFIS MODEL; FLY-ASH; PREDICTION; PERFORMANCE; PRESSURE; CAPACITY; SAFETY; SOILS	This paper investigates the capability of adaptive neuro-fuzzy inference system (ANFIS) and artificial neural network (ANN) modeling approach to predict the unconfined compressive strength (UCS) of stabilized pond ashes with lime alone and in combination with lime sludge. Out of 170 data set, a total of 119 data were randomly selected for training, whereas remaining 51 were used for testing the model. Four membership's functions (MFs) such as Gaussian, generalized bell-shaped, triangular, and trapezoidal were used with ANFIS model. Statistical parameters were used to compare the performance of four MF-based ANFIS and ANN models. A comparison of results suggests that Triangular MF-based ANFIS model exhibit better predictive performance with higher CC = 0.980 and lower MSE = 3028.515 and RMSE = 55.032 than other MF-based ANFIS and ANN model. The results of single-factor analysis of variance indicate that there is an insignificant difference between measured and predicted values of UCS using different models. Further, results of sensitivity analysis depict that the curing period, lime sludge, and lime are the most important parameters which affect the performance of Triangular MF-based ANFIS in predicting the UCS of stabilized pond ashes. Thus, the Triangular MF-based ANFIS model could be a useful tool in predicting the UCS value of stabilized pond ashes because of its adequacy in handling uncertainties in the test results with accuracy.																	1432-7643	1433-7479				OCT	2020	24	19					14561	14575		10.1007/s00500-020-04806-x		MAR 2020											
J								An intelligent computer-aided approach for target protein prediction in infectious diseases	SOFT COMPUTING										Graph colouring algorithm; Protein-protein interaction; Essential protein; Deep Learning	SECONDARY STRUCTURE PREDICTION; FEATURES	Essential proteins are the most important constituents for all kinds of organism. The research on predicting essential target proteins contributes significantly to drug development, disease diagnosis, and treatment. Numerous computational- and experimental-based approaches have been used in the recent past for essential protein prediction. However, it is highly challenging to bring remarkable improvement in the accuracy of the essential target proteins. In this method, we introduce an intelligent computational technique known as graph colouring-deep neural network which automatically extracts the target proteins by combining a graph-theoretic approach called graph colouring and neural network. In this approach, the protein-protein interaction network (PPI) of homosapiens are extracted from string DB (data base) and then applied to the graph colouring algorithm. Initially, each protein in the network is assigned a colour by checking the connectivity with the neighbourhood proteins. Secondly, the proteins with primary and secondary colours are extracted from the PPI. Finally, a deep neural network-based approach is used to automatically extract the essential target proteins depending on the physicochemical features of the proteins. To assess the performance of the proposed model, the experiment has been carried out in four different diseases such as cancer, diabetes, asthma and human papilloma virus viral infection. The proposed approach shows a remarkable performance than the traditional approaches in views of various metrics such as accuracy, precision, recall, andF-measure.																	1432-7643	1433-7479				OCT	2020	24	19					14707	14720		10.1007/s00500-020-04815-w		MAR 2020											
J								ROI-based feature learning for efficient true positive prediction using convolutional neural network for lung cancer diagnosis	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Deep structured algorithm; Batch normalization; ReLu; Max-pooling; Softmax; Benign and malignant; Morphological features	PULMONARY NODULE DETECTION; COMPUTER-AIDED DIAGNOSIS; CT IMAGES; CLASSIFICATION; SYSTEM; ALGORITHMS; REDUCTION	Convolutional neural network (CNN) is one of the deep structured algorithms widely applied to analyze the ability to visualize and extract the hidden texture features of image datasets. The study aims to automatically extract the self-learned features using an end-to-end learning CNN and compares the results with the conventional state-of-art and traditional computer-aided diagnosis system's performance. The architecture consists of eight layers: one input layer, three convolutional layers and three sub-sampling layers intercepted with batch normalization, ReLu and max-pooling for salient feature extraction, and one fully connected layer that uses softmax function connected to 3 neurons as output layer, classifying an input image into one of three classes categorized as nodules >= 3 mm as benign (low malignancy nodules), malignant (high malignancy nodules), and nodules < 3 mm and non-nodules >= 3 mm combined as non-cancerous. For the input layer, lung nodule CT images are acquired from the Lung Image Database Consortium public repository having 1018 cases. Images are pre-processed to uniquely segment the nodule region of interest (NROI) in correspondence to four radiologists' annotations and markings describing the coordinates and ground-truth values. A two-dimensional set of re-sampled images of size 52 x 52 pixels with random translation, rotation, and scaling corresponding to the NROI are generated as input samples. In addition, generative adversarial networks (GANs) are employed to generate additional images with similar characteristics as pulmonary nodules. CNNs are trained using images generated by GAN and are fine-tuned with actual input samples to differentiate and classify the lung nodules based on the classification strategy. The pre-trained and fine-tuned process upon the trained network's architecture results in aggregate probability scores for nodule detection reducing false positives. A total of 5188 images with an augmented image data store are used to enhance the performance of the network in the study generating high sensitivity scores with good true positives. Our proposed CNN achieved the classification accuracy of 93.9%, an average specificity of 93%, and an average sensitivity of 93.4% with reduced false positives and evaluated the area under the receiver operating characteristic curve with the highest observed value of 0.934 using the GAN generated images.																	0941-0643	1433-3058				OCT	2020	32	20			SI		15989	16009		10.1007/s00521-020-04787-w		MAR 2020											
J								A computational method based on Gustafson-Kessel fuzzy clustering for a novel islanding detection for grid connected devices and sensors	COMPUTATIONAL INTELLIGENCE										fuzzy clustering; Gustafson-Kessel; point of common coupling (PCC); total harmonic distortion (THD)	PREDICTION; SYSTEMS	Fuzzy Clustering-based (G-K) Gustafson-Kessel is used to create the fuzzy rule-based classifier in a grid connected photovoltaic (PV) system where it is tested using specific features in a grid connected PV inverter for detecting islanding condition. It is detected when harmonic content of voltages at the Point of Common Coupling and inverter increases beyond a threshold value. If islanding is not detected, distribution lines are rendered unsafe. The present study uses G-K fuzzy clustering to categorize islanding and nonislanding incidents. Two features based on Total Harmonic Distortion are extracted and used as inputs for the G-K fuzzy clustering classifier. The proposed technique is tested using nonlinear loads and its performance is verified by simulation using MATLAB Simulink. A hardware test set-up is developed to validate the proposed antiislanding technique and the results obtained are discussed.																	0824-7935	1467-8640															10.1111/coin.12311		MAR 2020											
J								Real-time filtering on parallel SIMD architectures for automated quality inspection	JOURNAL OF REAL-TIME IMAGE PROCESSING										Surface metrology; Real-time filtering; Laser profiling	3-D SENSORS; SURFACE; CALIBRATION	Surface metrology in automated quality inspection is a field, among many others, affected by noise and thus requiring filtering. In surface metrology, filtering is required to remove undesired information from data in order to extract surface features and relevant properties necessary for quality control. Moreover, filtering requires immediate results, while the product is being manufactured. This way, quick correcting actions can be directly applied to solve possible manufacturing issues. This work proposes different strategies to filter height maps in real-time acquired using laser profilers, the most widely used inspection method in industrial applications. Different models to apply the filtering operations are considered, particularly assessing different alternatives to store previous samples in memory, which are required for data filtering. FIFO, double FIFO, circular and double circular buffers are evaluated. Furthermore, CPU parallelism, SIMD instructions and cache-line friendly data structures are analyzed. The proposed methods are extremely efficient, capable of filtering laser profiles at extremely high acquisition rates. The proposed methods are designed for real-time surface metrology, but they are very likely to find potential applications in different areas. The filters are compared in terms of accuracy and speed, including other well-known filters such as the spline filter. Tests analyze execution time, including cache efficiency and filtering accuracy. Results with synthetic data and real data obtained from steel strips show excellent performance, providing accurate results at very high speeds.																	1861-8200	1861-8219															10.1007/s11554-020-00954-3		MAR 2020											
J								Respecializing swarms by forgetting reinforced thresholds	SWARM INTELLIGENCE										Threshold reinforcement; Emergent coordination; Decentralized division of labor; Specialization; Task allocation; Multiagent deployment	DIVISION-OF-LABOR; MULTIAGENT TASK ALLOCATION; ROBOCUP RESCUE; SPECIALIZATION; INFORMATION; SYSTEMS; HISTORY; ROBOTS; AGENTS	Response threshold reinforcement is a powerful model for decentralized task allocation and specialization in multiagent swarms. In dynamic environments, initial task assignments and specializations must be updated over time to meet changing system needs. The very nature of threshold reinforcement-based behavior can, however, hinder respecialization, limiting its usability in real-world applications. We propose a decentralized forgetting-based extension to response threshold reinforcement and show that it can improve the efficiency and stability of the resulting task assignments under changing system demands.																	1935-3812	1935-3820				SEP	2020	14	3					171	204		10.1007/s11721-020-00181-3		MAR 2020											
J								Comparison of recent optimization algorithms for design optimization of a cam-follower mechanism	KNOWLEDGE-BASED SYSTEMS										Algorithms; Metaheuristics; Optimization; Cam mechanism; Follower motion law; Grey wolf optimizer; Salp Swarm Optimizer; Moth-Flame optimizer; Multi verse optimizer; Ant lion optimizer	WATER CYCLE ALGORITHM; OPTIMUM DESIGN; PROFILE OPTIMIZATION; RESIDUAL VIBRATIONS; SLIDING VELOCITY; GREY WOLF; ANT LION; EVOLUTIONARY; SEARCH; MINIMIZATION	This study presents the application of seven recent meta-heuristic optimization algorithms to automate design of disk cam mechanism with translating roller follower regarding four follower motion laws. The algorithms are: salp swarm algorithm (SSA), moth-flame optimization (MFO), ant lion optimizer (ALO), multi verse optimizer (MVO), grey wolf optimizer (GWO), evaporation rate water cycle algorithm (ER-WCA), and mine blast algorithm (MBA). The optimum cam design problem is formulated with three objectives including the minimum congestion, maximum performance, and maximum strength resistance of the cam. Moreover, the effect of selecting follower motion law on the optimal design of mechanism is investigated. The computational results clearly indicate that the utilized algorithms are very competitive in structural design optimization, especially MBA, ER-WCA, MFO and GWO techniques. Among the four follower motion laws, the polynomial 3-4-5 degree is the best one. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105237	10.1016/j.knosys.2019.105237													
J								Improving the Reliability of Deep Neural Networks in NLP: A Review	KNOWLEDGE-BASED SYSTEMS										Adversarial examples; Adversarial texts; Natural language processing	ADVERSARIAL ATTACKS	Deep learning models have achieved great success in solving a variety of natural language processing (NLP) problems. An ever-growing body of research, however, illustrates the vulnerability of deep neural networks (DNNs) to adversarial examples - inputs modified by introducing small perturbations to deliberately fool a target model into outputting incorrect results. The vulnerability to adversarial examples has become one of the main hurdles precluding neural network deployment into safety-critical environments. This paper discusses the contemporary usage of adversarial examples to foil DNNs and presents a comprehensive review of their use to improve the robustness of DNNs in NLP applications. In this paper, we summarize recent approaches for generating adversarial texts and propose a taxonomy to categorize them. We further review various types of defensive strategies against adversarial examples, explore their main challenges, and highlight some future research directions. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105210	10.1016/j.knosys.2019.105210													
J								MoralStrength: Exploiting a moral lexicon and embedding similarity for moral foundations prediction	KNOWLEDGE-BASED SYSTEMS										Moral foundations; Moral values; Lexicon; Twitter data; Natural language processing; Machine learning	ATTITUDES; VALUES; WORDS	Moral rhetoric plays a fundamental role in how we perceive and interpret the information we receive, greatly influencing our decision-making process. Especially when it comes to controversial social and political issues, our opinions and attitudes are hardly ever based on evidence alone. The Moral Foundations Dictionary (MFD) was developed to operationalize moral values in the text. In this study, we present MoralStrength, a lexicon of approximately 1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary, based on WordNet synsets. Moreover, for each lemma it provides with a crowdsourced numeric assessment of Moral Valence, indicating the strength with which a lemma is expressing the specific value. We evaluated the predictive potentials of this moral lexicon, defining three utilization approaches of increased complexity, ranging from lemmas' statistical properties to a deep learning approach of word embeddings based on semantic similarity. Logistic regression models trained on the features extracted from MoralStrength, significantly outperformed the current stateof-the-art, reaching an F1-score of 87.6% over the previous 62.4% (p-value < 0.01), and an average F1-Score of 86.25% over six different datasets. Such findings pave the way for further research, allowing for an in-depth understanding of moral narratives in text for a wide range of social issues. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105184	10.1016/j.knosys.2019.105184													
J								MiFI-Outlier: Minimal infrequent itemset-based outlier detection approach on uncertain data stream	KNOWLEDGE-BASED SYSTEMS										Outlier detection; Minimal infrequent itemset mining; Uncertain data stream; Deviation indices; Data mining	CONCEPT DRIFT DETECTION; FREQUENT PATTERNS; EFFICIENT	Massive outlier detection approaches have been proposed for static datasets in the past twenty years, and they have acquired good achievements. In real life, uncertain data stream is more and more common, but most existing outlier detection approaches were not suitable for uncertain data stream environment. In addition, many outlier detection approaches have not considered the appearing frequency of each element, which resulted the detected outliers not coincide with the definition of outlier. Itemset-based outlier detection approaches provided a good solution for this problem, and they have got more attentions in these years. In this paper, a novel two-step minimal infrequent itemset-based outlier detection approach called MiFI-Outlier is proposed to effectively detect the outliers from uncertain data stream. In itemset mining phase, a matrix-based method called MiFIUDSM is proposed to mine the minimal infrequent itemsets (Mins) from uncertain data stream, and then an improved approach called MiFI-UDSM* is proposed for more effectively mining these minimal infrequent itemsets using the ideas of "item cap" and "support cap". In outlier detection phase, based on the mined MiFIs, three deviation indices including minimal infrequent itemset deviation index (MiFIDI), similarity deviation index (SDI) and transaction deviation index (TDI) are defined to measure the deviation degree of each transaction, and then the MiFI-Outlier is used to identify the outliers from uncertain data stream. Several experimental studies are conducted on public datasets and synthetic datasets, and the results show that the proposed approaches outperform in infrequent itemset mining phase and outlier detection phase. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105268	10.1016/j.knosys.2019.105268													
J								Multi-variable estimation-based safe screening rule for small sphere and large margin support vector machine	KNOWLEDGE-BASED SYSTEMS										Safe sample screening; Small sphere and large margin; Imbalanced data classification; Novelty detection; Support vector machine		Small Sphere and Large Margin (SSLM) SVM is one of the most competitive methods for Novelty Detection. However, the existing solvers for SSLM cannot deal with large data due to the expensive time cost. Although recently emerged safe screening methods can effectively enhance the computational speed, it is not available for SSLM because SSLM has multiple variables which cannot be represented explicitly by the linear combination of training samples. In this work, we construct a new safe screening rule for SSLM (MVE-SSR-SSLM) by integrating the v-property, KKT conditions and variational inequalities. It is the first safe screening rule for a family of hypersphere support vector machine with multiple variables. The inactive samples are removed before actually solving the problem to accelerate the solving procedure without any loss of safety. Numerical experiments on fifteen benchmark datasets and Chinese wine dataset are conducted to show the validity and stability of the proposed MVE-SSR-SSLM. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105223	10.1016/j.knosys.2019.105223													
J								A characterization for some type-2 fuzzy strong negations	KNOWLEDGE-BASED SYSTEMS										Type-2 fuzzy sets; Normal and convex functions; Negation; Strong negation; Order automorphism	SETS; CONTRADICTION; ALGEBRA	P. Hernandez et al. in 2014 established the axioms that an operation must fulfill in order to be a negation on a bounded poset (partially ordered set). In this work, we focus on the set L of the membership degrees of the type-2 fuzzy sets which are normal and convex functions in [0,1]. This set has a bounded and complete lattice structure, thank to which negations and strong negations have been constructed by the authors applying the Zadeh's Extension Principle. In addition, the authors showed new negations on L that are different from the negations presented in 2014 applying the Zadeh's Extension Principle. In this work, the authors obtain a characterization of the strong negations on L that leave the constant function 1 fixed. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105281	10.1016/j.knosys.2019.105281													
J								A second-order-based decision tool for evaluating decisions under conditions of severe uncertainty	KNOWLEDGE-BASED SYSTEMS										Decision analysis; Decision software; Imprecise criteria weights; Imprecise probabilities	UTILITY; ISSUES	The requirement to assign precise numerical values to model entities such as criteria weights, probabilities, and utilities is too strong in most real-life decision situations, and hence alternative representations and evaluation mechanisms are important to consider. In this paper, we discuss the DecideIT 3.0 state-of-the-art software decision tool and demonstrate its functionality using a real-life case. The tool is based on a belief mass interpretation of the decision information, where the components are imprecise by means of intervals and qualitative estimates, and we discuss how multiplicative and additive aggregations influence the resulting distribution over the expected values. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105219	10.1016/j.knosys.2019.105219													
J								Influence maximization based on the realistic independent cascade model	KNOWLEDGE-BASED SYSTEMS										Diffusion model; Influence maximization; Social networks; Seeding algorithm		In order to propagate information through the social network, how to find a seed set that can affect the maximum number of users is named as influence maximization problem. A lot of works have been done on this problem, mainly including two aspects: establishing a reasonable information diffusion model and putting forward the appropriate seeding strategy. However, there are few models in the existing ones that consider the acceptance probability of candidate seed nodes in social networks. So in this paper, we consider and solve this problem by introducing a more realistic model, which is the proposed Realistic Independent Cascade (RIC) model. Based on the RIC model, many state-of-the-art seeding algorithms perform not so well because there is no mechanism on dealing with the acceptance probability. So based on the RIC model, we propose a new seeding strategy which is called R-greedy. Furthermore, M-greedy algorithm is proposed to reduce the time complexity of R-greedy. Then, D-greedy algorithm which not only increased the performance but also reduced the time complexity of R-greedy is proposed by combining the advantages of R-greedy and M-greedy. Experiments on the real-world networks and synthetic networks demonstrate that the proposed R-greedy, M-greedy and D-greedy algorithms outperforms state-of-the-art algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105265	10.1016/j.knosys.2019.105265													
J								Equilibrium optimizer: A novel optimization algorithm	KNOWLEDGE-BASED SYSTEMS										Optimization; Metaheuristic; Genetic algorithm; Particle Swarm Optimization; Physics-based	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; CELLULAR-AUTOMATA; SIMULATION	This paper presents a novel, optimization algorithm called Equilibiium Optimizer (EO), inspired by control volume mass balance models used to estimate both dynamic and equilibrium states. In EO, each particle (solution) with its concentration (position) acts . as a search' agent. The search agents randomly update their concentration with respect to best-so-far solutions, namely equilibrium candidates, to finally reach to the equilibrium state (optimal result). A well-defined "generation rate" term is proved to invigorate EO's ability in exploration, exploitation, and local minima avoidance. The proposed algorithm is benchmarked with 58 unimodal, multimodal, and composition functions and three engineering application problems. Results of EO are compared to three categories of existing optimization methods, including: (i) the most well-known meta-heuristics, including Genetic Algorithm (GA), Particle Swarm Optimization (PSO); (ii) recently developed algorithms, including Grey Wolf Optimizer (GWO), Gravitational Search Algorithm (GSA), and Salp Swarm Algorithm (SSA); and (iii) high performance optimizers, including CMA-ES, SHADE, and LSHADE-SPACMA. Using average rank of Friedman test, for all 58 mathematical functions EO is able to outperform PSO, GWO, GA, GSA, SSA, and CMA-ES by 60%, 69%, 94%, 96%, 77%, and 64%, respectively, while it is outperformed by SHADE and LSHADE-SPACMA by 24% and 27%, respectively. The Bonferroni-Dunn and Holm's tests for all functions showed that EO is significantly a better algorithm than PSO, GWO, GA, GSA, SSA and CMA-ES while its performance is statistically similar to SHADE and LSHADE-SPACMA. The source code of EO is publicly availabe at https://github.comiafshinfaramarzi/Equilibrium-Optimizer, http://builtenvi.com/portfolio/equilibrium-optimizer/ and http://www.alimirjalili.com/SourceCodes/EOcode.zip. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105190	10.1016/j.knosys.2019.105190													
J								Multi-view Locality Low-rank Embedding for Dimension Reduction	KNOWLEDGE-BASED SYSTEMS										Multi-view learning; Low rank; Dimension reduction	ALGORITHM	During the last decades, we have witnessed a surge of interest in learning a low-dimensional space with discriminative information from one single view. Even though most of them can achieve satisfactory performance in certain situations, they fail to fully consider the information from multiple views which are highly relevant but sometimes look different from each other. Besides, correlations between features from multiple views always vary greatly, which challenges the capability of multi-view subspace learning methods. Therefore, how to learn an appropriate subspace which could maintain valuable information from multi-view features is of vital importance but challenging. To tackle this problem, this paper proposes a novel multi-view dimension reduction method named Multi-view Locality Low-rank Embedding for Dimension Reduction (MvL(2)E). MvL(2)E mainly focuses on capturing a common low-dimensional embedding among multiple different views, which makes full use of correlations between multi-view features by adopting low-rank representations. Meanwhile, it aims to maintain the correlations and construct a suitable manifold structure to capture: the low-dimensional embedding for multi-view features. A centroid based scheme is designed to' get one common low-dimensional manifold space and force multiple views to learn from each other. And an iterative alternating strategy is developed to obtain the optimal solution of MvL(2)E. The proposed method is evaluated on 5 benchmark datasets. Comprehensive experiments show that our proposed MvL(2)E can achieve comparable performance with previous approaches proposed in recent works of literature. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105172	10.1016/j.knosys.2019.105172													
J								Mining cost-effective patterns in event logs	KNOWLEDGE-BASED SYSTEMS										Event logs; Sequences; Pattern mining; Sequential patterns; Cost-effective patterns; Utility; Cost	EFFICIENT ALGORITHMS; SEQUENTIAL PATTERNS	High Utility Pattern Mining is a popular task for analyzing data. It consists of discovering patterns having a high importance in databases. A popular application of high utility pattern mining is to identify high utility (profitable) patterns in customer transaction data. Though such analysis can be useful to understand data, it does not consider the cost (e.g. effort, resources, money or time) required for obtaining the utility (benefits). In this paper, we argue that to discover interesting patterns in event sequences, it is useful to consider both a utility model and a cost model. For example, to identify cost-effective ways of treating patients from medical pathways data, it is desirable to consider not only the ability of treatments to inhibit symptoms or cure a disease (utility) but also the resources consumed and the time spent (cost) to provide these treatments. Based on this perspective, this paper defines a novel task of discovering Cost-Effective Event Sequences in event logs. In this task, cost is modeled as numeric values, while utility is represented either as binary or numeric values. Measures are proposed to evaluate the trade-off and correlation between cost and utility of patterns to identify cost-effective patterns (patterns having a low cost but providing a high utility). Three efficient algorithms called CEPB, corCEPB and CEPN are designed to extract these patterns. They rely on a tight lower-bound on the cost and a memory buffering technique to find patterns efficiently. Experiments show that the proposed algorithms achieve high efficiency, that proposed optimizations improve efficiency, and that insightful cost-effective patterns are found in real-life e-learning data. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105241	10.1016/j.knosys.2019.105241													
J								Epilepsy seizure detection using complete ensemble empirical mode decomposition with adaptive noise	KNOWLEDGE-BASED SYSTEMS										EEG; Epilepsy seizure; CEEMDAN; Normal inverse Gaussian pdf; AdaBoost	INVERSE GAUSSIAN PARAMETERS; OBSTRUCTIVE SLEEP-APNEA; DECISION-SUPPORT-SYSTEM; EEG-SIGNALS; NEURAL-NETWORK; AUTOMATED IDENTIFICATION; FEATURE-EXTRACTION; SPECTRAL FEATURES; WAVELET TRANSFORM; CLASSIFICATION	Background: Epileptic seizure detection is traditionally performed by visual observation of Electroencephalogram (EEG) signals. Owing to its onerous and time-consuming nature, seizure detection based on visual inspection hinders epilepsy diagnosis, monitoring, and large-scale data analysis in epilepsy research. So, there is a dire need of an automatic seizure detection scheme. Method: An automated scheme for epileptic seizure identification is developed in thii study. Here we utilize a signal processing technique, namely-complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) for epileptic seizure identification. First, we decompose segments of EEG signals into intrinsic mode functions by CEEMDAN. The mode functions are then modeled by normal inverse Gaussian (NIG) pdf parameters. In this work, NIG modeling is employed in conjunction with CEEMDAN for epileptic seizure detection for the first time. The efficacy of the NIG parameters in the CEEMDAN domain is demonstrated by intuitive, graphical, and statistical analyses. Adaptive Boosting, an eminent ensemble learning based classification model, is implemented to perform classification. Results: Experimental outcomes suggest that the algorithmic performance of the proposed scheme is promising in all the cases of clinical significance. Comparative evaluation of algorithmic performance with the state-of-the-art schemes manifest that the seizure detection scheme proposed herein outperforms competing algorithms in terms of accuracy, sensitivity, specificity, and Cohen's Kappa coefficient. Conclusions: Upon its implementation in clinical practice, the proposed seizure detection scheme will eliminate the onus of medical professionals and expedite epilepsy research and diagnosis. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105333	10.1016/j.knosys.2019.105333													
J								Deep transfer multi-wavelet auto-encoder for intelligent fault diagnosis of gearbox with few target training samples	KNOWLEDGE-BASED SYSTEMS										Deep transfer multi-wavelet auto-encode; Gearbox fault; Transfer diagnosis; Variable working conditions; Few target training samples	CONVOLUTIONAL NEURAL-NETWORK; CLASSIFICATION; APPROXIMATION; BEARINGS; ENTROPY; MODEL	Lack of typical fault samples remains a huge challenge for intelligent fault diagnosis of gearbox. In this paper, a novel approach named deep transfer multi-wavelet auto-encoder is presented for gearbox intelligent fault diagnosis with few training samples. Firstly, new-type deep multi-wavelet auto-encoder is designed for learning important features of the collected vibration signals of gearbox. Secondly, high-quality auxiliary samples are selected based on similarity measure to well pre-train a source model sharing similar characteristics with the target domain. Thirdly, parameter knowledge acquired from the source model is transferred to target model using very few target training samples. Transfer diagnosis cases for different fault severities and compound faults of gearbox confirm the feasibility of the proposed approach even if the working conditions have significant changes. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105313	10.1016/j.knosys.2019.105313													
J								Interpretable mammographic mass classification with fuzzy interpolative reasoning	KNOWLEDGE-BASED SYSTEMS										Mammographic mass classification; Fuzzy rule-based system; Weighted interpolative reasoning; Inference interpretability	BREAST-CANCER DIAGNOSIS; FEATURE-SELECTION; TEXTURE FEATURES; SEGMENTATION; ENHANCEMENT; ENSEMBLE; SYSTEMS; SCALE; SHAPE	Breast mass cancer remains a great challenge for developing advanced computer-aided diagnosis (CADx) systems, to assist medical professionals for the determination of benignancy or malignancy of masses. This paper presents a novel approach to building fuzzy rule-based CADx systems for mass classification of mammographic images, via the use of weighted fuzzy rule interpolation. It describes an integrated implementation of such a classification system that ensures interpretable classification of masses through firing the rules that match given observations, while having the capability of classifying unmatched observations through fuzzy rule interpolation (FRI). In particular, a feature weight-guided FRI scheme is exploited to enable such inference. The work is implemented through integrating feature weights with a popular scale and move transformation-based FRI, with the individual feature weights derived from feature selection as a preprocessing process. The efficacy of the proposed CADx system is systematically evaluated using two real-world mammographic image datasets, demonstrating its explicit interpretability and potential classification performance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105279	10.1016/j.knosys.2019.105279													
J								Fuzzy associative memories with autoencoding mechanisms	KNOWLEDGE-BASED SYSTEMS										Associative memories; Two-level fuzzy associative memories; Particle swarm optimization (PSO); Differential evolution (DE); Gradient decent (GD); Fuzzy associative memories	NEURAL-NETWORKS; RULE	Associative memories constructed and operating in the presence of big data offer an effective way to realize association mechanisms aimed at storing and recalling items. In this study, we develop a logic-driven model of two-level fuzzy associative memories augmented by autoencoding processing. It is composed of two functional modules. The first module of this architecture implements an efficient dimensionality reduction of the original high dimensional data with the use of an autoencoder. This helps achieve storing and completing the recall realized by a logic-oriented associative memory which constitutes the second module of the architecture. The optimization of the association matrices studied in the paper involves both gradient-based learning mechanisms and the algorithms of population-based optimization, i.e., particle swarm optimization (PSO) and differential evolution (DE). A suite of experimental studies is presented to quantify the performance of the proposed approach. Comparative studies are also conducted to show and quantify the advantages of the mechanisms of associative recall and storage augmented by the autoencoding process. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105090	10.1016/j.knosys.2019.105090													
J								A decomposition-based differential evolution with reinitialization for nonlinear equations systems	KNOWLEDGE-BASED SYSTEMS										Nonlinear equations systems; Decomposition technique; Reinitialization; Sub-population control strategy; Differential evolution	MULTIPLE OPTIMAL-SOLUTIONS; GLOBAL OPTIMIZATION; MULTIOBJECTIVE OPTIMIZATION; ALGORITHM; ROOTS	Solving nonlinear equations systems (NESS) is one of the most important challenges in numerical computation, especially to find multiple roots in one run. In this paper, a decomposition-based differential evolution with reinitialization is proposed to tackle this challenging task. The main advantages of our method are: (i) an improved parameter-free decomposition technique is exploited to partition the population into numerous sub-populations to locate multiple roots of NESs; (ii) to enhance the search ability of optimization algorithm, a sub-population control strategy is presented to control the number of solutions in the sub-populations; and (iii) the sub-population reinitialization mechanism is proposed to enrich the population diversity. To evaluate the performance of our approach, thirty NES problems with different characteristics are selected as the test suite. Moreover, to further indicate the superiority of our method, ten complex NESs with many roots are also tested. Experimental results show that the proposed approach can locate multiple roots in a single run. In addition, it is able to obtain better results compared with other state-of-the-art methods in terms of both root rate and success rate. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105312	10.1016/j.knosys.2019.105312													
J								A matrix factorization based dynamic granularity recommendation with three-way decisions	KNOWLEDGE-BASED SYSTEMS										Sequential three-way decisions; Granularity; Recommender systems; Matrix factorization	UPDATING REDUCTS; SYSTEMS; SUPPORT	Recommender systems (RSs) are effective technologies and tools used to deal with the problems of information overload, and have been developed rapidly in nearly two decades. In this paper, with the consideration of uncertain and multi-level characteristic of recommendation information (RI), we combine three-way decisions with granular computing, and build a novel dynamic three-way recommendation model to address the limitations of static two-way recommendation. First, we propose a dynamic three-way granularity recommendation (DTWGR), which adopts a top-down dynamic recommendation strategy based on the granular structure of RI. Second, we introduce a matrix factorization framework and further utilize three methods: SVD, SVD++ and NMF, to construct the granular structure of RI. In addition, three extended recommendation algorithms, namely, DTWGR-SVD, DTWGRSVD ++ and DTWGR-NMF, are proposed by combining DTWGR with matrix factorization. The recommendation cost caused by misclassification cost and teaching cost, is carefully investigated in our work. Finally, experimental results of three well-known Movielens data sets validate the effectiveness and reliability of our proposed model. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105243	10.1016/j.knosys.2019.105243													
J								Predicting lncRNA-miRNA interactions based on logistic matrix factorization with neighborhood regularized	KNOWLEDGE-BASED SYSTEMS										LncRNA; MiRNA; Matrix factorization; Neighborhood regularized; Interaction prediction	COMPLEX DISEASES; DATABASE; RNA; MICRORNAS; TARGET; SNPS	Long non-coding RNAs (lncRNAs) and microRNAs (miRNAs) interactions play important roles in diagnostic biomarkers and therapeutic targets for various human diseases. However, experimental methods for finding miRNAs associated with a particular lncRNA are costly, time consuming, and only a few theoretical approaches play a role in predicting potential lncRNA-miRNA associations. In this study, we have established a novel matrix factorization model to predict lncRNA-miRNA interactions, namely lncRNA-miRNA interactions prediction by logistic matrix factorization with neighborhood regularized (LMFNRLMI). Meanwhile, it only utilizes known positive samples to mine potential associations in data that lack negative samples. As a result, this new model obtains reliable performance in the leaveone-out cross validation (the AUC of 0.9319) and 5-fold cross validation (the AUC of 0.9220), which has significantly improved performance in predicting potential lncRNA-miRNA associations compared to other models. Furthermore, comparison with several other network algorithms, and test based on all kinds of similarity, our model successfully confirms the superiority of LMFNRLMI. Whereby, we hope that LMFNRLMI can be a useful tool for potential lncRNA-miRNA association identification in the future. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105261	10.1016/j.knosys.2019.105261													
J								CoFiGAN: Collaborative filtering by generative and discriminative training for one-class recommendation	KNOWLEDGE-BASED SYSTEMS										Collaborative filtering; One-class feedback; Pairwise preference learning; Adversarial training	USER	In this paper, we study an important collaborative filtering problem with users' one-class feedback such as purchases and likes that are pervasive in various recommendation scenarios. In particular, we make a significant extension of IRGAN by introducing rich interactions between a generator and a discriminator, and then design a novel collaborative filtering algorithm termed as CoFiGAN. In our CoFiGAN, the complementarity of the generative training and the discriminative training is exploited more completely, which enhances the accuracy of modeling users' behaviors. Similar to other GAN-based algorithms, our CoFiGAN can also be interpreted as playing a minimax game, i.e., the generator generates samples close to the true ones aiming to confuse the discriminator, while the latter focuses on distinguishing between the true and generated samples. Different from others, the generator in our CoFiGAN generates items from a more direct and effective way under the guidake of the discriminator in order to accelerate convergence in adversarial training and increase the diversity of the generated samples to avoid mode collapse to some extent. Extensive empirical studies on four public and real-world datasets show that our CoFiGAN performs better than IRGAN and other very strong recommendation algorithms in terms of the commonly used ranking-oriented evaluation metrics. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105255	10.1016/j.knosys.2019.105255													
J								Optimizing co-existing multicast routing trees in IP network via discrete artificial fish school algorithm	KNOWLEDGE-BASED SYSTEMS										Multicast; Optimization; Bandwidth competition; Artificial fish school algorithm; Steiner tree	PARTICLE SWARM OPTIMIZATION; PACKING	Increasingly more internet-applications require the supports of multiple multicasts, while existing multicast routing algorithms only aim to establish a single multicast routing tree (MRT). Sequentially optimizing the co-existing MRTs each is a common practice of handling the situation of multiple multicast session being concurrent, but it results in link-congestion on low-cost routes, or leads to non-optimal solution even if bandwidth reservation strategy is adopted. This paper proposes to optimize multiple co-existing MRTs as a whole via one-off optimization instead of sequentially optimizing each in isolation. To carry out the one-off optimization, a discrete artificial fish school algorithm (DAFSA) is proposed. The simulation results show that the proposed DAFSA is capable of optimally packing co-existing MRTs and exhibits remarkably better ability than several the most representative state-of-the-art algorithms in the sense of avoiding the link-congestion and minimizing the overall tree cost. The running time of the proposed DAFSA fully meets the requirements of the practical IP multicasting. Besides, Monte Carlo test proves that the convergence of the proposed DAFSA is not sensitive to its parameters. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105276	10.1016/j.knosys.2019.105276													
J								On modeling similarity and three-way decision under incomplete information in rough set theory	KNOWLEDGE-BASED SYSTEMS										Incomplete information; Possible-world semantics; Rough set; Similarity; Three-way decision	ATTRIBUTE REDUCTION; RULE ACQUISITION; INDISCERNIBILITY; APPROXIMATIONS	Although incomplete information is a well studied topic in rough set theory, there still does not exist a general , agreement on the semantics of various types of incomplete information. This has led to some confusions and many definitions of similarity or tolerance relations on a set of objects, without a sound of semantical justification. The main objective of this paper is to address semantics issues related to incomplete information. We present a four-step model of Pawlak rough set analysis, in order to gain insights on how an indiscernibility relation (i.e., an equivalence relation) is defined and used under complete information. The results enable us to propose a conceptual framework for studying the similarity of objects under incomplete information. The framework is based on a classification of four types of incomplete information (i.e., "do-not-care value", "partially-known value", "class-specific value", and "non-applicable value") and two groups of methods (i.e., relation-based and granule-based methods) for modeling similarity. We examine existing studies on similarity and their relationships. In spite of their semantics differences, all four types of incomplete information can be uniformly represented in a set-valued table. We are therefore able to have a common conceptual possible-world semantics. Finally, to demonstrate the value of the proposed framework, we examine three-way decisions under incomplete information. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105251	10.1016/j.knosys.2019.105251													
J								Diversity measure as a new drift detection method in data streaming	KNOWLEDGE-BASED SYSTEMS										Concept drift; Diversity measure; Disagreement measure; Data stream mining; Non-stationary environments	RECURRING CONCEPTS; ENSEMBLE; ONLINE	Data stream mining is an important research topic that has received increasing attention due to its use in a wide range of applications, such as sensor networks, banking, and telecommunication. A serious and challenging problem affecting data stream mining is concept drift. This problem occurs when the relation between the input data and the target variable changes over time. Several concept drift detection methods have been proposed, however; they either suffer from a high cost in terms of memory or run time or they are not fast enough in terms of detection speed. In this work, we propose a method, called diversity measure as a new drift detection method (DMDDM), which reacts rapidly to concept drift in less time and with less memory consumption. The proposed method combines one of the diversity measures, disagreement measure, known from static learning in streaming scenarios with the Page-Hinkley test and uses these calculations to detect drifts. The proposed method has been experimentally compared with ten drift detection methods in different drift scenarios using several datasets. The experiment results show that the proposed method is capable of detecting concept drifts faster than most of the compared methods with minimal consumption in terms of memory and run time. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105227	10.1016/j.knosys.2019.105227													
J								An Improved Moth-Flame Optimization algorithm with hybrid search phase	KNOWLEDGE-BASED SYSTEMS										Evolutionary algorithms; Moth flame optimization; Constrained optimization; Nature-inspired algorithms	SYMBIOTIC ORGANISMS SEARCH; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; SWARM; NETWORK	In order to solve real-life problems, several metaheuristic optimization algorithms have been developed. The Moth-Flame Optimization (MFO) algorithm is a search algorithm based on a mechanism called transverse orientation. In this mechanism, the moths tend to maintain a fixed angle with respect to the moon. MFO suffers from the degeneration of the global search capability and convergence speed. To overcome these imperfections, an Improved Moth-Flame Optimization (IMFO) algorithm is proposed. The main novelty of the proposed approach is the definition of a hybrid phase between exploration and exploitation. This phase is characterized by a fitness depended weight factor for updating the moths positions. IMFO is tested on selected benchmark functions, CEC2014 test functions and 6 design problems, and compared with recent well-known optimization algorithms. The results show that IMFO achieves the best results with respect to the comparison algorithms in terms of search capability and convergence performances. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105277	10.1016/j.knosys.2019.105277													
J								Knowledge reduction methods of covering approximate spaces based on concept lattice	KNOWLEDGE-BASED SYSTEMS										Concept lattices; Covering approximation spaces; Concept lattice reduction; Covering rough sets; Formal contexts	GENERALIZED ROUGH SETS; FORMAL CONCEPT ANALYSIS; ATTRIBUTE REDUCTION; NEIGHBORHOOD OPERATORS; 3-WAY; CONNECTIONS; OBJECT	Both rough sets and concept lattices, which are two complementary tools in data analysis, are analyzed based on binary relations. The relations between rough sets and concept lattices are important research topic. In this paper, the methods of union reduction and intersection reduction in covering approximation spaces based on concept lattice are discussed, and the relations between union reduction of covering approximation spaces and concept lattices reduction are investigated. We also discuss the relations of element characteristics between covering approximation spaces and the concept lattices. Meanwhile, the connections between reduction of a covering approximation space and that of its compliment space are revealed. The research results establish a bridge between the rough sets and concept lattices and help one to gain much more insights into the two theories. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105269	10.1016/j.knosys.2019.105269													
J								A fuzzy c-means algorithm based on the relationship among attributes of data and its application in tunnel boring machine	KNOWLEDGE-BASED SYSTEMS										Data clustering; FCM; SVR; TBM	SUPPORT VECTOR REGRESSION; PREDICTION; MODEL	In recent years, a number of operation data from engineering systems have been measured and recorded, which promotes the development of engineering data mining. However, the operating state of the engineering system usually changes greatly, which results that the patterns of operation data vary considerably as well. Thus, partitioning these data can provide useful references to the design and analysis of engineering systems. In this paper, a new clustering algorithm based on support vector regression and fuzzy c-means algorithm (SVR-FCM) is proposed to accomplish this work. The SVR-FCM algorithm is based on the framework of fuzzy c-means algorithm (FCM), in which the differences between the clusters are evaluated by the relationship among attributes of data. In the proposed algorithm, support vector regression (SVR) is utilized to describe the relationship among attributes of, and an alteration optimization method is designed to optimize the new designed clustering objective function. A series of experiments on synthetic datasets and real-world datasets are conducted to evaluate the performance of the SVR-FCM algorithm, which shows the higher effectiveness and advances of the SVR-FCM algorithm compared with other popular clustering algorithms. The SVR-FCM algorithm is applied to a tunnel boring machine (TBM) operation dataset collected from a real TBM project in China. The experimental results show that the proposed algorithm performs well in TBM operation data clustering. This paper also highlights the applicability and potential of data clustering in the analysis of other complex engineering systems similar to TBMs. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105229	10.1016/j.knosys.2019.105229													
J								Trust based group decision making in environments with extreme uncertainty	KNOWLEDGE-BASED SYSTEMS										Group decision making; Uncertainty; Incomplete information; Ignorance situations; Intuitionistic fuzzy preference relations; Consensus; Trust	PREFERENCE RELATIONS; LINGUISTIC INFORMATION; CONSENSUS MODEL; MISSING DATA; CONSISTENCY; IGNORANCE; NETWORK	In group decision making scenarios, where multiple anonymous agents interact, as is the case of social networks, the uncertainty in the provided information as well as the diversity in the experts' opinions make of them a real challenge from the point of view of information aggregation and consensus achievement. This contribution addresses these two main issues in the following way: On the one hand, in order to deal with highly uncertainty group decision making scenarios, whose main particularity is that some of their experts may not be able to provide any single judgment about an alternative, the proposed approach estimates these missing information using the preferences coming from other trusted similar experts who present high degrees of confidence and consistency. On the other hand, with the objective of increasing the consensus among the agents involved in the decision making process, a feedback based influence network has been proposed. In this network, the influence between the agents is calculated by means of a dynamic combination of the inter agents trust, their self confidence, and their similarity. Thanks to this influence network our approach is able to recognize and isolate malicious users adjusting their influence according to the trust degree between them. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105168	10.1016/j.knosys.2019.105168													
J								CTS-LSTM: LSTM-based neural networks for correlated time series prediction	KNOWLEDGE-BASED SYSTEMS										Correlated time series prediction; Spatio-temporal correlation		Correlated time series refer to multiple time series which are recorded simultaneously to monitor the changing of multiple observations in a whole system. Correlated time series prediction plays a significant role in many real-world applications to help people make reasonable decisions. Yet it is very challenging, because different from single time series, correlated time series show both intra-sequence temporal dependencies and inter-sequence spatial dependencies. In addition, correlated time series are also affected by external factors in actual scenarios. Although RNNs have been proved to be effective on sequential data modeling, existing related works only focus on sequential patterns in a single time series, failing to comprehensively consider the inter-dependencies among multiple time series, which is essential for correlated time series prediction. In this paper, we propose a novel variant of LSTM, named CTS-LSTM, to collectively forecast correlated time series. Specifically, spatial and temporal correlations are explicitly modeled and respectively maintained in cells to capture the complex non-linear patterns in correlated time series. A general interface for handling external factors is further designed to enhance forecasting performance of the model. Experiments are conducted on two types of real-world datasets, viz., civil aviation passenger demand data and air quality data. And our CTS-LSTM achieves at least 9.0%, 16.5% and 21.3% lower RMSE, MAE and MAPE compared to the state-of-the-art baselines. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105239	10.1016/j.knosys.2019.105239													
J								Optimizing Deep Belief Echo State Network with a Sensitivity Analysis Input Scaling Auto-Encoder algorithm	KNOWLEDGE-BASED SYSTEMS										Echo State Network; Reservoir computing; Recurrent neural network; Sensitivity Analysis Input Scaling; Auto-Encoder; Time-series prediction	CHAOTIC SYSTEMS; CLASSIFICATION	Echo State Network (ESN) is a specific class of recurrent neural networks, which displays very rich dynamics owing to its reservoir based hidden neurons. ESN has been viewed as a powerful approach to model real-valued time series processes. In order to integrate with deep learning theory, Deep Belief Echo State Network (DBESN) is employed to address the slow convergence in Deep Belief Network (DBN). In DBESN, the DBN part is employed for feature learning in an unsupervised fashion and the ESN part is utilized as a regression layer of DBN. However, the ESN input layer is still not working in an unsupervised status in DBESN. Moreover, ESN's input dimension increases dramatically because of the DBN layer in DBESN. Namely, the DBN layer in DBESN makes the ESN more difficult to construct the input scaling parameters. For purpose of constructing an optimal input weights matrix and input scaling parameters in the ESN layer of DBESN, a novel Sensitivity Analysis Input Scaling Auto-Encoder (SAIS-AE) algorithm is employed in this paper through an unsupervised pre-training process. Initially, the output weights matrix of ESN layer is pre-trained by total input data set. Then, the pretrained output weights matrix is injected into the input weights matrix of the ESN layer to ensure the specificity of AE. Finally, the input scaling parameters of ESN layer are tuned based on a sensitivity analysis algorithm. Two multivariable sequence tasks and one univariate sequence benchmark are applied to demonstrate the advantage and superiority of SAIS-AE. Extensive experimental results show that our SAIS-AE-DBESN model can effectively improve the performance of DBESN. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105257	10.1016/j.knosys.2019.105257													
J								Geometric Knowledge Embedding for unsupervised domain adaptation	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Graph-based model; Geometric knowledge; Graph convolutional network; Maximum Mean Discrepancy	KERNEL	Domain adaptation aims to transfer auxiliary knowledge from a source domain to enhance the learning performance on a target domain. Recent studies have suggested that deep networks are able to achieve promising results for domain adaptation problems. However, deep neural networks cannot reveal the underlying geometric information from input data. Indeed, such geometric information is very useful for describing the relationship between the samples from source and target domains. In this paper, we propose a novel learning algorithm named GKE, which stands for Geometric Knowledge Embedding. In GKE, we use a graph-based model to explore the underlying geometric structure of the input source and target data based on their similarities. Concretely, we develop a graph convolutional network to learn discriminative representations based on the constructed graph. To obtain effective transferable representations, we match source and target domains by reducing the Maximum Mean Discrepancy (MMD) between their learned representations. Extensive experiments on real-world data sets demonstrate that the proposed method outperforms existing domain adaption methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105155	10.1016/j.knosys.2019.105155													
J								Consensus analysis for AHP multiplicative preference relations based on consistency control: A heuristic approach	KNOWLEDGE-BASED SYSTEMS										Group decision making; Multiplicative preference relation; Consistency; Consensus; Simulation analysis	GROUP DECISION-MAKING; ANALYTIC HIERARCHY PROCESS; COMPARISON MATRIX; MODEL; NETWORKS; RANKING; SYSTEM	Consistency and consensus play different, but essential roles when solving group decision making problems using the analytical hierarchy process (AHP), in which multiplicative preference relations are used to represent individual pairwise comparison preferences. The existing AHP frameworks developed to manage individual consistency and group consensus have been widely investigated. However, in these frameworks, the consistency is often destroyed in the consensus reaching process, which means that the revised decision makers' judgments have little relationship to Saaty's original 1/9-9 evaluation scale. This paper outlines a new approach to aid the consensus decision making process, for which two heuristic algorithms are developed. The first algorithm is designed to assist the decision maker in achieving a predefined consistency level, and the second is designed to achieve consensus while controlling the individual consistency level. Several classical numerical examples are compared to validate the effectiveness of the proposed approach. Finally, some simulations are conducted to further demonstrate the feasibility of the proposed approach for general cases. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105317	10.1016/j.knosys.2019.105317													
J								Accelerated hyperbolic smoothing method for solving the multisource Fermat-Weber and k-Median problems	KNOWLEDGE-BASED SYSTEMS										Multisource Fermat-Weber; Min-sum-distances clustering; k-Median; Facility location; Smoothing	LOCATION	This article deals with the Multisource Fermat-Weber and continuous k-Median problems. The first problem is the continuous location-allocation problem, defined in a planar region, an important problem in facility location subject. The continuous k-Median problem, defined in a multidimensional space, is also known as the minimum sum-of-distances clustering problem. Their mathematical modellings lead to a min-sum-min formulation which is a global optimization problem with a bilevel nature, nondifferentiable and with many minimizers. To overcome these severe difficulties, the Hyperbolic Smoothing methodology is proposed, in connection with a partition of locations in two groups: location in the frontier and location in gravitational regions, which drastically simplify the computational tasks. For the purpose of illustrating both the reliability and the efficiency of the method, we perform a set of computational experiments making use of the traditional instances described in the literature. Apart from consistently presenting similar or even better results when compared to related approaches, the novel technique was able to deal with instances never tackled before, with up to 1243088 cities. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105226	10.1016/j.knosys.2019.105226													
J								Multiple indefinite kernel learning for feature selection	KNOWLEDGE-BASED SYSTEMS										Feature selection; Multiple indefinite kernel learning; Indefinite kernel; DC programming	DIMENSIONAL FEATURE-SELECTION	Multiple kernel learning for feature selection (MKL-FS) utilizes kernels to explore complex properties of features and performs better in embedded methods. However, the kernels in MKL-FS are generally limited to be positive definite. In fact, indefinite kernels often emerge in actual applications and can achieve better empirical performance. But due to the non-convexity of indefinite kernels, existing MKL-FS methods are usually inapplicable and the corresponding research is also relatively little. In this paper, we propose a novel multiple indefinite kernel feature selection method (MIK-FS) based on the primal framework of indefinite kernel support vector machine (IKSVM), which applies an indefinite base kernel for each feature and then exerts an l(1)-norm constraint on kernel combination coefficients to select features automatically. A two-stage algorithm is further presented to optimize the coefficients of IKSVM and kernel combination alternately. In the algorithm, we reformulate the non-convex optimization problem of primal IKSVM as a difference of convex functions (DC) programming and transform the non-convex problem into a convex one with the affine minorization approximation. We further utilize a leverage score sampling method to select landmark points for solving large-scale problems. Moreover, we extend MIK-FS to multi-class feature selection scenarios. Experiments on real-world datasets demonstrate that MIK-FS is superior to some related state-of-the-art methods in both feature selection and classification performance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105272	10.1016/j.knosys.2019.105272													
J								On the fusion of multiple multi-criteria aggregation functions with focus on the fusion of OWA aggregations	KNOWLEDGE-BASED SYSTEMS										Fuzzy measure; Multi-criteria; Aggregation; OWA; Choquet integral		We discuss the concepts of a fuzzy measure and the Choquet integral with respect to a fuzzy measure. We introduce aggregation functions and discus their role in fusing fuzzy measures. We describe the formulation of multi-criteria decision-making using fuzzy measures and explain how the measure is used to capture the relationship between the criteria. We look at some examples of multi-criteria decision-making using a measure-based approach. Two notable examples are the OWA aggregation of the criteria and a propositional logic expression of the relationship between criteria. We next look at the fusion of multiple multi-criteria decision functions. The major objective of this paper is to focus on the problem of the fusion of multiple OWA aggregation functions. A significant benefit of the methodology developed here is that it is based solely on the use of fuzzy measures, this allows us to represent in a unified fashion many different aggregation imperatives. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105216	10.1016/j.knosys.2019.105216													
J								Usage profiling from mobile applications: A case study of online activity for Australian primary schools	KNOWLEDGE-BASED SYSTEMS										User online behavior; Mobile applications; Alternating least squares; Smooth Gaussian mixture model		Last decade has witnessed a drastically increasing development of smart devices, while related mobile applications have emerged significantly in people's daily life. As such, understanding the pattern of mobile application usage and related online behavior is of great importance for a variety of purposes, such as application engineering, resource optimization, and marketing. Existing research of online usage discovery includes surveys from end-users, application provider-related analysis, and usage log mining. These works, however, suffer from some limitations, such as lacking of user socio-economics background, insufficient coverage and sample bias, etc. A novel and comprehensive application-usage profiling algorithm, termed as TAG, is proposed in this study to investigate online behavior. The proposed algorithm consists of three major steps: (i) T-step: representing usage data as a Term Frequency-Inverse Document Frequency based matrix; (ii) A-step: applying Alternating Least Squares factorization technique to reduce data sparseness and dimension; and last (iii) G-step: utilizing a smoothed Gaussian Mixture Model for clustering purpose. The performance of the proposed TAG algorithm is evaluated, taking a national dataset generated from 31,280 devices and 30,155 applications over 30 months as an example. Experimental results demonstrate that the proposed algorithm outperforms existing methods via forming accurate usage groups from school-level online behavior. As such, the superior clustering outcome demonstrates the flexibility and applicability of the proposed work for understanding online pattern using complex application usage data. Resultant knowledge can in turn be used to inform decision making and improve application development. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105214	10.1016/j.knosys.2019.105214													
J								Robust regression framework with asymmetrically analogous to correntropy-induced loss	KNOWLEDGE-BASED SYSTEMS										Robustness; Asymmetry least square loss; Expectile; Nonconvexity; Correntropy; Regression; CCCP	SUPPORT VECTOR MACHINE; CLASSIFICATION; QUANTILES	This work proposes a robust loss function based on expectile penalty (named as rescaled expectile loss, RE-loss), which includes and generalizes the existing loss functions. Then some important properties of RE-loss are demonstrated such as asymmetry, nonconvexity, smoothness, boundedness and asymptotic approximation behaviors. From the viewpoints of correntropy, we analyze that the proposed RE-loss can be viewed as a correntropy-induced loss by a reproducing piecewise kernel. Furthermore, a sparse version of RE-loss (called SRE-loss function) is developed to improve sparsity by introducing a epsilon-insensitive zone. Following that, two robust regression frameworks are proposed with the proposed loss functions. However, the non-convexity of the proposed losses makes the problems difficult to optimize. We apply concave-convex procedure (CCCP) and dual theory to solve the problems effectively. The resulting algorithms converge linearly. To validate the proposed methods, we carry out numerical experiments in different scale datasets with different levels of noises and, outliers, respectively. In three databases including artificial database, benchmark database and a practical application database, experimental results demonstrate that the proposed methods achieve better generalization than the traditional regression methods in most cases,especially when noise and outlier distribution are imbalance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105211	10.1016/j.knosys.2019.105211													
J								Deep learning-enabled intelligent process planning for digital twin manufacturing cell	KNOWLEDGE-BASED SYSTEMS										Intelligent process planning; Deep learning; Residual networks; Evaluation twin; Digital twin manufacture cell	FRAMEWORK; REUSE; ONTOLOGY; NETWORK; MODELS	The transition to intelligent manufacturing provides a fulcrum for the revolution of product lifecycle like design, manufacturing and maintenance, so does it for process planning. Specifically, digital twin manufacturing cell (DTMC) is regarded as a new means of and also a basic unit for implementing intelligent manufacturing. Incorporating process planning in DTMC could improve the integrity of DTMC and enhance the feasibility of process planning. Consequently, this paper proposes a deep learning-enabled framework for intelligent process planning towards DTMC. Firstly, a process knowledge reuse network (PKR-Net) that takes deep residual networks as base architecture is embedding into the framework, which could understand design intents expressed in a drawing or a 3D computer-aided design (CAD) model via its views and automatically retrieve relevant knowledge for the quick generation of theorical processes. Then, an evaluation twin is constructed to transform the theorical processes into practical operations and produce an optimal process plan. Finally, a test bed of the framework is constructed and the experimental results demonstrate the feasibility and effectiveness of the approach. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105247	10.1016/j.knosys.2019.105247													
J								Object semantics sentiment correlation analysis enhanced image sentiment classification	KNOWLEDGE-BASED SYSTEMS										Image sentiment classification; Object semantics; Bayesian network; Object semantics sentiment correlation model; Convolutional Neural Network	NETWORK; ALGORITHM	With the development of artificial intelligence and deep learning, image sentiment analysis has become a hotspot in computer vision and attracts more attention. Most of the existing methods focus on identifying the emotions by studying complex models or robust features from the whole image, which neglects the influence of object semantics on image sentiment analysis. In this paper, we propose a novel object semantics sentiment correlation model (OSSCM), which is based on Bayesian network, to guide the image sentiment classification. OSSCM is constructed by exploring the relationships between image emotions and the object semantics combination in the images, which can fully consider the effect of object semantics for image emotions. Then, a convolutional neural networks (CNN) based visual sentiment analysis model is proposed to analyze image sentiment from visual aspect. Finally, three fusion strategies are proposed to realize OSSCM enhanced image sentiment classification. Experiments on public emotion datasets Fl and Flickr_LDL, demonstrate that our proposed image sentiment classification method can achieve good performance on image emotion analysis, and outperform state of the art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105245	10.1016/j.knosys.2019.105245													
J								Gaussian prior based adaptive synthetic sampling with non-linear sample space for imbalanced learning	KNOWLEDGE-BASED SYSTEMS										Imbalanced learning; Error bound model; Adaptive method; Classification algorithm; Gaussian mixture model	LOCALIZED GENERALIZATION ERROR; CLASSIFICATION; IDENTIFICATION; SELECTION; MACHINE; SMOTE	In the presence of skewed category distribution, most learning algorithms fail to provide favorable performance on the representation about data characteristics. Thus learning from imbalanced data is a crucial challenge in the field of data engineering and knowledge discovery. In this work, we proposed an imbalanced learning method to generate minority samples for the compensation of class distribution skews. Different from existing synthetic over-sampling techniques, the data generation is conducted within the hyperplane rather than on the hyperline, thus the proposed method breaks down the ties imposed by the linear interpolation. In addition, this proposed method minimizes the sampling uncertain and risk by integrating a prior knowledge about the minority class instances. Moreover, a multi-objective optimization combined with error bound model develops this proposed method into an adaptive imbalanced learning. Extensive experiments have been performed on imbalanced issues, and the experimental results demonstrate that this method can improve the performance of different classification algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105231	10.1016/j.knosys.2019.105231													
J								Multi-source domain adaptation with joint learning for cross-domain sentiment classification	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Soft parameter sharing; Deep domain confusion; Cross-domain sentiment classification		Cross domain sentiment classification uses knowledge from source domain tasks to enhance the sentiment classification of the target task. It can reduce the workload of data annotations in the new domain, and significantly improve the utilization of labeled resources in the source domains. Available approaches generally use knowledge from a single-source domain and hard parameter sharing methods, which are likely to ignore the differences among domain-specific features. We propose a novel framework with multi-source domain adaptation and joint learning for multi-source cross-domain sentiment classification tasks This framework uses bi-directional gated recurrent units and convolutional neural networks for deep feature extraction and soft parameter sharing for information transfer across tasks. Furthermore, it minimizes distance constraints for deep domain fusion. Multi-source domain adaptation involves multiple concurrent task learning, and the gradients are simultaneously back propagated. We validate the proposed framework on multi-source cross-domain sentiment classification datasets in Chinese and English. The experimental results demonstrate that the proposed method is more effective than state-of-the-art methods in improving accuracy and generalization capability. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105254	10.1016/j.knosys.2019.105254													
J								A multi-task transfer learning method with dictionary learning	KNOWLEDGE-BASED SYSTEMS										Transfer learning; Dictionary learning; Support vector machine	REGULARIZATION; CATEGORIZATION; FRAMEWORK	Transfer learning is a problem that samples are generated from more than one domains, which focuses on transferring knowledge from source tasks to target tasks. A variety of methodologies are proposed for transfer learning. And a number of them concentrate on the inner relationship among each domain while some pay more attention to knowledge transfer. In this paper, based on the hinge loss and SVM, a new dictionary learning with multi-task transfer learning method(DMTTL) is proposed. The dictionary learning method is utilized to learn sparse representation of the given samples. Moreover, a regularization term for two dictionaries are exploited so that the similarity of samples can be well determined. Besides, a new optimization method based on alternate convex search is proposed with convergence analysis, which indicates that the DMTTL is a reasonable approach. After that, the comparison of DMTTL with the state-of-the-art approaches manifests the feasibility and the competitive performance for multi-task classification problem. And the statistic results show that the proposed method outperforms the previous methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 5	2020	191								105233	10.1016/j.knosys.2019.105233													
J								Knowledge mapping of supply chain risk research based on CiteSpace	COMPUTATIONAL INTELLIGENCE										CiteSpace; knowledge mapping analysis; supply chain risk	FUZZY TOPSIS; MANAGEMENT; SUSTAINABILITY; RESILIENCE; COCITATION; OPTIMIZATION; FLEXIBILITY; TRENDS; ISSUES; LEVEL	Supply chain risk is a relatively new research field, and research results have been published in journals, and in master and doctoral articles. In this article, we first performed a quantitative analysis of existing literature on supply chain risk indexed in Web of Science (WOS). Then, we analyzed the author, institution, keywords, research hotspot, and cocited literature of existing publications in the research field of supply chain risk by using CiteSpace. The aim of this study is to investigate the trend, geographical distribution, author distribution, research field, keywords, keyword clustering, and other features of scientific articles on supply chain risk published in global journals, using the most advanced bibliometric analysis tools. The results of this study can be used to determine the most influential research institutions and authors in the research field of supply chain risk, as well as the research hotspots in different periods, and different research directions in this field.																	0824-7935	1467-8640															10.1111/coin.12306		MAR 2020											
J								NoHR: An Overview Reasoning with Ontologies and Nonmonotonic Rules	KUNSTLICHE INTELLIGENZ										Query answering; Description logic ontologies; Rule-based languages	MULTI-CONTEXT SYSTEMS; OWL; QUERIES	Description logic ontologies, such as ontologies written in OWL, and non-monotonic rules, as known in Logic Programming, are two major approaches in Knowledge Representation and Reasoning. Even though their integration is challenging due to their inherent differences, the need to combine their distinctive features stems from real world applications. In this paper, we give an overview of NoHR, a reasoner designed to answer queries over theories composed of an OWL ontology in a Description logic and a set of non-monotonic rules. NoHR has been developed as a plug-in for the widely used ontology editor Protege, building on a combination of reasoners dedicated to OWL and rules, but it is also available as a library, allowing for its integration within other environments and applications. It comes with support for all polynomial OWL profiles and the integration of their constructors as well as for standard built-in Prolog predicates, and allows the direct consultation of databases during query evaluation and the usage of sophisticated mechanisms, such as tabling already computed results, all of which enhances the applicability and the efficiency of query answering.																	0933-1875	1610-1987															10.1007/s13218-020-00650-1		MAR 2020											
J								Hybrid GSW and DM based fully homomorphic encryption scheme for handling false data injection attacks under privacy preserving data aggregation in fog computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fog computing; False data injection attacks; Fully homomorphic encryption; Privacy preserving data aggregation; Internet of things (IoT)	EFFICIENT	In recent years, the area of fog computing is receiving maximum focus due to the potential improvements in the cloud computing field. Fog computing is capable of resolving issues that includes location awareness, inadequate mobility support and high latency in the cloud computing environment. The internet of things (IoT) comprises of a collection of IoT equipments connected to fog nodes in order to aid the cloud service center for storing and processed a portion of data in prior. This process of storing and processing greatly minimizes the pressure in data processing with enhanced service and real time quality. However, data modification attacks and false data injection attacks introduce significant challenges over the fog nodes during the event of data processing. In this paper, Hybrid GSW (Gentry, Sahai, and Waters) and DM (Ducas and Micciancio) based fully homomorphic encryption (HGSW-DM-FHE) scheme was proposed for handling false data injection attacks under privacy preserving data aggregation in fog computing. This proposed HGSW-DM-FHE scheme is highly fault tolerant, since the process of data aggregation from other devices is not influenced even when the fog devices fails in operation. In addition, the proposed HGSW-DM-FHE scheme is determined to be efficient and secure by preventing the data injected from legal IoT devices are unaltered and protected immaterial to the honesty and maliciousness of the cloud control center and fog nodes.																	1868-5137	1868-5145															10.1007/s12652-020-01849-8		MAR 2020											
J								Detection of data integrity attacks by constructing an effective intrusion detection system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Integrity; Intrusion detection; Network security; Sampling; Optimization	SUPPORT VECTOR MACHINE; MODIFIED K-MEANS; ALGORITHM	Network users are heavily targeted by data integrity attacks that affect the development of new security techniques. The main challenge in network security is to identify this kind of attack for the improvement of growing mechanisms. In this paper, a data integrity based effective intrusion detection system (DI-EIDS) is constructed to prevent the network with a high detection rate and low false alarm rates. It is classified into two phases; data sampling and selection of features. In the data sampling process, attacks are detected and inference based on the sample signatures. In this process, the Deviation forest (d-forest) is used to remove barriers; Grey Wolf Optimization (GWO) is used for sampling ratio optimization and Black forest (BF) classifier to obtain the best training data. To select the best features, GWO and BF are repeatedly used. Finally, DI-EIDS based on the black forest is constructed using the best training data set obtained by data sampling and feature selection. Rare Integrity attacks are detected in this technique when compared with other algorithms. Experimental results are analyzed using different datasets with a 22% sampling rate. The performance results show a higher rate of detection with low false-positive rates.																	1868-5137	1868-5145															10.1007/s12652-020-01850-1		MAR 2020											
J								The application of neural network with convolution algorithm in Western music recommendation practice	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Music classification; Convolutional neural network; Deep learning; Music recommendation		With the rapid development of digital music, the number of Western music works is continuously increasing, which makes users find it difficult to spot their favorite music works consequently quickly. Therefore, the music recommendation algorithm is applied to recommend music works in a targeted manner based on prior user behaviors, which could reduce the fatigue of the users and improve the overall user experiences. The convolutional neural network (CNN) is applied to classify the commonly-seen types of Western music, including classical music, pop music, jazz music, and Hip-Hop and Rap music. Afterward, CNN is trained to explore the two activation functions and the two gradient descent methods, which are compared and analyzed in terms of their features and performances during the training. Then, the classification methods, which are based on the spectrum and the comprehensive feature frequency of spectrum and musical notes, respectively, are compared. Research results have shown that the accuracy rate of spectrum-based classification method is 96.5%, while that of the classification method based on the comprehensive feature frequency of spectrum and musical notes increases by 2%. Thus, the proposed music classification algorithm is significant to the extraction of music high-level semantic features, as well as the promotion of deep learning method in the field of musical signal analysis.																	1868-5137	1868-5145															10.1007/s12652-020-01806-5		MAR 2020											
J								Maintaining ethical resolution in distributed constraint reasoning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed constraint satisfaction problem DisCSP; Multi-agent system; Ethics		Multi-agent systems (MAS) consist of autonomous agents that parcel out different tasks and make decisions in dynamic environments. Distributed constraint satisfaction problem (DisCSP) is the most effective and applicative MAS framework. In DisCSP, each agent is connected to the other agents via constraints and holds its own local constrained problem. Those agents find solutions satisfying their own constraints and the linking ones too by collaborating and exchanging messages holding their instantiations. This formalism does not take into consideration the possibility of the presence of unethical agents which can make irrelevant or even dangerous decisions, especially when human agents are involved in the resolution. In this paper, we propose an extension of the DisCSP into an ethical formalism "E-DisCSP". It allows to control agents, detect intrusions and apply the convenient actions when an unethical agent is picked up. All these functionalities are done via control framework, in order to maintain the DisCSP resolution as normal as possible. Experimental results show the efficiency of our contribution. The detection rate of unethical agents achieves up to 100%. And the convenient actions' application allows, to go from 45 to 0% of wrong solutions.																	1868-5137	1868-5145															10.1007/s12652-020-01812-7		MAR 2020											
J								A reliable multi-hop opportunistic routing scheme with bandwidth guarantee for multimedia wireless mesh networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Opportunistic routing; Expected available bandwidth; Malicious node detection; Forwarder candidate selection; Wireless mesh networks	NODES	Opportunistic routing (OR) is a novel challenging idea that has been designed for the wireless mesh network (WMN). This routing strategy captured the attention of numerous researchers due to its intense performance encountered in WMN. Broadcast characteristics of OR lead to performance enhancement in WMN. It possesses the ability to overhear packet transmission and accomplishing coordination between the relay nodes. In this paper, a reliable opportunistic routing scheme with bandwidth guarantee for multimedia wireless mesh networks (RORBG) scheme for multimedia WMN is proposed. This strategy is capable of recognizing various attacks like a black hole, grey hole, and denial of service attacks proficiently. The important pertains to treat the overhead and the latency of transmitting the data packet. In this scheme, we establish a node honesty rate to distinguish malicious behaviour and give assurance bandwidth. The expected available bandwidth (EAB) is computed by the local available bandwidth. Moreover, the forwarder candidate node is selected based on the bandwidth as well as the communication quality factor. This scheme grants to attain forwarder candidate selection procedure and coordination methods are a helping to improve the WMN execution along with efficient packet delivery. The application of RORBG is army and traffic management. The simulation results illustrate that RORBG scheme increases the 12.17% throughput and 16.81% delay minimization compare to the baseline routing protocols in WMN.																	1868-5137	1868-5145															10.1007/s12652-020-01838-x		MAR 2020											
J								An energy-efficient prediction model for data aggregation in sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data aggregation; Data prediction; Energy efficiency; Prediction accuracy wireless sensor network		Most environmental monitoring application periodically senses and aggregated data by sensor networks which usually exhibits high temporal redundancies. An enormous amount of energy is depleted in transmitting this redundant information making it extremely difficult to achieve an acceptable network lifespan, which has become a bottleneck in scaling such applications. To efficiently manage the energy depletion in concurrent data collection rounds, a prediction model based on Extended Cosine Regression (ECR) for Data Aggregation is proposed. The proposed technique delivers prediction with high accuracy and the energy consumption is minimized with successful predictions and thereby increases the data cycles and network lifetime. ECR also uses a two-vector model in the intra-cluster transmissions to synchronize the predicted data values and therefore minimizes cumulative errors from continuous predictions. The proposed ECR technique is simulated using NS2-34 shows high-energy efficiency as compared with the existing schemes. Results demonstrate high prediction accuracy, a number of successful predictions and a lesser degree of prediction errors, which obviously improve the network's lifetime.																	1868-5137	1868-5145															10.1007/s12652-020-01833-2		MAR 2020											
J								IoT composition based on self-controlled services	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Quality of service; Self-control; Service composition	SMART	The Internet of Things (IoT) includes a large diversity of devices as well as embedded sensors or actuators. The frontier between the physical and digital worlds is becoming more and more blurred. Applications are now being constructed as micro-service compositions integrating more and more functionalities. Services are at the heart of architecture. We propose a service composition entity called self-controlled service component (SCC) for IoT and show, thanks to it, that we control the QoS of a whole IoT application. We control the QoS of each micro-service and the whole composition. We have described our proposals through human-machine interaction which is at the heart of IoT applications. Human-machine interaction will indeed play a more important role in the future IoT. As the number of objects increases, human-machine interaction with the IoT becomes more and more complex and should be controlled, especially in critical domains such as automotive, aerospace, or health. Modelling such controlled interactions is particularly challenging. Human-machine interfaces will have a crucial role to play in the IoT when human decision-making is necessary, especially in critical and urgent situations. The interaction quality of service must be controlled. We have applied our approach through human-machine interaction in the following way: we show how IoT human-machine interaction can be decomposed into elementary self-controlled micro-services and show, thanks to them, that we control the quality of service rendered for the interaction. Furthermore, the self-controlling mechanisms integrated in the SCCs introduce the necessary automation for dynamic reactions. The objective of this new concept is to control the quality of service for the whole of an IoT composite application.																	1868-5137	1868-5145															10.1007/s12652-020-01831-4		MAR 2020											
J								Human behaviour in multimodal interaction: main effects of civic action and interpersonal and problem-solving skills	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multimodal interaction; Human behaviour; Skill training; Metacognitive; Individual; and community-level related attitudes and skills	SELF-EFFICACY; PEDAGOGICAL AGENTS; MOTIVATION; IMPACT; PROCRASTINATION; INVOLVEMENT; PERSPECTIVE; READINESS; COLLEGE; GOALS	Metacognitive skill training may rest within any kind of social interaction that requires awareness of what an individual and others think, in social, educational and organizational settings alike. This work is an extensive study of multimodal application interaction (virtual agent, spoken dialogue, visual communication of progress) for metacognitive skill training via negotiation skill training scenarios. Human behaviour, as effected by civic action and interpersonal and problem-solving skill training, is investigated through interaction sessions with a virtual agent on multimodal multiparty negotiation. This work reports on the results of the user-system evaluation sessions involving 41 participants before and after interaction with the system, integrating macro- (dialogue system performance) and micro- (metacognitive-related and individual- and community-level-related attitudes and skills) factors. Findings indicate significant and positive relationships between user and system evaluation questions after interaction with the dialogue system and between self-efficacy, self-regulation, individual readiness to change, mastery goal orientation, interpersonal and problem-solving skills and civic action before and after the interaction experience. Implications, limitations and further research issues are discussed in light of context of the multimodal interaction and its effects on the human behaviour during metacognitive skill training.																	1868-5137	1868-5145															10.1007/s12652-020-01846-x		MAR 2020											
J								A study on Arabic sign language recognition for differently abled using advanced machine learning classifiers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CNN; RNN; MLP; LDA; HMM; ANN; SVM; KNN and SLR system	FRAMEWORK; ALPHABET; FEATURES; SYSTEM	This study is proposed to review the sign language recognition system based on different classifier techniques. Mostly the Neural Network and Deep Learning-based classifiers were utilized to recognize the different sign languages and this survey is proposed to review the best classifier model to represent sign language recognition (SLR). We focused mainly on deep learning techniques and also on Arabic sign language recognition systems. Numerous classifiers like CNN, RNN, MLP, LDA, HMM, ANN, SVM, KNN and more were implemented to the SLR system. Each classifier is reviewed with the recognition accuracy, in which the deep learning-based classifiers executed the optimal recognition result as contrasted to the other types of classifiers.																	1868-5137	1868-5145															10.1007/s12652-020-01790-w		MAR 2020											
J								Memristor based high speed and low power consumption memory design using deep search method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep search pattern; Power; Memristor; Flip-flop; Analog mixed signal	HYBRID ARCHITECTURE; SRAM	The demand for low-power devices in today's world is increasing, and the reason behind this is scaling CMOS technology. Due to scaling, the size of the chip decreases and the number of transistors in System-On-Chip increases. However, transistor miniaturization also introduces many new challenges in circuit design for very large scale integrated circuits. Therefore in this work is introduced a memristor based memory design, the memristor breaks the scaling limitations of CMOS technology and prevails over emerging semiconductor devices. The memristor is forced to the Nano scale design of the invention, and successful fabrication begins to take into account the range of conventional metal oxide semiconductor field effect transistors or more specifically the use of transistors as a whole. The memristor has a history mechanism that allows the memory operation to be combined with the inherent bipolar resistance switching characteristics. Different types of existing mathematical models have been derived from shapes that can be further implemented and tested in a prototype with some crucial parameters thus determined and how they differ from conventional transistor-based designs for a suitable circuit memristor. In this work, low-power complementary metal oxide semiconductor (CMOS) flip-flops have been proposed with deep search pattern method and some key parameters such as delay, power, gate count and other memristor calculations are carried out. The simulations are carried out using Verilog-analog mixed signal. The proposed system is considered as potential devices for building memories because they are very dense, non-volatile scalable devices with faster switching times and low power dissipation and are also compatible with the existing CMOS-technology.																	1868-5137	1868-5145															10.1007/s12652-020-01817-2		MAR 2020											
J								Feature selection and classification methods for vehicle tracking and detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vehicle detection; Occlusions; Feature selection; Classification; Support vector machine; Convolutional neural network		Vehicle detection and tracking plays a major role in military and civilian applications. The accurate detection of multiple vehicles in a complicated traffic environment is too difficult. This process is made more difficult, if there are occlusions between vehicles. The optimal feature selection of a vehicle is also a challenging task. These issues are focused in the existing work namely convolution neural network (CNN) based vehicle detection. However in this work, feature selection is not concentrated and with the lesser feature information classification accuracy is reduced considerably. This is focused and resolved in the proposed research by introducing the method namely Enhanced Convolution neural network with Support Vector Machine (ECNN-SVM) based vehicle detection. In this work initially feature extraction is done. Haar like features are used for this purpose. This has compact representation, capture information from multiple scales, encoded edge and structural information and can be efficient computation. Haar like feature pool is large scale in nature. Enhanced bat optimization is used to select features from this pool. This selection is done by combining sample's class label and feature value. Confidence level between two thresholds of tracking rectangles is used for classification. Support vector machine (SVM) classifier is combined with local binary pattern to perform this classification. The interference areas between moving objects and vehicles are removed by Enhanced Convolutional Neural Network (ECNN) classifier. The efficiency of the proposed method is computed by using experimental and theoretical analysis.																	1868-5137	1868-5145															10.1007/s12652-020-01824-3		MAR 2020											
J								Multivariate uncertain regression model with imprecise observations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multivariate uncertain regression; Uncertainty theory; Parameter estimation; Residual; Confidence interval		The multivariate regression model is a mathematical tool for estimating the relationships among some explanatory variables and some response variables. In some cases, observed data are imprecise. In order to model those imprecise data, we can employ uncertainty theory to design the uncertain regression model by regarding those data as uncertain variables. Parameters estimation is an important topic in the uncertain regression model. In this paper, we explore a method of parameters estimation by the principle of least squares in the multivariate uncertain regression model containing more than one response variables and assuming both explanatory variables and response variables as uncertain variables. Besides, when the new explanatory variables are given, we propose an approach to obtain the forecast value and the confidence interval of the response variables. At last, a numerical example of the multivariate uncertain regression model is showed.																	1868-5137	1868-5145															10.1007/s12652-020-01763-z		MAR 2020											
J								Cost-sensitive hierarchical classification for imbalance classes	APPLIED INTELLIGENCE										Imbalance class; Misclassification cost; Cost-sensitive; Hierarchical classification	SUPPORT VECTOR MACHINES; FEATURE-SELECTION; MULTICLASS	The hierarchical classification with an imbalance class problem is a challenge for in machine learning, and is caused by data with an uneven distribution. Learning from an imbalanced dataset can lead to performance degradation of the classifier. Cost-sensitive learning is a useful solution for handling the gap probability of majority and minority classes. This paper proposes a cost-sensitive hierarchical classification for imbalance classes (CSHCIC), constructing a cost-sensitive factor to balance the relationship between majority and minority classes. First, we divide a large hierarchical classification task into several small subclassification tasks by class hierarchy. Second, we establish a cost-sensitive factor by more precisely using the number of different samples of subclassifications. Then, we calculate the probability of every node using logistic regression. Lastly, we update the cost-sensitive factor using the flexibility factor and the number of samples. The experimental results show that the cost-sensitive hierarchical classification method achieves excellent performance on handling imbalance class datasets. The running time cost of the proposed method is smaller than most state-of-the-art methods.																	0924-669X	1573-7497				AUG	2020	50	8					2328	2338		10.1007/s10489-019-01624-z		MAR 2020											
J								Nonlinear system modeling and application based on restricted Boltzmann machine and improved BP neural network	APPLIED INTELLIGENCE										BP neural network; Sensitivity analysis; Mutual information; Restricted boltzmann machine; Nonlinear system modeling	DEEP BELIEF NETWORK; PREDICTION; DESIGN	Aiming at the complexity, nonlinearity and difficulty in modeling of nonlinear system. In this paper, an improved back-propagation(BP) neural network based on restricted boltzmann machine(RBM-IBPNN) is proposed for nonlinear systems modeling. First, the structure of BP neural network(BPNN) is optimized by using sensitivity analysis(SA) and mutual information(MI) of the hidden neurons. Namely when the SA value and the MI value of the hidden neurons satisfy the set standard, the corresponding neurons will be pruned, split or merged. second, the restricted boltzmann machine(RBM) is employed to perform parameters initialization of training on the IBPNN. Finally, the proposed RBM-IBPNN is evaluated on nonlinear system identification, lorenz chaotic time series prediction and the total phosphorus prediction problems. The experimental results demonstrate that the proposed RBM-IBPNN not only has faster convergence speed and higher prediction accuracy, but also realizes a more compact network structure.																	0924-669X	1573-7497															10.1007/s10489-019-01614-1		MAR 2020											
J								Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in CNNs	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep learning; Sparse convolutions; Voxel grids		Convolutional neural networks (CNNs) are a powerful tool for pattern recognition and computer vision, but they do not scale well to higher-dimensional inputs, because of the associated memory demands for storing and manipulating high-dimensional tensors. This work starts from the observation that higher-dimensional data, like for example 3D voxel volumes, are sparsely populated. CNNs naturally lend themselves to densely sampled data, and sophisticated, massively parallel implementations are available. On the contrary, existing frameworks by and large lack the ability to efficiently process sparse data. Here, we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights of a CNN, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework, when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution, as well as sparse implementations of the ReLU and max-pooling layers; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of back-propagation that makes it possible to combine our approach with standard learning frameworks, while still benefitting from sparsity in the data as well as the model.																	0920-5691	1573-1405				APR	2020	128	4			SI		1047	1059		10.1007/s11263-020-01302-5		MAR 2020											
J								Deep Image Prior	INTERNATIONAL JOURNAL OF COMPUTER VISION										Convolutional networks; Generative deep networks; Inverse problems; Image restoration; Image superresolution; Image denoising; Natural image prior	SCALE-SPACE METHODS	Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity (Code and supplementary material are available at ).																	0920-5691	1573-1405				JUL	2020	128	7					1867	1888		10.1007/s11263-020-01303-4		MAR 2020											
J								MAP Inference Via l(2)-Sphere Linear Program Reformulation	INTERNATIONAL JOURNAL OF COMPUTER VISION										MAP inference; Continuous reformulation; Non-convex optimization	DUAL DECOMPOSITION; OPTIMIZATION; CONVERGENCE	Maximum a posteriori (MAP) inference is an important task for graphical models. Due to complex dependencies among variables in realistic models, finding an exact solution for MAP inference is often intractable. Thus, many approximation methods have been developed, among which the linear programming (LP) relaxation based methods show promising performance. However, one major drawback of LP relaxation is that it is possible to give fractional solutions. Instead of presenting a tighter relaxation, in this work we propose a continuous but equivalent reformulation of the original MAP inference problem, called LS-LP. We add the 2-sphere constraint onto the original LP relaxation, leading to an intersected space with the local marginal polytope that is equivalent to the space of all valid integer label configurations. Thus, LS-LP is equivalent to the original MAP inference problem. We propose a perturbed alternating direction method of multipliers (ADMM) algorithm to optimize the LS-LP problem, by adding a sufficiently small perturbation onto the objective function and constraints. We prove that the perturbed ADMM algorithm globally converges to the -Karush-Kuhn-Tucker ( -KKT) point of the LS-LP problem. The convergence rate will also be analyzed. Experiments on several benchmark datasets from Probabilistic Inference Challenge (PIC 2011) and OpenGM 2 show competitive performance of our proposed method against state-of-the-art MAP inference methods.																	0920-5691	1573-1405				JUL	2020	128	7					1913	1936		10.1007/s11263-020-01313-2		MAR 2020											
J								Discriminator Feature-Based Inference by Recycling the Discriminator of GANs	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial networks; Inference mapping; Conditional image generation; Quality metric for inference mapping; Spatial semantic manipulation		Generative adversarial networks (GANs) successfully generate high quality data by learning a mapping from a latent vector to the data. Various studies assert that the latent space of a GAN is semantically meaningful and can be utilized for advanced data analysis and manipulation. To analyze the real data in the latent space of a GAN, it is necessary to build an inference mapping from the data to the latent vector. This paper proposes an effective algorithm to accurately infer the latent vector by utilizing GAN discriminator features. Our primary goal is to increase inference mapping accuracy with minimal training overhead. Furthermore, using the proposed algorithm, we suggest a conditional image generation algorithm, namely a spatially conditioned GAN. Extensive evaluations confirmed that the proposed inference algorithm achieved more semantically accurate inference mapping than existing methods and can be successfully applied to advanced conditional image generation tasks.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2436	2458		10.1007/s11263-020-01311-4		MAR 2020											
J								A Memetic Algorithm Based on Breakout Local Search for the Generalized Traveling Salesman Problem	APPLIED ARTIFICIAL INTELLIGENCE											TRANSFORMATION	The Traveling Salesman Problem (TSP) is one of the most popular Combinatorial Optimization Problem. It is well solicited for the large variety of applications that it can solve, but also for its difficulty to find optimal solutions. One of the variants of the TSP is the Generalized TSP (GTSP), where the TSP is considered as a special case which makes the GTSP harder to solve. We propose in this paper a new memetic algorithm based on the well-known Breakout Local Search (BLS) metaheuristic to provide good solutions for GTSP instances. Our approach is competitive compared to other recent memetic algorithms proposed for the GTSP and gives at the same time some improvements to BLS to reduce its runtime.																	0883-9514	1087-6545				JUN 6	2020	34	7					537	549		10.1080/08839514.2020.1730629		MAR 2020											
J								Reimagining life (forms) with generative and bio art	AI & SOCIETY										Bio art; Generative art; Computational art; Biogenerativity; Artificial life	BIOLOGY	Artists and designers working in the fields of generative and bio art frequently focus on designing speculative visions of how nature can be reimagined with the use of computational media and synthetic biology. Centered on the unique artistic strategies of reimagining life forms, this paper analyzes and compares a selection of generative software-based projects, in which artists are mimicking different natural phenomena and have the tendency to beautify nature and life, with bio art projects, where ethical considerations are prioritized over other aspects of the work. Experimenting with software and producing code-based generative art can be a relatively accessible creative endeavor while working with synthetic biology requires more demanding and controlled workplace conditions and specialized equipment. Despite the differences in accessibility and usability, both fields of generative and bio art challenge the boundaries of contemporary artistic practice and research by mapping, provoking and contemplating the dramatic changes of life forms, in the era when living matter can be re-engineered into a new material capable of computing.																	0951-5666	1435-5655															10.1007/s00146-020-00937-9		MAR 2020											
J								Material hermeneutic of digital technologies in the age of AI	AI & SOCIETY										Postphenomenology; Embodiment relations; Hermeneutic relations; Cyborg intentionality; Artificial intelligence	CONSTRUCTION	Digital technologies are frequently considered as lacking material aspects. Today, it is evident that behind digital technologies lies a huge and complex material infrastructure in the form of fiber optic cables, servers, satellites, and screens. Postphenomenology has theorized the relations to material things as embodiment relations. Taking into account that technologies can also have hermeneutic aspects, this theory defines hermeneutic relations as those in which we read the world through technologies. The article opens with a review of some theoretical developments to hermeneutic relations with a special focus on digital technologies. The article suggests that in the digital world, material hermeneutics needs to be updated as it shifts from a scientific to an everyday technological context. Now, technologies not only "give voice" to things, they also produce new meanings to informational structures and direct users to certain meanings. When it comes to digital technologies, especially those involving artificial intelligence (AI), the technology actively mediates the world. In postphenomenological terms, it possesses a technological intentionality. The postphenomenological formula should be updated to reflect this type of technological intentionality, by reversing the arrow of intentionality so that it points to the user, rather than from the user.																	0951-5666	1435-5655															10.1007/s00146-020-00952-w		MAR 2020											
J								Real-time micro-expression recognition based on ResNet and atrous convolutions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Micro-expression recognition; Atrous convolutions; Residual network; Expression recognition; CNN		Based on the depth feature of facial micro-expression recognition, such as by using Convolutional Neural Network, the method of classifying facial micro-expression recognition has been gradually improved. Compared with the traditional feature extraction method, the improved technique can more easily realize real-time application. To perfect the details and extract the fine features of micro-expressions, a new algorithm named MACNN (Multi-scale convolutional neural network model), which combines the kernels of atrous convolutions and automatic face correction, is proposed to improve the feature extraction process of the CNN network. Residual blocks are introduced to solve the gradient disappearance problem and to accelerate convergence during the model training. The model is trained and tested on the micro-expression public data sets CASME and CASME II through real-time detection in real-time application of automatic face correction. The robustness of the model is improved by comparing the loss function schemes. The accuracy of the method is 73%, and the real-time detection occurs at a frame rate of 60 FPS. This method can effectively improve the accuracy of micro-expression detection, meet the real-time requirements, and demonstrate satisfactory robustness and generalizability.																	1868-5137	1868-5145															10.1007/s12652-020-01779-5		MAR 2020											
J								Detection and classification of landmines using machine learning applied to metal detector data	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Landmine localisation; metal detector; machine learning; classification		The current landmine clearance methods mostly rely on the manual use of metal detectors (MDs) and on the deminer's experience in differentiating between the sounds emitted due to the presence of a landmine or of harmless clutter. This process suffers from high false-alarm rates, which renders the demining effort slow and costly. In this paper, we report our attempts in using machine learning for decision making in the demining process. We have created our own database of the MD responses corresponding to landmines and/or clutter. A robotic rail is designed and assembled to accurately measure these responses and build the database. Several machine learning models are then developed using the database with the aim of detecting the presence of landmines and classifying them. It is shown that the classification algorithms lead to accurately discriminating the landmines and distinguishing between different buried objects including mines or other items based on the metal detector delivered data or signature.																	0952-813X	1362-3079															10.1080/0952813X.2020.1735529		MAR 2020											
J								DAMER: a novel diagnosis aggregation method with evidential reasoning rule for bearing fault diagnosis	JOURNAL OF INTELLIGENT MANUFACTURING										Bearing fault diagnosis; Condition based monitoring; Ensemble learning; Structured sparsity learning; Evidential reasoning rule	EMPIRICAL MODE DECOMPOSITION; SUPPORT VECTOR MACHINE; ROTATING MACHINERY; FEATURE-EXTRACTION; SELECTION; ALGORITHM	Ensemble learning method has shown its superiority in bearing fault diagnosis based on the condition based monitoring. Nevertheless, features extracted from the monitoring signals of bearing systems often contain interrelated and redundant components, leading to poor performances of the base classifiers in the ensemble. Moreover, the current ensemble methods rely on voting strategies to aggregate the diagnostic predictions of these base classifiers without considering their reliabilities and weights simultaneously. To address the aforementioned issues, we propose a novel Diagnosis Aggregation Method with Evidential Reasoning rule, i.e., DAMER, for bearing fault diagnosis. In this method, a semi-random subspace approach using a structured sparsity learning model is developed to decrease the negative effect of interrelated and redundant features, and in the meanwhile to generate accurate and diverse base classifiers. Furthermore, an adaptive evidential reasoning rule (ER rule) incorporating with ensemble learning theory is utilized to aggregate the diagnostic predictions of the base classifiers by taking both their weights and reliabilities into account. To validate the proposed DAMER, an empirical study is conducted on Case Western Reserve University bearing vibration dataset, and the experimental results verify the effectiveness of the proposed DAMER as well as its superiority over commonly used ensemble methods. The performances of feature subsets from multiple domains and the aggregation capability of the adaptive ER rule were also investigated. Results illustrate that DAMER can be utilized as an effective method for bearing fault diagnosis.																	0956-5515	1572-8145															10.1007/s10845-020-01554-5		MAR 2020											
J								Real-time task processing for spinning cyber-physical production systems based on edge computing	JOURNAL OF INTELLIGENT MANUFACTURING										Edge computing; Real-time task; Resource allocation; CPPS; Spinning	CLOUD; INTERNET; THINGS; MODEL	With a high-speed, dynamic and continuous yarn manufacturing process, spinning production suffers from different problems of dynamic disturbances such as yarn breakage, machine breakdown, and yarn quality. Processing real-time tasks is critical for tackling these problems, except for satisfying the requirements of mass production. The existing spinning cyber-physical production systems (CPPS), however, rely on a cloud center for centralized processing of real-time tasks. Thus, it becomes increasingly difficult for them to meet real-time requirements. As such, this paper proposes a novel real-time task processing method for spinning CPPS based on edge computing. First, a new hybrid structure of edge computing nodes (ECN) that consists of both 1-1 and N-1 modes is introduced for different types of tasks in spinning CPPS such as fixed tasks, decision-intensive tasks, and data-intensive tasks. Second, a collaboration mechanism is developed for collaborations between ECNs. The mathematical model and algorithms for real-time task processing are provided for a single ECN. Finally, a case study on a real spinning production is conducted. The results of this case study have demonstrated that the proposed method can significantly reduce the processing time of real-time tasks, as well as improve the production flexibility and production efficiency in spinning CPPS. The proposed method could be applied to continuous and batch manufacturing fields with high real-time requirements, such as weaving, chemical fiber production, and the pharmaceutical industry.																	0956-5515	1572-8145															10.1007/s10845-020-01553-6		MAR 2020											
J								Multiple birth support vector machine based on recurrent neural networks	APPLIED INTELLIGENCE										Multiple birth support vector machine; SVM; Long-short term Memory; Multilayer perceptron	MODEL	Multiple birth support vector machine (MBSVM) is a new classification algorithm, which includes the advantages of low complexity and high computing efficiency. However, the traditional MBSVM does not take into account the correlation sequence information among all dimensions of the samples when using the method to classify datasets, which limits the further improvement of the classification accuracy. Although some scholars have combined neural networks with support vector machine (SVM), these methods do not take into account the sequence correlation among different features. For the above problems, we present several variants of MBSVM algorithms to illustrate the validity and reliability of the theory: Multiple Birth Support Vector Machine based on Multilayer Perceptron (MLP-MBSVM), Multiple Birth Support Vector Machine based on Long-Short Term Memory Networks (LSTM-MBSVM), Multiple Birth Support Vector Machine based on Multilayer Perceptron and Long-Short Term Memory Networks(MLP-LSTM-MBSVM). After introducing multilayer perceptron and long-short term memory networks, these algorithms can take full account of the sequence correlation information between different features of samples. The experiments results show that the algorithms proposed in this paper are effective, and they can greatly improve the classification accuracy of multiple birth support vector machine.																	0924-669X	1573-7497				JUL	2020	50	7					2280	2292		10.1007/s10489-020-01655-x		MAR 2020											
J								A relation based algorithm for solving direct current circuit problems	APPLIED INTELLIGENCE										Problem solving; Relation acquisition; Syntax-semantics model; Schematics understanding; Unit-theorem inference		This paper addresses the challenging problem of developing the automatic algorithm for solving direct current circuit problem. Leveraging on the innovated methods it proposes a high-performance relation based algorithm, called RaDCC. The challenges of the problem lie in relation acquisition and relation inference presentation after adopting the newly-established relation principle of solving problems. A high-performance procedure is developed for the challenging task of relation acquisition by leveraging on three innovated methods. Three methods are an enhanced schematics understanding method that can understand complicated structures of schematics, an extended syntax-semantics model method and a unit-theorem inference method to acquire schematic relations, explicit text relations and implicit text relations, respectively. To address another challenging problem of readable solution generation an action-schema presentation method is proposed to convert relation inference actions into relation inference presentations. The experimental results show that the proposed algorithm is high-performance since it achieves an accuracy of over 83.2% for solving problems from textbooks and 70.6% for solving problems from examination papers on a dataset that contains 1012 direct current circuit problems collected from the authority sources, much higher than the performance of the baseline algorithm.																	0924-669X	1573-7497				JUL	2020	50	7					2293	2309		10.1007/s10489-020-01667-7		MAR 2020											
J								Parallel genetic algorithm for N-Queens problem based on message passing interface-compute unified device architecture	COMPUTATIONAL INTELLIGENCE										CUDA; genetic algorithm; island model; master slave model; MPI; N-Queens problem		N-Queens problem derives three variants: obtaining a specific solution, obtaining a set of solutions and obtaining all solutions. The purpose of the variant I is to find a constructive solution, which has been solved. Variant III is aiming to find all solutions and the largest number of queens currently being resolved is 26. Variant II whose purpose is to obtain a set of solutions for larger-scale problems relies on various intelligent algorithms. In this paper, we use a master-slave model genetic algorithm that combines the idea of the evolutionary algorithm and simulated annealing algorithm to solve Variant III, and use a parallel fitness function based on compute unified device architecture. Experimental results show that our scheme achieved a maximum 60-fold speedup over the single-CPU counterpart. On this basis, a two-level parallel genetic algorithm based on the island model and master-slave model is implemented on the GPU cluster by using message passing interface technology. Using two-node and three-node GPU cluster, speedup of 1.46 and 2.01 are obtained on average over single-node, respectively. Compared with the sequential genetic algorithm, the two-level parallel genetic algorithm makes full use of the parallel computing power of GPU cluster in solving N-Queen variant II and improves the performance by 99.19 times in the best case.																	0824-7935	1467-8640															10.1111/coin.12300		MAR 2020											
J								Achieving concurrency in cloud-orchestrated Internet of Things for resource sharing through multiple concurrent access	COMPUTATIONAL INTELLIGENCE										cloud computing; genetic and linear programming; IoT; multiple concurrent access; resource allocation	ALLOCATION; FRAMEWORK	Cloud-orchestrated Internet of Things (IoT) facilitates proper utilization of network resources and placating user demands in smart communications. Multiple concurrent access (MCA) techniques designed for cloud-assisted communication helps to achieve better resource sharing features with fault tolerance ability. A multi-objective resource allocation and sharing (RAS) for balancing MCA in cloud-orchestrated IoT is presented in this article. The RAS constraints are modeled through linear programming (LP) as an optimization approach. The constraints are resolved using genetic representations (GR) for reducing the unserviced requests and failed resource allocations. Conventional genetic stages are inherited by the LP model to solve resource allocation and access issues reducing latency. The combined LP and GR jointly resolve resource allocation and MCA stagnation in cloud network. A fair outcome of LP-GR is estimation using the metrics response latency, resource utilization, request handled, and average latency.																	0824-7935	1467-8640															10.1111/coin.12296		MAR 2020											
J								Sensor placement optimization and damage identification in a fuselage structure using inverse modal problem and firefly algorithm	EVOLUTIONARY INTELLIGENCE										Damage identification; Inverse problem; Firefly algorithm; Fuselage; Sensor placement optimization	STRAIN-ENERGY; TRANSVERSE CRACKS; CFRP PLATES; DELAMINATIONS; SYSTEMS; MODEL	The structural monitoring of mechanical systems is an extremely important task for ensuring its performance and structural health. To overcome limitations of traditional non-destructive inspections (NDIs), damage identification techniques have been developed from global indicators, mainly those based on modal data. In this study, damages are identified by solving an inverse problem. A fuselage model of an E190 aircraft is considered and the firefly algorithm (FA) metaheuristic is applied to solve the inverse problem in order to identify structural damages (location and severity). The method is then solved in two main fronts: (1) the direct problem using finite element analysis and (2) the inverse problem by minimizing an objective function. Evaluating modal response at many points on a large-scale structure can become prohibitive. For this, a method of optimizing sensors is performed using the Fisher information matrix (FIM). Results are compared considering the sensor placement optimization problem. It is noticed that optimized sensors contribute to an improvement in the identification of damages, mainly for complex and large-scale structures. The proposed optimized damage identification process using FIM-FA has the potential to be extended to a wide range of SHM applications in complex structures. Hence, traditional NDIs have many shortcomings due to the complexity of large-scale structures as well as modern design structures and may not be practicable if the structure has restricted access. Accordingly, an enhanced damage identification method is developed in order to better handle measurement data to find structural changes (or damages) in complex aerospace structures.																	1864-5909	1864-5917				DEC	2020	13	4					571	591		10.1007/s12065-020-00372-1		MAR 2020											
J								PAPR reduction and spectrum sensing in MIMO systems with optimized model	EVOLUTIONARY INTELLIGENCE										PAPR; Spectral sensing; MIMO systems; Cognitive radio; Gaussian mixture model	COGNITIVE RADIO NETWORKS; ALGORITHM	Cognitive radio is trending domain, which provides strong solution for addressing spectrum scarcity issues. Many cognitive radios standards suffer from high peak to average power ratio (PAPR), which may distort transmitted signal. This paper proposes a technique for spectrum sensing based on optimization enabled PAPR using hybrid Gaussian mixture model (GMM). The Eigen statistics, energy, and PAPR reduction block is adapted by hybrid mixture model for predicting the availability of spectrum. In order to model network with PAPR, the newly designed optimization algorithm, namely elephant-sunflower optimization (ESO) is adapted. The proposed ESO technique is combination of elephant herd optimization and sunflower optimization. The GMM is enabled using Eigen statistics, energy along with PAPR. The GMM is adjusted with an optimization algorithm, namely Whale elephant-herd optimization. The PAPR is reduced by optimally adjusting the parameters using proposed ESO. The channel availability is evaluated by providing energy, Eigen statistics and PAPR as input. The effectiveness of proposed ESO is illustrated with maximal probability of detection of 1.00, minimal PAPR of 7.534, and minimal bit error rate of 0.000 respectively.																	1864-5909	1864-5917															10.1007/s12065-020-00376-x		MAR 2020											
J								A multilevel paradigm for deep convolutional neural network features selection with an application to human gait recognition	EXPERT SYSTEMS										gait recognition; CNN features; features selection; parallel fusion; recognition	SKIN-LESION DETECTION; FUSION; SEGMENTATION; REPRESENTATION; ENTROPY; CLASSIFICATION; IMPLEMENTATION; FRAMEWORK; DISEASES; GENDER	Human gait recognition (HGR) shows high importance in the area of video surveillance due to remote access and security threats. HGR is a technique commonly used for the identification of human style in daily life. However, many typical situations like change of clothes condition and variation in view angles degrade the system performance. Lately, different machine learning (ML) techniques have been introduced for video surveillance which gives promising results among which deep learning (DL) shows best performance in complex scenarios. In this article, an integrated framework is proposed for HGR using deep neural network and fuzzy entropy controlled skewness (FEcS) approach. The proposed technique works in two phases: In the first phase, deep convolutional neural network (DCNN) features are extracted by pre-trained CNN models (VGG19 and AlexNet) and their information is mixed by parallel fusion approach. In the second phase, entropy and skewness vectors are calculated from fused feature vector (FV) to select best subsets of features by suggested FEcS approach. The best subsets of picked features are finally fed to multiple classifiers and finest one is chosen on the basis of accuracy value. The experiments were carried out on four well-known datasets, namely, AVAMVG gait, CASIA A, B and C. The achieved accuracy of each dataset was 99.8, 99.7, 93.3 and 92.2%, respectively. Therefore, the obtained overall recognition results lead to conclude that the proposed system is very promising.																	0266-4720	1468-0394														e12541	10.1111/exsy.12541		MAR 2020											
J								Generalization and extension of partitioned Bonferroni mean operator to model optional prerequisites	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										aggregation operator; Bonferroni mean; generalized partitioned Bonferroni mean; optional partitioned Bonferroni mean; partitioned Bonferroni mean	GROUP DECISION-MAKING; LINGUISTIC 2-TUPLES; OWA OPERATOR	The partitioned Bonferroni mean (PBM) operator, which was oriented as an elementary attempt to outstretch the Bonferroni mean (BM) operator, has enlarged the class of BM-type aggregation operators for information accumulation by modeling interrelationship among pairwise disjoint partition sets with the presupposition that the criteria of intra-partition are homogeneously related to each other, while no relationship exists among criteria of inter-partition. Although PBM has encountered a lot of attraction from the researchers due to its versatility in information aggregation technique, the principal disadvantage of the existing PBM definitions evolution is that they do not provide any specification regarding the relationship among criteria of partition structure during design, development, and applications of PBM over unalike situations of information fusion. This consideration propels us to focus on the systematic investigation of different variations of PBM operators based on various mandatory requisites to be imposed on information retrieved from the partition sets. In this regard, we propose the construction of novel generalized partitioned Bonferroni mean (GPBM) operator by befitting its suitable components to provide a descriptive configuration, which is quite interpretable, understandable and thus facilitates the ability to model specific mandatory prerequisites in a single operator. To enrich the capacity for modeling real-life decision situations, the PBM operator is customized to propose optional partitioned Bonferroni mean (OPBM) operator that captures partition-wise interrelationship among attributes while taking into consideration optional conditions jumbled in each partition set. Furthermore, we demonstrate the construction methodology of generalized OPBM operator that amalgamate the concept of GPBM and OPBM operator to enhance and model-specific requirements along with optional requirements as per the desires of decision makers.																	0884-8173	1098-111X				MAY	2020	35	5					891	919		10.1002/int.22229		MAR 2020											
J								Basic theory of line integrals under the q-rung orthopair fuzzy environment and their applications	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										multiple attribute decision making; nonstandard fuzzy set; q-rung orthopair fuzzy aggregation operator; q-rung orthopair fuzzy line integral	DECISION-MAKING METHOD; LINGUISTIC TERM SETS; MEAN OPERATORS; CONSENSUS; SIMILARITY	In the process of decision making (DM), to have more freedom in expressing DM experts' belief about membership and nonmembership grade, Yager presented the q-rung orthopair fuzzy sets (q-ROFSs) as an extension of fuzzy sets. Recently, for aggregating discrete and continuous q-ROF information, many scholars provided various aggregation operators. However, there are still some kinds of continuous q-ROF information cannot be aggregated through existing aggregation operators. In this paper, we present two novel kinds of line integrals under the q-rung orthopair fuzzy environment (q-ROFE), which supply a wider range on method choice for multiple attribute decision making (MADM) concerning discrete or continuous q-ROF information. We first construct the first form of line integral under the q-ROFE and study their properties, we prove the specific condition mean valued theorem but negate the general condition for it. Besides, several inequalities with regard to it are also provided. After that, we give two characterization forms of the second form of line integral under the q-ROFE, we study the relationship between two kinds of line integrals. What is more important, we show the Green formula under q-ROFE, which connect the existing q-rung orthopair fuzzy integration theory. Afterward, we provide two kinds of aggregation operators on the basis of two line integrals, respectively. As their applications, we give two novel MADM methods based on two kinds of line integral aggregation operators. And some examples are shown to demonstrate the aggregation process of line integral operators. We not only stress their availabilities and superiorities on aggregating continuous q-ROF information, but also comparing with other existing aggregation methods for emphasizing the novel methods' abilities when deal with abnormal q-ROF information.																	0884-8173	1098-111X				JUL	2020	35	7					1163	1199		10.1002/int.22238		MAR 2020											
J								Three-way active learning through clustering selection	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Active learning; Clustering; Granular computing; Three-way decision	DECISION; CLASSIFIER; MODEL	In clustering-based active learning, the performance of the learner relies heavily on the quality of clustering results. Empirical studies have shown that different clustering techniques are applicable to different data. In this paper, we propose the three-way active learning through clustering selection (TACS) algorithm to dynamically select the appropriate techniques during the learning process. The algorithm follows the coarse-to-fine scheme of granular computing coupled with three-way instance processing. For label query, we select both representative instances with density peaks, and informative instances with the maximal total distance. For block partition, we revise six popular clustering techniques to speed up learning and accommodate binary splitting. For clustering evaluation, we define weighted entropy with 1-nearest-neighbor. For insufficient labels, we design tree pruning techniques with the use of a block queue. Experiments are undertaken on twelve UCI datasets. The results show that TACS is superior to single clustering technique based algorithms and other state-of-the-art active learning algorithms.																	1868-8071	1868-808X				MAY	2020	11	5					1033	1046		10.1007/s13042-020-01099-2		MAR 2020											
J								An agent-based modeling for collective scene criticality assessment in multi-UV systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent agents; UVs; GDM; Consensus; Intelligent monitoring; scene Criticality assessment	UAVS; OPTIMIZATION; ARCHITECTURE; ALLOCATION; CONSENSUS; TASK	In recent years, the role of unmanned vehicles (UVs) is increased in many surveillance applications; they are substituting the humans in many risky activities, especially when cooperative tasks from UV team are required. To this purpose, this paper presents an agent-based framework that models a multi-UV system for surveillance applications. The agents act as wrappers for the different types of UVs, that capture data from the scene (in the area of the UV mission) and then process them, each one according to its own skills and features. The collected and processed data are then shared from the agent team to find a common agreement on the comprehension and criticality assessment of the scenario. The agent paradigm provides a seamless framework for UV interaction, making the different methodologies and technologies, designed for the different UV types, transparent. The proposal shows the agent-based modeling for a multi-UV system, where each agent hides the facilities and features of the UV it wrapped, with the aim of deploying a homogeneous interface to facilitate the collective scenario assessment in terms of critical or alerting issues, detected in the evolving scene.																	1868-5137	1868-5145															10.1007/s12652-020-01830-5		MAR 2020											
J								Variable control chart for detecting black hole attack in vehicular ad-hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Statistical process control; Black hole attack; VANET	PERFORMANCE ANALYSIS; ROUTING PROTOCOLS; AODV	The nature of VANET networks makes the network vulnerable to several attacks such as the black hole attack which is part of Denial of Service attacks (DOS). The aim of these attacks is to prohibit users from using one or more services by making the communication unavailable. In this sense and during the routing process, the malicious node tries to acquire the route by forging routing information in order to receive the data and then drops it without transferring it to its destination. In this paper, we propose a novel detection method of such attack during communication in a VANET network. This method is based on a variable control chart which is widely used in the industrial field to monitor the quality of a given process. This method can identify malicious nodes in real time by deploying the monitoring system in each receiving node within the network. Our method is easy to implement and does not require any modification to the 802.11p standard or the routing protocol as we will demonstrate by NS-2 simulations and SUMO microscopic and continuous road traffic simulator using a real map.																	1868-5137	1868-5145															10.1007/s12652-020-01825-2		MAR 2020											
J								Machine learning method based detection and diagnosis for epilepsy in EEG signal	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Epilepsy; Seizures; Focal signal; Detection; Diagnosis	CLASSIFICATION; IDENTIFICATION	The epileptic seizure can be detected using electroencephalogram (EEG) signals. The detection of epileptogenic region in brain is important for the detection of epilepsy disease. The signals from epileptogenic region in brain are focal signal and the signal from normal regions in brain is non-focal signal. Hence, the detection of focal signal is important for epilepsy disease detection. This paper proposes an automatic detection and diagnosis of EEG signals for epilepsy disease using soft computing approaches as adaptive neuro fuzzy inference system (ANFIS) and neural networks (NN). In this paper, the features from decomposed coefficients as bias (B), weight feature (W), entropy(E), activity feature (AF), mobility feature (MF), complexity feature (CF), skewness (S) and kurtosis (K) are extracted for the classification of EEG signals into either focal or non-focal signals for epilepsy disease detection and diagnosis. The detection of focal signal is achieved by ANFIS classifier and the diagnosis of the severity levels in focal signal is achieved by NN classification approach. The proposed method is used in many clinical diagnosis.																	1868-5137	1868-5145															10.1007/s12652-020-01816-3		MAR 2020											
J								Ontology driven human activity recognition in heterogeneous sensor measurements	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ontology; Sensor fusion; Activity recognition; SPARQL; OWL; RDF	CONTEXT	In recent years, sensor-based activity recognition has integrated the field of sensor networks with data mining techniques to model a broad range of human activities and behaviour. Huge amounts of sensor data coming from smart gadgets such as smartphones and smartwatches opens up the possibility of probing and extracting knowledge from the data in the direction of monitoring and health care. Due to the immense popularity and extensive use of smart gadgets equipped with sensors, it is more realistic and effective to utilize them in the activity recognition systems. Sensor-based activity recognition is a challenging task due to the heterogeneous nature and noisy aspect of sensor data. This work presents an ontology-based knowledge model that conceptualizes the task of human activity recognition. The knowledge model is based on two newly developed ontologies: Sensor Measurements Ontology to model the heterogeneous sensor data and Activity Recognition Ontology to conceptualise the activity recognition process by capturing the relationships between the low level acts (simple activities) and high level (inferred activity). Besides activity recognition, the proposed model handles the issue of sensor heterogeneity and provides reusability, interoperability and exchange. The proposed model is validated with a real life dataset containing sensor observations of 60 users with more than 300,000 (three hundred thousand) samples to illustrate the functionalities in the task of human activity recognition.																	1868-5137	1868-5145															10.1007/s12652-020-01835-0		MAR 2020											
J								Peak detection based energy detection of a spectrum under rayleigh fading noise environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cognitive radio; Optimization; Peak detection process		Spectrum is a resource inadequate and future licensed spectrum will only be used by spectrum holders. Reuse of licensed spectrum into unlicensed technique is Cognitive radios fresh approach. No previous data is crucial in relation to the uniqueness of the main user signal as energy detection is most commonly used spectrum sensing methods. But this energy detection calculation and assuming the presence of primary user in spectrum based on static threshold is not a simple task. It may leads to loss in the signal coefficients. Hence, in the existing method the threshold selection is adaptive which depends on the noise variance. This method increases the probability of detection i.e. the detection of presence of the primary user in the spectrum, but calculating the energy of all the received symbols may lead to computational complex and high sensing time. Hence in the proposed method, the power of all the symbols collected near the spectrum is optimized using peak detection technique. This optimization method reduces the power caused due to unwanted or noisy data and leads to less power usage. Thus energy detection for this optimized data is simple with less sensing time. The experimental results suggest that this optimization based energy detection would also useful to get 99% of detection of primary user in spectrum at low SNR also.																	1868-5137	1868-5145															10.1007/s12652-020-01818-1		MAR 2020											
J								A resilient and secure two-stage ITA and blockchain mechanism in mobile crowd sourcing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile crowd sourcing; Fault tolerance; ITA; TORU; Security; Blockchain	INCENTIVE MECHANISM; OPTIMIZATION; CHALLENGES; PRIVACY	Mobile crowd sourcing offers a way for an organization to gain afar their "base of minds" of workforce offers in the form of idea competitions or innovation contests where task is divided among individuals to achieve a pooled result using mobile devices. However, mobile crowd sourcing consequence to an urban bias in addition to other key challenging issues such as safety, privacy and connectivity or data fault tolerance. Fault tolerance in mobile crowd sourcing is a property to tolerate any fault related to data collected or the working of the system. Further, it is needed to ensure a secure resilient mechanism due to de-centralized behavior of the network. In this paper, we have proposed a resilient-improved two-stage auction (ITA) and blockchain mechanism in mobile crowd sourcing beneficial for the organizations. The resilient technique and secure scheme is beneficial for the organizations that select the workers according to their budget and time. Further, an akka API tool is implemented upon already existing mobile crowd sourcing technique i.e. ITA. The proposed resilient mechanism is initially implemented over. The proposed resilient ITA mechanism is analyzed over further crowd sourcing mechanisms needed for workers selection in order to benefit the organizations. Further, the resilient-ITA is measured against database connection loss that is considered as a promising type of fault occurring during the workers selection.																	1868-5137	1868-5145															10.1007/s12652-020-01800-x		MAR 2020											
J								Meta-heuristics for the one-dimensional cutting stock problem with usable leftover	JOURNAL OF HEURISTICS										Cutting problem; Combinatorial optimization; Multiobjective optimization; Meta-heuristics	OPTIMIZATION	This work considers the one-dimensional cutting stock problem in which the non-used material in the cutting patterns may be used in the future, if large enough. We show that a multiobjective criteria to classify the solutions could be more accurate than previous classifications attempts, also we give a heuristic algorithm and two meta-heuristic approaches to the problem and we use them to solve practical and randomly generated instances from the literature. The results obtained by the computational experiments are quite good for all the tested instances.																	1381-1231	1572-9397				AUG	2020	26	4					585	618		10.1007/s10732-020-09443-z		MAR 2020											
J								A New Viewpoint on Control Algorithms for Anthropomorphic Robotic Arms	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Control algorithm; Null space control; Anthropomorphic robotic arm; Fuzzy system	INVERSE KINEMATICS; MANIPULATORS	We investigate the singularity for the anthropomorphic robotic arm and develop an algorithm to handle the calculation. It is demonstrated that although the limit characteristics and the interior characteristics of these robotic arms can be distinguished through an examination of their unique properties, it is still not possible to obtain decent kinematic decoupling properties for modern automated arms. In light of this, the methodology of our calculation is changed to deal with these two cases. The control contribution is altered to again allow bending of the robotic arm from the solitary straight stance. The interior singularities are taken into consideration in the calculation to control against invalid spatial movement of the automated arm. By incorporating fuzzy control theorem, we demonstrate that modeling system enables the robotic arm to move within solitary areas and return back to certain locations, so that the usable workspace is expanded. The viability of the proposed calculation method for reproduction and use in ongoing control tests is proven.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		647	658		10.1007/s10846-020-01149-5		MAR 2020											
J								Multi-Robot Patrolling with Sensing Idleness and Data Delay Objectives	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-robot systems; Mobile robotics; Patrolling; Coordination; Cooperating robots	SURVEILLANCE; CONNECTIVITY; STRATEGIES; ROBOTS; UAVS	Multi-robot patrolling represents a fundamental problem for many monitoring and surveillance applications and has gained significant interest in recent years. In patrolling, mobile robots repeatedly travel through an environment, capture sensor data at certain sensing locations and deliver this data to the base station in a way that maximizes the changes of detection. Robots move on tours, exchange data when they meet with robots on neighboring tours and so eventually deliver data to the base station. In this paper we jointly consider two important optimization criteria of multi-robot patrolling: (i) idleness, i.e. the time between consecutive visits of sensing locations, and (ii) delay, i.e. the time between capturing data at the sensing location and its arrival at the base station. We systematically investigate the effect of the robots' moving directions along their tours and the selection of meeting points for data exchange. We prove that the problem of determining the movement directions and meeting points such that the data delay is minimized is NP-hard. For this purpose, we define a structure called tour graph which models the neighborhood of the tours defined by potential meeting points. We propose two heuristics that are based on a shortest-path-search in the tour graph. We provide a simulation study which shows that the cooperative approach can outperform an uncooperative approach where every robot delivers the captured data individually to the base station. Additionally, the experiments show that the heuristic which is computational more expensive performs slightly better on average than the less expensive heuristic in the considered scenarios.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		949	967		10.1007/s10846-020-01156-6		MAR 2020											
J								A two-stage method for spectral-spatial classification of hyperspectral images	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Hyperspectral image classification; Image segmentation; Image denoising; Mumford-Shah model; Support vector machine; Alternating direction method of multipliers	SUPPORT VECTOR MACHINES; REGULARIZATION PARAMETER; SEGMENTATION METHOD; NOISE REMOVAL; ALGORITHMS; SELECTION; FOOD	We propose a novel two-stage method for the classification of hyperspectral images. Pixel-wise classifiers, such as the classical support vector machine (SVM), consider spectral information only. As spatial information is not utilized, the classification results are not optimal and the classified image may appear noisy. Many existing methods, such as morphological profiles, superpixel segmentation, and composite kernels, exploit the spatial information. In this paper, we propose a two-stage approach inspired by image denoising and segmentation to incorporate the spatial information. In the first stage, SVMs are used to estimate the class probability for each pixel. In the second stage, a convex variant of the Mumford-Shah model is applied to each probability map to denoise and segment the image into different classes. Our proposed method effectively utilizes both spectral and spatial information of the data sets and is fast as only convex minimization is needed in addition to the SVMs. Experimental results on three widely utilized real hyperspectral data sets indicate that our method is very competitive in accuracy, timing, and the number of parameters when compared with current state-of-the-art methods, especially when the inter-class spectra are similar or the percentage of training pixels is reasonably high.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		790	807		10.1007/s10851-019-00925-9		MAR 2020											
J								FPGA-accelerated adaptive projection-based image registration	JOURNAL OF REAL-TIME IMAGE PROCESSING										Parallel processing; FPGA; High speed; Image registration	SIMILARITY; ALGORITHM	In this paper, a high-speed hardware-based image registration is proposed exploiting parallelism and adaptive sampling technique to fulfill the requirement of high-speed portable multimedia devices. This technique computes radial and angular projections in parallel way without converting the image into polar domain, but by adjusting the number of samples along angular direction according to radius length which decreases computational load. Further, a complete image registration algorithm without using any geometric transformation is proposed. The software-based implementation of the proposed algorithm is 1.33 times faster than its latest available method in the literature. The proposed algorithm is mapped in field-programmable gate array (FPGA, Virtex6-xc6vlx760-2-ff1760) and it utilizes 2.03% Slice LUTs, 35.14% LUT-FF pair and 1.27% DSP48E1s; and maximum clock frequency is 266 Mz. The hardware-based implementation of the proposed algorithm is 104 times faster than software counterpart.																	1861-8200	1861-8219															10.1007/s11554-020-00952-5		MAR 2020											
J								A multi-objective open set orienteering problem	NEURAL COMPUTING & APPLICATIONS										Set orienteering problem; Multi-objective optimization; NSGAII; SPEA2	ALGORITHM	We have constructed a multi-objective set orienteering problem to model real-world problems more fittingly than the existing models. It attaches (i) a predefined profit associated with each cluster of customers, and (ii) a preset maximum service time associated with each customer of all the clusters. When a customer from cluster visits, it allows the earning of a profit score. Our purpose is primarily to search for a route that (i) on the one hand allows us to service each cluster for maximizing customer satisfaction and (ii) on the other hand allows us to maximize our profits, too. The model assumes that the more time we spend on servicing, the more customer satisfaction it yields. It tries to cover as many clusters as possible within a specified time budget. In this paper, we also consider third-party logistics to allow the flexibility of ending our journey at any cluster of our choice. The proposed model is solved using the nondominated sorting genetic algorithm and the strength Pareto evolutionary algorithm. Here, we also generate the dataset to test the proposed model by using instances from the literature of the generalized traveling salesman problem. Finally, we present a comparative result analysis with the help of some statistical tools and discuss the results.																	0941-0643	1433-3058				SEP	2020	32	17					13953	13969		10.1007/s00521-020-04798-7		MAR 2020											
J								A hybrid classifier combination for home automation using EEG signals	NEURAL COMPUTING & APPLICATIONS										EEG; IoT; LSTMs; Hybrid machine learning	PREDICTION; FEATURES; MODEL	Over the years, the usage of artificial intelligence (AI) algorithms is increased to develop various smart applications using Internet-of-Things. Home automation is a fast emerging area that involves monitoring and controlling of household appliances for user comfort and efficient management. Using mental commands to control different electrical appliances and objects in house is a very interesting application. Brain-Computer Interface is used to relay the information from the subject's brain to an Electronic device, and such devices can be used for this purpose. The information from the subject's brain is collected in form of Electroencephalogram (EEG) signals. In this paper, we analyze the use of EEG signals for applications related to home automation. We present a hybrid model which makes use of Long Short-Term Memory which is considered as a robust temporal classification model in AI and classical Random Forest Classifier for EEG classification. We also discuss how our proposed hybrid model overcomes the limitation presented by the individual models. To arrive at the best model, we have analyzed various parameters such as sampling rate and combination of different brain rhythms which we finally use in our hybrid model. Based on experiments conducted on a custom-built dataset, we also discuss the spatial significance of different electrodes of the EEG device and get insight in signals generated from different areas of the brain.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16135	16147		10.1007/s00521-020-04804-y		MAR 2020											
J								Developer Activity Motivated Bug Triaging: Via Convolutional Neural Network	NEURAL PROCESSING LETTERS										Bug triage; Mining software repositories; Convolution neural network; Deep learning	SEVERITY PREDICTION; ALGORITHM	As bugs become prevalent in software development, bug triaging has become one of the most important activities in software maintenance. To decrease the time cost in manual work, text classification techniques have been applied in automatic bug triaging. In this paper, we present a new automatic bug triaging approach which is based on convolution neural network (CNN) and developer activities. Firstly, we implement the word vector representation of the text features in bug report by using Word2vec. Then, we combine CNN with batch normalization, pooling and full connection approach to learn from the word vector representation of bug report with known fixers. In addition, we also study the recent activities of the developers which can effectively distinguish similar bug reports and get a more suitable developer recommendation list. We empirically investigate the accuracy of automatic bug triaging on three large open source projects, namely Eclipse, Mozilla and NetBeans. The experimental results show that our approach can effectively improve the performance of automatic bug triaging.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2589	2606		10.1007/s11063-020-10213-y		MAR 2020											
J								A Human Auditory Perception Loss Function Using Modified Bark Spectral Distortion for Speech Enhancement	NEURAL PROCESSING LETTERS										Speech enhancement; Loss function; MSE; MBSD; DNN	NETWORKS; QUALITY	Human listeners often have difficulties understanding speech in the presence of background noise in daily speech communication environments. Recently, deep neural network (DNN)-based techniques have been successfully applied to speech enhancement and achieved significant improvements over the conventional approaches. However, existing DNN-based methods usually minimize the log-power spectral-based or the masking-based mean squared error (MSE) between the enhanced output and the training target (e.g., the ideal ratio mask (IRM) of the clean speech), which is not closely related to human auditory perception. In this letter, a modified bark spectral distortion loss function, which can be considered as an auditory perception-based MSE, is proposed to replace the conventional MSE in DNN-based speech enhancement approaches to further improve the objective perceptual quality. Experimental results reveal that the proposed method can obtain improved speech enhancement performance, especially in terms of objective perceptual quality in all experimental settings when compared with the DNN-based methods using the conventional MSE criterion.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2945	2957		10.1007/s11063-020-10212-z		MAR 2020											
J								Time-dependent intuitionistic fuzzy system reliability analysis	SOFT COMPUTING										Intuitionistic fuzzy random variable; Intuitionistic fuzzy reliability system; Intuitionistic fuzzy system component		The purpose of the present paper is to provide a method for constructing time-dependent reliability systems based on intuitionistic fuzzy random variables. A lifetime variable cannot be precisely recorded due to machine errors, experimentation pitfalls, personal judgments, estimation errors or other unexpected sources of error. In order to satisfy the purpose of this paper, an intuitionistic fuzzy random variable with exact parameters was introduced and adopted to evaluate the reliability functions of a k-out-of-n system, with some reliability evaluation criteria discussed and interpreted. Numerical evaluations were further presented to illustrate the calculation of the system reliability criteria in the form of intuitionistic fuzzy numbers. Finally, a number of potential engineering applications of the proposed method were presented.																	1432-7643	1433-7479				OCT	2020	24	19					14441	14448		10.1007/s00500-020-04796-w		MAR 2020											
J								Improvising the performance of image-based recommendation system using convolution neural networks and deep learning	SOFT COMPUTING										Deep learning; Convolution neural network; Recommendation system; Dimensionality reduction; Tensor flow		Recommendation systems now hold special significance, for in a world full of choices, order is the need of the hour. Without proper sorting, the gift of choice means nothing. The online retail world is fast-paced and ever-growing. With the exponential waning of attention span, it has become crucial to convert a casual visitor into a buyer within a limited window. Different ways can be used to do this: analysing buying patterns, surveys, user-user relationships, user-item relationships, and so on. This can be done with simple data analysis or with complex algorithms-the data must be harnessed one way or another. Deep learning is a branch of machine learning that has now become synonymous with computer vision, as these deep architectures closely emulate the biological process of vision. In this paper, the primary focus is the incorporation of a recommendation system with the visual features of products. This is done with the help of a deep architecture and a series of "convolution" operations that cause the overlapping of edges and blobs in images. We find that when the dimensionality problem has been dealt with, the features extracted serve as good quality representations of the images. Our empirical study compares the different linear and nonlinear reduction techniques on convolutional neural network features for building a recommendation model entirely based on the images.																	1432-7643	1433-7479				OCT	2020	24	19					14531	14544		10.1007/s00500-020-04803-0		MAR 2020											
J								Twin support vector machine based on improved artificial fish swarm algorithm with application to flame recognition	APPLIED INTELLIGENCE										Twin support vector machine; Artificial fish swarm algorithm; Fire flame recognition; Color model	CONVOLUTIONAL NEURAL-NETWORKS; FIRE DETECTION; MODEL; SURVEILLANCE; SELECTION	In this paper, a twin support vector machine (TWSVM) based on improved artificial fish swarm algorithm (IAFSA) for fire flame recognition is proposed in view of the large computation burden and slow classification speed of the traditional support vector machine (SVM). Twin support vector machine is a machine learning algorithm developing from standard support vector machine. However, twin support vector machine cannot deal with the parameter selection problem well. The difficulty of parameter selection may greatly restrict the application of TWSVM in flame recognition problem. So a novel artificial fish swarm algorithm (AFSA) is used to solve the parameter selection problem of TWSVM. In order to make up for the drawbacks of the basic AFSA, the chaotic transformation is first applied to initialize the position of artificial fish swarm since it may be non-uniformly initialized in the basic artificial fish swarm algorithm. Then, the Cauchy mutation is used to make the fish swarm jump out of the local optimal solution after continuously expanding the visual scope of the artificial fish during the foraging procedure. An adaptively step-size adjusting method is then developed to optimize the moving steps of the swarming and following behaviors in order to accelerate the convergence speed of the developed algorithm. Last, to further improve the efficiency and accuracy of the algorithm, an elimination and regeneration mechanism based on adaptive t-distribution mutation is utilized to update the artificial fish swarm at each iterative procedure. Experimental results show that the TWSVM algorithm based on improved artificial fish swarm algorithm is a more effective method to identify the flame and greatly improves the accuracy and real-time performance of the flame recognition compared with PSO-TWSVM, Grid-TWSVM, GA-TWSVM, FOA-TWSVM, GSO-TWSVM, AFSA-TWSVM and the traditional SVM.																	0924-669X	1573-7497				AUG	2020	50	8					2312	2327		10.1007/s10489-020-01676-6		MAR 2020											
J								An approach for semantic integration of heterogeneous data sources	PEERJ COMPUTER SCIENCE										Data integration; Heterogeneous data sources; Semantic integration; Ontologies		Integrating data from multiple heterogeneous data sources entails dealing with data distributed among heterogeneous information sources, which can be structured, semistructured or unstructured, and providing the user with a unified view of these data. Thus, in general, gathering information is challenging, and one of the main reasons is that data sources are designed to support specific applications. Very often their structure is unknown to the large part of users. Moreover, the stored data is often redundant, mixed with information only needed to support enterprise processes, and incomplete with respect to the business domain. Collecting, integrating, reconciling and efficiently extracting information from heterogeneous and autonomous data sources is regarded as a major challenge. In this paper, we present an approach for the semantic integration of heterogeneous data sources, DIF (Data Integration Framework), and a software prototype to support all aspects of a complex data integration process. The proposed approach is an ontology-based generalization of both Global-as-View and Local-as-View approaches. In particular, to overcome problems due to semantic heterogeneity and to support interoperability with external systems, ontologies are used as a conceptual schema to represent both data sources to be integrated and the global view.																	2376-5992					MAR 2	2020									e254	10.7717/peerj-cs.254													
J								A Viterbi decoder and its hardware Trojan models: an FPGA-based implementation study	PEERJ COMPUTER SCIENCE										Coded communication system; Hardware Trojan; Viterbi decoder; Bit error rate		Integrated circuits may be vulnerable to hardware Trojan attacks during its design or fabrication phases. This article is a case study of the design of a Viterbi decoder and the effect of hardware Trojans on a coded communication system employing the Viterbi decoder. Design of a Viterbi decoder and possible hardware Trojan models for the same are proposed. An FPGA-based implementation of the decoder and the associated Trojan circuits have been discussed. The noise-added encoded input data stream is stored in the block RAM of the FPGA and the decoded data stream is monitored on the PC through an universal asynchronous receiver transmitter interface. The implementation results show that there is barely any change in the LUTs used (0.5%) and power dissipation (3%) due to the insertion of the proposed Trojan circuits, thus establishing the surreptitious nature of the Trojan. In spite of the fact that the Trojans cause negligible changes in the circuit parameters, there are significant changes in the bit error rate (BER) due to the presence of Trojans. In the absence of Trojans, BER drops down to zero for signal to noise rations (SNRs) higher than 6 dB, but with the presence of Trojans, BER doesn't reduce to zero even at a very high SNRs. This is true even with the Trojan being activated only once during the entire duration of the transmission.																	2376-5992					MAR 2	2020									e250	10.7717/peerj-cs.250													
J								Forma mentis networks map how nursing and engineering students enhance their mindsets about innovation and health during professional growth	PEERJ COMPUTER SCIENCE										Complex networks; Stance detection; Psycholinguistics; Education; Learning outcomes; Healthcare; Mindset modelling; STEM; Attitude; Entropy	SEMANTIC DISTANCE; KNOWLEDGE; EDUCATION; COGNITION	Reconstructing a "forma mentis", a mindset, and its changes, means capturing how individuals perceive topics, trends and experiences over time. To this aim we use forma mentis networks (FMNs), which enable direct, microscopic access to how individuals conceptually perceive knowledge and sentiment around a topic, providing richer contextual information than machine learning. FMNs build cognitive representations of stances through psycholinguistic tools like conceptual associations from semantic memory (free associations, i.e., one concept eliciting another) and affect norms (valence, i.e., how attractive a concept is). We test FMNs by investigating how Norwegian nursing and engineering students perceived innovation and health before and after a 2-month research project in e-health. We built and analysed FMNs by six individuals, based on 75 cues about innovation and health, and leading to 1,000 associations between 730 concepts. We repeated this procedure before and after the project. When investigating changes over time, individual FMNs highlighted drastic improvements in all students' stances towards "teamwork", "collaboration", "engineering" and "future", indicating the acquisition and strengthening of a positive belief about innovation. Nursing students improved their perception of 'robots" and "technology" and related them to the future of nursing. A group-level analysis related these changes to the emergence, during the project, of conceptual associations about openness towards multidisciplinary collaboration, and a positive, leadership-oriented group dynamics. The whole group identified "mathematics" and "coding" as highly relevant concepts after the project. When investigating persistent associations, characterising the core of students' mindsets, network distance entropy and closeness identified as pivotal in the students' mindsets concepts related to "personal well-being", "professional growth" and "teamwork". This result aligns with and extends previous studies reporting the relevance of teamwork and personal well-being for Norwegian healthcare professionals, also within the novel e-health sector. Our analysis indicates that forma mentis networks are powerful proxies for detecting individual- and group-level mindset changes due to professional growth. FMNs open new scenarios for data-informed, multidisciplinary interventions aimed at professional training in innovation.																	2376-5992					MAR 2	2020									e255	10.7717/peerj-cs.255													
J								Comparative investigation of parallel spatial interpolation algorithms for building large-scale digital elevation models	PEERJ COMPUTER SCIENCE										Digital elevation model (DEM); Spatial interpolation; Radial basis function (RBF); Moving least square (MLS); Parallel algorithm; Graphics processing unit (GPU); Geographic information system(GIS)	GRID DEM; GPU; ACCURACY; GENERATION	The building of large-scale Digital Elevation Models (DEMs) using various interpolation algorithms is one of the key issues in geographic information science. Different choices of interpolation algorithms may trigger significant differences in interpolation accuracy and computational efficiency, and a proper interpolation algorithm needs to be carefully used based on the specific characteristics of the scene of interpolation. In this paper, we comparatively investigate the performance of parallel Radial Basis Function (RBF)-based, Moving Least Square (MLS)-based, and Shepard's interpolation algorithms for building DEMs by evaluating the influence of terrain type, raw data density, and distribution patterns on the interpolation accuracy and computational efficiency. The drawn conclusions may help select a suitable interpolation algorithm in a specific scene to build large-scale DEMs.																	2376-5992					MAR 2	2020									e263	10.7717/peerj-cs.263													
J								Discrete two dimensional Fourier transform in polar coordinates part II: numerical computation and approximation of the continuous transform	PEERJ COMPUTER SCIENCE										Fourier theory; DFT in polar coordinates; Polar coordinates; Multidimensional DFT; Discrete Hankel Transform; Discrete Fourier Transform; Orthogonality	SAMPLING THEOREMS; INTEGER-ORDER; RECONSTRUCTION; TOMOGRAPHY; ALGORITHM	The theory of the continuous two-dimensional (2D) Fourier Transform in polar coordinates has been recently developed but no discrete counterpart exists to date. In the first part of this two-paper series, we proposed and evaluated the theory of the 2D Discrete Fourier Transform (DFT) in polar coordinates. The theory of the actual manipulated quantities was shown, including the standard set of shift, modulation, multiplication, and convolution rules. In this second part of the series, we address the computational aspects of the 2D DFT in polar coordinates. Specifically, we demonstrate how the decomposition of the 2D DFT as a DFT, Discrete Hankel Transform and inverse DFT sequence can be exploited for coding. We also demonstrate how the proposed 2D DFT can be used to approximate the continuous forward and inverse Fourier transform in polar coordinates in the same manner that the 1D DFT can be used to approximate its continuous counterpart.																	2376-5992					MAR 2	2020									e257	10.7717/peerj-cs.257													
J								An evolutionary decomposition-based multi-objective feature selection for multi-label classification	PEERJ COMPUTER SCIENCE										Feature selection; Multi-label classification; Multi-objective optimization; Decomposition-based algorithm; Evolutionary algorithm	PARTICLE SWARM OPTIMIZATION; ALGORITHM; PERFORMANCE; SEARCH	Data classification is a fundamental task in data mining. Within this field, the classification of multi-labeled data has been seriously considered in recent years. In such problems, each data entity can simultaneously belong to several categories. Multi-label classification is important because of many recent real-world applications in which each entity has more than one label. To improve the performance of multi-label classification, feature selection plays an important role. It involves identifying and removing irrelevant and redundant features that unnecessarily increase the dimensions of the search space for the classification problems. However, classification may fail with an extreme decrease in the number of relevant features. Thus, minimizing the number of features and maximizing the classification accuracy are two desirable but conflicting objectives in multi-label feature selection. In this article, we introduce a multi-objective optimization algorithm customized for selecting the features of multi-label data. The proposed algorithm is an enhanced variant of a decomposition-based multi-objective optimization approach, in which the multi-label feature selection problem is divided into single-objective subproblems that can be simultaneously solved using an evolutionary algorithm. This approach leads to accelerating the optimization process and finding more diverse feature subsets. The proposed method benefits from a local search operator to find better solutions for each subproblem. We also define a pool of genetic operators to generate new feature subsets based on old generation. To evaluate the performance of the proposed algorithm, we compare it with two other multi-objective feature selection approaches on eight real-world benchmark datasets that are commonly used for multi-label classification. The reported results of multi-objective method evaluation measures, such as hypervolume indicator and set coverage, illustrate an improvement in the results obtained by the proposed method. Moreover, the proposed method achieved better results in terms of classification accuracy with fewer features compared with state-of-the-art methods.																	2376-5992					MAR 2	2020									e261	10.7717/peerj-cs.261													
J								A novel design method for dual-passband IIR digital filters	APPLIED INTELLIGENCE										Digital filters; Variable structure systems; Evolutionary computation; Dual-passband filters	DIFFERENTIAL EVOLUTION ALGORITHM; ENSEMBLE	With the rapid development of wireless communication technology, digital filters are now key components in many modern digital systems. Dual-passband digital filter is an important module of digital filter and has attracted wide attention. This paper proposes a novel evolutionary method to design diversified structure digital filters. Our proposed method using an adaptive multiple-elites- guide composite differential evolution algorithm, coupled with a shift mechanism (AMECoDEs) doesn't need to use known circuit structures. Structures and parameters are evolved by crossover, mutation, and selection. Thus, our proposed method can directly design the diversified dual-passband digital filter structure and can effectively balance exploration and exploitation to prevent individuals from premature convergence. In our experiment, the connection probability, the subsystem number of the filter structure, as well as the scale factor and the crossover rate of AMECoDEs are explored to determine the optimal configuration. Compared with exiting state-of-the-art evolutionary algorithms for the design of the symmetrical and asymmetrical dual-bandpass filters, our proposed method has the smallest average passband ripple and stopband attenuation with the fastest convergence.																	0924-669X	1573-7497				JUL	2020	50	7					2132	2150		10.1007/s10489-020-01631-5		MAR 2020											
J								P-GWO and MOFA: two new algorithms for the MSRCPSP with the deterioration effect and financial constraints (case study of a gas treating company)	APPLIED INTELLIGENCE										Project scheduling; Deterioration effect; Financial constraints; Grey wolf optimizer; Fibonacci numbers	PROJECT SCHEDULING PROBLEM; GREY WOLF OPTIMIZER; SINGLE-MACHINE; MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; RESOURCE CONSTRAINTS; MODEL; JOBS; TIME; MAINTENANCE	This paper presents a bi-objective mathematical formulation for the multi-skill resource-constrained project scheduling problem (MSRCPSP) with the deterioration effect and financial constraints. The objectives are to optimize the makespan and cost of project, simultaneously. Due to the high NP-hardness of the proposed model, a Pareto-based Grey Wolf Optimizer (P-GWO) algorithm has been developed to solve the problem. A new procedure based on the Weighted Sum Method (WSM) has been designed for the P-GWO to rank the solutions of population in order to find the alpha, beta, delta, and omega wolves. The P-GWO also uses a new procedure based on the Data Envelopment Analysis (DEA) to keep the most efficient newly found solutions and update the archive of non-dominated solutions. Besides, a Multi-Objective Fibonacci-based Algorithm (MOFA) based on the characteristics of the Fibonacci sequence has been proposed to solve the problem. The MOFA utilizes a novel neighborhood operator to generate as many feasible solutions as required in each iteration. For the MOFA, new procedures for finding the best solution of each iteration, elitism and updating archive of non-dominated solutions have been developed as well. To evaluate the proposed algorithms, a series of numerical experiments have been conducted and the outputs of our proposed methods were compared with the Non-dominated Sorting Genetic Algorithm II (NSGA-II), Multi-Objective Imperialist Competitive Algorithm (MOICA), and Multi-Objective Fruit-Fly Optimization Algorithm (MOFFOA) in terms of several performance measures. Moreover, a real-life overhaul project in a gas treating company has been studied to demonstrate the practicality of the proposed model. The results of all numerical experiments demonstrate that the P-GWO outperforms other algorithms in terms of most of the metrics. The outputs imply that the MOFA can generate high quality solutions within a reasonable computation time.																	0924-669X	1573-7497				JUL	2020	50	7					2151	2176		10.1007/s10489-020-01663-x		MAR 2020											
J								Residual learning based densely connected deep dilated network for joint deblocking and super resolution	APPLIED INTELLIGENCE										Deblocking; Dilated convolution; Field-of-view; JPEG compression; Skip connection; Super resolution	IMAGE SUPERRESOLUTION; QUALITY ASSESSMENT	In many practical situations, images are not only down sampled but also compressed for efficient transmission and storage. JPEG and MPEG-2 compressions often introduce blocking artifacts because they process the data as 8 x 8 blocks. Many of the existing super resolution (SR) methods assume low resolution images as a down sampled version of high resolution (HR) image, and neglect the degradation due to compression. This exacerbates artifacts in the SR image and reduces the user experience. To address the joint deblocking and SR (DbSR), a novel deep network with dense skip connections and dilated convolutions is proposed in this paper, and we name it as DenseDbSR. Recently, many researchers have proposed deeper networks and achieved improvement in the SR performance. However, training deeper networks is very challenging because of the problem of vanishing gradients. Simply increasing the depth of the network leads to cumbersome computational costs. To enlarge the field-of-view (FOV) without increasing the computational cost, the dilated convolution is used. The dilated convolution exponentially expands the FOV and helps to exploit the contextual information efficiently. Moreover, the dense skip connections create short paths for gradients to be back-propagated efficiently and alleviates the problem of vanishing gradients. Furthermore, the network is relieved from the training burden by learning residuals of the SR image instead of learning raw images. From the conducted extensive experimentation, the proposed DenseDbSR network produced better performance in terms of PSNR and SSIM than the compared state-of-the-art methods.																	0924-669X	1573-7497				JUL	2020	50	7					2177	2193		10.1007/s10489-020-01670-y		MAR 2020											
J								The classification and denoising of image noise based on deep neural networks	APPLIED INTELLIGENCE										Image denoising; Deep learning; Classification; PSNR; SSIM	SPARSE; REPRESENTATIONS	Currently, image denoising is a challenge in many applications of computer vision. The existing denoising methods depend on the information of noise types or levels, which are generally classified by experts. These methods have not applied computational methods to pre-classify the image noise types. Furthermore, some methods assume that the noise type of the image is a certain one like Gaussian noise, which limits the ability of the denoising in real applications. Different from the existing methods, this paper introduces a new method that can classify and denoise not only a certain type noise but also mixed types of noises for real demand. Our method utilizes two types of deep learning networks. One is used to classify the noise type of the images and the other one performs denoising based on the classification result of the first one. Our framework can automatically denoise single or mixed types of noises with these efforts. Our experimental results show that our classification network achieves higher accuracy, and our denoising network can ensure higher PSNR and SSIM values than the existing methods.																	0924-669X	1573-7497				JUL	2020	50	7					2194	2207		10.1007/s10489-019-01623-0		MAR 2020											
J								FRWCAE: joint faster-RCNN and Wasserstein convolutional auto-encoder for instance retrieval	APPLIED INTELLIGENCE										Instance-level retrieval; Convolutional auto-encoder; Wasserstein distance; Dimensionality reduction	IMAGE; DIMENSIONALITY; FEATURES	Based on the powerful feature extraction capability of deep convolutional neural networks, image-level retrieval methods have achieved superior performance compared to the hand-crafted features and indexing algorithms. However, people tend to focus on foreground objects of interest in images. Locating objects accurately and using object-level features for retrieval become the essential tasks of instance search. In this work, we propose a novel instance retrieval method FRWACE, which combines the Faster R-CNN framework for object-level feature extraction with a brand-new Wasserstein Convolutional Auto-encoder for dimensionality reduction. In addition, we propose a considerate category-first spatial re-rank strategy to improve instance-level retrieval accuracy. Extensive experiments on four large datasets Oxford 5K, Paris 6K, Oxford 105K and Paris 106K show that our approach has achieved significant performance compared to the state-of-the-arts.																	0924-669X	1573-7497				JUL	2020	50	7					2208	2221		10.1007/s10489-019-01625-y		MAR 2020											
J								Distributed learning automata-based scheme for classification using novel pursuit scheme	APPLIED INTELLIGENCE										Classification; Learning automata; Polygons	ANT COLONY OPTIMIZATION; ALGORITHM; SIMULATION; MODEL; MINER; SET	Learning Automata (LA) is a popular decision making mechanism to "determine the optimal action out of a set of allowable actions" (Agache and Oommen, IEEE Trans Syst Man Cybern-Part B Cybern 2002(6): 738-749, 2002). The distinguishing characteristic of automata-based learning is that the search for the optimising parameter vector is conducted in the space of probability distributions defined over the parameter space, rather than in the parameter space itself (Thathachar and Sastry, IEEE Trans Syst Man Cybern-Part B Cybern 32(6): 711-722, 2002). Recently, Goodwin and Yazidi pioneered the use of Ant Colony Optimisation (ACO) for solving classification problems (Goodwin and Yazidi 2016). In this paper, we propose a novel classifier based on the theory of LA. The classification problem is formulated as a deterministic optimization problem involving a team of LA that operate collectively to optimize an objective function. Although many LA algorithms have been devised in the literature, those LA schemes are not able to solve deterministic optimization problems as they suppose that the environment is stochastic. In this paper, we develop a novel pursuit LA which can be seen as the counterpart of the family of pursuit LA developed for stochastic environments (Agache and Oommen, IEEE Trans Syst Man Cybern Part B Cybern 32(6): 738-749, 2002). While classical pursuit LA are able to pursue the action with the highest reward estimate, our pursuit LA rather pursues the collection of actions that yield the highest performance. The theoretical analysis of the pursuit scheme does not follow classical LA proofs and can pave the way towards more schemes where LA can be applied to solve deterministic optimization problems. When applied to classification, the essence of our scheme is to search for a separator in the feature space by imposing a LA based random walk in a grid system. To each node in the gird we attach an LA, whose actions are the choice of the edges forming the separator. The walk is self-enclosing, i.e., a new random walk is started whenever the walker returns to starting node forming a closed classification path yielding a multiedged polygon. In our approach, the different LA attached at the different nodes search for a polygon that best encircles and separates each class. Based on the obtained polygons, we perform classification by labelling items encircled by a polygon as part of a class using ray casting function. Seen from a methodological perspective, PolyPursuit-LA has appealing properties compared to SVM. In fact, unlike PolyPursuit-LA, the SVM performance is dependent on the right choice of kernel function (e.g. Linear Kernel, Gaussian Kernel)- which is considered a "black art". PolyPursuit-LA can find arbitrarily complex separators in the feature space. Experimental results from both synthetic and real-life data show that our scheme is able to perfectly separate both simple and complex patterns outperforming existing classifiers, including polynomial and linear SVM, without the need of any "kernel trick". We believe that the results are impressive given the simplicity of PolyPursuit-LA compared to other approaches such as SVM.																	0924-669X	1573-7497				JUL	2020	50	7					2222	2238		10.1007/s10489-019-01627-w		MAR 2020											
J								Teach machine to learn: hand-drawn multi-symbol sketch recognition in one-shot	APPLIED INTELLIGENCE										Multi-symbol sketch recognition; Few-shot learning; Lifelong learning; Probabilistic inference		The ability to sequentially learn from few examples and re-utilize previous knowledge is an important milestone on the path to artificial general intelligence. In this paper, we propose Teach Machine to Learn (TML), a few-shot learning model for hand-drawn multi-symbol sketch recognition. The model decomposes multi-symbol sketch into stroke primitives and then explains the observed sequences in a bayesian criterion. A Bidirectional Long Short Term Memory (BiLSTM) encoder is employed for stroke primitives encoding. Meanwhile, a probabilistic Hidden Markov Model (HMM) is constructed for complete sketch inference and recognition. The challenging task of hand-drawn multi-symbol sketch recognition is implemented on two public datasets. The comparative results indicate that the proposed method outperforms the currently booming image-based deep models in recognition accuracy. Furthermore, our method is capable to continuously learn new concepts even in one-shot. The codes are currently available in .																	0924-669X	1573-7497				JUL	2020	50	7					2239	2251		10.1007/s10489-019-01607-0		MAR 2020											
J								A new multiple attribute decision making method for selecting design schemes in sponge city construction with trapezoidal interval type-2 fuzzy information	APPLIED INTELLIGENCE										MADM; Sponge city construction; TIT2FN; Choquet integral; Generalized Shapley function	HAMACHER AGGREGATION OPERATORS; BONFERRONI MEAN OPERATOR; GREEN SUPPLIER SELECTION; SETS; MODEL; NUMBERS; SYSTEM; DEMATEL	Selecting the most suitable design scheme in sponge city construction can be seen as a multiple attribute decision-making (MADM) problem. To express the uncertain and fuzzy decision-making information, interval type-2 fuzzy sets (IT2FSs) are useful tools. The paper focuses on decision making with trapezoidal interval type-2 fuzzy numbers (TIT2FNs) and discusses the evaluation of municipal road design schemes in sponge city construction. To do these, several operations on TIT2FNs based on Hamacher t-norm and t-conorm are first defined, where both the operations on the membership degree and on eight non-negative real values are considered. Then, two (2-additive) generalized Shapley trapezoidal interval type-2 fuzzy Hamacher Choquet integral operators are presented, which globally reflect interactions among elements. Considering the case where the decision-making weighting information is incomplete known, Manhattan distance measure-based models for obtaining the optimal fuzzy measure and 2-additive measure are constructed, respectively. Furthermore, an approach for trapezoidal interval type-2 fuzzy MADM is developed. Finally, a practical example is provided to illustrate the utilization of the new method, and comparison analysis is provided.																	0924-669X	1573-7497				JUL	2020	50	7					2252	2279		10.1007/s10489-019-01608-z		MAR 2020											
J								Dense-CaptionNet: a Sentence Generation Architecture for Fine-grained Description of Image Semantics	COGNITIVE COMPUTATION										Image captioning; Semantics understanding; Sentence generation; Recurrent neural networks; LSTM		Automatic image captioning, a highly challenging research problem, aims to understand and describe the contents of the complex scene in human understandable natural language. The majority of the recent solutions are based on holistic approaches where the scene is described as a whole, potentially losing the important semantic relationship of objects in the scene. We propose Dense-CaptionNet, a region-based deep architecture for fine-grained description of image semantics, which localizes and describes each object/region in the image separately and generates a more detailed description of the scene. The proposed network contains three components which work together to generate a fine-grained description of image semantics. Region descriptions and object relationships are generated by the first module, whereas the second one generates the attributes of objects present in the scene. The textual descriptions obtained as an output of the two modules are concatenated to feed as an input to the sentence generation module, which works on encoder-decoder formulation to generate a grammatically correct but single line, fine-grained description of the whole scene. The proposed Dense-CaptionNet is trained and tested using Visual Genome, MSCOCO, and IAPR TC-12 datasets. The results establish a new state-of-the-art when compared with the existing top performing methodologies, e.g., Up-Down-Captioner, Show, Attend and Tell, Semstyle, and Neural Talk, especially on complex scenes. The implementation has been shared on GitHub for other researchers:																	1866-9956	1866-9964															10.1007/s12559-019-09697-1		MAR 2020											
J								Emoji Helps! A Multi-modal Siamese Architecture for Tweet User Verification	COGNITIVE COMPUTATION										Authorship verification; Siamese neural network; Multimodality; Twitter	AUTHORSHIP VERIFICATION	In the current paper, we have proposed a new multi-modal authorship verification approach for social media texts. Authorship verification is a task of verifying whether an unknown text is written by a suspect or not. Use of social media like Facebook and Twitter is increasing day by day because of digitization. People have grown accustomed to regularly post or tweet about their everyday life, memorable incidences, random thoughts, opinions, and much more. Emojis are widely used in these tweets and posts. The writing style of a user can differ from others, since word choices, sentence structures, usage of punctuation symbols, and use of emoji can be different. We have applied a multi-modal Siamese-based framework for automatic extraction of features from the given texts and emojis. After the extraction of features, the extracted features are applied to a neural network-based architecture for binary classification. A multi-modal Twitter-based dataset is created for evaluating the performance of the proposed framework. We obtained an average accuracy of 61.56% with 78.08%, 61.50%, and 58.32% precision, recall, and f-measure values, respectively.																	1866-9956	1866-9964															10.1007/s12559-020-09715-7		MAR 2020											
J								A novel authentication and authorization scheme in P2P networking using location-based privacy	EVOLUTIONARY INTELLIGENCE										Location-based privacy; Authentication; Authorization; P2P network; Fuzzy	FRAMEWORK; SERVICE; SYSTEM	In recent years, peer-to-peer (P2P) network has reached popularity in file sharing as it is a distributed and decentralized network architecture. As there is no centralized authority, there arise various attacks, which lead to insecurity in the network. Thus, the security issues of the P2P networks are to be considered with more care. This paper proposes an authentication and authorization approach, named fuzzy enabled advanced encryption standard (AES)-based multi-level authentication and authorization to offer security against various kinds of attacks that occur in the P2P networks. Here, the authentication is carried out with the security factors, namely location profile, one-time password, spatial information, session password, a hashing function, and so on. Initially, the user and the server are registered in the authentication process, and then, hashing functions and AES are used to perform multi-level authorization and authentication processes. Thus, the proposed scheme improves the security of the P2P network. Using the proposed system, the hit ratio obtained is 0.9, and the success rate is 0.7666.																	1864-5909	1864-5917															10.1007/s12065-020-00375-y		MAR 2020											
J								Chatbot design issues: building intelligence with the Cartesian paradigm	EVOLUTIONARY INTELLIGENCE										Artificial intelligence; Neural networks; Philosophy of consciousness; Model of consciousness		The article discusses the functioning of human-like consciousness and the potential for developing a chatbot based on human-like consciousness. The proposed approach was verified experimentally using a sociological method and by attracting a cohort of student volunteers. The chatbot population was created on the back of our complex neural network architecture design. The volunteers were asked to identify their interlocutor, which was either a human agent or a chatbot. For integrity, the conversations between bots and people were organized randomly so that each volunteer could interact several times with all bots in the population and with all participants in the sample. The article discusses the results of the study, the details of the proposed approach. The article explains the features of the functioning and self-reconfiguration of the neural network that provide high reliability of chatbot replicas and high speed of responses to replicas of human users so that the delay time does not raise suspicion of human users. The main idea of the authors' approach is an attempt to model human self-awareness and self-reflection. The results prove the proposed neural network architecture design successful in terms of real-time self-learning.																	1864-5909	1864-5917															10.1007/s12065-020-00358-z		MAR 2020											
J								Sea lion with enhanced exploration phase for optimization of polynomial fitness with SEM in lean technology	EVOLUTIONARY INTELLIGENCE										Lean manufacturing; SEM; CFA; Regression analysis; SLnO-EE	6 SIGMA; PERFORMANCE; SUSTAINABILITY; IMPLEMENTATION	With the establishment of lean manufacturing, myriad industries implemented the lean manufacturing principles and guidelines. To train the professionals based on certain policies to achieve continuous improvements in terms of productivity and minimizing wastages. However, the complexities such as variability, sustainability, multi-dimensional views, factory size, to name just a few negatively influences the performance of deploying lean manufacturing in industries. It is, therefore, very important for companies to recognize and understand the critical success factors for successfully implementing lean manufacturing. Hence, this paper plans to develop a model on concerning the analysis of lean manufacturing to find the most important factor regarding the technology among the industries. With this intention, this paper is analyzed through three phases. In the first phase, the prepared questionnaire is distributed to the professionals in various companies. In the questionnaire, all the mandatory questions are included. Then, the professional are recommended to fill the precise information as far as possible. In the second phase, the responses from the concerned practitioners associated with the industries are considered for analysis. Herein, the analysis is carried out based on structural equation modeling approaches with the contribution of higher order statistical analysis, which is performed using the input factors such as the lean awareness, lean technology, organizational support, organizational performance, employee involvement and management commitment among the industries via attaining better maximum likelihood values of the questionnaires. In the third phase, the Prediction of polynomial fitting of the response (i.e. the objective function) is achieved with the aid a novel optimization algorithm SLnO-EE model (Sea Lion with Enhanced Exploration phase), which is the extended version of SLnO (Sea Lion Optimization). Finally, the proposed SLnO-EE model is evaluated over traditional SLnO model in terms of certain performance evaluations to exhibit the improvement in prediction accuracy.																	1864-5909	1864-5917															10.1007/s12065-020-00370-3		MAR 2020											
J								An option contract on emergency material reserve considering quality catastrophic change	EVOLUTIONARY INTELLIGENCE										Option contract; Emergency material reserve; Quality catastrophic change; Contingency strategy		This paper investigates an option contract for emergency material reserve because of quality catastrophic change. The quality catastrophic change happens randomly and the realized number can not be observed until it happened. Thus, it is necessary to reserve some emergency material beforehand. However, the demander is not qualified to make the storage. Meanwhile, the supplier has no motivation to make storage for the demander. Thus, an option contract is proposed to encourage the supplier to make backup for the demander and share the shortage cost if the backup less than the realized option order quantity. Three models are discussed: the centralized model without option, the decentralized model without option, and the decentralized model with option. The results demonstrate that the order quantity for the first time under the centralized scenario is greater than that under the decentralized model without option. And, the order of the purchase quantity for the first time under the three scenarios depend on the parameters decided by the both players. Meanwhile, through analysing, the effective range of option fee is proved to be limited to an interval.																	1864-5909	1864-5917															10.1007/s12065-020-00371-2		MAR 2020											
J								A general conflict analysis model based on three-way decision	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Conflict analysis; Three-valued situation table; Three-way decision; Trisection	ROUGH SET; CONCEPT LATTICE	In a three-valued situation table, researchers have investigated the relationship between two agents by using auxiliary and distance functions, and proposed several models of conflict analysis for trisecting the set of all pairs of agents. But we have not observed a unification of these conflict analysis models. In this paper, we introduce the concepts of conditional alliance and conflict evaluations of two agents towards a subset of issues. In the framework of three-way decision, we propose a general model of three-way conflict analysis, and study three levels of conflict between two agents. We illustrate that five models of conflict analysis for trisecting the set of all pairs of agents are special cases of the general model, and provide a deep insight into the models of conflict analysis based on three-way decision.																	1868-8071	1868-808X				MAY	2020	11	5					1083	1094		10.1007/s13042-020-01100-y		MAR 2020											
J								W-Metagraph2Vec: a novel approval of enriched schematic topic-driven heterogeneous information network embedding	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Heterogeneous information network; Meta-path; Meta-graph; Network embedding	SIMILARITY SEARCH	Recently, heterogeneous information network (HIN) embedding is wide studied due to its various applications. In general, network embedding is a way of representation network's nodes into a low-dimensional space. Most of previous embedding techniques concentrate on the homogeneous networks only in which all nodes are considered as a single type. Heterogeneous network embedding is a challenging problem due to the complexity of different node's types and link's types. Recent heterogeneous network embedding studies are based on meta-path and meta-graph to guide the random walks over the networks. These heterogeneous network embedding approaches outperform state-of-the-art homogeneous embedding models in multiple heterogeneous network mining tasks. However, recent meta-graph-based approaches are ineffective in capturing topic similarity between nodes. There is no doubt that most of common HINs (DBLP, Facebook, etc.) are rich-text which contain many text-based nodes, such as paper, comment, post, etc. In this paper, we propose a novel embedding approach, namely W-MetaGraph2Vec. The W-MetaGraph2Vec uses the topic-driven meta-graph-based random walk mechanism in weighted HIN to guide the generation of heterogeneous neighborhood of a node. Extensive experiments on real-world datasets demonstrate that our proposed model not only leverage HIN mining tasks, such as node similarity search, clustering, classification, etc. in performance accuracy but also discern the problems of topic relevance between text-based nodes.																	1868-8071	1868-808X				AUG	2020	11	8					1855	1874		10.1007/s13042-020-01076-9		MAR 2020											
J								Image annotation: the effects of content, lexicon and annotation method	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Image annotation; Crowdsourcing; Manual annotation; Annotation quality	AGREEMENT; DATASETS; MODEL	Image annotation is the process of assigning metadata to images, allowing effective retrieval by text-based search techniques. Despite the lots of efforts in automatic multimedia analysis, automatic semantic annotation of multimedia is still inefficient due to the problems in modeling high-level semantic terms. In this paper, we examine the factors affecting the quality of annotations collected through crowdsourcing platforms. An image dataset was manually annotated utilizing: (1) a vocabulary consists of preselected set of keywords, (2) an hierarchical vocabulary and (3) free keywords. The results show that the annotation quality is affected by the image content itself and the used lexicon. As we expected while annotation using the hierarchical vocabulary is more representative, the use of free keywords leads to increased invalid annotation. Finally, it is shown that images requiring annotations that are not directly related to their content (i.e., annotation using abstract concepts) lead to accrue annotator inconsistency revealing in that way the difficulty in annotating such kind of images is not limited to automatic annotation, but it is a generic problem of annotation.																	2192-6611	2192-662X				SEP	2020	9	3					191	203		10.1007/s13735-020-00193-z		MAR 2020											
J								A classification model to predict onset of smoking and drinking habits based on socio-economic and sociocultural factors	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Logistic regression; Alcohol and other drugs (AODs); Drinking; Smoking; Habits formation; Peer pressure; Socio-economic factors; Socio-cultural factors; Deep learning	CIGARETTE-SMOKING; ALCOHOL-USE; DRUG-USE; ADOLESCENTS; STRESS	Addictive habits are often initiated due to peer pressure. Smoking, drinking, and exercising are examples of such habits that are also performed as a part of socializing activities. Past researches have tried to predict the early onset of smoking and drinking problems using machine learning. However, these researches were mainly based on daily stress levels, and mood. Taking daily measures is inconvenient for a surveyor. The studies have also failed to account for socio-cultural and socio-economic factors which also play an important in the onset of these behaviours. Availability of items like tobacco and alcohol can significantly impact the onset of early alcohol and smoking in adolescents. In this paper, we analyze how socio-economic such as lifestyle, monthly savings, and socio-cultural factors like size of friends group, number of friends that drink and smoke, details about parents, etc., play a role in the initiation and cultivation of addictive behaviours and use a machine learning approach to predict the early onset of such behaviours. We compared Gaussian Naive Bayes, Support Vector Machine and Logistic Regression algorithms in order to train and predict our multi-classifier prediction system. We found Logistic Regression to be the best performing classifier to predict both drinking and smoking habits with 86.4% and 97.2% accuracies respectively. We also achieved an F1 measure of 0.76 for the drinking classifier and an F1 measure of 0.85 for the smoking classifier.																	1868-5137	1868-5145															10.1007/s12652-020-01796-4		MAR 2020											
J								Hypervisor injection attack using X-cross API calls (HI-API attack)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Malware; Data security; Privacy; Antimalware; API; Hypervisor; Firmware	MALWARE DETECTION	Progressive cyber-attacks emphasize secrecy and industriousness the more they are able to move alongside, exfiltration of information and cause harm. The more they stay under radar. The abusers swing progressively to cross-process infusion to preserve a strategic distance from identification. Cross-process infusion helps attackers to execute malicious codes that take on truly project appearance. Code infusion does not require aggressors to use specific procedures that can be quickly differentiated. Alternatively, they incorporate malignancy code into the normal procedure, allowing their operations a wider range of secrecy and naivety (e.g. explorer.exe, regsvr32.exe, svchost.exe horizontal ellipsis ). For the purpose of detect malware injection The hypervisor injection attack proposed in this paper by using a method of X-cross application programming interface calls (API-HI-attack) raises awareness that malware is injecting into the simulation tool with X-cross-language API calls. The experimental results of the proposed work shows antimalware protector need to take more attention on API call hooking at process level injection by X-cross languages. The results proposed method with high true positive (92.96%) and less false positive (0.07%) over the existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-01837-y		MAR 2020											
J								An exploratory study on computer-aided affective product design based on crowdsourcing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Affective design; Product styling; Kansei Engineering; Crowdsourcing; Design evaluation	AFFECTIVE RESPONSES; FEATURES	This research investigates the possibility of incorporating affective factors in 3D model creation using handbags as an exemplary product. We demonstrate a crowd-based experimental approach that establishes the correlations between human emotional responses described by bi-polar adjectives and design factors of a bag. A factorial experiment is conducted to understand which design factors independently and/or interactively influence the responses by systematically varying 3D bag models. In-depth statistical analysis of the experimental results indicates that increasing the bag height makes it look more casual; a black bag tends to look luxurious and formal; a bag made of linen feels more classic and sophisticated. Those insights provide a basis for implementing a prototyping design tool that enables users to create 3D bag models with certain affective features using free-form deformation. The proposed approach demonstrates the preliminary feasibility of improving Kansei Engineering applications with big data obtained from crowdsourcing.																	1868-5137	1868-5145															10.1007/s12652-020-01821-6		MAR 2020											
J								Co-design of a TV-based home support for early stage of dementia	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										TV-based home support; Mild Cognitive Impairment; User-centered design; Assistive product prototype; Long term assessment; Health care systems	QUALITY-OF-LIFE; COGNITIVE IMPAIRMENT; INFORMAL CAREGIVERS; OLDER-PEOPLE; SMART HOME; INTERVENTIONS; TECHNOLOGY; DISEASE; VERSION; BURDEN	This article describes the work done to create an innovative system to support people with Mild Cognitive Impairment or Mild Dementia. The basic idea of the system is to exploit an extremely simple and familiar technology for older people, namely television (TV). In fact, the TV-AssistDem system exploits the smart-TV technology to provide a series of support services, among which the connection via video conference with healthcare professional or with relatives and friends; monitoring of vital parameters; cognitive stimulation and reminders of important medicines and appointments. The article describes the co-design approach of the prototype pursued during the project which involves a constant and repeated involvement of potential users of the system over time. The refined prototype of TV-AssistDem went under an ecological test in the homes of representative users to assess its robustness and compliance to users needs. The additional objective of the work is to carry out a long-term study that involves the realisation of a clinical trial in two European countries (Spain and Romania) aimed at showing the validity of the proposed solution on the patient's quality of life but also on caregivers and socio-healthcare systems.																	1868-5137	1868-5145															10.1007/s12652-020-01823-4		MAR 2020											
J								Inverse kinematics solution of Robotics based on neural network algorithms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Improved BP neural network algorithm; Robotics kinematics inverse solution algorithm; Excitation function; Learning rate	MODEL; SERIAL	The rapid development of artificial intelligence technology makes Robotics more intelligent and flexible. In this context, the kinematics inverse algorithm of the Robotics has become the basis and key technology for the further development of the Robotics. The key point of the inverse algorithm of the Robotics is to coordinate the Robotics operation arm with the corresponding action actuator at the end to realize the space attitude control of the Robotics system, and to make a theoretical basis for the motion analysis of the later Robotics. However, the traditional form of Robotics kinematics inverse algorithm avoids a lot of iterative computational solution process, which increases the complexity of the whole algorithm. Therefore, based on the above situation, this paper proposes a Robotics inverse solution algorithm based on improved BP (back propagation) neural network. In this paper, in the application of the actual algorithm, aiming at the convergence problem of the traditional BP neural network algorithm, an improved BP neural network algorithm based on the excitation function is proposed. By selecting the adaptive processing function in each layer of the neural network, the selection is matched with it. The learning rate, thus improving the accuracy of the entire motion inverse algorithm. At the same time, in order to further reduce the calculation of joint quantification, this paper also creatively introduces the algorithm of plane division auxiliary dynamic model construction. The simulation results show that the inverse kinematics algorithm based on improved BP neural network proposed in this paper has obvious advantages in solving the kinematics inverse problem of six-degree-of-freedom Robotics compared with the traditional inverse solution algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-01815-4		MAR 2020											
J								Cheater-identifiable homomorphic secret sharing for outsourcing computations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											ENCRYPTION; DELEGATION	Homomorphic secret sharing (HSS) allows a dealer to share a secret x among m participants such that: (1) any unauthorized subset of the participants learns no information about x; and (2) every participant in an authorized subset can perform the computation of a function f on its share to obtain a partial result and these partial results suffice to recover f(x). In a multi-client multi-server setting, HSS can be used to outsource the computation of a function f on the dealer's (clients') private inputs and thus resolve one of the main security issues in outsourcing computation, i.e., the privacy of the client's data. Tsaloli, Liang, and Mitrokotsa (ProvSec 2018) proposed a verifiable HSS (VHSS) model where the partial results of the servers can be verified, in order to resolve another main security issue in outsourcing computation, i.e., the integrity of the outsourced computation. They also constructed a VHSS scheme for computing the product of the dealers' private inputs such that any proper subset of the servers learns no information about the private inputs. In this paper, we present an easy attack of their scheme with which even a single server is able to distinguish between two different sets of private inputs. We propose a new VHSS model and construct a new VHSS scheme for computing the same function. By properly choosing the parameters, our scheme allows cheater detection, cheater identification, robust decoding, and extremely fast verification and result decoding.																	1868-5137	1868-5145															10.1007/s12652-020-01814-5		MAR 2020											
J								Advanced resilient data consigning mechanism for mobile adhoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MANET routing; ACO; Signature schemes; Hash value; Trust routing	HOC; SECURITY	Routing in network is a classic issue to be solved. To route the data among the nodes without any interruptions and without getting yield in any kind of malicious attack is a tedious task. Though there exist many routing techniques which were near success in the past some part of the problems are always left unsolved. Trust-based routing techniques and the bio-inspired path is connecting techniques area embedded together to solve this. The security of the proposed technique is given a big significance, which takes care of the rest of the key points. Initially, when a node tries to enter into the network which was already secured and confined by trustable nodes, we provide a two-way security check both from the new node and from the host node. This is made possible by applying digital signature algorithm, hash message authentication code, and message-digest algorithms for signature generations, hash value generation, and its corresponding encrypt and decrypt procedure. Elliptic curve Diffie-Hellman algorithm is used for certificate verification, after which the node is provided with the broadcast key. In the case of routing, ant colony optimization is applied to detect the best path for routing from root to destiny, and this path is again verified by trust frameworks and certain calculations. This highly secured verification and authentication process make the proposed advanced resilient data consigning mechanism (ARDC) stand out from the existing routing mechanisms. The proposed ARDC mechanism is analyzed against various network frameworks as throughput, packet loss, overhead, delivery ratio, delivery latency, and hop count and path reliability to prove its effectiveness. Further, the routing capability is also analyzed by comparing it against traditional DSR, destination-sequenced distance-vector and open shortest path first protocols.																	1868-5137	1868-5145															10.1007/s12652-020-01820-7		MAR 2020											
J								Video analytics for semantic substance extraction using OpenCV in python	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Video surveillance; Semantic content extraction	EVENT DETECTION; DATA CENTERS; REPRESENTATION; ANNOTATION; IMAGES; SYSTEM	With the rise of crimes all over the world, video surveillance is gaining more significance day by day. Presently, monitoring videos is done manually. If a crime occurs in a city, to find the sequence or event, it is necessary to play the entire video after which searching and processing needs to be done manually. Due to the lack of human resource, it is necessary to develop a new video analytics framework to perform higher level tasks in semantic content extraction. Manual processing of video is wasteful, one-sided, and more expensive thereby limiting the searching abilities. So it is necessary to model a framework for extracting objects from the video data. A Semantic Substance Extraction model using OpenCV is proposed for organizing video resources. Video analytics for semantic substance extraction is an effort to use real time, publicly available data to improve the prediction of the moving objects from the video streams. Background separation and Haar Cascade algorithms are used in this model to perform video analytics. Usage of this method has achieved a detection precision of 84.11% and a recall of 50.27%. These results are 78% faster than content extraction using existing fuzzy and neural methods.																	1868-5137	1868-5145															10.1007/s12652-020-01780-y		MAR 2020											
J								Improving QoS and efficient multi-hop and relay based communication frame work against attacker in MANET	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MANET; QoS; Random repeat trust computational approach; Trust management		The habit of using mobile devices increasing constantly, Considerably MANETs as the nodes are mobile, Trust management can help to improve the security in routing that guaranteed QoS provisioning in MANETs to achieve better deterministic behavior and appropriately the networks delivered the information in a better way and it can be well gain to exploit the network resources. Trust Calculation solves the problem of providing corresponding access control based on judging the quality of Sensor Nodes and their services and to analyze the route and alternate to route for efficient data transmission. This paper deals with the efficient approach based on multi-hop and relay dependent communication for enhancing the security. The improvement of QoS is based on Random Repeat Trust Computational Approach obtain a various trust evaluation Stages by estimating the direct and indirect trust degree to avoid the incorrect trust derivation problem and later than update the node trust of routing table as detection of malicious node subsequent to attain the trusted QoS routing of data transmission. Then it investigates the node location and distances among the nodes for data transmission to verify the false injection. To evaluate the trustworthy paths and nodes using to design and develop a trust based QoS routing integrated by Random Repeat Trust Computational Approach to improve QoS. Simulation results show that the progressing QOS and distrust worthy node detection of the proposed system more than 30% when compared to the existing system.																	1868-5137	1868-5145															10.1007/s12652-020-01787-5		MAR 2020											
J								Dynamic resource allocation with optimized task scheduling and improved power management in cloud computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Resource allocation; Task scheduling; DRA table; Power management	FRAMEWORK; MECHANISM; SYSTEMS	Cloud computing is one among the emerging platforms in business, IT enterprise and mobile computing applications. Resources like Software, CPU, Memory and I/O devices etc. are utilized and charged as per the usage, instead of buying it. A Proper and efficient resource allocation in this dynamic cloud environment becomes the challenging task due to drastic increment in cloud usage. Various promising technologies have been developed to improve the efficiency of resource allocation process. But still there is some incompetency in terms of task scheduling and power consumption, when the system gets overloaded. So an energy efficient task scheduling algorithm is required to improve the efficiency of resource allocation process. In this paper an improved task scheduling and an optimal power minimization approach is proposed for efficient dynamic resource allocation process. Using prediction mechanism and dynamic resource table updating algorithm, efficiency of resource allocation in terms of task completion and response time is achieved. This framework brings an efficient result in terms of power reduction since it reduces the power consumption in data centers. The proposed approach gives accurate values for updating resource table. An efficient resource allocation is achieved by an improved task scheduling technique and reduced power consumption approach. The Simulation result gives 8% better results when comparing to other existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-01794-6		MAR 2020											
J								Energy efficient data transmission in WSN thru compressive slender penetrative etiquette	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN-Wireless sensor network; IoT-Internet of things; QoS-Quality of service	WIRELESS SENSOR NETWORKS; INTRUSION DETECTION SYSTEM; ROUTING PROTOCOL; ALGORITHM; MAC	Internet of Things (IoT) becomes a fabulous domain as its ruling spreads throughout the universal domains. In IoT, Wireless Sensor Network (WSN) is the commonly used technology today. But when dealing with WSN, it is highly prone to data security and energy efficiency during data transmission, due to interference in transmission path and transmission of big size of data. Existing mechanisms have used multiple accessing techniques for effective transmission but failed in transmission due to less memory storage of sensor node and also lacked in preserving originality of the compressed data due to bad compression ratio. So, to deal with this scenario, a compressive slender penetrative etiquette is designed as our proposed work for effective transmission of data in WSNs that attains high data transmission rate with diminishing of interference and attenuation. The optimal way for transferring data with limited energy is achieved with a compressive sensing technique called Chaotic Demodulation Stratagem. It makes use of a Mock Random Generator which generates a high-speed sequence which multiplies the signal in the analog domain. The obtained output from the low pass filter is then supposed to sampling process at a rate lower than the Nyquist rate. Similarly, the other end of the signal reconstruction is done by means of the Convex relaxation iterative algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-01724-6		MAR 2020											
J								An intelligent cross layer security based fuzzy trust calculation mechanism (CLS-FTCM) for securing wireless sensor network (WSN)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensors network (WSN); Cross layer security; Fuzzy trust calculation mechanism (FTCM); Fault monitoring system; Enhanced convolutional neural network (ECNN) classifier		Wireless sensor networks (WSNs) have a significant number of sensing nodes with computing and communication resources for data transmission. Many essential aspects such as computational power, storage capability, and energy consumption must be taken into account to use WSNs for data transmission. Previous research work developed a cross layer security based intrusion detection system (CLS-IDS) framework that has improved network performance, minimal energy consumption and to prolong network life, meeting QoS requirements. However, this CLS-IDS method cannot detect the faults happening at various places at the same time. So, this research work proposes the cross layer security based fuzzy trust calculation mechanism (CLS-FTCM) and also least overhead monitoring for WSNs by means of memory and energy demands to resolve the above issue. This cross-layer security was designed with the aid of a fuzzy logic-based calculation method for a protocol called cross-layer protocol, uses multiple parameters extracted by the exchange of inter-layer information in order to alleviate the effects of security hazards to the network of wireless sensors. In order to identify malicious nodes on the network, the fault monitoring system is carried out using enhanced convolutional neural network (ECNN) classifier and the confidential value is determined for the trust values. The proposed cross layer security based fuzzy trust mechanism (CLS-FLTCM) is highly efficient technique to ensure optimum safety in wireless sensor environment when compared to the previous techniques. The simulation results conducted using the NS-2 simulator and testing the safety performance of the proposed system demonstrates that this approach is better than the current detection precision techniques.																	1868-5137	1868-5145															10.1007/s12652-020-01834-1		MAR 2020											
J								Design optimization by integrating limited simulation data and shape engineering knowledge with Bayesian optimization (BO-DK4DO)	JOURNAL OF INTELLIGENT MANUFACTURING										Bayesian optimization; Limited simulation data; Engineering knowledge; Surrogate model; Design optimization	GLOBAL OPTIMIZATION; ALGORITHM; AIRFOIL; MODEL	Surrogate models have been widely studied for optimization tasks in the domain of engineering design. However, the expensive and time-consuming simulation cycles needed for complex products always result in limited simulation data, which brings a challenge for building high accuracy surrogate models because of the incomplete information contained in the limited simulation data. Therefore, a method that builds a surrogate model and conducts design optimization by integrating limited simulation data and engineering knowledge through Bayesian optimization (BO-DK4DO) is presented. In this method, the shape engineering knowledge is considered and used as derivative information which is integrated with the limited simulation data with a Gaussian process (GP). Then the GP is updated sequentially by sampling new simulation data and the optimal design solutions are found by maximizing the GP. The aim of BO-DK4DO is to significantly reduce the required number of computer simulations for finding optimal design solutions. The BO-DK4DO is verified by using benchmark functions and an engineering design problem: hot rod rolling. In all scenarios, the BO-DK4DO shows faster convergence rate than the general Bayesian optimization without integrating engineering knowledge, which means the required amount of data is decreased.																	0956-5515	1572-8145															10.1007/s10845-020-01551-8		MAR 2020											
J								Defeasible Description Logics	KUNSTLICHE INTELLIGENZ										Knowledge representation and reasoning; Formal ontologies; Description logic; Defeasible reasoning; Preferential semantics; Rationality	COMPLEXITY	The present paper is a summary of a habilitation (Habilitation a Diriger des Recherches, in French), which has been perused and evaluated by a committee composed by the following members: Franz Baader, Stephane Demri, Hans van Ditmarsch, Sebastien Konieczny, Pierre Marquis, Marie-Laure Mugnier, Odile Papini and Leon van der Torre. It was defended on 26 November 2019 at Universite d'Artois in Lens, France.																	0933-1875	1610-1987															10.1007/s13218-020-00649-8		MAR 2020											
J								Negative correlation learning in the extreme learning machine framework	NEURAL COMPUTING & APPLICATIONS										Negative correlation learning; Extreme learning machine; Ensemble; Diversity	NEURAL-NETWORKS; REGRESSION; DIVERSITY; ALGORITHM; ADABOOST	Extreme learning machine (ELM) has shown to be a suitable algorithm for classification problems. Several ensemble meta-algorithms have been developed in order to generalize the results of ELM models. Ensemble approaches introduced in the ELM literature mainly come from boosting and bagging frameworks. The generalization of these methods relies on data sampling procedures, under the assumption that training data are heterogeneously enough to set up diverse base learners. The proposed ELM ensemble model overcomes this strong assumption by using the negative correlation learning (NCL) framework. An alternative diversity metric based on the orthogonality of the outputs is proposed. The error function formulation allows us to develop an analytical solution to the parameters of the ELM base learners, which significantly reduce the computational burden of the standard NCL ensemble method. The proposed ensemble method has been validated by an experimental study with a variety of benchmark datasets, comparing it with the existing ensemble methods in ELM. Finally, the proposed method statistically outperforms the comparison ensemble methods in accuracy, also reporting a competitive computational burden (specially if compared to the baseline NCL-inspired method).																	0941-0643	1433-3058				SEP	2020	32	17					13805	13823		10.1007/s00521-020-04788-9		MAR 2020											
J								On self-organised aggregation dynamics in swarms of robots with informed robots	NEURAL COMPUTING & APPLICATIONS										Swarm robotics; Aggregation; Self-organisation; Informed individuals	COLLECTIVE DECISION; FLOCKING; BEHAVIOR	Aggregation is a process observed in natural systems whereby individuals gather together to form large cluster. Recent studies with cockroaches and robots have shown that relatively simple individual mechanisms can account for how individuals manage to gather on a single shelter when two or more are available in the environment. In this paper, we use simulated swarms of robots to further explore the aggregation dynamics generated by these simple individual mechanisms. Our objective is to study the introduction of "informed robots", and to study how many of these are needed to direct the aggregation process towards a pre-defined site among those available in the environment. Informed robots are members of a group that selectively avoid the site/s where no aggregate should emerge and stop only on the experimenter pre-defined site/s for aggregation. We study the aggregation process with informed robots in three different scenarios: two that are morphologically symmetric, whereby the different types of aggregation site are equally represented in the environment; and an asymmetric scenario, whereby the target site has an area that is half the area of the sites that should be avoided. We first show what happens when no robot in the swarm is informed: in symmetric environments, the swarm is able to break the symmetry and aggregates on one of the two types of site at random, not necessarily on the target site, while in the asymmetric environment, the swarm tends to aggregate on the sites that are most represented in terms of area. The original contribution of this study is to demonstrate the effect of the introduction of a small proportion of informed robots in both environments: In symmetric environments, they selectively direct the aggregation process towards the experimenter chosen site; in the asymmetric environment, informed robots can invert the spontaneous preference for the most represented site and induce the swarm to aggregate on the least represented type of site. Moreover, for each scenario, we analyse how the dynamics of the aggregation process depends on the proportion of informed robots. As a further valuable contribution of this study, we provide analytical results by studying a system of ordinary differential equations that is an extension of a well-known model. Using this model, we show how, for certain values of the parameters, the model can predict the dynamics observed with simulated robots in one of the two symmetric scenarios.																	0941-0643	1433-3058				SEP	2020	32	17					13825	13841		10.1007/s00521-020-04791-0		MAR 2020											
J								Airfoil-slat arrangement model design for wind turbines in fuzzy environment	NEURAL COMPUTING & APPLICATIONS										Wind turbine; Airfoil-slat arrangement; Fuzzy logic; Fuzzy linear regression; Fuzzy linear programming; 2(k) Factorial design	LINEAR-REGRESSION ANALYSIS	In this study, a multi-element wind turbine blade that consists of NACA 6411 and NACA 4412 leading-edge slat design is investigated computationally. Optimum design parameters of the slatted wind turbine blade leading to maximum value of C-L/C-D related to the turbine power are obtained. In the optimization process, a new fuzzy logic linear programming methodology integrating with fuzzy linear regression and 2D CFD analysis is proposed. The aerodynamic characteristics of the slatted blade are computed by using Incompressible Navier-Stokes equations and k-omega turbulence modeling. Results are compared with the results of linear programming method and direct search optimization method. The computational results reveal that the proposed methodology for performance optimization is more effective than other methods to obtain high-performance value of the C-L/C-D. The maximum value of the C-L/C-D is obtained as 25.1 leading the maximum efficiency of 0.52.																	0941-0643	1433-3058				SEP	2020	32	17					13931	13939		10.1007/s00521-020-04796-9		MAR 2020											
J								Prediction of actual evapotranspiration by artificial neural network models using data from a Bowen ratio energy balance station	NEURAL COMPUTING & APPLICATIONS										Evapotranspiration; Bowen ratio energy balance method; Artificial neural networks; Activation function; Learning algorithm; ReLU	LIMITED CLIMATIC DATA; LATENT-HEAT FLUX; EQUATIONS	Seven artificial neural network (ANN) models were developed to predict daytime actual evapotranspiration (ET) for Nissouri Creek in Oxford County, Canada, from April to July 2018, using the Bowen ratio energy balance method as target output for the first time. In total, 12 variations of each model were deployed using different combinations of model parameters, including the sigmoid and rectified linear unit (ReLU) activation functions, stochastic gradient descent (SGD), and root-mean-square-propagation (RMSprop) learning algorithms, three different network architectures, and 100 and 500 training epochs. This is the first time that ReLU has been used in ANNs that predict ET and it outperformed sigmoid in six of the seven models. This is particularly significant because until now the sigmoid activation function (or variations therein) had been exclusively employed in the ET literature. RMSprop was also used for the first time and typically demonstrated equivalent performance to that of SGD. The optimal model employs the ReLU activation function, consists of a 4-4-1 network architecture, includes the input parameters of net radiation, air temperature, soil heat flux, and wind speed, and is trained by the SGD learning algorithm for 500 training epochs. This model boasts a coefficient of determination (R-2) of 0.997, root-mean-square error (RMSE) of 0.39 mm/day, and mean absolute error (MAE) of 0.18 mm/day. Furthermore, all seven models developed adequately model the ET process, with R-2 ranging from 0.988 to 0.997, RMSE from 0.39 to 0.78 mm/day, and MAE from 0.18 to 0.58 mm/day.																	0941-0643	1433-3058				SEP	2020	32	17					14001	14018		10.1007/s00521-020-04800-2		MAR 2020											
J								Multi-attribute group decision-making using double hierarchy hesitant fuzzy linguistic preference information	NEURAL COMPUTING & APPLICATIONS										COPRAS method; Double hierarchy; Group decision-making; Maclaurin symmetric mean; Programming model	COMPLEX PROPORTIONAL ASSESSMENT; TERM SETS; SUPPLIER SELECTION; AGGREGATION OPERATORS; SIMILARITY MEASURES; COPRAS; TOPOLOGY; DISTANCE; TOPSIS	Double hierarchy hesitant fuzzy linguistic term set (DHHFLTS) is one of the successful extensions of the hesitant fuzzy linguistic term set used for describing the uncertain information in a more prominent manner for solving the group decision-making problems. In DHHFLTS, the membership functions are represented in terms of linguistic membership degrees which are a flexible structure for preference elicitation and enrich the ability for rational decision-making with complex linguistic expressions. Driven by the flexibility of DHHFLTS, in this paper, a new decision framework is developed for solving decision-making problems, which provides scientific and rational decisions based on the preference information. For it, initially, a new aggregation operator is proposed for aggregating decision-makers' preferences. Later, the importance of the attribute weights in the problems is determined by formulating a mathematical model and the COPRAS method is extended to the DHHFLTS context for prioritizing alternatives. The applicability of the presented approach is demonstrated through a numeric example related to green supplier selection. A comparative analysis with existing studies is also administered to test the effectiveness and verify the method.																	0941-0643	1433-3058				SEP	2020	32	17					14031	14045		10.1007/s00521-020-04802-0		MAR 2020											
J								Deep neural learning techniques with long short-term memory for gesture recognition	NEURAL COMPUTING & APPLICATIONS										Gesture recognition; Shift invariant; Convolutional deep structured; Neural learning; Long short-term memory; Bivariate fully recurrent deep neural network	SENSOR DATA	Gesture recognition is a kind of biometric which has assumed great significance in the field of computer vision for communicating information through human activities. To recognize the various gestures and achieve efficient classification, an efficient computational machine learning technique is required. The Shift Invariant Convolutional Deep Structured Neural Learning with Long Short-Term Memory (SICDSNL-LSTM) and Bivariate Fully Recurrent Deep Neural Network with Long Short-Term Memory (BFRDNN-LSTM) have been introduced for classifying human activities with efficient accuracy and minimal time complexity. The SICDSNL-LSTM technique collects gesture data (a kind of biometric) from the dataset and gives it to the input layers of Shift Invariant Convolutional Deep Structured Neural Learning. The SICDSNL-LSTM technique uses two hidden layers for performing regression and classification. In the regression process, dice similarity is used for measuring the relationship between data and output classes. In the second process, the input data are classified into dissimilar classes for each time step using LSTM unit with soft-step activation function. The soft-step activation function uses 'forget gate' for removing the less significant data from the cell state. This is also used to make a decision to display the output at a particular time step and to remove other class results. Then, LSTM output is given to the output layers, and convolutional deep neural learning is performed to classify the gesture. Based on the classification results, human activity and gesture are recognized with high accuracy. The BFRDNN-LSTM technique also performs regression in the first hidden layers using bivariate correlation to find relationship between data and classes. The LSTM unit in BFRDNN-LSTM technique uses Gaussian activation function in the second hidden layers for categorizing incoming data into various classes at each time step. In this proposed BFRDNN-LSTM method, fully recurrent deep neural network utilizes gradient descent function to minimize the error rate at the output layers and to increase the accuracy of the gesture recognition. Both SICDSNL-LSTM and BFRDNN-LSTM techniques automatically learn the features and the data to minimize time complexity in gesture recognition. Experimental evaluation is carried out using factors such as gesture recognition accuracy, false-positive rate and time complexity with a number of data.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16073	16089		10.1007/s00521-020-04742-9		MAR 2020											
J								Global Exponential Stability of High-Order Bidirectional Associative Memory (BAM) Neural Networks with Proportional Delays	NEURAL PROCESSING LETTERS										High-order BAM neural networks; Global exponential stability; Proportional delays; Delay differential inequality	ALMOST-PERIODIC SOLUTIONS; ASYMPTOTIC STABILITY; DISTRIBUTED DELAYS; NEUTRAL TYPE; IMPULSES; CRITERIA	This paper considers the global exponential stability (GES) of high-order bidirectional associative memory (BAM) neural networks with proportional delays. Here, proportional delays are unbounded time-varying delays, which are different from constant delays, bounded time-varying delays and distributed delays. Through variable transformations, the original system can be transformed equivalently into high-order BAM neural networks with multi-constant delays and time-varying coefficients. By utilizing Brouwer's fixed point theorem and constructing appropriate delay differential inequalities, new sufficient criteria are established to guarantee the existence, uniqueness and GES of the equilibrium point for the considered model. Finally, two examples with numerical simulations are presented to demonstrate the effectiveness of the proposed results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2531	2549		10.1007/s11063-020-10206-x		MAR 2020											
J								Traffic Sign Recognition in Harsh Environment Using Attention Based Convolutional Pooling Neural Network	NEURAL PROCESSING LETTERS										Convolutional neural network; Traffic sign recognition; Attention mechanism; Max pooling; Convolutional pooling		Convolutional neural networks (CNNs) have achieved significant progress in computer vision systems, helping to efficiently obtain feature information by sliding filters on the input images. However, CNNs have difficulty capturing specific properties when the images are affected by various noises. This paper proposes an attention based convolutional pooling neural network (ACPNN) where an attention-mechanism is applied to feature maps to obtain key features, and max pooling is replaced with convolutional pooling to improve recognition accuracy in harsh environments. The ACPNN with attention mechanism and convolutional pooling structure is robust against external noises and maintains classification performance under such conditions. The proposed ACPNN was validated on the German traffic sign recognition benchmark with various cases. Considering the traffic signs are suffered from various noises, the recognition performances were demonstrated with conventional CNN and state-of-the art CNNs such as multi-scale CNN, committee of CNN, hierarchical CNN, and multi-column deep neural network. Under such harsh conditions, the proposed ACPNN shows 66.981% and 83.198% respectively, which are the best performances compared to other CNNs.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2551	2573		10.1007/s11063-020-10211-0		MAR 2020											
J								Infrared Image Extraction Algorithm Based on Adaptive Growth Immune Field	NEURAL PROCESSING LETTERS										Image segmentation; Artificial immune; Target extraction; Immune field		In criminal investigation, there are hidden traces that many people can't find, so infrared image is becoming an effective means to obtain these scene traces. The extraction algorithm with growth immune field can extract the target of infrared image relatively effectively, but it is lack of efficiency and reliability in complex environment. Here we propose a new target extraction algorithm with adaptive growth immune field, combining the image information of region and edge gradient. The region of the target in complex environment is obtained by K-means clustering algorithm and the source seed points are selected from the region. The regional characteristics around the seed points as the criteria for growth and the image gradient information is applied as the condition of the adaptive growth immune field. This algorithm improves the accuracy of target extraction in complex environment while preventing overgrowth. We compare the algorithm with the original algorithm and other algorithms and we find that the new algorithm combining edge gradient information can reduce the probability of over growth and ensure the integrity of target extraction under complex background.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2575	2587		10.1007/s11063-020-10218-7		MAR 2020											
J								Joint Feature Selection with Dynamic Spectral Clustering	NEURAL PROCESSING LETTERS										Clustering; K-means; Spectral clustering; Feature selection; Outlier reduction; Similarity measure; Unsupervised learning	ROBUST; REPRESENTATION; ENSEMBLE; FUSION	Current clustering algorithms solved a few of the issues around clustering such as similarity measure learning, or the cluster number estimation. For instance, some clustering algorithms can learn the data similarity matrix, but to do so they need to know the cluster number beforehand. On the other hand, some clustering algorithms estimate the cluster number, but to do so they need the similarity matrix as an input. Real-world data often contains redundant features and outliers, which many algorithms are susceptive to. None of the current clustering algorithms are able to learn the data similarity measure and the cluster number simultaneously, and at the same time reduce the influence of outliers and redundant features. Here we propose a joint feature selection with dynamic spectral clustering (FSDS) algorithm that not only learns the cluster number k and data similarity measure simultaneously, but also employs the L-2,L-1-norm to reduce the influence of outliers and redundant features. The optimal performance could be reached when all the separated stages are combined in a unified way. Experimental results on eight real-world benchmark datasets show that our FSDS clustering algorithm outperformed the comparison clustering algorithms in terms of two evaluation metrics for clustering algorithms including ACC and Purity.																	1370-4621	1573-773X															10.1007/s11063-020-10216-9		MAR 2020											
J								A new expert system in prediction of lung cancer disease based on fuzzy soft sets	SOFT COMPUTING										Fuzzy inference system; Fuzzy soft expert system; Comparison diagnosed; Prevention and control of cancer-like diseases	INFERENCE SYSTEM; THEORETIC APPROACH; COMPONENT ANALYSIS; DIAGNOSIS; ALGORITHM	Every year, millions of people worldwide (including amajor portion in China) are suffering from lung cancer disease (Chinese report of Smoking and Health 2017). The aim of this paper is to develop a new fuzzy soft expert system which can be used to predict lung cancer disease. A prediction process using this fuzzy soft expert system is composed of four main steps: (1) Transform real-valued inputs into fuzzy numbers. (2) Transform fuzzy numbers of data into fuzzy soft sets. (3) Reduce, using normal parameter reduction method, the obtained family of fuzzy soft sets into a new family of fuzzy soft sets. (4) Use the proposed algorithm to get the output data. An experiment is conducted on forty five patients (thirty males, fifteen females, all are cigarette smokers) who endure treatment in the Respiratory Department of Nanjing Chest Hospital, China. The number of training data taken was 55 records, and the remaining 45 records were used for the testing process in our system by using weight loss, shortness of breath, chest pain, persistence a cough, blood in sputum, and age of patients. The quantized accuracies of the proposed system were found to be 100%. In this work, we developed a fuzzy soft expert system based on fuzzy soft sets; we used a fuzzy membership functions and an algorithm to predict those patients who may suffer lung cancer. In this way, it is possible to conclude that the use of fuzzy soft expert system can produce valuable results for lung cancer detection. It is found that the fuzzy soft expert system developed is useful to the expert doctor to decide if a patient has lung cancer or not. Finally, we introduce comparison diagnosed between our proposed system and the fuzzy inference system.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14179	14207		10.1007/s00500-020-04787-x		MAR 2020											
J								Research on the clustering algorithm of ocean big data based on self-organizing neural network	COMPUTATIONAL INTELLIGENCE										clustering; competitive learning; marine big data; self-organizing feature mapping		In the construction of a smart marine, marine big data mining has a significant impact on the growing maritime industry in the Beibu Gulf. Clustering is the key technology of marine big data mining, but the conventional clustering algorithm cannot achieve the efficient clustering of marine data. According to the characteristics of marine big data, a marine big data clustering scheme based on self-organizing neural network (SOM) algorithm is proposed. First, the working principle of SOM algorithm is analyzed, and the algorithm's two-dimensional network model, similarity model and competitive learning model are focused. Secondly, combining with the working principle of algorithm, the marine big data clustering process and algorithm achievement based on SOM algorithm are developed; finally, experiments show that all vectors in marine big data clustering are stable, and the neurons in the output layer of clustering result have obvious consistency with the data itself, which shows the effectiveness of SOM algorithm in marine big data clustering.																	0824-7935	1467-8640															10.1111/coin.12299		MAR 2020											
J								Cascade convolutional neural network-long short-term memory recurrent neural networks for automatic tonal and nontonal preclassification-based Indian language identification	EXPERT SYSTEMS										Cascade CNN-LSTM; Chroma; database; formants; PSA and GCR; syllables; tonal and nontonal languages	SPEAKER; EXTRACTION; FEATURES	This work presents an automatic tonal/nontonal preclassification-based Indian language identification (LID) system. Languages are firstly classified into tonal and nontonal categories, and then, individual languages are identified from the languages of the respective categories. This work proposes the use of pitch Chroma and formant features for this task, and also investigates how Mel-frequency Cepstral Coefficients (MFCCs) complement these features. It further explores block processing (BP), pitch synchronous analysis (PSA)- and glottal closure regions (GCRs)-based approaches for feature extraction, using syllables as basic units. Cascade convolutional neural network (CNN)-long short-term memory (LSTM) model using syllable-level features has been developed. National Institute of Technology Silchar language database (NITS-LD) and OGI-Multilingual Telephone Speech Corpus (OGI-MLTS) have been used for experimental validation. The proposed system based on the score combination of Cascade CNN-LSTM models of Chroma (extracted from BP method), first two formants and MFCCs (both extracted from GCR method) reports the highest accuracies. In the preclassification stage, the observed accuracies are 91%, 87.3%, and 85.1% for NITS-LD, for 30 s, 10 s, and 3 s test data respectively. For OGI-MLTS database, the respective accuracies are 86.7%, 83.1%, and 80.6%. That amounts to absolute improvements of 11.6%, 12.3%, and 13.9% for NITS-LD, and 12.5%, 11.9%, and 12.6% for OGI-MLTS database with respect to that of the baseline system. The proposed preclassification-based LID system shows improvements of 7.3%, 6.4%, and 7.4% for NITS-LD and 6.1%, 6.7%, and 7.2% for OGI-MLTS database over the baseline system for the three respective test data conditions.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12544	10.1111/exsy.12544		MAR 2020											
J								Creation of original Tamil character dataset through segregation of ancient palm leaf manuscripts in medicine	EXPERT SYSTEMS										ancient Tamil characters; expert systems for character recognition; Gaussian distortion; palm leaf manuscripts in Tamil; Tamil and medicine; Tamil medical manuscripts		Palm leaves were one of the essential and primary sources for writing before the advent of paper. Tamil is one of the oldest southern Indian languages and among the ten oldest languages of the world. Agathiyar, a renowned Siddhar of ancient India, considered as the Father of Siddha Medicine, wrote all his therapeutic procedures only on palm leaf manuscripts, in the Tamil language. For modern day, people who try to only write and read new aspects of Tamil, identifying the ancient characters of the language is difficult. To expand readability and secure the written medicinal practices and traditions, a better recognition system is needed that can transform the ancient text images to modern ones, interpreting the ancient Tamil characters from palm leaves and understanding their contexts a time-consuming and complicated process. Especially when it comes to medicine, the practitioners need to understand the contents of a manuscript, to apply them on a daily basis. Therefore, a recognition system is of much use to understand, interpret, and apply the techniques explained in the manuscript on a daily basis. This study is an attempt to create a considerable volume of Tamil character datasets through the segregation of ancient Tamil palm leaf manuscripts related to the field of medicine. In this study, the characters created are fed as inputs to expert systems for intelligent recognition of the context and content perceived to be present in the selected medical manuscripts. The characters have been identified in large numbers manually, and datasets are created using Gaussian distortion.																	0266-4720	1468-0394														e12538	10.1111/exsy.12538		MAR 2020											
J								An objective and interactive-information-based feedback mechanism for the consensus-reaching process considering a non-support degree for minority opinions	EXPERT SYSTEMS										consensus; large-scale group decision-making; linguistic term sets; minority opinion; new product development	GROUP DECISION-MAKING; REPRESENTATION MODEL; MINIMUM ADJUSTMENT; SOCIAL NETWORK; SCALE; MAJORITY; METHODOLOGY; SELECTION; 2-TUPLES; DISSENT	The consensus-reaching process (CRP) to achieve higher unanimity and ensure common agreement before deriving a final decision has become an important procedure in group decision-making problems. The demand for high-quality decision results has motivated the development of large-scale group decision-making (LGDM). In such cases, the issue of minority opinion has gained awareness due to the related effects on enhancing consensus and decision quality. A minority opinion cannot exert an effect unless the majority attach importance to whether that opinion is supported or not. To reflect the effect of minority opinions on consensus, this paper establishes a LGDM framework with an objective and interactive-information-based feedback mechanism for the CRP. Given the natural forms of human expression, multi-granular linguistic information and a 2-tuple linguistic model are used. First, initial weights are objectively assigned to decision-makers (DMs) to weaken the impact of the majority. Subsequently, a non-support degree function is newly defined to reflect the extent to which other DMs dissent from a minority opinion. More importantly, feedback rules are constructed to make corresponding adjustments to the powers of discourse among all DMs in the attempt to reach consensus. Finally, the proposed three-phase LGDM framework is applied to new product development (NPD), and simulation experiments are conducted based on two algorithms to verify the framework's applicability and feasibility.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12543	10.1111/exsy.12543		MAR 2020											
J								New data envelopment analysis models for assessing sustainability	EXPERT SYSTEMS											EFFICIENCY																		0266-4720	1468-0394														e12545	10.1111/exsy.12545		MAR 2020											
J								DABGEO: A reusable and usable global energy ontology for the energy domain	JOURNAL OF WEB SEMANTICS										Ontology; Energy domain; Ontology reusability; Ontology usability	ONTOCAPE	The heterogeneity of energy ontologies hinders the interoperability between ontology-based energy management applications to perform a large-scale energy management. Thus, there is the need for a global ontology that provides common vocabularies to represent the energy subdomains. A global energy ontology must provide a balance of reusability-usability to moderate the effort required to reuse it in different applications. This paper presents DABGEO: a reusable and usable global ontology for the energy domain that provides a common representation of energy domains represented by existing energy ontologies. DABGEO can be reused by ontology engineers to develop ontologies for specific energy management applications. In contrast to previous global energy ontologies, it follows a layered structure to provide a balance of reusability-usability. In this work, we provide an overview of the structure of DABGEO and we explain how to reuse it in a particular application case. In addition, the paper includes an evaluation of DABGEO to demonstrate that it provides a balance of reusability-usability. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				MAR	2020	61-62								100550	10.1016/j.websem.2020.100550													
J								FEEL: Framework for the integration of Entity Extraction and Linking systems	JOURNAL OF WEB SEMANTICS										Named entity disambiguation; Entity linking; Entity joining; Ensemble extractor	KNOWLEDGE-BASE; RECOGNITION	Entity extraction and linking (EEL) is an important task of the Semantic Web that allows to identify real-world objects from text and associate them with their respective resources from a Knowledge Base. Thus, one purpose of the EEL task is to extract knowledge from text. In recent years, several systems have been proposed for addressing such a task in several domains, languages, and knowledge bases. In this sense, some systems that combine the benefits of varied EEL systems have been proposed in a kind of ensemble system (like in Machine Learning) to provide better performance and extractions than using a single system. However, there are no clear indications for the selection, configuration, and result integration of EEL systems in an ensemble setting. This paper proposes a framework for the integration of EEL systems by providing recommendations for the selection of systems, the configuration of input parameters, the execution of systems, and the final integration of results through a filtering strategy that measures the occurrence of entities and detects the overlapping of entities. Based on the proposed framework, we implemented a system using existing EEL systems (through publicly available APIs). The experiments were performed through the GERBIL framework. Our results demonstrate an improvement of the micro/macroprecision and recall of the implemented system regarding the selected individual EEL systems over seven datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				MAR	2020	61-62								100561	10.1016/j.websem.2020.100561													
J								The role of knowledge in determining identity of long-tail entities	JOURNAL OF WEB SEMANTICS										Long-tail entities; NIL clustering; Knowledge-based completion		Identifying entities in text is an important step of semantic analysis. Some entity mentions comprise a name or description, but many include no information that identifies them in the system's knowledge resources, which means that their identity cannot be established through traditional disambiguation. Consequently, such NIL (not in lexicon) entities have received little attention in entity linking systems and tasks so far. However, given the non-redundancy of knowledge on NIL entities, their lack of frequency priors, their potentially extreme ambiguity, and their numerousness, they constitute an important class of long-tail entities and pose a great challenge for state-of-the-art systems. In this paper, we describe a method for imputing identifying knowledge to NILs from generalized characteristics. We enrich the locally extracted information with profile models that rely on background knowledge in Wikidata. We describe and implement two profiling machines using state-of-the-art neural models. We evaluate their intrinsic behavior and their impact on the task of determining the identity of NIL entities. (C) 2020 Published by Elsevier B.V.																	1570-8268	1873-7749				MAR	2020	61-62								100565	10.1016/j.websem.2020.100565													
J								Relaxing relationship queries on graph data	JOURNAL OF WEB SEMANTICS										Semantic association search; Complex relationship; Query relaxation; Graph data	KEYWORD SEARCH; RELAXATION; APPROXIMATION	In many domains we have witnessed the need to search a large entity-relation graph for direct and indirect relationships between a set of entities specified in a query. A search result, called a semantic association (SA), is typically a compact (e.g., diameter-constrained) connected subgraph containing all the query entities. For this problem of SA search, efficient algorithms exist but will return empty results if some query entities are distant in the graph. To reduce the occurrence of failing query and provide alternative results, we study the problem of query relaxation in the context of SA search. Simply relaxing the compactness constraint will sacrifice the compactness of an SA, and more importantly, may lead to performance issues and be impracticable. Instead, we focus on removing the smallest number of entities from the original failing query, to form a maximum successful sub-query which minimizes the loss of result quality caused by relaxation. We prove that verifying the success of a sub-query turns into finding an entity (called a certificate) that satisfies a distance-based condition about the query entities. To efficiently find a certificate of the success of a maximum sub-query, we propose a best-first search algorithm that leverages distance-based estimation to effectively prune the search space. We further improve its performance by adding two fine-grained heuristics: one based on degree and the other based on distance. Extensive experiments over popular RDF datasets demonstrate the efficiency of our algorithm, which is more scalable than baselines. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				MAR	2020	61-62								100557	10.1016/j.websem.2020.100557													
J								Curriculum learning for distant supervision relation extraction	JOURNAL OF WEB SEMANTICS										Relation extraction; Curriculum learning; Mentor network		Relation extraction under distant supervision leverages the existing knowledge base to label data automatically, thus greatly reduced the consumption of human labors. Although distant supervision is an efficient method to obtain a large amount of labeled data, the training dataset labeled by distant supervision suffers from noise problem resulting in poor generalization ability of the relation extractor. To alleviate the noise problem, we propose a novel relation extraction method based on curriculum learning. Curriculum learning is utilized to guide the training process of relation extractor, specifically through the predefined curriculum-driven mentor network. Mentor network can dynamically adjust the weights of sentences during training, giving lower weights to noisy sentences and higher weights to truly labeled sentences. Relation extractor and mentor network are trained collaboratively to optimize joint objective. The experimental results show that the proposed method can improve the generalization ability of relation extractor in a noisy environment and obtains better performance for relation extraction. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				MAR	2020	61-62								100559	10.1016/j.websem.2020.100559													
J								Generative molecular design in low data regimes	NATURE MACHINE INTELLIGENCE											LIBRARIES; LANGUAGE; DRUGS	Generative machine learning models sample molecules from chemical space without the need for explicit design rules. To enable the generative design of innovative molecular entities with limited training data, a deep learning framework for customized compound library generation is presented that aims to enrich and expand the pharmacologically relevant chemical space with drug-like molecular entities on demand. This de novo design approach combines best practices and was used to generate molecules that incorporate features of both bioactive synthetic compounds and natural products, which are a primary source of inspiration for drug discovery. The results show that the data-driven machine intelligence acquires implicit chemical knowledge and generates novel molecules with bespoke properties and structural diversity. The method is available as an open-access tool for medicinal and bioorganic chemistry. With the aid of deep learning, the space of chemical molecules, such as candidates for drugs, can be constrained to find new bioactive molecules. A new open source tool can generate libraries of novel molecules with user defined properties.																		2522-5839				MAR	2020	2	3					171	180		10.1038/s42256-020-0160-y													
J								Rapid online learning and robust recall in a neuromorphic olfactory circuit	NATURE MACHINE INTELLIGENCE											CHOLINERGIC MODULATION; DEPENDENT PLASTICITY; GAMMA-OSCILLATIONS; BULB; CELLS; MODEL; REPRESENTATIONS; NEUROGENESIS; COMPUTATION; INHIBITION	We present a neural algorithm for the rapid online learning and identification of odourant samples under noise, based on the architecture of the mammalian olfactory bulb and implemented on the Intel Loihi neuromorphic system. As with biological olfaction, the spike timing-based algorithm utilizes distributed, event-driven computations and rapid (one shot) online learning. Spike timing-dependent plasticity rules operate iteratively over sequential gamma-frequency packets to construct odour representations from the activity of chemosensor arrays mounted in a wind tunnel. Learned odourants then are reliably identified despite strong destructive interference. Noise resistance is further enhanced by neuromodulation and contextual priming. Lifelong learning capabilities are enabled by adult neurogenesis. The algorithm is applicable to any signal identification problem in which high-dimensional signals are embedded in unknown backgrounds. Integrating knowledge about the circuit-level organization of the brain into neuromorphic artificial systems is a challenging research problem. The authors present a neural algorithm for the learning of odourant signals and their robust identification under noise, based on the architecture of the mammalian olfactory bulb and implemented on the Intel Loihi neuromorphic system.																		2522-5839				MAR	2020	2	3					181	191		10.1038/s42256-020-0159-4													
J								Interval MULTIMOORA Method Integrating Interval Borda Rule and Interval Best-Worst-Method-Based Weighting Model: Case Study on Hybrid Vehicle Engine Selection	IEEE TRANSACTIONS ON CYBERNETICS										Decision making; Entropy; Gold; Engines; Mathematical model; Computational modeling; Cybernetics; Group interval best-worst method (BWM); hybrid vehicle engine selection; interval Borda rule; interval entropy; interval MULTIMOORA; multiple criteria decision making (MCDM)	DECISION-MAKING PROBLEMS; VIKOR METHOD; EXTENSION; ENTROPY; TOPSIS	In this paper, we present an interval MULTIMOORA method with complete interval computation in which the interval distances of interval numbers and preference matrix are used. In addition, we propose a group interval best-worst method (BWM) with interval preference degree. The group interval BWM has a hierarchical structure of group decision making with two levels of experts. Beside employing the dominance theory to integrate subordinate rankings, we introduce the interval Borda rule as an aggregation function which does not have the defects of the dominance theory. We calculate the objective interval weights of criteria based on the interval entropy method, which are integrated by the subjective weights computed by the group interval BWM. The approach presented in this paper is verified by a real-world engineering selection problem of a hybrid vehicle engine based on real data and opinions of engineering design experts of the automotive industry of Iran. The preference-based and dominance-based ranking lists are presented for the problem. We solve the same case by employing the interval TOPSIS and VIKOR methods. Eventually, all resultant rankings are compared based on Spearman rank correlation coefficients.																	2168-2267	2168-2275				MAR	2020	50	3					1157	1169		10.1109/TCYB.2018.2889730													
J								Discriminative Fisher Embedding Dictionary Learning Algorithm for Object Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dictionaries; Encoding; Training; Image coding; Image reconstruction; Dimensionality reduction; Analytical structure promotion; dictionary learning; discriminative embedding learning; Fisher criterion; sparse representation	LOW-RANK; SHARED DICTIONARY; FACE-RECOGNITION; IMAGE; REPRESENTATION; ROBUST; CLASSIFICATION; OPTIMIZATION; MINIMIZATION	Both interclass variances and intraclass similarities are crucial for improving the classification performance of discriminative dictionary learning (DDL) algorithms. However, existing DDL methods often ignore the combination between the interclass and intraclass properties of dictionary atoms and coding coefficients. To address this problem, in this paper, we propose a discriminative Fisher embedding dictionary learning (DFEDL) algorithm that simultaneously establishes Fisher embedding models on learned atoms and coefficients. Specifically, we first construct a discriminative Fisher atom embedding model by exploring the Fisher criterion of the atoms, which encourages the atoms of the same class to reconstruct the corresponding training samples as much as possible. At the same time, a discriminative Fisher coefficient embedding model is formulated by imposing the Fisher criterion on the profiles (row vectors of the coding coefficient matrix) and coding coefficients, which forces the coding coefficient matrix to become a block-diagonal matrix. Since the profiles can indicate which training samples are represented by the corresponding atoms, the proposed two discriminative Fisher embedding models can alternatively and interactively promote the discriminative capabilities of the learned dictionary and coding coefficients. The extensive experimental results demonstrate that the proposed DFEDL algorithm achieves superior performance in comparison with some state-of-the-art dictionary learning algorithms on both hand-crafted and deep learning-based features.																	2162-237X	2162-2388				MAR	2020	31	3					786	800		10.1109/TNNLS.2019.2910146													
J								Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Travel product reconunendation; probabilistic matrix factorization; linear regression; multiple auxiliary information; recommender systems		As an e-commerce feature, the personalized recommendation is invariably highly-valued by both consumers and merchants. The e-tourism has become one of the hottest industries with the adoption of recommendation systems. Several lines of evidence have confirmed the travel-product recommendation is quite different from traditional recommendations. Travel products are usually browsed and purchased relatively infrequently compared with other traditional products (e.g., books and food), which gives rise to the extreme sparsity of travel data. Meanwhile, the choice of a suitable travel product is affected by an army of factors such as departure, destination, and financial and time budgets. To address these challenging problems, in this article, we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information (PMF-MAI) model in the context of the travel-product recommendation. In particular, PMF-MAI is able to fuse the probabilistic matrix factorization on the user-item interaction matrix with the linear regression on a suite of features constructed by the multiple auxiliary information. In order to fit the sparse data, PMF-MAI is built by a whole-data based learning approach that utilizes unobserved data to increase the coupling between probabilistic matrix factorization and linear regression. Extensive experiments are conducted on a real-world dataset provided by a large tourism e-commerce company. PMF-MAI shows an overwhelming superiority over all competitive baselines on the recommendation performance. Also, the importance of features is examined to reveal the crucial auxiliary information having a great impact on the adoption of travel products.																	2157-6904	2157-6912				MAR	2020	11	2							22	10.1145/3372118													
J								Single Image Snow Removal Using Sparse Representation and Particle Swarm Optimizer	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Snow removal; image restoration; sparse representation	MOVING OBJECT DETECTION; RAIN STREAKS REMOVAL; DETECTION ALGORITHM	Images are often corrupted by natural obscuration (e.g., snow, rain, and haze) during acquisition in bad weather conditions. The removal of snowflakes from only a single image is a challenging task due to situational variety and has been investigated only rarely. In this article, we propose a novel snow removal framework for a single image, which can be separated into a sparse image approximation module and an adaptive tolerance optimization module. The first proposed module takes the advantage of sparsity-based regularization to reconstruct a potential snow-free image. An auto-tuning mechanism for this framework is then proposed to seek a better reconstruction of a snow-free image via the time-varying inertia weight particle swarm optimizers in the second proposed module. Through collaboration of these two modules iteratively, the number of snowflakes in the reconstructed image is reduced as generations progress. By the experimental results, the proposed method achieves a better efficacy of snow removal than do other state-of-the-art techniques via both objective and subjective evaluations. As a result, the proposed method is able to remove snowflakes successfully from only a single image while preserving most original object structure information.																	2157-6904	2157-6912				MAR	2020	11	2							20	10.1145/3372116													
J								Pair-based Uncertainty and Diversity Promoting Early Active Learning for Person Re-identification	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Active learning; person re-identification		The effective training of supervised Person Re-identification (Re-ID) models requires sufficient pairwise labeled data. However, when there is limited annotation resource, it is difficult to collect pairwise labeled data. We consider a challenging and practical problem called Early Active Learning, which is applied to the early stage of experiments when there is no pre-labeled sample available as references for human annotating. Previous early active learning methods suffer from two limitations for Re-ID. First, these instance-based algorithms select instances rather than pairs, which can result in missing optimal pairs for Re-ID. Second, most of these methods only consider the representativeness of instances, which can result in selecting less diverse and less informative pairs. To overcome these limitations, we propose a novel pair-based active learning for Re-ID. Our algorithm selects pairs instead of instances from the entire dataset for annotation. Besides representativeness, we further take into account the uncertainty and the diversity in terms of pairwise relations. Therefore, our algorithm can produce the most representative, informative, and diverse pairs for Re-ID data annotation. Extensive experimental results on five benchmark Re-ID datasets have demonstrated the superiority of the proposed pair-based early active learning algorithm.																	2157-6904	2157-6912				MAR	2020	11	2							21	10.1145/3372121													
J								Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Rank aggregation; recommender systems; machine learning		Recommender Systems are tools designed to help users find relevant information from the myriad of content available online. They work by actively suggesting items that are relevant to users according to their historical preferences or observed actions. Among recommender systems, top-N recommenders work by suggesting a ranking of N items that can be of interest to a user. Although a significant number of top-N recommenders have been proposed in the literature, they often disagree in their returned rankings, offering an opportunity for improving the final recommendation ranking by aggregating the outputs of different algorithms. Rank aggregation was successfully used in a significant number of areas, but only a few rank aggregation methods have been proposed in the recommender systems literature. Furthermore, there is a lack of studies regarding rankings' characteristics and their possible impacts on the improvements achieved through rank aggregation. This work presents an extensive two-phase experimental analysis of rank aggregation in recommender systems. In the first phase, we investigate the characteristics of rankings recommended by 15 different top-N recommender algorithms regarding agreement and diversity. In the second phase, we look at the results of 19 rank aggregation methods and identify different scenarios where they perform best or worst according to the input rankings' characteristics. Our results show that supervised rank aggregation methods provide improvements in the results of the recommended rankings in six out of seven datasets. These methods provide robustness even in the presence of a big set of weak recommendation rankings. However, in cases where there was a set of non-diverse high-quality input rankings, supervised and unsupervised algorithms produced similar results. In these cases, we can avoid the cost of the former in favor of the latter.																	2157-6904	2157-6912				MAR	2020	11	2							16	10.1145/3365375													
J								Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in Discrete Environments	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Commitments; plan abandonment; plan execution; landmarks; domain-independent heuristics; optimal plan; sub-optimal plan		Assessing whether an agent has abandoned a goal or is actively pursuing it is important when multiple agents are trying to achieve joint goals, or when agents commit to achieving goals for each other. Making such a determination for a single goal by observing only plan traces is not trivial, as agents often deviate from optimal plans for various reasons, including the pursuit of multiple goals or the inability to act optimally. In this article, we develop an approach based on domain independent heuristics from automated planning, landmarks, and fact partitions to identify sub-optimal action steps-with respect to a plan-within a fully observable plan execution trace. Such capability is very important in domains where multiple agents cooperate and delegate tasks among themselves, such as through social commitments, and need to ensure that a delegating agent can infer whether or not another agent is actually progressing towards a delegated task. We demonstrate how a creditor can use our technique to determine-by observing a trace-whether a debtor is honouring a commitment. We empirically show, for a number of representative domains, that our approach infers suboptimal action steps with very high accuracy and detects commitment abandonment in nearly all cases.																	2157-6904	2157-6912				MAR	2020	11	2							23	10.1145/3372119													
J								Newton Methods for Convolutional Neural Networks	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Convolution neural networks; newton methods; large-scale classification; subsampled Hessian		Deep learning involves a difficult non-convex optimization problem, which is often solved by stochastic gradient (SG) methods. While SG is usually effective, it may not be robust in some situations. Recently, Newton methods have been investigated as an alternative optimization technique, but most existing studies consider only fully connected feedforward neural networks. These studies do not investigate some more commonly used networks such as Convolutional Neural Networks (CNN). One reason is that Newton methods for CNN involve complicated operations, and so far no works have conducted a thorough investigation. In this work, we give details of all building blocks, including the evaluation of function, gradient, Jacobian, and Gauss-Newton matrix-vector products. These basic components are very important not only for practical implementation but also for developing variants of Newton methods for CNN. We show that an efficient MATLAB implementation can be done in just several hundred lines of code. Preliminary experiments indicate that Newton methods are less sensitive to parameters than the stochastic gradient approach.																	2157-6904	2157-6912				MAR	2020	11	2							19	10.1145/3368271													
J								XLearn: Learning Activity Labels across Heterogeneous Datasets	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Human activity recognition; ensemble learning; transfer learning; clustering; smart home	ACTIVITY RECOGNITION	Sensor-driven systems often need to map sensed data into meaningfully labelled activities to classify the phenomena being observed. A motivating and challenging example comes from human activity recognition in which smart home and other datasets are used to classify human activities to support applications such as ambient assisted living, health monitoring, and behavioural intervention. Building a robust and meaningful classifier needs annotated ground truth, labelled with what activities are actually being observed-and acquiring high-quality, detailed, continuous annotations remains a challenging, time-consuming, and error-prone task, despite considerable attention in the literature. In this article, we use knowledge-driven ensemble learning to develop a technique that can combine classifiers built from individually labelled datasets, even when the labels are sparse and heterogeneous. The technique both relieves individual users of the burden of annotation and allows activities to be learned individually and then transferred to a general classifier. We evaluate our approach using four third-party, real-world smart home datasets and show that it enhances activity recognition accuracies even when given only a very small amount of training data.																	2157-6904	2157-6912				MAR	2020	11	2							17	10.1145/3368272													
J								Web Table Extraction, Retrieval, and Augmentation: A Survey	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Table extraction; table search; table retrieval; table mining; table augmentation; table interpretation	SEARCH	Tables are powerful and popular tools for organizing and manipulating data. A vast number of tables can be found on the Web, which represent a valuable knowledge resource. The objective of this survey is to synthesize and present two decades of research on web tables. In particular, we organize existing literature into six main categories of information access tasks: table extraction, table interpretation, table search, question answering, knowledge base augmentation, and table augmentation. For each of these tasks, we identify and describe seminal approaches, present relevant resources, and point out interdependencies among the different tasks.																	2157-6904	2157-6912				MAR	2020	11	2							13	10.1145/3372117													
J								Flexible Multi-modal Hashing for Scalable Multimedia Retrieval	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Multi-modal hashing; efficient discrete optimization		Multi-modal hashing methods could support efficient multimedia retrieval by combining multi-modal features for binary hash learning at the both offline training and online query stages. However, existing multi-modal methods cannot binarize the queries, when only one or part of modalities are provided. In this article, we propose a novel Flexible Multi-modal Hashing (FMH) method to address this problem. FMH learns multiple modality-specific hash codes and multi-modal collaborative hash codes simultaneously within a single model. The hash codes are flexibly generated according to the newly coming queries, which provide any one or combination of modality features. Besides, the hashing learning procedure is efficiently supervised by the pair-wise semantic matrix to enhance the discriminative capability. It could successfully avoid the challenging symmetric semantic matrix factorization and O(n(2)) storage cost of semantic matrix. Finally, we design a fast discrete optimization to learn hash codes directly with simple operations. Experiments validate the superiority of the proposed approach.																	2157-6904	2157-6912				MAR	2020	11	2							14	10.1145/3365841													
J								Discovering Underlying Plans Based on Shallow Models	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Plan recognition; shallow model; action representation; recurrent neural networks	RECOGNITION; PEOPLE	Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally "matching" observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real-world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this article, we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (Recurrent Neural Networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages.																	2157-6904	2157-6912				MAR	2020	11	2							18	10.1145/3368270													
J								Ontology-based Document Spanning Systems for Information Extraction	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Information Extraction; Ontology-based Data Access; knowledge representation		Information Extraction (IE) is the task of automatically organizing in a structured form data extracted from free text documents. In several contexts, it is often desirable that the extracted data are then organized according to an ontology, which provides a formal and conceptual representation of the domain of interest. Ontologies allow for a better data interpretation, as well as for their semantic integration with other information, as in Ontology-based Data Access (OBDA), a popular declarative framework for data management where an ontology is connected to a data layer through mappings. However, the data layer considered so far in OBDA has consisted essentially of relational databases, and how to declaratively couple an ontology with unstructured data sources is still unexplored. By leveraging the recent study on document spanners for rule-based IE by Fagin et al., in this paper, we propose a new framework that allows to map text documents to ontologies, in the spirit of OBDA. We investigate the problem of answering conjunctive queries in this framework. For ontologies specified in the Description Logics DL-Lite(R) and DL-Lite(F), we show that the problem is polynomial in the size of the underlying documents. We also provide algorithms to solve query answering by rewriting the input query on the basis of the ontology and its mapping toward the source documents. Through these techniques, we pursue a virtual approach, similar to that typically adopted in OBDA, which allows us to answer a query without having to first populate the entire ontology. Interestingly, for DL-Lite(R), both the spanners used in the mapping and the one computed by the rewriting algorithm belong to the same class of expressiveness. This holds also for DL-Lite(F), modulo some limitations on the form of the mapping. These results say that in these cases our framework can be easily implemented by decoupling ontology management and document access, which can be delegated to an external IE system able to process the extraction rules we use in the mapping.																	1793-351X	1793-7108				MAR	2020	14	1			SI		3	26		10.1142/S1793351X20400012													
J								A Study on Information-Preserving Schema Transformations	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Schema transformation; schema dominance; database design; information preservation; database reverse engineering	RELATIONAL DATABASES; EQUIVALENCE; INCLUSION; SYSTEMS; MODEL	The problem of determining the relative information capacity between two knowledge bases or schemas, of the same or different models, is inherent when implementing schema transformations. When restructuring one schema into another, one expects that the schema transformation supports the complete and correct mapping of all the information contents from the source schema to the target schema. Such a characteristic is commonly referred to as information capacity preservation or schema dominance. This paper presents a formal and constructive approach to measure the relative information capacity, in the restricted case of first-order schemas related by first-order mappings. It complements the existing definitions of information capacity preservation from the perspective of model theory, showing the exact relationships among the constraints of the involved schemas, the mappings between the components of these schemas, and the database states which the schemas admit. Since satisfying some sort of schema equivalence property is essential in areas such as database conceptual design and database reverse engineering, our approach allows us to characterize the notion of normalization in database design. We review the current literature concerning database normal forms and decompositions. We also review the process of reverse engineering a database schema. In addition, we provide deeper insight into database reverse engineering methodologies, suggesting horizontal decompositions as a useful tool for facilitating the discovery of more specific objects and relationships in the conceptualization phase of the process. With the aid of simple examples, we show the essence behind our reasoning. We discuss the need for an unambiguous means through which objects in the output schema can be identified. Ultimately, the knowledge this paper ensues will be beneficial to database engineers in performing a correct schema transformation.																	1793-351X	1793-7108				MAR	2020	14	1			SI		27	53		10.1142/S1793351X20400024													
J								Improved Semantic Segmentation of Water Bodies and Land in SAR Images Using Generative Adversarial Networks	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										GANs; SAR images; semantic segmentation; U-net; deep learning		The availability of computationally efficient and powerful Deep Learning frameworks and high-resolution satellite imagery has created new approach for developing complex applications in the field of remote sensing. The easy access to abundant image data repository made available by different satellites of space agencies such as Copernicus, Landsat, etc. has opened various avenues of research in monitoring the world's oceans, land, rivers, etc. The challenging research problem in this direction is the accurate identification and subsequent segmentation of surface water in images in the microwave spectrum. In the recent years, deep learning methods for semantic segmentation are the preferred choice given its high accuracy and ease of use. One major bottleneck in semantic segmentation pipelines is the manual annotation of data. This paper proposes Generative Adversarial Networks (GANs) on the training data (images and their corresponding labels) to create an enhanced dataset on which the networks can be trained, therefore, reducing human effort of manual labeling. Further, the research also proposes the use of deep-learning approaches such as U-Net and FCN-8 to perform an efficient segmentation of auto annotated, enhanced data of water body and land. The experimental results show that the U-Net model without GAN achieves superior performance on SAR images with pixel accuracy of 0.98 and F1 score of 0.9923. However, when augmented with GANs, the results saw a rise in these metrics with PA of 0.99 and F1 score of 0.9954.																	1793-351X	1793-7108				MAR	2020	14	1			SI		55	69		10.1142/S1793351X20400036													
J								Click-Through Rate Prediction of Online Banners Featuring Multimodal Analysis	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Online advertisement; deep learning; visual features; textual features; multimodal processing		As the online advertisement industry continues to grow, it is predicted that online advertisement will account for about 45% of global advertisement spending by 2020.a Thus, predicting the click-through rates (CTRs) of advertisements is increasingly crucial for the advertisement industry. Many studies have already addressed the CTR prediction. However, most studies tried to solve the problem using only metadata such as user id, URL of the landing page, business category, device type, etc., and did not include multimedia contents such as images or texts. Using these multimedia features with deep learning techniques, we propose a method to effectively predict CTRs for online banners, a popular form of online advertisements. We show that multimedia features of advertisements are useful for the task at hand. In our previous work [1], we proposed a CTR prediction model, which outperformed the state-of-the-art method that uses the three features mentioned above, and also we introduced an attention network for visualizing how much each feature affected the prediction result. In this work, we introduce another text analysis technique and more detailed metadata. As a result, we have achieved much better performance as compared to our previous work. Besides, for better analyzing of our model, we introduce another visualization technique to show regions in an image that make its CTR better or worse. Our prediction model gives us useful suggestions for improving design of advertisements to acquire higher CTRs.																	1793-351X	1793-7108				MAR	2020	14	1			SI		71	91		10.1142/S1793351X20400048													
J								Graph Theory and Classifying Security Events in Grid Security Gateways	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Grid security; grid trust; trusted communications; network security; network anomaly detection		In recent years, the use of security gateways (SG) located within the electrical grid distribution network has become pervasive. SGs in substations and renewable distributed energy resource aggregators (DERAs) protect power distribution control devices from cyber and cyber-physical attacks. When encrypted communications within a DER network is used, TCP/IP packet inspection is restricted to packet header behavioral analysis which in most cases only allows the SG to perform anomaly detection of blocks of time-series data (event windows). Packet header anomaly detection calculates the probability of the presence of a threat within an event window, but fails in such cases where the unreadable encrypted payload contains the attack content. The SG system log (syslog) is a time-series record of behavioral patterns of network users and processes accessing and transferring data through the SG network interfaces. Threatening behavioral pattern in the syslog are measurable using both anomaly detection and graph theory. In this paper, it will be shown that it is possible to efficiently detect the presence of and classify a potential threat within an SG syslog using light-weight anomaly detection and graph theory.																	1793-351X	1793-7108				MAR	2020	14	1			SI		93	105		10.1142/S1793351X2040005X													
J								Discriminative Robust Head-Pose and Gaze Estimation Using Kernel-DMCCA Features Fusion	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Gaze; head-pose; pupil; quadtree; kernel-DMCCA	CALIBRATION; APPEARANCE; DEPTH	There remain outstanding challenges for improving accuracy of multi-feature information for head-pose and gaze estimation. The proposed framework employs discriminative analysis for head-pose and gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA). The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Head-pose and gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using K-DMCCA for accurate estimation. The two main contributions are: Enhancing fusion performance through the use of kernel-based DMCCA, and by introducing an improved iris region descriptor based on quadtree. The overall approach is also inclusive of statistical and geometrical indexing that are calibration free (does not require any subsequent adjustment). We validate the robustness of the proposed framework across a wide variety of datasets, which consist of different modalities (RGB and Depth), constraints (wide range of head-poses, not only frontal), quality (accurately labelled for validation), occlusion (due to glasses, hair bang, facial hair) and illumination. Our method achieved an accurate head-pose and gaze estimation of 4.8 degrees using Cave, 4.6 degrees using MPII, 5.1 degrees using ACS, 5.9 degrees using EYEDIAP, 4.3 degrees using OSLO and 4.6 degrees using UULM datasets.																	1793-351X	1793-7108				MAR	2020	14	1			SI		107	135		10.1142/S1793351X20500014													
J								Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Quality estimation; word embeddings; unsupervised alignment; machine translation		We demonstrate the potential for using aligned bilingual word embeddings in developing an unsupervised method to evaluate machine translations without a need for parallel translation corpus or reference corpus. We explain different aspects of digital entertainment content subtitles. We share our experimental results for four languages pairs English to French, German, Portuguese, Spanish, and present findings on the shortcomings of Neural Machine Translation for subtitles. We propose several improvements over the system designed by Gupta et al. [P. Gupta, S. Shekhawat and K. Kumar, Unsupervised quality estimation without reference corpus for subtitle machine translation using word embeddings, IEEE 13th Int. Conf. Semantic Computing, 2019, pp. 32-38.] by incorporating custom embedding model curated to subtitles, compound word splits and punctuation inclusion. We show a massive run time improvement of the order of similar to 600 x by considering three types of edits, removing Proximity Intensity Index (PII) and changing post-edit score calculation from their system.																	1793-351X	1793-7108				MAR	2020	14	1			SI		137	151		10.1142/S1793351X20500026													
J								Accurate Lane Detection for Self-Driving Cars: An Approach Based on Color Filter Adjustment and K-Means Clustering Filter	INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING										Autonomous driving; lane detection; color filters; K-means clustering	VISION SYSTEM	Lane detection is a crucial factor for self-driving cars to achieve a fully autonomous mode. Due to its importance, lane detection has drawn wide attention in recent years for autonomous driving. One challenge for accurate lane detection is to deal with noise appearing in the input image, such as object shadows, brake marks, breaking lane lines. To address this challenge, we propose an effective road detection algorithm. We leverage the strength of color filters to find a rough localization of the lane marks and employ a K-means clustering filter to screen out the embedded noises. We use an extensive experiment to verify the effectiveness of our method. The result indicates that our approach is robust to process noises appearing in input image, which improves the accuracy in lane detection.																	1793-351X	1793-7108				MAR	2020	14	1			SI		153	168		10.1142/S1793351X20500038													
J								Automated Deployment of a Spark Cluster with Machine Learning Algorithm Integration	BIG DATA RESEARCH										Big data analytics; Apache Spark; Machine learning; Cluster deployment	APACHE SPARK; BIG	The vast amount of data stored nowadays has turned big data analytics into a very trendy research field. The Spark distributed computing platform has emerged as a dominant and widely used paradigm for cluster deployment and big data analytics. However, to get started up is still a task that may take much time when manually done, due to the requisites that all nodes must fulfill. This work introduces LadonSpark, an open-source and non-commercial solution to configure and deploy a Spark cluster automatically. It has been specially designed for easy and efficient management of a Spark cluster with a friendly graphical user interface to automate the deployment of a cluster and to start up the distributed file system of Hadoop quickly. Moreover, LadonSpark includes the functionality of integrating any algorithm into the system. That is, the user only needs to provide the executable file and the number of required inputs for proper parametrization. Source codes developed in Scala, R, Python, or Java can be supported on LadonSpark. Besides, clustering, regression, classification, and association rules algorithms are already integrated so that users can test its usability from its initial installation. (C) 2020 Elsevier Inc. All rights reserved.																	2214-5796					MAR	2020	19-20								100135	10.1016/j.bdr.2020.100135													
J								PatSeg: A Sequential Patent Segmentation Approach	BIG DATA RESEARCH										Patent segmentation; Machine learning; Text segmentation; Sequential classifiers; Word embeddings	TEXT; SPEECH; MODELS	Patents are an important source of information in industry and academia. However, quickly grasping the essence of a given patent is difficult as they typically are very long and written in a rather inaccessible style. These essential information, especially the invention itself and the experimental part of the invention, are usually contained in the description section. However, in many patents the description parts are neither annotated nor easily detectable. Here, we describe our novel PatSeg method for patent segmentation, which aims at automatically and directly identifying the most important parts of a patent. PatSeg uses a two-step approach, where a patent is first segmented into text blocks in an unsupervised fashion followed by a supervised classification step for each identified segment. In contrast to previous work, PatSeg uses semantic word embeddings in both phases and applies a sequential learning algorithm for the second step. These modifications lead to, on average, an improvement of 9.47% (8.78%, 9.00%) in terms of F1-score (precision, recall) and 7.29 in terms of accuracy in comparison to a baseline, as evaluated on two novel and manually segmented gold standard patent corpora. The method also is easily parallelizable, fast, making it applicable for truly large patent collections. (C) 2020 Elsevier Inc. All rights reserved.																	2214-5796					MAR	2020	19-20								100133	10.1016/j.bdr.2020.100133													
J								Entity Resolution with Recursive Blocking	BIG DATA RESEARCH										Entity resolution; Record linkage; Data integration; Big data	RECORD LINKAGE	Entity resolution is a well-known challenge in data management for the lack of unique identifiers of records and various errors hidden in the data, undermining the identifiability of entities they refer to. To reveal matching records, every record potentially needs to be compared with all other records in the database, which is computationally intractable even for moderately-sized databases. To circumvent this quadratic challenge, blocking methods are typically employed to facilitate restricting promising comparisons of pairs within small subsets, called blocks, of records. Existing effective methods typically rely on blocking keys created by experts to capture matches, which inevitably involves a large amount of human labor and do not guarantee high-quality results. To reduce manual labor and promote accuracy, machine learning approaches are investigated to meet the challenge with limited success, due to high requirements of training data and inefficiency, especially for large databases. The exhaustive method produces exact results but suffers from efficiency problems. In this paper, we propose a paradigm of divide-and-conquer entity resolution, named recursive blocking, which derives comparatively good results while largely alleviating efficiency concerns. Specifically, recursive blocking refines blocks and traps matches in an iterative fashion to derive high-quality results, and we study two types of recursive blocking, i.e. redundancy- and partition-based approaches, and investigate their relative performance. Comprehensive experiments on both real-world and synthetic datasets verified the superiority of our approaches over the existing ones. (C) 2020 Elsevier Inc. All rights reserved.																	2214-5796					MAR	2020	19-20								100134	10.1016/j.bdr.2020.100134													
J								A Modified Balanced Scorecard Based Hybrid Pythagorean Fuzzy AHP-Topsis Methodology for ATM Site Selection Problem	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										ATM; pythagorean fuzzy-AHP; pythagorean fuzzy TOPSIS	GROUP DECISION-MAKING; OCCUPATIONAL-HEALTH; LOCATION; SAFETY; DELPHI; MODEL; FIRMS	Researchers work on facility location problems in different structures and solve them by developing different models over the years. In this study, we focus on the automated teller machine (ATM) site selection problem for a public bank in Istanbul. Twenty-five districts, which are located at the European side of Istanbul, are designated as ATM candidate districts. The balanced scorecard (BSC) model is modified to determine the criteria according to the literature review and experts' judgments for this problem. Internal, Customer, and Financial perspectives of the traditional BSC model are used as the main criteria and an external perspective is considered as the fourth criterion for modified balanced scorecard. The experts' criteria evaluations are gained and consolidated by the Modified Delphi Method. The criteria weights are determined by the Pythagorean Fuzzy-AHP method using the final pairwise comparison results obtained from the Modified Delphi Method. Subsequently, the Pythagorean Fuzzy TOPSIS method is applied to determine the best alternative site among alternatives to locate the ATM according to these criteria. Finally, the results are presented and discussed by sensitivity analysis.																	0219-6220	1793-6845				MAR	2020	19	2					365	384		10.1142/S0219622020500017													
J								Integrating Sentiment Analysis on Hybrid Collaborative Filtering Method in a Big Data Environment	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Big data; sentiment analysis; hybrid collaborative filtering model; apache's spark; opinion bias	RECOMMENDATION; FACTORIZATION; ALGORITHMS	Most of the traditional recommendation systems are based on user ratings. Here, users provide the ratings towards the product after use or experiencing it. Accordingly, the user item transactional database is constructed for recommendation. The rating based collaborative filtering method is well known method for recommendation system. This system leads to data sparsity problem as the user is unaware of other similar items. Web cataloguing service such as tags plays a significant role to analyse the user's perception towards a particular product. Some system use tags as additional resource to reduce the data sparsity issue. But these systems require lot of specific details related to the tags. Existing system either focuses on ratings or tags based recommendation to enhance the accuracy. So these systems suffer from data sparsity and efficiency problem that leads to ineffective recommendations accuracy. To address the above said issues, this paper proposed hybrid recommendation system (Iter_ALS Iterative Alternate Least Square) to enhance the recommendation accuracy by integrating rating and emotion tags. The rating score reveals overall perception of the item and emotion tags reflects user's feelings. In the absence of emotional tags, scores found in rating is assumed as positive or negative emotional tag score. Lexicon based semantic analysis on emotion tags value is adopted to represent the exclusive value of tag. Unified value is represented into iter_ALS model to reduce the sparsity problem. In addition, this method handles opinion bias between ratings and tags. Experiments were tested and verified using a benchmark project of MovieLens dataset. Initially this model was tested with different sparsity levels varied between 0%-100 percent and the results obtained from the experiments shows the proposed method outperforms with baseline methods. Further tests were conducted to authenticate how it handles opinion bias by users before recommending the item. The proposed method is more capable to be adopted in many real world applications																	0219-6220	1793-6845				MAR	2020	19	2					385	412		10.1142/S0219622020500108													
J								A DSS-Based Novel Approach Proposition Employing Decision Techniques for System Design	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Operational re-design; activity-based DSS; multi-criteria decision making; efficiency enhancement; ORESTE	DUE-DATE ASSIGNMENT; GENETIC ALGORITHM; SINGLE-MACHINE; SCHEDULING PROBLEM; PARALLEL MACHINES; WEIGHTED NUMBER; ORESTE METHOD; MINIMIZE; FLOWSHOP; JOBS	In this study, to enhance flexibility and agility, a special DSS is developed for a system re-design problem, while a decision-making technique is employed afterwards to enable the accelerated and reliable results, as a progressive approach to close a gap of related literature. Proposed approach generates all possible machine schedules regarding some tangible traditional constraints, after that, generated schedules are ranked by ORESTE method regarding human related linguistic variables and ergonomic factors this time, with the aim of harvesting the best outcome. Thus, all traditional and newly introduced problem constraints could be optimized in the best schedule, while time and effort sources of organisation are preserved by employment of proposed DSS, mathematical modelling, and ORESTE. A numerical application of proposed approach is executed and mechanism of proposed novel model is explained in detail as well as usability and salutary of the approach is proved with real life data.																	0219-6220	1793-6845				MAR	2020	19	2					413	445		10.1142/S0219622020500029													
J								Effect of Social Media Interactions on CLV in Telecommunications	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Telecommunications; CLV; social media; ANN	CUSTOMER LIFETIME VALUE; ARTIFICIAL NEURAL-NETWORKS; LOYALTY; MODELS	The main goal of this study is to investigate whether social media, as a recent communication channel, has an impact on customer lifetime value (CLV). No studies have been done in Turkey with similar purposes in the telecommunication sector. To reach this goal, there has been an attempt to develop both artificial neural network models and sector-specific applicable models. Four years of data between 2011 and 2014 belonging to customers in the telecommunication sector who have a Twitter account are used in this study. The CLV is modeled through radial basis function (RBF), multilayer perceptron (MLP), and Email neural network approaches, and the performance of such models is compared. According to the findings, calculated CLV error values are at an acceptable range in all formed models. Additionally, it is determined that the CLV was calculated with a lower error value in models where social media variables were used. The Elman neural network is determined to perform better compared to RBF and MLP.																	0219-6220	1793-6845				MAR	2020	19	2					447	468		10.1142/S0219622020500030													
J								Cloud Computing-Based Socially Important Locations Discovery on Social Media Big Datasets	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Socially important locations discovery; spatial social media mining; cloud computing; Hadoop MapReduce; Twitter	SENTIMENT ANALYSIS; FRAMEWORK; NETWORKS; USERS; RECOMMENDATIONS; SYSTEM	Socially important locations are places which are frequently visited by social media users in their social media lifetime. Discovering socially important locations provides valuable information, such as which locations are frequently visited by a social media user, which locations are common for a social media user group, and which locations are socially important for a group of urban area residents. However, discovering socially important locations is challenging due to huge volume, velocity, and variety of social media datasets, inefficiency of current interest measures and algorithms on social media big datasets, and the need of massive spatial and temporal calculations for spatial social media analyses. In contrast, cloud computing provides infrastructure and platforms to scale compute-intensive jobs. In the literature, limited number of studies related to socially important locations discovery takes into account cloud computing systetns to scale increasing dataset size and to handle massive calculations. This study proposes a cloud-based socially important locations discovery algorithm of Cloud SS-ILM to handle volume and variety of social media big datasets. In particular, in this study, we used Apache Hadoop framework and Hadoop MapReduce programming model to scale dataset size and handle massive spatial and temporal calculations. The performance evaluation of the proposed algorithm is conducted on a cloud computing environment using Turkey Twitter social media big dataset. The experimental results show that using cloud computing systems for socially important locations discovery provide much faster discovery of results than classical algorithms. Moreover, the results show that it is necessary to use cloud computing systems for analyzing social media big datasets that could not be handled with traditional stand-alone computer systems. The proposed Cloud SS-ILM algorithm could be applied on many application areas, such as targeted advertisement of businesses, social media utilization of cities for city planners and local governments, and handling emergency situations.																	0219-6220	1793-6845				MAR	2020	19	2					469	497		10.1142/S0219622020500091													
J								A Multi-Attribute Group Decision-Making Method Based on Linguistic Intuitionistic Fuzzy Numbers and Dempster-Shafer Evidence Theory	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Linguistic intuitionistic fuzzy numbers; evidence theory; combination weight; MAGDM	SETS; OPERATORS; FRAMEWORK; ENTROPY	In this paper, we propose a multi-attribute group decision-making (MAGDM) method based on Dempster-Shafer Evidence Theory (DST) and linguistic intuitionistic fuzzy numbers (LIFNs), in which both the expert weights and attribute weights are unknown. Firstly, we represent LIFNs as basic probability assignments (BPAs) by DST based on linguistic scale function (LSF), and a linear programming model is proposed to combine the objective weights and subjective weights of attributes to obtain the combined weights. At the same time, the experts' weights are obtained through Jousselme distance. Secondly, we use the weights to correct the evidence, and the comprehensive evaluation value of each alternative is calculated by the combination rule of evidence. Further, a new MAGDM approach with DST and LIFNs is presented. Finally, we give au example to explain the proposed method and compare it with other methods to show the feasibility and superiority.																	0219-6220	1793-6845				MAR	2020	19	2					499	524		10.1142/S0219622020500042													
J								Pre-Evaluating the Technical Efficiency Gains from Potential Mergers and Acquisitions in the IC Design Industry	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										DEA; IC design industry; efficiency; merger; merger potential gains	RESEARCH-AND-DEVELOPMENT; PERFORMANCE EVALUATION; COST EFFICIENCY; DEA; SCALE; CONSOLIDATION; COMPETITION; MODEL; POWER	Increased global competition has led to a slowdown in Taiwan's domestic semiconductor industry growth, which has resulted in many semiconductor companies reducing their investments and or seeking mergers and acquisitions (M & As) to increase market power, expand their business territories or increase their competitive edge. However, as there is general uncertainty regarding the efficiencies to be gained from these M & As, there has been an increase in M & A supervision. While past research has explored company operations and management efficiency after mergers, there has been less focus on potential mergers. Therefore, this study used a resample slacks-based measure (RSBM) and merger potential gains models to evaluate potential merger efficiency gains. Data on 29 Taiwanese-listed integrated circuit (IC) design industry firms were collected to evaluate the efficiency of potential M & As, from which it was found that the potential M & As efficiencies had positive and negative values, indicating that efficiency gains were not guaranteed. A positive value was found for a potential M & A between MTK NOVATEK and MTK & DAVICOM, which meant that a potential M & A would increase operating efficiencies and reduce costs.																	0219-6220	1793-6845				MAR	2020	19	2					525	559		10.1142/S021962202050008X													
J								An Artificial Bee Colony-Guided Approach for Electro-Encephalography Signal Decomposition-Based Big Data Optimization	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Big data optimization; EEG signal and Artificial bee colony	COOPERATIVE COEVOLUTION; ALGORITHM	The digital age has added a new term to the literature of information and computer sciences called as the big data in recent years. Because of the individual properties of the newly introduced term, the definitions of the data-intensive problems including optimization problems have been substantially changed and investigations about the solving capabilities of the existing techniques and then developing their specialized variants for big data optimizations have become important research topic. Artificial Bee Colony (ABC) algorithm inspired by the clever foraging characteristics of the real honey bees is one of the most successful swarm intelligence-based metaheuristics. in this study, a new ABC algorithm-based technique that is named source-linked ABC (slinkABC) was proposed by considering the properties of the optimization problems related with the big data. The slinkABC algorithm was tested on the big data optimization problems presented at the Congress on Evolutionary Computation (CEC) 2015 Big Data Optimization Competition. The results obtained from the experimental studies were compared with the different variants of the ABC algorithm including gbest-guided ABC (GABC), ABC/best/1, ABC/best/2, crossover ABC (CABC), converge-onlookers ABC (COABC), quick ABC (qABC) and modified gbest-guided ABC (MGABC) algorithms. In addition to these, the results of the proposed ABC algorithm were also compared with the results of the Differential Evolution (DE) algorithm, Genetic algorithm (GA), Firefly algorithm (FA), Phase-Based Optimization (PBO) algorithm and Particle Swarm Optimization (PSO) algorithm-based approaches. From the experimental studies, it was understood that the ABC algorithm modified by considering the unique properties of the big data optimization problems as in the slinkABC produces better solutions for most of the tested instances compared to the mentioned optimization techniques.																	0219-6220	1793-6845				MAR	2020	19	2					561	600		10.1142/S0219622020500078													
J								Using Ordered Weighted Average for Weighted Averages Inflation	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										OWA operator; inflation; decision making	GROUP DECISION-MAKING; AGGREGATION OPERATORS; DISTANCE MEASURES; BONFERRONI MEANS; BIAS; CONSISTENCY; DISCRETION; MODEL	This paper presents the ordered weighted average weighted average inflation (OWAWAI) and some extensions using induced and heavy aggregation operators and presents the generalized operators and some of their families. The main advantage of these new formulations is that they can use two different sets of weighting vectors and generate new scenarios based on the reordering of the arguments with the weights. With this idea, it is possible to generate new approaches that under- or overestimate the results according to the knowledge and expertise of the decision-maker. The work presents an application of these new approaches in the analysis of the inflation in Chile, Colombia, and Argentina during 2017.																	0219-6220	1793-6845				MAR	2020	19	2					601	628		10.1142/S0219622020500066													
J								A Novel Hybrid Fuzzy AHP-GA Method for Test Sheet Question Selection	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Fuzzy analytic hierarchy process; genetic algorithm; decision-making; test sheet composition; recommendation system	GENETIC ALGORITHM; ORDER DISTRIBUTION; HIERARCHY PROCESS; RISK ANALYSIS; OPTIMIZATION; CLASSIFICATION; CONSTRUCTION; MCDM	The use of web-based education and e-learning environments has increased with the developments in educational technology. Schools, universities, public institutions, and other private sector companies started deploying these systems to train their students, members, and employees. Exams are carried out during the evaluation process of these trainings. Web-based tests are sometimes used for these exams. When there are so many questions about the same topic, it is a time-consuming and difficult problem to prepare these exams in terms of the best quality, quickly and effectively. In order to overcome this issue, artificial intelligence techniques are utilized as well as conventional methods for producing test papers. In this study, an Intelligent Question Evaluation and Selection Software (I-QUESS), that enables the selection of questions according to desired preferences by using Fuzzy Analytic Hierarchy Process (FAHP) and genetic algorithm (GA) as hybrid, was developed. This proposed hybrid system was used in a case study to create test sheet for web-based environments.																	0219-6220	1793-6845				MAR	2020	19	2					629	647		10.1142/S0219622020500054													
J								Optimizing Nonlinear Parameters of Sugeno Type Fuzzy Rules using GWO for Data Classification	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Artificial intelligence; machine learning; biologically inspired computing; swarm; evolutionary algorithms; grey wolf optimizer; fuzzy rules; data classification	OPTIMIZATION; ALGORITHMS	In this paper, a Sugeno type fuzzy system based on the fuzzy clustering has been developed for a variety of datasets. The number of rules for each dataset is based on the optimum number of clusters in that dataset. Rule sets provide the knowledge base for the classification of data. Each rule set is fine-tuned using the GWO with the intention to improve the classification. The approach is compared with the work of previous researchers on similar data sets using a variety of techniques, including nature-inspired algorithms such as genetic algorithms and Swarm based algorithms. Statistical Analysis of the performance of GWO shows that it is better than five other algorithms 95% of the time.																	1469-0268	1757-5885				MAR	2020	19	1							2050009	10.1142/S1469026820500091													
J								Feature Selection in GPCR Classification Using BAT Algorithm	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										GPCR; feature selection; BAT algorithm; feature extraction strategies; classification; local descriptors; data mining algorithms; protein representation methods	PROTEIN-COUPLED RECEPTORS; AMINO-ACID-COMPOSITION; ANT COLONY OPTIMIZATION; DIPEPTIDE COMPOSITION; SUBCELLULAR LOCATION; PREDICTION; FAMILIES; SEQUENCE	G-Protein-Coupled Receptors (GPCR) are the large family of protein membrane; and until now some of them still remain orphans. Predicting GPCR functions is a challenging task, it depends closely to their classification, which requires a digital representation of each protein chain as an attribute vector. A major problem of GPCR databases is their great number of features which can produce combinatorial explosion and increase the complexity of classification algorithms. Feature selection techniques are used to deal with this problem by minimizing features space dimension, and keeping the most relevant ones. In this paper, we propose to use the BAT algorithm for extracting the pertinent features and to improve the classification results. We compared the results obtained by our system with two other bio-inspired algorithms, Evolutionary Algorithm and PSO search. Metrics quality measures used for comparison are Error Rate, Accuracy, MCC and F-measure. Experimental results indicate that our system is more efficient.																	1469-0268	1757-5885				MAR	2020	19	1							2050006	10.1142/S1469026820500066													
J								Optimizing Design of Fuzzy Model for Software Cost Estimation Using Particle Swarm Optimization Algorithm	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Software cost estimation; evolutionary computing; particle swarm optimization; fuzzy logic	IDENTIFICATION; SYSTEMS	Estimation of software cost and effort is of prime importance in software development process. Accurate and reliable estimation plays a vital role in successful completion of the project. To estimate software cost, various techniques have been used. Constructive Cost Model (COCOMO) is amongst most prominent algorithmic model used for cost estimation. Different versions of COCOMO consider different types of parameters affecting overall cost. Parameters involved in estimation using COCOMO possess vagueness which introduces some degree of uncertainty in algorithmic modelling. The concept of fuzzy logic can deal with uncertainty involved in Intermediate COCOMO cost driver measurements via Fuzzy Inference System (FIS). In the proposed research, an effort has been made wherein, for each cost driver, an FIS is designed to calculate the corresponding effort multiplier. Proposed research provides an insight through evolutionary-based optimization techniques to optimize fuzzy logic-based COCOMO using Particle Swarm Optimization Algorithm. The magnitude of relative error and its mean, calculated using COCOMO NASA2 and COCOMONASA datasets are used as evaluation metrics to validate the proposed model. The model outperforms when compared to other optimization techniques like Genetic Algorithm.																	1469-0268	1757-5885				MAR	2020	19	1							2050005	10.1142/S1469026820500054													
J								A Memetic Artificial Bee Colony Algorithm for High Dimensional Problems	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Artificial bee colony; local search; memetic algorithm; swarm intelligence; artificial intelligence	DIFFERENTIAL EVOLUTION; OPTIMIZATION	This work proposed a memetic version of Artificial Bee Colony algorithm, or called LSABC, which employed a "shrinking" local search strategy. By gradually shrinking the local search space along with the optimization process, the proposed LSABC algorithm randomly explores a large space in the early run time. This helps to avoid premature convergence. Then in the later evolution process, the LSABC finely exploits a small region around the current best solution to achieve a more accurate output value. The optimization behavior of the LSABC algorithm was studied and analyzed in the work. Compared with the classic ABC and several other state-of-the-art optimization algorithms, the LSABC shows a better performance in terms of convergence rate and quality of results for high-dimensional problems.																	1469-0268	1757-5885				MAR	2020	19	1							2050008	10.1142/S146902682050008X													
J								Variational Autoencoder-Based Dimensionality Reduction for High-Dimensional Small-Sample Data Classification	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										High-dimensional and small-sample size dataset; variational autoencoder; classification; computational biology; deep learning	NONNEGATIVE MATRIX FACTORIZATION; PRINCIPAL COMPONENT ANALYSIS; GENE	Classification problems in which the number of features (dimensions) is unduly higher than the number of samples (observations) is an essential research and application area in a variety of domains, especially in computational biology. It is also known as a high-dimensional small-sample-size (HDSSS) problem. Various dimensionality reduction methods have been developed, but they are not potent with the small-sample-sized high-dimensional datasets and suffer from overfitting and high-variance gradients. To overcome the pitfalls of sample size and dimensionality, this study employed variational autoencoder (VAE), which is a dynamic framework for unsupervised learning in recent years. The objective of this study is to investigate a reliable classification model for high-dimensional and small-sample-sized datasets with minimal error. Moreover, it evaluated the strength of different architectures of VAE on the HDSSS datasets. In the experiment, six genomic microarray datasets from Kent Ridge Biomedical Dataset Repository were selected, and several choices of dimensions (features) were applied for data preprocessing. Also, to evaluate the classification accuracy and to find a stable and suitable classifier, nine state-of-the-art classifiers that have been successful for classification tasks in high-dimensional data settings were selected. The experimental results demonstrate that the VAE can provide superior performance compared to traditional methods such as PCA, fastlCA, FA, NMF, and LDA in terms of accuracy and AUROC.																	1469-0268	1757-5885				MAR	2020	19	1							2050002	10.1142/S1469026820500029													
J								A Robust Deep Neural Network Based Breast Cancer Detection and Classification	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Breast cancer detection; computer aided diagnosis; convolutional neural network; AlexNet-DNN; linear discriminant analysis	MAMMOGRAPHY; MASSES	The exponential upward push in breast cancer cases across the globe has alarmed academia-industries to obtain certain more effect and strong Breast cancer laptop Aided prognosis (BC-CAD) device for breast most cancers detection. Some of techniques have been evolved with focus on case centric segmentation, feature extraction and class of breast cancer Histopathological photos. However, rising complexity and accuracy regularly demands more sturdy answer. Recently, Convolutional Neural community (CNN) has emerged as one of the maximum efferent techniques for medical records evaluation and diverse picture classification issues. On this paper, a notably strong and green BC-CAD solution has been proposed. Our proposed gadget consists of pre-processing, more suitable adaptive learning based totally Gaussian aggregate model (GMM), connected element analysis based vicinity of interest localization, and AlexNet-DNN primarily based characteristic extraction. The precept factor analysis (PCA) and Linear Discriminant analysis (LDA) primarily based on characteristic selection that's used as dimensional discount. One of the blessings of the proposed method is that not one of the current dimensional reduction algorithms hired with SVM to perform breast most cancers detection and class. The overall results acquired signify that the AlexNet-DNN based capabilities at completely connected layer; FC6 together with LDA dimensional discount and SVM-based totally classification outperforms other country-of-artwork techniques for breast cancer detection. The proposed method completed 96.20 for AlexNet-FC6 and 96.70 for AlexNet-FC7 in term of assessment measures.																	1469-0268	1757-5885				MAR	2020	19	1							2050007	10.1142/S1469026820500078													
J								Predicting Maintenance and Rehabilitation Cost for Buildings Based on Artificial Neural Network and Fuzzy Logic	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Cost modeling; fuzzy logic (FL); artificial neural network (ANN); multi-layer regression (MLR); statistical analysis	MODEL	In this paper, the study aims to develop a model for predicting and budgeting maintenance and rehabilitation projects costs for residential buildings throughout their life cycle based on artificial neural network, fuzzy logic and statistical techniques (multi-layer regression). Data consisting of bills of quantities were collected from local Algerian building construction agencies. The principle of costing significant items and work packages has been applied to optimize accurate and efficient document contract model. The results of the research show that neural network has more accuracy with 97% than the multi-layer regression analysis model.																	1469-0268	1757-5885				MAR	2020	19	1							2050001	10.1142/S1469026820500017													
J								Determining Subcategories of Facial Expressions for Improved Performance in Practical Applications	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Facial expression recognition; facial expression categories; multi-view facial expression recognition	CORE AFFECT; RECOGNITION; EMOTION	In the context of facial expression recognition (FER), this paper reviews the fundamental theories of emotions and further explains the key dimensions of a defined emotional space. The main contribution of this paper is to propose a set of novel categorization methods for facial expressions to be used in the design of an automatic FER system. This novel categorization enables the facial expression to be interpreted in a better way that and to be more effective in practical applications of automatic FER systems. In order to validate the feasibility of the proposed categorization methods, a set of experiments is reported which investigates and analyzes the influence that the novel categorization brings to a multi-view FER system.																	1469-0268	1757-5885				MAR	2020	19	1							2050004	10.1142/S1469026820500042													
J								Modified Selfish Herd Optimizer for Function Optimization	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS										Selfish herd optimizer; Levy-flight distribution strategy; global search ability; Benchmark functions	PARTICLE SWARM OPTIMIZATION; LEVY FLIGHT; FIREFLY ALGORITHM; COLONY; SIZE; HYBRID; SPACE	Selfish herd optimizer (SHO) is a new optimization algorithm. However, its optimization performance is not satisfactory. The main reason for this phenomenon is the weak global search ability of SHO. In this paper, in order to increase the global search ability of SHO, we add Levy-flight distribution strategy. To verify the performance of the proposed algorithm, we use 10 benchmark functions as test cases. Experiment results show that our algorithm is more competitive.																	1469-0268	1757-5885				MAR	2020	19	1							2050003	10.1142/S1469026820500030													
J								Computational complexity versus statistical performance on sparse recovery problems	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										sharpness; sparse recovery; error bounds; restart; Renegar's condition number	SIGNAL RECOVERY; CONDITION NUMBER; ALGORITHM; OPTIMIZATION; MINIMIZATION; SHRINKAGE; GEOMETRY	We show that several classical quantities controlling compressed-sensing performance directly match classical parameters controlling algorithmic complexity. We first describe linearly convergent restart schemes on first-order methods solving a broad range of compressed-sensing problems, where sharpness at the optimum controls convergence speed. We show that for sparse recovery problems, this sharpness can be written as a condition number, given by the ratio between true signal sparsity and the largest signal size that can be recovered by the observation matrix. In a similar vein, Renegar's condition number is a data-driven complexity measure for convex programmes, generalizing classical condition numbers for linear systems. We show that for a broad class of compressed-sensing problems, the worst case value of this algorithmic complexity measure taken over all signals matches the restricted singular value of the observation matrix which controls robust recovery performance. Overall, this means in both cases that, in compressed-sensing problems, a single parameter directly controls both computational complexity and recovery performance. Numerical experiments illustrate these points using several classical algorithms.																	2049-8764	2049-8772				MAR	2020	9	1					1	32		10.1093/imaiai/iay020													
J								State evolution for approximate message passing with non-separable functions	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										message passing; compressed sensing; statistical estimation; random matrices	PHASE-TRANSITIONS; ALGORITHMS; SURE; EQUATIONS; RISK	Given a high-dimensional data matrix A epsilon R-mxn, approximate message passing (AMP) algorithms construct sequences of vectors u(t) epsilon R-n, v(t) epsilon R-m, indexed by t epsilon {0, 1, 2 . . .} by iteratively applying Lambda or Lambda(T) and suitable nonlinear functions, which depend on the specific application. Special instances of this approach have been developed-among other applications-for compressed sensing reconstruction, robust regression, Bayesian estimation, low-rank matrix recovery, phase retrieval and community detection in graphs. For certain classes of random matrices A, AMP admits an asymptotically exact description in the high-dimensional limit m, n -> infinity, which goes under the name of state evolution. Earlier work established state evolution for separable nonlinearities (under certain regularity conditions). Nevertheless, empirical work demonstrated several important applications that require non-separable functions. In this paper we generalize state evolution to Lipschitz continuous non-separable nonlinearities, for Gaussian matrices A. Our proof makes use of Bolthausen's conditioning technique along with several approximation arguments. In particular, we introduce a modified algorithm (called LoAMP for Long AMP), which is of independent interest.																	2049-8764	2049-8772				MAR	2020	9	1					33	79		10.1093/imaiai/iay021													
J								Second-order asymptotically optimal statistical classification	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										binary classification; classification with rejection; dispersion; second-order asymptotics; finite-length analyses		Motivated by real-world machine learning applications, we analyse approximations to the non-asymptotic fundamental limits of statistical classification. In the binary version of this problem, given two training sequences generated according to two unknown distributions P-1 and P-2, one is tasked to classify a test sequence that is known to be generated according to either P-1 or P-2. This problem can be thought of as an analogue of the binary hypothesis testing problem, but, in the present setting, the generating distributions are unknown. Due to finite sample considerations, we consider the second-order asymptotics (or dispersion-type) trade-off between type-I and type-II error probabilities for tests that ensure that (i) the type-I error probability for all pairs of distributions decays exponentially fast, and (ii) the type-II error probability for a particular pair of distributions is non-vanishing. We generalize our results to classification of multiple hypotheses with the rejection option.																	2049-8764	2049-8772				MAR	2020	9	1					81	111		10.1093/imaiai/iay023													
J								Low noise sensitivity analysis of l(q)-minimization in oversampled systems	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										high-dimensional linear model; l(q)-regularized least squares; ordinary least squares; LASSO; phase transition; asymptotic mean square error; second-order expansion; classical asymptotics	LARGE UNDERDETERMINED SYSTEMS; LINEAR-EQUATIONS; PHASE-TRANSITION; REGRESSION; SELECTION; LASSO; RECOVERY; NEIGHBORLINESS; GRAPHS	The class of l(q)-regularized least squares (LQLS) are considered for estimating beta epsilon R-p from its n noisy linear observations y = X beta + w. The performance of these schemes are studied under the high-dimensional asymptotic setting in which the dimension of the signal grows linearly with the number of measurements. In this asymptotic setting, phase transition (PT) diagrams are often used for comparing the performance of different estimators. PT specifies the minimum number of observations required by a certain estimator to recover a structured signal, e.g. a sparse one, from its noiseless linear observations. Although PT analysis is shown to provide useful information for compressed sensing, the fact that it ignores the measurement noise not only limits its applicability in many application areas, but also may lead to misunderstandings. For instance, consider a linear regression problem in which n > p and the signal is not exactly sparse. If the measurement noise is ignored in such systems, regularization techniques, such as LQLS, seem to be irrelevant since even the ordinary least squares (OLS) returns the exact solution. However, it is well known that if n is not much larger than p, then the regularization techniques improve the performance of OLS. In response to this limitation of PT analysis, we consider the low-noise sensitivity analysis. We show that this analysis framework (i) reveals the advantage of LQLS over OLS, (ii) captures the difference between different LQLS estimators even when n > p, and (iii) provides a fair comparison among different estimators in high signal-to-noise ratios. As an application of this framework, we will show that under mild conditions LASSO outperforms other LQLS even when the signal is dense. Finally, by a simple transformation, we connect our low-noise sensitivity framework to the classical asymptotic regime in which n/p -> infinity, and characterize how and when regularization techniques offer improvements over ordinary least squares, and which regularizer gives the most improvement when the sample size is large.																	2049-8764	2049-8772				MAR	2020	9	1					113	155		10.1093/imaiai/iay024													
J								Generalized notions of sparsity and restricted isometry property. Part I: a unified framework	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										restricted isometry property; Banach space; dimensionality reduction; compressed sensing	MAJORIZING MEASURES; INEQUALITIES; OPERATORS; RECOVERY; NUMBERS	The restricted isometry property (RIP) is an integral tool in the analysis of various inverse problems with sparsity models. Motivated by the applications of compressed sensing and dimensionality reduction of low-rank tensors, we propose generalized notions of sparsity and provide a unified framework for the corresponding RIP, in particular when combined with isotropic group actions. Our results extend an approach by Rudelson and Vershynin to a much broader context including commutative and non-commutative function spaces. Moreover, our Banach space notion of sparsity applies to affine group actions. The generalized approach in particular applies to high-order tensor products.																	2049-8764	2049-8772				MAR	2020	9	1					157	193		10.1093/imaiai/iay018													
J								Empirical Bayes estimators for high-dimensional sparse vectors	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										sparse estimation; shrinkage estimators; Stein's unbiased risk estimate; soft-thresholding; large deviations; concentration inequalities	SHRINKAGE; SELECTION; LASSO; RISK	The problem of estimating a high-dimensional sparse vector theta epsilon R-n from an observation in i.i.d. Gaussian noise is considered. The performance is measured using squared-error loss. An empirical Bayes shrinkage estimator, derived using a Bernoulli-Gaussian prior, is analyzed and compared with the well-known softthresholding estimator. We obtain concentration inequalities for the Stein's unbiased risk estimate and the loss function of both estimators. The results show that for large n, both the risk estimate and the loss function concentrate on deterministic values close to the true risk. Depending on the underlying theta, either the proposed empirical Bayes (eBayes) estimator or soft thresholding may have smaller loss. We consider a hybrid estimator that attempts to pick the better of the soft-thresholding estimator and the eBayes estimator by comparing their risk estimates. It is shown that (i) the loss of the hybrid estimator concentrates on the minimum of the losses of the two competing estimators, and (ii) the risk of the hybrid estimator is within order 1/root n of the minimum of the two risks. Simulation results are provided to support the theoretical results. Finally, we use the eBayes and hybrid estimators as denoisers in the approximate message passing algorithm for compressed sensing, and show that their performance is superior to the soft-thresholding denoiser in a wide range of settings.																	2049-8764	2049-8772				MAR	2020	9	1					195	234		10.1093/imaiai/iaz001													
J								A characterization of the Non-Degenerate Source Condition in super-resolution	INFORMATION AND INFERENCE-A JOURNAL OF THE IMA										super-resolution; l(1) norm; total variation minimization; LASSO for measures; T-systems	SUPPORT RECOVERY; SPARSE; RECONSTRUCTION; PARAMETERS	In a recent article, Schiebinger et al. provided sufficient conditions for the noiseless recovery of a signal made of M Dirac masses given 2M + 1 observations of, e.g., its convolution with a Gaussian filter, using the basis pursuit for measures. In the present work, we show that a variant of their criterion provides a necessary and sufficient condition for the Non-Degenerate Source Condition that was introduced by Duval and Peyre to ensure support stability in super-resolution. We provide sufficient conditions that, for instance, hold unconditionally for the Laplace kernel, provided one has at least 2M measurements. For the Gaussian filter, we show that those conditions are fulfilled in two very different configurations: samples that approximate the uniform Lebesgue measure or, more surprisingly, samples that are all confined in a sufficiently small interval.																	2049-8764	2049-8772				MAR	2020	9	1					235	269		10.1093/imaiai/iaz002													
J								Punjabi to ISO 15919 and Roman Transliteration with Phonetic Rectification	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Transliteration; Punjabi; phonology; natural language processing; computational linguistics		Transliteration removes the script barriers. Unfortunately, Punjabi is written in four different scripts, i.e., Gurmukhi, Shahmukhi, Devnagri, and Latin. The Latin script is understandable for nearly all factions of the Punjabi community. The objective of our work is to transliterate the Punjabi Gurmukhi script into Latin script. There has been considerable progress in Punjabi to Latin transliteration, but the accuracy of present-day systems is less than 50% (Google Translator has approximately 45% accuracy). We do not have the facility of a rich parallel corpus for Punjabi, so we cannot use the corpus-based techniques of machine learning that are in vogue these days. The existing systems of transliteration follow grapheme-based approach. The grapheme-based transliteration is unable to handle many scenarios such as tones, inherent schwa, glottal stops, nasalization, and gemination. In this article, the grapheme-based transliteration has been augmented with phonetic rectification where the Punjabi script is rectified phonetically before applying character-to-character mapping. Handling the inherent short vowel schwa was the major challenge in phonetic rectification. Instead of following the fixed syllabic pattern, we devised a generic finite state transducer to insert schwa. The accuracy of our transliteration system is approximately 96.82%.																	2375-4699	2375-4702				MAR	2020	19	2							28	10.1145/3359991													
J								Wasf-Vec: Topology-based Word Embedding for Modern Standard Arabic and Iraqi Dialect Ontology	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Word embedding; words features; Topology; orthographic; morphology; phonology; Arabic language; dialect; class-based language modeling; 2D visualizing; words ontology; words classification		Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word's topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word's nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.																	2375-4699	2375-4702				MAR	2020	19	2							22	10.1145/3345517													
J								Subword Attentive Model for Arabic Sentiment Analysis: A Deep Learning Approach	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Arabic; sentiment evaluation; unstructured texts; data augmentation; gated recurrent unit; convolutional neural network		Social media data is unstructured data where these big data are exponentially increasing day to day in many different disciplines. Analysis and understanding the semantics of these data are a big challenge due to its variety and huge volume. To address this gap, unstructured Arabic texts have been studied in this work owing to their abundant appearance in social media Web sites. This work addresses the difficulty of handling unstructured social media texts, particularly when the data at hand is very limited. This intelligent data augmentation technique that handles the problem of less availability of data are used. This article has proposed a novel architecture for hand Arabic words classification and understands based on convolutional neural networks (CNNs) and recurrent neural networks. Moreover, the CNN technique is the most powerful for the analysis of Arabic tweets and social network analysis. The main technique used in this work is character-level CNN and a recurrent neural network stacked on top of one another as the classification architecture. These two techniques give 95% accuracy in the Arabic texts dataset.																	2375-4699	2375-4702				MAR	2020	19	2							29	10.1145/3360016													
J								SentiFars: A Persian Polarity Lexicon for Sentiment Analysis	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Sentiment analysis; polarity lexicon; polarity extraction; classifier combination; translation		There is no doubt about the usefulness of public opinion toward different issues in social media and the World Wide Web. Extracting the feelings of people about an issue from text is not straightforward. Polarity lexicons that assign polarity tags or scores to words and phrases play an important role in sentiment analysis systems. As English is the richest language in this area, getting benefits from existing English resources in order to build new ones has attracted the interest of many researchers in recent years. In this article, we propose a new translation-based approach for building polarity resources in resource-lean languages such as Persian. The results of empirical evaluation of the proposed approach prove its effectiveness. The generated resource is the largest publicly available polarity lexicon for Persian.																	2375-4699	2375-4702				MAR	2020	19	2							21	10.1145/3345627													
J								A Deep Neural Network Framework for English Hindi Question Answering	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Question answering; gated recurrent units; neural networks; attention mechanism; low-resourced languages; snippet generation; character embedding		In this article, we propose a unified deep neural network framework for multilingual question answering (QA). The proposed network deals with the multilingual questions and answers snippets. The input to the network is a pair of factoid question and snippet in the multilingual environment (English and Hindi), and output is the relevant answer from the snippet. We begin by generating the snippet using a graph-based language-independent algorithm, which exploits the lexico-semantic similarity between the sentences. The soft alignment of the question words from the English and Hindi languages has been used to learn the shared representation of the question. The learned shared representation of question and attention-based snippet representation are passed as an input to the answer extraction layer of the network, which extracts the answer span from the snippet. Evaluation on a standard multilingual QA dataset shows the state-of-the-art performance with 39.44 Exact Match (EM) and 44.97 F1 values. Similarly, we achieve the performance of 50.11 Exact Match (EM) and 53.77 F1 values on Translated SQuAD dataset.																	2375-4699	2375-4702				MAR	2020	19	2							25	10.1145/3359988													
J								Children's Story Classification in Indian Languages Using Linguistic and Keyword-based Features	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										K-nearest neighbour; keyword features; latent semantic analysis; linear discriminant analysis; linguistic features; naive Bayes; sparse representation; story classification; support vector machines; text-to-speech; vector space model		The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end. We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy.																	2375-4699	2375-4702				MAR	2020	19	2							30	10.1145/3342356													
J								Filtered Pseudo-parallel Corpus Improves Low-resource Neural Machine Translation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Pseudo-parallel corpus; filtering; low-resource language pairs; round-trip translation; sentence-level similarity metrics; bootstrapping		Large-scale parallel corpora are essential for training high-quality machine translation systems; however, such corpora are not freely available for many language translation pairs. Previously, training data has been augmented by pseudo-parallel corpora obtained by using machine translation models to translate monolingual corpora into the source language. However, in low-resource language pairs, in which only low-accurate machine translation systems can be used, translation quality degrades when a pseudo-parallel corpus is naively used. To improve machine translation performance with low-resource language pairs, we propose a method to effectively expand the training data via filtering the pseudo-parallel corpus using quality estimation based on sentence-level round-trip translation. For experiments with three language pairs that utilized small, medium, and large size parallel corpora, BLEU scores significantly improved for low-resource language pairs. Additionally, the effects of iterative bootstrapping on translation performance quality is investigated; resultingly, it is confirmed that bootstrapping can further improve the translation performance.																	2375-4699	2375-4702				MAR	2020	19	2							24	10.1145/3341726													
J								Word Reordering for Translation into Korean Sign Language Using Syntactically-guided Classification	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Sign language; machine translation; word reordering; neural networks; sentence embedding	MACHINE TRANSLATION	Machine translation aims to break the language barrier that prevents communication with others and increase access to information. Deaf people face huge language barriers in their daily lives, including access to digital and spoken information. There are very few digital resources for sign language processing. In this article, we present a transfer-based machine translation system for translating Korean-to-Korean Sign Language (KSL) glosses, mainly composed of (1) dictionary-based lexical transfer and (2) a hybrid syntactic transfer based on a data-driven model. In particular, we formulate complicated word reordering problems in syntactic transfer as multi-class classification tasks and propose "syntactically guided" data-driven syntactic transfer. The core part of our study is a neural classification model for reordering order-important constituent pairs with a reordering task that is newly designed for Korean-to-KSL translation. The experiment results evaluated on news transcript data show that the proposed system achieves a BLEU score of 0.512 and a RIBES score of 0.425, significantly improving upon the baseline system performance.																	2375-4699	2375-4702				MAR	2020	19	2							31	10.1145/3357612													
J								Transliteration of Arabizi into Arabic Script for Tunisian Dialect	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Natural language processing; diacritization; Tunisian dialect; transliteration; rule-based approach; CRF model; Arabizi corpus		The evolution of information and communication technology has markedly influenced communication between correspondents. This evolution has facilitated the transmission of information and has engendered new forms of written communication (email, chat, SMS, comments, etc.). Most of these messages and comments are written in Latin script, also called Arabizi. Moreover, the language used in social media and SMS messaging is characterized by the use of informal and non-standard vocabulary, such as repeated letters for emphasis, typos, non-standard abbreviations, and nonlinguistic content like emoticons. Since the Tunisian dialect suffers from the unavailability of basic tools and linguistic resources compared to Modern Standard Arabic, we resort to the use of these written sources as a starting point to build large corpora automatically. In the context of natural language processing and to benefit from these networks' data, transliterating from Arabizi to Arabic script is a necessary step because most recently available tools for processing the Tunisian dialect expect Arabic script input. Indeed, the transliteration task can help construct and enrich parallel corpora and dictionaries for the Tunisian dialect and can be useful for developing various natural language processing applications such as sentiment analysis, opinion mining, topic detection, and machine translation. In this article, we focus on converting the Tunisian dialect text that is written in Latin script to Arabic script following the Conventional Orthography for Dialectal Arabic. Then, we propose two models to transliterate Arabizi into Arabic script for the Tunisian dialect, namely a rule-based model and a discriminative model as a sequence classification task based on conditional random fields). In the first model, we use a set of transliteration rules to convert the Tunisian dialect Arabizi texts to Arabic script. In the second model, transliteration is performed both at word and character levels. In the end, our models got a character error rate of 10.47%.																	2375-4699	2375-4702				MAR	2020	19	2							32	10.1145/3364319													
J								Fusion of Spatio-temporal Information for Indic Word Recognition Combining Online and Offline Text Data	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Indic Word Recognition; Offline to Online Conversion; LSTM; Fusion	WRITER IDENTIFICATION; SEGMENTATION; COMBINATION; CLASSIFIERS; RECOVERY	We present a novel Indic handwritten word recognition scheme by fusion of spatio-temporal information extracted from handwritten images. The main challenge in Indic word recognition lies in its complexity because of modifiers, touching characters, and compound characters. Hidden Markov Models (HMMs) are being used to model such data due to their ability to learn sequential data, however, the recognition performance is not satisfactory. We propose here a Long Short-Term Memory (LSTM)-based architecture for offline Indic word recognition. Offline recognition methods usually involve spatial data, whereas it has been observed that online recognition schemes show better performance than the offline methodologies. Online information usually refers to the temporal information obtained from the strokes of the pen tip while writing, which is missing in offline word images. In this article, an effort has been made to extract the online temporal information from offline images using stroke recovery and later it is combined with spatial information in LSTM architecture. During recognition, the character models are trained using both offline and extracted pseudo-online handwritten data separately. Finally, a novel fusion scheme has been used to combine them together. From the experiment, it is noted that recognition performance of handwritten Indic words improves considerably due to the fusion scheme of spatial and temporal data.																	2375-4699	2375-4702				MAR	2020	19	2							33	10.1145/3364533													
J								Isarn Dharma Word Segmentation Using a Statistical Approach with Named Entity Recognition	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Isarn Dharma; Isarn Tham; statistical model; word segmentation approach		In this study, we developed an Isarn Dharma word segmentation system. We mainly focused on solving the word ambiguity and unknown word problems in unsegmented Isarn Dharma text. Ambiguous Isarn Dharma words occur frequently in word construction due to the writing style without tone markers. Thus, words can be interpreted as having different tones and meanings in the same writing text. To overcome these problems, we developed an Isarn Dharma character cluster-(IDCC) based statistical model and affixation and integrated it with the named entity recognition method (IDCC-C-based statistical model and affixation with named entity recognition (NER)). This method integrates the IDCC-based and character-based statistical models to distinguish the word boundaries. The IDCC-based statistical model utilizes the IDCC feature to disambiguate any ambiguous words. The unknown words are handled using the character-based statistical model, based on the character features. In addition, linguistic knowledge is employed to detect the boundaries of a new word based on the construction morphology and NER. In evaluations, we compared the proposed method with various word segmentation methods. The experimental results showed that the proposed method performed slightly better than the other methods when the corpus size increased. Using the test set, the proposed method obtained the best F-measure of 92.19, an F-measure that was better than the IDCC longest matching grouping at 2.85.																	2375-4699	2375-4702				MAR	2020	19	2							27	10.1145/3359990													
J								Extracting Polarity Shifting Patterns from Any Corpus Based on Natural Annotation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Sentiment analysis; natural annotation; polarity shifting; sequence mining; prior polarity		In recent years, online sentiment texts are generated by users in various domains and in different languages. Binary polarity classification (positive or negative) on business sentiment texts can help both companies and customers to evaluate products or services. Sometimes, the polarity of sentiment texts can be modified, making the polarity classification difficult. In sentiment analysis, such modification of polarity is termed as polarity shifting, which shifts the polarity of a sentiment clue (emotion, evaluation, etc.). It is well known that detection of polarity shifting can help improve sentiment analysis in texts. However, to detect polarity shifting in corpora is challenging: (1) polarity shifting is normally sparse in texts. making human annotation difficult; (2) corpora with dense polarity shifting are few; we may need polarity shifting patterns from various corpora. In this article, an approach is presented to extract polarity shifting patterns from any text corpus. For the first time, we proposed to select texts rich in polarity shifting by the idea of natural annotation, which is used to replace human annotation. With a sequence mining algorithm, the selected texts are used to generate polarity shifting pattern candidates, and then we rank them by C-value before human annotation. The approach is tested on different corpora and different languages. The results show that our approach can capture various types of polarity shifting patterns, and some patterns are unique to specific corpora. Therefore, for better performance, it is reasonable to construct polarity shifting patterns directly from the given corpus.																	2375-4699	2375-4702				MAR	2020	19	2							23	10.1145/3345518													
J								Layer-Wise De-Training and Re-Training for ConvS2S Machine Translation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										ConvS2S; neural machine translation; local optimum		The convolutional sequence-to-sequence (ConvS2S) machine translation system is one of the typical neural machine translation (NMT) systems. Training the ConvS2S model tends to get stuck in a local optimum in our pre-studies. To overcome this inferior behavior, we propose to de-train a trained ConvS2S model in a mild way and retrain to find a better solution globally. In particular, the trained parameters of one layer of the NMT network are abandoned by re-initialization while other layers' parameters are kept at the same time to kick off re-optimization from a new start point and safeguard the new start point not too far from the previous optimum. This procedure is executed layer by layer until all layers of the ConvS2S model are explored. Experiments show that when compared to various measures for escaping from the local optimum, including initialization with random seeds, adding perturbations to the baseline parameters, and continuing training (con-training) with the baseline models, our method consistently improves the ConvS2S translation quality across various language pairs and achieves better performance.																	2375-4699	2375-4702				MAR	2020	19	2							26	10.1145/3358414													
J								Neural Conversation Generation with Auxiliary Emotional Supervised Models	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Neural conversation; sequence-to-sequence model; natural language processing		An important aspect of developing dialogue agents involves endowing a conversation system with emotion perception and interaction. Most existing emotion dialogue models lack the adaptability and extensibility of different scenes because of their limitation to require a specified emotion category or their reliance on a fixed emotional dictionary. To overcome these limitations, we propose a neural conversation generation with auxiliary emotional supervised model (nCG-FSM) comprising a sequence-to-sequence (Seq2Seq) generation model and an emotional classifier used as an auxiliary model. The emotional classifier was trained to predict the emotion distributions of the dialogues, which were then used as emotion supervised signals to guide the generation model to generate diverse emotional responses. The proposed nCG-ESM is flexible enough to generate responses with emotional diversity, including specified or unspecified emotions, which can be adapted and extended to different scenarios. We conducted extensive experiments on the popular dataset of Weibo post-response pairs. Experimental results showed that the proposed model was capable of producing more diverse, appropriate, and emotionally rich responses, yielding substantial gains in diversity scores and human evaluations.																	2375-4699	2375-4702				MAR	2020	19	2							19	10.1145/3344788													
J								Order-Sensitive Keywords Based Response Generation in Open-Domain Conversational Systems	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Order sensitive; response generation; conversational system; sequence-to-sequence		External keywords are crucial for response generation models to address the generic response problems in open-domain conversational systems. The occurrence of keywords in a response depends heavily on the order of the keywords as they are generated sequentially. Meanwhile, the order of keywords also affects the semantics of a response. Previous keywords based methods mainly focus on the composite of keywords, while the order of keywords has not been sufficiently discussed. In this work, we propose an order-sensitive keywords based model to explore the influence of the order of keywords in open-domain response generation. It automatically inferences the most suitable order that is optimized to generate a natural and relevant response, and subsequently generates the response using the ordered keywords as building blocks. We conducted experiments on a public Twitter dataset and the results show that our approach outperforms the state-of-the-art baselines in both automatic and human evaluations.																	2375-4699	2375-4702				MAR	2020	19	2							18	10.1145/3343258													
J								Enhanced Double-Carrier Word Embedding via Phonetics and Writing	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Word embedding; phonetic embedding; linguistic		Word embeddings, which map words into a unified vector space, capture rich semantic information. From a linguistic point of view, words have two carriers, speech and writing. Yet the most recent word embedding models focus on only the writing carrier and ignore the role of the speech carrier in semantic expressions. However, in the development of language, speech appears before writing and plays an important role in the development of writing. For phonetic language systems, the written forms are secondary symbols of spoken ones. Based on this idea, we carried out our work and proposed double-carrier word embedding (DCWE). We used DCWE to conduct a simulation of the generation order of speech and writing. We trained written embedding based on phonetic embedding. The final word embedding fuses writing and phonetic embedding. To illustrate that our model can be applied to most languages, we selected Chinese, English, and Spanish as examples and evaluated these models through word similarity and text classification experiments.																	2375-4699	2375-4702				MAR	2020	19	2							20	10.1145/3344920													
J								Interpreting Recurrent Neural Networks Behaviour via Excitable Network Attractors	COGNITIVE COMPUTATION										Recurrent neural networks; Dynamical systems; Network attractors; Bifurcations	ECHO STATE PROPERTY; APPROXIMATION; ITINERANCY; DYNAMICS; SYSTEMS	Machine learning provides fundamental tools both for scientific research and for the development of technologies with significant impact on society. It provides methods that facilitate the discovery of regularities in data and that give predictions without explicit knowledge of the rules governing a system. However, a price is paid for exploiting such flexibility: machine learning methods are typically black boxes where it is difficult to fully understand what the machine is doing or how it is operating. This poses constraints on the applicability and explainability of such methods. Our research aims to open the black box of recurrent neural networks, an important family of neural networks used for processing sequential data. We propose a novel methodology that provides a mechanistic interpretation of behaviour when solving a computational task. Our methodology uses mathematical constructs called excitable network attractors, which are invariant sets in phase space composed of stable attractors and excitable connections between them. As the behaviour of recurrent neural networks depends both on training and on inputs to the system, we introduce an algorithm to extract network attractors directly from the trajectory of a neural network while solving tasks. Simulations conducted on a controlled benchmark task confirm the relevance of these attractors for interpreting the behaviour of recurrent neural networks, at least for tasks that involve learning a finite number of stable states and transitions between them.																	1866-9956	1866-9964				MAR	2020	12	2			SI		330	356		10.1007/s12559-019-09634-2													
J								Energy Consumption Forecasting for the Nonferrous Metallurgy Industry Using Hybrid Support Vector Regression with an Adaptive State Transition Algorithm	COGNITIVE COMPUTATION										Energy consumption forecasting; Support vector regression; Adaptive; State transition algorithm	MULTIOBJECTIVE OPTIMIZATION; MACHINES; MODEL	The nonferrous metallurgy industry is a major energy consumer in China, and accurate energy consumption forecasting for the nonferrous metallurgy industry can help government policymakers with energy planning. For this purpose, a hybrid support vector regression (HSVR) with an adaptive state transition algorithm (ASTA) named ASTA-HSVR is proposed to forecast energy consumption in the nonferrous metallurgy industry. The proposed support vector regression (SVR) model consists of a linear weighting of epsilon-SVR and nu-SVR. The ASTA was developed to optimize the parameters of the HSVR. Two cases of energy consumption from the nonferrous metallurgy industry in China are used to demonstrate the performance of the proposed method. The results indicate that the ASTA-HSVR method is superior to other methods. In this study, a hybrid support vector regression with an adaptive state transition algorithm (ASTA-HSVR) was developed and successfully applied to energy consumption forecasting for the nonferrous metallurgy industry. However, it should be noted that the outliers were not considered in this study. In the future, we expect to extend the ASTA-HSVR method to include energy consumption forecasting problems with outliers.																	1866-9956	1866-9964				MAR	2020	12	2			SI		357	368		10.1007/s12559-019-09644-0													
J								A Novel Algorithm for Online Inexact String Matching and its FPGA Implementation	COGNITIVE COMPUTATION										FPGA; Approximate string matching; Dynamic programming; Systolic arrays	GRANULAR COMPUTING TECHNIQUES; BIT-VECTOR ALGORITHM; FACTOR-BINDING SITES; SEQUENCE; RNA; CLASSIFICATION; MICRORNAS; DISCOVERY; SEARCH	Among the basic cognitive skills of the biological brain in humans and other mammals, a fundamental one is the ability to recognize inexact patterns in a sequence of objects or events. Accelerating inexact string matching procedures is of utmost importance when dealing with practical applications where huge amounts of data must be processed in real time, as usual in bioinformatics or cybersecurity. Inexact matching procedures can yield multiple shadow hits, which must be filtered, according to some criterion, to obtain a concise and meaningful list of occurrences. The filtering procedures are often computationally demanding and are performed offline in a post-processing phase. This paper introduces a novel algorithm for online approximate string matching (OASM) able to filter shadow hits on the fly, according to general purpose priority rules that greedily assign priorities to overlapping hits. A field-programmable gate array (FPGA) hardware implementation of OASM is proposed and compared with a serial software version. Even when implemented on entry-level FPGAs, the proposed procedure can reach a high degree of parallelism and superior performance in time compared to the software implementation, while keeping low the usage of logic elements. This makes the developed architecture very competitive in terms of both performance and cost of the overall computing system.																	1866-9956	1866-9964				MAR	2020	12	2			SI		369	387		10.1007/s12559-019-09646-y													
J								How Deep Should be the Depth of Convolutional Neural Networks: a Backyard Dog Case Study	COGNITIVE COMPUTATION										Non-iterative learning; Principal component analysis; Convolutional neural networks	AI SYSTEMS; CONSTRUCTION; ALGORITHMS	The work concerns the problem of reducing a pre-trained deep neuronal network to a smaller network, with just few layers, whilst retaining the network's functionality on a given task. In this particular case study, we are focusing on the networks developed for the purposes of face recognition. The proposed approach is motivated by the observation that the aim to deliver the highest accuracy possible in the broadest range of operational conditions, which many deep neural networks models strive to achieve, may not necessarily be always needed, desired or even achievable due to the lack of data or technical constraints. In relation to the face recognition problem, we formulated an example of such a use case, the 'backyard dog' problem. The 'backyard dog', implemented by a lean network, should correctly identify members from a limited group of individuals, a 'family', and should distinguish between them. At the same time, the network must produce an alarm to an image of an individual who is not in a member of the family, i.e. a 'stranger'. To produce such a lean network, we propose a network shallowing algorithm. The algorithm takes an existing deep learning model on its input and outputs a shallowed version of the model. The algorithm is non-iterative and is based on the advanced supervised principal component analysis. Performance of the algorithm is assessed in exhaustive numerical experiments. Our experiments revealed that in the above use case, the 'backyard dog' problem, the method is capable of drastically reducing the depth of deep learning neural networks, albeit at the cost of mild performance deterioration. In this work, we proposed a simple non-iterative method for shallowing down pre-trained deep convolutional networks. The method is generic in the sense that it applies to a broad class of feed-forward networks, and is based on the advanced supervise principal component analysis. The method enables generation of families of smaller-size shallower specialized networks tuned for specific operational conditions and tasks from a single larger and more universal legacy network.																	1866-9956	1866-9964				MAR	2020	12	2			SI		388	397		10.1007/s12559-019-09667-7													
J								AEKOC plus : Kernel Ridge Regression-Based Auto-Encoder for One-Class Classification Using Privileged Information	COGNITIVE COMPUTATION										One-class classification; Kernel learning; Kernel ridge regression (KRR); Learning using privileged information (LUPI)	EXTREME LEARNING-MACHINE; SUPPORT VECTOR; CLASSIFIERS; NETWORK; SVM	In recent years, non-iterative learning approaches for kernel have received quite an attention by researchers and kernel ridge regression (KRR) approach is one of them. Recently, KRR-based Auto-Encoder is developed for the one-class classification (OCC) task and named as AEKOC. OCC is generally used for outlier or novelty detection. The brain can detect outlier just by learning from only normal samples. Similarly, OCC also uses only normal samples to train the model, and trained model can be used for outlier detection. In this paper, AEKOC is enabled to utilize privileged information, which is generally ignored by AEKOC or any traditional machine learning technique but usually present in human learning. For this purpose, we have combined learning using privileged information (LUPI) framework with AEKOC, and proposed a classifier, which is referred to as AEKOC+. Privileged information is only available during training but not during testing. Therefore, AEKOC is unable to utilize this information for building the model. However, AEKOC+ can efficiently handle the privileged information due to the inclusion of the LUPI framework with AEKOC. Experiments have been conducted on MNIST dataset and on various other datasets from UCI machine learning repository, which demonstrates the superiority of AEKOC+ over AEKOC. Our formulation shows that AEKOC does not utilize the privileged features in learning; however, formulation of AEKOC+ helps it in learning from the privileged features differently from other available features and improved generalization performance of AEKOC. Moreover, AEKOC+ also outperformed two LUPI framework-based one-class classifiers (i.e., OCSVM+ and SSVDD+).																	1866-9956	1866-9964				MAR	2020	12	2			SI		412	425		10.1007/s12559-019-09705-4													
J								A Cognitively Inspired Clustering Approach for Critique-Based Recommenders	COGNITIVE COMPUTATION										Recommender systems; Critiquing feedback; User model; Machine learning	ALGORITHM	The purpose of recommender systems is to support humans in the purchasing decision-making process. Decision-making is a human activity based on cognitive information. In the field of recommender systems, critiquing has been widely applied as an effective approach for obtaining users' feedback on recommended products. In the last decade, there have been a large number of proposals in the field of critique-based recommenders. These proposals mainly differ in two aspects: in the source of data and in how it is mined to provide the user with recommendations. To date, no approach has mined data using an adaptive clustering algorithm to increase the recommender's performance. In this paper, we describe how we added a clustering process to a critique-based recommender, thereby adapting the recommendation process and how we defined a cognitive user preference model based on the preferences (i.e., defined by critiques) received by the user. We have developed several proposals based on clustering, whose acronyms are MCP, CUM, CUM-I, and HGR-CUM-I. We compare our proposals with two well-known state-of-the-art approaches: incremental critiquing (IC) and history-guided recommendation (HGR). The results of our experiments showed that using clustering in a critique-based recommender leads to an improvement in their recommendation efficiency, since all the proposals outperform the baseline IC algorithm. Moreover, the performance of the best proposal, HGR-CUM-I, is significantly superior to both the IC and HGR algorithms. Our results indicate that introducing clustering into the critique-based recommender is an appealing option since it enhances overall efficiency, especially with a large data set.																	1866-9956	1866-9964				MAR	2020	12	2			SI		428	441		10.1007/s12559-018-9586-5													
J								Creating, Interpreting and Rating Harmonic Colour Palettes Using a Cognitively Inspired Model	COGNITIVE COMPUTATION										Cognitive system; Colour combination; Colour harmony; Colour naming; Colour feeling; Qualitative colour descriptor; Colour operator; Machine learning; Cognitive colour; Cognitive semantics	PREFERENCE; EMOTION	This paper presents a cognitively inspired qualitative theory, QCharm which defines five operators for colour combination based on the qualitative colour descriptor (QCD) and applies these operators to recommend palettes of harmonic colours. Machine learning techniques have been applied to learn the QCD colour coordinates in Kobayashi's colour space, in order to assign the resulting QCharmharmonic-colour palettes to cognitive keywords representing a feeling or a lifestyle. Furthermore, a regression model has been implemented to learn users' preferences based on the COLOURlovers dataset. The resulting model is used as an additional criterion for recommendation. The resulting cognitive system can recommend (i) colour palettes using keywords on feelings/lifestyle, and (ii) colour palettes using the learnt user's preference model. As an example of the practical applicability of the model, a web application, the QCharm tool, has been implemented to provide recommendations to users in an interactive way. The QCharm tool can also extract colour palettes from digital images and assign a cognitive adjective to describe colour combinations, to serve as a starting point for the design process.																	1866-9956	1866-9964				MAR	2020	12	2			SI		442	459		10.1007/s12559-018-9589-2													
J								Doctor Recommendation Based on an Intuitionistic Normal Cloud Model Considering Patient Preferences	COGNITIVE COMPUTATION										Decision support model; Intuitionistic normal cloud model; Medical websites; Doctor recommendation	GROUP DECISION-MAKING; PICTURE FUZZY-SETS; SIMILARITY MEASURES; AGGREGATION OPERATORS; DIVERSITY	Chinese medical websites help patients search for satisfactory doctors via the Internet regardless of time and location. Existing website systems recommend the same doctors for all patients using a global ranking but disregard patient preferences and online reviews. Additionally, these models do not consider the effects of interdependencies among criteria when making recommendations. We propose a systematic decision support model to improve such recommendations using intuitionistic fuzzy sets (IFSs) with the Bonferroni mean (BM) to address interdependencies. Our system accommodates patient preferences using multiple intuitionistic normal clouds (INCs). A case study using production data from , the largest such website, shows that our model improves the diversity and coverage of doctor recommendations while considering patient preferences when compared to the existing approach. This pattern continued with testing using data from several other Chinese healthcare sites. Our proposal is thus both applicable and readily implemented to improve the recommendations of these websites.																	1866-9956	1866-9964				MAR	2020	12	2			SI		460	478		10.1007/s12559-018-9616-3													
J								Regression Test Case Prioritization Based on Fixed Size Candidate Set ART Algorithm	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Regression testing; test case prioritization; randomness; adaptive random testing; exclusive OR	SELECTION	Regression testing is a very time-consuming and expensive testing activity. Many test case prioritization techniques have been proposed to speed up regression testing. Previous studies show that no one technique is always best. Random strategy, as the simplest strategy, is not always so bad. Particularly, when a test suite has higher fault detection capability, the strategy can generate a better result. Nevertheless, due to the randomness, the strategy is not always as satisfactory as expected. In this context, we present a test case prioritization approach using fixed size candidate set adaptive random testing algorithm to reduce the effect of randomness and improve fault detection effectiveness. The distance between pair-wise test cases is assessed by exclusive OR. We designed and conducted empirical studies on eight C programs to validate the effectiveness of the proposed approach. The experimental results, confirmed by a statistical analysis, indicate that the approach we proposed is more effective than random and the total greedy prioritization techniques in terms of fault detection effectiveness. Although the presented approach has comparable fault detection effectiveness to ART-based and the additional greedy techniques, the time cost is much lower. Consequently, the proposed approach is much more cost-effective.																	0218-1940	1793-6403				MAR	2020	30	3					291	320		10.1142/S0218194020500138													
J								Generation and Application of Constrained Interaction Test Suites Using Base Forbidden Tuples with a Mixed Neighborhood Tabu Search	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Constraint interaction testing; combinatorial optimization; tabu search; software testing	PARTICLE SWARM OPTIMIZATION; COMBINATORIAL	To ensure the quality of current highly configurable software systems, intensive testing is needed to test all the configuration combinations and detect all the possible faults. This task becomes more challenging for most modern software systems when constraints are given for the configurations. Here, intensive testing is almost impossible, especially considering the additional computation required to resolve the constraints during the test generation process. In addition, this testing process is exhaustive and time-consuming. Combinatorial interaction strategies can systematically reduce the number of test cases to construct a minimal test suite without affecting the effectiveness of the tests. This paper presents a new efficient search-based strategy to generate constrained interaction test suites to cover all possible combinations. The paper also shows a new application of constrained interaction testing in software fault searches. The proposed strategy initially generates the set of all possible t-tuple combinations; then, it filters out the set by removing the forbidden t-tuples using the Base Forbidden Tuple (BFT) approach. The strategy also utilizes a mixed neighborhood tabu search (TS) to construct optimal or near-optimal constrained test suites. The efficiency of the proposed method is evaluated through a comparison against two well-known state-of-the-art tools. The evaluation consists of three sets of experiments for 35 standard benchmarks. Additionally, the effectiveness and quality of the results are assessed using a real-world case study. Experimental results show that the proposed strategy outperforms one of the competitive strategies, ACTS, for approximately 83% of the benchmarks and achieves similar results to CASA for 65% of the benchmarks when the interaction strength is 2. For an interaction strength of 3, the proposed method outperforms other competitive strategies for approximately 60% and 42% of the benchmarks. The proposed strategy can also generate constrained interaction test suites for an interaction strength of 4, which is not possible for many strategies. The real-world case study shows that the generated test suites can effectively detect injected faults using mutation testing.																	0218-1940	1793-6403				MAR	2020	30	3					363	398		10.1142/S0218194020500151													
J								Automated Test Input Generation via Model Inference Based on User Story and Acceptance Criteria for Mobile Application Development	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										AgileUATM; user story; acceptance criteria; agile methodology; BDD; mobile application testing; test generation; model inference		There has been observed explosive growth in the development of mobile applications (apps) for Android and iOS operating systems, which has led to the direct impact towards mobile app development. In order to design and propose quality-oriented apps, it is the primary responsibility of developers to devote time and sufficient efforts towards testing to make the apps bug-free and operational in the hands of end-users without any hiccup. Manual testing procedures take a prolonged amount of time in writing test cases, and in some cases, the full testing requirements are not met. Besides, the insufficient knowledge of tester also impacts the overall quality and bug-free apps. To overcome the obstacles of testing, we propose a new testing methodology cum tool called "AgileUATM" which works primarily towards white-box and black-box testing. To evaluate the validity of the proposed tool, we put the tool in a real-time operational environment concerning mobile test apps. By using this tool, all the acceptance criteria are determined via user stories. The testers/developers specify requirements with formal specifications based on programs properties, predicates, invariants, and constraints. The results show that the proposed tool generated effective and accurate test cases, test input. Meanwhile, expected output was also generated in a unified fashion from the user stories to meet acceptance criteria. The proposed solution also reduced the development time to identify test data as compared to manual Behavior-Driven Development (BDD) methodologies. This tool can support the developers to get a better idea about the required tests and able to translate the customer's natural languages to computer languages as well. This paper fulfills an approach to suitably test mobile application development.																	0218-1940	1793-6403				MAR	2020	30	3					399	425		10.1142/S0218194020500163													
J								AppPerm Analyzer: Malware Detection System Based on Android Permissions and Permission Groups	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Android malware; Android permission; permission group; static analysis; malware detection; mobile security		Besides the applications aimed at increasing the efficiency of the Android mobile devices, also many malicious applications, millions of Android malware according to various security company reports, are being developed and uploaded into the application stores. In order to detect those applications, a malicious Android application detection system based on permission and permission groups namely, AppPerm Analyzer has been developed. The AppPerm Analyzer software extracts the manifest and code permissions of analyzed applications, creates duple and triple permission groups from them, calculates risk scores of these permissions and permission groups according to their usage rates in malicious and benign applications and calculates the total risk score of the analyzed application. After training the software with 7776 applications in total, it is tested with 1664 benign and 1664 malicious applications. In the tests, AppPerm Analyzer detected malicious applications with an accuracy of 96.19% at most. At this point, sensitivity (true-positive ratio) is 95.50% and specificity (true-negative ratio) is 96.88%. If a false-positive ratio up to 10% is accepted, the sensitivity increases to 99.04%.																	0218-1940	1793-6403				MAR	2020	30	3					427	450		10.1142/S0218194020500175													
J								Towards Generating Realistic and High Coverage Test Data for Constraint-Based Fault Injection	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Fault injection; constraint solving; constraint negation	MUTATION	Generating faulty data is a key issue in fault injection. The faulty data include not only the ones of extreme values or bad formats, but also the ones which are logically unreasonable. Constraint-based fault injection which negates interface constraints to solve faulty data is effective for logically unreasonable data generation. However, the existing constraint-based approaches just solve brand new data for testing. Such brand new data may easily violate some hidden environment constraints on the test inputs and hence be nonrealistic. Besides, there can be different strategies to negate a constraint in order to derive the constraint-unsatisfied faulty data. What are the possible negation strategies and which strategies are better for high coverage fault injection are still unclear. To these ends, this paper presents a new constraint-based fault injection approach. The approach introduces 10 different strategies for constraint negation and relaxes constraint variables to generate faulty data instead of solving brand new data for fault injection. It can produce faulty data which are closer to the original non-faulty ones and hence likely to be more realistic. We experimentally investigated the effectiveness and cost of the introduced constraint negation strategies. The results provide insights for the application of these strategies in fault injection.																	0218-1940	1793-6403				MAR	2020	30	3					451	479		10.1142/S0218194020500187													
J								On the Linguistic Representational Power of Neural Machine Translation Models	COMPUTATIONAL LINGUISTICS												Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.																	0891-2017	1530-9312				MAR	2020	46	1					1	52		10.1162/coli_a_00367													
J								The Design and Implementation of XiaoIce, an Empathetic Social Chatbot	COMPUTATIONAL LINGUISTICS												This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human-machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.																	0891-2017	1530-9312				MAR	2020	46	1					53	93		10.1162/coli_a_00368													
J								An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models	COMPUTATIONAL LINGUISTICS												Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.																	0891-2017	1530-9312				MAR	2020	46	1					95	134		10.1162/coli_a_00369													
J								Data-Driven Sentence Simplification: Survey and Benchmark	COMPUTATIONAL LINGUISTICS											TEXT SIMPLIFICATION; READABILITY; ALGORITHMS	Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.																	0891-2017	1530-9312				MAR	2020	46	1					135	187		10.1162/coli_a_00370													
J								Corpora Annotated with Negation: An Overview	COMPUTATIONAL LINGUISTICS											MACHINE-LEARNING APPROACH; SENTIMENT CLASSIFICATION; SPECULATION DETECTION; CORPUS; SYSTEM; EXTRACTION; MODALITY; SCOPE	Negation is a universal linguistic phenomenon with a great qualitative impact on natural language processing applications. The availability of corpora annotated with negation is essential to training negation processing systems. Currently, most corpora have been annotated for English, but the presence of languages other than English on the Internet, such as Chinese or Spanish, is greater every day. In this study, we present a review of the corpora annotated with negation information in several languages with the goal of evaluating what aspects of negation have been annotated and how compatible the corpora are. We conclude that it is very difficult to merge the existing corpora because we found differences in the annotation schemes used, and most importantly, in the annotation guidelines: the way in which each corpus was tokenized and the negation elements that have been annotated. Differently than for other well established tasks like semantic role labeling or parsing, for negation there is no standard annotation scheme nor guidelines, which hampers progress in its treatment.																	0891-2017	1530-9312				MAR	2020	46	1					189	244		10.1162/coli_a_00371													
J								Method for Constructing Periodic Solutions of a Controlled Dynamic System with a Cylindrical Phase Space	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											LIMIT-CYCLES; NUMERICAL COMPUTATION; OPERATION MODES; OSCILLATIONS; BIFURCATION; EQUATIONS; PENDULUM	We consider a controlled mechanical system with one degree of freedom described by an angular coordinate. The system is under the action of conservative and nonconservative forces. It is assumed that a corresponding dynamic system has a variable parameter that describes the control-impact gain factor. An iterative numerical-analytical method designed to form autorotation modes with assigned properties is proposed. The conditions for the orbital stability of such modes are formulated. The proposed approach represents a modification of the Andronov-Pontryagin method and, as opposed to it, can be applied not only to systems close to Hamiltonian systems but also to a certain class of systems that do not contain a small parameter. An example of the method's application to an aerodynamic pendulum model is presented. The ability to expand the method's convergence domain by using the parameter-continuation procedure is demonstrated.																	1064-2307	1555-6530				MAR	2020	59	2					139	150		10.1134/S1064230720020082													
J								Controlling a Linear MIMO System by a Measurement Vector Using Multilevel Decomposition	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											STATIC-OUTPUT-FEEDBACK; POLE-PLACEMENT; ASSIGNMENT; SPECTRUM	A variant of the synthesis method for controlling a linear MIMO system by an output vector is developed that provides the given spectrum of a multidimensional dynamic system in the state space. It is based on the principle of duality of the task of controlling a MIMO system by a state vector and the task of constructing a state observer. The algorithm, without any changes, is applicable for both continuous and discrete cases of describing the mathematical model of the MIMO system, has no restrictions on specifying the spectrum's elements of the closed MIMO system, and allows us to obtain synthesis problems in analytical form and to parameterize the set of equivalent control laws with feedback. An example of an analytical synthesis of the control law of a hypothetical aircraft is given.																	1064-2307	1555-6530				MAR	2020	59	2					151	160		10.1134/S1064230720020136													
J								Robust Stability Criterion and Design of Optimal Robust Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Based on the Kharitonov criterion, a robust stability limit is constructed. Methods for calculating this limit are developed. An algorithm for designing an optimal robust control system with a given value of this limit is presented. The effectiveness of the algorithm is demonstrated by numerical examples.																	1064-2307	1555-6530				MAR	2020	59	2					161	170		10.1134/S106423072001013X													
J								Separation Theorem in Control Problems for Bundles of Trajectories of Deterministic Linear Switchable Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We consider optimal (on average) control problems and guaranteeing optimal control problems for bundles of trajectories of deterministic linear switchable systems such that the continuous variation of their state is described by differential equations, while the instant discrete changes of their state (switchings) are described by recurrent equations. The switching times and the number of switchings are not given. For one trajectory, the quality of the control is characterized by a quadratic functional taking into account the expenditure for each switching as well. Exact information about the initial state of the system is not known; therefore, control problems for bundles of trajectories are investigated. For linear-quadratic control problems for switching systems, the classical separation principle is not fulfilled. It turns out that its modification called the conditional separation principle holds. We provide academic examples of the synthesis of the optimal control on average and guaranteeing optimal control in which the separation principle is not fulfilled, while the conditional separation principle holds.																	1064-2307	1555-6530				MAR	2020	59	2					171	197		10.1134/S1064230720010025													
J								Parametric Identification of Models of Multicomponent Chemical Systems under Uncertain Initial Data	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A method for identifying models of a given specification based on the use of the maximum admissible estimates of the requisite parameters is presented. This method provides parametric identification of models in the case when prior information about the system under study is characterized by a small number of observations and its uncertain character cannot be neglected. The method is validated for the quantitative analysis of multicomponent fullerene-containing mixtures, which is carried out using the Vierordt method.																	1064-2307	1555-6530				MAR	2020	59	2					198	208		10.1134/S1064230720020069													
J								Energy Efficient Modes of the Motion of Mobile Robots with Orthogonal Stepping Motors when Overcoming Obstacles	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The features of controlling the movement of a walking robot with orthogonal propulsion when overcoming obstacles are considered. The problem of the coordinated control of horizontal and vertical displacement drives is formulated to implement a motion mode that ensures optimality in the given criterion. The criterion of optimality is the minimum heat loss in the drive motors. A methodology for solving the problem and the simulation results are presented.																	1064-2307	1555-6530				MAR	2020	59	2					209	216		10.1134/S1064230720010037													
J								Simulating an Object's Altitude for Two-Position Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A simulation problem for echo signals of two-position radar systems is considered. A four-point configuration of the two-dimensional geometric model is proposed to solve it. Relations are obtained for the arrangement of radiators of the model provided that the signals at both reception points are cophased. Based on these relations, we design an iteration algorithm to form the matrix of radiators to extend the range of possible locations of the simulated object. The obtained theoretical results are confirmed by numerical simulation methods.																	1064-2307	1555-6530				MAR	2020	59	2					217	222		10.1134/S1064230720020070													
J								Decision Making in MultiCriteria Problems at the Image Design Stage of Aviation Rocket Technique	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A number of fundamental decisions that will determine the technical and economic characteristics of the project in the future are laid when creating new products at the image design stage. Therefore, it is very important, on the one hand, to form and, on the other, to evaluate a large number of alternatives that cover a wide space of possible engineering solutions. Alternatives are evaluated by the decision maker (DM) in terms of performing the problems for which the new product is being created. A comparison and analysis of the solution of the multicriteria decision-making problem by the method of value function, concessions, ELECTRE and confident judgments are proposed.																	1064-2307	1555-6530				MAR	2020	59	2					223	231		10.1134/S1064230720020057													
J								Generation of Alternative Connections of Series-Connected Subsystems in a Redundant Equipment Complex	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											CONFIGURATIONS	An analytical approach is developed to integrate redundant equipment complexes (ECs) with linear stationary models. A general technique for managing the redundancy of the complex at the design stage is formulated in the case of ensuring the invariance of the given set of transfer functions. A formalism is obtained for solving the integration problem of a complex when it is configured as a sequence of related subsystems. The formalism includes both the conditions for the admissibility of a preselected configuration and the formula for the entire set of alternative relationships between components that ensure the fulfillment of the given objective function of the complex. An example of a simple navigation system consisting of measuring and indicator subsystems is given.																	1064-2307	1555-6530				MAR	2020	59	2					232	244		10.1134/S1064230720020021													
J								Methods and Models for Decision-Making in Systems Engineering for Creating (Developing) Distributed Organizational Information and Control Systems	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												This article considers the development and systematization of methods and models for decision-making in systems engineering for creating (developing) distributed organizational information and control systems and is a continuation and development of the general provisions and results previously obtained and published by the authors. An improved formulation of the problem of decision-making in systems engineering for creating (developing) distributed organizational information and control systems and the approach to an evaluation of efficiency of the implementation of functional processes in these systems using a complex indicator that takes into account the completeness and quality of implementation of functional processes different stages of the system's life cycle are proposed. The analysis of the algorithmic complexity of the solution of the formulated problem shows that it is impossible to use exact methods, which makes it necessary to develop approximate problem-solving methods. Methods and models for estimating the time and probabilistic time characteristics of the operation of distributed organizational information and control systems, as well as approaches to evaluate the technical and economic parameters of the process of their creation (development), are analyzed. A decision-making algorithm in systems engineering for creating (developing) distributed organizational information and control systems that combines all the considered and proposed methods and models is developed as a result of the research.																	1064-2307	1555-6530				MAR	2020	59	2					245	260		10.1134/S1064230720020033													
J								Fault-Tolerant Integrated Navigation System for an Unmanned Apparatus Using Computer Vision	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The structure and software of a complex navigation system for an unmanned apparatus are presented. It includes a platformless inertial navigation system, receivers of the satellite navigation system, and a computer vision system. An approach to the construction of an algorithm to exclude failures is proposed. Simulation results are provided.																	1064-2307	1555-6530				MAR	2020	59	2					261	275		10.1134/S1064230720020045													
J								Using a Three-Impulse Maneuvering Scheme for Returning from the Lunar Orbit to the Reentry Point of the Earth's Atmosphere	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												A practically useful algorithm for constructing the trajectory of returning from a Lunar orbit to the Earth using one-impulse and three-impulse maneuvering schemes is presented. This algorithm forms the return trajectory to the reentry point of the atmosphere satisfying the given constraints, taking into account the current ephemeris and the fact that the gravitational fields of the Moon and Earth are noncentral. The computer simulation results make it possible to draw conclusions about the advantages and expedience of using a particular maneuvering scheme.																	1064-2307	1555-6530				MAR	2020	59	2					276	288		10.1134/S1064230720010050													
J								Controlling a Satellite using Single-Gimbal Control Moment Gyroscope During Their Spinning and Braking	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											COMMUNICATION SATELLITE; STABILIZATION	The capabilities of using single-gimbal control moment gyroscopes (CMGs) for the attitude control of Earth remote sensing (ERS) satellites are studied. The situation of the incomplete spinning of CMGs used for controlling the angular motion of the satellite when these gyros have a different angular momentum is considered. Control algorithms that can be applied in this situation are proposed, and it is shown that they can be used to stabilize the satellite. Using simulation, it is shown that a satellite's angular motion can be controlled by CMGs having different angular momenta.																	1064-2307	1555-6530				MAR	2020	59	2					289	300		10.1134/S1064230720020100													
J								Control System of an Aerodynamically Adhesive Wall-Climbing Robot	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												We consider the control system of an adaptive wall-climbing robot. The robot adheres to the surface aerodynamically using a centrifugal-type pump. A mathematical model of the robot is constructed and its equilibrium conditions on an arbitrarily oriented plane are investigated. In order to optimize the energy consumption, an algorithm of the adaptive control of an aerodynamic pump is proposed. The results are tested experimentally.																	1064-2307	1555-6530				MAR	2020	59	2					301	309		10.1134/S1064230720010098													
J								A Novel Secure Video Steganography Technique using Temporal Lifted Wavelet Transform and Human Vision Properties	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Data hiding; integer wavelets transform; color space; peak signal to noise ratio; first complement		Steganography is a term that refers to the process of concealing secret data inside a cover media which can be audio, image and video. A new video steganography scheme in the wavelet domain is presented in this paper. Since the convolutional discrete wavelet transform produces float numbers, a lifted wavelet transform is used to conceal data. The method embeds secret data in the detail coefficients of each temporal array of the cover video at spatial localization using a unique embedding via YCbCr color space and complementing the secret data to minimize error in the stego video before embedding. Three secret keys are used in the scheme. Method's performance matrices such as peak signal to noise ratio and Normalized Cross Correlation (NCC) expresses good imperceptibility for the stego-video. The value of Peak Signal to Noise Ratio (PSNR) is in range of 34-40dB, and high embedding capacity.																	1683-3198					MAR	2020	17	2					147	153		10.34028/iajit/17/2/1													
J								A Daubechies DWT Based Image Steganography Using Smoothing Operation	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Steganography; Daubechies DWT; Arnold transform; smoothing operation; genetic algorithm; morphological operation		Steganography is a capability which conceals the top-secret information into cover media (e.g., digital images, sound files etc.,). This Paper presents a secure, higher embedding capacity Discrete Wavelet Transformation (DWT) based technique. Before embedding correlation in between cover and the secret image is increased by multiplying some variable (i.e., 1/k) to the secret image. In embedding process, the Daubechies DWT of both Arnold transformed secret and cover images are taken followed by alpha blending operation. Arnold is a type of scrambling process which increases the confidentiality of secret image and alpha blending is a type of mixing operation of two images, the alpha value indicates the amount of secret image is embedded into the cover image. Daubechies Inverse Discrete Wavelet Transformation (IDWT) of the resulting image is performed to obtain the stego image. Smoothing operation inspired by the Genetic Algorithm (GA) is used to improve the quality of stego-image by minimizing Mean square error and morphological operation is used to extract the image component from the extracted secret image. Simulation results of the proposed steganography technique are also presented. The projected method is calculated on different parameters of image visual quality measurements.																	1683-3198					MAR	2020	17	2					154	161		10.34028/iajit/17/2/2													
J								Toward Building Video Multicast Tree with Congestion Avoidance Capability in Software-Defined Networks	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Network congestion; multicast; Software-Defined Network; congestion avoidance		Network congestion is an obstacle to a Quality of Service (QoS) guarantee for online video applications, because it leads to a high packet loss rate and long transmission delay. In the Software-Defined Network (SDN), the controller can conveniently obtain the network topology and link bandwidth use situation. Based on the above advantages, an SDN-based video multicast routing solution, called Congestion Avoidance Video Multicast (CAVM), is proposed in this paper. CAVM obtains overall network topology, monitors available bandwidth resource and measures the link delays based on the OpenFlow, a popular SDN southbound interface. We introduce a novel multicast routing problem, named the Delay-Constrained and Minimum Congestion-Cost Multicast Routing (DCMCCMR) problem, which finds the multicast tree with the lowest congestion cost and a source-destination delay constraint in the SDN environment. The DCMCCMR problem is NP-hard. CAVM uses an algorithm to solve it in polynomial time. Our experimental results confirm that the proposed algorithm can build multicast trees with good congestion avoidance capability.																	1683-3198					MAR	2020	17	2					162	169		10.34028/iajit/17/2/3													
J								Gammachirp Filter Banks Applied in Roust Speaker Recognition Based on GMM-UBM Classifier	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Feature exfraction; gammachirp filter bank; RASTA; CMVN; GMM-UBM		In this paper, authors propose an auditory feature extraction algorithm in order to improve the performance of the speaker recognition system in noisy environments. In this auditory feature exfraction algorithm, the Gammachirp filter bank is adapted to simulate the auditory model of human cochlea. In addition, the following three techniques are applied: cube-root compression method, Relative Spectral Filtering Technique (RASTA), and Cepsfral Mean and Variance Normalization algorithm (CMVN).Subsequently, based on the theory of Gaussian Mixes Model-Universal Background Model (GMM-UBM), the simulated experiment was conducted. The experimental results implied that speaker recognition systems with the new auditory feature has better robustness and recognition performance compared to Mel-Frequency Cepstral Coefficients (MFCC), Relative Spectral-Perceptual Linear Predictive (RASTA-PLP), Cochlear Filter Cepstral Coefficients (CFCC) and gammatone Frequency Cepsfral Coefficeints (GFCC).																	1683-3198					MAR	2020	17	2					170	177		10.34028/iajit/17/2/4													
J								A New Application for Gabor Filters in Face-Based Gender Classification	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Gabor filters; gender recognition; statistical features; PCA	NEURAL-NETWORKS; RECOGNITION; FEATURES	Human face is one of the most important biometrics as it contains information such as gender, race, and age. Identifying the gender based on human face images is a challenging problem that has been extensively studied due to its various relevant applications. Several approaches were used to address this problem by specifying suitable features. In this study, we present an extension of feature extraction technique based on statistical aggregation and Gabor filters. We extract statistical features from the image of a face after applying Gabor filters; subsequently, we use seven classifiers to investigate the performance of the selected features. Experiments show that the accuracy achieved using the proposed features is comparable to accuracies reported in recent studies. We used seven classifiers to investigate the performance of our proposed features. Experiments reveal that k-Nearest Neighbors algorithm (k-NN), K-Star classifier (K*), and Rotation Forest offer the best accuracies.																	1683-3198					MAR	2020	17	2					178	187		10.34028/iajit/17/2/5													
J								Enhancement of the Heuristic Optimization Based on Extended Space Forests using Classifier Ensembles	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Classifier ensembles; extended space forests; ant colony optimization; decision tree	RANDOM SUBSPACE METHOD; FEATURE-SELECTION	Extended space forests are a matter of common knowledge for ensuring improvements on classification problems. They provide richer feature space and present better performance than the original feature space-based forests. Most of the contemporary studies employs original features as well as various combinations of them as input vectors for extended space forest approach. In this study, we seek to boost the performance of classifier ensembles by integrating them with heuristic optimization-based features. The contributions of this paper are fivefold. First, richer feature space is developed by using random combinations of input vectors and features picked out with ant colony optimization method which have high importance and not have been associated before. Second, we propose widely used classification algorithm which is utilized baseline classifier. Third, three ensemble strategies, namely bagging, random subspace, and random forests are proposed to ensure diversity. Fourth, a wide range of comparative experiments are conducted on widely used biomedicine datasets gathered from the University of California Irvine (UCI) machine learning repository to contribute to the advancement of proposed study. Finally, extended space forest approach with the proposed technique turns out remarkable experimental results compared to the original version and various extended versions of recent state-of-art studies.																	1683-3198					MAR	2020	17	2					188	195		10.34028/iajit/17/2/6													
J								MPKC-based Threshold Proxy Signcryption Scheme	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Multivariate public key cryptosystem; signcryption; threshold proxy signcryption; quantum attack		The threshold proxy signcryption can implement signature and encryption simultaneously in one logical step, and can be used to realize the decentralized protection of the group signature key, so it is an efficient technology for network security. Currently, most of the existing threshold proxy signcryption schemes are designed based on the traditional public key cryptosystems, and their security mainly depends on the difficulty of the large integer decomposition and the discrete logarithm. However, the traditional public key cryptosystems cannot resist the quantum computer attack, which makes the existing threshold proxy signcryption schemes based on traditional public key cryptosystems insecure against quantum attacks. Motivated by these concerns, we proposed a threshold proxy signcryption scheme based on Multivariate Public Key Cryptosystem (MPKC) which is one of the quantum attack-resistent public key algorithms. Under the premise of satisfying the threshold signcryption requirements of the threshold proxy, our scheme can not only realize the flexible participation of the proxy signcrypters but also resist the quantum computing attack. Finally, based on the assumption of Multivariate Quadratic (MQ) problem and Isomorphism Polynomial (IP) problem, the proof of the confidentiality and the unforgeability of the proposed scheme under the random oracle model is given.																	1683-3198					MAR	2020	17	2					196	206		10.34028/iajit/17/2/7													
J								Referential DNA Data Compression using Hadoop Map Reduce Framework	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Compression; map reduce sequences; dna sequences		The indispensable knowledge of Deoxyribonucleic Acid (DNA) sequences and sharply reducing cost of the DNA sequencing techniques has attracted numerous researchers in the field of Genetics. These sequences are getting available at an exponential rate leading to the bulging size of molecular biology databases making large disk arrays and compute clusters inevitable for analysis.In this paper, we proposed referential DNA data compression using hadoop MapReduce Framework to process humongous amount of genetic data in distributed environment on high performance compute clusters. Our method has successfully achieved a better balance between compression ratio and the amount of time required for DNA data compression as compared to other Referential DNA Data Compression methods.																	1683-3198					MAR	2020	17	2					207	214		10.34028/iajit/17/2/8													
J								Intrusion Detection Model Using Naive Bayes and Deep Learning Technique	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Classification; intrusion detection; deep learning; NSL-KDD; genetic algorithm; naive bayes	DETECTION SYSTEM	The increase of security threats and hacking the computer networks are one of the most dangerous issues should treat in these days. Intrusion Detection Systems (IDSs), are the most appropriate methods to prevent and detect the attacks of networks and computer systems. This study presents several techniques to discover network anomalies using data mining tasks, Machine learning technology and dependence of artificial intelligence techniques. In this research, the smart hybrid model was developed to explore any penetrations inside the network. The model divides into two basic stages. The first stage includes the Genetic Algorithm (GA) in selecting the characteristics with depends on a process of extracting, Discretize And dimensionality reduction through Proportional K-Interval Discretization (PKID) and Fisher Linear Discriminant Analysis (FLDA) on respectively. At the end of the first stage combining Naive Bayes classifier (NB) and Decision Table (DT) using NSL-KDD data set divided into two separate groups for training and testing. The second stage completely depends on the first stage outputs (predicted class) and reclassified with multilayer percepfrons using Deep Learning4J (DL) and the use of algorithm Stochastic Gradient Descent (SGD). In order to improve the performance in terms of the accuracy in classification of penetrations, raising the average of discovering and reducing the false alarms. The comparison of the proposed model and conventional models show the superiority of the proposed model and the previous conventional hybrid models. The result of the proposed model is 99.9325 of classification accuracy, the rate of detection is 99.9738 and 0.00093 of false alarms.																	1683-3198					MAR	2020	17	2					215	224		10.34028/iajit/17/2/9													
J								Fault Tolerance Based Load Balancing Approach for Web Resources in Cloud Environment	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Scheduler; checkpoint manager; cloud computing; checkpointing; fault index; high performance computing	ALGORITHM	Cloud computing consists group of heterogeneous resources scattered around the world connected through the network. Since high performance computing is strongly interlinked with geographically distributed service to interact with each other in wide area network, Cloud computing makes the architecture consistent, low-cost, and well-suited with concurrent services. This paper presents a fault tolerance load balancing technique based on resource load and fault index value. The proposed technique works in two phases: resource selection and task execution. The resource selection phase selects the suitable resource for task execution. A resource with least resource load and fault index value is selected for task execution. Further task execution phase sets checkpoints at various intervals for saving the task state periodically. The checkpoints are set at various intervals based on resource fault index. When a task is executed on a resource, fault index value of selected resource is updated accordingly. This reduces the checkpoint overhead by avoiding unnecessary placements of checkpoints. The proposed model is validated on CloudSim and provides improved performance in terms of response time, makespan, throughput and checkpoint overhead in comparison to other state-of-the-art methods.																	1683-3198					MAR	2020	17	2					225	232		10.34028/iajit/17/2/10													
J								Empirical Evaluation of Leveraging Named Entities for Arabic Sentiment Analysis	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Named entity recognition; Arabic sentiment analysis; supervised learning method; lexicon-based method		Social media reflects the attitudes of the public towards specific events. Events are often related to persons, locations or organizations, the so-called Named Entities (NEs). This can define NEs as sentiment-bearing components. In this paper, we dive beyond NEs recognition to the exploitation of sentiment-annotated NEs in Arabic sentiment analysis. Therefore, we develop an algorithm to detect the sentiment of NEs based on the majority of attitudes towards them. This enabled tagging NEs with proper tags and, thus, including them in a sentiment analysis framework of two models: supervised and lexicon-based. Both models were applied on datasets of multi-dialectal content. The results revealed that NEs have no considerable impact on the supervised model, while employing NEs in the lexicon-based model improved the classification performance and outperformed most of the baseline systems.																	1683-3198					MAR	2020	17	2					233	240		10.34028/iajit/17/2/11													
J								A New Vector Representation of Short Texts for Classification	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Text representation; short-text classification; Latent Dirichlet Allocation; topic model		Short and sparse characteristics and synonyms and homonyms are main obstacles for short-text classification. In recent years, research on short-text classification has focused on expanding short texts but has barely guaranteed the validity of expanded words. This study proposes a new method to weaken these effects without external knowledge. The proposed method analyses short texts by using the topic model based on Latent Dirichlet Allocation (LDA), represents each short text by using a vector space model and presents a new method to adjust the vector of short texts. In the experiments, two open short-text data sets composed of google news and web search snippets are utilised to evaluate the classification performance and prove the effectiveness of our method.																	1683-3198					MAR	2020	17	2					241	249		10.34028/iajit/17/2/12													
J								A Hybrid Approach for Providing Improved Link Connectivity in SDN	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Open flow; SDN; link failure; protection and restoration	PROTECTION	Software-Defined Networking (SDN) is a unique approach to design and build networks. The networks services can be better handled by administrators with the abstraction that SDN provides. The problem of re-routing the packets with minimum overhead in case of link failure is handled in this work. Protection and restoration schemes have been used in the past to handle such issues by giving more priority to minimal response time or controller overhead based on the use case. A hybrid scheme has been proposed with per-link Bidirectional forwarding mechanism to handle the failover. The proposed method makes sure that the controller overhead does not impact the flow of packets, thereby decreasing the overall response time, even with guaranteed network resiliency. The computation of the next shortest backup path also guarantees that the subsequent routing of packets always chooses the shortest path available. The proposed method is compared with the traditional approaches and proven by results to perform better with minimal response time.																	1683-3198					MAR	2020	17	2					250	256		10.34028/iajit/17/2/13													
J								On 2-Colorability Problem for Hypergraphs with P-8-free Incidence Graphs	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Hypergraph; Dominating set; P-k-free graph; Computational Complexity		A 2-coloring of a hypergraph is a mapping from its vertex set to a set of two colors such that no edge is monochromatic. The hypergraph 2- Coloring Problem is the question whether a given hypergraph is 2-colorable. It is known that deciding the 2-colorability of hypergraphs is NP-complete even for hypergraphs whose hyperedges have size at most 3. In this paper, we present a polynomial time algorithm for deciding if a hypergraph, whose incidence graph is P-8-free and has a dominating set isomorphic to C-8, is 2-colorable or not. This algorithm is semi generalization of the 2-colorability algorithm for hypergraph, whose incidence graph is P-7 freepresented by Camby and Schaudt.																	1683-3198					MAR	2020	17	2					257	263		10.34028/iajit/17/2/14													
J								Enhanced Median Flow Tracker for Videos with Illumination Variation Based on Photometric Correction	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Illumination variation; median flow tracker; entropy; gamma correction	FACE RECOGNITION; OBJECT TRACKING; NORMALIZATION	Object tracking is a fundamental task in video surveillance, human-computer interaction and activity analysis. One of the common challenges in visual object fracking is illumination variation. A large number of methods for tracking have been proposed over the recent years, and median flow tracker is one of them which can handle various challenges. Median flow tracker is designed to track an object using Lucas-Kanade optical flow method which is sensitive to illumination variation, hence fails when sudden illumination changes occur between the frames. In this paper, we propose an enhanced median flow fracker to achieve an illumination invariance to abruptly varying lighting conditions. In this approach, illumination variation is compensated by modifying the Discrete Cosine Transform (DCT) coefficients of an image in the logarithmic domain. The illumination variations are mainly reflected in the low frequency coefficients of an image. Therefore, a fixed number of DCT coefficients are ignored. Moreover, the Discrete Cosine (DC) coefficient is maintained almost constant all through the video based on entropy difference to minimize the sudden variations of lighting impacts. In addition, each video frame is enhanced by employing pixel transformation technique that improves the contrast of dull images based on probability distribution of pixels. The proposed scheme can effectively handle the gradual and abrupt changes in the illumination of the object. The experiments are conducted on fast-changing illumination videos, and results show that the proposed method improves median flow tracker with outperforming accuracy compared to the state-of-the-art trackers.																	1683-3198					MAR	2020	17	2					264	271		10.34028/iajit/17/2/15													
J								FAAD: A Self-Optimizing Algorithm for Anomaly Detection	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Anomaly detection; outliers; firefly algorithm; big data; parallel algorithms and apache spark	OUTLIER DETECTION; OPTIMIZATION	Anomaly/Outlier detection is the process of finding abnormal data points in a dataset or data stream. Most of the anomaly detection algorithms require setting of some parameters which significantly affect the performance of the algorithm. These parameters are generally set by hit-and-trial; hence performance is compromised with default or random values. In this paper, the authors propose a self-optimizing algorithm for anomaly detection based on firefly meta-heuristic, and named as Firefly Algorithm for Anomaly Detection (FAAD). The proposed solution is a non-clustering unsupervised learning approach for anomaly detection. The algorithm is implemented on Apache Spark for scalability and hence the solution can handle big data as well. Experiments were conducted on various datasets, and the results show that the proposed solution is much accurate than the standard algorithms of anomaly detection.																	1683-3198					MAR	2020	17	2					272	280		10.34028/iajit/17/2/16													
J								Smart Infrastructure for Future Urban Mobility	AI MAGAZINE											CONTROL-SYSTEM; REAL-TIME; SIGNAL; ARCHITECTURE; TECHNOLOGY	Real-time traffic signal control presents a challenging multiagent planning problem, particularly in urban road networks where, unlike simpler arterial settings, there are competing dominant traffic flows that shift through the day. Further complicating matters, urban environments require attention to multimodal traffic flows (vehicles, pedestrians, bicyclists, buses) that move at different speeds and may be given different priorities. For the past several years, my research group has been developing and refining a real-time, adaptive traffic signal control system to address these challenges, referred to as scalable urban traffic control (Surtrac). Combining principles from automated planning and scheduling, multiagent systems, and traffic theory, Surtrac treats traffic signal control as a decentralized online planning process. In operation, each intersection repeatedly generates and executes (in rolling horizon fashion) signal- timing plans that optimize the movement of currently sensed approaching traffic through the intersection. Each time a new plan is produced (nominally every couple of seconds), the intersection communicates to its downstream neighbors what traffic it expects to send their way, allowing intersections to construct longer horizon plans and achieve coordinated behavior. Initial evaluation of Surtrac in the field has demonstrated significant performance improvements, and the technology is now deployed and operating in several U.S. cities. More recent work has focused on integrating real-time adaptive signal control with emerging connected vehicle technology, and exploration of the opportunities for enhanced mobility that direct vehicle (or pedestrian) to infrastructure communication can provide. Current technology development efforts center on vehicle route sharing, smart transit priority, safe intersection crossing for pedestrians with disabilities, real-time incident detection, and integrated optimization of signal control and route choice decisions. This article provides an overview of this overall research effort.																	0738-4602					SPR	2020	41	1					5	18															
J								Sketch Worksheets in Science, Technology, Engineering, and Mathematics Classrooms: Two Deployments	AI MAGAZINE												Sketching is a valuable but underutilized tool for science education. Sketch worksheets were developed to help change this, by using artificial intelligence technology to give students immediate feedback and to give instructors assistance in grading. Sketch worksheets use automatically computed visual representations combined with conceptual information to give feedback to students, by computing analogies between students' sketches and an instructor's solution sketch. This enables domain experts to develop sketch worksheets, to facilitate dissemination. We describe our experiences in deploying them in geoscience and artificial intelligence classes. The geoscience worksheets, authored by geoscientists at University of WisconsinMadison, were used at both Wisconsin and Northwestern University. The artificial intelligence worksheets were developed and used at Northwestern. Our experience indicates that sketch worksheets can provide helpful on-thespot feedback to students, and significantly improve grading efficiency, to the point where sketching assignments can be more practical to use broadly in science, technology, engineering, and mathematics education.																	0738-4602					SPR	2020	41	1					19	32															
J								Addressing the Challenges of Government Service Provision with Artificial Intelligence	AI MAGAZINE												In complete contract theory, the main approach to limit moral hazard is through modifying incentives for the agents. However, such modifications are not always feasible. One prominent example is Chinese government service provision. Over the years, it has been plagued with inefficiencies as a result of moral hazard. Previous attempts to address these challenges are not effective, as reforms on civil servant incentives face stiff hindrance. In this article, we report an alternative platform - SmartHS - to address these challenges in China without modifying incentives. Through dynamic teamwork, automation of key steps involved in service provision, and improved transparency with the help of artificial intelligence, it places civil servants into an environment that promotes efficiency and reduces the opportunities for moral hazard. Deployment tests in the field of social insurance service provision in three Chinese cities involving close to 3 million social insurance service cases per year demonstrated that the proposed approach significantly reduces moral hazard symptoms. The findings are useful for informing current policy discussions on government reform in China and have the potential to address long-standing problems in government service provision to benefit almost onefifth of the world's population.																	0738-4602					SPR	2020	41	1					33	43															
J								Ascend by Evolv: Artificial Intelligence-Based Massively Multivariate Conversion Rate Optimization	AI MAGAZINE												Conversion rate optimization (CRO) means designing an e-commerce web interface so that as many users as possible take a desired action such as registering for an account, requesting a contact, or making a purchase. Such design is usually done by hand, evaluating one change at a time through A/B testing, evaluating all combinations of two or three variables through multivariate testing, or evaluating multiple variables independently. Traditional CRO is thus limited to a small fraction of the design space only, and often misses important interactions between the design variables. This article describes Ascend by Evolv,1 an automatic CRO system that uses evolutionary search to discover effective web interfaces given a human-designed search space. Design candidates are evaluated in parallel online with real users, making it possible to discover and use interactions between the design elements that are difficult to identify otherwise. A commercial product since September 2016, Ascend has been applied to numerous web interfaces across industries and search space sizes, with up to fourfold improvements over human design. Ascend can therefore be seen as massively multivariate CRO made possible by artificial intelligence.																	0738-4602					SPR	2020	41	1					44	60															
J								Governance, Risk, and Artificial Intelligence	AI MAGAZINE												Artificial intelligence, whether embodied as robots or Internet of Things, or disembodied as intelligent agents or decision-support systems, can enrich the human experience. It will also fail and cause harms, including physical injury and financial loss as well as more subtle harms such as instantiating human bias or undermining individual dignity. These failures could have a disproportionate impact because strange, new, and unpredictable dangers may lead to public discomfort and rejection of artificial intelligence. Two possible approaches to mitigating these risks are the hard power of regulating artificial intelligence, to ensure it is safe, and the soft power of risk communication, which engages the public and builds trust. These approaches are complementary and both should be implemented as artificial intelligence becomes increasingly prevalent in daily life.																	0738-4602					SPR	2020	41	1					61	69															
J								Experiences and Insights for Collaborative Industry-Academic Research in Artificial Intelligence	AI MAGAZINE												The factors that define and influence the success of industry-academic research in artificial intelligence have evolved significantly in the last decade. In this article, we consider what success means from both sides of a collaboration and offer our perspectives on how to approach the opportunities and challenges that come with achieving success. These perspectives are grounded on the recent and significant investments that have been made between IBM and several higher education institutions around the world, including IBM's Artificial Intelligence Horizons Network, the Massachusetts Institute of Technology-IBM Watson Artificial Intelligence Lab, and the Massachusetts Institute of Technology Quest for Intelligence.																	0738-4602					SPR	2020	41	1					70	81															
J								Patterns and Antipatterns, Principles, and Pitfalls: Accountability and Transparency in Artificial Intelligence	AI MAGAZINE												This article discusses a set of principles for accountability and transparency in AI as well as a set of antipatterns or harmful trends too often seen in deployed systems. It provides concrete suggestions for what can be done to shift the balance away from these antipatterns and toward more positive ones.																	0738-4602					SPR	2020	41	1					82	89															
J								What I Wish I Had Known Early in Graduate School but Didn't - and How to Prepare For a Good Job Afterward	AI MAGAZINE												Begin with the end in mind!(1) PhD students in artificial intelligence can start to prepare for their career after their PhD degree immediately when joining graduate school, and probably in many more ways than they think. To help them with that, I asked current PhD students and recent PhD computer-science graduates from the University of Southern California and my own PhD students to recount the important lessons they learned (perhaps too late) and added the advice of Nobel Prize and Turing Award winners and many other researchers (including my own reflections), to create this article.																	0738-4602					SPR	2020	41	1					90	100															
J								A Riemannian approach for free-space extraction and path planning using catadioptric omnidirectional vision	IMAGE AND VISION COMPUTING										Catadioptric vision; Riemannian metric; Geodesic distance; Free-space segmentation; Path planning		This paper presents a Riemannian approach for free-space extraction and path planning using color catadioptric vision. The problem is formulated considering color catadioptric images as Riemannian manifolds and solved using the Riemannian Eikonal equation with an anisotropic fast marching numerical scheme. This formulation allows the integration of adapted color and spatial metrics in an incremental process. First, the traversable ground (namely free-space) is delimited using a color structure tensor built on the multidimensional components of the catadioptric image. Then, the Eikonal equation is solved in the image plane incorporating a generic metric tensor for central catadioptric systems. This built Riemannian metric copes with the geometric distortions in the catadioptric image plane introduced by the curved mirror in order to compute the geodesic distance map and the shortest path between image points. We present comparative results using Euclidean and Riemannian distance transforms and show the effectiveness of the Riemannian approach to produce safest path planning. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103872	10.1016/j.imavis.2020.103872													
J								GANILLA: Generative adversarial networks for image to illustration translation	IMAGE AND VISION COMPUTING										Generative adversarial networks; Image to image translation; Illustrations style transfer		In this paper, we explore illustrations in children's books as a new domain in unpaired image-to-image translation. We show that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the sametime. We propose a new generator network to address this issue and show that the resulting network strikes a better balance between style and content. There are no well-defined or agreed-upon evaluation metrics for unpaired image-to-image translation. So far, the success of image translation models has been based on subjective, qualitative visual comparison on a limited number of images. To address this problem, we propose a new framework for the quantitative evaluation of image-to-illustration models, where both content and style are taken into account using separate classifiers. In this new evaluation framework, our proposed model performs better than the current state-of-the-art models on the illustrations dataset. Our code and pretrained models can be found at https://github.com/giddyyupp/ganilla. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103886	10.1016/j.imavis.2020.103886													
J								EML-NET: An Expandable Multi-Layer NETwork for saliency prediction	IMAGE AND VISION COMPUTING										Saliency detection; Scalability; Loss function	CONVOLUTIONAL NEURAL-NETWORK; ALLOCATION	Existing advanced saliency systems have been proposed using Convolutional Neural Networks (CNNs), the prior knowledge of objectness is crucial for saliency detection. However, we show that the use of objectness may also limit the power of CNNs on the images which do not contain a salient object. Besides, one previous study has shown applying a deeper CNN model may not improve the performance, the reason behind could be due to the limited training data. In this work, we aim at investigating the effect of prior knowledge from other domains and proposing a multi-modality system. Our work shows the depth of the model still plays an important role in saliency detection when the training data is large enough. To do this in a sophisticated manner can be complex, and also result in unwieldy networks or produce competing objectives that are hard to balance. For the scalability, our multi-modality system is trained in an almost end-to-end piece-wise fashion. The encoder and decoder components are separately trained to deal with complexity tied to the computational paradigm and required space. Therefore, our system can be easily extended to include a variety of prior knowledge for saliency detection. Besides, we also propose a combined saliency loss based on modifications of Pearson correlation and normalized scanpath saliency. Our experiment shows the combined loss can train a CNN model more comprehensively for saliency detection. We denote our expandable multi-layer network as EML-NET and our method achieves the state-of-the-art results on the public saliency benchmarks, SALICON, MIT300 and CAT2000. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103887	10.1016/j.imavis.2020.103887													
J								Local-adaptive and outlier-tolerant image alignment using RBF approximation	IMAGE AND VISION COMPUTING										image alignment; radial basis function; scattered data approximation; outlier removal; computer vision	POLYHARMONIC SPLINES; REGISTRATION; WARPS	Image alignment is a crucial step to generate a high quality panorama. The state-of-the-art approaches use local-adaptive transformations to deal with multi-view parallax, but still suffer from unreliable feature correspondences and high computational cost. In this paper, we propose a local-adaptive and outlier-tolerant image alignment method using RBF (radial basis function) approximation. To eliminate the visible artifacts, the input images are warped according to a constructed projection error function, whose parameters are estimated by solving a linear system. The outliers are efficiently removed by screening out the abnormal weights of RBFs, such that better alignment quality can be achieved compared to the existing approaches. Moreover, a weight assignment strategy is introduced to further address the overfitting issues caused by extrapolation, and hence the global projectivity can be well preserved. The proposed method is computationally efficient, whose performance is verified by comparative experiments on several challenging cases. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103890	10.1016/j.imavis.2020.103890													
J								Salient object detection based on backbone enhanced network	IMAGE AND VISION COMPUTING										Salient object detection; Deep learning; Backbone network; Over-fitting	MODEL	The Convolutional Neural Networks (CNNs) with encoder-decoder architecture has shown powerful ability in semantic segmentation and it has also been applied in saliency detection. In most researches, the parameters of the backbone network which have been pre-trained on the ImageNet dataset will be retrained using the new training dataset to let CNNs adapt to the new task better. But the retraining will weaken generalization of the pre-trained backbone network and result in over-fitting, especially when the scale of the new training data is not very large. To make a balance between generalization and precision, and to further improve the performance of the CNNs with encoder-decoder architecture in salient object detection, we proposed a framework with enhanced backbone network (BENet). A encoder with structure of dual backbone networks (DBNs) is adopted in BENet to extract more diverse feature maps. In addition, BENet includes a connection module based on improved Res2Net to efficiently fuse feature maps from the two backbone networks and a decoder based on weighted multi-scale feedback module (WMFM) to perform synchronous learning. Our approach is extensively evaluated on six public datasets, and experimental results show significant and consistent improvements over the state-of-the-art methods without any additional supervision. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103876	10.1016/j.imavis.2020.103876													
J								Person re-identification with expanded neighborhoods distance reranking	IMAGE AND VISION COMPUTING										Person re-identification; Re-ranking; Expanded neighborhoods distance; Two-level neighborhoods	OBJECT RETRIEVAL; ACCURATE	In the person re-identification (re-ID) community, pedestrians often have great changes in appearance, and there are many similar persons, which incurs will degrades the accuracy. Re-ranking is an effective method to solve these problems, this paper proposes an expanded neighborhoods distance (END) to re-rank the re-ID results. We assume that if the two persons in different image are same, their initial ranking lists and two-level neighborhoods will be very similar when they are taken as the query. Our method follows the principle of similarity, and selects expanded neighborhoods in initial ranking list to calculate the END distance. Final distance is calculated as the combination of the END distance and Jaccard distance. Experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets confirm the effectiveness of the novel re-ranking method in this article. Compare with re-ID baseline, the proposed method in this paper increases mAP by 14.2% on Market-1501 and Rank1 by 12.9% on DukeMTMC-reID. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103875	10.1016/j.imavis.2020.103875													
J								A complementary regression network for accurate face alignment	IMAGE AND VISION COMPUTING										Facial landmark detection; Complementary regression network; Coordinate-to-heatmap transform; Heatmap-to-coordinate transform		This paper proposes a complementary regression network (CRN) that combines global and local regression methods to align faces. A global regression network (GRN) generates the coordinates of facial landmark points directly such that all facial feature points are fitted to the input face on the whole and a local regression network (LRN) generates the heatmap of facial landmark points such that each channel localizes the detail of its facial landmark point well. The CRN converts the GRN's coordinates to another heatmap, then uses with the LRN's heatmap to get the final facial landmark points. The CRN works complementarily such that the GRN's overall fitting tendency compensates for the LRN's poor alignment caused by missing local information, whereas the LRN's detailed representation compensates for the GRN's poor alignment caused by global miss-fitting. We conducted several experiments on the 300-W public dataset, the 300-W private dataset, and the Menpo dataset and the proposed CRN achieved 3.14%, 3.74%, and 1.996% the-state-of-art face alignment accuracy in terms of percentage of normalized mean error, respectively. (C) 2020 Published by Elsevier B.V.																	0262-8856	1872-8138				MAR	2020	95								103883	10.1016/j.imavis.2020.103883													
J								Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment	IMAGE AND VISION COMPUTING										Unsupervised domain adaptation; Semantic segmentation; Adversarial learning; Transfer learning; Image-to-image translation		The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model, based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task. An additional couple of discriminators, working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103889	10.1016/j.imavis.2020.103889													
J								Depth-guided view synthesis for light field reconstruction from a single image	IMAGE AND VISION COMPUTING										Light field; Convolutional neural network; Depth estimation; View synthesis; View inpainting		Light field imaging has recently become a promising technology for 3D rendering and displaying. However, capturing real-world light field images still faces many challenges in both the quantity and quality. In this paper, we develop a learning based technique to reconstruct light field from a single 2D RGB image. It includes three steps: unsupervised monocular depth estimation, view synthesis and depth-guided view inpainting. We first propose a novel monocular depth estimation network to predict disparity maps of each sub-aperture views from the central view of light field. Then we synthesize the initial sub-aperture views by using the warping scheme. Considering that occlusion makes synthesis ambiguous for pixels invisible in the central view, we present a simple but effective fully convolutional network (FCN) for view inpainting. Note that the proposed network architecture is a general framework for light field reconstruction, which can be extended to take a sparse set of views as input without changing any structure or parameters of the network. Comparison experiments demonstrate that our method outperforms the state-of-the-art light field reconstruction methods with single-view input, and achieves comparable results with the multi-input methods. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103874	10.1016/j.imavis.2020.103874													
J								Attention-guided RGBD saliency detection using appearance information	IMAGE AND VISION COMPUTING										RGBD; Saliency; Bottom-up; Top-down; Attention; Appearance	OBJECT DETECTION; FUSION; FEATURES	Most of the deep convolutional neural networks (CNNs) based RGBD saliency models either regard the RGB and depth cues as the same status or trust the depth information excessively. However, they ignore that the low-quality depth map is an interference factor and the multi-level deep features that originated from RGB images contain abundant appearance information. Therefore, we propose a novel RGBD saliency model, where the attention-guided bottom-up and top-down modules are powerfully combined by using multilevel deep RGB features, to utilize the deep RGB and depth features in a sufficient and appropriate way. Concretely, a two-stream structure based bottom-up module is first constructed to dig and fuse the RGB and depth information, yielding the fused deep feature. Besides, the module embeds the depth cue based attention maps to guide the indication of salient objects. Then, considering the abundant appearance information, a top-down module is deployed to perform coarse-to-fine saliency inference, where the fused deep feature is progressively integrated with appearance information. Similarly, the attention map is also inserted into this module for locating salient objects. Extensive experiments are performed on five public RGBD datasets and the corresponding experimental results firmly demonstrate the effectiveness and superiority of our model when compared with the state-of-the-art RGBD saliency models. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAR	2020	95								103888	10.1016/j.imavis.2020.103888													
J								A Cryptographic Ensemble for secure third party data analysis: Collaborative data clustering without data owner participation	DATA & KNOWLEDGE ENGINEERING										Data mining as a service; Privacy preserving data mining; Security; Data outsourcing	PRIVACY PRESERVING DATA	This paper introduces the twin concepts Cryptographic Ensembles and Global Encrypted Distance Matrices (GEDMs), designed to provide a solution to outsourced secure collaborative data clustering. The cryptographic ensemble comprises: Homomorphic Encryption (HE) to preserve raw data privacy, while supporting data analytics; and Multi-User Order Preserving Encryption (MUOPE) to preserve the privacy of the GEDM. Clustering can therefore be conducted over encrypted datasets without requiring decryption or the involvement of data owners once encryption has taken place, all with no loss of accuracy. The GEDM concept is applicable to large scale collaborative data mining applications that feature horizontal data partitioning. In the paper DBSCAN clustering is adopted for illustrative and evaluation purposes. The results demonstrate that the proposed solution is both efficient and accurate while maintaining data privacy.																	0169-023X	1872-6933				MAR	2020	126				SI				101734	10.1016/j.datak.2019.101734													
J								Query processing on large graphs: Approaches to scalability and response time trade offs	DATA & KNOWLEDGE ENGINEERING										Graph query processing; Plan generation; Query evaluation on partitioned graphs; Scalability; Map/Reduce		Graphs, being an expressive data structure, have become increasingly important for modeling real-world applications, such as collaboration, different kinds of transactions, social networks, to name a few. With the advent of social networks and the web, the graph sizes have grown too large to fit in main memory precipitating the need for alternative approaches for an efficient, scalable evaluation of queries on graphs of any size. In this paper, we use the time-tested "divide and conquer" approach by partitioning a graph into desired number of partitions (and possibly with appropriate characteristics) and process queries over those partitions to obtain all or specified number of answers. This entails correctly computing answers that span multiple partitions or even need the same partition more than once. Given a set of partitions, there are a number of approaches using which a query can be evaluated: (i) One Partition At a Time (OPAT) approach, (ii) Traditional use of Multiple Processors (TraditionalMP), and (iii) using the Map/Reduce Multi-Processor approach (MapReduceMP) approach. The first approach, detailed in this paper, has established scalability through independent processing of partitions. The other two approaches address response time in addition to scalability. For the OPAT query evaluation approach, necessary minimal book keeping has been identified and its correctness established in this paper. Query answering on partitioned graphs also requires analyzing partitioning schemes for their impact on query processing and determining the number as well as the sequence in which partitions need to be loaded to reduce the response time for processing queries. We correlate query properties and partition characteristics to reduce query processing time in terms of the resources available. We also identify a set of quantitative metrics and use them for formulating heuristics to determine the order of loading partitions for efficient query processing. For OPAT approach, extensive experiments on large graphs (synthetic and real-world) using different partitioning schemes analyze the proposed heuristics on a variety of query types. The other two approaches are fleshed out, analyzed, and contrasted with the OPAT approach. An existing graph querying system has been extended to evaluate queries on partitioned graphs. Finally all three approaches are compared for their strengths and weaknesses.																	0169-023X	1872-6933				MAR	2020	126				SI				101736	10.1016/j.datak.2019.101736													
J								Discovering rare correlated periodic patterns in multiple sequences	DATA & KNOWLEDGE ENGINEERING										Periodic pattern; Rare pattern; Correlated pattern; Sequences	FREQUENT PATTERNS; TRANSACTIONAL DATABASES; ASSOCIATIONS	Periodic-Frequent Pattern Mining (PFPM) is an emerging problem, which consists of identifying frequent patterns that periodically occur over time in a sequence of events. Though PFPM is useful in many domains, traditional algorithms have two important limitations. First, they are not designed to find rare patterns. But discovering rare patterns is useful in many domains (e.g. to study rare diseases). Second, traditional PFPM algorithms are generally designed to find patterns in a single sequence, but identifying periodic patterns that are common to a set of sequences is often desirable (e.g. to find patterns common to several hospital patients or customers). To address these limitations, this paper proposes to discover a novel type of patterns in multiple sequences called Rare Correlated Periodic Patterns. Properties of the problem are studied, and an efficient algorithm named MRCPPS (Mining Rare Correlated Periodic Patterns common to multiple Sequences) is presented to efficiently find these patterns. It relies on a novel RCPPS-list structure to avoid repeatedly scanning the database. Experiments have been done on several real datasets, and it was observed that the proposed MRCPPS algorithm can efficiently discover all rare correlated periodic patterns common to multiple sequences, and filter many non rare and correlated patterns.																	0169-023X	1872-6933				MAR	2020	126				SI				101733	10.1016/j.datak.2019.101733													
J								CloudDBGuard: A framework for encrypted data storage in NoSQL wide column stores	DATA & KNOWLEDGE ENGINEERING										Property-preserving encryption; NoSQL databases; Wide Column Stores		Nowadays, cloud storage providers are widely used for outsourcing data. These remote cloud servers are not trustworthy when storing sensitive data. In this article we focus on the use case of storing data in a cloud database using a particular sub-category of NoSQL databases - so-called wide column stores. Unfortunately security was not a primary concern of the NoSQL systems designers. Using encryption before outsourcing the data can provide security. Conventional encryption however limits the options for interaction because the encrypted data lacks properties of the plaintext data that the database systems rely on. Various schemes have been proposed for property-preserving encryption in order to overcome these issues, allowing a database to process queries over encrypted data. In this article we comprehensively present details of our framework CloudDBGuard that allows using property-preserving encryption in unmodified wide column stores. It hides the complexity of the encryption and decryption process and allows various adjustments on specific use cases in order to achieve a maximum of security, functionality and performance.																	0169-023X	1872-6933				MAR	2020	126				SI				101732	10.1016/j.datak.2019.101732													
J								A Transformation-Based Framework for KNN Set Similarity Search	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Similarity search; KNN; jaccard; indexing	EFFICIENT; QUERIES	Set similarity search is a fundamental operation in a variety of applications. While many previous studies focus on threshold based set similarity search and join, few efforts have been paid for KNN set similarity search. In this paper, we propose a transformation based framework to solve the problem of KNN set similarity search, which given a collection of set records and a query set, returns results with the largest similarity to the query. We devise an effective transformation mechanism to transform sets with various lengths to fixed length vectors which can map similar sets closer to each other. Then, we index such vectors with a tiny tree structure. Next, we propose efficient search algorithms and pruning strategies to perform exact KNN set similarity search. We also design an estimation technique by leveraging the data distribution to support approximate KNN search, which can speed up the search while retaining high recall. Experimental results on real world datasets show that our framework significantly outperforms state-of-the-art methods in both memory and disk based settings.																	1041-4347	1558-2191				MAR 1	2020	32	3					409	423		10.1109/TKDE.2018.2886189													
J								ChronoGraph: Enabling Temporal Graph Traversals for Efficient Information Diffusion Analysis over Time	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Semantics; Parallel processing; Time factors; Syntactics; Databases; Prototypes; Standards; ChronoGraph; temporal networks; temporal graph; graph traversal language; temporal aggregation; parallelism		ChronoGraph is a novel system enabling temporal graph traversals. Compared to snapshot-oriented systems, this traversal-oriented system is suitable for analyzing information diffusion over time without violating a time constraint on temporal paths. The cornerstone of ChronoGraph aims at bridging the chasm between point-based semantics and period-based semantics and the gap between temporal graph traversals and static graph traversals. Therefore, our graph model and traversal language provide the temporal syntax for both semantics, and we present a method converting point-based semantics to period-based ones. Also, ChronoGraph exploits the temporal support and parallelism to handle the temporal degree, which explosively increases compared to static graphs. We demonstrate how three traversal recipes can be implemented on top of our system: temporal breadth-first search (tBFS), temporal depth-first search (tDFS), and temporal single source shortest path (tSSSP). According to our evaluation, our temporal support and parallelism enhance temporal graph traversals in terms of convenience and efficiency. Also, ChronoGraph outperforms existing property graph databases in terms of temporal graph traversals. We prototype ChronoGraph by extending Tinkerpop, a de facto standard for property graphs. Therefore, we expect that our system would be readily accessible to existing property graph users.																	1041-4347	1558-2191				MAR 1	2020	32	3					424	437		10.1109/TKDE.2019.2891565													
J								Deep Inductive Graph Representation Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Graph representation learning; inductive representation learning; relational function learning; transfer learning; graph-based feature learning; higher-order structures		This paper presents a general inductive graph representation learning framework called DeepGL for learning deep node and edge features that generalize across-networks. In particular, DeepGL begins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that naturally generalize across-networks and are therefore useful for graphbased transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of O(vertical bar E vertical bar), and scalable for large networks via an efficient parallel implementation. Compared with recent methods, DeepGL is (1) effective for across-network transfer learning tasks and large (attributed) graphs, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 106x speedup in runtime performance, and (4) accurate with an average improvement in AUC of 20 percent or more on many learning tasks and across a wide variety of networks.																	1041-4347	1558-2191				MAR 1	2020	32	3					438	452		10.1109/TKDE.2018.2878247													
J								Dynamic Connection-Based Social Group Recommendation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Recommender systems; Collaboration; Social groups; Motion pictures; Australia; Data mining; Media; Group recommendation; social item; social connection; collaborative filtering	SIMILARITY; SVD	Group recommendation has become highly demanded when users communicate in the forms of group activities in online sharing communities. These group activities include student group study, family TV program watching, friends travel decision, etc. Existing group recommendation techniques mainly focus on the small user groups. However, online sharing communities have enabled group activities among thousands of users. Accordingly, recommendation over large groups has become urgent. In this paper, we propose a new framework to accomplish this goal by exploring the group interests and the connections between group users. We first divide a big group into different interest subgroups, each of which contains users closely connected with each other and sharing the similar interests. Then, for each interest subgroup, our framework exploits the connections between group users to collect a comparably compact potential candidate set of media-user pairs, on which the collaborative filtering is performed to generate an interest subgroup-based recommendation list. After that, a novel aggregation function is proposed to integrate the recommended media lists of all interest subgroups as the final group recommendation results. Extensive experiments have been conducted on two real social media datasets to demonstrate the effectiveness and efficiency of our proposed approach.																	1041-4347	1558-2191				MAR 1	2020	32	3					453	467		10.1109/TKDE.2018.2879658													
J								Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Correlation; Predictive models; Urban areas; Matrix converters; Sparse matrices; Sun; Deep learning; spatio-temporal data; urban computing	VOLUMES	Predicting flows (e.g., the traffic of vehicles, crowds, and bikes), consisting of the in-out traffic at a node and transitions between different nodes, in a spatio-temporal network plays an important role in transportation systems. However, this is a very challenging problem, affected by multiple complex factors, such as the spatial correlation between different locations, temporal correlation among different time intervals, and external factors (like events and weather). In addition, the flow at a node (called node flow) and transitions between nodes (edge flow) mutually influence each other. To address these issues, we propose a multitask deep-learning framework that simultaneously predicts the node flow and edge flow throughout a spatio-temporal network. Based on fully convolutional networks, our approach designs two sophisticated models for predicting node flow and edge flow, respectively. These two models are connected by coupling their latent representations of middle layers, and trained together. The external factor is also integrated into the framework through a gating fusion mechanism. In the edge flow prediction model, we employ an embedding component to deal with the sparse transitions between nodes. We evaluate our method based on the taxicab data in Beijing and New York City. Experimental results show the advantages of our method beyond 11 baselines, such as ConvLSTM, CNN, and Markov Random Field.																	1041-4347	1558-2191				MAR 1	2020	32	3					468	478		10.1109/TKDE.2019.2891537													
J								Learning New Words from Keystroke Data with Local Differential Privacy	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										New words; keystroke data; differential privacy; robust regression		Keystroke data collected from smart devices includes various sensitive information about users. Collecting and analyzing such data raise serious privacy concerns. Google and Apple have recently applied local differential privacy (LDP) to address privacy issue on learning new words from users' keystroke data. However, these solutions require multiple LDP reports for a single word, which result in inefficient use of privacy budget and high computational cost. In this paper, we develop a novel algorithm for learning new words under LDP. Unlike the existing solutions, the proposed method generates only one LDP report for a single word. This enables the proposed method to use full privacy budget for generating a report and brings the benefit that the proposed method provides better utility at the same privacy degree than the existing methods. In our algorithm, each user appends a hash value to new word and sends only one LDP report of an n-gram selected randomly from the string packed by each new word and its hash value. The server then decodes frequent n-grams at each position of the string and discovers the candidate words by exploring graph-theoretic links between n-grams and checking integrity of candidates with hash values. Frequencies of frequent new words discovered are estimated from distribution estimates of n-grams by robust regression. We theoretically show that our algorithm can recover popular new words even though the server does not know the domain of the raw data. In addition, we theoretically and empirically demonstrate that our algorithm achieves higher accuracy compared to the existing solutions.																	1041-4347	1558-2191				MAR 1	2020	32	3					479	491		10.1109/TKDE.2018.2885749													
J								Re-Revisiting Learning on Hypergraphs: Confidence Interval, Subgradient Method, and Extension to Multiclass	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Linear programming; Semisupervised learning; Laplace equations; Task analysis; Image edge detection; Standards; Machine learning; Semi-supervised learning; hypergraph model; subgradient method		We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our subgradient method gives faster running times when the number of vertices is much larger than the number of edges. Our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy. Furthermore, our model can be readily extended to capture multiclass learning.																	1041-4347	1558-2191				MAR 1	2020	32	3					506	518		10.1109/TKDE.2018.2880448													
J								SAL-Hashing: A Self-Adaptive Linear Hashing Index for SSDs	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Indexes; Parallel processing; Proposals; Bandwidth; Terminology; Drives; Linear hashing; solid state drives; self-adaptive	EXPLOITING INTERNAL PARALLELISM; FLASH	Flash memory based solid state drives (SSDs) have emerged as a new alternative to replace magnetic disks due to their high performance and low power consumption. However, random writes on SSDs are much slower than SSD reads. Therefore, traditional index structures, which are designed based on the symmetrical I/O property of magnetic disks, cannot completely exert the high performance of SSDs. In this paper, we propose an SSD-optimized linear hashing index called Self-Adaptive Linear Hashing (SAL-hashing) to reduce small random-writes to SSDs that are caused by index operations. The contributions of our work are manifold. First, we propose to organize the buckets of a linear hashing index into groups and sets to facilitate coarse-grained writes and adaptivity to access patterns. A group consisting of a fixed number of buckets is proposed to transform small random writes to buckets into coarse-grained writes and in turn improve write performance of the index. A set consists of a number of groups, and we propose to employ different split strategies for each set. With this mechanism, SAL-hashing is able to adapt to the changes of access patterns. Second, we attach a log region to each set, and amortize the cost of reads and writes by committing updates to the log region in batch. Third, in order to reduce search cost, each log region is equipped with Bloom filters to index update logs. We devise a cost-based online algorithm to adaptively merge the log region with the corresponding set when the set becomes search-intensive. Fourth, we propose a new technique called virtual split to optimize the search performance of SAL-hashing. Finally, we propose a new scheme for the management of the log buffer. We conduct extensive experiments on real SSDs. The results suggest that our proposal is self-adaptive according to the change of access patterns, and outperforms several competitors under various workloads.																	1041-4347	1558-2191				MAR 1	2020	32	3					519	532		10.1109/TKDE.2018.2884714													
J								Scheduling Resources to Multiple Pipelines of One Query in a Main Memory Database Cluster	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Pipelines; Servers; Task analysis; Parallel processing; Optimal scheduling; Job shop scheduling; Main memory database; query processing; resource scheduling; independent parallelism; preemption; filling	TASK; EXECUTION; WORKLOADS	To fully utilize the resources of a main memory database cluster, we additionally take the independent parallelism into account to parallelize multiple pipelines of one query. However, scheduling resources to multiple pipelines is an intractable problem. Traditional static approaches to this problem may lead to a serious waste of resources and suboptimal execution order of pipelines, because it is hard to predict the actual data distribution and fluctuating workloads at compile time. In response, we propose a dynamic scheduling algorithm, List with Filling and Preemption (LFPS), based on two novel techniques. (1) Adaptive filling improves resource utilization by issuing more extra pipelines to adaptively fill idle resource "holes" during execution. (2) Rank-based preemption strictly guarantees scheduling the pipelines on the critical path first at run time. Interestingly, the latter facilitates the former filling idle "holes" with best efforts to finish multiple pipelines as soon as possible. We implement LFPS in our prototype database system. Under the workloads of TPC-H, experiments show our work improves the finish time of parallelizable pipelines from one query up to 2.5X than a static approach and 2.1X than a serialized execution.																	1041-4347	1558-2191				MAR 1	2020	32	3					533	546		10.1109/TKDE.2018.2884724													
J								Unlocking Author Power: On the Exploitation of Auxiliary Author-Retweeter Relations for Predicting Key Retweeters	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Social network services; Predictive models; Prediction algorithms; Computers; Technological innovation; Information processing; Task analysis; Microblogging; key retweeter prediction; information propagation; user behavior	INFLUENTIAL SPREADERS; NETWORKS	Retweeting is a powerful driving force in information propagation on microblogging sites. However, identifying the most effective retweeters of a message (called the "key retweeter prediction" problem) has become a significant research topic. Conventional approaches have addressed this topic from two main aspects: by analyzing either the personal attributes of microblogging users or the structures of user graph networks. However, according to sociological findings, author-retweeter dependencies also play a crucial role in influencing message propagation. In this paper, we propose a novel model to solve the key retweeter prediction problem by incorporating the auxiliary relations between a tweet author and potential retweeters. Without loss of generality, we formulate the relations from four relational factors: status relation, temporal relation, locational relation, and interactive relation. In addition, we propose a novel method, called "Relation-based Learning to Rank (RL2R)," to determine the key retweeters for a given tweet by ranking the potential retweeters in terms of their spreadability. The experimental results show that our method outperforms the state-of-the-art algorithms at top-k retweeter prediction, achieving a significant relative average improvement of 19.7-29.4 percent. These findings provide new insights for understanding user behaviors on social media for key retweeter prediction purposes.																	1041-4347	1558-2191				MAR 1	2020	32	3					547	559		10.1109/TKDE.2018.2889664													
J								User Interface Derivation for Business Processes	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Interviews; Data models; Containers; Proposals; Data mining; Business process; participant role; user interface		User Interfaces (UI) are the bridge to connect Business Processes (BPs) and end users. The implementation of UIs normally needs a lot of manual efforts of developers. Aiming to resolve this issue, this work proposes a UI derivation method with a role-enriched BP (REBP) model as its foundation. This process model has the capability to present the details of task control flow and data operations in tasks. A set of control flow patterns and data operation patterns is identified. For each participant role, tasks of a process are abstracted and aggregated, then data relationships are extracted according to the identified control flow patterns and data operation patterns. A set of mandatory and recommended rules has been developed for deriving the UI logic from a BP. The solution for the UI derivation has been provided and implemented in the prototype. This proposed UI derivation method can provide help for the analysis, design, and maintenance of UI components of BPs.																	1041-4347	1558-2191				MAR 1	2020	32	3					560	573		10.1109/TKDE.2019.2891655													
J								Using Latent Knowledge to Improve Real-Time Activity Recognition for Smart IoT	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Hidden Markov models; Probability distribution; Real-time systems; Windows; Deep learning; Microsoft Windows; Activity recognition; streaming data; latent knowledge; activity prediction; unsupervised learning		Real-time/online activity recognition (AR) is an important technology in smart Internet of Things (IoT) systems where users are assisted by smart devices in their daily activities. How to generate appropriate feature representation from sensor event streaming is a challenging issue for accurate and efficient real-time AR. Previous AR models that rely on explicit domain knowledge are not appropriate for online recognition of complex human activities. We propose to use unsupervised learning to learn about the latent knowledge and embed the activity probability distribution prediction as high-level features to boost real-time AR performance. The proposed approach first learns the latent knowledge from explicit-activity window sequences using unsupervised learning, and derives the probability distribution prediction of activity classes for a given sliding window. Our approach then feeds the prediction with other basic features of the sliding window into a classifier to produce the final class result on each event-count sliding window. Experiments on five smart home datasets show that the proposed method achieves a higher accuracy by at least 20 percent improvement on F1_score than previous traditional algorithms, while maintaining a lower time cost than deep learning based methods. An analysis on the feature importance shows that the addition of probability distribution prediction about activity classes leads to a promising direction for real-time AR.																	1041-4347	1558-2191				MAR 1	2020	32	3					574	587		10.1109/TKDE.2019.2891659													
J								Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Depression; early detection; linguistic metadata; convolutional neural network; word embeddings	MENTAL-HEALTH-CARE; LANGUAGE USE; SEEKING; ADULTS; WORDS	Depression is ranked as the largest contributor to global disability and is also a major reason for suicide. Still, many individuals suffering from forms of depression are not treated for various reasons. Previous studies have shown that depression also has an effect on language usage and that many depressed individuals use social media platforms or the internet in general to get information or discuss their problems. This paper addresses the early detection of depression using machine learning models based on messages on a social platform. In particular, a convolutional neural network based on different word embeddings is evaluated and compared to a classification based on user-level linguistic metadata. An ensemble of both approaches is shown to achieve state-of-the-art results in a current early detection task. Furthermore, the currently popular ERDE score as metric for early detection systems is examined in detail and its drawbacks in the context of shared tasks are illustrated. A slightly modified metric is proposed and compared to the original score. Finally, a new word embedding was trained on a large corpus of the same domain as the described task and is evaluated as well.																	1041-4347	1558-2191				MAR 1	2020	32	3					588	601		10.1109/TKDE.2018.2885515													
J								VA-Store: A Virtual Approximate Store Approach to Supporting Repetitive Big Data in Genome Sequence Analyses	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Bioinformatics (genome or protein) databases; data storage representations; query processing; algorithms for data and knowledge management	INDEXING METHOD; ALIGNMENT; CHALLENGES	In recent years, we have witnessed an increasing demand to process big data in numerous applications. It is observed that there often exist substantial amounts of repetitive data in different portions of a big data repository/dataset for applications such as genome sequence analyses. In this paper, we present a novel method, called the VA-Store, to reduce the large space requirement for repetitive data in prevailing genome sequence analysis tasks usingk(0)-mers (i.e., subsequences of length k(0)) with multiple k values. The VA-Store maintains a physical store for one portion of the input dataset (i.e., k(0) k(0)-mers) and supports multiple virtual stores for other portions of the dataset (i.e., kk-mers with k(0) k(0)). Utilizing important relationships among repetitive data, the VA-Store transforms a given query on a virtual store into one or more queries on the physical store for execution. Both precise and approximate transformations are considered. Accuracy estimation models for approximate solutions are derived. Query optimization strategies are suggested to improve query performance. Our experiments using real and synthetic datasets demonstrate that the VA-Store is quite promising in providing effective storage and efficient query processing for solving a kernel database problem on repetitive big data for genome sequence analysis applications.																	1041-4347	1558-2191				MAR 1	2020	32	3					602	616		10.1109/TKDE.2018.2885952													
J								Optimisation for image salient object detection based on semantic-aware clustering and CRF	IET COMPUTER VISION										feature extraction; optimisation; pattern clustering; neural nets; object detection; image classification; saliency maps; semantically similar images; semantic-aware clustering; CRF; training images; image semantic features; image classification; grid search method; image salient object detection; optimisation methods; salient object detection neglect; different imperfections; object detection methods		State-of-the-art optimisation methods for salient object detection neglect that saliency maps of different images usually show different imperfections. Therefore, the saliency maps of some images cannot achieve effective optimisation. Based on the observation that the saliency maps of semantically similar images usually show similar imperfections, the authors propose an optimisation method for salient object detection based on semantic-aware clustering and conditional random field (CRF), named CCRF. They first cluster the training images into some clusters using the image semantic features extracted by using a deep convolutional neural network model for image classification. Then for each cluster, they use a CRF to optimise the saliency maps generated by existing salient object detection methods. A grid search method is used to compute the optimal weights of the kernels of the CRF. The saliency maps of the testing images are optimised by the corresponding CRFs with the optimal weights. The experimental results with 13 typical salient object detection methods on four datasets show that the proposed CCRF algorithm can effectively improve the results of a variety of image salient object detection methods and outperforms the compared optimisation methods.																	1751-9632	1751-9640				MAR	2020	14	2					49	58		10.1049/iet-cvi.2019.0063													
J								Scale specified single shot multibox detector	IET COMPUTER VISION										object detection; computer vision; feature maps; multibranch architecture; scale specific inference module; scale specific training scheme; object instances; single-stage detectors; single shot multibox detector; computer vision; receptive fields; image pyramid strategy; single-shot based detector; specific scale range; COCO detection; PASCAL VOC; scale specified single-shot multibox detector; scale specified single-shot multibox detector		Detecting objects at vastly different scales is a fundamental challenge in computer vision. To solve this, some approaches (e.g. TridentNet) investigate the effect of receptive fields, whereas other approaches (e.g. SNIP, SNIPER) are based on the image pyramid strategy. In this study, a novel single-shot based detector, called scale specified single-shot multibox detector (4SD) is proposed. It aims to predict objects of a specific scale range separately by using feature maps of different sizes. First, a parallel multi-branch architecture with feature maps of different sizes is generated by scale specific inference module. Then, the authors propose a scale specific training scheme to specialise each branch by sampling object instances of proper scales for training. Results are shown on both PASCAL VOC and COCO detection. The proposed method can achieve a mean average precision of 83.1% on PASCAL VOC 2007, and 36.9% on MS-COCO at a speed of 28 frames per second, which is superior to most single-stage detectors.																	1751-9632	1751-9640				MAR	2020	14	2					59	64		10.1049/iet-cvi.2019.0461													
J								ADN for object detection	IET COMPUTER VISION										object detection; feature extraction; computer vision; ADN; object detection; large-scale diversity; location uncertainty; semantic information; attentional detection network; feature maps; extra attention branch; classic detection network; aspect ratios; extra anchors; different object detectors		Owing to large-scale diversity and location uncertainty in object detection, how to enrich semantic information has become an important issue that attracts a lot of concern. In this study, the authors propose a novel attentional detection network (ADN) to enrich semantic information of feature maps by adding an extra attention branch to the classic detection network. Compared to previous methods (e.g. feature pyramid network (FPN), single shot multibox detector (SSD)) that producing massive anchors in different layers of feature maps to detect objects with different scales and aspect ratios, which is very time-consuming, their network is lightweight and do not need to produce extra anchors. Furthermore, ADN can be applied to different object detectors with little computational cost. Extensive experiments indicate that ADN has good detection performance on different datasets without bells and whistles.																	1751-9632	1751-9640				MAR	2020	14	2					65	72		10.1049/iet-cvi.2018.5651													
J								Dynamical Emergence Theory (DET): A Computational Account of Phenomenal Consciousness	MINDS AND MACHINES										Perception; Vision; Awareness; Neural dynamics; Macrostates; Metastability	INTEGRATED INFORMATION; BRAIN; METASTABILITY; REPRESENTATION; NEUROSCIENCE; HYPOTHESIS; MODELS; NEUROPHENOMENOLOGY; CLASSIFICATION; EXPLANATION	Scientific theories of consciousness identify its contents with the spatiotemporal structure of neural population activity. We follow up on this approach by stating and motivating Dynamical Emergence Theory (DET), which defines the amount and structure of experience in terms of the intrinsic topology and geometry of a physical system's collective dynamics. Specifically, we posit that distinct perceptual states correspond to coarse-grained macrostates reflecting an optimal partitioning of the system's state space-a notion that aligns with several ideas and results from computational neuroscience and cognitive psychology. We relate DET to existing work, offer predictions for empirical studies, and outline future research directions.																	0924-6495	1572-8641				MAR	2020	30	1					1	21		10.1007/s11023-020-09516-9													
J								The Physical Mandate for Belief-Goal Psychology	MINDS AND MACHINES										Folk psychology; Intentional stance; Teleology		This article describes a heuristic argument for understanding certain physical systems in terms of properties that resemble the beliefs and goals of folk psychology. The argument rests on very simple assumptions. The core of the argument is that predictions about certain events can legitimately be based on assumptions about later events, resembling Aristotelian 'final causation'; however, more nuanced causal entities (resembling fallible beliefs) must be introduced into these types of explanation in order for them to remain consistent with a causally local Universe.																	0924-6495	1572-8641				MAR	2020	30	1					23	45		10.1007/s11023-020-09515-w													
J								Ethical Foresight Analysis: What it is and Why it is Needed?	MINDS AND MACHINES										Artificial intelligence; Data science; Ethics; Foresight analysis; Machine learning; Innovation	TECHNOLOGY-ASSESSMENT; POLICY DELPHI; SOCIOLOGY	An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed.																	0924-6495	1572-8641				MAR	2020	30	1					77	97		10.1007/s11023-020-09521-y													
J								"Oh, Dignity too?" Said the Robot: Human Dignity as the Basis for the Governance of Robotics	MINDS AND MACHINES										Human dignity; Governance; Robotics; Human-robot interaction; Healthcare robots	HEALTH; LAW	Healthcare robots enable practices that seemed far-fetched in the past. Robots might be the solution to bridge the loneliness that the elderly often experience; they may help wheelchair users walk again, or may help navigate the blind. European Institutions, however, acknowledge that human contact is an essential aspect of personal care and that the insertion of robots could dehumanize caring practices. Such instances of human-robot interactions raise the question to what extent the use and development of robots for healthcare applications can challenge the dignity of users. In this article, therefore, we explore how different robot applications in the healthcare domain support individuals in achieving 'dignity' or pressure it. We argue that since healthcare robot applications are novel, their associated risks and impacts may be unprecedented and unknown, thus triggering the need for a conceptual instrument that is binding and remains flexible at the same time. In this respect, as safety rules and data protection are often criticized to lack flexibility, and technology ethics to lack enforceability, we suggest human dignity as the overarching governance instrument for robotics, which is the inviolable value upon which all fundamental rights are grounded.																	0924-6495	1572-8641				MAR	2020	30	1					121	143		10.1007/s11023-019-09514-6													
J								INPUT RECONSTRUCTION BY FEEDBACK CONTROL FOR THE SCHLOGL AND FITZHUGH-NAGUMO EQUATIONS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										semilinear parabolic equation; input reconstruction	SPARSE OPTIMAL-CONTROL; PARAMETER; DESIGN	Dynamical reconstruction of unknown time-varying controls from inexact measurements of the state function is investigated for a semilinear parabolic equation with memory. This system includes as particular cases the Schlogl model and the FitzHugh-Nagumo equations. A numerical method is suggested that is based on techniques of feedback control. An error analysis is performed. Numerical examples confirm the theoretical predictions.																	1641-876X	2083-8492				MAR	2020	30	1					5	22		10.34768/amcs-2020-0001													
J								AN INFORMATION BASED APPROACH TO STOCHASTIC CONTROL PROBLEMS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										stochastic control; feedback; information; entropy	ENTROPY FORMULATION	An information based method for solving stochastic control problems with partial observation is proposed. First, information-theoretic lower bounds of the cost function are analysed. It is shown, under rather weak assumptions, that reduction in the expected cost with closed-loop control compared with the best open-loop strategy is upper bounded by a non-decreasing function of mutual information between control variables and the state trajectory. On the basis of this result, an information based control (IBC) method is developed. The main idea of IBC consists in replacing the original control task by a sequence of control problems that are relatively easy to solve and such that information about the system state is actively generated. Two examples of the IBC operation are given. It is shown that the method is able to find an optimal solution without using dynamic programming at least in these examples. Hence the computational complexity of IBC is substantially smaller than that of dynamic programming, which is the main advantage of the proposed method.																	1641-876X	2083-8492				MAR	2020	30	1					23	34		10.34768/amcs-2020-0002													
J								NONLINEAR MODEL PREDICTIVE CONTROL FOR PROCESSES WITH COMPLEX DYNAMICS: A PARAMETERISATION APPROACH USING LAGUERRE FUNCTIONS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										process control; nonlinear model predictive control; Laguerre functions; linearisation	SYSTEM; IDENTIFICATION	Classical model predictive control (MPC) algorithms need very long horizons when the controlled process has complex dynamics. In particular, the control horizon, which determines the number of decision variables optimised on-line at each sampling instant, is crucial since it significantly affects computational complexity. This work discusses a nonlinear MPC algorithm with on-line trajectory linearisation, which makes it possible to formulate a quadratic optimisation problem, as well as parameterisation using Laguerre functions, which reduces the number of decision variables. Simulation results of classical (not parameterised) MPC algorithms and some strategies with parameterisation are thoroughly compared. It is shown that for a benchmark system the MPC algorithm with on-line linearisation and parameterisation gives very good quality of control, comparable with that possible in classical MPC with long horizons and nonlinear optimisation.																	1641-876X	2083-8492				MAR	2020	30	1					35	46		10.34768/amcs-2020-0003													
J								HEALTH-AWARE AND FAULT-TOLERANT CONTROL OF AN OCTOROTOR UAV SYSTEM BASED ON ACTUATOR RELIABILITY	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										prognostics and health management; health-aware control; fault-tolerant control; reliability analysis; octorotor; UAV	FTC	A major goal in modern flight control systems is the need for improving reliability. This work presents a health-aware and fault-tolerant control approach for an octorotor UAV that allows distributing the control effort among the available actuators based on their health information. However, it is worth mentioning that, in the case of actuator fault occurrence, a reliability improvement can come into conflict with UAV controllability. Therefore, system reliability sensitivity is redefined and modified to prevent uncontrollable situations during the UAV's mission. The priority given to each actuator is related to its importance in system reliability. Moreover, the proposed approach can reconfigure the controller to compensate actuator faults and improve the overall system reliability or delay maintenance tasks.																	1641-876X	2083-8492				MAR	2020	30	1					47	59		10.34768/amcs-2020-0004													
J								FAULT TOLERANT MULTICONTROLLERS FOR NONLINEAR SYSTEMS: A REAL VALIDATION ON A CHEMICAL PROCESS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										multicontroller; experimental validation; transesterification reactor; discrete unknown input multiobserver; fault tolerant control; sensor fault estimation	CONTROL DESIGN; BIODIESEL PRODUCTION; SIMULTANEOUS STATE; ACTUATOR; IDENTIFICATION; DIAGNOSIS	An active sensor fault tolerant controller for nonlinear systems represented by a decoupled multimodel is proposed. Active fault tolerant control requires accurate fault estimation. Thus, to estimate both state variables and sensor faults, a discrete unknown input multiobserver, based on an augmented state multimodel, is designed. The multiobserver gains are computed by solving linear matrix inequalities with equality constraints. A multicontrol strategy is proposed for the compensation of the sensor fault and recovering the desired performances. This strategy integrates a bank of controllers, corresponding to a set of partial models, to generate a set of control laws compensating the fault effect. Then, a switching strategy between the generated local control laws is established in order to apply the most suitable control law that tolerates the fault and maintains good closed loop performances. The effectiveness of the proposed strategy is proven through a numerical example and also through a real time application on a chemical reactor. The obtained results confirm satisfactory closed loop performance in terms of trajectory tracking and fault tolerance.																	1641-876X	2083-8492				MAR	2020	30	1					61	74		10.34768/amcs-2020-0005													
J								EXTREMAL PROPERTIES OF LINEAR DYNAMIC SYSTEMS CONTROLLED BY DIRAC'S IMPULSE	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										external properties; Dirac's impulse; linear systems; transfer function		The paper concerns the properties of linear dynamical systems described by linear differential equations, excited by the Dirac delta function. A differential equation of the form a(n)x((n)) (t) +...+a(1)x' (t) + a(0)x(t) = b(m)u((m)) (t) +...+b(1)u' (t) + b(0)u(t) is considered with a(i), b(j) > 0. In the paper we assume that the polynomials M-n(s) = a(n)s(n) +...+ a(1)s a(0) and L-m (s) = b(m)s(m) +...+ b(1)s + b(0) partly interlace. The solution of the above equation is denoted by x(t, L-m, M-n). It is proved that the function x(t, L-m , M-n) is nonnegative for t is an element of (0, infinity), and does not have more than one local extremum in the interval (0, infinity) (Theorems 1, 3 and 4). Besides, certain relationships are proved which occur between local extrema of the function x(t, L-m, M-n ), depending on the degree of the polynomial M-n(s) or L-m (s) (Theorems 5 and 6).																	1641-876X	2083-8492				MAR	2020	30	1					75	81		10.34768/amcs-2020-0006													
J								ANTI-PERIODIC SOLUTIONS FOR CLIFFORD-VALUED HIGH-ORDER HOPFIELD NEURAL NETWORKS WITH STATE-DEPENDENT AND LEAKAGE DELAYS	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										Clifford-valued high-order Hopfield neural network; anti-periodic solution; coincidence degree; time-varying delay	GLOBAL EXPONENTIAL STABILITY; SYNCHRONIZATION; EXISTENCE; CRITERIA	A class of Clifford-valued high-order Hopfield neural networks (HHNNs) with state-dependent and leakage delays is considered. First, by using a continuation theorem of coincidence degree theory and the Wirtinger inequality, we obtain the existence of anti-periodic solutions of the networks considered. Then, by using the proof by contradiction, we obtain the global exponential stability of the anti-periodic solutions. Finally, two numerical examples are given to illustrate the feasibility of our results.																	1641-876X	2083-8492				MAR	2020	30	1					83	98		10.34768/amcs-2020-0007													
J								FINDING ROBUST TRANSFER FEATURES FOR UNSUPERVISED DOMAIN ADAPTATION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										unsupervised domain adaptation; feature reduction; generalized eigenvalue decomposition; object recognition	DIMENSIONALITY REDUCTION; KERNEL	An insufficient number or lack of training samples is a bottleneck in traditional machine learning and object recognition. Recently, unsupervised domain adaptation has been proposed and then widely applied for cross-domain object recognition, which can utilize the labeled samples from a source domain to improve the classification performance in a target domain where no labeled sample is available. The two domains have the same feature and label spaces but different distributions. Most existing approaches aim to learn new representations of samples in source and target domains by reducing the distribution discrepancy between domains while maximizing the covariance of all samples. However, they ignore subspace discrimination, which is essential for classification. Recently, some approaches have incorporated discriminative information of source samples, but the learned space tends to be overfitted on these samples, because they do not consider the structure information of target samples. Therefore, we propose a feature reduction approach to learn robust transfer features for reducing the distribution discrepancy between domains and preserving discriminative information of the source domain and the local structure of the target domain. Experimental results on several well-known cross-domain datasets show that the proposed method outperforms state-of-the-art techniques in most cases.																	1641-876X	2083-8492				MAR	2020	30	1					99	112		10.34768/amcs-2020-0008													
J								A SECOND-ORDER TV-BASED COUPLING MODEL AND AN ADMM ALGORITHM FOR MR IMAGE RECONSTRUCTION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										MRI reconstruction; LLT model; LOT model; coupling model; ADMM; split Bregman; wavelet transform	SPACE	Motivated by ideas from two-step models and combining second-order TV regularization in the LLT model, we propose a coupling model for MR image reconstruction. By applying the variables splitting technique, the split Bregman iterative scheme, and the alternating minimization method twice, we can divide the proposed model into several subproblems only related to second-order PDEs so as to avoid solving a fourth-order PDE. The solution of every subproblem is based on generalized shrinkage formulas, the shrink operator or the diagonalization technique of the Fourier transform, and hence can be obtained very easily. By means of the Barzilai-Borwein step size selection scheme, an ADMM type algorithm is proposed to solve the equations underlying the proposed model. The results of numerical implementation demonstrate the feasibility and effectiveness of the proposed model and algorithm.																	1641-876X	2083-8492				MAR	2020	30	1					113	122		10.34768/amcs-2020-0009													
J								CURVE SKELETON EXTRACTION VIA K-NEAREST-NEIGHBORS BASED CONTRACTION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										curve skeleton; points contraction; point cloud; k nearest neighbors		We propose a skeletonization algorithm that is based on an iterative points contraction. We make an observation that the local center that is obtained via optimizing the sum of the distance to k nearest neighbors possesses good properties of robustness to noise and incomplete data. Based on such an observation, we devise a skeletonization algorithm that mainly consists of two stages: points contraction and skeleton nodes connection. Extensive experiments show that our method can work on raw scans of real-world objects and exhibits better robustness than the previous results in terms of extracting topology-preserving curve skeletons.																	1641-876X	2083-8492				MAR	2020	30	1					123	132		10.34768/amcs-2020-0010													
J								A QUATERNION CLUSTERING FRAMEWORK	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										data clustering; quaternions data processing; human gait data processing	TIME-SERIES	Data clustering is one of the most popular methods of data mining and cluster analysis. The goal of clustering algorithms is to partition a data set into a specific number of clusters for compressing or summarizing original values. There are a variety of clustering algorithms available in the related literature. However, the research on the clustering of data parametrized by unit quaternions, which are commonly used to represent 3D rotations, is limited. In this paper we present a quaternion clustering methodology including an algorithm proposal for quatemion based k-means along with quaternion clustering quality measures provided by an enhancement of known indices and an automated procedure of optimal cluster number selection. The validity of the proposed framework has been tested in experiments performed on generated and real data, including human gait sequences recorded using a motion capture technique.																	1641-876X	2083-8492				MAR	2020	30	1					133	147		10.34768/amcs-2020-0011													
J								AN ALGORITHM FOR QUATERNION-BASED 3D ROTATION	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										quaternions; space rotation; design of algorithms	TRANSFORM	In this work a new algorithm for quaternion-based spatial rotation is presented which reduces the number of underlying real multiplications. The performing of a quaternion-based rotation using a rotation matrix takes 15 ordinary multiplications, 6 trivial multiplications by 2 (left-shifts), 21 additions, and 4 squarings of real numbers, while the proposed algorithm can compute the same result in only 14 real multiplications (or multipliers-in a hardware implementation case), 43 additions, 4 right-shifts (multiplications by 1/4), and 3 left-shifts (multiplications by 2).																	1641-876X	2083-8492				MAR	2020	30	1					149	160		10.34768/amcs-2020-0012													
J								HEURISTIC SEARCH OF EXACT BICLUSTERS IN BINARY DATA	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										biclustering; Boolean reasoning; prime implicant approximation; biomedical data analysis; Johnson heuristic	BOOLEAN REPRESENTATION	The biclustering of two-dimensional homogeneous data consists in finding a subset of rows and a subset of columns whose intersection provides a set of cells whose values fulfil a specified condition. Usually it is defined as equality or comparability. One of the presented approaches is based on the model of Boolean reasoning, in which finding biclusters in binary or discrete data comes down to the problem of finding prime implicants of some Boolean function. Due to the high computational complexity of this task, the application of some heuristics should be considered. In the paper, a modification of the wellknown Johnson strategy for prime implicant approximation induction is presented, which is necessary for the biclustering problem. The new method is applied to artificial and biomedical datasets.																	1641-876X	2083-8492				MAR	2020	30	1					161	171		10.34768/amcs-2020-0013													
J								A GENETIC ALGORITHM FOR THE MAXIMUM 2-PACKING SET PROBLEM	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										maximum 2-packing set; genetic algorithms; graph algorithms	SELF-STABILIZING ALGORITHM; GRAPH	Given an undirected connected graph G = (V, E), a subset of vertices S is a maximum 2-packing set if the number of edges in the shortest path between any pair of vertices in S is at least 3 and S has the maximum cardinality. In this paper, we present a genetic algorithm for the maximum 2-packing set problem on arbitrary graphs, which is an NP-hard problem. To the best of our knowledge, this work is a pioneering effort to tackle this problem for arbitrary graphs. For comparison, we extended and outperformed a well-known genetic algorithm originally designed for the maximum independent set problem. We also compared our genetic algorithm with a polynomial-time one for the maximum 2-packing set problem on cactus graphs. Empirical results show that our genetic algorithm is capable of finding 2-packing sets with a cardinality relatively close (or equal) to that of the maximum 2-packing sets. Moreover, the cardinality of the 2-packing sets found by our genetic algorithm increases linearly with the number of vertices and with a larger population and a larger number of generations. Furthermore, we provide a theoretical proof demonstrating that our genetic algorithm increases the fitness for each candidate solution when certain conditions are met.																	1641-876X	2083-8492				MAR	2020	30	1					173	184		10.34768/amcs-2020-0014													
J								A DECOMPOSITION APPROACH TO TYPE 2 INTERVAL ARITHMETIC	INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE										multi-dimensional RDM interval arithmetic; type 2 interval arithmetic; RDM type 2 interval arithmetic; decomposition type 2 interval arithmetic; interval arithmetic	DECISION-MAKING	The classic interval has precise borders A = [(a) under bar, (a) over bar] Therefore, it can be called a type 1 interval. Because of great practical importance of such interval data, several versions of type 1 interval arithmetic have been created. However, sometimes precise borders a and (a) over bar, of intervals cannot be determined in practice. If the borders are uncertain, then we have to do with type 2 intervals. A type 2 interval can be denoted as A(T2) = [[(a) under bar (L), (a) under bar (L)], [(a) over bar (L), (a) over bar (R)]]. The paper presents multidimensional decomposition RDM type 2 interval arithmetic (D-RDM-T2-I arithmetic), where RDM means relative-distance measure. The decomposition approach considerably simplifies calculations and is transparent for users. Apart from this arithmetic, examples of its applications are also presented. To the authors' best knowledge, no papers on this arithmetic exist. D-RDM-T2-I arithmetic is necessary to create type 2 fuzzy arithmetic based on horizontal mu-cuts, which the authors aim to do.																	1641-876X	2083-8492				MAR	2020	30	1					185	201		10.34768/amcs-2020-0015													
J								Improved Control Schemes for Projective Synchronization of Delayed Neural Networks with Unmatched Coefficients	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Projective synchronization; neural network; distributed delay; adaptive control	GLOBAL ASYMPTOTIC STABILITY; LAG SYNCHRONIZATION; DYNAMICAL NETWORKS; IMPULSIVE CONTROL; SYSTEMS	Projective synchronization (PS) is a type of chaos synchronization where the states of slave system are scaled replicas of the states of master system. This paper studies the asymptotic projective synchronization (APS) between master-slave chaotic neural networks (NNs) with mixed time-delays and unmatched coefficients. Based on useful inequality techniques and constructing a suitable Lyapunov functional, some simple criteria are derived to ensure the APS of considered networks via designing a novel adaptive feedback controller. In addition, a numerical example and its MATLAB simulations are provided to check the feasibility of the obtained results. The main innovation of our work is that we dealt with the APS problem between two different chaotic NNs, while most of the existing works only concerned with the PS of chaotic systems with the same topologies. In addition, compared with the controllers introduced in the existing papers, the designed controller in this paper does not require any knowledge about the activation functions, which can be seen as another novelty of the paper.																	0218-0014	1793-6381				MAR	2020	34	3							2051005	10.1142/S0218001420510052													
J								Improving Accuracy of Evolving GMM Under GPGPU-Friendly Block-Evolutionary Pattern	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Gaussian Mixture Model; incremental clustering; GPGPU; block-evolution	CLASSIFICATION	As a classical clustering model, Gaussian Mixture Model (GMM) can be the footstone of dominant machine learning methods like transfer learning. Evolving GMM is an approximation to the classical GMM under time-critical or memory-critical application scenarios. Such applications often have constraints on time-to-answer or high data volume, and raise high computation demand. A prominent approach to address the demand is GPGPU-powered computing. However, the existing evolving GMM algorithms are confronted with a dilemma between clustering accuracy and parallelism. Point-wise algorithms achieve high accuracy but exhibit limited parallelism due to point-evolutionary pattern. Block-wise algorithms tend to exhibit higher parallelism. Whereas, it is challenging to achieve high accuracy under a block-evolutionary pattern due to the fact that it is difficult to track evolving process of the mixture model in fine granularity. Consequently, the existing blockwise algorithm suffers from significant accuracy degradation, compared to its batch-mode counterpart: the standard EM algorithm. To cope with this dilemma, we focus on the accuracy issue and develop an improved block-evolutionary GMM algorithm for GPGPU-powered computing systems. Our algorithm leverages evolving history of the model to estimate the latest model order in each incremental clustering step. With this model order as a constraint, we can perform similarity test in an elastic manner. Finally, we analyze the evolving history of both mixture components and the data points, and propose our method to merge similar components. Experiments on real images show that our algorithm significantly improves accuracy of the original general purpose block-wise algorithm. The accuracy of our algorithm is at least comparable to that of the standard EM algorithm and even outperforms the latter under certain scenarios.																	0218-0014	1793-6381				MAR	2020	34	3							2050006	10.1142/S0218001420500068													
J								Freshness Classification of Horse Mackerels with E-Nose System Using Hybrid Binary Decision Tree Structure	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Electronic nose; fish freshness; Bayes classifier; k-nearest neighbor; linear discriminant analysis; support vector machine	ELECTRONIC NOSE; FISH FRESHNESS; ODOR CLASSIFICATION; QUALITY; SPOILAGE; ARRAY; MILK	The aim of this study is to test the freshness of horse mackerels by using a low cost electronic nose system composed of eight different metal oxide sensors. The process of freshness evaluation covers a seals of seven different classes corresponding to 1, 3, 5, 7, 9, 11, and 13 storage days. These seven classes are categorized according to six different classifiers in the proposed binary decision tree structure. Classifiers at each particular node of the tree are individually trained with the training dataset. To increase success in determining the level of fish freshness, one of the k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), Linear Discriminant Analysis (LDA) and Bayes methods is selected for every classifier and the feature spaces change in every node. The significance of this study among the others in the literature is that this proposed decision tree structure has never been applied to determine fish freshness before. Because the freshness of fish is observed under actual market storage conditions, the classification is more difficult. The results show that the electronic nose designed with the proposed decision tree structure is able to determine the freshness of horse mackerels with 85.71% accuracy for the test data obtained one year after the training process. Also, the performances of the proposed methods are compared against conventional methods such as Bayes, k-NN, and LDA.																	0218-0014	1793-6381				MAR	2020	34	3							2050003	10.1142/S0218001420500032													
J								Weak Transient Electromagnetic Radiation Signal Detection Method Considering the New Watershed Image Segmentation Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Weak transient electromagnetic radiation; new watershed image segmentation algorithm; signal detection; remote detection		In order to study the detection methods of weak transient electromagnetic radiation signals, a detection algorithm integrating generalized cross-correlation and chaotic sequence prediction is proposed in this paper. Based on the dual-antenna test and cross-correlation information estimation method, the detection of aperiodic weak discharge signals under low signal-to-noise ratio is transformed into the estimation of periodic delay parameters, and the noise is reduced at the same time. The feasibility of this method is verified by simulation and experimental analysis. The results show that under the condition of low signal-to-noise ratio, the integrated method can effectively suppress the influence of 10 noise disturbances. It has a high detection probability for weak transient electromagnetic radiation signals, and needs fewer pulse accumulation times, which improves the detection efficiency and is more suitable for long-distance detection of weak electromagnetic radiation sources.																	0218-0014	1793-6381				MAR	2020	34	3							2054009	10.1142/S0218001420540099													
J								Webpage Recommendation System Using Interesting Subgraphs and Laplace Based k-Nearest Neighbor	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Webpage recommendation; interesting subgraph; CS algorithm; k-Nearest neighbor; Laplace correction		An interesting research area that permits the user to mine the significant information, called frequent subgraph, is Graph-Based Data Mining (GBDM). One of the well-known algorithms developed to extract frequent patterns is GASTON algorithm. Retrieving the interesting webpages from the log files contributes heavily to various applications. In this work, a webpage recommendation system has been proposed by introducing Chronological Cuckoo Search (Chronological-CS) algorithm and the Laplace correction based k-Nearest Neighbor (LKNN) to retrieve the useful webpage from the interesting webpage. Initially, W-Gaston algorithm extracts the interesting subgraph from the log files and provides it to the proposed webpage recommendation system. The interesting subgraphs subjected to clustering with the proposed Chronological-CS algorithm, which is developed by integrating the chronological concept into Cuckoo Search (CS) algorithm, provide various cluster groups. Then, the proposed LKNN algorithm recommends the webpage from the clusters. Simulation of the proposed webpage recommendation algorithm is done by utilizing the data from MSNBC and weblog database. The results are compared with various existing webpage recommendation models and analyzed based on precision, recall, and F-measure. The proposed webpage recommendation model achieved better performance than the existing models with the values of 0.9194, 0.8947, and 0.86736, respectively, for the precision, recall, and F-measure.																	0218-0014	1793-6381				MAR	2020	34	3							2053003	10.1142/S0218001420530031													
J								A Multi-Objective Particle Swarm Optimization Based on Grid Distance	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										External archive control; grid distance; learning sample; multi-objective particle swarm optimization; premature convergence	ALGORITHM; PERFORMANCE	In modern intelligent algorithms and real-industrial applications, there are many fields involving multi-objective particle swarm optimization algorithms, but the conflict between each objective in the optimization process will easily lead to the algorithm falling into local optimal. In order to prevent the algorithm from quickly falling into local optimization and improve the robustness of the algorithm, a multi-objective particle swarm optimization algorithm based on grid distance (GDMOPSO) was proposed, which has to improve the diversity of the algorithm and the search ability. Based on the MOPSO algorithm, a new external archive control strategy was established by using the grid technology and Pareto-dominant ordering principle, and the learning samples were improved. The proposed GDMOPSO is compared with a group of benchmark function tests and four classical algorithms. The results of experiment show that our proposed algorithm can effectively avoid premature convergence in terms of generational distance and hyper-volume (HV) indicator compared with other four classical MOPSO algorithms.																	0218-0014	1793-6381				MAR	2020	34	3							2059008	10.1142/S0218001420590089													
J								Generating High-Quality Air-Coupled Ultrasonic Images for Wooden Material Characterization by Single Image Super-Resolution	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Air-coupled ultrasonic imaging; wooden material characterization; super-resolution; convolutional neural networks	NONDESTRUCTIVE EVALUATION; CONVOLUTIONAL NETWORK; STRENGTH; TIMBER	As the practical applications in other fields, high-resolution images are usually expected to provide a more accurate assessment for the air-coupled ultrasonic (ACU) characterization of wooden materials. This paper investigated the feasibility of applying single image superresolution (SISR) methods to recover high-quality ACU images from the raw observations that were constructed directly by the on-the-shelf ACU scanners. Four state-of-the-art SISR methods were applied to the low-resolution ACU images of wood products. The reconstructed images were evaluated by visual assessment and objective image quality metrics, including peak signal-to-noise-ratio and structural similarity. Both qualitative and quantitative evaluations indicated that the substantial improvement of image quality can be yielded. The results of the experiments demonstrated the superior performance and high reproducibility of the method for generating high-quality ACU images. Sparse coding based super-resolution and super-resolution convolutional neural network (SRCNN) significantly outperformed other algorithms. SRCNN has the potential to act as an effective tool to generate higher resolution ACU images due to its flexibility.																	0218-0014	1793-6381				MAR	2020	34	3							2054008	10.1142/S0218001420540087													
J								Classification of Fashion Article Images Based on Improved Random Forest and VGG-IE Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image recommendation; deep learning; random forest; VGG; Fashion-MNIST	DEEP UNCERTAINTY	In classification of fashion article images based on e-commerce image recommendation system, the classification accuracy and computation time cannot meet the actual requirements. Herein, for the first time to our knowledge, we present two diverse image recognition approaches for classification of fashion article images called random-forest method based on genetic algorithm (GA-RF) and Visual Geometry Group-Image Enhancement algorithm (VGG-IE) to solve classification accuracy and computation time problem. In GA-RF, the number of segmentation times and the decision trees are the key factors affecting the classification results. improved genetic algorithm is introduced into the parameter optimization of forests to determine the optimal combination of the two parameters with minimal manual intervention. Finally, we propose six different Deep Neural Network architectures, including VGG-IE, to improve classification accuracy. The VGG-IE algorithm uses batch normalization and seven kinds training-data augmentation for ease and promotion of learning process. We investigate the effectiveness of the proposed method using Fashion-MNIST dataset and 70 000 pictures, Experimental results demonstrate that, in comparison with the state-of-the-art algorithms for 10 categories of image recognition, our VGG algorithm has the shortest computational time when it satisfies certain classification accuracy. VGG-IE approach has the highest classification accuracy.																	0218-0014	1793-6381				MAR	2020	34	3							2051004	10.1142/S0218001420510040													
J								Fusion of Underwater Image Enhancement and Restoration	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Underwater images; image enhancement; image restoration; pyramid-based fusion; dehazing		Optical properties of water distort the quality of underwater images. Underwater images are characterized by poor contrast, color cast, noise and haze. These images need to be pre-processed so as to get some information. In this paper, a novel technique named Fusion of Underwater Image Enhancement and Restoration (FUIER) has been proposed which enhances as well as restores underwater images with a target to act on all major issues in underwater images, i.e. color cast removal, contrast enhancement and dehazing. It generates two versions of the single input image and these two versions are fused using Laplacian pyramid-based fusion to get the enhanced image. The proposed method works efficiently for all types of underwater images captured in different conditions (turbidity, depth, salinity, etc.). Results obtained using the proposed method are better than those for state-of-the-art methods.																	0218-0014	1793-6381				MAR	2020	34	3							2054007	10.1142/S0218001420540075													
J								Instance-Based Cost-Sensitive Boosting	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Classification; instance-based cost sensitive; Bayesian consistent; boosting; loss function	COMPUTER-AIDED DIAGNOSIS; FACE RECOGNITION; COMBINATION; ADABOOST	Many classification algorithms aim to minimize just their training error count; however, it is often desirable to minimize a more general cost metric, where distinct instances have different costs. In this paper, an instance-based cost-sensitive Bayesian consistent version of exponential loss function is proposed. Using the modified loss function, the derivation of instance-based costsensitive extensions of AdaBoost, RealBoost and GentleBoost are developed which are termed as ICSAdaBoost, ICSRealBoost and ICSGentleBoost, respectively. In this research, a new instance-based cost generation method is proposed instead of doing this expensive process by experts. Thus, each sample takes two cost values; a class cost and a sample cost. The first cost is equally assigned to all samples of each class while the second cast is generated according to the probability of each sample within its class probability density function. Experimental results of the proposed schemes imply 12% enhancement in terms of F-measure and 13% on cost-persample over a variety of UCI datasets, compared to the state-of-the-art methods. The significant priority of the proposed method is supported by applying the pair of T-tests to the results.																	0218-0014	1793-6381				MAR	2020	34	3							2050002	10.1142/S0218001420500020													
J								Research on Correlation Between Underground Squares' Interface Morphology and Spatial Experience Based on Virtual Reality	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Virtual reality; isovist analysis; computer vision; underground squares; interface morphology; spatial experience	DESIGN; SPACE; EXPOSURE	Underground square, as a recreation and activity place for citizens, represents the underground space quality of a city, so its spatial experience is of great significance. This research discussed the influence and effects of underground squares' interface morphology on spatial experience. By field research of ten large-scale urban underground spaces, the basic elements and related numerical interval of underground squares' interface morphology were summarized, and on basis of this, underground square virtual models with different interface morphology were built. Based on the platform of immersive virtual reality system (IVRS), combing the isovist method and the SD method, this paper made a quantitative analysis on the relevance between interface morphology and spatial experience, and the relevant compliance indicators of interface morphology were summarized. The experimental results show that the spatial experience is good at the interface density of 0.402 and interface opening's aspect ratio of 2-3. This can provide reference and foundation for urban underground spatial design in future.																	0218-0014	1793-6381				MAR	2020	34	3							2050004	10.1142/S0218001420500044													
J								Research on Visual Comfort of Underground Commercial Streets' Pavement in China on the Basis of Virtual Simulation	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Underground commercial streets; ground pavement; visual comfort; virtual reality technology; computer vision; quantitative analysis	REALITY; PERCEPTION	As the physical environment of urban underground streets space is continuously perfect, people start to anticipate that underground streets with negative impression could satisfy their psychological comfort appeal as traditional ground streets do. Streets' pavement is of great significance in creating humanized and comfortable underground spatial atmosphere, and different ground pavements of underground commercial streets display different visual comfort effects. We conducted field investigations of a dozen typical underground streets in eight big cities in China with 42 test samples, and this research obtained the pavement elements' data of underground commercial streets, built virtual 3D scenes based on orthogonal analysis and virtual reality technology of computer vision, acquired the psychological data of spatial experience through virtual reality test, and analyzed the relationship between six elements (glossiness, roughness, color, dimension, pattern, and collage of ground pavement in underground commercial streets) and spatial visual comfort based on quantitative analysis of experimental data. Results show that, "Cold-hot sensation", "Glossiness", and "Collage ordered degree" are chief elements for underground commercial streets' pavement design. The pavement design featured with neutral color, medium- and large-scale and ordered collage form are the design preference and emphasis for underground commercial streets. Neutral color of underground commercial streets pavement may have a great influence on the recognition of pavement glossiness.																	0218-0014	1793-6381				MAR	2020	34	3							2050005	10.1142/S0218001420500056													
J								Recaptured Image Forensics Algorithm Based on Image Texture Feature	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Information security; image forensics; recaptured image; gray level co-occurrence matrix		With the rapid development of digital phones, the digital image forensics system in current times has had a great impact. It will lead to a serious threat for us, and especially the emergence of the recaptured image makes the existing digital image forensics algorithm invalid. So, it needs an effective image detection algorithm for us to identify recaptured images. In this paper, a new detection algorithm of the recaptured image is presented based on gray level co-occurrence matrix by analyzing the differences between the real and recaptured images. In order to analyze the differences, a new image evaluation model was put forward in this paper, which is called image variance ratio. Firstly, the algorithm proposed extracted high-frequency and low-frequency information of images by wavelet transform, based on which we calculated the relative gray level co-occurrence matrices. Secondly, the features of gray level co-occurrence matrix were extracted. At last, the recaptured image was classified by the support vector machine according to the features. The experimental results showed the algorithm proposed can not only effectively identify the recaptured image obtained from different media but also have better identification rate.																	0218-0014	1793-6381				MAR	2020	34	3							2054011	10.1142/S0218001420540117													
J								A Meta-Algorithm for Improving Top-N Prediction Efficiency of Matrix Factorization Models in Collaborative Filtering	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Matrix factorization; approximate nearest neighbor search; learning to rank; recommender systems		Matrix factorization models often reveal the low-dimensional latent structure in high-dimensional spaces while bringing space efficiency to large-scale collaborative filtering problems. Improving training and prediction time efficiencies of these models are also important since an accurate model may raise practical concerns if it is slow to capture the changing dynamics of the system. For the training task, powerful improvements have been proposed especially using SGD, ALS, and their parallel versions. In this paper, we focus on the prediction task and combine matrix factorization with approximate nearest neighbor search methods to improve the efficiency of top-N prediction queries. Our efforts result in a meta-algorithm, MMFNN, which can employ various common matrix factorization models, drastically improve their prediction efficiency, and still perform comparably to standard prediction approaches or sometimes even better in terms of predictive power. Using various batch, online, and incremental matrix factorization models, we present detailed empirical analysis results on many large implicit feedback datasets from different application domains.																	0218-0014	1793-6381				MAR	2020	34	3							2059007	10.1142/S0218001420590077													
J								Design and Manufacturing of Industrial Products Considering the Principle of Information Graphics	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Industrial design; product modeling; principle of information graphics; modeling feature variable	QUALITY	Combining the advantages of graphical visualization in the field of information dissemination, aiming at a large number of complicated information data in the stage of product design preparation, this paper discusses the guidance and reference function of drawing visual product design outline for product design behavior. The principle of information graphics is introduced into the field of the style design for the industrial products, and it is put forward in this paper that the formation principle and the decision points of a certain kind of modeling can be analyzed through the calculation of the characteristic variables of the pairwise modeling based on the principle of the information graphics. The industrial products are taken as an example, and the representative samples are selected to extract the modeling characteristic variables and calculate the information graphics on the basis of the principle of the information graphics. The results show that the formation principle and design decision sequence for different types of modeling can be obtained. The instance has verified the feasibility of the method put forward in this paper, which can provide an effective reference for the design of the modeling for the industrial products.																	0218-0014	1793-6381				MAR	2020	34	3							2054010	10.1142/S0218001420540105													
J								ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image reconstruction; Transforms; Imaging; Task analysis; Computer architecture; Data models; Compressive sensing; deep learning; MR imaging; ADMM; ADMM-CSNet	GENERATIVE ADVERSARIAL NETWORKS; SIGNAL RECOVERY; THRESHOLDING ALGORITHM; SPARSE REPRESENTATION; MRI RECONSTRUCTION; INVERSE PROBLEMS; LOW-RANK; CNN	Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods.																	0162-8828	1939-3539				MAR 1	2020	42	3					521	538		10.1109/TPAMI.2018.2883941													
J								Adversarial Action Prediction Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Action prediction; action recognition; sequential context; variational autoencoder; adversarial learning		Different from after-the-fact action recognition, action prediction task requires action labels to be predicted from partially observed videos containing incomplete action executions. It is challenging because these partial videos have insufficient discriminative information, and their temporal structure is damaged. We study this problem in this paper, and propose an efficient and powerful deep network for learning representative and discriminative features for action prediction. Our approach exploits abundant sequential context information in full videos to enrich the feature representations of partial videos. This information is encoded in latent representations using a variational autoencoder (VAE), which are encouraged to be progress-invariant. Decoding such latent representations using another VAE, we can reconstruct missing information in the features extracted from partial videos. An adversarial learning scheme is adopted to differentiate the reconstructed features from the features directly extracted from full videos in order to well align their distributions. A multi-class classifier is also used to encourage the features to be discriminative. Our network jointly learns features and classifiers, and generates the features particularly optimized for action prediction. Extensive experimental results on UCF101, Sports-1M and BIT datasets demonstrate that our approach remarkably outperforms state-of-the-art methods, and shows significant speedup over these methods. Results also show that actions differ in their prediction characteristics; some actions can be correctly predicted even though only the beginning 10% portion of videos is observed.																	0162-8828	1939-3539				MAR 1	2020	42	3					539	553		10.1109/TPAMI.2018.2882805													
J								Context-Aware Query Selection for Active Learning in Event Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Labeling; Activity recognition; Context modeling; Streaming media; Feature extraction; Entropy; Manuals; Active learning; activity recognition; visual context; information theory	HISTOGRAMS; FLOW	Activity recognition is a challenging problem with many practical applications. In addition to the visual features, recent approaches have benefited from the use of context, e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled, entirely available beforehand, and not designed to be updated continuously, which make them unsuitable for surveillance applications. In contrast, we propose a continuous-learning framework for context-aware activity recognition from unlabeled video, which has two distinct advantages over existing methods. First, it employs a novel active-learning technique that not only exploits the informativeness of the individual activities but also utilizes their contextual information during query selection; this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field model that encodes the context and devise an information-theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative queries, which are labeled by a human. These labels are combined with graphical inference techniques for incremental updates. We provide a theoretical formulation of the active learning framework with an analytic solution. Experiments on six challenging datasets demonstrate that our framework achieves superior performance with significantly less manual labeling.																	0162-8828	1939-3539				MAR 1	2020	42	3					554	567		10.1109/TPAMI.2018.2878696													
J								Deep Neural Network Compression by In-Parallel Pruning-Quantization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Quantization (signal); Image coding; Neural networks; Visualization; Training; Convolution; Network architecture; Deep learning; neural network compression; weight pruning; weight quantization; Bayesian optimization		Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern networks contain millions of learned connections, and the current trend is towards deeper and more densely connected architectures. This poses a challenge to the deployment of state-of-the-art networks on resource-constrained systems, such as smartphones or mobile robots. In general, a more efficient utilization of computation resources would assist in deployment scenarios from embedded platforms to computing clusters running ensembles of networks. In this paper, we propose a deep network compression algorithm that performs weight pruning and quantization jointly, and in parallel with fine-tuning. Our approach takes advantage of the complementary nature of pruning and quantization and recovers from premature pruning errors, which is not possible with two-stage approaches. In experiments on ImageNet, CLIP-Q (Compression Learning by In-Parallel Pruning-Quantization) improves the state-of-the-art in network compression on AlexNet, VGGNet, GoogLeNet, and ResNet. We additionally demonstrate that CLIP-Q is complementary to efficient network architecture design by compressing MobileNet and ShuffleNet, and that CLIP-Q generalizes beyond convolutional networks by compressing a memory network for visual question answering.																	0162-8828	1939-3539				MAR 1	2020	42	3					568	579		10.1109/TPAMI.2018.2886192													
J								Deep Variational and Structural Hashing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Binary codes; Training; Visualization; Semantics; Quantization (signal); Probabilistic logic; Convolution; Scalable image search; fast similarity search; hashing; deep learning; cross-modal retrieval	NEAREST-NEIGHBOR; QUANTIZATION; ALGORITHMS	In this paper, we propose a deep variational and structural hashing (DVStH) method to learn compact binary codes for multimedia retrieval. Unlike most existing deep hashing methods which use a series of convolution and fully-connected layers to learn binary features, we develop a probabilistic framework to infer latent feature representation inside the network. Then, we design a struct layer rather than a bottleneck hash layer, to obtain binary codes through a simple encoding procedure. By doing these, we are able to obtain binary codes discriminatively and generatively. To make it applicable to cross-modal scalable multimedia retrieval, we extend our method to a cross-modal deep variational and structural hashing (CM-DVStH). We design a deep fusion network with a struct layer to maximize the correlation between image-text input pairs during the training stage so that a unified binary vector can be obtained. We then design modality-specific hashing networks to handle the out-of-sample extension scenario. Specifically, we train a network for each modality which outputs a latent representation that is as close as possible to the binary codes which are inferred from the fusion network. Experimental results on five benchmark datasets are presented to show the efficacy of the proposed approach.																	0162-8828	1939-3539				MAR 1	2020	42	3					580	595		10.1109/TPAMI.2018.2882816													
J								Face-from-Depth for Head Pose Estimation on Depth Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Head pose estimation; shoulder pose estimation; automotive; deterministic conditional GAN; CNNs		Depth cameras allow to set up reliable solutions for people monitoring and behavior understanding, especially when unstable or poor illumination conditions make unusable common RGB sensors. Therefore, we propose a complete framework for the estimation of the head and shoulder pose based on depth images only. A head detection and localization module is also included, in order to develop a complete end-to-end system. The core element of the framework is a Convolutional Neural Network, called POSEidon(+), that receives as input three types of images and provides the 3D angles of the pose as output. Moreover, a Face-from-Depth component based on a Deterministic Conditional GAN model is able to hallucinate a face from the corresponding depth image. We empirically demonstrate that this positively impacts the system performances. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Experimental results show that our method overcomes several recent state-of-art works based on both intensity and depth input data, running in real-time at more than 30 frames per second.																	0162-8828	1939-3539				MAR 1	2020	42	3					596	609		10.1109/TPAMI.2018.2885472													
J								Flexible High-Dimensional Unsupervised Learning with Missing Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Analytical models; Computational modeling; Data models; Unsupervised learning; Covariance matrices; Clustering algorithms; Mixture models; Clustering; factor analysis; generalized hyperbolic; missing data; mixture of factor analyzers; mixture model; model-based clustering; unsupervised classification	STOCHASTIC OZONE DAYS; T-FACTOR ANALYZERS; MIXTURE-MODELS; DISCRIMINANT-ANALYSIS; MAXIMUM-LIKELIHOOD; BAYESIAN-ANALYSIS; ALGORITHM	The mixture of factor analyzers (MFA) model is a famous mixture model-based approach for unsupervised learning with high-dimensional data. It can be useful, inter alia, in situations where the data dimensionality far exceeds the number of observations. In recent years, the MFA model has been extended to non-Gaussian mixtures to account for clusters with heavier tail weight and/or asymmetry. The generalized hyperbolic factor analyzers (MGHFA) model is one such extension, which leads to a flexible modelling paradigm that accounts for both heavier tail weight and cluster asymmetry. In many practical applications, the occurrence of missing values often complicates data analyses. A generalization of the MGHFA is presented to accommodate missing values. Under a missing-at-random mechanism, we develop a computationally efficient alternating expectation conditional maximization algorithm for parameter estimation of the MGHFA model with different patterns of missing values. The imputation of missing values under an incomplete-data structure of MGHFA is also investigated. The performance of our proposed methodology is illustrated through the analysis of simulated and real data.																	0162-8828	1939-3539				MAR 1	2020	42	3					610	621		10.1109/TPAMI.2018.2885760													
J								Force from Motion: Decoding Control Force of Activity in a First-Person Video	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Cameras; Visualization; Gravity; Torque; Dynamics; Three-dimensional displays; First-person vision; physical sensation; optimal control	EGOCENTRIC VIDEO; SHAPE; CALIBRATION; PEOPLE; SCENES	A first-person video delivers what the camera wearer (actor) experiences through physical interactions with surroundings. In this paper, we focus on a problem of Force from Motion-estimating the active force and torque exerted by the actor to drive her/his activity-from a first-person video. We use two physical cues inherited in the first-person video. (1) Ego-motion: the camera motion is generated by a resultant of force interactions, which allows us to understand the effect of the active force using Newtonian mechanics. (2) Visual semantics: the first-person visual scene is deployed to afford the actor's activity, which is indicative of the physical context of the activity. We estimate the active force and torque using a dynamical system that can describe the transition (dynamics) of the actor's physical state (position, orientation, and linear/angular momentum) where the latent physical state is indirectly observed by the first-person video. We approximate the physical state with the 3D camera trajectory that is reconstructed up to scale and orientation. The absolute scale factor and gravitation field are learned from the ego-motion and visual semantics of the first-person video. Inspired by an optimal control theory, we solve the dynamical system by minimizing reprojection error. Our method shows quantitatively equivalent reconstruction comparing to IMU measurements in terms of gravity and scale recovery and outperforms the methods based on 2D optical flow for an active action recognition task. We apply our method to first-person videos of mountain biking, urban bike racing, skiing, speedflying with parachute, and wingsuit flying where inertial measurements are not accessible.																	0162-8828	1939-3539				MAR 1	2020	42	3					622	635		10.1109/TPAMI.2018.2883327													
J								Image and Sentence Matching via Semantic Concepts and Order Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Semantics; Image representation; Task analysis; Context modeling; Logic gates; Pattern matching; Image annotation; Semantic concept; semantic order; context-modulated attention; image and sentence matching		Image and sentence matching has made great progress recently, but it remains challenging due to the existing large visual-semantic discrepancy. This mainly arises from two aspects: 1) images consist of unstructured content which is not semantically abstract as the words in the sentences, so they are not directly comparable, and 2) arranging semantic concepts in different semantic order could lead to quite diverse meanings. The words in the sentences are sequentially arranged in a grammatical manner, while the semantic concepts in the images are usually unorganized. In this work, we propose a semantic concepts and order learning framework for image and sentence matching, which can improve the image representation by first predicting semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its included semantic concepts in terms of object, property and action. These word-level semantic concepts are directly comparable with the words of noun, adjective and verb in the matched sentence. Then, to organize these concepts and make them express similar meanings as the matched sentence, we use a context-modulated attentional LSTM to learn the semantic order. It regards the predicted semantic concepts and image global scene as context at each timestep, and selectively attends to concept-related image regions by referring to the context in a sequential order. To further enhance the semantic order, we perform additional sentence generation on the image representation, by using the groundtruth order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.																	0162-8828	1939-3539				MAR 1	2020	42	3					636	650		10.1109/TPAMI.2018.2883466													
J								Incremental Learning Through Deep Adaptation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Task analysis; Switches; Training; Neural networks; Computer architecture; Convolutional codes; Incremental learning; transfer learning; domain adaptation		Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added domain, typically as many as the original network. We propose a method called Deep Adaptation Modules (DAM) that constrains newly learned filters to be linear combinations of existing ones. DAMs precisely preserve performance on the original domain, require a fraction (typically 13 percent, dependent on network architecture) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3 percent of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.																	0162-8828	1939-3539				MAR 1	2020	42	3					651	663		10.1109/TPAMI.2018.2884462													
J								Joint Face Alignment and 3D Face Reconstruction with Application to Face Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Face; Three-dimensional displays; Two dimensional displays; Shape; Image reconstruction; Face recognition; Solid modeling; 3D face reconstruction; face alignment; cascaded regression; pose and expression normalization; face recognition	SHAPE; REGISTRATION; MODEL	Face alignment and 3D face reconstruction are traditionally accomplished as separated tasks. By exploring the strong correlation between 2D landmarks and 3D shapes, in contrast, we propose a joint face alignment and 3D face reconstruction method to simultaneously solve these two problems for 2D face images of arbitrary poses and expressions. This method, based on a summation model of 3D faces and cascaded regression in 2D and 3D shape spaces, iteratively and alternately applies two cascaded regressors, one for updating 2D landmarks and the other for 3D shape. The 3D shape and the landmarks are correlated via a 3D-to-2D mapping matrix, which is updated in each iteration to refine the location and visibility of 2D landmarks. Unlike existing methods, the proposed method can fully automatically generate both pose-and-expression-normalized (PEN) and expressive 3D faces and localize both visible and invisible 2D landmarks. Based on the PEN 3D faces, we devise a method to enhance face recognition accuracy across poses and expressions. Both linear and nonlinear implementations of the proposed method are presented and evaluated in this paper. Extensive experiments show that the proposed method can achieve the state-of-the-art accuracy in both face alignment and 3D face reconstruction, and benefit face recognition owing to its reconstructed PEN 3D face.																	0162-8828	1939-3539				MAR 1	2020	42	3					664	678		10.1109/TPAMI.2018.2885995													
J								Learning Reasoning-Decision Networks for Robust Face Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Shape; Face; Training; Neural networks; Two dimensional displays; Computer architecture; Face alignment; deep neural networks; deep reinforcement learning; policy gradient		In this paper, we propose an end-to-end reasoning-decision networks (RDN) approach for robust face alignment via policy gradient. Unlike the conventional coarse-to-fine approaches which likely lead to bias prediction due to poor initialization, our approach aims to learn a policy by leveraging raw pixels to reason a subset of shape candidates, sequentially making plausible decisions to remove outliers for robust initialization. To achieve this, we formulate face alignment as a Markov decision process by defining an agent, which typically interacts with a trajectory of states, actions, state transitions and rewards. The agent seeks an optimal shape searching policy over the whole shape space by maximizing a discounted sum of the received values. To further improve the alignment performance, we develop an LSTM-based value function to evaluate the shape quality. During the training procedure, we adjust the gradient of our value function in directions of the policy gradient. This prevents our training goal from being trapped into local optima entangled by both the pose deformations and appearance variations especially in unconstrained environments. Experimental results show that our proposed RDN consistently outperforms most state-of-the-art approaches on four widely-evaluated challenging datasets.																	0162-8828	1939-3539				MAR 1	2020	42	3					679	693		10.1109/TPAMI.2018.2885298													
J								Mutually Guided Image Filtering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image edge detection; Image restoration; Computer vision; Task analysis; Kernel; Image color analysis; Sensors; Image filtering; joint image filtering; guided image filtering; mutually guided image filtering		Filtering images is required by numerous multimedia, computer vision and graphics tasks. Despite diverse goals of different tasks, making effective rules is key to the filtering performance. Linear translation-invariant filters with manually designed kernels have been widely used. However, their performance suffers from content-blindness. To mitigate the content-blindness, a family of filters, called joint/guided filters, have attracted a great amount of attention from the community. The main drawback of most joint/guided filters comes from the ignorance of structural inconsistency between the reference and target signals like color, infrared, and depth images captured under different conditions. Simply adopting such guidelines very likely leads to unsatisfactory results. To address the above issues, this paper designs a simple yet effective filter, named mutually guided image filter (muGIF), which jointly preserves mutual structures, avoids misleading from inconsistent structures and smooths flat regions. The proposed muGIF is very flexible, which can work in various modes including dynamic only (self-guided), static/dynamic (reference-guided) and dynamic/dynamic (mutually guided) modes. Although the objective of muGIF is in nature non-convex, by subtly decomposing the objective, we can solve it effectively and efficiently. The advantages of muGIF in effectiveness and flexibility are demonstrated over other state-of-the-art alternatives on a variety of applications. Our code is publicly available at https://sites.google.com/view/xjguo/mugif.																	0162-8828	1939-3539				MAR 1	2020	42	3					694	707		10.1109/TPAMI.2018.2883553													
J								Photometric Stereo in Participating Media Using an Analytical Solution for Shape-Dependent Forward Scatter	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Media; Three-dimensional displays; Shape; Image reconstruction; Backscatter; Computational modeling; Scattering; Photometric stereo; participating media; single scattering	SINGLE SCATTERING; LIGHT	Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from prvious studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. We discuss the approximation in the proposed method using synthesized data. Then, experiments with real data demonstrate that the proposed method improves 3D reconstruction in participating media.																	0162-8828	1939-3539				MAR 1	2020	42	3					708	719		10.1109/TPAMI.2018.2889088													
J								Single Image Dehazing Using Haze-Lines	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image color analysis; Atmospheric modeling; Cameras; Clustering algorithms; Estimation; Channel estimation; Image restoration; Single image dehazing; haze removal	VISIBILITY; FRAMEWORK; VISION; POINT	Haze often limits visibility and reduces contrast in outdoor images. The degradation varies spatially since it depends on the objects' distances from the camera. This dependency is expressed in the transmission coefficients, which control the attenuation. Restoring the scene radiance from a single image is a highly ill-posed problem, and thus requires using an image prior. Contrary to methods that use patch-based image priors, we propose an algorithm based on a non-local prior. The algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, which form tight clusters in RGB space. Our key observation is that pixels in a given cluster are often non-local, i.e., spread over the entire image plane and located at different distances from the camera. In the presence of haze these varying distances translate to different transmission coefficients. Therefore, each color cluster in the clear image becomes a line in RGB space, that we term a haze-line. Using these haze-lines, our algorithm recovers the atmospheric light, the distance map and the haze-free image. The algorithm has linear complexity, requires no training, and performs well on a wide variety of images compared to other state-of-the-art methods.																	0162-8828	1939-3539				MAR 1	2020	42	3					720	734		10.1109/TPAMI.2018.2882478													
J								The Whole Is More Than Its Parts? From Explicit to Implicit Pose Normalization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Fine-grained classification; object recognition; convolutional neural networks		Fine-grained classification describes the automated recognition of visually similar object categories like birds species. Previous works were usually based on explicit pose normalization, i.e., the detection and description of object parts. However, recent models based on a final global average or bilinear pooling have achieved a comparable accuracy without this concept. In this paper, we analyze the advantages of these approaches over generic CNNs and explicit pose normalization approaches. We also show how they can achieve an implicit normalization of the object pose. A novel visualization technique called activation flow is introduced to investigate limitations in pose handling in traditional CNNs like AlexNet and VGG. Afterward, we present and compare the explicit pose normalization approach neural activation constellations and a generalized framework for the final global average and bilinear pooling called alpha-pooling. We observe that the latter often achieves a higher accuracy improving common CNN models by up to 22.9 percent, but lacks the interpretability of the explicit approaches. We present a visualization approach for understanding and analyzing predictions of the model to address this issue. Furthermore, we show that our approaches for fine-grained recognition are beneficial for other fields like action recognition.																	0162-8828	1939-3539				MAR 1	2020	42	3					749	763		10.1109/TPAMI.2018.2885764													
J								Predictive Path Following and Collision Avoidance of Autonomous Connected Vehicles	ALGORITHMS										nonlinear model predictive control; autonomous driving; path following; optimization	AUTOMATED VEHICLES; TIME	This paper considers nonlinear model predictive control for simultaneous path-following and collision avoidance of connected autonomous vehicles. For each agent, a nonlinear bicycle model is used to predict a sequence of the states and then optimize them with respect to a sequence of control inputs. The objective function of the optimal control problem is to follow the planned path which is represented by a Bezier curve. In order to achieve collision avoidance among the networked vehicles, a geometric shape must be selected to represent the vehicle geometry. In this paper, an elliptic disk is selected for that as it represents the geometry of the vehicle better than the traditional circular disk. A separation condition between each pair of elliptic disks is formulated as time-varying state constraints for the optimization problem. Driving corridors are assumed to be also Bezier curves, which could be obtained from digital maps, and are reformulated to suit the controller algorithm. The algorithm is validated using MATLAB simulation with the aid of ACADO toolkit.																		1999-4893				MAR	2020	13	3							52	10.3390/a13030052													
J								Bi-Objective Dynamic Multiprocessor Open Shop Scheduling: An Exact Algorithm	ALGORITHMS										Industry 4.0; dynamic scheduling; multi-processor open shop scheduling; multi-objective optimization; exact algorithms	MODELS	An important element in the integration of the fourth industrial revolution is the development of efficient algorithms to deal with dynamic scheduling problems. In dynamic scheduling, jobs can be admitted during the execution of a given schedule, which necessitates appropriately planned rescheduling decisions for maintaining a high level of performance. In this paper, a dynamic case of the multiprocessor open shop scheduling problem is addressed. This problem appears in different contexts, particularly those involving diagnostic operations in maintenance and health care industries. Two objectives are considered simultaneously-the minimization of the makespan and the minimization of the mean weighted flow time. The former objective aims to sustain efficient utilization of the available resources, while the latter objective helps in maintaining a high customer satisfaction level. An exact algorithm is presented for generating optimal Pareto front solutions. Despite the fact that the studied problem is NP-hard for both objectives, the presented algorithm can be used to solve small instances. This is demonstrated through computational experiments on a testbed of 30 randomly generated instances. The presented algorithm can also be used to generate approximate Pareto front solutions in case computational time needed to find proven optimal solutions for generated sub-problems is found to be excessive. Furthermore, computational results are used to investigate the characteristics of the optimal Pareto front of the studied problem. Accordingly, some insights for future metaheuristic developments are drawn.																		1999-4893				MAR	2020	13	3							74	10.3390/a13030074													
J								A Geolocation Analytics-Driven Ontology for Short-Term Leases: Inferring Current Sharing Economy Trends	ALGORITHMS										sharing economy; short-term rentals; Airbnb; Athens; Greece; geolocation ontology; ghost hotel discovery; rapid automatic keyword extraction	KEYWORD EXTRACTION; AIRBNB; RECESSION	Short-term property rentals are perhaps one of the most common traits of present day shared economy. Moreover, they are acknowledged as a major driving force behind changes in urban landscapes, ranging from established metropolises to developing townships, as well as a facilitator of geographical mobility. A geolocation ontology is a high level inference tool, typically represented as a labeled graph, for discovering latent patterns from a plethora of unstructured and multimodal data. In this work, a two-step methodological framework is proposed, where the results of various geolocation analyses, important in their own respect, such as ghost hotel discovery, form intermediate building blocks towards an enriched knowledge graph. The outlined methodology is validated upon data crawled from the Airbnb website and more specifically, on keywords extracted from comments made by users of the said platform. A rather solid case-study, based on the aforementioned type of data regarding Athens, Greece, is addressed in detail, studying the different degrees of expansion & prevalence of the phenomenon among the city's various neighborhoods.																		1999-4893				MAR	2020	13	3							59	10.3390/a13030059													
J								Two-Step Classification with SVD Preprocessing of Distributed Massive Datasets in Apache Spark	ALGORITHMS										Apache Spark; Apache MLlib; PySpark; big data; machine learning; 10V data; two-step classification; ensemble classification; SVD; SparkQL; computing performance; F1 Metric; dataframe	DANTZIG SELECTOR; MAPREDUCE	At the dawn of the 10V or big data data era, there are a considerable number of sources such as smart phones, IoT devices, social media, smart city sensors, as well as the health care system, all of which constitute but a small portion of the data lakes feeding the entire big data ecosystem. This 10V data growth poses two primary challenges, namely storing and processing. Concerning the latter, new frameworks have been developed including distributed platforms such as the Hadoop ecosystem. Classification is a major machine learning task typically executed on distributed platforms and as a consequence many algorithmic techniques have been developed tailored for these platforms. This article extensively relies in two ways on classifiers implemented in MLlib, the main machine learning library for the Hadoop ecosystem. First, a vast number of classifiers is applied to two datasets, namely Higgs and PAMAP. Second, a two-step classification is ab ovo performed to the same datasets. Specifically, the singular value decomposition of the data matrix determines first a set of transformed attributes which in turn drive the classifiers of MLlib. The twofold purpose of the proposed architecture is to reduce complexity while maintaining a similar if not better level of the metrics of accuracy, recall, and F1. The intuition behind this approach stems from the engineering principle of breaking down complex problems to simpler and more manageable tasks. The experiments based on the same Spark cluster indicate that the proposed architecture outperforms the individual classifiers with respect to both complexity and the abovementioned metrics.																		1999-4893				MAR	2020	13	3							71	10.3390/a13030071													
J								Optimizing Convolutional Neural Network Hyperparameters by Enhanced Swarm Intelligence Metaheuristics	ALGORITHMS										convolutional neural network; swarm intelligence; optimization; tree growth algorithm; firefly algorithm; enhanced metaheuristics	ELEPHANT HERDING OPTIMIZATION; FIREFLY ALGORITHM; PERFORMANCE	Computer vision is one of the most frontier technologies in computer science. It is used to build artificial systems to extract valuable information from images and has a broad range of applications in various areas such as agriculture, business, and healthcare. Convolutional neural networks represent the key algorithms in computer vision, and in recent years, they have attained notable advances in many real-world problems. The accuracy of the network for a particular task profoundly relies on the hyperparameters' configuration. Obtaining the right set of hyperparameters is a time-consuming process and requires expertise. To approach this concern, we propose an automatic method for hyperparameters' optimization and structure design by implementing enhanced metaheuristic algorithms. The aim of this paper is twofold. First, we propose enhanced versions of the tree growth and firefly algorithms that improve the original implementations. Second, we adopt the proposed enhanced algorithms for hyperparameters' optimization. First, the modified metaheuristics are evaluated on standard unconstrained benchmark functions and compared to the original algorithms. Afterward, the improved algorithms are employed for the network design. The experiments are carried out on the famous image classification benchmark dataset, the MNIST dataset, and comparative analysis with other outstanding approaches that were tested on the same problem is conducted. The experimental results show that both proposed improved methods establish higher performance than the other existing techniques in terms of classification accuracy and the use of computational resources.																		1999-4893				MAR	2020	13	3							67	10.3390/a13030067													
J								Adding Edges for Maximizing Weighted Reachability dagger	ALGORITHMS										graph augmentation; approximation algorithms; greedy algorithms; submodularity; DAG; trees; dynamic programming	APPROXIMABILITY; ALGORITHM; DIAMETER	In this paper, we consider the problem of improving the reachability of a graph. We approach the problem from a graph augmentation perspective, in which a limited set size of edges is added to the graph to increase the overall number of reachable nodes. We call this new problem the Maximum Connectivity Improvement (MCI) problem. We first show that, for the purpose of solve solving MCI, we can focus on Directed Acyclic Graphs (DAG) only. We show that approximating the MCI problem on DAG to within any constant factor greater than 1-1/e is NP-hard even if we restrict to graphs with a single source or a single sink, and the problem remains NP-complete if we further restrict to unitary weights. Finally, this paper presents a dynamic programming algorithm for the MCI problem on trees with a single source that produces optimal solutions in polynomial time. Then, we propose two polynomial-time greedy algorithms that guarantee (1-1/e)-approximation ratio on DAGs with a single source, a single sink or two sources.																		1999-4893				MAR	2020	13	3							68	10.3390/a13030068													
J								GeoAI: A Model-Agnostic Meta-Ensemble Zero-Shot Learning Method for Hyperspectral Image Analysis and Classification	ALGORITHMS										model-agnostic meta-learning; ensemble learning; GIS; hyperspectral images; deep learning; remote sensing; scene classification; geospatial data; Zero-shot Learning	RECOGNITION	Deep learning architectures are the most effective methods for analyzing and classifying Ultra-Spectral Images (USI). However, effective training of a Deep Learning (DL) gradient classifier aiming to achieve high classification accuracy, is extremely costly and time-consuming. It requires huge datasets with hundreds or thousands of labeled specimens from expert scientists. This research exploits the MAML++ algorithm in order to introduce the Model-Agnostic Meta-Ensemble Zero-shot Learning (MAME-ZsL) approach. The MAME-ZsL overcomes the above difficulties, and it can be used as a powerful model to perform Hyperspectral Image Analysis (HIA). It is a novel optimization-based Meta-Ensemble Learning architecture, following a Zero-shot Learning (ZsL) prototype. To the best of our knowledge it is introduced to the literature for the first time. It facilitates learning of specialized techniques for the extraction of user-mediated representations, in complex Deep Learning architectures. Moreover, it leverages the use of first and second-order derivatives as pre-training methods. It enhances learning of features which do not cause issues of exploding or diminishing gradients; thus, it avoids potential overfitting. Moreover, it significantly reduces computational cost and training time, and it offers an improved training stability, high generalization performance and remarkable classification accuracy.																		1999-4893				MAR	2020	13	3							61	10.3390/a13030061													
J								Fractional Sliding Mode Nonlinear Procedure for Robust Control of an Eutrophying Microalgae Photobioreactor	ALGORITHMS										fractional-order control; sliding modes; stabilization; trajectory tracking; eutrophication control	WASTE-WATER; DESIGN; OPTIMIZATION; OBSERVERS; CULTURES; STATE	This paper proposes a fractional-order sliding mode controller (FOSMC) for the robust control of a nonlinear process subjected to unknown parametric disturbances. The controller aims to ensure optimal growth in photobioreactors of native microalgae involved in eutrophication of the Sinaloa rivers in Mexico. The controller design is based on the Caputo fractional integral-order derivative and on the convergence properties of a sliding surface. For nonlinear systems, the proposed FOSMC guarantees convergence to the sliding surface even in the presence of model disturbances. The proposed controller is compared to an Internal Model Control (IMC) through numerical simulations.																		1999-4893				MAR	2020	13	3							50	10.3390/a13030050													
J								A Dynamic Bayesian Network Structure for Joint Diagnostics and Prognostics of Complex Engineering Systems	ALGORITHMS										dynamic Bayesian networks; prognostics and health management; complex engineering systems; causal modeling; risk assessment; safety monitoring	PREDICTION	Dynamic Bayesian networks (DBNs) represent complex time-dependent causal relationships through the use of conditional probabilities and directed acyclic graph models. DBNs enable the forward and backward inference of system states, diagnosing current system health, and forecasting future system prognosis within the same modeling framework. As a result, there has been growing interest in using DBNs for reliability engineering problems and applications in risk assessment. However, there are open questions about how they can be used to support diagnostics and prognostic health monitoring of a complex engineering system (CES), e.g., power plants, processing facilities and maritime vessels. These systems' tightly integrated human, hardware, and software components and dynamic operational environments have previously been difficult to model. As part of the growing literature advancing the understanding of how DBNs can be used to improve the risk assessments and health monitoring of CESs, this paper shows the prognostic and diagnostic inference capabilities that are possible to encapsulate within a single DBN model. Using simulated accident sequence data from a model sodium fast nuclear reactor as a case study, a DBN is designed, quantified, and verified based on evidence associated with a transient overpower. The results indicate that a joint prognostic and diagnostic model that is responsive to new system evidence can be generated from operating data to represent CES health. Such a model can therefore serve as another training tool for CES operators to better prepare for accident scenarios.																		1999-4893				MAR	2020	13	3							64	10.3390/a13030064													
J								Time Series Clustering Model based on DTW for Classifying Car Parks	ALGORITHMS										time series clustering; dynamic time warping; car park; Euclidean distance		An increasing number of automobiles have led to a serious shortage of parking spaces and a serious imbalance of parking supply and demand. The best way to solve these problems is to achieve the reasonable planning and classify management of car parks, guide the intelligent parking, and then promote its marketization and industrialization. Therefore, we aim to adopt clustering method to classify car parks. Owing to the time series characteristics of car park data, a time series clustering framework, including preprocessing, distance measurement, clustering and evaluation, is first developed for classifying car parks. Then, in view of the randomness of existing clustering models, a new time series clustering model based on dynamic time warping (DTW) is proposed, which contains distance radius calculation, obtaining density of the neighbor area, k centers initialization, and clustering. Finally, some UCR datasets and data of 27 car parks are employed to evaluate the performance of the models and results show that the proposed model performs obviously better results than those clustering models based on Euclidean distance (ED) and traditional clustering models based on DTW.																		1999-4893				MAR	2020	13	3							57	10.3390/a13030057													
J								MDAN-UNet: Multi-Scale and Dual Attention Enhanced Nested U-Net Architecture for Segmentation of Optical Coherence Tomography Images	ALGORITHMS										optical coherence tomography; fully convolutional network; layer segmentation; fluid segmentation	FLUID SEGMENTATION; RETINAL LAYER; NETWORKS	Optical coherence tomography (OCT) is an optical high-resolution imaging technique for ophthalmic diagnosis. In this paper, we take advantages of multi-scale input, multi-scale side output and dual attention mechanism and present an enhanced nested U-Net architecture (MDAN-UNet), a new powerful fully convolutional network for automatic end-to-end segmentation of OCT images. We have evaluated two versions of MDAN-UNet (MDAN-UNet-16 and MDAN-UNet-32) on two publicly available benchmark datasets which are the Duke Diabetic Macular Edema (DME) dataset and the RETOUCH dataset, in comparison with other state-of-the-art segmentation methods. Our experiment demonstrates that MDAN-UNet-32 achieved the best performance, followed by MDAN-UNet-16 with smaller parameter, for multi-layer segmentation and multi-fluid segmentation respectively.																		1999-4893				MAR	2020	13	3							60	10.3390/a13030060													
J								Approximation and Uncertainty Quantification of Systems with Arbitrary Parameter Distributions Using Weighted Leja Interpolation	ALGORITHMS										adaptive algorithms; arbitrary probability distributions; sparse interpolation; uncertainty quantification; weighted Leja sequences	PARTIAL-DIFFERENTIAL-EQUATIONS; STOCHASTIC COLLOCATION METHOD; POLYNOMIAL CHAOS; CLENSHAW-CURTIS; SPARSE; EXPANSIONS; RULES	Approximation and uncertainty quantification methods based on Lagrange interpolation are typically abandoned in cases where the probability distributions of one or more system parameters are not normal, uniform, or closely related distributions, due to the computational issues that arise when one wishes to define interpolation nodes for general distributions. This paper examines the use of the recently introduced weighted Leja nodes for that purpose. Weighted Leja interpolation rules are presented, along with a dimension-adaptive sparse interpolation algorithm, to be employed in the case of high-dimensional input uncertainty. The performance and reliability of the suggested approach is verified by four numerical experiments, where the respective models feature extreme value and truncated normal parameter distributions. Furthermore, the suggested approach is compared with a well-established polynomial chaos method and found to be either comparable or superior in terms of approximation and statistics estimation accuracy.																		1999-4893				MAR	2020	13	3							51	10.3390/a13030051													
J								Model of Multi-branch Trees for Efficient Resource Allocation	ALGORITHMS										multi-branch trees; resource allocation; budget allocation; internal assessment; assessment costs; allocational efficiency; AHP; ternary diagram		Although exploring the principles of resource allocation is still important in many fields, little is known about appropriate methods for optimal resource allocation thus far. This is because we should consider many issues including opposing interests between many types of stakeholders. Here, we develop a new allocation method to resolve budget conflicts. To do so, we consider two points-minimizing assessment costs and satisfying allocational efficiency. In our method, an evaluator's assessment is restricted to one's own projects in one's own department, and both an executive's and mid-level executives' assessments are also restricted to each representative project in each branch or department they manage. At the same time, we develop a calculation method to integrate such assessments by using a multi-branch tree structure, where a set of leaf nodes represents projects and a set of non-leaf nodes represents either directors or executives. Our method is incentive-compatible because no director has any incentive to make fallacious assessments.																		1999-4893				MAR	2020	13	3							55	10.3390/a13030055													
J								Nature-Inspired Optimization Algorithms for the 3D Reconstruction of Porous Media	ALGORITHMS										3D material reconstruction; porous media; nature-inspired algorithms; optimization; correlation functions	GA ALGORITHM; FLOW; RECOGNITION; STRATEGIES; MOMENTS; SEM; GSA	One of the most challenging problems that are still open in the field of materials science is the 3D reconstruction of porous media using information from a single 2D thin image of the original material. Such a reconstruction is only feasible subject to some important assumptions that need to be made as far as the statistical properties of the material are concerned. In this study, the aforementioned problem is investigated as an explicitly formulated optimization problem, with the phase of each porous material point being decided such that the resulting 3D material model shows the same statistical properties as its corresponding 2D version. Based on this problem formulation, herein for the first time, several traditional (genetic algorithms-GAs, particle swarm optimization-PSO, differential evolution-DE), as well as recently proposed (firefly algorithm-FA, artificial bee colony-ABC, gravitational search algorithm-GSA) nature-inspired optimization algorithms were applied to solve the 3D reconstruction problem. These algorithms utilized a newly proposed data representation scheme that decreased the number of unknowns searched by the optimization process. The advantages of addressing the 3D reconstruction of porous media through the application of a parallel heuristic optimization algorithm were clearly defined, while appropriate experiments demonstrating the greater performance of the GA algorithm in almost all the cases by a factor between 5%-84% (porosity accuracy) and 3%-15% (auto-correlation function accuracy) over the PSO, DE, FA, ABC, and GSA algorithms were undertaken. Moreover, this study revealed that statistical functions of a high order need to be incorporated into the reconstruction procedure to increase the reconstruction accuracy.																		1999-4893				MAR	2020	13	3							65	10.3390/a13030065													
J								Observability of Uncertain Nonlinear Systems Using Interval Analysis	ALGORITHMS										observability; uncertain nonlinear systems; interval analysis	DESIGN	In the field of control engineering, observability of uncertain nonlinear systems is often neglected and not examined. This is due to the complex analytical calculations required for the verification. Therefore, the aim of this work is to provide an algorithm which numerically analyzes the observability of nonlinear systems described by finite-dimensional, continuous-time sets of ordinary differential equations. The algorithm is based on definitions for distinguishability and local observability using a rank check from which conditions are deduced. The only requirements are the uncertain model equations of the system. Further, the methodology verifies observability of nonlinear systems on a given state space. In case that the state space is not fully observable, the algorithm provides the observable set of states. In addition, the results obtained by the algorithm allows insight into why the remaining states cannot be distinguished.																		1999-4893				MAR	2020	13	3							66	10.3390/a13030066													
J								Kalman Filter-Based Online Identification of the Electric Power Characteristic of Solid Oxide Fuel Cells Aiming at Maximum Power Point Tracking	ALGORITHMS										Kalman filter design; fuel cells; maximum power point tracking; real-time optimization		High-temperature fuel cells are one of the devices currently investigated for an integration into distributed power supply grids. Such distributed grids aim at the simultaneous production of thermal energy and electricity. To maximize the efficiency of fuel cell systems, it is reasonable to track the point of maximum electric power production and to operate the system in close vicinity to this point. However, variations of gas mass flows, especially the concentration of hydrogen contained in the anode gas, as well as variations of the internal temperature distribution in the fuel cell stack module lead to the fact that the maximum power point changes in dependence of the aforementioned phenomena. Therefore, this paper first proposes a real-time capable stochastic filter approach for the local identification of the electric power characteristic of the fuel cell. Second, based on this estimate, a maximum power point tracking procedure is derived. It is based on an iteration procedure under consideration of the estimation accuracy of the stochastic filter and adjusts the fuel cell's electric current so that optimal operating points are guaranteed. Numerical simulations, based on real measured data gathered at a test rig available at the Chair of Mechatronics at the University of Rostock, Germany, conclude this paper.																		1999-4893				MAR	2020	13	3							58	10.3390/a13030058													
J								On a Hybridization of Deep Learning and Rough Set Based Granular Computing	ALGORITHMS										rough sets; deep learning; classification		The set of heuristics constituting the methods of deep learning has proved very efficient in complex problems of artificial intelligence such as pattern recognition, speech recognition, etc., solving them with better accuracy than previously applied methods. Our aim in this work has been to integrate the concept of the rough set to the repository of tools applied in deep learning in the form of rough mereological granular computing. In our previous research we have presented the high efficiency of our decision system approximation techniques (creating granular reflections of systems), which, with a large reduction in the size of the training systems, maintained the internal knowledge of the original data. The current research has led us to the question whether granular reflections of decision systems can be effectively learned by neural networks and whether the deep learning will be able to extract the knowledge from the approximated decision systems. Our results show that granulated datasets perform well when mined by deep learning tools. We have performed exemplary experiments using data from the UCI repository-Pytorch and Tensorflow libraries were used for building neural network and classification process. It turns out that deep learning method works effectively based on reduced training sets. Approximation of decision systems before neural networks learning can be important step to give the opportunity to learn in reasonable time.																		1999-4893				MAR	2020	13	3							63	10.3390/a13030063													
J								Classical and Deep Learning Paradigms for Detection and Validation of Key Genes of Risky Outcomes of HCV	ALGORITHMS										key genes; HCC; HCV; classical machine learning; deep learning; autoencoders	HEPATOCELLULAR-CARCINOMA; COMPONENT ANALYSIS; EXPRESSION; PROLIFERATION; TUMORIGENESIS; REPLICATION; PROGRESSION; PATTERNS; STRESS; LEVEL	Hepatitis C virus (HCV) is one of the most dangerous viruses worldwide. It is the foremost cause of the hepatic cirrhosis, and hepatocellular carcinoma, HCC. Detecting new key genes that play a role in the growth of HCC in HCV patients using machine learning techniques paves the way for producing accurate antivirals. In this work, there are two phases: detecting the up/downregulated genes using classical univariate and multivariate feature selection methods, and validating the retrieved list of genes using Insilico classifiers. However, the classification algorithms in the medical domain frequently suffer from a deficiency of training cases. Therefore, a deep neural network approach is proposed here to validate the significance of the retrieved genes in classifying the HCV-infected samples from the disinfected ones. The validation model is based on the artificial generation of new examples from the retrieved genes' expressions using sparse autoencoders. Subsequently, the generated genes' expressions data are used to train conventional classifiers. Our results in the first phase yielded a better retrieval of significant genes using Principal Component Analysis (PCA), a multivariate approach. The retrieved list of genes using PCA had a higher number of HCC biomarkers compared to the ones retrieved from the univariate methods. In the second phase, the classification accuracy can reveal the relevance of the extracted key genes in classifying the HCV-infected and disinfected samples.																		1999-4893				MAR	2020	13	3							73	10.3390/a13030073													
J								Uncertainty Propagation through a Point Model for Steady-State Two-Phase Pipe Flow	ALGORITHMS										two-phase flow; unit cell; uncertainty quantification; sensitivity analysis; Monte Carlo; polynomial chaos	SENSITIVITY-ANALYSIS; SLUG; QUANTIFICATION; IDENTIFICATION; PATTERN	Uncertainty propagation is used to quantify the uncertainty in model predictions in the presence of uncertain input variables. In this study, we analyze a steady-state point-model for two-phase gas-liquid flow. We present prediction intervals for holdup and pressure drop that are obtained from knowledge of the measurement error in the variables provided to the model. The analysis also uncovers which variables the predictions are most sensitive to. Sensitivity indices and prediction intervals are calculated by two different methods, Monte Carlo and polynomial chaos. The methods give similar prediction intervals, and they agree that the predictions are most sensitive to the pipe diameter and the liquid viscosity. However, the Monte Carlo simulations require fewer model evaluations and less computational time. The model predictions are also compared to experiments while accounting for uncertainty, and the holdup predictions are accurate, but there is bias in the pressure drop estimates.																		1999-4893				MAR	2020	13	3							53	10.3390/a13030053													
J								Multidimensional Group Recommendations in the Health Domain	ALGORITHMS										recommendations; group recommendations; semantic similarity; group aggregation	SYSTEMS	Providing useful resources to patients is essential in achieving the vision of participatory medicine. However, the problem of identifying pertinent content for a group of patients is even more difficult than identifying information for just one. Nevertheless, studies suggest that the group dynamics-based principles of behavior change have a positive effect on the patients' welfare. Along these lines, in this paper, we present a multidimensional recommendation model in the health domain using collaborative filtering. We propose a novel semantic similarity function between users, going beyond patient medical problems, considering additional dimensions such as the education level, the health literacy, and the psycho-emotional status of the patients. Exploiting those dimensions, we are interested in providing recommendations that are both high relevant and fair to groups of patients. Consequently, we introduce the notion of fairness and we present a new aggregation method, accumulating preference scores. We experimentally show that our approach can perform better recommendations to small group of patients for useful information documents.																		1999-4893				MAR	2020	13	3							54	10.3390/a13030054													
J								Misalignment Fault Prediction of Wind Turbines Based on Combined Forecasting Model	ALGORITHMS										misalignment; fault prediction; combined prediction; multivariate grey model; quantum genetic algorithm; least squares support vector machine	DIAGNOSIS	Due to the harsh working environment of wind turbines, various types of faults are prone to occur during long-term operation. Misalignment faults between the gearbox and the generator are one of the latent common faults for doubly-fed wind turbines. Compared with other faults like gears and bearings, the prediction research of misalignment faults for wind turbines is relatively few. How to accurately predict its developing trend has always been a difficulty. In this paper, a combined forecasting model is proposed for misalignment fault prediction of wind turbines based on vibration and current signals. In the modelling, the improved Multivariate Grey Model (IMGM) is used to predict the deterministic trend and the Least Squares Support Vector Machine (LSSVM) optimized by quantum genetic algorithm (QGA) is adopted to predict the stochastic trend of the fault index separately, and another LSSVM optimized by QGA is used as a non-linear combiner. Multiple information of time-domain, frequency-domain and time-frequency domain of the wind turbine's vibration or current signals are extracted as the input vectors of the combined forecasting model and the kurtosis index is regarded as the output. The simulation results show that the proposed combined model has higher prediction accuracy than the single forecasting models.																		1999-4893				MAR	2020	13	3							56	10.3390/a13030056													
J								Oil Spill Monitoring of Shipborne Radar Image Features Using SVM and Local Adaptive Threshold	ALGORITHMS										oil spill; SVM; real-time monitoring; shipborne radar; remote sensing; image processing; GIS	SAR; ALGORITHM; MODEL	In the case of marine accidents, monitoring marine oil spills can provide an important basis for identifying liabilities and assessing the damage. Shipborne radar can ensure large-scale, real-time monitoring, in all weather, with high-resolution. It therefore has the potential for broad applications in oil spill monitoring. Considering the original gray-scale image from the shipborne radar acquired in the case of the Dalian 7.16 oil spill accident, a complete oil spill detection method is proposed. Firstly, the co-frequency interferences and speckles in the original image are eliminated by preprocessing. Secondly, the wave information is classified using a support vector machine (SVM), and the effective wave monitoring area is generated according to the gray distribution matrix. Finally, oil spills are detected by a local adaptive threshold and displayed on an electronic chart based on geographic information system (GIS). The results show that the SVM can extract the effective wave information from the original shipborne radar image, and the local adaptive threshold method has strong applicability for oil film segmentation. This method can provide a technical basis for real-time cleaning and liability determination in oil spill accidents.																		1999-4893				MAR	2020	13	3							69	10.3390/a13030069													
J								Ensemble Learning of Hybrid Acoustic Features for Speech Emotion Recognition	ALGORITHMS										emotion recognition; ensemble algorithm; feature extraction; hybrid feature; machine learning; supervised learning	CLASSIFICATION; PERFORMANCE; SELECTION	Automatic recognition of emotion is important for facilitating seamless interactivity between a human being and intelligent robot towards the full realization of a smart society. The methods of signal processing and machine learning are widely applied to recognize human emotions based on features extracted from facial images, video files or speech signals. However, these features were not able to recognize the fear emotion with the same level of precision as other emotions. The authors propose the agglutination of prosodic and spectral features from a group of carefully selected features to realize hybrid acoustic features for improving the task of emotion recognition. Experiments were performed to test the effectiveness of the proposed features extracted from speech files of two public databases and used to train five popular ensemble learning algorithms. Results show that random decision forest ensemble learning of the proposed hybrid acoustic features is highly effective for speech emotion recognition.																		1999-4893				MAR	2020	13	3							70	10.3390/a13030070													
J								Sovereign Rating Analysis through the Dominance-Based Rough Set Approach	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Multi-Criteria Decision Analysis; Sovereign rating; Dominance principle; Dominance-based Rough Set Approach		The classifications of risk made by international rating agencies aim at guiding investors when it comes to the capacity and disposition of the evaluated countries to honor their public debt commitments. In this study, the analysis of economic variables of sovereign rating, in a context of vagueness and uncertainty, leads the inference of patterns (multicriteria rules) by following the Dominance-based Rough Set Approach (DRSA). The discovery of patterns in data may be useful for subsidizing foreign investment decisions in countries; and this knowledge base may be used in rule-based expert systems (learning from training examples).The present study seeks to complement the analysis produced by an international credit rating agency, Standard & Poor's (S&P), for the year 2018.																	0867-6356	2300-3405				MAR	2020	45	1					3	16		10.2478/fcds-2020-0001													
J								Morality, protection, security and gain: lessons from a minimalistic, economically inspired multi-agent model	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										morality; social norm; equilibrium; mathematical modeling; simulation	SIMULATION; ETHICS	In this work, we introduce a simple multi-agent simulation model with two roles of agents that correspond to moral and immoral attitudes. The model is given explicitly by a set of mathematical equations with continuous variables and is characterized by four parameters: morality, protection, and two efficiency parameters. Agents are free to adjust their roles to maximize individual gains. The model is analyzed theoretically to find conditions for its stability, i.e., the fractions of agents of both roles that lead to an equilibrium in their gains. A multi-agent simulation is also developed to verify the dynamics of the model for all values of morality and protection parameters, and to identify potential discrepancies with the theoretical analysis.																	0867-6356	2300-3405				MAR	2020	45	1					17	33		10.2478/fcds-2020-0002													
J								Recent Results on Computational Molecular Modeling of The Origins of Life	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										ab initio; molecular modeling; quantum dynamics; origins of life; atomistic simulations	FREE-ENERGY; DENSITY FUNCTIONALS; PREBIOTIC SYNTHESIS; REPLICA-EXCHANGE; SHOCK SYNTHESIS; FORCE-FIELD; AMINO-ACIDS; AB-INITIO; DYNAMICS; SIMULATION	In the last decade of research in the origins of life, there has been an increase in the interest on theoretical molecular modeling methods aimed to improve the accuracy and speed of the algorithms that solve the molecular mechanics and chemical reactions of the matter. Research on the scenarios of prebiotic chemistry has also advanced. The presented work attempts to discuss the latest computational techniques and trends implemented so far. Although it is difficult to cover the full extent of the current publications, we tried to orient the reader into the modern tendencies and challenges faced by those who are in the origins of life field.																	0867-6356	2300-3405				MAR	2020	45	1					35	46		10.2478/fcds-2020-0003													
J								Scheduling High Multiplicity Coupled Tasks	FOUNDATIONS OF COMPUTING AND DECISION SCIENCES										Coupled tasks; scheduling; complexity theory; asymptotically optimal algorithms; high multiplicity	COMPLEXITY	The coupled tasks scheduling problem is class of scheduling problems, where each task consists of two operations and a separation gap between them. The high-multiplicity is a compact encoding, where identical tasks are grouped together, and the group is specified instead of each individual task. Consequently the encoding of a problem instance is decreased significantly. In this article we derive a lower bound for the problem variant as well as propose an asymptotically optimal algorithm. The theoretical results are complemented with computational experiment, where a new algorithm is compared with three other algorithms implemented.																	0867-6356	2300-3405				MAR	2020	45	1					47	61		10.2478/fcds-2020-0004													
J								Fuzzy Information Diffusion in Twitter by Considering User's Influence	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Graph mining; fuzzy sets; information diffusion; knowledge extraction; social media analytics; user influence; sentiment analysis		Does a post with specific emotional content that is posted on Twitter by an influential user have the capability to affect and even alter the opinions of those who read it? Accordingly, "influential" users affected by this post can then affect their followers so that eventually a large number of users may change their opinions about the subject the aforementioned post was made on? Social Influence can be described as the power or even the ability of a person to yet influence the thoughts and actions of other users. So, User Influence stands as a value that depends on the interest of the followers (via replies, mentions, retweets, favorites). Our study focuses on identifying such phenomena on the Twitter graph of posts and on determining which users' posts can trigger them. Furthermore, we analyze the Influence Metrics of all users taking part in specific discussions and verify the differences among them. Finally the percentage of Graph cover when the diffusion starts from the "influential" users, is measured and corresponding results are extracted. Hence, results show that the proposed implementations and methodology can assist in identifying "influential" users, that play a dominant role in information diffusion.																	0218-2130	1793-6349				MAR	2020	29	2			SI				2040003	10.1142/S0218213020400035													
J								Anticipointment Detection in Event Tweets	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Event detection; emotion detection; Twitter		We developed a system to detect positive expectation, disappointment, and satisfaction in tweets that refer to events automatically discovered in the Twitter stream. The emotional content shared on Twitter when referring to public events can provide insights into the presumed and experienced quality of the event. We expected to find a connection between positive expectation and disappointment, a succession that is referred to as anticipointment. The application of computational approaches makes it possible to detect the presence and strength of this hypothetical relation for a large number of events. We extracted events from a longitudinal dataset of Dutch Twitter posts, and modeled classifiers to detect emotion in the tweets related to those events by means of hashtag-labeled training data. After classifying all tweets before and after the events in our dataset, we summarized the collective emotions for over 3000 events as the percentage of tweets classified as positive expectation (in anticipation), disappointment and satisfaction (in hindsight). Only a weak correlation of around 0.2 was found between positive expectation and disappointment, while a higher correlation of 0.6 was found between positive expectation and satisfaction. The most anticipointing events were events with a clear loss, such as a canceled event or when the favored sports team had lost. We conclude that senders of Twitter posts might be more inclined to share satisfaction than disappointment after a much anticipated event.																	0218-2130	1793-6349				MAR	2020	29	2			SI				2040001	10.1142/S0218213020400011													
J								Short Semantic Patterns: A Linguistic Pattern Mining Approach for Content Analysis Applied to Hate Speech	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Natural language processing; linguistic pattern; hate speech; word embeddings	TWITTER; SENTIMENT	Microblog posts such as tweets frequently contain users' opinions and thoughts about events, products, people, institutions, etc. However, the usage of social media to propagate hate speech is not an uncommon occurrence. Analyzing hateful speech in social media is essential for understanding, fighting and discouraging such actions. We believe that by extracting fragments of text that are semantically similar it is possible to depict recurrent linguistic patterns in certain kinds of discourse. Therefore, we aim to use these patterns to encapsulate frequent statements textually expressed in microblog posts. In this paper, we propose to exploit such linguistic patterns in the context of hate speech. Through a technique that we call SSP (Short Semantic Pattern) mining, we are able to extract sequences of words that share a similar meaning in their word embedding representation. By analyzing the extracted patterns, we reveal some kinds of discourses that are replayed across a dataset, such as racist and sexist statements. Afterwards, we experiment using SSP as features to build classifiers that detect if a tweet contains hate speech (binary classification) and to distinguish between sexist, racist and clean tweets (ternary classification). The SSP instances encountered in tweets containing sexism have shown that a large number of sexist tweets began with the introduction 'I'm not sexist but' and 'Call me sexist but'. Meanwhile, SSP instances found in tweets reproducing racism revealed a prominence of contents against the Islamic religion, associated entities and organizations.																	0218-2130	1793-6349				MAR	2020	29	2			SI				2040002	10.1142/S0218213020400023													
J								Sentiment Analysis of Teachers Using Social Information in Educational Platform Environments	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Text mining; machine learning; opinion mining; data mining; feature extraction; feature selection	CLASSIFICATION	Learners' opinions constitute an important source of information that can be useful to teachers and educational instructors in order to improve learning procedures and training activities. By analyzing learners' actions and extracting data related to their learning behavior, educators can specify proper learning approaches to stimulate learners' interest and contribute to constructive monitoring of learning progress during the course or to improve future courses. Learners-generated content and their feedback and comments can provide indicative information about the educational procedures that they attended and the training activities that they participated in. Educational systems must possess mechanisms to analyze learners' comments and automatically specify their opinions and attitude towards the courses and the learning activities that are offered to them. This paper describes a Greek language sentiment analysis system that analyzes texts written in Greek language and generates feature vectors which together with classification algorithms give us the opportunity to classify Greek texts based on the personal opinion and the degree of satisfaction expressed. The sentiment analysis module has been integrated into the hybrid educational systems of the Greek school network that offers life-long learning courses. The module offers a wide range of possibilities to lecturers, policymakers and educational institutes that participate in the training procedure and offers life-long learning courses, to understand how their learners perceive learning activities and specify what aspects of the learning activities they liked and disliked. The experimental study show quite interesting results regarding the performance of the sentiment analysis methodology and the specification of users' opinions and satisfaction. The feature analysis demonstrates interesting findings regarding the characteristics that provide indicative information for opinion analysis and embeddings combined with deep learning approaches yield satisfactory results.																	0218-2130	1793-6349				MAR	2020	29	2			SI				2040004	10.1142/S0218213020400047													
J								A GPU fully vectorized approach to accelerate performance of NSGA-2 based on stochastic non-domination sorting and grid-crowding	APPLIED SOFT COMPUTING										Multi-Objective Evolutionary Algorithms (MOEAs); Graphic Processing Unit (GPU); High Performance Computing (HPC); Gridding; Stochastic non-domination sorting	PARALLEL GENETIC ALGORITHM; OPTIMIZATION	This work introduces an accelerated implementation of NSGA-2 on a graphics processing unit (GPU) to reduce execution time. Parallelism is achieved in the population level using vectorization. All the components of the algorithm are run on the device, minimizing communication overhead. New stochastic versions of both non-domination sorting and crowding are introduced in the article. They are designed to be efficiently vectorized on GPU, therefore, the proposed approach is finally limited by the sorting procedure (O(nlog(n))), while the original algorithm was of order O(n(2)). This improvement is reflected on the speed-ups attained in the experiments. The results include metrics regarding solution quality to show there is not a significant trade-off between acceleration and performance. The possibilities to apply multi-objective evolutionary algorithms to real-time applications are discussed in the conclusions, where the possibility of devising restricted multi-objective evolutionary algorithms is concluded a possibility to implement faster methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106047	10.1016/j.asoc.2019.106047													
J								A genetic Artificial Bee Colony algorithm for signal reconstruction based big data optimization	APPLIED SOFT COMPUTING										Big data optimization; Signal decomposition; Artificial Bee Colony	CONVOLUTIONAL NEURAL-NETWORKS; COOPERATIVE COEVOLUTION; PROMISES	In recent years, the researchers have witnessed the changes or transformations driven by the existence of the big data on the definitions, complexities and future directions of the real world optimization problems. Analyzing the capabilities of the previously introduced techniques, determining possible drawbacks of them and developing new methods by taking into consideration of the unique properties related with the big data are nowadays in urgent demands. Artificial Bee Colony (ABC) algorithm inspired by the clever foraging behaviors of the real honey bees is one of the most successful swarm intelligence based optimization algorithms. In this study, a novel ABC algorithm based big data optimization technique was proposed. For exploring the solving abilities of the proposed technique, a set of experimental studies has been carried out by using different signal decomposition based big data optimization problems presented at the Congress on Evolutionary Computation (CEC) 2015 Big Data Optimization Competition. The results obtained from the experimental studies first were compared with the well-known variants of the standard ABC algorithm named gbest-guided ABC (GABC), ABC/best/1, ABC/best/2, crossover ABC (CABC), converge-onlookers ABC (COABC) and quick ABC (qABC). The results of the proposed ABC algorithm were also compared with the Differential Evolution (DE) algorithm, Genetic algorithm (GA), Firefly algorithm (FA), Fireworks algorithm (FW), Phase Base Optimization (PBO) algorithm, Particle Swarm Optimization (PSO) algorithm and Dragonfly algorithm (DA) based big data optimization techniques. From the experimental studies, it was understood that the newly introduced ABC algorithm based technique is capable of producing better or at least promising results compared to the mentioned big data optimization techniques for all of the benchmark instances. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106053	10.1016/j.asoc.2019.106053													
J								A nonlinear method of learning neuro-fuzzy models for dynamic control systems	APPLIED SOFT COMPUTING										ANFIS; RMSE; MAPE; Soft computing; Method of areas' ratio; MAR-ANFIS	DECISION-MAKING; ANFIS; IDENTIFICATION; ALGORITHM	The paper describes a new learning algorithm of adaptive neuro-fuzzy inference systems that is based on the method of areas' ratio (MAR-ANFIS). Using linear and nonlinear functions we obtain a generalized model for fuzzy inference. Considering various implication methods, different t- or s-norms and equations for fuzzy inference composition we can change the properties of the resulting output variable. As an example, we illustrate the proposed learning algorithm and show its distinctive characteristics. Firstly, MAR-ANFIS learning algorithm is additive. Secondly, soft operators provide symmetry for the output variable. Also, the proposed algorithm that allows improving accuracy when learning fuzzy system and speed of its learning. Using detailed numerically calculated RMSE and MAPE we evaluate the proposed algorithm. High accuracy of the proposed MAR-ANFIS is confirmed through the calculation of the learning time of neuro-fuzzy network RMSE and MAPE. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106030	10.1016/j.asoc.2019.106030													
J								A multi-objective perspective on performance assessment and automated selection of single-objective optimization algorithms	APPLIED SOFT COMPUTING										Algorithm selection; Multi-objective optimization; Performance measurement; Combinatorial optimization; Traveling Salesperson Problem		We build upon a recently proposed multi-objective view onto performance measurement of single-objective stochastic solvers. The trade-off between the fraction of failed runs and the mean runtime of successful runs - both to be minimized - is directly analyzed based on a study on algorithm selection of inexact state-of-the-art solvers for the famous Traveling Salesperson Problem (TSP). Moreover, we adopt the hypervolume indicator (HV) commonly used in multi-objective optimization for simultaneously assessing both conflicting objectives and investigate relations to commonly used performance indicators, both theoretically and empirically. Next to Penalized Average Runtime (PAR) and Penalized Quantile Runtime (PQR), the HV measure is used as a core concept within the construction of perinstance algorithm selection models offering interesting insights into complementary behavior of inexact TSP solvers. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105901	10.1016/j.asoc.2019.105901													
J								Novel quantum inspired approaches for automatic clustering of gray level images using Particle Swarm Optimization, Spider Monkey Optimization and Ageist Spider Monkey Optimization algorithms	APPLIED SOFT COMPUTING										Automatic clustering; Cluster validity indices; Quantum computing; Meta-heuristic algorithms; Ageist spider monkey; Spider monkey; Particle swarm optimization; t-test; Friedman test	EVOLUTIONARY ALGORITHM; VALIDITY MEASURE; SEGMENTATION	This paper is intended to identify the optimal number of clusters automatically from an image dataset using some quantum behaved nature inspired meta-heuristic algorithms. Due to the lack of sufficient information, it is difficult to identify the appropriate number of clusters from a dataset, which has enthused the researchers to solve the problem of automatic clustering and to open up a new era of cluster analysis with the help of several natures inspired meta-heuristic algorithms. In this paper, three quantum inspired meta-heuristic techniques, viz., Quantum Inspired Particle Swarm Optimization (QIPSO), Quantum Inspired Spider Monkey Optimization (QISMO) and Quantum Inspired Ageist Spider Monkey Optimization (QIASMO), have been proposed. A comparison has been outlined between the quantum inspired algorithms with their corresponding classical counterparts. The efficiency of the quantum inspired algorithms has been established over their corresponding classical counterparts with regards to fitness, mean, standard deviation, standard errors of fitness, convergence curves (for benchmarked mathematical functions) and computational time. Finally, the results of two statistical superiority tests, viz., t- test and Friedman test have been provided to prove the superiority of the proposed methods. The superiority of the proposed methods has been established on five publicly available real life image datasets, five Berkeley image datasets of different dimensions and four benchmark mathematical functions both visually and quantitatively. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106040	10.1016/j.asoc.2019.106040													
J								Benchmarking discrete optimization heuristics with IOHprofiler	APPLIED SOFT COMPUTING										Combinatorial optimization; Black-box optimization; Randomized search heuristics; Benchmarking; Evolutionary computation	BLACK-BOX COMPLEXITY; ISING-MODEL; FRAMEWORK; SEARCH	Automated benchmarking environments aim to support researchers in understanding how different algorithms perform on different types of optimization problems. Such comparisons provide insights into the strengths and weaknesses of different approaches, which can be leveraged into designing new algorithms and into the automation of algorithm selection and configuration. With the ultimate goal to create a meaningful benchmark set for iterative optimization heuristics, we have recently released IOHprofiler, a software built to create detailed performance comparisons between iterative optimization heuristics. With this present work we demonstrate that IOHprofiler provides a suitable environment for automated benchmarking. We compile and assess a selection of 23 discrete optimization problems that subscribe to different types of fitness landscapes. For each selected problem we compare performances of twelve different heuristics, which are as of now available as baseline algorithms in IOHprofiler. We also provide a new module for IOHprofiler which extents the fixed-target and fixed-budget results for the individual problems by ECDF results, which allows one to derive aggregated performance statistics for groups of problems. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106027	10.1016/j.asoc.2019.106027													
J								Multi-Criteria Decision Making techniques for the management of public procurement tenders: A case study	APPLIED SOFT COMPUTING										Multi-Criteria Decision Making; Public procurement; Compensatory models	OF-THE-ART; ANALYTIC HIERARCHY PROCESS; SUPPLIER SELECTION; CROSS-EFFICIENCY; AHP; PROMETHEE; DESIGN; MODEL; DEA	Multi-Criteria Decision Making (MCDM) techniques are mathematical tools that help decision makers evaluating and ranking in an automatic way many possible alternatives over multiple conflicting criteria in highly complex situations. Several MCDM approaches exist, and their application fields are numerous, including the Supplier Selection Problem (SSP), which is an important problem in the management field. The aim of this paper is to perform a comparative analysis among some selected well-known MCDM techniques to show how they can properly support the specific decision making process of Public Procurement (PP) tenders, which is a particular type of the SSP, characterized by very stringent rules, thus requiring a specific assessment. Indeed, PP is a field characterized by the need for transparency, objectivity, and non-discrimination, which requires tendering organizations to explicitly state the adopted awarding method, the chosen decision criteria, and their relative importance in the call for proposals. However, this field has been seldomly investigated in the pertinent literature and thus the aim of this paper is to overcome such a limitation. In particular, this work focuses on the most commonly adopted methods in the field of supplier selection, namely the Analytic Hierarchy Process (AHP), the Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE), the Multi Attribute Utility Theory (MAUT), and the Data Envelopment Analysis (DEA). First, we adapt these techniques to the PP problem and its requirements. Then, by means of some real tenders at a European Institution, the selected techniques are compared with each other and with the currently adopted methodology in their classical deterministic setting, to identify which method best suits the specific requirements of PP tenders. Hence, since nowadays uncertainty is inherent in data from real applications, and can be modelled by expert evaluations through fuzzy logic, the comparison is extended to the fuzzy counterparts of two of the most promising selected approaches, i.e., the Fuzzy AHP and the Fuzzy DEA, showing that these methods can be effectively applied to the PP sector also in the presence of uncertainty on the tenders data. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106064	10.1016/j.asoc.2020.106064													
J								Adaptive protection scheme for microgrids based on SOM clustering technique	APPLIED SOFT COMPUTING										Adaptive protection scheme; Microgrid; Overcurrent relays; Clustering; Self-Organizing Map (SOM); Distributed energy resources	OVERCURRENT RELAY COORDINATION; DISTRIBUTION-SYSTEMS; NETWORKS; GENERATION; ALGORITHM; STRATEGY; ISSUES; DG	Microgrids are penetrating into the power systems at an unprecedented rate. The reason is the mutual economic and environmental benefits of microgrids, both for power grid utility and the consumers. Some special features of microgrids such as, the two main operational conditions called, islanded and grid-connected modes, and being composed of various types of distributed energy resources along with different uncertainties cause some tough challenges to protection and control systems. From the protection aspect, the coordination of overcurrent relays protection will become a difficulty, due to the extensive changes in the fault current levels sensed by these devices. In this paper, a new adaptive protection coordination scheme based on Self-Organizing Map (SOM) clustering algorithm is proposed for digital overcurrent relays equipped with several setting groups. Considering the similarity of mis-coordinated relay pairs for the clustering purpose, the proposed protection scheme focuses on solving the mis-coordination between main/backup relay pairs. As a case study, a modified IEEE 33-bus test system is used as a microgrid. In the case study, a synchronous distributed generation and two electric vehicle charging stations are installed. The results suggest that not only the proposed method is fully capable and flexible to significantly improve the mis-coordination of overcurrent relay pairs, but it can also ameliorate the operating time of relay. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106062	10.1016/j.asoc.2020.106062													
J								A hybrid ANN-based imperial competitive algorithm methodology for structural damage identification of slab-on-girder bridge using data mining	APPLIED SOFT COMPUTING										Damage identification; Data mining; Imperial competitive algorithm; Artificial neural network; Cross Industry Standard Process for Data Mining; Evolutionary algorithms	ARTIFICIAL NEURAL-NETWORK; NATURAL FREQUENCIES; RCC DAMS; OPTIMIZATION; BEAMS; CLASSIFICATION; PREDICTION; MODELS	Implementation of data mining (DM) techniques in different areas of civil engineering has recently given very good results. However, application of DM in structural health monitoring (SHM) is not used as much as expected, thus, many challenges are still ahead. Therefore, it seems a vital need is required to develop the applicability of DM in SHM. To this end, the current study attempts to present a DM-based damage detection methodology using modal parameter data, which trained by means of a hybrid artificial neural network-based imperial competitive algorithm (ANN-ICA). Likewise, the hybrid ANN is optimized by a new optimization-based evolutionary algorithm, called ICA, to predict the severity and location of multiple damage cases obtained from experimental modal analysis of intact and damaged slab-on-girder bridge structures. Furthermore, the applicability of DM approach was developed to detect the hidden patterns in vibration data using Cross Industry Standard Process for DM (CRISP-DM) tool. The performance of the model was carried out using comparison of a pre-developed ANN and ANN-ICA model. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106013	10.1016/j.asoc.2019.106013													
J								Type II fuzzy set-based data analytics to explore amino acid associations in protein sequences of Swine Influenza Virus	APPLIED SOFT COMPUTING										Type II fuzzy set; Swine Influenza Virus; Data analytics	LOGIC; PATTERNS	The veracity present in molecular data available in biological databases possesses new challenges for data analytics. The analysis of molecular data of various diseases can provide vital information for developing better understanding of the molecular mechanism of a disease. In this paper, an attempt has been made to propose a model that addresses the issue of veracity in data analytics for amino acid association patterns in protein sequences of Swine Influenza Virus. The veracity is caused by intrasequential and inter-sequential biases present in the sequences due to varying degrees of relationships among amino acids. A complete dataset of 63,682 protein sequences is downloaded from NCBI and is refined. The refined dataset consists of 26,594 sequences which are employed in the present study. The type I fuzzy set is employed to explore amino acid association patterns in the dataset. The type I fuzzy support is refined to partially remove the inter-sequential biases causing veracity in data. The remaining inter-sequential biases present in refined fuzzy support are evaluated and eliminated using type II fuzzy set. Hence, it is concluded that a combination of type II fuzzy & refined fuzzy approach is the optimal approach for extracting a better picture of amino acid association patterns in the molecular dataset. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105856	10.1016/j.asoc.2019.105856													
J								Solution to economic emission dispatch problem including wind farms using Exchange Market Algorithm Method	APPLIED SOFT COMPUTING										Economic dispatch; Economic emission dispatch; Wind energy; Exchange market algorithm; Weibull function	PARTICLE SWARM OPTIMIZATION; COLONY OPTIMIZATION; LOAD DISPATCH; SYSTEM	This paper presents an Exchange Market Algorithm (stocktickerEMA) method for solving the Economic Emission Dispatch (EED) problem including wind farms in the power systems. The stocktickerEMA algorithm is a powerful and useful method for finding the optimal value of an optimization problem with high accuracy. In recent years, because of the emission of harmful gases from fossil fuels and global warming issues, the penetration level of cleaner energies such as the wind and solar energy has been increased in order to produce the desired electrical energy. Therefore, it is vital to consider the wind turbines and wind farms in the EED optimization problem. Due to the probabilistic nature of wind speed in wind turbines, the generated power by wind turbines and wind farms has uncertain nature. Hence, the Weibull probability distribution function is used to model the wind power in the EED problem. The proposed method is tested on the IEEE 40-units test system. The analysis shows that, compared to other algorithms the EMA method has faster convergence and better ability in finding the optimal solution for the EED problem. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106044	10.1016/j.asoc.2019.106044													
J								Grid based clustering for satisfiability solving	APPLIED SOFT COMPUTING										Satisfiability problem; Computational complexity; Problem solving; Data mining techniques; Data distribution; Grid clustering; BSO; DPLL	ALGORITHM	The originality of this work resides into the exploitation of data mining techniques for problem solving. Two major phases define this work. The first one is to determine the clustering technique that best suits each SAT instance based on the distribution of the later. The clustering technique is then applied to reduce the complexity of each instance by creating sub-instances that can be solved independently in the second phase. The latter consists into a resolution step where the DPLL or BSO algorithms are executed depending on the number of variables to be assigned in each cluster. This two-phase resolution strategy provides more efficient problem solving. The Boolean Satisfiability problem (SAT) is considered in this study because of its importance for the Artificial Intelligence (AI) community and the impact of its solving on other complex problems. Three different distributions of the problem were observed. The first distribution defines a space where the variables are dispersed forming regions of considerable density interspersed with regions of lower density or empty regions. On the other hand, the other two distributions do not show any significant shape, as the variables are randomly scattered, with one of these two dispersions having the particularity that almost all its variables are of high occurrence. To each of the three distributions, a clustering technique is associated. Density-based clustering techniques are the most appropriate type of clustering for the first distribution. Meanwhile, grid-based clustering and frequent patterns mining seem to be the most suitable clustering techniques for the second and third distributions. Investigations are undertaken on these latter issues and contributions are presented in this paper. Experiments were conducted on public benchmarks and the results showed the importance of the pre-processing step of data mining to solve the SAT problem. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106069	10.1016/j.asoc.2020.106069													
J								Extending social responsibility to small and medium-sized suppliers in supply chains: A fuzzy-set qualitative comparative analysis	APPLIED SOFT COMPUTING										Socially responsible supplier development (SRSD); Fuzzy-set qualitative comparative analysis (fsQCA); Corporate social responsibility (CSR); Supply chain partnership; Market turbulence	MARKET ORIENTATION; PERFORMANCE OUTCOMES; PRODUCT DEVELOPMENT; CORPORATE IMAGE; MANAGEMENT; PARTNERSHIP; INNOVATION; SUSTAINABILITY; COLLABORATION; INVOLVEMENT	Buying firms' sales and reputation will be greatly damaged by any non-responsible behaviors on the part of suppliers, especially when those suppliers, like small and medium-sized enterprises (SMEs), have restricted resources and capabilities. To eradicate these risks, a growing number of buying firms have introduced socially responsible supplier development (SRSD). SRSD, including monitoring and evaluating suppliers, can provide them with incentives and assistance. Based on configuration theory and contingency theory, fuzzy-set qualitative comparative analysis (fsQCA) is adopted in this study to examine how SRSD practices adopted by buying firms, supply chain partnership, and market turbulence affect the corporate social responsibility (CSR) performance of their SME suppliers. We find that, as core factors, supplier monitoring, supplier assistance, and supply chain partnership can work together with peripheral conditions to achieve superior CSR performance. In addition, even at different levels of market turbulence, superior CSR performance can be realized through different causal configurations. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105899	10.1016/j.asoc.2019.105899													
J								Glucose forecasting combining Markov chain based enrichment of data, random grammatical evolution and Bagging	APPLIED SOFT COMPUTING										Grammatical evolution; Diabetes management; Time series forecasting; Data augmentation; Ensemble models; Random-GE; Bagging	IDENTIFICATION; EQUATION; TIME	Diabetes Mellitus is a disease affecting more and more people every year. Depending on the kind of diabetes and sometimes on the stage of the illness, diabetic patients have to inject some amount of artificial insulin, namely bolus, before the meals, to make up the absence or malfunctioning of their natural insulin. This decision is a difficult task since they need to estimate the number of carbohydrates they are going to ingest, take into account the past and future circumstances, know the past values of glucose, evaluate if the effect of previously injected insulin has already finished and any other relevant information. In this paper, we present and compare a set of methodologies to automate the decision of the insulin bolus, which reduces the number of dangerous predictions. We combine two different data enrichment techniques based on Markov chains with grammatical evolution engines to generate models of blood glucose, and univariate marginal distribution algorithms and bagging techniques to select the set of models to assemble. In particular, we propose the Random-GE procedure, an adaptation of Random Forests to Grammatical Evolution, which leads to excellent prediction models, with a simple configuration and a reduced execution time. The ensemble gives the prediction of glucose for a duple of food and insulins, helping patients in the selection of the appropriate bolus to maintain healthy glucose levels after the meals. Experimental results show that our models get more accurate and robust predictions than previous approaches. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105923	10.1016/j.asoc.2019.105923													
J								A hybrid wavelet decomposer and GMDH-ELM ensemble model for Network function virtualization workload forecasting in cloud computing	APPLIED SOFT COMPUTING										Workload forecasting; Cloud computing; NFV; Wavelet decomposition; GMDH; ELM	TIME-SERIES PREDICTION; SUPPORT VECTOR REGRESSION; EXTREME LEARNING-MACHINE; NEURAL-NETWORK; OPTIMIZATION; ALGORITHM; STATE	Nowadays Network function virtualization (NFV) has drawn immense attention from many cloud providers because of its benefits. NFV enables networks to virtualize node functions such as firewalls, load balancers, and WAN accelerators, conventionally running on dedicated hardware, and instead implements them as virtual software components on standard servers, switches, and storages. In order to provide NFV resources and meet Service Level Agreement (SLA) conditions, minimize energy consumption and utilize physical resources efficiently, resource allocation in the cloud is an essential task. Since network traffic is changing rapidly, an optimized resource allocation strategy should consider resource auto-scaling property for NFV services. In order to scale cloud resources, we should forecast the NFV workload. Existing forecasting methods are providing poor results for highly volatile and fluctuating time series such as cloud workloads. Therefore, we propose a novel hybrid wavelet time series decomposer and GMDH-ELM ensemble method named Wavelet-GMDH-ELM (WGE) for NFV workload forecasting which predicts and ensembles workload in different time-frequency scales. We evaluate the WGE model with three real cloud workload traces to verify its prediction accuracy and compare it with state of the art methods. The results show the proposed method provides better average prediction accuracy. Especially it improves Mean Absolute Percentage Error (MAPE) at least 8% compared to the rival forecasting methods such as support vector regression (SVR) and Long short term memory (LSTM). (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105940	10.1016/j.asoc.2019.105940													
J								Non-inertial opposition-based particle swarm optimization and its theoretical analysis for deep learning applications	APPLIED SOFT COMPUTING										Deep learning applications; Particle swarm optimization; Non-inertial velocity update formula; Theoretical analysis	ALGORITHM; STRATEGY; MUTATION	Particle Swarm Optimization (PSO) and its variants are often used to train and optimize the structure and parameters of deep learning models to improve accuracy of learning results in the reasonable time-consuming. The performance of PSO is completed determined by its kinetic equation of particles. To accelerate convergence rate, a novel kinetic equation without inertial term is devised and applied to PSO, and then a non-inertial opposition-based particle swarm optimization (NOPSO) is generated combined with a adaptive elite mutation strategy and generalized opposition-based learning strategy. Simulation Experimental results show that the new kinetic equation has effectively accelerated convergence rate of PSO. Meanwhile, Theoretical analysis of the new kinetic equation is carried out by order-2 difference recurrence equation, the inference conclusions of which are consistent with the results of simulation experiments. NOPSO algorithm with a new kinetic equation is a highly competitive algorithm compared with some state-of-art PSOs and is suitable for deep learning applications. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106038	10.1016/j.asoc.2019.106038													
J								APAD: Autoencoder-based Payload Anomaly Detection for industrial IoE	APPLIED SOFT COMPUTING										Network anomaly detection; Industrial IoE; Industrial control system; False data injection; Security with deep learning		The Internet of Things era is being replaced by the Internet of Everything (IoE) era, where everything can communicate with everything else. With the advent of the fourth industrial revolution and the IoE era, industrial control systems (ICSs) are transitioning into industrial IoE (IIoE). The ICS, which is no longer a closed network, suffers from various cybersecurity threats. Since the attack of Stuxnet in 2010, the malware used to attack critical infrastructure (CI) has become increasingly sophisticated. Furthermore, recent attacks have mimicked normal network traffic. Therefore, investigating intrusion detection systems is necessary to detect these advanced cyberattacks. However, detecting advanced attacks, such as false data injection, is difficult because the traditional detection methods focus only on the protocol header fields. Most studies have not considered low-performance field devices, which are vulnerable to threats. Therefore, we have classified the extended reference model RAMI 4.0, which is the new ICS reference model in the fourth industrial revolution, into two levels: an operative level and a product process management level. A fast and lightweight algorithm is required at the operative level because the low-performance devices communicate with each other in real time. In addition, efficient data processing is important because considerable amount of data is concentrated at the product process management level. Based on these characteristics, a payload-based abnormal behavior detection method, i.e., the autoencoder-based payload anomaly detection (APAD), is proposed for each level. APAD uses an autoencoder to distinguish between normal and abnormal behaviors in low-performance devices. Furthermore, traffic analysis and a considerable amount of time are required to apply a traditional detection method at the product process management level. However, the proposed method does not require long-time traffic analysis. It exhibits a higher detection rate compared with those exhibited by other methods based on verification using the open secure water treatment dataset called SWaT. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106017	10.1016/j.asoc.2019.106017													
J								Unified model for interpreting multi-view echocardiographic sequences without temporal information	APPLIED SOFT COMPUTING										Unified model; Dense pyramid; Deep supervision; Multi-view; Echocardiographic sequence; Multi-vendor; Multi-center; Without temporal information	LEFT-VENTRICLE; SEGMENTATION; TRACKING; QUANTIFICATION	The robust and fully automatic interpretation of multi-view echocardiographic sequences across multi-vendor and multi-center is a challenging task due to abounding artifacts, low signal-to-noise ratio, large shape variations among different views, and large gaps across different centers and vendors. In this paper, a dense pyramid and deep supervision network (DPSN) is proposed to tackle this challenging task. DPSN incorporates the advantages of the densely connected network, feature pyramid network, and deeply supervised network, which help to extract and fuse multi-level and multi-scale holistic semantic information. This capability endows DPSN with prominent generalization and robustness, enabling it to yield a precise interpretation. To reduce the computational complexity and avoid the frequent information loss in temporal modeling, DPSN processes all frames independently (i.e., without utilizing temporal information) but can still obtain stable and coherent performance in the sequence. Adequate experiments on the heterogeneous (multi-view, multi-center, and multi-vendor) dataset (10858 labeled images) corroborate that DPSN achieves not only superior segmentation results but also prominent computational efficiency and stable performance. Estimation of the ejection fraction also shows good clinical correlation, revealing the clinical potential of DPSN. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106049	10.1016/j.asoc.2019.106049													
J								Immune/Neural approach to characterize salivary gland neoplasms (SGN)	APPLIED SOFT COMPUTING										Bioinformatics; Salivary gland neoplasm; Immune/Neural approach	SQUAMOUS-CELL CARCINOMA; KIT PROTEIN; GENE-EXPRESSION; CLASSIFICATION; OVEREXPRESSION; BIOINFORMATICS; HER-2/NEU; CYTOSCAPE; NETWORKS; RECEPTOR	The purpose of the current study was to use immune-inspired algorithm ClonALG whose performance is increased by using the Kohonen neural network training algorithm (Immune/Neural approach), to characterize the nature of salivary gland neoplasms (SGNs). The leader gene approach in order to identify biomarkers for SGNs. Extensive data were obtained for each of the 35 types of neoplasms. The gene leaders for each type of SGN were identified in a table and then divided according to the two different methods: K-means clustering and Immune/Neural approach. Genes related to SGNs were identified using PubMed, OMIM and Genecards databases. A bioinformatics algorithm was then applied, and the STRING database was employed to build networks of protein-protein interactions for each nature of an SGNs. The weighted number of links (WNL) and total interactions score (TIS) values were then obtained. Finally, the genes were clustered, and the gene leaders were identified using the K-means clustering method and the Immune/Neural approach. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105877	10.1016/j.asoc.2019.105877													
J								Fuzzy forecasting for long-term time series based on time-variant fuzzy information granules	APPLIED SOFT COMPUTING										Fuzzy information granules; Long-term forecasting; Recurrent fuzzy neural networks; Time series	MODEL; OPTIMIZATION; PREDICTION; ALGORITHM; ENSEMBLE; SYSTEMS	As to the long-term time series forecasting, it is more challenging and practical to obtain the trend information and fluctuation range of sequence data than single-step prediction values. In this article, by means of fuzzy information granules (FIGs) and recurrent fuzzy neural networks, a novel long-term prediction model for time series is proposed. Based on a variable-length division method, generalized zonary time-variant fuzzy information granule (GZT-FIG) is constructed, which can express the variation trend, fluctuation range and dispersion degree of sequence data. Furthermore, in order to improve the anti-noise ability and memory ability, type-2 fuzzy sets and long short-term memory mechanism are introduced into the prediction scheme, based on which a self-evolving interval type-2 LSTM fuzzy neural network (eIT2FNN-LSTM) is provided. Compared with the existing works related to fuzzy-neural models, the involvement of long short-term memory mechanism effectively improves the memory ability to achieve the long-term prediction. In order to verify the validation and effectiveness of the proposed scheme, several groups of experiments, including synthetic sequences and real-life time series, are carried out. The experimental results reveal the better predictive performance and rich semantic information. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106046	10.1016/j.asoc.2019.106046													
J								Side-Blotched Lizard Algorithm: A polymorphic population approach	APPLIED SOFT COMPUTING										Optimization; Metaheuristics; Bio-inspired computation	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; FIREFLY ALGORITHM; GOLDEN RATIO; DESIGN; COLONY; METAHEURISTICS; INTELLIGENCE; EXPLORATION; PERFORMANCE	In metaheuristic algorithms, finding the optimal balance between exploration and exploitation is a key research topic that remains open. In the nature, a reptile called Side-Blotched Lizard has achieved an interesting dynamic balance over its population. Such lizards evolved with three morphs associated with distinctive mating strategies. The synergy between the morphs generates a polymorphic population, able to self-balance the subpopulations of each morph without depleting the weakest morph. This equilibrium is achieved as the most common morph becomes the weakest, and the smaller subpopulations increase their chances of mating. In this paper, the Side-Blotched Lizard Algorithm (SBLA) is proposed to emulate the polymorphic population of the lizard. For this purpose, three operators are used to guarantee a dynamic over the population that allows the coexistence of multiple morphs. From the computational point, SBLA uses a subpopulation managing strategy which emulates the sinusoidal distribution of the lizard population over time. Even more, the mating behavior of each morph is modeled with three concepts, defensive, expansive, and sneaky. The performance of SBLA is tested on a set of five unimodal, eighteen multimodal, four composite benchmark functions, and engineering problems like; the welded beam, FM synthesizer, and rolling element bearing. To validate the results, we compared them to ten well-established algorithms and using the Wilcoxon test and the Bonferroni correction. The examination of the experimental results exhibits the accuracy, robustness and unique problem-solving method of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106039	10.1016/j.asoc.2019.106039													
J								Application of neural network and weighted improved PSO for uncertainty modeling and optimal allocating of renewable energies along with battery energy storage	APPLIED SOFT COMPUTING										Green energy provider (GEP); Battery energy compensatory system (BECS); Dynamic artificial neural network (DANN); Weighted improved particle swarm optimization (WIPSO); Reliability	PARTICLE SWARM OPTIMIZATION; POWER-GENERATION; CONTROL STRATEGIES; DISPATCH PROBLEMS; FREQUENCY CONTROL; LEVELIZED COST; WIND POWER; SYSTEMS; ELECTRICITY; ALGORITHM	The power uncertainty of renewable energy encounters their optimal allocation with obstacle in an on-grid microgrids. Hence, power prediction is a logical tactic. Besides, optimal performance of battery energy storage for compensating electrical loads is another strategy. However, these two solutions must meet the economic expectations as well as reliability of customers. In this paper, generating section includes wind, solar and wave energies in which wind speed and solar irradiance are uncertain parameters and information of Hormoz Island, Iran is the input of problem. Hence, artificial neural network (DANN) is trained by three adaptive techniques to minimize the prediction error dynamically. Then, eco-statistic objective function, reliability criterion and efficient battery strategy are modeled thoroughly. After introducing five feasible scenarios, a heuristic algorithm which is weighted improved PSO, searches the global answers of optimization to test the efficiency of hybrid architecture. Moreover, forecasting results are compared with five adaptive neural network based fuzzy inference system (ANFIS) and one conventional Levenberg Marquardt (LM) based ANN for addressing the functionality of proposed DANN against the uncertainty of renewables. Consequently, simulation results of optimization are tested with heuristic algorithms to demonstrate the optimal answers of problem. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105979	10.1016/j.asoc.2019.105979													
J								A novel fractional-order fuzzy control method based on immersion and invariance approach	APPLIED SOFT COMPUTING										Immersion and invariance; Non-singleton type-2 fuzzy neural network; Robust control; Fractional-order chaotic systems	CHAOTIC SYSTEMS; INPUT NONLINEARITIES; TYPE-2; SYNCHRONIZATION; OPTIMIZATION; SUBJECT; STABILIZATION; WEIGHTS; MODEL	In most of industrial applications, the dynamics of the system in hand are perturbed by a number of operational conditions. Also the outputs of the sensors always include noise. To alleviate these common problems, this paper presents a novel fuzzy control approach based on the immersion and invariance (I&I) approach under the conditions of unknown dynamics and measurement errors. The adaptation laws for the parameters of the proposed non-singleton type-2 fuzzy neural network (NT2FNN) are derived through a stability analysis based on I&I method. The effectiveness of the proposed membership function (MF) and non-singleton fuzzification is verified by comparison with the conventional Gaussian MF in the presence of measurement errors. The performance of the proposed control method is compared with other techniques and an experimental study is provided to show the capability of the proposed control scheme in real-time applications. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106043	10.1016/j.asoc.2019.106043													
J								Maintaining filter structure: A Gabor-based convolutional neural network for image analysis	APPLIED SOFT COMPUTING										Convolutional neural networks; Image segmentation; Deep learning; Gabor filter	LEFT-VENTRICLE; CARDIAC MR; SEGMENTATION; EFFICIENT; DESIGN	In image segmentation and classification tasks, utilizing filters based on the target object improves performance and requires less training data. We use the Gabor filter as initialization to gain more discriminative power. Considering the mechanism of the error backpropagation procedure to learn the data, after a few updates, filters will lose their initial structure. In this paper, we modify the updating rule in Gradient Descent to maintain the properties of Gabor filters. We use the Left Ventricle (LV) segmentation task and handwritten digit classification task to evaluate our proposed method. We compare Gabor initialization with random initialization and transfer learning initialization using convolutional autoencoders and convolutional networks. We experimented with noisy data and we reduced the amount of training data to compare how different methods of initialization can deal with these matters. The results show that the pixel predictions for the segmentation task are highly correlated with the ground truth. In the classification task, in addition to Gabor and random initialization, we initialized the network using pre-trained weights obtained from a convolutional Autoencoder using two different data sets and pre-trained weights obtained from a convolutional neural network. The experiments confirm the out-performance of Gabor filters comparing to the other initialization method even when using noisy inputs and a lesser amount of training data. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105960	10.1016/j.asoc.2019.105960													
J								Non-parametric predictive inference for solving multi-label classification	APPLIED SOFT COMPUTING										Multi-Label Classification; Multi-Label Decision Tree; NPI-M; Multi-Label Credal Decision Tree; Noise	DECISION TREES; IMPRECISE PROBABILITIES; CREDAL-C4.5; CLASSIFIERS; ENSEMBLES; DATASETS	Decision Trees (DTs) have been adapted to Multi-Label Classification (MLC). These adaptations are known as Multi-Label Decision Trees (ML-DT). In this research, a new ML-DT based on the Nonparametric Predictive Inference Model on Multinomial data (NPI-M) is proposed. The NPI-M is an imprecise probabilities model that provides good results when it is applied to DTs in standard classification. Unlike other models based on imprecise probabilities, the NPI-M is a nonparametric approach and it does not make unjustified assumptions before observing data. It is shown that the new ML-DT based on the NPI-M is more robust to noise than the ML-DT based on precise probabilities. As the intrinsic noise in MLC might be higher than in traditional classification, it is expected that the new ML-DT based on the NPI-M outperforms the already existing ML-DT. This fact is validated with an exhaustive experimentation carried out in this work on different MLC datasets with several levels of added noise. In it, many MLC evaluation metrics are employed in order to measure the performance of the algorithms. The experimental analysis shows that the proposed ML-DT based on NPI-M obtains better results than the ML-DT that uses precise probabilities, especially when we work on data with noise. (C) 2019 Published by Elsevier B.V.																	1568-4946	1872-9681				MAR	2020	88								106011	10.1016/j.asoc.2019.106011													
J								Gradient Immune-based Sparse Signal Reconstruction Algorithm for Compressive Sensing	APPLIED SOFT COMPUTING										Compressive sensing; Sparsity problem; Immune evolutionary algorithm; Multi-objective optimization; Sparse reconstruction	EVOLUTIONARY ALGORITHM; DECOMPOSITION; RECOVERY	The reconstruction aspect is the main core of the compressive sensing theory, in which the sparse signal is reconstructed from an incomplete set of random measurements. The constraint of spare signal reconstruction is the minimization of l(0)-norm, especially under noise condition. Thus, this paper proposes a new method called Gradient Immune-based Sparse Signal Reconstruction Algorithm for Compressive Sensing (GISSRA-CS) to optimize the trade-off between the reconstruction error and the sparsity requirements. The principle of the GISSRA-CS method is embedding the Gradient Local Search (GLS) method in the evolutionary process of the Immune Algorithm (IA) for solving the sparsity problem. Here, the sparsity problem is formulated as a multi-objective problem (MOP) by combining l(0) and l(1)-norms of a solution and l(2)-norm of a residual error in the same criterion to optimize the trade-off between the sparsity requirements and the error. This MOP problem is solved in a several subproblems manner by assigning different weights for each subproblem to increase the population diversity. For a long-term sparse signal, the window method is used to divide it into multiple short signals to improve the performance and computational complexity of the proposed method. Mathematical analysis and simulation experiments are presented to validate the performance and complexity of the GISSRA-CS method. Results of different simulation scenarios based on the benchmark and simulated signals show that the GISSRA-CS method outperforms the other methods in recovering the sparse signals with a small reconstruction error from noiseless and noisy measurements. Furthermore, the convergence of GISSRA-CS is faster than the other evolutionary recovery methods, but it is slower than the traditional recovery methods. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106032	10.1016/j.asoc.2019.106032													
J								Application of Fuzzy Reference Ideal Method (FRIM) to the military advanced training aircraft selection	APPLIED SOFT COMPUTING										Advanced training aircraft; Multi-Criteria Decision Making (MCDM); Fuzzy logic; Analytic Hierarchy Process (AHP); Reference Ideal Method (RIM)	ANALYTIC HIERARCHY PROCESS; EVALUATING WEAPON SYSTEM; DECISION-MAKING; TOPSIS METHODS; AHP; STRATEGIES	When a nation needs to acquire a new military training aircraft for its Air Force, many factors must be taken into account. This requires a good command of conflicting factors, which can benefit from the domain of Multi-Criteria Decision Making (MCDM). However, some criteria involved in the assessment process are often imprecise or vague and the use of linguistic terms characterized by fuzzy numbers could be advisable. The aim of this research is thus to extract the best of a combination of Fuzzy MCDM approaches with the aim of solving a real decision problem of interest for the Spanish Air Force, specifically, the selection of the best military advanced training aircraft, based on a set of criteria of differing natures. This decision problem involves, on the one hand, quantitative or technical criteria (combat ceiling, operational speed, take-off race, etc.) and, on the other hand, qualitative criteria (maneuverability, ergonomics, etc.) based on the experience of a set of flight instructors of the 23rd Fighter and Attack Training Wing, collected via questionnaires. The Analytic Hierarchy Process (AHP) is applied to obtain the weights of the criteria, whereas the Reference Ideal Method (RIM) and its Fuzzy version (FRIM) are used to evaluate the alternatives based on a reference ideal alternative defined by the flight instructors mentioned above. As a result, the Italian Alenia Aermacchi M-346 Master aircraft was selected as the best option. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106061	10.1016/j.asoc.2020.106061													
J								A cooperative advertising collaboration policy in supply chain management under uncertain conditions	APPLIED SOFT COMPUTING										Supply chain management; Three-echelon supply chain; Collaboration policy; Advertisement cost; Fuzzy costs	PRODUCTION-INVENTORY MODEL; CARBON EMISSION; NETWORK DESIGN; COORDINATION; IMPERFECT; STRATEGY; COST; MANUFACTURERS; MULTIPRODUCT; SIMULATION	Many industries are facing big challenges to design supply chains in a way to maximize the profit and meet the heightened expectations of the customer. This new era entirely relies on the dynamic advantages of competition and the role played by the collaboration policy. A global economy and increasing demand have put a huge pressure on supply chain partners to build a collaboration policy based on price, order quantity, and advertising. Companies are adopting the idea of "shaking hands" to obtain more profit instead of taking risks through competition. Cooperative (co-op) advertising is a significant policy of centralized supply chain management (SCM) to boost the revenues generated by the supplier, manufacturer, and retailers. The uncertain costs associated with the supply chain management also create obstacles in economic analysis and feasibility. These uncertainties are associated with the basic costs of all supply chain partners, which are represented using a signed distance formula. This paper develops the concept of co-op advertising among the supplier, manufacturer, and retailers with a variable demand driven by selling price and advertising costs, where all basic costs are considered as fuzzy. The profit is optimized by considering variable cycle time, shipments, pricing and advertising costs for the decision support system of the supply chain management. The optimal results of the co-op advertisement ensured an increase in the revenue of whole supply chain. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105948	10.1016/j.asoc.2019.105948													
J								Fuzzy c-means clustering using Jeffreys-divergence based similarity measure	APPLIED SOFT COMPUTING										Jeffreys-divergence; Jeffreys-divergence based similarity measure; Fuzzy c-means; Jeffreys-fuzzy-c-means clustering	K-MEANS; CONVERGENCE	In clustering, similarity measure has been one of the major factors for discovering the natural grouping of a given dataset by identifying hidden patterns. To determine a suitable similarity measure is an open problem in clustering analysis for several years. The purpose of this study is to make known a divergence based similarity measure. The notion of the proposed similarity measure is derived from Jeffrey-divergence. Various features of the proposed similarity measure are explained. Afterwards we develop fuzzy c-means (FCM) by making use of the proposed similarity measure, which guarantees to converge to local minima. The various characteristics of the modified FCM algorithm are also addressed. Some well known real-world and synthetic datasets are considered for the experiments. In addition to that two remote sensing image datasets are also adopted in this work to illustrate the effectiveness of the proposed FCM over some existing methods. All the obtained results demonstrate that FCM with divergence based proposed similarity measure outperforms three latest FCM algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106016	10.1016/j.asoc.2019.106016													
J								A two-step method for damage identification in moment frame connections using support vector machine and differential evolution algorithm	APPLIED SOFT COMPUTING										Damage identification; Moment frame; Connection; Support vector machine; Optimization	OPTIMIZATION; NETWORKS; SYSTEM	The main aim of this study is to introduce a two-step method for damage identification in moment frame connections using a support vector machine (SVM) and differential evolution algorithm (DEA). In the first step, the potential location of damage in connections is determined through SVM leading to reducing the dimension of the search space. Then, the accurate location and precise amount of damage in connections are determined in the second step via DEA with a high speed. In order to simulate damage in connections, a moment frame is modeled through semi-rigid beam to column connections and the analytical model is used to randomly generate structures with damaged connections as data. Then, SVM is trained and tested using this data, to facilitate natural frequencies are considered as input data and the characteristics of damage in beam to column connections are considered as output data of the network. Now, the possible location of the damage in connections can be determined using the SVM trained. The accurate location and severity of damage are determined by DEA based on the prediction of SVM in the first step. In order to assess the efficiency of the proposed method, two numerical examples are considered with different damage cases and considering noise. A comparative study is also made to judge the performance of the method with that of a work available in the literature. The outcome shows the high efficiency of the proposed method to identify the location and severity of the damage in moment frame connections. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106008	10.1016/j.asoc.2019.106008													
J								Cooperative coevolution with an improved resource allocation for large-scale multi-objective software project scheduling	APPLIED SOFT COMPUTING										Cooperative coevolution; Software project scheduling; Large-scale; Multi-objective optimization; Mathematical modeling	EVOLUTIONARY ALGORITHM; OPTIMIZATION; DECOMPOSITION; SUPPORT; SYSTEM	The existing literature of search-based software project scheduling merely studied to schedule a small to medium-scale project in static scenarios, while little work has considered to schedule a large-scale software project with uncertainties. However, many real-world software projects involve a large number of tasks and employees. Meanwhile, they are confronted with uncertain environments. To tackle such problems, this paper constructs a mathematical model for the large-scale multi-objective software project scheduling problem, and proposes a cooperative coevolutionary multi-objective genetic algorithm to solve the established model. In our model, more practical features of human resources and tasks are captured in the context of large-scale projects than the previous studies. Two efficiency related objectives of duration and cost are considered together with robustness to uncertainties and employees' satisfaction to allocations subject to various realistic constraints. Three novel strategies are incorporated in the proposed algorithm, which include the problem feature-based variable decomposition method, the improved computational resource allocation mechanism and the problem-specific subcomponent optimizer. To evaluate the performance of the proposed algorithm, empirical experiments have been performed on 15 randomly generated large-scale software project scheduling instances with up to 2048 decision variables, and three instances derived from real-world software projects. Experimental results indicate that on most of the 15 random instances and three real-world instances, the proposed algorithm achieves significantly better convergence performance than several state-of-the-art evolutionary algorithms, while maintaining a set of well-distributed solutions. Thus, it can be concluded that the proposed algorithm has a promising scalability to decision variables on software project scheduling problems. We also demonstrate how different compromises among the four objectives can offer software managers a deeper insight into various trade-offs among many objectives, and enabling them to make an informed decision. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106059	10.1016/j.asoc.2019.106059													
J								IBGSS: An Improved Binary Gravitational Search Algorithm based search strategy for QoS and ranking prediction in cloud environments	APPLIED SOFT COMPUTING										Cloud computing; QoS prediction; Ranking prediction; Cosine similarity; Binary gravitational search algorithm	PROBABILISTIC NEURAL-NETWORK; FEATURE-SELECTION; ROUGH SET; SERVICE; HYPERGRAPH; OPTIMIZATION; NEIGHBORHOOD; MODEL	Quality of Service (QoS) value prediction and QoS ranking prediction have their significance in optimal service selection and service composition problems. QoS based service ranking prediction is an NP-Complete problem which examines the order of ranked service sequence with respect to the unique QoS requirements. To address the NP-Complete problem, greedy and optimization-based strategies such as CloudRank and PSO have been widely employed in service oriented environments. However, they pose several challenges with respect to the similarity measure based QoS prediction, trap at local optima, and near optimal solution. Hence, this paper presents Improved Binary Gravitational Search Strategy (IBGSS), an optimization based search strategy to address the challenges in the state-of-the-art QoS value prediction and service ranking prediction techniques. IBGSS employs improved cosine similarity measure, and Newton-Raphson inspired Binary Gravitational Search Algorithm (NR-BGSA) for accurate QoS value prediction and optimal service ranking prediction respectively. The effectiveness of IBGSS over the state-of-the-art QoS value prediction and ranking prediction techniques was validated using two real world QoS datasets, namely WSDream#1 and web service QoS dataset in terms of various statistical measures (Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Average Precision Correlation (APC)). (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105945	10.1016/j.asoc.2019.105945													
J								Distribution linguistic preference relations with incomplete symbolic proportions for group decision making	APPLIED SOFT COMPUTING										Distribution linguistic preference relation (DLPR); Numerical scale model; Consistency; Numerical scale computation model; Group decision making (GDM)	TERM SETS; DISTRIBUTION ASSESSMENTS; REPRESENTATION MODEL; NUMERICAL SCALE; CONSENSUS MODEL; INFORMATION; CONSISTENCY; METHODOLOGY; 2-TUPLES; HIERARCHY	Distribution linguistic preference relations (DLPRs) with complete symbolic proportions have been recently investigated to record the comparison information coming from decision makers (DMs) in the context of linguistic decisions. Due to various reasons such as a lack of experience and partial knowledge about the pairs of decision alternatives, it is not always easy for DMs to provide complete symbolic proportions in DLPRs. In this paper, we propose a new style of pairwise comparison called DLPR with incomplete symbolic proportions to represent DMs' comparison information. Two aggregation operators for DLPRs with incomplete symbolic proportions and their desirable properties are presented. An expectation-based numerical preference relation (EBNPR) is deduced from a DLPR with incomplete symbolic proportions using numerical scale models. The consistency of DLPR with incomplete symbolic proportions is defined via its associated EBNPR. On the other hand, solving linguistic decision problems implies the need for invoking the principles of computing with words (CW). The key point about CW is that words might exhibit different meaning for different people. Hence, another aim of this paper is to deal with the point about CW by setting personalized numerical scales of linguistic terms for different DMs in group decision making (GDM) with the newly introduced preference relations. Several numerical scale computation models are developed to personalize numerical scales for each DM to show their individual difference in understanding the meaning of words. Finally, we present the applications of the aforesaid theoretical results to GDM situations, which are demonstrated by solving a GDM problem of evaluating and selecting research projects. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106005	10.1016/j.asoc.2019.106005													
J								A three-dimensional group search optimization approach for simultaneous planning of distributed generation units and distribution network reconfiguration	APPLIED SOFT COMPUTING										Three-dimensional group search optimization (3D-GSO); Distribution network reconfiguration (DNR); DG placement	BIG CRUNCH ALGORITHM; OPTIMAL PLACEMENT; MULTIOBJECTIVE APPROACH; DISTRIBUTION-SYSTEM; POWER ALLOCATION; LOSS REDUCTION; DG ALLOCATION; MINIMIZATION; INTEGRATION; CAPACITORS	This paper deals with the simultaneous distributed generation (DG) planning and distribution network reconfiguration (SDGNR) issue. The problem is formulized as an optimization model which includes three types of variables, i.e. DGs location as the integer variables, DGs operating point as the continuous ones and switches open\close state as the binary variables. A new approach entitled three-dimensional group search optimization (3D-GSO) method is also introduced to cope with such a problem. The proposed method is a general optimization scheme applicable to all types of optimization problems which deal with an integer, continuous, and binary variables at the same time. The revised approach is successfully applied to the SDGNR problem with the objective of total loss reduction in power distribution systems. Power flow criteria, as well as operation constraints, are all together accommodated in the process of optimization. Five different scenarios at three load levels are also considered to cover all possible conditions. The validity of the proposed 3D-GSO approach in handling SDGNR problem is assured through comprehensive simulation studies on 33-bus and 69-bus test systems. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106012	10.1016/j.asoc.2019.106012													
J								Quantifying reusability of software components using hybrid fuzzy analytical hierarchy process (FAHP)-Metrics approach	APPLIED SOFT COMPUTING										Fuzzy analytical hierarchy process (FAHP); Quality metric; Reusability estimation; Software component; Multi-criteria decision making (MCDM)	DECISION-MAKING PROBLEMS; QUALITY EVALUATION; REUSE; MODELS; AHP; PRODUCTIVITY; PERFORMANCE; FRAMEWORK; SELECTION; INDUSTRY	The role of reusability cannot be neglected in the software component selection since it determines the worth of a component for its potential (re)use. The reusability is a qualitative feature and it is arduous to measure it directly. Typically, it is assessed subjectively without paying much heed to the involvement of befitting stakeholders and the use of a quantitative approach. We aim to quantify the reusability of a software component based on its quality through multi-criteria decision making (MCDM) solution to rank alternatives. To do this, the quality preferences of stakeholders about the reuse of components are determined. The reusability is quantified using a hybrid fuzzy analytic hierarchy process (FAHP) and quality metrics approach. In this approach, the weights of both FAHP and quality metrics are integrated to get the final ranking. FAHP-Metrics approach is applied to payment gateway (software) components, and the ranking of given components is obtained based on their reusability value. Finally, the comparative analysis of our results with respect to other recent studies reveals that the Spearman rank correlation result is highly significant and acceptable, and the obtained weights have a good association with other studies. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105997	10.1016/j.asoc.2019.105997													
J								A Quasi-Oppositional-Chaotic Symbiotic Organisms Search algorithm for optimal allocation of DG in radial distribution networks	APPLIED SOFT COMPUTING										Symbiotic organisms search; Power loss reduction; Distributed generation; Optimal power factor; Optimal placement; Voltage profile; Quasi oppositional	PARTICLE SWARM OPTIMIZATION; OPTIMAL PLACEMENT; GENERATION ALLOCATION; DISTRIBUTION-SYSTEMS; DIFFERENTIAL EVOLUTION; OPTIMAL LOCATION; RECONFIGURATION; OBJECTIVES; CAPACITY; MODEL	This paper aims to apply an improved meta-heuristic method to optimize the allocation of distributed generation (DG) units in radial distribution networks (RDNs). The proposed method, namely the Quasi-Oppositional Chaotic Symbiotic Organisms Search (QOCSOS) algorithm, is the improved version of the original SOS algorithm. QOCSOS incorporates the Quasi-Opposition-Based Learning (QOBL) and Chaotic Local Search (CLS) strategies into SOS to improve the global search capacity. In this study, the objective of the optimal DG allocation (OGDA) problem is to optimally reduce the real power loss, improve the voltage profile, and increase the voltage stability in RDNs. The proposed QOCSOS algorithm was applied to find the optimal locations and sizes of DG units with different DG power factors (unity and non-unity) in the RDNs including 33, 69, and 118-bus. It was found that the operation of DG units with optimal power factor significantly improved the performance of RDNs in terms of voltage deviation minimization, and voltage stability maximization, especially for power loss reduction. After the DG integration, for the case of DG units operating with unity power factor, the power loss reduction was reduced by 65.50%, 69.14%, and 60.23% for the 33, 69, and 118-bus RDNs, respectively. In addition, it should be emphasized that for the cases of DG units operating with optimal power factor, the power loss reduction was reduced up to 94.44%, 98.10%, and 90.28% for these RDNs, respectively. The obtained results from QOCSOS were evaluated by comparing to those from SOS and other optimization methods in the literature. The results showed that the proposed QOCSOS method performed greater than SOS, and offered better quality solutions than many other compared methods, suggesting the feasibility of QOCSOS in solving the ODGA problem, especially for a complex and large-scale system. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106067	10.1016/j.asoc.2020.106067													
J								Optimizing hyperparameters of deep learning in predicting bus passengers based on simulated annealing	APPLIED SOFT COMPUTING										Bus transportation system; Simulated annealing; Deep learning; Hyperparameter optimization	NETWORKING; MODEL	Bus is certainly one of the most widely used public transportation systems in a modern city because it provides an inexpensive solution to public transportation users, such as commuters and tourists. Most people would like to avoid taking a crowded bus on the way. That is why forecasting the number of bus passengers has been a critical problem for years. The proposed method is inspired by the fact that there is no easy way to know the suitable parameters for most of the deep learning methods in solving the optimization problem of forecasting the number of passengers on a bus. To address this issue, the proposed algorithm uses a simulated annealing (SA) to find out a suitable number of neurons for each layer of a fully connected deep neural network (DNN) to enhance the accuracy rate in solving this particular optimization problem. The proposed method is compared with support vector machine, random forest, eXtreme gradient boosting, deep neural network, and deep neural network with dropout for the data provided by the Taichung city smart transportation big data research center, Taiwan (TSTBDRC). Our simulation results indicate that the proposed method outperforms all the other forecasting methods for forecasting the number of bus passengers in terms of the accuracy rate and the prediction time. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106068	10.1016/j.asoc.2020.106068													
J								Surrogate-based optimisation using adaptively scaled radial basis functions	APPLIED SOFT COMPUTING										Evolutionary algorithm; Black box optimisation; Bayesian optimisation; Benchmarking; Optimisation; Global optimisation; Gradient-free; Aerodynamics; Surrogate model; Proper Orthogonal Decomposition; Radial Basis Function interpolation; Latin Hypercube Sampling	NEURAL-NETWORK; DECOMPOSITION; ALGORITHM; DESIGN	Aerodynamic shape optimisation is widely used in several applications, such as road vehicles, aircraft and trains. This paper investigates the performance of two surrogate-based optimisation methods; a Proper Orthogonal Decomposition-based method and a force-based surrogate model. The generic passenger vehicle DrivAer is used as a test case where the predictive capability of the surrogate in terms of aerodynamic drag is presented. The Proper Orthogonal Decomposition-based method uses simulation results from topologically different meshes by interpolating all solutions to a common mesh for which the decomposition is calculated. Both the Proper Orthogonal Decomposition- and force-based approaches make use of Radial Basis Function interpolation. The Radial Basis Function hyperparameters are optimised using differential evolution. Additionally, the axis scaling is treated as a hyperparameter, which reduces the interpolation error by more than 50% for the investigated test case. It is shown that the force-based approach performs better than the Proper Orthogonal Decomposition method, especially at low sample counts, both with and without adaptive scaling. The sample points, from which the surrogate model is built, are determined using an optimised Latin Hypercube sampling plan. The Latin Hypercube sampling plan is extended to include both continuous and categorical values, which further improve the surrogate's predictive capability when categorical design parameters, such as on/off parameters, are included in the design space. The performance of the force-based surrogate model is compared with four other gradient-free optimisation techniques: Random Sample, Differential Evolution, Nelder-Mead and Bayesian Optimisation. The surrogate model performed as good as, or better than these algorithms, for 17 out of the 18 investigated benchmark problems. (C) 2020 The Authors. Published by Elsevier B.V.																	1568-4946	1872-9681				MAR	2020	88								106050	10.1016/j.asoc.2019.106050													
J								A multi-population differential evolution algorithm based on cellular learning automata and evolutionary context information for optimization in dynamic environments	APPLIED SOFT COMPUTING										Differential evolution; Dynamic optimization problems; Hyper-heuristics; Cellular learning automata; Learning automata	PARTICLE SWARM OPTIMIZER; MEMETIC ALGORITHM; SEARCH; FRAMEWORK; ENSEMBLE; MEMORY	This paper presents a multi-population differential evolution algorithm to address dynamic optimization problems. In the proposed approach, a cellular learning automaton adjusts the behavior of each subpopulation by adaptively controlling its updating schemes. As the environment changes over time, an evolving population may go through a quite number of state transitions. Each state demands specific characteristics from an optimizer; hence, an adapted evolutionary scheme for one state may be unsuitable for the upcoming ones. Additionally, a learning approach may have limited time to adapt to a newly encountered state due to the frequentness of the environmental changes. Hence, it is infeasible for a dynamic optimizer to unlearn its existing beliefs to accommodate the practices required to embrace newly encountered states. In order to address this issue, we introduce a context dependent learning approach, which can adapt the behavior of each subpopulation according to the contexts of its different states. The performance of the proposed approach is compared with several state-of-the-art dynamic optimizers over the GDBG benchmark set. Comparison results indicate that the proposed method can achieve statistically superior performances on a wide range of tested instances. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106009	10.1016/j.asoc.2019.106009													
J								Identifying good algorithm parameters in evolutionary multi- and many-objective optimisation: A visualisation approach	APPLIED SOFT COMPUTING										Benchmarking; Parametrisation; Visualisation; Multi-objective; Many-objective; Optimisation	GENETIC ALGORITHM	Evolutionary algorithms are often highly dependent on the correct setting of their parameters, and benchmarking different parametrisations allows a user to identify which parameters offer the best performance on their given problem. Visualisation offers a way of presenting the results of such benchmarking so that a non-expert user can understand how their algorithm is performing. By examining the characteristics of their algorithm, such as convergence and diversity, the user can learn how effective their chosen algorithm parametrisation is. This paper presents a technique intended to offer this insight, by presenting the relative performance of a set of EAs optimising the same multi-objective problem in a simple visualisation. The visualisation characterises the behaviour of the algorithm in terms of known performance indicators drawn from the literature, and is capable of visualising the optimisation of many-objective problems also. The method is demonstrated with benchmark test problems from the popular DTLZ and CEC 2009 problem suites, optimising them with different parametrisations of both NSGA-II and NSGA-III, and it is shown that known characteristics of optimisers solving these problems can be observed in the visualisations resulting. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105902	10.1016/j.asoc.2019.105902													
J								Chaotic multi-swarm whale optimizer boosted support vector machine for medical diagnosis	APPLIED SOFT COMPUTING										Whale optimization algorithm; Multi-swarm; Support vector machine; Model selection; Medical diagnosis	ALGORITHM; SELECTION; CLASSIFICATION	Support vector machine (SVM) is a widely used pattern classification method that its classification accuracy is greatly influenced by both kernel parameter setting and feature selection. Therefore, in this study, to perform parameter optimization and feature selection simultaneously for SVM, we propose an improved whale optimization algorithm (CMWOA), which combines chaotic and multi-swarm strategies. Using several well-known medical diagnosis problems of breast cancer, diabetes, and erythemato-squamous, the proposed SVM model, termed CMWOAFS-SVM, was compared with multiple competitive SVM models based on other optimization algorithms including the original algorithm, particle swarm optimization, bacterial foraging optimization, and genetic algorithms. The experimental results demonstrate that CMWOAFS-SVM significantly outperformed all the other competitors in terms of classification performance and feature subset size. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105946	10.1016/j.asoc.2019.105946													
J								Fibonacci multi-modal optimization algorithm in noisy environment	APPLIED SOFT COMPUTING										Noisy environment; Multi-modal optimization; Fibonacci; Global-local alternation; Probability distribution	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY OPTIMIZATION; MULTIOBJECTIVE OPTIMIZATION; LEARNING AUTOMATA; GOLDEN SECTION; SEARCH; IDENTIFICATION; SIGNALS	Noises are very common in practical optimization problems. It will cause interference on optimization algorithms and thus makes the algorithms difficult to find a true global extreme point and multiple local extreme points. For the problem, this paper proposes a Fibonacci multi-modal optimization (FMO) algorithm. Firstly, the proposed algorithm alternates between global search and local optimization in order not to fall into local optimum points and to retain multiple optimum points. And then, a Fibonacci regional scaling criterion is proposed in the FMO algorithm to alleviate the effects of noise, and the position of optimum point is determined according to its probability distribution under noise interference. In experiments, we evaluate the performance of the proposed FMO algorithm through 35 benchmark functions. The experimental results show that compared with Particle Swarm Optimization (PSO) algorithm, three improved versions of PSO, and Genetic algorithm (GA), the proposed FMO algorithm can gain more accurate location of optimum point and more global and local extreme points under noisy environment. Finally, an example of practical optimization in radio spectrum monitoring is used to show the performance of the FMO algorithm. (C) 2019 Published by Elsevier B.V.																	1568-4946	1872-9681				MAR	2020	88								105874	10.1016/j.asoc.2019.105874													
J								Multi-objective feature selection based on artificial bee colony: An acceleration approach with variable sample size	APPLIED SOFT COMPUTING										Artificial bee colony; Feature selection; Multi-objective optimization; Variable sample size	PARTICLE SWARM OPTIMIZATION; BRAIN STORM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; SEARCH	Due to the need to repeatedly call a classifier to evaluate individuals in the population, existing evolutionary feature selection algorithms have the disadvantage of high computational cost. In view of it, this paper studies a multi-objective feature selection framework based on sample reduction strategy and evolutionary algorithm, significantly reducing the computational cost of algorithm without affecting optimal results. In the framework, a selection strategy of representative samples, called K-means clustering based differential selection, and a ladder-like sample utilization strategy are proposed to reduce the size of samples used in the evolutionary process. Moreover, a fast multi-objective evolutionary feature selection algorithm, called FMABC-FS, is proposed by embedding an improved artificial bee colony algorithm based on the particle update model into the framework. By applying FMABC-FS to several typical UCI datasets, and comparing with three multi-objective feature selection algorithms, experimental results show that the proposed variable sample size strategy is more suitable to FMABC-FS, and FMABC-FS can obtain better feature subsets with much less running time than those comparison algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106041	10.1016/j.asoc.2019.106041													
J								A two phase removing algorithm for minimum independent dominating set problem	APPLIED SOFT COMPUTING										Minimum independent dominating set problem; Tabu search; Local search; Double-checked removing strategy; Random diversity removing strategy	LOCAL SEARCH ALGORITHM; OPTIMIZATION	The minimum independent dominating set (MIDS) problem is a famous combinatorial optimization problem and is widely used in real-world domains. In this paper, we design a novel local search algorithm with tabu method and two phase removing strategies including double-checked removing strategy and random diversity removing strategy to solve the MIDS problem. The first removing strategy checks and then removes the second-level neighbourhood of the just removal vertex to break the limitation of the independence property. When the quality of candidate solution has not been improved after some steps, the second removing strategy dynamically and greedily removes lots of vertices so that the current candidate solution can escape from suboptimal search space, and then we introduce the random walk into the repair process. Experiments are carried out on two classical benchmarks named DIMACS and BHOSLIB, and the results show that the proposed algorithm significantly outperforms the previous state-of-the-art MIDS heuristic algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105949	10.1016/j.asoc.2019.105949													
J								A new global best guided artificial bee colony algorithm with application in robot path planning	APPLIED SOFT COMPUTING										Artificial bee colony; Global best leading; Dimension-independent; Coevolution; Robot path planning	OPTIMIZATION; STRATEGY	Artificial bee colony has received much attention in recent years as a competitive population-based optimization algorithm. However, its slow convergence speed and one-dimensional search strategy limit it from demonstrating advantage in separable functions. To address these concerning issues, this paper introduces a coevolution framework into ABC and designs a global best leading artificial bee colony algorithm with an improved strategy to accelerate its convergence and conquer the dependency of dimension separately. A set of classical and Congress on Evolutionary Computation 2015 benchmark functions are adopted for validating the efficiency of our algorithm. In addition, in order to show the practicality of our algorithm, a robot path-planning problem is tested, and our algorithm still achieves superior results. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106037	10.1016/j.asoc.2019.106037													
J								Self-adaptive parameter and strategy based particle swarm optimization for large-scale feature selection problems with multiple classifiers	APPLIED SOFT COMPUTING										Particle swarm optimization; Feature selection; Large-scale problems; Self-adaptive; Classification	DIFFERENTIAL EVOLUTION ALGORITHM; GENETIC ALGORITHM; CLASSIFICATION; ENSEMBLE; INFORMATION; MACHINE; SCHEME	Feature selection has been widely used in classification for improving classification accuracy and reducing computational complexity. Recently, evolutionary computation (EC) has become an important approach for solving feature selection problems. However, firstly, as the datasets processed by classifiers become increasingly large and complex, more and more irrelevant and redundant features may exist and there may be more local optima in the large-scale feature space. Therefore, traditional EC algorithms which have only one candidate solution generation strategy (CSGS) with fixed parameter values may not perform well in searching for the optimal feature subsets for large-scale feature selection problems. Secondly, many existing studies usually use only one classifier to evaluate feature subsets. To show the effectiveness of evolutionary algorithms for feature selection problems, more classifiers should be tested. Thus, in order to efficiently solve large-scale feature selection problems and to show whether the EC-based feature selection method is efficient for more classifiers, a self-adaptive parameter and strategy based particle swarm optimization (SPS-PSO) algorithm is proposed in this paper using multiple classifiers. In SPS-PSO, a representation scheme of solutions and five CSGSs have been used. To automatically adjust the CSGSs and their parameter values during the evolutionary process, a strategy self-adaptive mechanism and a parameter self-adaptive mechanism are employed in the framework of particle swarm optimization (PSO). By using the self-adaptive mechanisms, the SPS-PSO can adjust both CSGSs and their parameter values when solving different large-scale feature selection problems. Therefore, SPS-PSO has good global and local search ability when dealing with these large-scale problems. Moreover, four classifiers, i.e., k-nearest neighbor (KNN), linear discriminant analysis (LDA), extreme learning machine (ELM), and support vector machine (SVM), are individually used as the evaluation functions for testing the effectiveness of feature subsets generated by SPS-PSO. Nine datasets from the UCI Machine Learning Repository and Causality Workbench are used in the experiments. All the nine datasets have more than 600 dimensions, and two of them have more than 5,000 dimensions. The experimental results show that the strategy and parameter self-adaptive mechanisms can improve the performance of the evolutionary algorithms, and that SPSPSO can achieve higher classification accuracy and obtain more concise solutions than those of the other algorithms on the large-scale feature problems selected in this research. In addition, feature selection can improve the classification accuracy and reduce computational time for various classifiers. Furthermore, KNN is a better surrogate model compared with the other classifiers used in these experiments. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106031	10.1016/j.asoc.2019.106031													
J								A novel system for multi-step electricity price forecasting for electricity market management	APPLIED SOFT COMPUTING										Deterministic forecasting; Interval forecasting; Electricity market; Electricity price; Multi-step forecasting	EXTREME LEARNING-MACHINE; FUZZY NEURAL-NETWORK; WIND-SPEED; MULTIOBJECTIVE OPTIMIZATION; HYBRID MODEL; STRATEGY; DECOMPOSITION; CONSUMPTION; SCHEME	Electricity price forecasting is an important and challenging issue for all participants in the power market because of the wide application of electricity in our society and its inherent features. In this context, some current forecasting systems use data preprocessing and optimization for theoretical and practical achievements. However, some limitations to these systems exist which need to be urgently solved. First, future information is overdrawn in the data preprocessing stage of these forecasting systems, which is actually unknown in practical applications. The crucial question, therefore, is how to develop a forecasting system without using any future information. Second, the complex features of original nonlinear and nonstationary electricity price have a negative influence on the generalization ability of these previously developed models. To decrease the negative effects on management, a method to develop a forecasting system to improve the model's generalization ability is required. Therefore, in this study, we developed an adaptive deterministic and probabilistic interval forecasting system for multi-step electricity price forecasting, which can present more valuable information to power market decision makers. Two cases and one comparative study are provided and analyzed to validate the performance of the developed system in multi-step electricity price forecasting. Furthermore, further discussions are presented to illustrate the significance of this study, thus proving that the results of the present study fill the present knowledge gap and provide some new future directions for related studies. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106029	10.1016/j.asoc.2019.106029													
J								Oil project selection in Iran: A hybrid MADM approach in an uncertain environment	APPLIED SOFT COMPUTING										Oil project selection; WASPAS; Best-worst method; Z-numbers; Delphi method	GAS PROJECTS; OVERSEAS OIL; SUPPLIER; AHP; METHODOLOGY; CONSUMPTION; TOPSIS; CHINA; TOOL	This research focusses on the selection of oil projects by using Multi-Attribute Decision Making (MADM) methods in an uncertain environment. Oil production plays a crucial role in the economy of Iran, a country with many opportunities for onshore oil exploration. Differently from other countries, however, oil producers in Iran face some constraints with respect to oil extraction because some areas are shared with other producers. Additionally, oil producers in Iran must also allocate scarce physical, human, and monetary resources among different projects. Inaccurate decision-making may not only yield sub-optimal revenue generation, but also adversely affect the national economy For these reasons, priority oil projects must observe a sequence of steps. In the first step, critical factors for selecting oil projects are collected from previous related studies and experts are interviewed. These factors are subsequently filtered using the Delphi method. The oil projects are then ranked using a comprehensive approach involving novel alternative MADM methods. The best-worst method (BWM) is a new MADM stream that relies on pairwise comparison. It presents several distinct advantages with respect to fewer computational steps and higher discriminatory power among alternatives. Differently from previous research, this paper couples BWM with Weighted Aggregated Sum-Product Assessment (WASPAS) to improve result sensitivity under uncertain decision-making environments as modelled by Z-numbers. A robustness cross-check against other MADM models is also presented. Results indicate that quality has the highest priority and that production technology has the lowest priority among ten factors for oil project selection, thus reflecting the impact of US sanctions on oil production in Iran. Managerial implications and future avenues of research are derived. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106066	10.1016/j.asoc.2020.106066													
J								Picture fuzzy normalized projection and extended VIKOR approach to software reliability assessment	APPLIED SOFT COMPUTING										Group decision-making; Normalization projection; Extended VIKOR method; Picture fuzzy number; Software reliability assessment	GROUP DECISION-MAKING; AGGREGATION OPERATORS; LOCATION SELECTION; SENTIMENT ANALYSIS; MODEL; INFORMATION; FRAMEWORK; MAKERS; MCDM; SETS	An extended Vlsekriterijumska optimizacija i KOmpromisno Resenje (VIKOR) method in group decision-making (GDM) setting is developed in this paper. The decision information is characterized by picture fuzzy number. A new GDM model is established and applied to software reliability assessment. First, this research finds that the current projection measure is not always reasonable in picture fuzzy setting. To solve this problem, a new normalization projection measure is developed in picture fuzzy setting. In addition, this research also finds that the most VIKOR-based GDM method requires a collective decision as a carrier to get an optimal solution. However, if a collective decision is aggregated by all the individual decisions, the positive and negative information might be canceled each other out in the decision process. To solve this problem, this paper proposes a direct VIKOR-based GDM method. The values of three VIKOR indexes are determined by the original decision matrices. The feasibility and practicability developed method in this work are illustrated by an example of software reliability assessment and some experimental analyses. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106056	10.1016/j.asoc.2019.106056													
J								Distance-based consensus reaching process for group decision making with intuitionistic multiplicative preference relations	APPLIED SOFT COMPUTING										Group decision-making; Intuitionistic multiplicative preference relation; Distance measure; Consensus measure; Project investment	FUZZY INFORMATION; MODEL	The intuitionistic multiplicative preference relation uses Saaty's 1/9-9 scale to depict people's opinions from the prior and not prior aspects. Due to its objectivity in representing people's cognition, the intuitionistic multiplicative preference relation has attracted extensive attentions of scholars. In this paper, we study the distance-based consensus measures in the context of group decision-making with intuitionistic multiplicative preference relations. First of all, some new distance measures between intuitionistic multiplicative numbers/sets are proposed, which contains the improved Hamming distance, the improved Euclidean distance, and their weighted forms. Then, we investigate their desirable properties. To aid the group decision-making process, we further develop a new consensus measures regarding intuitionistic multiplicative preference relations based on the proposed distance measures. Afterwards, a new group decision-making method is proposed to solve the complex group decisionmaking problems with intuitionistic multiplicative preference relations. Finally, an example concerning the project investment selection is given to demonstrate the proposed group decision-making method, and then we compare the proposed method with other existing group decision-making methods with intuitionistic multiplicative preference relations in detail. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106045	10.1016/j.asoc.2019.106045													
J								A priority-based intuitionistic multiplicative UTASTAR method and its application in low-carbon tourism destination selection	APPLIED SOFT COMPUTING										Multiple criteria decision making; Intuitionistic multiplicative preference relation; Intuitionistic multiplicative priority weight; UTASTAR method; Low-carbon tourism destination selection	GROUP DECISION-MAKING; PREFERENCE RELATIONS; UTILITY-FUNCTIONS; ALTERNATIVES; WEIGHTS	The intuitionistic multiplicative preference relation extends Saaty's multiplicative preference relation by membership and non-membership functions to describe decision-makers' preferences on objects (alternatives or criteria), and the product of pairwise membership and non-membership degrees is less than or equal to one. In recent years, the intuitionistic multiplicative preference relation has attracted increasing attention and many methods have been proposed to handle multiple criteria decision making (MCDM) problems. However, few of the existing methods can be used to tackle the MCDM problems with both quantitative and qualitative criteria. To overcome this challenge, in this paper, a novel MCDM method called the priority-based intuitionistic multiplicative UTASTAR method is proposed, which combines a new intuitionistic multiplicative prioritization method and the UTASTAR method within the context of intuitionistic multiplicative sets. The consistency checking and improving processes are considered in the weight-determining method. A case study regarding the low-carbon tourism destination selection is adopted to illustrate the applicability of the proposed method. The effectiveness and superiority of the proposed method are further verified by comparative analyses and discussions. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106026	10.1016/j.asoc.2019.106026													
J								Automatic vessel lumen segmentation in optical coherence tomography (OCT) images	APPLIED SOFT COMPUTING										Image segmentation; Gradient convolution field; Star prior; Optical coherence tomography; Graph cuts	GRAPH-CUTS; ENERGY MINIMIZATION; ACTIVE CONTOURS; MAXIMUM-FLOW; SHAPE PRIOR; QUANTIFICATION	This paper presents a graph based method to automatically and accurately segment the lumen borders from optical coherence tomography (OCT) images. The proposed method unravels the OCT images from the Cartesian coordinates to polar coordinates so that the segmentation is transferred into a height field delineation problem. In effect, the method imposes a simplistic star shape prior but without the bias towards narrower lumen size. The lumen border is identified as the solution to finding the minimum closed set on a node-weighed, directed graph. In order to cope with both the variability in imaging condition and different forms of image artefacts, we adopt an image feature that relies on very little assumption on the appearance of lumen border but is resilient to image noise and so on. This feature is derived from a convolution of the image gradient field and thus it takes into account gradient vector interactions at a much more global scale compared to conventional gradient based approaches. The proposed method is fully automatic without the need for an initialisation. We compare this method with a number of techniques, including both conventional methods and data driven models. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106042	10.1016/j.asoc.2019.106042													
J								Knowledge distilling based model compression and feature learning in fault diagnosis	APPLIED SOFT COMPUTING										Fault diagnosis; Neural networks; Knowledge distilling; Feature learning	DEEP NEURAL-NETWORKS; ROTATING MACHINERY	Recently, there has been interest in developing diagnosis methods that combine model-based and data-driven diagnosis. In both approaches, selecting the relevant measurements or extracting important features from historical data is a key determiner of the success of the algorithm. Recently, deep learning methods have been effective in automating the feature selection process. Autoencoders have been shown to be an effective neural network configuration for extracting features from complex data, however, they may also learn irrelevant features. In addition, end-to-end classification neural networks have also been used for diagnosis, but like autoencoders, this method may also learn unimportant features thus making the diagnostic inference scheme inefficient. To rapidly extract significant fault features, this paper employs end-to-end networks and develops a new feature extraction method based on importance analysis and knowledge distilling. First, a set of cumbersome neural network models are trained to predict faults and some of their internal values are defined as features. Then an occlusion-based importance analysis method is developed to select the most relevant input variables and learned features. Finally, a simple student neural network model is designed based on the previous analysis results and an improved knowledge distilling method is proposed to train the student model. Because of the way the cumbersome networks are trained, only fault features are learned, with the importance analysis further pruning the relevant feature set. These features can be rapidly generated by the student model. We discuss the algorithms, and then apply our method to two typical dynamic systems, a communication system and a 10-tank system employed to demonstrate the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								105958	10.1016/j.asoc.2019.105958													
J								Research on Circular Area Search algorithm of multi-robot service based on SOA cloud platform	APPLIED SOFT COMPUTING										Density of regional services; Circular Area Search; Multi-robot service scheduling; SOA multi-robot service cloud platform; Gaode Map; Screening optimal service		Considering autonomous mobile robots with a variety of specific functions as a kind of service, when there are many types and quantities of services and the density of regional services is large, proposing an algorithm of Circular Area Search (CAS) because of the problem of multi-robot service scheduling in various areas. Firstly, Django is used as the web framework to build the Service-Oriented Architecture (SOA) multi-robot service cloud platform, which is the basic platform for multi-service combination. Then, the service type, the latitude and longitude and the scoring parameters of the service are selected as the service search metrics to design the CAS algorithm that based on the existing service information registered in MySQL and the Gaode Map for screening optimal service, and then providing the service applicant with the best service. Finally, the service applicant applies for the self-driving tour service as an example to perform performance simulation test on the proposed CAS algorithm. The results show that the CAS algorithm of the multi-robot service cloud platform proposed in this paper is practical compared to the global search. And compared with the Greedy Algorithm experiment, the service search time is reduced about 58% compared with the Greedy Algorithm, which verifies the efficiency of CAS algorithm. (C) 2019 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				MAR	2020	88								105816	10.1016/j.asoc.2019.105816													
J								Laplacian least learning machine with dynamic updating for imbalanced classification	APPLIED SOFT COMPUTING										Least learning machine; Imbalanced classification; Laplacian matrix; Inverse matrix; Incremental learning of hidden nodes	EXTREME; REGRESSION; NETWORKS	When handling imbalanced datasets, the existing least learning machine ignores both the interrelationship between instances and the prior knowledge about the classes and hence tends to provide unfavorable accuracies across the classes. With the Laplacian matrix based loss function, Laplacian least learning machine ((LMM)-M-2) is developed to address the infeasibility of the existing least learning machine for imbalanced data classification in this study. In order to find out the optimal number of hidden nodes in (LMM)-M-2, by means of the derived update equations without any re-calculation of inverse matrix, its two incremental versions (i.e., with and without regularization) are invented for dynamically updating the hidden nodes in one-by-one way, thereby saving much training time. (LMM)-M-2 and its incremental versions inherit fast learning and good generalization capability of least learning machine. Experimental results on 19 benchmarking imbalanced datasets indicate the effectiveness of the proposed incremental (LMM)-M-2 for imbalanced classification tasks. (C) 2019 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106028	10.1016/j.asoc.2019.106028													
J								Stacked pruning sparse denoising autoencoder based intelligent fault diagnosis of rolling bearings	APPLIED SOFT COMPUTING										Stacked pruning sparse denoising autoencoder; Deep learning; Rolling bearing; Fault diagnosis; Pruning operation	MODE DECOMPOSITION; NETWORK	This paper proposes a new stacked pruning sparse denoising autoencoder (sPSDAE) model for intelligent fault diagnosis of rolling bearings. Different from the traditional autoencoder, the proposed sPSDAE model, including a fully connected autoencoder network, uses the superior features extracted in all the previous layers to participate in the subsequent layers. This means that some new channels are created to connect the front layers and the back layers, which reduces information loss. To improve the training efficiency and precision of the sPSDAE model, a pruning operation is added into the sPSDAE model so as to prohibit non-superior units from participating in all the subsequent layers. Meanwhile, a feature fusion mechanism is introduced to ensure the uniqueness of the feature dimensions. After that, the sparse expression of the sPSDAE model is strengthened, thereby improving the generalization ability. The proposed method is evaluated by using a public bearing dataset and is compared with other popular fault diagnosis models. The results show that the ability of the sPSDAE model to extract features is significantly enhanced and the phenomenon of gradient disappearance is further reduced. The proposed model achieves higher diagnostic accuracy than other popular fault diagnosis models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAR	2020	88								106060	10.1016/j.asoc.2019.106060													
J								Learning Models for Semantic Classification of Insufficient Plantar Pressure Images	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Machine Learning; Artificial Neural Networks; Feature Extraction; Image Processing; Image Analysis; Image Classification	SHOT; OBJECTS	Establishing a reliable and stable model to predict a target by using insufficient labeled samples is feasible and effective, particularly, for a sensor-generated data-set. This paper has been inspired with insufficient data-set learning algorithms, such as metric-based, prototype networks and meta-learning, and therefore we propose an insufficient data-set transfer model learning method. Firstly, two basic models for transfer learning are introduced. A classification system and calculation criteria are then subsequently introduced. Secondly, a data-set of plantar pressure for comfort shoe design is acquired and preprocessed through foot scan system; and by using a pre-trained convolution neural network employing AlexNet and convolution neural network (CNN)-based transfer modeling, the classification accuracy of the plantar pressure images is over 93.5%. Finally, the proposed method has been compared to the current classifiers VGG, ResNet, AlexNet and pre-trained CNN. Also, our work is compared with known-scaling and shifting (SS) and unknown-plain slot (PS) partition methods on the public test databases: SUN, CUB, AWA1, AWA2, and aPY with indices of precision (tr, ts, H) and time (training and evaluation). The proposed method for the plantar pressure classification task shows high performance in most indices when comparing with other methods. The transfer learning-based method can be applied to other insufficient data-sets of sensor imaging fields.																	1989-1660					MAR	2020	6	1					51	61		10.9781/ijimai.2020.02.005													
J								Classification of engraved pottery sherds mixing deep-learning features by compact bilinear pooling	PATTERN RECOGNITION LETTERS										Classification of engraved pottery sherds; Deep learning; Pooling strategies; Compact bilinear pooling		The ARCADIA project aims at using pattern recognition and machine learning to promote a systematic analysis of the large corpus of archaeological pottery fragments excavated in Saran (France). Dating from the High Middle Ages, these sherds have been engraved with repeated patterns using a carved wooden wheel. The study of these engraved patterns allows archaeologists to better understand the diffusion of ceramic productions. In this paper, we present a method that classifies patterns of ceramic sherds by combining deep learning-based features extracted from some pre-trained Convolutional Neural Network (CNN) models. A dataset composed of 888 digital patterns extracted from 3D scans of pottery sherds was used to evaluate our approach. The classification capacity of each CNN model was first assessed individually. Then, several combinations of common pooling methods using different classifiers were tested. The best result was obtained when features of the VGG19 and ResNet50 models were combined using Compact Bilinear Pooling (CBP) with a high classification rate of 95.23%. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						1	7		10.1016/j.patrec.2019.12.009													
J								Variance-preserving deep metric learning for content-based image retrieval	PATTERN RECOGNITION LETTERS										Metric learning; Content-based information retrieval; Deep learning		Supervised deep metric learning led to spectacular results for several Content-based Information Retrieval (CBIR) applications. The success of these approaches slowly led to the belief that image retrieval and classification are just slightly different variations of the same problem. However, recent evidence suggests that learning highly discriminative representation for a (limited) set of training classes removes valuable information from the representation, potentially harming both the in-domain, as well as the out-of-domain retrieval precision. In this paper, we propose a regularized discriminative deep metric learning method that aims to not only learn a representation that allows for discriminating between different classes, but it is also capable of encoding the latent generative factors separately for each class, overcoming this limitation. This allows for modeling the in-class variance and, as a result, maintaining the ability to represent both sub-classes of the in-domain data, as well as objects that belong to classes outside the training domain. The effectiveness of the proposed method, over existing supervised and unsupervised representation/metric learning approaches, is demonstrated under different in-domain and out-of-domain setups and three challenging image datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						8	14		10.1016/j.patrec.2019.11.041													
J								A dimension-reduction based multilayer perception method for supporting the medical decision making	PATTERN RECOGNITION LETTERS										Deep learning; Multilayer perceptron; Weight initialization; Medical decision support	FUZZY-SETS	Due to the rapid development of Medical IoT recently, how to effectively apply these huge amounts of IoT data to enhance the reliability of the clinical decision making has become an increasing issue in the medical field. These data usually comprise high-complicated features with tremendous volume, and it implies that the simple inference models may less powerful to be practiced. In deep learning, multilayer perceptron (MLP) is a kind of feed-forward artificial neural network, and it is one of the high-performance methods about stochastic scheme, fitness approximation, and regression analysis. To process these high uncertain data, the proposed work based on MLP structure in particular integrates the boosting scheme and dimension-reduction process. In this proposed work, the advanced ReLU-based activation function is used. Also, the weight initialization is applied to improve the stable prediction and convergence. After the improved dimension-reduction process is introduced, the proposed method can effectively learn the hidden information from the reformative data and the precise labels also can be recognized by stacking a small amount of neural network layers with paying few extra cost. The proposed work shows a possible path of embedding dimension reduction in deep learning structure with minor price. In addition to the prediction issue, the proposed method can also be applied to assess risk and forecast trend among different information systems. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						15	22		10.1016/j.patrec.2019.11.026													
J								Long short term memory based patient-dependent model for FOG detection in Parkinson's disease	PATTERN RECOGNITION LETTERS										Parkinson's disease; Wearable sensors; Accelerometer sensor; Freezing of gait; Classification; Support vector machine; Long short term memory deep learning model	GAIT; DIAGNOSIS	Deep learning has a great impact on healthcare for discovering hidden patterns in the clinical data to detect or predict the different diseases. This work proposed a monitoring procedure for Parkinson's disease (PD) using a recorded signals dataset from multiple wearable on body sensors placed at different positions on the leg, namely on knee, hip and ankle. Different symptoms of PD patients can be detected from the acceleration signals, where the Freezing of Gait (FOG) is considered the main sign. Typically, FOG is patient-dependent that varies in severity and incidence from patient to another. In this work, a deep learning model, namely the Long Short Term Memory (LSTM) network-based patient-dependent model was adopted for FOG detection. A comparison between the proposed model and the traditional machine learning methods, including the linear support vector machine (SVM) was conducted using the signals of the three sensors. The results established the superiority of the LSTM model, which achieved 83.38% in terms of the average accuracy in comparison with the SVM which achieved 79.48%. For example, in patient 2, the maximum accuracy achieved using the LSTM is 98.89%, while the corresponding maximum accuracy is 80% using the linear SVM. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						23	29		10.1016/j.patrec.2019.11.036													
J								Thorax disease classification with attention guided convolutional neural network	PATTERN RECOGNITION LETTERS										CXR image classification; Visual attention; Feature ensemble		This paper considers the task of thorax disease diagnosis on chest X-ray (CXR) images. Most existing methods generally learn a network with global images as input. However, thorax diseases usually happen in (small) localized areas which are disease specific. Thus training CNNs using global images may be affected by the (excessive) irrelevant noisy areas. Besides, due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance. For addressing the above problems, we propose to integrate the global and local cues into a three-branch attention guided convolution neural network (AG-CNN) to identify thorax diseases. An attention guided mask inference based cropping strategy is proposed to avoid noise and improve alignment in the global branch. AG-CNN also integrates the global cues to compensate the lost discriminative cues by the local branch. Specifically, we first learn a global CNN branch using global images. Then, guided by the attention heatmap generated from the global branch, we infer a mask to crop a discriminative region from the global image. The local region is used for training a local CNN branch. Lastly, we concatenate the last pooling layers of both the global and local branches for fine-tuning the fusion branch. Experiments on the ChestX-ray14 dataset demonstrate that after integrating the local cues with the global information, the average AUC scores are improved by AG-CNN. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						38	45		10.1016/j.patrec.2019.11.040													
J								Speckle noise suppression in 2D ultrasound kidney images using local pattern based topological derivative	PATTERN RECOGNITION LETTERS										Speckle noise; Neighbourhood; Ultrasound; Local pattern; Discrete Topological Derivative; Gradient; Holes		One of the affordable and least harmful modalities used to efficiently detect and diagnose the kidney diseases is the Ultrasound scans. The main drawback of the ultrasound images is the presence of speckle noise that reduces the efficiency of image processing and hinders the interpretation. This paper proposes a novel technique named Local Binary Pattern based Discrete Topological Derivative and its variants to address speckle noise reduction problem in 2D ultrasound kidney images. In typical Discrete Topological Derivative, the execution time is higher and as a result an optimizer is incorporated based on local pattern and gradient tolerance value resulting in 20 times reduction in execution time with improved results. The experimentation is carried out on 100 clinical 2D ultrasound images and moreover, proposed methods are compared with the competing Discrete Topological Derivative and some commonly used speckle noise removal filters. The results are promising and thereby confirms that the proposed Local Binary Pattern based Discrete Topological Derivative can produce a better speckle noise reduction that enables the doctors to detect and diagnose kidney diseases in 2D ultrasound images with lesser strain. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						49	55		10.1016/j.patrec.2019.12.005													
J								A bio-inspired quaternion local phase CNN layer with contrast invariance and linear sensitivity to rotation angles	PATTERN RECOGNITION LETTERS												Deep learning models have been particularly successful with image recognition using Convolutional Neural Networks (CNN). However, the learning of a contrast invariance and rotation equivariance response may fail even with very deep CNNs or by large data augmentations in training. We were inspired by the V1 visual features of the mammalian visual system to emulate as much as possible the early visual system and add more invariant capacities to the CNN. We present a new quaternion local phase convolutional neural network layer encoding three local phases. We present two experimental setups: An image classification task with four contrast levels, and a linear regression task that predicts the rotation angle of an image. In sum, we obtain new patterns and feature representations for deep learning, which capture illumination invariance and a linear response to rotation angles. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						56	62		10.1016/j.patrec.2019.12.001													
J								Integrated design of deep features fusion for localization and classification of skin cancer	PATTERN RECOGNITION LETTERS										Cells; Principle component analysis (PCA); Melanoma; Alexnet; VGG-16	COMPUTER-AIDED DIAGNOSIS; MELANOMA	The common fatal type of skin cancer is melanoma. Recently, numerous intelligent systems are used to detect skin cancer at an early stage. These systems are helpful for a dermatologist as a preliminary judgment to diagnose skin cancer. However, accurate skin lesion detection is an intricate task. This work comprises three main phases, firstly perform preprocessing to resize the images to 240 x 240 x 3 and convert RGB into L-boolean AND* a(boolean AND*) b(boolean AND*) in which the luminance channel is selected. Secondly, Biorthogonal 2-D wavelet transform, Otsu algorithm are used to segment the skin lesion. Thirdly, deep features extracted from pre-trained Alex net and VGG16 and serially fused. The applied PCA for optimal features selection for classification into benign and malignant. The publically available datasets (PH2, ISBI 2016- 2017) are merged to form a single large dataset for the validated of proposed method. The results comparison is performed with the existing work which confirms that the proposed method classifies the skin lesion more accurately. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				MAR	2020	131						63	70		10.1016/j.patrec.2019.11.042													
J								Parallel connected-Component-Labeling based on homotopy trees	PATTERN RECOGNITION LETTERS										Connected-Component-Labeling; Computational topology; Adjacency tree; Digital image; Parallelism	COMPUTATION	Taking advantage of the topological and isotopic properties of binary digital images, we present here a new algorithm for connected component labeling (CLL). A local-to-global treatment of the topological information within the image, allows us to develop an inherent parallel approach. The time complexity order for an image of m x n pixels, under the assumption that a processing element exists for each pixel, is near O(log(m + n). Additionally, our method computes both the foreground and background CCL, and allows a straightforward computation of topological features like Adjacency Trees. Experiments show that our method obtains better performance metrics than other approaches. Our work aims at generating a new class of labeling algorithms: those centered in fully parallel approaches based on computational topology, thus allowing a perfect concurrent execution in multiple threads and preventing the use of critical sections and atomic instructions. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						71	78		10.1016/j.patrec.2019.11.039													
J								Attacking NIST biometric image software using nonlinear optimization	PATTERN RECOGNITION LETTERS										NIST biometric image software; MINDTCT; BOZORTH3; NLopt		Automated fingerprint identification systems are deployed by law enforcement agencies all over the world for authentication. In the US, the NIST biometric image software (NBIS) is used by the Department of Homeland Security and the Federal Bureau of Investigation for fingerprint matching. NBIS uses MINDTCT as the minutia extractor and BOZORTH3 as the fingerprint matcher. We use nonlinear optimization to attack the BOZORTH3 fingerprint matching system. We use FVC2002, ATVS and CASIA datasets to validate the performance of our attack. We show that the average match score of attack fingerprints is 111.2 for FVC2002, 97.17 for ATVS and 111.07 for the CASIA dataset. We show that for all three datasets, changing only 14 minutia features allows us to attack the BOZORTH3 fingerprint matcher with more than 75% probability of successful attack. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						79	84		10.1016/j.patrec.2019.12.003													
J								I-PETER (Interactive platform to experience tours and education on the rocks): A virtual system for the understanding and dissemination of mineralogical-petrographic science	PATTERN RECOGNITION LETTERS										Interactive museum; Cultural tourism; Cultural heritage; Web application; Dataset; Petrography; Mineralogy		Nowadays mineralogical and petrographic sciences represent important resources for the understanding of natural and cultural heritage. Indeed, the implication in economic and social scenario, from the exploitation of geo-resources to the natural stone application in architecture, involves the whole society. In this context, scientific museums are called to enhance society awareness about the importance of minerals and rocks, while facilitating their understanding. It is well-known that the majority of the population is still left out from the museum fruition, usually because of problems of cultural accessibility. In order to find a solution, the Museum of Mineralogy, Petrography and Volcanology of the University of Catania implemented a new communication system based on the visitors personal experience. The idea is to give to all users, regardless of their educational level, the possibility to learn by playing. The collaboration among geologists, conservators and computer scientists led to the implementation of a web application called I-PETER: Interactive Platform to Experience Tours and Education on the Rocks. The application, interacting with a database, offers to the public two different modes of exploring the museums collections. The visitor can decide to observe the rocks or minerals from the sample exposed to the internal structure and then explore their external application on monuments, or he/she can choose to make a tour in the interactive map, focusing on a particular monument and from there make the virtual reverse path from the macro to the micro scale. Furthermore, thanks to the effort made to construct the knowledge base at the base of I-PETER web app, a labelled dataset of images of rocks and minerals can be easily released for future petrological investigations based on machine learning. Thanks to this pilot project, the museums scientific knowledge can promote cultural tourism by making it more accessible to a wide public, as well as support scientific research on petrography and mineralogy. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						85	90		10.1016/j.patrec.2019.12.002													
J								Attention guided neural network models for occluded pedestrian detection	PATTERN RECOGNITION LETTERS										Pedestrian detection; Occlusion; Convolutional neural networks; Attention networks; Recurrent neural networks	OCCLUSION	Occluded pedestrian detection has been a research difficulty in computer vision for a long time. The conventional approach to solve this problem is to learn partial detectors, which can be properly integrated for occlusion handling. However, the efficiency of this type of methods is limited in practical applications since the partial detectors can not cover all occlusion patterns. In this paper, an attention guided neural network model (AGNN) is proposed for the occlusion handling of pedestrian detections, which is inspired by the approaches of sentiment classification. Firstly, a fixed-size window slides on a still image without overlapping to generate a set of sub-images. Secondly, a convolutional neural network is employed to extract the high-level features from resulting sub-images. Then, the attention network performs local feature weighting, from which the features representing the body parts of pedestrians are selected. Finally, the feature sequences are classified by recurrent neural network in proper order based on the weighted results. In addition, we explore different mechanisms of attention guidance on the detector for the detections. Compared with the state-of-the-art methods on two standard pedestrian datasets, experimental results demonstrate the comparable performance of our approach in terms of miss rates. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						91	97		10.1016/j.patrec.2019.12.010													
J								A multi-feature bipartite graph ensemble for image segmentation	PATTERN RECOGNITION LETTERS										Image segmentation; Feature fusion; Bipartite graph; Spectral clustering	TUTORIAL	Image segmentation benefits from using multi-feature ensembles. In this paper, we propose a novel multi-layer bipartite graph model for more effective feature fusion. This model employs multiple graph layers, each representing a feature space. They share common vertices but have individual edge sets that are obtained from different feature spaces. The features are fused by the regularization defined on a Grassmann manifold, which compresses the graph layers into subspace representations and merges them into one. The merged graph is then fed into a graph-cut algorithm to generate the final segmentation. Experiments carried out using the Berkeley segmentation benchmark show that our model is effective in feature fusion and image segmentation, outperforming the state-of-the-art superpixel-based methods. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						98	104		10.1016/j.patrec.2019.12.017													
J								Hierarchical segmentation from a non-increasing edge observation attribute	PATTERN RECOGNITION LETTERS										Hierarchical segmentation; Non-increasing attributes; Mathematical morphology	BINARY PARTITION TREE; IMAGE SEGMENTATION; CONNECTED OPERATORS; REPRESENTATION; SALIENCY	Hierarchical image segmentation provides region-oriented scale-spaces: sets of image segmentations at different detail levels in which the segmentations at finer levels are nested with respect to those at coarser levels. Guimaraes et al. proposed a hierarchical graph-based image segmentation (HGB) method based on the Felzenszwalb-Huttenlocher dissimilarity. It computes, for each edge of a graph, the minimum scale in a hierarchy at which two regions linked by this edge should be merged according to the dissimilarity. We provide an explicit definition of the (edge-) observation attribute and Boolean criterion which are at the basis of this method and show that they are not increasing. Then, we propose an algorithm to compute all the scales for which the criterion holds true. Finally, we propose new methods to regularize the observation attribute and criterion and to set up the observation scale value of each edge of a graph, following the current trend in mathematical morphology to study criteria which are not increasing on a hierarchy. Assessments on Pascal VOC 2010 and 2012 show that these strategies lead to better segmentation results than the ones obtained with the original HGB method. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						105	112		10.1016/j.patrec.2019.12.014													
J								CNN based spatial classification features for clustering offline handwritten mathematical expressions	PATTERN RECOGNITION LETTERS										Clustering images; Offline handwritten; Mathematical expression; CNN; Weakly supervised learning		To help human markers mark a large number of answers of handwritten mathematical expressions (HMEs), clustering them makes marking more efficient and reliable. Clustering HMEs, however, faces the problem of extracting both localization and classification representation of mathematical symbols for an HME image and defining the distance between two HME images. First, we propose a method based on Convolutional Neural Networks (CNN) to extract the representations for an HME. Symbols in various scales are located and classified by a combination of features from a multi-scale CNN. We use weakly supervised training combined with symbols attention to enhance localization and classification predictions. Second, we propose a multi-level spatial distance between two representations for clustering HMEs. Experiments on CROHME 2016 and CROHME 2019 dataset show the promising results of 0.99 and 0.96 in purity, respectively. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						113	120		10.1016/j.patrec.2019.12.015													
J								Single-image raindrop removal using concurrent channel-spatial attention and long-short skip connections	PATTERN RECOGNITION LETTERS										Image recovery; Raindrop removal; Deep nerual networks; Attention; Skip connection		Image raindrop removal refers to removing raindrops from the images taken through glass under rainy weather. It is a challenging problem due to the large variations of raindrop appearances as well as the nonlinear visual distortions caused by raindrops. This problem becomes much more difficult when only a single image is provided. This paper aims at tackling the single-image raindrop removal problem by leveraging the power of deep learning. We proposed an end-to-end approach using the deep neural network. To address the challenges of raindrop removal, we introduce a concurrent channel and spatial attention mechanism implemented by Squeeze & Excitation into the network. The channel attention mechanism allows individually selecting useful features in the network for handling the raindrops of different appearances and restoring different image patterns. The spatial attention enables different treatments to the image regions with different degrees of distortion and with different local image patterns. In addition, long-short skip connections are added for utilizing the intermediate features of the network for better restoration. The experimental results show that the proposed approach outperformed the existing ones. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						121	127		10.1016/j.patrec.2019.12.012													
J								Facial expression recognition based on deep convolution long short-term memory networks of double-channel weighted mixture	PATTERN RECOGNITION LETTERS										Facial expression recognition; Computer applications; CNN; LSTM	NEURAL-NETWORK	With the aging population and the increasing number of empty nest elderly, more and more researches focus on home service robots. Autonomous analysis of human emotions by robots is helpful to provide better services for human beings. Facial expression, as an important modality in emotional recognition, is helpful to improve emotional recognition. In order to explore a new method that can effectively improve the recognition rate of expression two facial expression recognition(FER) methods are proposed in this paper. They are double-channel weighted mixture deep convolution neural networks (WMDCNN) based on static images and deep cnn long short-term memory networks of double-channel weighted mixture(WMCNN-LSTM) based on image sequences. WMDCNN network can quickly recognize facial expressions and provide static image features for WMCNN-LSTM network. WMCNN-LSTM network utilizes the static image features to further acquire the temporal features of image sequence, which can realize the accurate recognition of facial expressions. The experimental results show that the average recognition rate of the WMDCNN network on the four datasets of CK+, JAFFE, Oulu-CASIA and MMI are 0.985, 0.923,0.86,0.78 respectively. The WMCNN-LSTM method has an average recognition rate of 0.975, 0.88, and 0.87 on the three datasets CK+, Oulu-CASIA and MMI respectively. By comparing with the existing FER method, our method further improves the recognition rate in the above four expression data sets. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						128	134		10.1016/j.patrec.2019.12.013													
J								Challenges in automatic Munsell color profiling for cultural heritage	PATTERN RECOGNITION LETTERS										Color specification; Munsell color; Color space conversion; Digital archaeology; Measurements in cultural heritage	SENSOR	Color specification is the process of measuring the color of a sample in a given color space. We focused onto the Munsell color space as archaeologists are used to employ the so called Munsell Soil Color Charts (MSCCs) directly in the excavation sites. For these scholars and researchers, being enabled to perform Munsell color specification in an automatic way is crucial, as they spend a lot of time to subjectively specify colors in the Munsell system. We extended the dataset ARCA328, which was specifically thought for the automatic Munsell color specification issue, increasing the number of images from 328 to 1,488, and the number of samples from 56,160 to 315,333. Then, we conducted generalization-tests of color conversion for color specification, adopting a classification approach instead of a regression one. This choice was motivated by the fact that the set of all the possible HVC coordinates in the MSCCs is a discrete one. Hence, we decided to consider each chip in the MSCCs as a class to be learnt and recognized by the SVC. With these tests we highligthed the limits of automatic Munsell color specification without any reference-system or calibration phase. Finally, we gave insights for future works aimed to design automatic illuminant calibration phase and to investigate deep learning approaches, leveraging a synthetic images rendering procedure we also present in this work. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						135	141		10.1016/j.patrec.2019.12.008													
J								Improving smart interactive experiences in cultural heritage through pattern recognition techniques	PATTERN RECOGNITION LETTERS										Internet of Things; Smart interactive experiences; Smart objects; Deep learning; Convolutional neural network; CH fruition		New Information and Communication Technologies have a large potential to improve general public awareness of the importance of Cultural Heritage (CH) and to provide tools that can make visits to historical sites more interesting and enjoyable. The Internet of Things (IoT) technology can further contribute to these goals, by allowing visitors to museum and CH sites to manipulate smart objects by receiving information that stimulates emotions, understanding and appropriation of the contents. In our research, interaction paradigms and innovative methods are developed to allow curators and guides of cultural sites (i.e., domain experts) to manage interactive IoT-based environments, in order to create Smart Interactive Experiences, which are usage situations created by synchronizing many available smart objects to specific situations that might better satisfy the needs of the visitors. This article illustrates a system that, by means of a tangible user interface, integrated by pattern recognition and computer vision techniques, supports CH experts in creating Smart Interactive Experiences by properly tailoring the behavior of the involved smart objects. An experimental evaluation of the used techniques has been performed and it is presented and discussed. (c) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						142	149		10.1016/j.patrec.2019.12.011													
J								EGO-CH: Dataset and fundamental tasks for visitors behavioral understanding using egocentric vision	PATTERN RECOGNITION LETTERS										Egocentric vision; First person vision; Localization; Object detection; Object retrieval	IMAGE RETRIEVAL	Equipping visitors of a cultural site with a wearable device allows to easily collect information about their preferences which can be exploited to improve the fruition of cultural goods with augmented reality. Moreover, egocentric video can be processed using computer vision and machine learning to enable an automated analysis of visitors' behavior. The inferred information can be used both online to assist the visitor and offline to support the manager of the site. Despite the positive impact such technologies can have in cultural heritage, the topic is currently understudied due to the limited number of public datasets suitable to study the considered problems. To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first dataset of egocentric videos for visitors' behavior understanding in cultural sites. The dataset has been collected in two cultural sites and includes more than 27hours of video acquired by 70 subjects, with labels for 26 environments and over 200 different Points of Interest. A large subset of the dataset, consisting of 60 videos, is associated with surveys filled out by real visitors. To encourage research on the topic, we propose 4 challenging tasks (room-based localization, point of interest/object recognition, object retrieval and survey prediction) useful to understand visitors' behavior and report baseline results on the dataset. (c) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						150	157		10.1016/j.patrec.2019.12.016													
J								Two sides of the same coin: Improved ancient coin classification using Graph Transduction Games	PATTERN RECOGNITION LETTERS											RECOGNITION	In this work we tackle the problem of automatic recognition of ancient coin types using a semisupervised learning method, namely Graph Transduction Games. Such problem is complex, mainly due to the low inter-class and large intra-class variations and the task becomes even more complex due to lack of labeled large datasets from certain ancient ages. In this paper we propose a new dataset which is chiefly the extension of a previous one both in terms of quantity and diversity. Moreover, we propose a game-theoretic model that exploits both sides of a coin to achieve higher classification accuracy. We experimentally demonstrate that proposed approach brings performance improvement in this complex task even when few number of labelled images are available. (c) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						158	165		10.1016/j.patrec.2019.12.007													
J								Facial expression recognition based on a multi-task global-local network	PATTERN RECOGNITION LETTERS										Facial expression recognition; Global-local network; Spatial-temporal representation	MODEL	Facial expression recognition plays an important role in intelligent human-computer interaction. The clues for understanding facial expressions lie not in global facial appearance, but also in local informative dynamics among different but confusing expressions. In this paper, we design a multi-task learning framework for global-local representation of facial expressions. First, a shared shallow module is designed to learn information from local regions and the global image. Then we construct a part-based module, which processes critical local regions including the eyes, the nose, and the mouth to extract local informative dynamics related to facial expressions. A global face module is proposed to extract global appearance features related to expressions. The proposed network extracts both local-global and spatio-temporal information for a discriminative and robust representation of facial expressions. Through properly fusing these modules into a system, we have achieved competitive results on the CK+ and Oulu-CASIA databases. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						166	171		10.1016/j.patrec.2020.01.016													
J								High cursive traditional Asian character recognition using integrated adaptive constraints in ensemble of DenseNet and Inception models	PATTERN RECOGNITION LETTERS										Deep learning; Adaptive regularizer; Korean document; Pattern recognition		In this paper, we propose integrated adaptive sensitivity and robustness terms for the cost function of a convolutional neural network (CNN). The sensitivity term considers the slight variations and high frequency components of the input image samples. It distinguishes between images that look similar but belong to different classes. This regularizer is designed to enhance the between-class distance which is a biological definition for the simple cells of the visual system. On the other hand, the robustness term is used to develop a more stable CNN structure against disturbances and perturbations. The robust term provides better within-class features because it recognizes images that look different but are actually from the same class. The robust term symbolizes the complex cell characteristics of the visual system. The coefficients of the sensitivity and robustness regularization terms are adaptively tuned along with the network parameters using gradient descent. Two optimizers are assigned to tune the parameters: one for tuning the model parameters and the other one to adjust the sensitivity and robustness coefficients. This approach is applied to Korean traditional documents for character classification. The results show better within- and between-class classification ability for highly complex character styles with imbalanced number of samples. (c) 2020 Elsevier B.V. Allrights reserved.																	0167-8655	1872-7344				MAR	2020	131						172	177		10.1016/j.patrec.2020.01.013													
J								A cockpit of multiple measures for assessing film restoration quality	PATTERN RECOGNITION LETTERS										Image quality assessment; Film restoration; Features identification	RETINEX IMPLEMENTATION; MILANO-RETINEX; COLOR	In machine vision, the idea of expressing the quality of a films by a single value is very popular. Usually this value is computed by processing a set of image features with the aim of resembling as much as possible a kind of human judgment of the film quality. Since human quality assessment is a complex mechanism involving many different perceptual aspects, we believe that such approach may scarcely provide a comprehensive analysis. Especially in the field of digital movie restoration, a single score can hardly provide reliable information about the effects of the various restoring operations. For this reason we introduce an alternative approach, where a set of measures, describing over time basic global and local visual properties of the film frames, is computed in an unsupervised way and delivered to expert evaluators for checking the restoration pipeline and results. The proposed framework can be viewed as a car or airplane cockpit, whose parameters (i.e. the computed measures) are necessary to control the machine status and performance. This cockpit, which is publicly available online, would like to support the digital restoration process and its assessment. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						178	184		10.1016/j.patrec.2020.01.009													
J								Efficient hierarchical graph partitioning for image segmentation by optimum oriented cuts	PATTERN RECOGNITION LETTERS										Unsupervised segmentation; Hierarchical graph partitioning; Graph-cut measure	FORESTING TRANSFORM	In this work, a hierarchical graph partitioning based on optimum cuts in graphs is proposed for unsupervised image segmentation, that can be tailored to the target group of objects, according to their boundary polarity, by extending Oriented Image Foresting Transform (OIFT). The proposed method, named UOIFT, theoretically encompasses as a particular case the single-linkage algorithm by minimum spanning tree (MST) and gives superior segmentation results compared to other approaches commonly used in the literature, usually requiring a lower number of image partitions to accurately isolate the desired regions of interest with known polarity. The method is supported by new theoretical results involving the usage of non-monotonic-incremental cost functions in directed graphs and exploits the local contrast of image regions, being robust in relation to illumination variations and inhomogeneity effects. UOIFT is demonstrated using a region adjacency graph of superpixels in medical and natural images. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						185	192		10.1016/j.patrec.2020.01.008													
J								Gastrointestinal diseases segmentation and classification based on duo-deep architectures	PATTERN RECOGNITION LETTERS										Stomach diseases; Mask RCNN; CNN features; Feature selection	CAPSULE ENDOSCOPY IMAGES; REDUCTION	Nowadays, almost one million gastrointestinal patients are successfully treated by Wireless Capsule Endoscopy (WCE). It is the latest technology in the area of medical imaging for the diagnosis of gastrointestinal diseases such as ulcer, polyp, bleeding, etc. Manual diagnosis process is time-consuming and hard for doctors; therefore, researchers have proposed computerized techniques for detection and classification of these diseases. In this article, a deep learning-based method is presented for ulcer detection and gastrointestinal diseases (ulcer, polyp, bleeding) classification. Modified mask Recurrent Convolutional Neural Network (RCNN) based ulcer segmentation is proposed. The ulcer annotated images are utilized to train the Mask RCNN model to obtain output in the form of bounding box ulcer detected area and mask segmented region. In the classification phase, the ResNet101 pre-trained CNN model is fine-tuned through transfer learning to derive deep features. The acquired deep features are optimized through grasshopper optimization along with minimum distance fitness function. The best-selected features are finally supplied to a Multi-class Support Vector Machine (MSVM) of cubic kernel function for final classification. Experiments have been performed in two-steps; first, the ulcer segmentation results are computed through recall, precision, and Mean Overlap Coefficient (MOC). The ResNet50+FPN as backbone and training all the layers of Mask-RCNN gives best results in terms of MOC = 0.8807 and average precision = 1.0. Second, the best classification accuracy of 99.13% is achieved on the cubic SVM for K = 10. It is clearly perceived that the proposed method outperforms when compared and analyzed with the existing methods. (c) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						193	204		10.1016/j.patrec.2019.12.024													
J								Deterministic dropout for deep neural networks using composite random forest	PATTERN RECOGNITION LETTERS										Deterministic dropout; Composite random forest; Deep neural network; Regularizer	RECOGNITION	Dropout prevents overfitting in deep neural networks. Typical strategy of dropout involves random termination of connections irrespective of their importance. Termination blocks the propagation of class discriminative information across the network. As a result, dropout may lead to inferior performance. We propose a deterministic dropout where only unimportant connections are dropped ensuring propagation of class discriminative information. We identify the unimportant connections using a novel composite random forest, integrated into the network. We prove that better generalization is achieved by terminating these unimportant connections. The proposed algorithm is useful in preventing overfitting in noisy datasets. The proposal is equally good for datasets with smaller number of training examples. Experiments on several benchmark datasets show up to 8% improvement in classification accuracy. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						205	212		10.1016/j.patrec.2019.12.023													
J								Creating speaker independent ASR system through prosody modification based data augmentation	PATTERN RECOGNITION LETTERS											CHILDRENS SPEECH	In this paper, the effect of prosody-modification-based data augmentation is explored in the context of automatic speech recognition (ASR). The primary motive is to develop ASR systems that are less affected by speaker-dependent acoustic variations. Two factors contributing towards inter-speaker variability that are focused on in this paper are pitch and speaking-rate variations. In order to simulate such an ASR task, we have trained an ASR system on adults' speech and tested it using speech data from adult as well as child speakers. Compared to adults' speech test case, the recognition rates are noted to be extremely degraded when the test speech is from child speakers. The observed degradation is basically due to large differences in pitch and speaking-rate between adults' and children's speech. To overcome this problem, pitch and speaking-rate of the training speech are modified to create new versions of the data. The original and the modified versions are then pooled together in order to capture greater acoustic variability. The ASR system trained on augmented data is noted to be more robust towards speaker-dependent variations. Relative improvements of 11.5% and 27.0% over the baseline are obtained on decoding adults' and children's speech test sets, respectively. (c) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				MAR	2020	131						213	218		10.1016/j.patrec.2019.12.019													
J								Conditional GAN based individual and global motion fusion for multiple object tracking in UAV videos	PATTERN RECOGNITION LETTERS										Multi-object tracking; Neural networks; UAV	MULTITARGET	Multiple Object Tracking (MOT) meets great challenges in videos captured by Unmanned Aerial Vehicles (UAVs). Different from traditional videos, due to high altitude and abrupt motion changes of UAVs, the sizes of target objects in UAVs videos are usually very small and the appearance information of target objects is unreliable. The motion analysis is meaningful to associate multiple objects in UAV videos. However, the traditional motion analysis models inevitably suffer from the autonomous motion of UAVs. In this paper, we proposed a Conditional Generative Adversarial Networks (GAN) based model to predict complex motions in UAV videos. We regard the objects motions and the UAV movement as the individual motions and global motions respectively. They are complementary with each other and are employed jointly to facilitate accurate motion prediction. Specifically, a social Long Short Term Memory network is exploited to estimate the individual motion of objects, and a Siamese network is constructed to generate the global motion to reflect the view changes from UAVs, and a conditional GAN is developed to generate the final motion affinity. Extensive experimental results are conducted on public UAV datasets contained various types of objects and 4 different kinds of object detection inputs. Robust motion prediction and improved MOT performance are achieved compared with state-of-the-art methods. (c) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						219	226		10.1016/j.patrec.2019.12.018													
J								A group lasso based sparse KNN classifier	PATTERN RECOGNITION LETTERS										Sparse learning; Group lasso; Explainable classifier	FACE RECOGNITION; DISCRIMINATIVE DICTIONARY; K-SVD; ALGORITHM; REGRESSION; SELECTION; MODELS	Sparse features have been shown effective in applications of computer vision, machine learning, signal processing, etc. Group sparsity was proposed by considering that there exist natural group structures in many problems. Previous research mainly focuses on improving the way to extract sparse features, such as lasso, group lasso, overlapped group lasso, sparse group lasso. In existing work, sparse features are usually taken as input for classifiers, such as SVM, KNN, or SRC (Sparse Representation based Classification). In this paper, we find that, instead of using sparse group features as input for classifiers, sparse group features are good candidates for selection of most relevant classes/groups. We design a new classifier to improve classification accuracy: (1) we use sparse group lasso to select K most relevant classes/groups, which makes this approach robust, because it filters out unrelated classes/groups in group level, instead of individual sample level; (2) KSVD is used to get exact desired sparsity (k nonzero entries) and thus eliminates the difficulty of hyperparameter tuning; (3) simple summation of regression weights within each class/group contains sufficient class discriminant information, and the chance of a sample belonging to a specific class is denoted simply by the summation of corresponding regression weights within each class, which is in line with the need of Explainable AI (XAI). The K most relevant groups/classes can be considered as K neighbors of the correct class. Thus, we call this classifier Group Lasso based Sparse KNN (GLSKNN). Compared to 8 other approaches, GLSKNN classifier outperforms other methods in term of classification accuracy for two public image datasets and images with different occlusion/noise levels. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						227	233		10.1016/j.patrec.2019.12.020													
J								CHAT-Bot: A cultural heritage aware teller-bot for supporting touristic experiences	PATTERN RECOGNITION LETTERS										Recommender system; Context-aware computing; Digital storytelling; Chatbot; Tourism; Cultural heritage		Cultural heritage is an important resource that allows us to know and promote a territory. In this respect, it is important to experiment with the enhancement of cultural heritage by adopting approaches that meet the dynamic needs of various types of users. The aim of this paper is to introduce a recommender system capable of developing adaptive tourist routes. In fact, the proposed system suggests points of interest and related services according to both the profile of the tourist and contextual aspects. In particular, the interaction of the user with the system occurs through a chatbot that allows to build a real dialog. In order to show the potential of the proposed approach, a prototype was developed to support the user in building a customized tourist route related to some of the most important cultural sites in Campania (a region in Southern Italy): Herculaneum, Paestum and Pompeii. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						234	243		10.1016/j.patrec.2020.01.003													
J								A novel face presentation attack detection scheme based on multi-regional convolutional neural networks	PATTERN RECOGNITION LETTERS										Face presentation attack detection; Adversarial examples; Local classification loss; Multi-regional convolutional neural networks		Face presentation attack detection methods based on deep learning have achieved noticeable results. However, such methods tend to over-emphasize a certain local area, which limits their performance against traditional attacks, and makes the system vulnerable to adversarial example attacks. To utilize more information of the input and enhance the robustness of face presentation attack detection methods against adversarial examples, this paper proposes multi-regional convolutional neural networks, and introduces the concept of local classification loss to local patches, so as to utilize the input information in the entire face region and to avoid over-emphasizing certain local areas. Experimental results demonstrate that the proposed method is more robust against adversarial example attacks, and its performance against traditional attacks is also improved compared to existing methods. (c) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				MAR	2020	131						261	267		10.1016/j.patrec.2020.01.002													
J								Sample reduction using farthest boundary point estimation (FBPE) for support vector data description (SVDD)	PATTERN RECOGNITION LETTERS										Support vector data description (SVDD); Data distribution; Sample reduction; Farthest boundary point	ONE-CLASS CLASSIFICATION; INTRUSION DETECTION; COMPONENT ANALYSIS; NOVELTY DETECTION; FEATURE-SELECTION; ENSEMBLE	The objective of this paper is to design an algorithm to maximize the learning ability and knowledge about the target class while minimizing the number of training samples for support vector data description (SVDD). With this motivation, a novel training sample reduction algorithm is proposed in this paper that selects the most promising boundary data points as training set. The proposed approach uses the local geometry of the distribution to estimate the farthest boundary points (also known as extreme points). The legitimacy of the proposed algorithm is verified via experiments performed on MNIST, Iris, UCI default credit card, svmguide and Indian Pines datasets. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						268	276		10.1016/j.patrec.2020.01.004													
J								An efficient unsupervised feature selection procedure through feature clustering	PATTERN RECOGNITION LETTERS										Unsupervised feature selection; Feature clustering; Feature redundancy		Due to the scarcity of readily available labels, unsupervised feature selection (UFS) methods are widely adopted in the analysis of high-dimensional data. However, most of the existing UFS methods primarily focus on the significance of features in maintaining the data structure while ignoring the redundancy among features. Moreover, the determination of the proper number of features is another challenge. In this paper, an efficient unsupervised feature selection method through feature clustering (EUFSFC) is proposed to address the redundancy among features, and to determine the size of the final feature subset. The proposed methodology is comprised of two steps: (a) feature cluster analysis, and (b) the selection of the representative features. An extended density-based clustering algorithm is proposed to separate features into an appropriate number of disjoint clusters with no requirement for predefined cluster numbers or radii. The selection of features is performed by choosing the most representative features from those feature clusters. Experiments are conducted to show the effectiveness of the proposed feature selection method. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						277	284		10.1016/j.patrec.2019.12.022													
J								Estimation of Gaussian mixture models via tensor moments with application to online learning	PATTERN RECOGNITION LETTERS										Method of moments; Alternating gradient descent; Online learning; Tensor analysis	NUMERICAL OPTIMIZATION; EM; DECOMPOSITIONS; FACTORIZATION	In this paper, we present an alternating gradient descent algorithm for estimating parameters of a spherical Gaussian mixture model by the method of moments (AGD-MoM). We formulate the problem as a constrained optimisation problem which simultaneously matches the third order moments from the data, represented as a tensor, and the second order moment, which is the empirical covariance matrix. We derive the necessary gradients (and second derivatives), and use them to implement alternating gradient search to estimate the parameters of the model. We show that the proposed method is applicable in both a batch as well as in a streaming (online) setting. Using synthetic and benchmark datasets, we demonstrate empirically that the proposed algorithm outperforms the more classical algorithms like Expectation Maximisation and variational Bayes. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						285	292		10.1016/j.patrec.2020.01.001													
J								A multimodal approach for human activity recognition based on skeleton and RGB data	PATTERN RECOGNITION LETTERS										Human activity recognition; Kinect sensor; Temporal images	ACTIONLET ENSEMBLE; FEATURES	Human action recognition plays a fundamental role in the design of smart solution for home environments, particularly in relation to ambient assisted living applications, where the support of an automated system could improve the quality of life of humans trying to interpret and anticipate user needs, recognizing unusual behaviors or preventing dangerous situations (e.g. falls). In this work the potentialities of the Kinect sensor are fully exploited to design a robust approach for activity recognition combining the analysis of skeleton and RGB data streams. The skeleton representation is designed to capture the most representative body postures, while the temporal evolution of actions is better highlighted by the representation obtained from RGB images. The experimental results confirm that the combination of these two data sources allow to capture highly discriminative features resulting in an approach able to achieve state-of-the-art performance on public benchmarks. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						293	299		10.1016/j.patrec.2020.01.010													
J								Persistence-based resolution-independent meshes of superpixels	PATTERN RECOGNITION LETTERS										Edge detection; Polygonal meshes; Persistent homology	SKELETON	The over-segmentation problem is to split a pixel-based image into a smaller number of superpixels that can be treated as indecompasable regions to speed up higher level image processing such as segmentation or object detection. A traditional superpixel is a potentially disconnected union of square pixels, which can have complicated topology (with holes) and geometry (highly zigzag boundaries). This paper contributes to new resolution-independent superpixels modeled as convex polygons with straightline edges and vertices with real coordinates not restricted to a fixed pixel grid. Any such convex polygon can be rendered at any resolution higher than in original images, hence superpixels are resolution-independent. The key difficulty in obtaining resolution-independent superpixels is to find continuous straight-line edges, while classical edge detection focuses on extracting only discrete edge pixels. The recent Persistent Line Segment Detector (PLSD) avoids intersections and small angles between line segments, which are hard to fix before a proper polygonal mesh can be constructed. The key novelty is an automatic selection of strongest straight-line segments by using the concept of persistence from Topological Data Analysis, which allows to rank segments by their strength. The PLSD performed well in comparison with the only past Line Segment Detector Algorithm (LSDA) on the Berkeley Segmentation Database of 500 real-life images. The PLSD is now extended to the Persistent Resolution-Independent Mesh (PRIM). (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						300	306		10.1016/j.patrec.2020.01.014													
J								Shaping for PET image analysis	PATTERN RECOGNITION LETTERS											METABOLIC TUMOR VOLUME; CONNECTED OPERATORS; HODGKIN-LYMPHOMA; SEGMENTATION	Component-trees constitute an efficient data structure for hierarchical image modeling. In particular they are relevant for processing and analyzing images where the structures of interest correspond either to local maxima or local minima of intensity. This is indeed the case of functional data in medical imaging. This motivates the use of component-tree-based approaches for analyzing Positron Emission Tomography (PET) images in the context of oncology. In this article, we present a simple, yet efficient, methodological framework for PET image analysis based on component-trees. More precisely, we show that the second-order paradigm of shaping, that broadly consists of computing the component-tree of a component-tree, provides a relevant way of generalizing the threshold-based strategies classically used by medical practitioners for handling PET images. In addition, it also allows to embed relevant priors regarding the sought cancer lesions. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						307	313		10.1016/j.patrec.2020.01.017													
J								Approximating lower-star persistence via 2D combinatorial map simplification	PATTERN RECOGNITION LETTERS										Persistent homology computation; 2D combinatorial map; Mesh simplification	COMPLEXES; HOMOLOGY	Filtration simplification consists of simplifying a given filtration while simultaneously controlling the perturbation in the associated persistence diagrams. In this paper, we propose a filtration simplification algorithm for orientable 2-dimensional (2D) manifolds with or without boundary (meshes) represented by 2D combinatorial maps. Given a lower-star filtration of the mesh, faces are added into contiguous clusters according to a "height" function and a parameter epsilon. Faces in the same cluster are merged into a single face, resulting in a lower resolution mesh and a simpler filtration. We prove that the parameter epsilon bounds the perturbation in the original persistence diagrams, and we provide experiments demonstrating the computational advantages of the simplification process. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						314	321		10.1016/j.patrec.2020.01.018													
J								Deep skin detection on low resolution grayscale images	PATTERN RECOGNITION LETTERS										Skin detection; CNN; Transfer learning; Low resolution; Grayscale image; Skin segmentation; SPAD		In this work we present a facial skin detection method, based on a deep learning architecture, that is able to precisely associate a skin label to each pixel of a given image depicting a face. This is an important preliminary step in many applications, such as remote photoplethysmography (rPPG) in which the hearth rate of a subject needs to be estimated analyzing a video of his/her face. The proposed method can detect skin pixels even in low resolution grayscale face images (64 x32 pixel). A dataset is also described and proposed in order to train the deep learning model. Given the small amount of data available, a transfer learning approach is adopted and validated in order to learn to solve the skin detection problem exploiting a colorization network. Qualitative and quantitative results are reported testing the method on different datasets and in presence of general illumination, facial expressions, object occlusions and it is able to work regardless of the gender, age and ethnicity of the subject. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						322	328		10.1016/j.patrec.2019.12.021													
J								Using keyword spotting systems as tools for the transcription of historical handwritten documents: Models and procedures for performance evaluation	PATTERN RECOGNITION LETTERS										Historical documents; Keyword spotting systems; Performance model		The paper proposes a performance model for estimating the improvement of the time needed to transcribe small collections of handwritten documents by using a keyword spotting system (KWS) with respect to the time for manually achieving the transcription. The proposed model assumes that no other information than those obtained from the samples and the KWS system performance on the training set are available, and, depending on them, establishes analytically the condition the performance measures must satisfy to make it profitable to use the system, and, in the affirmative case, estimates the gain and the accuracy of such estimation. The model is complemented by a step-by-step procedure for building the training set, running the KWS on it, estimating the performance parameters on the data set, and eventually estimating the overall improvement and the accuracy of this estimate. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						329	335		10.1016/j.patrec.2020.01.007													
J								Matching ostraca fragments using a siamese neural network	PATTERN RECOGNITION LETTERS												As part of sociological studies, artifacts such as pottery ostraca from Upper Egypt are studied by egyptologists. These pottery pieces are covered with textual inscriptions many of which concern accounting and other economic and administrative matters. Thus, these writings contain pertinent information for understanding ancient Egyptian civilizations. The ostraca fragments retrieved from excavations are available in large quantities, but most of them are not yet analyzed, sorted, and reassembled by the egyptologists. We present a fragment matching approach based on pairwise local assemblies, using a 2D Siamese Neural Network to evaluate matching probabilities. This network is designed to predict simultaneously the existence or absence of a match, and the spatial relationship of one fragment in relation to the other (up, down, left, or right). We trained our deep learning model on a dataset of 6000 patches extracted from ostraca images, and achieved 96% accuracy on a validation dataset of 100 0 patches. Then we propose a pipeline to reconstruct larger ostraca images using step by step pairwise matching, and able to produce a series of image reconstruction proposals. This method is based on the construction of a graph through iterative addition of small fragments. This work is intended as a proof of concept that archaeologists can benefit from automatic processing of their ostraca dataset. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						336	340		10.1016/j.patrec.2020.01.012													
J								An agent-based approach for recommending cultural tours	PATTERN RECOGNITION LETTERS										Multi-agent systems; Recommender systems; Formal planning	HERITAGE	In the Cultural Heritage domain, the introduction of intelligent systems for planning of cultural visit was very appealing. Indeed when people decide to visit museums or archaeological sites, they usually would like to organize and schedule their time in order to fulfill some requirements and to match, as far as possible, their preferences and needs. In this work, we propose a novel methodology integrating recommendation facilities with agent-based planning techniques in order to implement a planner of routes within cultural sites as museums. In particular, the introduced methodology exploits from one hand a user-centered recommendation strategy to suggest the most suitable cultural items with respect to user's preferences and, from the other one, it leverages multi-agents planning methods to generate the related routes consisting of the sequence of steps necessary to reach precise cultural goals depending on the context, by means of a state space exploration. We present a case of study for the proposed methodology and we describe some experimental results on system efficiency. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						341	347		10.1016/j.patrec.2020.01.005													
J								Multi-Scale Weight Sharing Network for Image Recognition	PATTERN RECOGNITION LETTERS										Multi-scale weight sharing; Image recognition; Convolutional neural networks; Image classification		In this paper, we explore the idea of weight sharing over multiple scales in convolutional networks. Inspired by traditional computer vision approaches, we share the weights of convolution kernels over different scales in the same layers of the network. Although multi-scale feature aggregation and sharing inside convolutional networks are common in practice, none of the previous works address the issue of convolutional weight sharing. We evaluate our weight sharing scheme on two heterogeneous image recognition datasets - ImageNet (object recognition) and Places365-Standard (scene classification). With approximately 25% fewer parameters, our shared weight ResNet model provides similar performance compared to baseline ResNets. Shared-weight models are further validated via transfer learning experiments on four additional image recognition datasets - Caltech256 and Stanford 40 Actions (object-centric) and SUN397 and MIT Inddor67 (scene-centric). Experimental results demonstrate significant redundancy in the vanilla implementations of the deeper networks, and also indicate that a shift towards increasing the receptive field per parameter may improve future convolutional network architectures. (c) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				MAR	2020	131						348	354		10.1016/j.patrec.2020.01.011													
J								Image fusion via sparse regularization with non-convex penalties	PATTERN RECOGNITION LETTERS										Sparse approximate solutions; Non-convex penalties; Cost function; Image fusion; Convex optimization; Multispectral image; Noisy image; Multifocus image		The L-1 norm regularized least squares method is often used for finding sparse approximate solutions and is widely used in signal restoration. Basis pursuit denoising (BPD) performs noise reduction in this way. However, the shortcoming of using L-1 norm regularization is the underestimation of the true solution. Recently, a class of non-convex penalties have been proposed to improve this situation. This kind of penalty function is non-convex itself, but preserves the convexity property of the whole cost function. This approach has been confirmed to offer good performance in 1-D signal denoising. This paper demonstrates the aforementioned method to 2-D signals (images) and applies it to multisensor image fusion. The problem is posed as an inverse one and a corresponding cost function is judiciously designed to include two data attachment terms. The whole cost function is proved to be convex upon suitably choosing the non-convex penalty, so that the cost function minimization can be tackled by convex optimization approaches, which comprise simple computations. The performance of the proposed method is benchmarked against a number of state-of-the-art image fusion techniques and superior performance is demonstrated both visually and in terms of various assessment measures. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						355	360		10.1016/j.patrec.2020.01.020													
J								Discovering Leonardo with artificial intelligence and holograms: A user study	PATTERN RECOGNITION LETTERS										Holograms; Conversational systems; Artificial intelligence; Touchless interfaces; Cultural heritage	VIRTUAL-REALITY; VISITORS	Cutting-edge visualization and interaction technologies are increasingly used in museum exhibitions, providing novel ways to engage visitors and enhance their cultural experience. Existing applications are commonly built upon a single technology, focusing on visualization, motion or verbal interaction (e.g., high-resolution projections, gesture interfaces, chatbots). This aspect limits their potential, since museums are highly heterogeneous in terms of visitors profiles and interests, requiring multi-channel, customizable interaction modalities. To this aim, this work describes and evaluates an artificial intelligence powered, interactive holographic stand aimed at describing Leonardo Da Vinci's art. This system provides the users with accurate 3D representations of Leonardo's machines, which can be interactively manipulated through a touchless user interface. It is also able to dialog with the users in natural language about Leonardo's art, while keeping the context of conversation and interactions. Furthermore, the results of a large user study, carried out during art and tech exhibitions, are presented and discussed. The goal was to assess how users of different ages and interests perceive, understand and explore cultural objects when holograms and artificial intelligence are used as instruments of knowledge and analysis. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						361	367		10.1016/j.patrec.2020.01.006													
J								End-to-end video subtitle recognition via a deep Residual Neural Network	PATTERN RECOGNITION LETTERS										Video subtitle recognition; Residual network; Gated recurrent unit; Connectionist temporal classification		Video subtitle recognition can substantially facilitate a wide range of applications like automatic video retrieval and summarization. However, current methods face great challenges in video subtitle recognition, due to complex backgrounds, diverse fonts, and low contrast between texts and backgrounds. In this paper, we propose an end-to-end pipeline for recognizing subtitles of video. Connectionist Text Proposal Network (CTPN) is utilized for video subtitle detection, while Residual Network (ResNet), Gated Recurrent Unit (GRU) and Connectionist Temporal Classification (CTC) are used to recognize Chinese and English subtitles in video images. Specifically, the subtitle area in video images is firstly located via the CTPN method. Afterwards, the detected subtitle area is inputted into the ResNet for extracting the feature sequences. Next, a bidirectional GRU layer is employed to model the feature sequences. Finally, CTC is adopted to calculate the loss and output the final result. On two public datasets ICDAR2003 and ICDAR2013, the proposed method can get better performance with 92.3% and 89.2% recognition accuracy in all of the current methods. In addition, experiments on real video subtitle have also proved that the proposed method achieves the best performance in the state-of-art methods. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						368	375		10.1016/j.patrec.2020.01.019													
J								DV-Net: Dual-view network for 3D reconstruction by fusing multiple sets of gated control point clouds	PATTERN RECOGNITION LETTERS										3D reconstruction; Deep learning; Point cloud fusion; Multiple views		Deep learning for 3D reconstruction have just shown some promising advantanges, where 3D shapes can be predicted from a single RGB image. However, such works are often limited by single feature cue, which does not capture the 3D shape of objects well. To address this problem, an end-to-end 3D reconstruction approach that predicts 3D point cloud from dual-view RGB images is proposed in this paper. It consists of several processing parts. A dual-view 3D reconstruction network is proposed for 3D reconstruction, which predicts object's point clouds by exploiting two RGB images with different views, and avoids the limitation of single feature cue. Another structure feature learning network is performed to extract the structure features with stronger representation ability from point clouds. A gated control network for data fusion is proposed to gather point clouds. It takes two sets of point clouds with different views as input and fuses them. The proposed approach is thoroughly evaluated with extensive experiments on the widely-used ShapeNet dataset. Both the qualitative results and quantitative analysis demonstrate that this method not only captures the detailed geometric structures of 3D shapes for different object categories with complex topologies, but also achieves state-of-the-art performance. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAR	2020	131						376	382		10.1016/j.patrec.2020.02.001													
