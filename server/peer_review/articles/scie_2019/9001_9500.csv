PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Dangerous goods detection based on transfer learning in X-ray images	NEURAL COMPUTING & APPLICATIONS										Transfer learning; Convolutional neural networks; SSD; X-ray security detection	OBJECT CLASSIFICATION; RECOGNITION	Computer vision technology is used to analyze X-ray images and detect dangerous goods in the process of logistics and express delivery. It is a security technology which can reduce labor strength and improve working efficiency. At present, there are many excellent detection models and methods in the field of object detection for visible light images, such as R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD. These deep neural network-based detection methods achieved excellent performance on ImageNet. The training of object detection models on X-ray image datasets for dangerous goods detection is the focus of research in the field. Due to practical reasons, it is difficult to collect a comprehensive image dataset of dangerous goods (positive samples). In order to overcome this problem, this paper uses a multi-task transfer learning method on the basis of classification task and location search task on SSD network. The research in this paper focuses on adding additional convolutional layers in the SSD network to re-learn the knowledge of the model learned from the source domain. Experiments show that compared with the traditional method of fine-tuning, this method has better transfer learning ability on SSD network. This method was used to perform experiments in SSD300 on the image datasets screened from GDXray and achieved a mean average precision (mAP) of 0.915.																	0941-0643	1433-3058				JUN	2020	32	12			SI		8711	8724		10.1007/s00521-019-04360-0													
J								Adaptive filter design for active noise cancellation using recurrent type-2 fuzzy brain emotional learning neural network	NEURAL COMPUTING & APPLICATIONS										Active noise cancellation; Type-2 fuzzy system; Brain emotional learning network; Recurrent neural network	CONTROL-SYSTEM DESIGN; CONTROLLER; MODEL	This article aims to develop a more efficient adaptive filter for the active noise cancellation (ANC). A novel recurrent interval type-2 fuzzy brain emotional learning filter (RT2BELF) is proposed for achieving favourable filtering performance. The ANC is a method to eliminate noise by creating an anti-noise signal which has the same magnitude but opposite phase with the unwanted noise. In order to adapt to the change of the noise, the parameters for the RIT2BELF are online updated based on the adaptive laws, which are derived by the steepest descent gradient approach. The performance of the proposed ANC design method is successfully demonstrated based on numerical simulation results in the real signals. Finally, the superiority of the proposed method is confirmed by the results comparison with some noise cancellation methods.																	0941-0643	1433-3058				JUN	2020	32	12			SI		8725	8734		10.1007/s00521-019-04366-8													
J								Asymptotic tracking control of uncertain nonholonomic wheeled mobile robot with actuator saturation and external disturbances	NEURAL COMPUTING & APPLICATIONS										Asymptotic tracking; Adaptive neural controller; Nonholonomic wheeled mobile robot; Actuator saturation	OUTPUT-FEEDBACK CONTROL; TRAJECTORY TRACKING; NONLINEAR-SYSTEMS; STABILIZATION; DESIGN	This paper deals with the asymptotic tracking control for the uncertain nonholonomic wheeled mobile robot system subjected to actuator saturation and external disturbances simultaneously. A dynamic system is introduced to deal with the actuator saturation, radial basis function neural networks (RBF NNs) are employed to approximate the unknown closed-loop system dynamics, and an adaptive sliding mode feedback term is used to compensate for the approximation error as well as external disturbances. Consequently, a novel adaptive neural controller is designed to guarantee the stability of the closed-loop system and the asymptotic convergence of tracking errors. Meanwhile, the convergence of NN weights is verified, which means that accurate approximation of the unknown closed-loop system dynamics can be obtained and the constant weights can be reused to perform the same or similar control tasks. Finally, simulation studies illustrate the effectiveness of the proposed scheme.																	0941-0643	1433-3058				JUN	2020	32	12			SI		8735	8745		10.1007/s00521-019-04373-9													
J								Accurate ride comfort estimation combining accelerometer measurements, anthropometric data and neural networks	NEURAL COMPUTING & APPLICATIONS										Ride; Comfort perception; Vibration; Neural networks	DRIVER SITTING COMFORT; SEAT COMFORT; VIBRATION; DISCOMFORT; PREDICTION; ABSORPTION; ALGORITHM; FRAMEWORK; DESIGN; MODEL	Ride comfort can heavily influence user experience and therefore comprises one of the most important vehicle design targets. Although ride comfort has been heavily researched, there is still no definite solution to its accurate estimation. This can be attributed, to a large extent, to the subjective nature of the problem. Aim of this study was to explore the use of neural networks for the accurate estimation of ride comfort by combining anthropometric data and acceleration measurements. Different acceleration inputs, neural network architectures, training algorithms and objective functions were systematically investigated, and optimal parameters were derived. New insight into the influence of anthropometric data on ride comfort has been gained. The results indicate that the proposed method improves the accuracy of subjective ride comfort estimation compared to current standards. Neural networks were trained using data derived from a range of field trials involving ten participants, on public roads and controlled environment. A clustering and sensitivity analysis complements the study and identifies the most important factors influencing subjective ride comfort evaluation.																	0941-0643	1433-3058				JUN	2020	32	12			SI		8747	8762		10.1007/s00521-019-04351-1													
J								Enhancement of conformational B-cell epitope prediction using CluSMOTE	PEERJ COMPUTER SCIENCE										Cluster-based undersampling; SMOTE; Class imbalance; Hybrid sampling; Hierarchical DBSCAN; Vaccine design	AMINO-ACID; SEQUENCES; DATABASE; SMOTE	Background. A conformational B-cell epitope is one of the main components of vaccine design. It contains separate segments in its sequence, which are spatially close in the antigen chain. The availability of Ag-Ab complex data on the Protein Data Bank allows for the development predictive methods. Several epitope prediction models also have been developed, including learning-based methods. However, the performance of the model is still not optimum. The main problem in learning-based prediction models is class imbalance. Methods. This study proposes CluSMOTE, which is a combination of a clusterbased undersampling method and Synthetic Minority Oversampling Technique. The approach is used to generate other sample data to ensure that the dataset of the conformational epitope is balanced. The Hierarchical DBSCAN algorithm is performed to identify the cluster in the majority class. Some of the randomly selected data is taken from each cluster, considering the oversampling degree, and combined with the minority class data. The balance data is utilized as the training dataset to develop a conformational epitope prediction. Furthermore, two binary classification methods, Support Vector Machine and Decision Tree, are separately used to develop model prediction and to evaluate the performance of CluSMOTE in predicting conformational B-cell epitope. The experiment is focused on determining the best parameter for optimal CluSMOTE. Two independent datasets are used to compare the proposed prediction model with state of the art methods. The first and the second datasets represent the general protein and the glycoprotein antigens respectively. Result. The experimental result shows that CluSMOTE Decision Tree outperformed the Support Vector Machine in terms of AUC and Gmean as performance measurements. The mean AUC of CluSMOTE Decision Tree in the Kringelum and the SEPPA 3 test sets are 0.83 and 0.766, respectively. This shows that CluSMOTE Decision Tree is better than other methods in the general protein antigen, though comparable with SEPPA 3 in the glycoprotein antigen.																	2376-5992					JUN 1	2020									e275	10.7717/peerj-cs.275													
J								Augmented learning, smart glasses and knowing how	AI & SOCIETY										Higher education; Augmented reality; Knowing-How; Smart glasses; Assistive technology; STEM	COGNITIVE LOAD; CONTEXT	While recent studies suggest that augmented learning employing smart glasses (ALSG) increases overall learning performance, in this paper we are more interested in the question which repercussions ALSG will have on the type of knowledge that is acquired. Drawing from the theoretical discussion within epistemology about the differences between Knowledge-How and Knowledge-That, we will argue that ALSG furthers understanding as a series of epistemic and non-epistemic Knowing-Hows. Focusing on academic knowledge acquisition, especially with respect to early curriculum experiments in various STEM disciplines as investigated by the BmBF "Be-Greifen" project, we take the Be-Greifen holo.lab setup as an example for showing that ALSG shifts the learning focus from propositional knowledge to epistemic competencies, which can be differentiated as "grasping", "wielding", and "transferring".																	0951-5666	1435-5655				JUN	2020	35	2					297	308		10.1007/s00146-019-00881-3													
J								Black-box artificial intelligence: an epistemological and critical analysis	AI & SOCIETY										Artificial intelligence; Philosophy of technology; Machine learning; XAI; Deep neural networks; GDPR; Instrumental reason		The artificial intelligence models with machine learning that exhibit the best predictive accuracy, and therefore, the most powerful ones, are, paradoxically, those with the most opaque black-box architectures. At the same time, the unstoppable computerization of advanced industrial societies demands the use of these machines in a growing number of domains. The conjunction of both phenomena gives rise to a control problem on AI that in this paper we analyze by dividing the issue into two. First, we carry out an epistemological examination of the AI's opacity in light of the latest techniques to remedy it. And second, we evaluate the rationality of delegating tasks in opaque agents.																	0951-5666	1435-5655				JUN	2020	35	2					309	317		10.1007/s00146-019-00888-w													
J								Risk management standards and the active management of malicious intent in artificial superintelligence	AI & SOCIETY										Risk; ISO 31000; Threat; Existential; Superintelligence; Artificial intelligence; Artificial stupidity; Criminal threat management	SINGULARITY	The likely near future creation of artificial superintelligence carries significant risks to humanity. These risks are difficult to conceptualise and quantify, but malicious use of existing artificial intelligence by criminals and state actors is already occurring and poses risks to digital security, physical security and integrity of political systems. These risks will increase as artificial intelligence moves closer to superintelligence. While there is little research on risk management tools used in artificial intelligence development, the current global standard for risk management, ISO 31000:2018, is likely used extensively by developers of artificial intelligence technologies. This paper argues that risk management has a common set of vulnerabilities when applied to artificial superintelligence which cannot be resolved within the existing framework and alternative approaches must be developed. Some vulnerabilities are similar to issues posed by malicious threat actors such as professional criminals and terrorists. Like these malicious actors, artificial superintelligence will be capable of rendering mitigation ineffective by working against countermeasures or attacking in ways not anticipated by the risk management process. Criminal threat management recognises this vulnerability and seeks to guide and block the intent of malicious threat actors as an alternative to risk management. An artificial intelligence treachery threat model that acknowledges the failings of risk management and leverages the concepts of criminal threat management and artificial stupidity is proposed. This model identifies emergent malicious behaviour and allows intervention against negative outcomes at the moment of artificial intelligence's greatest vulnerability.																	0951-5666	1435-5655				JUN	2020	35	2					319	328		10.1007/s00146-019-00890-2													
J								God-like robots: the semantic overlap between representation of divine and artificial entities	AI & SOCIETY										Artificial intelligence; Robots; Gods; Semantic representation; Perception of robots	INDIVIDUAL-DIFFERENCES; SPREADING ACTIVATION; TECHNOLOGY READINESS; SOCIAL COGNITION; ATTITUDES; MACHINES; IMPLICIT; MIND; CATEGORIZATION; DEHUMANIZATION	Artificial intelligence and robots may progressively take a more and more prominent place in our daily environment. Interestingly, in the study of how humans perceive these artificial entities, science has mainly taken an anthropocentric perspective (i.e., how distant from humans are these agents). Considering people's fears and expectations from robots and artificial intelligence, they tend to be simultaneously afraid and allured to them, much as they would be to the conceptualisations related to the divine entities (e.g., gods). In two experiments, we investigated the proximity of representation between artificial entities (i.e., artificial intelligence and robots), divine entities and natural entities (i.e., humans and other animals) at both an explicit (Study 1) and an implicit level (Study 2). In the first study, participants evaluated these entities explicitly on positive and negative attitudes. Hierarchical clustering analysis showed that participants' representation of artificial intelligence, robots and divine entities were similar, while the representation of humans tended to be associated with that of animals. In the second study, participants carried out a word/non-word decision task including religious semantic-related words and neutral words after the presentation of a masked prime referring to divine entities, artificial entities and natural entities (or a control prime). Results showed that after divine and artificial entity primes, participants were faster to identify religious words as words compared to neutral words arguing for a semantic activation. We conclude that people make sense of the new entities by relying on already familiar entities and in the case of artificial intelligence and robots, people appear to draw parallels to divine entities.																	0951-5666	1435-5655				JUN	2020	35	2					329	341		10.1007/s00146-019-00902-1													
J								Legal personhood for artificial intelligence: citizenship as the exception to the rule	AI & SOCIETY										Artificial intelligence; Bioethics; Legal personhood; Technoethics	RIGHTS	The concept of artificial intelligence is not new nor is the notion that it should be granted legal protections given its influence on human activity. What is new, on a relative scale, is the notion that artificial intelligence can possess citizenship-a concept reserved only for humans, as it presupposes the idea of possessing civil duties and protections. Where there are several decades' worth of writing on the concept of the legal status of computational artificial artefacts in the USA and elsewhere, it is surprising that law makers internationally have come to a standstill to protect our silicon brainchildren. In this essay, it will be assumed that future artificial entities, such as Sophia the Robot, will be granted citizenship on an international scale. With this assumption, an analysis of rights will be made with respect to the needs of a non-biological intelligence possessing legal and civic duties akin to those possessed by humanity today. This essay does not present a full set of rights for artificial intelligence-instead, it aims to provide international jurisprudence evidencealiunde ab extra de lege latafor any future measures made to protect non-biological intelligence.																	0951-5666	1435-5655				JUN	2020	35	2					343	354		10.1007/s00146-019-00897-9													
J								15 challenges for AI: or what AI (currently) can't do	AI & SOCIETY										Artificial intelligence; Machine learning; Capabilities; Data; Technology development	GO	The current "AI Summer" is marked by scientific breakthroughs and economic successes in the fields of research, development, and application of systems with artificial intelligence. But, aside from the great hopes and promises associated with artificial intelligence, there are a number of challenges, shortcomings and even limitations of the technology. For one, these challenges arise from methodological and epistemological misconceptions about the capabilities of artificial intelligence. Secondly, they result from restrictions of the social context in which the development of applications of machine learning is embedded. And third, they are a consequence of current technical limitations in the development and use of artificial intelligence. The paper intends to provide an overview of current challenges which the research and development of applications in the field of artificial intelligence and machine learning have to face, whereas all three mentioned areas are to be further explored in this paper.																	0951-5666	1435-5655				JUN	2020	35	2					355	365		10.1007/s00146-019-00886-y													
J								The race for an artificial general intelligence: implications for public policy	AI & SOCIETY										Artificial intelligence; Innovation; Technology; Public policy		An arms race for an artificial general intelligence (AGI) would be detrimental for and even pose an existential threat to humanity if it results in an unfriendly AGI. In this paper, an all-pay contest model is developed to derive implications for public policy to avoid such an outcome. It is established that, in a winner-takes-all race, where players must invest in R&D, only the most competitive teams will participate. Thus, given the difficulty of AGI, the number of competing teams is unlikely ever to be very large. It is also established that the intention of teams competing in an AGI race, as well as the possibility of an intermediate outcome (prize), is important. The possibility of an intermediate prize will raise the probability of finding the dominant AGI application and, hence, will make public control more urgent. It is recommended that the danger of an unfriendly AGI can be reduced by taxing AI and using public procurement. This would reduce the pay-off of contestants, raise the amount of R&D needed to compete, and coordinate and incentivize co-operation. This will help to alleviate the control and political problems in AI. Future research is needed to elaborate the design of systems of public procurement of AI innovation and for appropriately adjusting the legal frameworks underpinning high-tech innovation, in particular dealing with patenting by AI.																	0951-5666	1435-5655				JUN	2020	35	2					367	379		10.1007/s00146-019-00887-x													
J								Do people with social anxiety feel anxious about interacting with a robot?	AI & SOCIETY										Communication robots; Social anxiety; Anticipatory anxiety		To investigate whether people with social anxiety have less actual and "anticipatory" anxiety when interacting with a robot compared to interacting with a person, we conducted a 2 x 2 psychological experiment with two factors: social anxiety and interaction partner (a human confederate and a robot). The experiment was conducted in a counseling setting where a participant played the role of a client and the robot or the confederate played the role of a counselor. First, we measured the participants' social anxiety using the Social Avoidance and Distress Scale, after which, we measured their anxiety at two specific moments: "anticipatory anxiety" was measured after they knew that they would be interacting with a robot or a human confederate, and actual anxiety was measured after they actually interacted with the robot or confederate. Measurements were performed using the Profile of Mood States and the State-Trait Anxiety Inventory. The results indicated that participants with higher social anxiety tended to feel less "anticipatory anxiety" and tension when they knew that they would be interacting with robots compared with humans. Moreover, we found that interaction with a robot elicited less tension compared with interaction with a person regardless of the level of social anxiety.																	0951-5666	1435-5655				JUN	2020	35	2					381	390		10.1007/s00146-019-00889-9													
J								AI and the path to envelopment: knowledge as a first step towards the responsible regulation and use of AI-powered machines	AI & SOCIETY										AI ethics; Machine ethics; Meaningful human control; Robot ethics		With Artificial Intelligence (AI) entering our lives in novel ways-both known and unknown to us-there is both the enhancement of existing ethical issues associated with AI as well as the rise of new ethical issues. There is much focus on opening up the 'black box' of modern machine-learning algorithms to understand the reasoning behind their decisions-especially morally salient decisions. However, some applications of AI which are no doubt beneficial to society rely upon these black boxes. Rather than requiring algorithms to be transparent we should focus on constraining AI and those machines powered by AI within microenvironments-both physical and virtual-which allow these machines to realize their function whilst preventing harm to humans. In the field of robotics this is called 'envelopment'. However, to put an 'envelope' around AI-powered machines we need to know some basic things about them which we are often in the dark about. The properties we need to know are the: training data, inputs, functions, outputs, and boundaries. This knowledge is a necessary first step towards the envelopment of AI-powered machines. It is only with this knowledge that we can responsibly regulate, use, and live in a world populated by these machines.																	0951-5666	1435-5655				JUN	2020	35	2					391	400		10.1007/s00146-019-00891-1													
J								Smart Sankey picturization for energy management systems in India	AI & SOCIETY										Sankey diagrams; Energy Management; Energy visualization technique; Mobile telephony	DIAGRAMS; NEXUS	India's energy demand is predicted to rise by 135% within a span of 20 years. Coping up with surging energy demands requires several reforms in both renewable and non-renewable sectors. Factors such as rising population, reduction in the cost of renewable energy technology and their effect on the nation's GDP, can make policy making a herculean task and the justification for such policies, quite opaque to the public. Artificial Intelligence (AI) technology can help decision makers to quickly draw conclusions from voluminous datasets under different heads. However, AI results need to be post-processed so that they are easily understood by the layperson. This paper focuses on using Sankey diagrams as a post-processing tool for AI systems. Policy formulation often requires an overall assessment of energy sources, production pathways, end used destinations and wastage encountered-these can be easily visualized with a Sankey diagram. For this work, a Sankey protocol was written out in a form where a prompt asks the user to supply information through a spreadsheet interfaced with a customized mobile app-a first anywhere in the subcontinent. India's mobile sector is vibrant, and the paper presents a enabling the user to have a handle on the dynamics of energy distribution locally. The app gives an instant feel of the apportioned energy across grids, the projected changes for the next decade, the efficiency and feasibility of each sector and lastly a telling visual representation showing the main strengths and weaknesses in the energy sector.																	0951-5666	1435-5655				JUN	2020	35	2					401	407		10.1007/s00146-019-00900-3													
J								Collective bread diaries: cultural identities in an artificial intelligence framework	AI & SOCIETY										Bread; Cultural identity; Artificial intelligence; MTurk; Drawing; Arabic language; Egyptian culture; Sustenance; Protest		The complex relationship between the current advancement of technology, including the wide scope of settings at which machinery plays substantial roles, and the cultural, historical, and political realities that have long existed across the history of mankind, is one that deserves absolute attention and exploration. This interconnection has been investigated in light of bread, and the meaning it signifies to people from all over the world. Drawing on the commonly unnoticed value of bread, and the everlasting impregnable imprint it has always had on revolutions throughout history, "Collective Bread Diaries" came to being as an interactive art project, employing the artificial intelligence of the MTurk platform in its investigation of "bread" as one's peculiar voice and political statement. The author amongst other participants was granted the opportunity to draw and share personal visual representations of bread, eventually forming an array of visual diaries, each peculiar to its creator and each reflecting the cultural significance of the place from which it originated. The results are exceptionally reproduced drawings by a machine, with no apparent threads to culture, tradition, or history, emphasizes the conscious perception of one's distinct identity. A number of 100 distinct drawings were machine-generated featuring various bread types. The process eventually investigates one's perception of cultural identity, by exploring the cultural, socio-political, and religious threads that have long been weaved into the definition of "bread" across history.																	0951-5666	1435-5655				JUN	2020	35	2					409	416		10.1007/s00146-019-00882-2													
J								Culture, the process of knowledge, perception of the world and emergence of AI	AI & SOCIETY										Technology; Industrial process; Cultural values; "Eastern" and "Western" cultures; Perception; Cognition and knowledge; Language; Algorithms; AI; "Self-reflection"; Ethical decisions and judgements; Nature and human nature		Considering the technological development today, we are facing an emerging crisis. We are in the midst of a scientific revolution, which promises to radically change not only the way we live and work-but beyond that challenge the stability of the very foundations of our civilization and the international political order. All our attention and effort is thus focused on cushioning its impacts on life and society. Looking back in history, it would be pertinent to ask whether this process is a "quasi" natural event destined to continue its path which forebodes severe ethical, social and political repercussions, or is it the outcome of a particular socio-cultural value system? What then are the different value systems and mind-sets? How do these impact on our perception of the world, our cognition systems and our self-perception? At the core of this revolution are the recent technological developments in information and data processing and the creation of AI and the role of algorithms. Are algorithms quasi "independent" artificial languages for assessing situations, solving problems and taking decisions on fundamental social issues? Or do they rest on the fundament of natural language? What ethical and moral code do they follow? What drivers underlie this technological process? Are these culture specific or universal? Could a change of "cultural perspective" provide us with new insights to steer the process and change its course?																	0951-5666	1435-5655				JUN	2020	35	2					417	430		10.1007/s00146-019-00885-z													
J								The greatest epistemological externalisation: reflecting on the puzzling direction we are heading to through algorithmic automatisation	AI & SOCIETY										Epistemological externalisation; Algorithm; Future of humankind		The aim of the article is reflecting on a fundamental epistemological issue which characterises our present technological progress: where are we heading to, as humankind, while we are progressively externalising our most crucial decision processes towards algorithms, from which decisive data, coming from human experience and mind (including the very experience of human abilities), are left out? By reflecting on some cases, I shall try to argue that the most puzzling issue which engineers and philosophers should be aware that they have to jointly challenge may be that what we are actually doing through algorithmic automatisation is developing a novel human condition, according to which: (1) we are progressively thinking that algorithmic abstraction is always better than mental abstraction, because, at least in the Western culture, we come from a history of a progressive restriction of the best use of our minds to the realm of rationality, first, then to the realm of computation, second, and then to the realm of algorithmic automatisation, third, which finally exceeds our minds and (2) in doing so, we are progressively externalising not only human contents, but also human abilities, i.e., we are progressively atrophying ourselves, by becoming creatures who are progressively delegating the core of their very essence, which has always included the epistemological ability, together with the ethical courage, of making complex decisions on both our lives and the others' lives.																	0951-5666	1435-5655				JUN	2020	35	2					431	440		10.1007/s00146-019-00905-y													
J								Classical AI linguistic understanding and the insoluble Cartesian problem	AI & SOCIETY										Descartes; AI; Meaning; Awareness; Turing test		This paper examines an insoluble Cartesian problem for classical AI, namely, how linguistic understanding involves knowledge and awareness ofu'smeaning, a cognitive process that is irreducible to algorithms. As analyzed, Descartes' view about reason and intelligence has paradoxically encouraged certain classical AI researchers to suppose that linguistic understanding suffices for machine intelligence. Several advocates of the Turing Test, for example, assume that linguistic understanding only comprises computational processes which can be recursively decomposed into algorithmic mechanisms. Against this background, in the first section, I explain Descartes' view about language and mind. To show that Turing bites the bullet with his imitation game and in the second section I analyze this method to assess intelligence. Then, in the third section, I elaborate on Schank and Abelsons' Script Applier Mechanism (SAM, hereby), which supposedly casts doubt on Descartes' denial that machines can think. Finally, in the fourth section, I explore a challenge that any algorithmic decomposition of linguistic understanding faces. This challenge, I argue, is the core of the Cartesian problem: knowledge and awareness of meaning require a first-person viewpoint which is irreducible to the decomposition of algorithmic mechanisms.																	0951-5666	1435-5655				JUN	2020	35	2					441	450		10.1007/s00146-019-00906-x													
J								Movie films consumption in Brazil: an analysis of support vector machine classification	AI & SOCIETY										Film at theaters; SVM; LDA; KDD; Classification; Consumers; Individual data		We employ the support vector machine (SVM) classifier, over different types of kernels, to investigate whether observable variables of individuals and their household information are able to describe their consumption decision of film at theaters in Brazil. Using a very big dataset of 340,000 individuals living in metropolitan areas of a whole large developing economy, we performed a Knowledge Discovery in Databases to classify the film consumers, which results in 80% instances correctly classified. To reduce the degrees of freedom for SVM and to learn the more important determinants of film consumption, we apply the Linear Discriminant Analysis that allows us to identify the key determinants of this consumption. The main individual characteristics are age, education (that merges to be a student), income, and preferences for cultural goods. Regarding the main geographic characteristics, these are the timing of sample, population concentration, and supply of movie theaters. The results point to an ineffective policy for the sector at the time investigated.																	0951-5666	1435-5655				JUN	2020	35	2					451	457		10.1007/s00146-019-00899-7													
J								E-MIIM: an ensemble-learning-based context-aware mobile telephony model for intelligent interruption management	AI & SOCIETY										Mobile interruptions; Machine learning; Classification; Decision tree; Ensemble learning; Random forest; User behavior modeling; Context-aware computing; Personalization; Mobile service; Intelligent systems; IoT applications		Nowadays, mobile telephony interruptions in our daily life activities are common because of the inappropriate ringing notifications of incoming phone calls in different contexts. Such interruptions may impact on the work attention not only for the mobile phone owners, but also for the surrounding people. Decision tree is the most popularmachine-learningclassification technique that is used in existing context-aware mobile intelligent interruption management (MIIM) model to overcome such issues. However, a single-decision tree-based context-aware model may cause over-fitting problem and thusdecreasethe prediction accuracy of the inferred model. Therefore, in this paper, we propose anensemblemachine-learning-based context-aware mobile telephony model for the purpose of intelligent interruption management by taking into account multi-dimensional contexts and name it "E-MIIM". The experimental results on individuals' real-life mobile telephony data sets show that our E-MIIM model is more effective and outperforms existing MIIM model for predicting and managing individual's mobile telephony interruptions based on their relevant contextual information.																	0951-5666	1435-5655				JUN	2020	35	2					459	467		10.1007/s00146-019-00898-8													
J								Presenting a hybrid model in social networks recommendation system architecture development	AI & SOCIETY										Recommendation systems; Collaborative filtering; Artificial neural network; Fuzzy logic; Supply-chain management; Social networks		There are many studies conducted on recommendation systems, most of which are focused on recommending items to users and vice versa. Nowadays, social networks are complicated due to carrying vast arrays of data about individuals and organizations. In today's competitive environment, companies face two significant problems: supplying resources and attracting new customers. Even the concept of supply-chain management in a virtual environment is changed. In this article, we propose a new and innovative combination approach to recommend organizational people in social networks based on organizational communication and SCM. The proposed approach uses a hybrid strategy that combines basic collaborative filtering and demographic recommendation systems, using data mining, artificial neural networks, and fuzzy techniques. The results of experiments and evaluations based on a real dataset collected from the LinkedIn social network showed that the hybrid recommendation system has higher accuracy and speed than other essential methods, even substantially has eliminated the fundamental problems with such systems, such as cold start, scalability, diversity, and serendipity.																	0951-5666	1435-5655				JUN	2020	35	2					469	483		10.1007/s00146-019-00893-z													
J								One robot doesn't fit all: aligning social robot appearance and job suitability from a Middle Eastern perspective	AI & SOCIETY										Social robotics; Human-robot interaction; Jobs; Appearance; Middle East	ANTHROPOMORPHISM; ATTITUDES; SAFETY; JAPAN	Social robots are expected to take over a significant number of jobs in the coming decades. The present research provides the first systematic evaluation of occupation suitability of existing social robots based on user perception derived classification of them. The study was conducted in the Middle East since the views of this region are rarely considered in human-robot interaction research, although the region is poised to increasingly adopt the use of robots. Laboratory-based experimental data revealed that a robot's appearance plays an important role in the perception of its capabilities and preference for it to perform a particular job. Participants showed a preference for machine-like robots to perform dull and dirty occupations and humanoids, but not androids, to perform jobs requiring extensive social interaction with humans. However, other aspects of appearance than morphology determine whether a robot is preferred for a job irrespective of its perceived capability to do it.																	0951-5666	1435-5655				JUN	2020	35	2					485	500		10.1007/s00146-019-00895-x													
J								Can artificial intelligency revolutionize drug discovery?	AI & SOCIETY										Artificial intelligence; Chemistry; Drug discovery; Deep learning machine		Artificial intelligency can bring speed and reliability to drug discovery process. It represents an additional intelligence, which in any case can replace the strategic and logic creative insight of the medicinal chemist who remains the architect and molecule master designer. In terms of drug design, artificial intelligency, deep learning machines, and other revolutionary technologies will match with the medicinal chemist's natural intelligency, but for sure never go beyond. This manuscript tries to assess the impact of the artificial intelligency on drug discovery today.																	0951-5666	1435-5655				JUN	2020	35	2					501	504		10.1007/s00146-019-00892-0													
J								Why friendly AIs won't bethatfriendly: a friendly reply to Muehlhauser and Bostrom	AI & SOCIETY										Machine ethics; Friendly AI; Superintelligence; Counterfactual reasoning		In "Why We Need Friendly AI", Luke Muehlhauser and Nick Bostrom propose that for our species to survive the impending rise of superintelligent AIs, we need to ensure that they would be human-friendly. This discussion note offers a more natural but bleaker outlook: that in the end, if these AIs do arise, they won't bethatfriendly.																	0951-5666	1435-5655				JUN	2020	35	2					505	507		10.1007/s00146-019-00903-0													
J								Multioracle Coevolutionary Learning of Requirements Specifications from Examples in On-The-Fly Markets	EVOLUTIONARY COMPUTATION										Multiobjective optimization; proactive learning; multioracle; coevolution; search-based software engineering; requirements specification; grammatical inference	EVOLUTIONARY APPROACH; SOFTWARE; DFA	In software engineering, the imprecise requirements of a user are transformed to a formal requirements specification during the requirements elicitation process. This process is usually guided by requirements engineers interviewing the user. We want to partially automate this first step of the software engineering process in order to enable users to specify a desired software system on their own. With our approach, users are only asked to provide exemplary behavioral descriptions. The problem of synthesizing a requirements specification from examples can partially be reduced to the problem of grammatical inference, to which we apply an active coevolutionary learning approach. However, this approach would usually require many feedback queries to be sent to the user. In this work, we extend and generalize our active learning approach to receive knowledge from multiple oracles, also known as proactive learning. The ``user oracle'' represents input received from the user and the "knowledge oracle" represents available, formalized domain knowledge. We call our two-oracle approach the "first apply knowledge then query" (FAKT/Q) algorithm. We compare FAKT/Q to the active learning approach and provide an extensive benchmark evaluation. As result we find that the number of required user queries is reduced and the inference process is sped up significantly. Finally, with so-called On-The-Fly Markets, we present a motivation and an application of our approach where such knowledge is available.																	1063-6560	1530-9304				JUN	2020	28	2					165	193		10.1162/evco_a_00266													
J								Automatically Designing State-of-the-Art Multi- and Many-Objective Evolutionary Algorithms	EVOLUTIONARY COMPUTATION										Multiobjective optimization; evolutionary algorithms; automatic algorithm design	DIFFERENTIAL EVOLUTION; OPTIMIZATION; PERFORMANCE; SELECTION; MOEA/D	A recent comparison of well-established multiobjective evolutionary algorithms (MOEAs) has helped better identify the current state-of-the-art by considering (i) parameter tuning through automatic configuration, (ii) a wide range of different setups, and (iii) various performance metrics. Here, we automatically devise MOEAs with verified state-of-the-art performance for multi- and many-objective continuous optimization. Our work is based on two main considerations. The first is that high-performing algorithms can be obtained from a configurable algorithmic framework in an automated way. The second is that multiple performance metrics may be required to guide this automatic design process. In the first part of this work, we extend our previously proposed algorithmic framework, increasing the number of MOEAs, underlying evolutionary algorithms, and search paradigms that it comprises. These components can be combined following a general MOEA template, and an automatic configuration method is used to instantiate high-performing MOEA designs that optimize a given performance metric and present state-of-the-art performance. In the second part, we propose a multiobjective formulation for the automatic MOEA design, which proves critical for the context of many-objective optimization due to the disagreement of established performance metrics. Our proposed formulation leads to an automatically designed MOEA that presents state-of-the-art performance according to a set of metrics, rather than a single one.																	1063-6560	1530-9304				JUN	2020	28	2					195	226		10.1162/evco_a_00263													
J								What Weights Work for You? Adapting Weights for Any Pareto Front Shape in Decomposition-Based Evolutionary Multiobjective Optimisation	EVOLUTIONARY COMPUTATION										Multiobjective optimisation; many-objective optimisation; evolutionary algorithms; decomposition-based EMO; weight adaptation	MANY-OBJECTIVE OPTIMIZATION; NONDOMINATED SORTING APPROACH; GENETIC LOCAL SEARCH; REFERENCE-POINT; PART I; ALGORITHM; MOEA/D; DESIGN; PERFORMANCE; SURFACE	The quality of solution sets generated by decomposition-based evolutionary multi-objective optimisation (EMO) algorithms depends heavily on the consistency between a given problem's Pareto front shape and the specified weights' distribution. A set of weights distributed uniformly in a simplex often leads to a set of well-distributed solutions on a Pareto front with a simplex-like shape, but may fail on other Pareto front shapes. It is an open problem on how to specify a set of appropriate weights without the information of the problem's Pareto front beforehand. In this article, we propose an approach to adapt weights during the evolutionary process (called AdaW). AdaW progressively seeks a suitable distribution of weights for the given problem by elaborating several key parts in weight adaptation-weight generation, weight addition, weight deletion, and weight update frequency. Experimental results have shown the effectiveness of the proposed approach. AdaW works well for Pareto fronts with very different shapes: 1) the simplex-like, 2) the inverted simplex-like, 3) the highly nonlinear, 4) the disconnect, 5) the degenerate, 6) the scaled, and 7) the high-dimensional.																	1063-6560	1530-9304				JUN	2020	28	2					227	253		10.1162/evco_a_00269													
J								A New Generalized Partition Crossover for the Traveling Salesman Problem: Tunneling between Local Optima	EVOLUTIONARY COMPUTATION										Traveling Salesman Problem; recombination operator; evolutionary combinatorial optimization	HYBRID GENETIC ALGORITHM	Generalized Partition Crossover (GPX) is a deterministic recombination operator developed for the Traveling Salesman Problem. Partition crossover operators return the best of 2k reachable offspring, where k is the number of recombining components. This article introduces a new GPX2 operator, which finds more recombining components than GPX or Iterative Partial Transcription (IPT). We also show that GPX2 has O(n) runtime complexity, while also introducing new enhancements to reduce the execution time of GPX2. Finally, we experimentally demonstrate the efficiency of GPX2 when it is used to improve solutions found by the multitrial Lin-Kernighan-Helsgaum (LKH) algorithm. Significant improvements in performance are documented on large (n>5000) and very large (n=100,000) instances of the Traveling Salesman Problem.																	1063-6560	1530-9304				JUN	2020	28	2					255	288		10.1162/evco_a_00254													
J								A Predictive-Reactive Approach with Genetic Programming and Cooperative Coevolution for the Uncertain Capacitated Arc Routing Problem	EVOLUTIONARY COMPUTATION										Capacitated arc routing problem; cooperative coevolution; hyper-heuristics; genetic programming	ROBUST; ALGORITHMS	The uncertain capacitated arc routing problem is of great significance for its wide applications in the real world. In the uncertain capacitated arc routing problem, variables such as task demands and travel costs are realised in real time. This may cause the predefined solution to become ineffective and/or infeasible. There are two main challenges in solving this problem. One is to obtain a high-quality and robust baseline task sequence, and the other is to design an effective recourse policy to adjust the baseline task sequence when it becomes infeasible and/or ineffective during the execution. Existing studies typically only tackle one challenge (the other being addressed using a naive strategy). No existing work optimises the baseline task sequence and recourse policy simultaneously. To fill this gap, we propose a novel proactive-reactive approach, which represents a solution as a baseline task sequence and a recourse policy. The two components are optimised under a cooperative coevolution framework, in which the baseline task sequence is evolved by an estimation of distribution algorithm, and the recourse policy is evolved by genetic programming. The experimental results show that the proposed algorithm, called Solution-Policy Coevolver, significantly outperforms the state-of-the-art algorithms to the uncertain capacitated arc routing problem for the ugdb and uval benchmark instances. Through further analysis, we discovered that route failure is not always detrimental. Instead, in certain cases (e.g., when the vehicle is on the way back to the depot) allowing route failure can lead to better solutions.																	1063-6560	1530-9304				JUN	2020	28	2					289	316		10.1162/evco_a_00256													
J								Learning and Searching Pseudo-Boolean Surrogate Functions from Small Samples	EVOLUTIONARY COMPUTATION										Fitness function modelling; estimation of distribution algorithms; pseudo-Boolean functions; linkage learning; Walsh decomposition; mixed order hyper networks; statistical machine learning	NEURAL-NETWORKS; FITNESS APPROXIMATION; OPTIMIZATION; SELECTION; DEUM	When searching for input configurations that optimise the output of a system, it can be useful to build a statistical model of the system being optimised. This is done in approaches such as surrogate model-based optimisation, estimation of distribution algorithms, and linkage learning algorithms. This article presents a method for modelling pseudo-Boolean fitness functions using Walsh bases and an algorithm designed to discover the non-zero coefficients while attempting to minimise the number of fitness function evaluations required. The resulting models reveal linkage structure that can be used to guide a search of the model efficiently. It presents experimental results solving benchmark problems in fewer fitness function evaluations than those reported in the literature for other search methods such as EDAs and linkage learners.																	1063-6560	1530-9304				JUN	2020	28	2					317	338		10.1162/evco_a_00257													
J								An improvement of multi-scale covariance descriptor for embedded system	JOURNAL OF REAL-TIME IMAGE PROCESSING										Multi-scale covariance descriptor; Person re-identification; Multi-core architecture; ARM; Embedded system	REGION COVARIANCE; FEATURES; DESIGN; SCALE	Video surveillance has been a major area of focus for researchers and engineers. Actually, video surveillance includes several useful and complex tasks such as tracking, human detection, re-identification and recognition. Multi-scale covariance (MSCOV) descriptor has recently grown in interest due to its good performances for person detection, re-identification and matching. Unfortunately, its original version requires heavy computations, and it is difficult to be executed in real time on embedded systems. This paper presents two aspects of improvement to adapt the MSCOV descriptor for embedded systems. First, the local binary pattern (LBP) features are introduced and a trade-off between accuracy and processing cost is used to define the best features combination. Second, parallel implementation and embedded co-processor are exploited to accelerate processing time on multi-core CPU architectures. Both optimizations are implemented and evaluated for executing a complete application of person re-identification systems. The software implementation is performed using the VIPeR dataset. Using LBP, 21.57% processing speed-up and 50% less memory requirements for the descriptor computation are achieved without any accuracy performance degradation. We also prototype the proposed design using Zynq platform based on ARM Cortex-A9. The results demonstrate the effectiveness of the parallelization and conduct more than 11 times processing speed-up against the original algorithm.																	1861-8200	1861-8219				JUN	2020	17	3					419	435		10.1007/s11554-018-0759-y													
J								Real-time drogue detection and template tracking strategy for autonomous aerial refueling	JOURNAL OF REAL-TIME IMAGE PROCESSING											OBJECT TRACKING; ALGORITHMS; SIMULATION	Autonomous aerial refueling technology is an effective solution to extend flight duration of unmanned aerial vehicles, and also a great challenge due to its high risk. A novel real-time drogue detection and tracking strategy of monocular vision system for autonomous aerial refueling is proposed. It uses a direct image registration-based tracking method to ensure reliable and real-time tracking, and an ROI detection based on edge features to solve the tracking drift problem. A multiply patches fusion structure is adopted in the tracking method to improve the tracking accuracy and slow the divergence speed. Finally, various experiments are conducted to validate the proposed image processing strategy. These results show that the proposed strategy obtains a high accuracy as well as the real-time performance, and achieves a better performance than state-of-the-art methods.																	1861-8200	1861-8219				JUN	2020	17	3					437	446		10.1007/s11554-018-0787-7													
J								Parallel binocular stereo-vision-based GPU accelerated pedestrian detection and distance computation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Pedestrian detection; HOG; Cascade classifier; Distance estimation; Parallel binocular vision system; GPU		Pedestrian detection has become a very hot research field in computer vision, because it is widely used in many practical applications. However, the real-time requirement of these applications is a great challenge for pedestrian detection. To address this problem, this paper accelerates the pedestrian detection in parallel using NVIDIA's Graphics Processing Units (GPUs). In addition, we developed a distance estimation system based on the results of the pedestrian detection, which aims to obtain the distance between the pedestrians and the camera. The whole system including pedestrian detection and distance estimation is for embedded applications. The method of pedestrian detection is to combine the Histogram of Oriented Gradients (HOG) feature with the cascade classifier, and the distance estimation system is built by utilizing a parallel binocular vision system. The performance of the parallel implementation of the whole system is tested on two kinds of different GPUs, an embedded board Jetson TK1 and a Tesla K80 GPU specialized for science computation. The speed of the whole system on Jetson TK1 over 640 x 480 images is about 16 fps, which basically reaches the real-time requirement, and the speed on Tesla K80 over 640 x 480 images is much higher, about 86 fps.																	1861-8200	1861-8219				JUN	2020	17	3					447	457		10.1007/s11554-018-0783-y													
J								Real-time impulse noise removal	JOURNAL OF REAL-TIME IMAGE PROCESSING										Impulse noise removal; Cuda; Real-time denoising; High definition	SWITCHING MEDIAN FILTER; WEIGHTED MEAN FILTER; PEPPER NOISE; IMAGES; MODEL; REDUCTION	An adaptive interpolation-based impulse noise removal (AIBINR) algorithm is proposed to remove impulse noise from color and gray-scale images in real time. AIBINR works fast and has no need for parameter tuning to remove fixed-valued impulse noise. A GPU application has been developed to demonstrate the speed and inherent parallelization capabilities of the proposed method. Using the high-speed implementation, we have shown that AIBINR can denoise color images fast enough to be used in real-time video denoising, while having comparable denoising performance when compared to the state-of-the-art methods without any modification to its parameters.																	1861-8200	1861-8219				JUN	2020	17	3					459	469		10.1007/s11554-018-0791-y													
J								Performance optimization of rotation-tolerant Viola-Jones-based blackbird detection	JOURNAL OF REAL-TIME IMAGE PROCESSING										Viola-Jones algorithm; Histogram of oriented gradients (HOG); In-plane object rotation; Region of interest (ROI); Gradient orientation		The research described in this paper investigates the rotational robustness of the Viola-Jones algorithm (VJA) object detection method when used for red-winged blackbird (Agelaius phoeniceus) detection. VJA has been successfully used for face detection, but can be adapted to detect a variety of objects. This work uses the histogram of oriented gradients (HOG) descriptor to train the blackbird classifier. Since VJA object detection is inherently not invariant to in-plane object rotation, additional effort is required during training and detection. The proposed method extends the object detection framework developed by Viola and Jones to efficiently handle rotated blackbirds and provide a balance between detection accuracy and computation cost.																	1861-8200	1861-8219				JUN	2020	17	3					471	478		10.1007/s11554-018-0795-7													
J								Real-time shadow detection using multi-channel binarization and noise removal	JOURNAL OF REAL-TIME IMAGE PROCESSING										Shadow detection; Parallel processing; Binarization; Noise removal; Real time		High-quality automatic shadow detection remains a challenging problem in image processing and computer vision. Existing techniques for shadow detection typically make use of deep learning strategies to obtain accurate shadow detection results, at the cost of demanding high processing time, making their use unsuitable for augmented reality and robotic applications. In this paper, we propose a novel approach to perform high-quality shadow detection in real time. To do so, we convert an input image into different color spaces to perform multi-channel binarization and detect different shadow regions in the image. Then, a filtering algorithm is proposed to remove the noisy false-positive shadow regions on the basis of their sizes. Experimental results evaluated in two different datasets show that the proposed approach may run entirely on the GPU, requiring only approximate to 13 ms to detect shadows in an image with 3840x2160 (4k) resolution. That makes our approach about 1.8 (66x) orders of magnitude faster than related work for 4k resolution images, at the cost of only approximate to 5% of accuracy loss compared to the best results achieved for each dataset.																	1861-8200	1861-8219				JUN	2020	17	3					479	492		10.1007/s11554-018-0799-3													
J								Fast intra coding unit partition decision in H.266/FVC based on spatial features	JOURNAL OF REAL-TIME IMAGE PROCESSING										H; 266; future video coding (FVC); Fast algorithm; Intra coding; Spatial feature; Quadtree plus binary tree (QTBT)	CU SIZE DECISION; MODE DECISION; HEVC	With the development of technology, the requirements of hardware equipment and user expectations of visual enjoyment are increasingly gradually. The Joint Video Exploration Team (JVET) has established the latest video compression standard, Future Video Coding (FVC). FVC adopts QuadTree plus Binary Tree (QTBT) based Coding Unit (CU) structure, which not only removes the complex hierarchical structure of the CU, Prediction Unit (PU), and Transform Unit (TU) but also supports square and rectangular coding blocks based on the texture of the video content. Although the QTBT structure can provide superior coding performance, it significantly increases the encoding time, particularly in intra coding. Therefore, developing a fast intra CU partition decision algorithm is essential. In this paper, a fast CU partition decision algorithm in FVC intra coding based on spatial features is proposed. Different spatial features in the pixel domain are proposed in the binary tree and quadtree decision processes. Spatial features for the binary tree are employed for early skipping of the encoding process of CUs with binary tree depth and for early determination of binary tree split mode. Spatial features for the quadtree are employed for early splitting or termination of CUs with quadtree depth. Compared with JEM 5.0, the proposed method can save 23% encoding time on average with a slight increase of 0.62% in the Bjontegaard delta bitrate (BDBR).																	1861-8200	1861-8219				JUN	2020	17	3					493	510		10.1007/s11554-018-0794-8													
J								Fast guided filter for power-efficient real-time 1080p streaming video processing	JOURNAL OF REAL-TIME IMAGE PROCESSING										Guided image filtering; Real-time video processing; Power-efficient video processing	BILATERAL FILTER; DESIGN	With the advent of embedded vision systems, smart sensors with integrated image signal processing (ISP) become a hot topic. This poses a need for efficient hardware implementation, regarding resource utilization and power consumption, of core image processing algorithms. Power consumption is especially important, since many of the target devices are usually battery operated. Edge-aware filtering, although it is used in many core image processing algorithms, is still challenging operation, especially in cases where large kernels are needed. In this paper, efficient hardware realization of fast guided filter (FGF) is proposed. It is based on idea that large filter of size R=K.S can be calculated by downsampling input image by factor S and using filter of size K. Besides reduced memory and logic requirements, this optimization enables that, for the scaling factor S, core processing is done at 1/S2 pixel clock, providing significantly lower power consumption. Experimental results on Cyclone V FPGA chip demonstrate that, for FGF of size 35x35 with downsampling factor S=7 the proposed design achieves 60 fps for 1080p video. Memory utilization is 147.3 kB without need for any off-chip memory. Core dynamic power consumption is 79.89 mW. Proposed design consumes less total power than state-of-the-art guided filter realizations including ASIC-based solutions. This module can be seamlessly integrated into smart sensors ISP units, because it is designed for power-efficient streaming processing.																	1861-8200	1861-8219				JUN	2020	17	3					511	525		10.1007/s11554-018-0802-z													
J								Real-time ultrasound image reconstruction as an inverse problem on a GPU	JOURNAL OF REAL-TIME IMAGE PROCESSING										Ultrasound imaging; Ultrasonic linear model; Inverse problems; Model-based; GPU parallel processing		Ultrasonic image reconstruction methods based on inverse problems have been shown to produce sharp, high-quality images using more information about the acquisition process in its processing. This improved reconstruction has high computational cost, usually requiring to solve large systems and making real-time imaging very difficult. Parallelizing the reconstruction using graphics processing units (GPU) can significantly accelerate this processing, but the amount of memory needed by current system models is high for current GPU capacity. This paper presents a new system model to halve this memory requirement; it exploits the symmetry of the point spread functions (PSF) of the system matrix that occurs when symmetric transducers are used for acquisition. In this case, only one of the two symmetric PSFs needs to be stored; the other function is produced by reordering the stored one. Thus, we can reconstruct ultrasound images that are twice as large, making real-time reconstruction on a GPU possible for this application.																	1861-8200	1861-8219				JUN	2020	17	3					543	554		10.1007/s11554-018-0806-8													
J								Color HDR video processing architecture for smart camera How to capture the HDR video in real-time	JOURNAL OF REAL-TIME IMAGE PROCESSING										HDR; HDR camera; Tone mapping; Real-time HDR processing; HDR deghosting		This paper presents a novel FPGA architecture of high dynamic range (HDR) video processing pipeline, based on the capturing of a sequence of differently exposed images. An acquisition process enabling multi-exposure HDR as well as fast implementation of local tone mapping operator involving bilateral filtering is proposed. The HDR acquisition process is enhanced by the application of novel deghosting method, which is dedicated for hardware implementation and proposed in this paper. The hardware processing pipeline is designed with regards to efficiency and performance and the calculations are performed in fixed point arithmetic. The pipeline is suitable for programmable hardware (FPGA-Field Programmable Gate Arrays) implementation and it achieves real-time performance on full HD HDR video which overcomes state-of-the-art solutions that use local tone mapping and deghosting algorithm.																	1861-8200	1861-8219				JUN	2020	17	3					555	566		10.1007/s11554-018-0810-z													
J								Reduction of intra-coding time for HEVC based on temporary direction map	JOURNAL OF REAL-TIME IMAGE PROCESSING										Intra-coding; Prediction mode decision; Temporary direction map; Coding unit size decision; HEVC	MODE DECISION ALGORITHM; CU SIZE DECISION; PREDICTION	The high-efficiency video coding (HEVC) standard uses 35 intra-prediction modes for 2(N) x 2(N) (N is an integer number ranging from six to two) luma blocks and five modes for chroma blocks. To find the luma block with the minimum rate-distortion, it must perform 11935 different rate-distortion cost calculations. Although this approach improves coding efficiency compared to the previous standards such as H.264/AVC, computational complexity is increased significantly. In this paper, an intra-prediction technique has been described to improve the performance of the HEVC standard by minimizing its computational complexity. The proposed algorithm consists of two stages. The first stage, called prediction unit size decision (PUSD) was introduced to decrease evaluation of prediction unit sizes by ~ 38%. The second stage called prediction mode fast decision (PMFD) was developed to minimize the number of modes in the rough mode decision (RMD) stage. The simulation results show that the time complexity is decreased by ~ 47%, while the BD rate is increased by 1.08%, and PSNR is decreased by 0.04 db. Accordingly, the proposed algorithms have a negligible effect on the video quality with great saving in the time complexity.																	1861-8200	1861-8219				JUN	2020	17	3					567	579		10.1007/s11554-018-0815-7													
J								A two-stage method to improve the quality of quantized images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Color quantization; Binary Splitting algorithm; Artificial ants; Ant-tree algorithm; Clustering	COLOR QUANTIZATION; ALGORITHM	This article proposes a color quantization strategy that combines two color quantization methods: Binary Splitting and Ant tree for Color Quantization. This solution combines a splitting method, which is faster, and a clustering-based method, which generates better quantized images. Given that time is a fundamental factor when considering a method for real-time applications, the proposed strategy attempts to exploit both of these methods for obtaining good quantized images with a low computational cost. The result of this approach not only generates better images than when Binary Splitting and Ant tree for Color Quantization are applied separately, but also helps to improve other methods frequently used for color quantization such as Wu's method, Octree, Variance-based method and Neuquant.																	1861-8200	1861-8219				JUN	2020	17	3					581	605		10.1007/s11554-018-0814-8													
J								A fast image dehazing method that does not introduce color artifacts	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image dehazing; Color image processing; Vision models	HAZE REMOVAL ALGORITHM; ENHANCEMENT; FRAMEWORK; WEATHER; VISION	We propose a method for color dehazing with four main characteristics: it does not introduce color artifacts, it does not depend on inverting any physical equation, it is based on models of visual perception, and it is fast, potentially real time. Our method converts the original input image to the HSV color space and works in the saturation and value domains by: (1) reducing the value component via a global constrained histogram flattening; (2) modifying the saturation component in consistency with the previous reduced value; and (3) performing a local contrast enhancement in the value component. Results show that our method competes with the state-of-the-art when dealing with standard hazy images, and outperforms it when dealing with challenging haze cases. Furthermore, our method is able to dehaze a FullHD image on a GPU in 90 ms.																	1861-8200	1861-8219				JUN	2020	17	3					607	622		10.1007/s11554-018-0816-6													
J								Algorithm optimization and hardware implementation for Merge mode in HEVC	JOURNAL OF REAL-TIME IMAGE PROCESSING										High-efficiency video coding (HEVC); Merge mode; Rate-distortion optimization (RDO); Hardware architecture; Sum of absolute transformed difference (SATD)	PREDICTION	Merge mode is a new tool for improving inter-frame coding efficiency in high-efficiency video coding. This tool can save the bitrate for the motion vector by sharing this vector with neighboring blocks. Merge is a process that selects a candidate motion vector by calculating the cost of rate-distortion. However, this process requires a large number of complex computations and memory access, thereby resulting in the low efficiency of hardware implementation. This paper proposes a new Merge candidate decision scheme that determines the most favorable Merge candidate from a full list of candidates by comparing the sum of absolute transformed difference with the weighted header bit instead of performing a complex calculation for sum of squared difference and entropy coding process in HM16.7. The simulation results show that the performance of the proposed algorithm is close to that of HM16.7 and increases the BD-rate only by 0.22-1.21%. The multilevel pipelines architecture is also exploited in the hardware design. The weighted header bit operation is performed by using the look-up table, which reduces both the complexity and encoding clock cycle. The designed system is implemented with a register transfer level code. The synthesis results from the Design Compiler show that compared with other architecture, the proposed architecture offers great advantages in resource utilization and can process 1920 x 1080 at 353 frame/s for P-slices with a clock frequency of 1057 MHz and logic gate count of 285.2 K.																	1861-8200	1861-8219				JUN	2020	17	3					623	630		10.1007/s11554-018-0818-4													
J								Real-time attacks on robust watermarking tools in the wild by CNN	JOURNAL OF REAL-TIME IMAGE PROCESSING										Real time; Watermarking removal; Watermarking attack; CNN	DIGITAL WATERMARKS; IMAGE	Robust watermarking is a widely used technology to protect image copyright. Robustness, the ability to resist various distortions, is the most important property of robust watermarking algorithm. So to improve the robustness of the watermarking schemes, watermark attacking algorithms also attract much attention. Far from now, the existing watermarking attack methods cannot well balance the removal ability and visual quality. To address this issue, this paper proposed a removal attack by a convolutional neural network (CNN). Considering the speed requirements of real-time attack applications, for short computing time, we use a simple but powerful CNN. According to the amount of knowledge of watermarking, a corresponding dataset of watermark images is constructed. After that, the CNN model is trained to remove watermark with these datasets. The experiments show that the trained model can not only effectively remove the watermark, but also recover the original image without much image quality degradation.																	1861-8200	1861-8219				JUN	2020	17	3					631	641		10.1007/s11554-020-00941-8													
J								HW/SW co-design of a visual SLAM application	JOURNAL OF REAL-TIME IMAGE PROCESSING										FPGA; Co-design; SLAM; Machine-vision; ADAS		Vision-based advanced driver assistance systems (ADAS), appeared in the 2000s, are increasingly integrated on-board mass-produced vehicles, as off-the-shelf low-cost cameras are now available. But ADAS implement most of the time-specific and basic functionalities such as lane departure or control of the distance to other vehicles. Integrating accurate localization and mapping functionalities meeting the constraints of ADAS (high-throughput, low-consumption, and small-design footprint) would pave the way towards obstacle detection, identification and tracking on-board vehicles at potential high speed. While the SLAM problem has been widely addressed by the robotics community, very few embedded operational implementations can be found, and they do not meet the ADAS-related constraints. In this paper, we implement the first 3D monocular EKF-SLAM chain on a heterogeneous architecture, on a single System on Chip (SoC), meeting these constraints. In order to do so, we picked up a standard co-design method (Shaout et al. Specification and modeling of hw/sw co-design for heterogeneous embedded systems, 2009) and adapted it to the implementation of potentially any of such complex processing chains. The refined method encompasses a hardware-in-the-loop approach allowing to progressively integrate hardware accelerators on the basis of a systematic rule. We also have designed original hardware accelerators for all the image processing functions involved, and for some algebraic operations involved in the filtering process.																	1861-8200	1861-8219				JUN	2020	17	3					667	689		10.1007/s11554-018-0836-2													
J								A low-latency DMM-1 encoder for 3D-HEVC	JOURNAL OF REAL-TIME IMAGE PROCESSING										3D-HEVC; Intra-prediction; DMM-1	MODE DECISION; DEPTH; SELECTION	Depth modeling mode 1 (DMM-1) is vital for 3D high-efficiency video coding, since it results in low distortion and is suitable for sharp boundaries. When designing DMM-1 encoders, traditional work used serial methods to calculate all the wedgelets, leading to large latency; they may meet the requirements of 30 fps 1080P 3D video at most, but may be powerless for higher frame rate or 3D video resolution. In this paper, we propose a flexible parallel architecture for DMM-1 encoder. It can simultaneously evaluate all the wedgelets, saving encoding time without increasing distortion; it can also be configured into a partial-parallel architecture to save area. Experiments show that our method can save 33.6-94.3% encoding time for different test schemes. Synthesis results in SMIC 55 nm show that the VLSI design for proposed parallel architecture can meet the requirements of 1080P and higher-resolution video processing in real time.																	1861-8200	1861-8219				JUN	2020	17	3					691	702		10.1007/s11554-019-00875-w													
J								A high-dynamic range CMOS camera based on dual-gain channels	JOURNAL OF REAL-TIME IMAGE PROCESSING										CMOS camera; HDR; Dual channels; FPGA		To overcome the ghosting phenomenon of multi-exposure technology, a new high-dynamic range (HDR) image processing method is proposed in this paper, which combines the features from dual-gain channel images. Further, a complete CMOS camera based on the HDR method is implemented, which produces a real-time HDR live video streams. This camera can capture the details of bright and dark areas in the scene completely with an extended dynamic range up to 95 dB. An ALTERA FPGA is the core processing unit of the entire camera, and it completes all the functional modules of the camera efficiently, including dual-channel video capture, image caching, HDR synthesis and tone mapping. Finally, the real-time HDR video flow has a display resolution of 1920 x 1080 and a frame rate of 60 fps.																	1861-8200	1861-8219				JUN	2020	17	3					703	712		10.1007/s11554-019-00877-8													
J								Performance and energy-efficient implementation of a smart city application on FPGAs	JOURNAL OF REAL-TIME IMAGE PROCESSING										Smart city; Image processing; Background subtraction; Lucas-Kanade; High-level synthesis; Field programmable gate array; Graphical processing unit	PRIVACY; CITIES	The continuous growth of modern cities and the request for better quality of life, coupled with the increased availability of computing resources, lead to an increased attention to smart city services. Smart cities promise to deliver a better life to their inhabitants while simultaneously reducing resource requirements and pollution. They are thus perceived as a key enabler to sustainable growth. Out of many other issues, one of the major concerns for most cities in the world is traffic, which leads to a huge waste of time and energy, and to increased pollution. To optimize traffic in cities, one of the first steps is to get accurate information in real time about the traffic flows in the city. This can be achieved through the application of automated video analytics to the video streams provided by a set of cameras distributed throughout the city. Image sequence processing can be performed both peripherally and centrally. In this paper, we argue that, since centralized processing has several advantages in terms of availability, maintainability and cost, it is a very promising strategy to enable effective traffic management even in large cities. However, the computational costs are enormous, and thus require an energy-efficient High-Performance Computing approach. Field Programmable Gate Arrays (FPGAs) provide comparable computational resources to CPUs and GPUs, yet require much lower amounts of energy per operation (around 6x for the application considered in this case study). They are thus preferred resources to reduce both energy supply and cooling costs in the huge datacenters that will be needed by Smart Cities. In this paper, we describe efficient implementations of high-performance algorithms that can process traffic camera image sequences to provide traffic flow information in real-time at a low energy and power cost.																	1861-8200	1861-8219				JUN	2020	17	3					729	743		10.1007/s11554-018-0792-x													
J								Smoke vehicle detection based on multi-feature fusion and hidden Markov model	JOURNAL OF REAL-TIME IMAGE PROCESSING										Smoke vehicle detection; Local binary pattern; Visual background extractor algorithm; Edge orientation histogram; Hidden Markov model; Discrete cosine transform	MULTIRESOLUTION GRAY-SCALE; TEXTURE; COLOR; FRAMEWORK; PATTERNS; SHAPE	Existing smoke vehicle detection methods and vision-based smoke detection methods are vulnerable to false alarms. This paper presents an automatic smoke vehicle detection method based on multi-feature fusion and hidden Markov model (HMM). In this method, we first detect moving objects using an improved visual background extractor (ViBe) algorithm and obtain smoke-colored blocks using color histogram features in the HSI (hue, saturation, and intensity) color space. The adaptive scale local binary pattern (AS-LBP) and the discriminative edge orientation histogram (disEOH) are proposed and combined to characterize the smoke-colored blocks. More specifically, the proposed AS-LBP, a texture feature descriptor, is based on the quadratic fitting of our labelled data to obtain the best scale. The proposed disEOH, a gradient-based feature descriptor, is robust to noise by extracting discriminative edge information using Gaussian filters and principal component analysis (PCA). The discrete cosine transform (DCT) is employed to extract frequency domain information from the region fused by smoke blocks. To utilize the dynamic features, the HMMs are employed to analyze and classify the smoke-colored block sequences and region sequences in continuous frames. The experimental results show that the proposed method achieves better performances than existing smoke detection methods, especially achieves lower false alarms.																	1861-8200	1861-8219				JUN	2020	17	3					745	758		10.1007/s11554-019-00856-z													
J								M-Link: a link clustering memetic algorithm for overlapping community detection	MEMETIC COMPUTING										Overlapping community detection; Memetic algorithm; Evolutionary algorithms		Graphs and networks are a useful abstraction to represent a wide range of systems. Sets of nodes that are more highly interconnected constitute a 'community'. Community detection algorithms help to reveal a decomposition of a network in modules. These communities can overlap, and nodes can have several community memberships. We present M-Link, a memetic algorithm for overlapping community detection. It maximises an objective function called link partition density. The communities of edges obtained with this method naturally translate to overlapping communities of nodes. The method is based on local expansion and a specialised local search mechanism. Label propagation methods are used for initialising a multi-agent tertiary tree population structure. We use the normalised mutual information to evaluate the similarity between the known community structure in a collection of benchmark networks and the community structure detected by M-Link. The method outperforms other state-of-the-art algorithms for overlapping community detection and it has better accuracy and stability.																	1865-9284	1865-9292				JUN	2020	12	2					87	99		10.1007/s12293-020-00300-x													
J								"All the world's a stage": incongruity humour revisited	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Philosophy of humour; Incongruity humour; Humour theories; Human-computer interaction; Human-robot interaction; Computational humour; Smart environments	SURPRISE	Eighteenth and nineteenth century philosophers took interest in humour and, in particular, humorous incongruities. Humour was not necessarily their main interest; however, observations on humour could support their more general philosophical theories. Spontaneous and unintentional humour such as anecdotes, witty remarks and absurd events were the styles of humour that they analysed and made part of their theories. Prepared humour such as verbal jokes were rarely included in their observations, likely dismissed as too vulgar and not requiring intellectual effort. Humour, as analysed by several eighteenth and nineteenth century philosophers, was seen as part of daily life or life simulated on stage. In the twentieth century, Freud emphasized a possible 'relief' function of 'prepared' humour such as jokes. Additionally, linguists began developing theories to analyse jokes. A joke has a particular structure that is constructed with the aim of achieving a humorous effect. This structure makes jokes suitable for linguistic analysis. In the present-day humour research, jokes have become a main topic of research. This linguistically oriented joke research neglects many other forms of humour: spontaneous humour, non-verbal humour, physical humour, and many forms of unintentional humour that appear in real life. We want to survey and re-evaluate the contributions to the humour research of these eighteenth, nineteenth and early twentieth century philosophers and clarify that their more general contributions to the humour research have been neglected in favour of the very restricted form of prepared humour and linguistically expressed and analysed humour as it appears in jokes. We hope that the views expressed in this paper will help to steer the humour research away from joke research and help to integrate humour in the design of human-computer interfaces and smart environments. That is, rather than considering only verbal jokes, we should aim at generating smart environments that understand, facilitate or create humour that goes beyond jokes.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		405	438		10.1007/s10472-018-9609-7													
J								Changing channels: divergent approaches to the creative streaming of texts	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Metaphor; Computational creativity; Twitterbots; Divergent thinking; Story generation	METAPHOR COMPREHENSION; INCONGRUITY	Text is an especially malleable medium for human and machine creativity. When guided by the appropriate symbolic and/or statistical models, even a small and seemingly superficial change at the formal level can result in a predictable yet profound change at the semantic and pragmatic level. Text is also a virtually unlimited resource on the web, which offers abundant, free-flowing channels of topical texts for almost every genre and register. In this paper we consider diverse approaches to transforming these input channels into new and creative streams of machine-generated outputs. We focus on the specific kind of linguistic creativity associated with metaphor, yet also demonstrate that divergent approaches to metaphor generation can, in turn, enable divergent uses and applications for machine creativity.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		439	456		10.1007/s10472-018-9614-x													
J								Towards a model of creative understanding: deconstructing and recreating conceptual blends using image schemas and qualitative spatial descriptors	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Computational creativity; Concept blending; Qualitative spatial descriptors; Image schemas; Concept understanding; Novel concepts		Computational models of novel concept understanding and creativity are addressed in this paper from the viewpoint of conceptual blending theory (CBT). In our approach, a novel, unknown concept is addressed in a communication setting, where this novel concept, created as a blend by an emitter agent, sends a communicative object (words, or in this paper, a visual representation of that concept) to another agent. When received by a computational agent, a novel concept for that communicative object can only be understood by blending concepts already known by that agent. In this paper, we first posit that understanding new concepts via blending is also a creative process. Albeit different from generating conceptual blends, understanding a novel concept via blending involves the disintegration and decompression of that novel concept, in such a way that the receiver of that concept is able to re-create the conceptual network supposedly intended by the emitter of the novel concept. Secondly, we also propose image schemas as a tool that agents can use to interpret the spatial information obtained when disintegrating/unpacking novel concepts and then re-create the new blend. This process is studied in a communication setting where semiotics and meaning are conveyed by visual and spatial signs (instead of the usual setting of natural language or text). In this case study, qualitative spatial descriptors are applied for obtaining a formal description of an icon or pictogram, which is later assigned a meaning by a process of conceptual blending using image schemas.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		457	477		10.1007/s10472-019-09619-9													
J								Blending under deconstruction The roles of logic, ontology, and cognition in computational concept invention	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Computational concept invention; Conceptual blending; Ontologies; Image schemas; Refinement operators	IMAGE SCHEMAS	The cognitive-linguistic theory of conceptual blending was introduced by Fauconnier and Turner in the late 90s to provide a descriptive model and foundational approach for the (almost uniquely) human ability to invent new concepts. Whilst blending is often described as 'fluid' and 'effortless' when ascribed to humans, it becomes a highly complex, multi-paradigm problem in Artificial Intelligence. This paper aims at presenting a coherent computational narrative, focusing on how one may derive a formal reconstruction of conceptual blending from a deconstruction of the human ability of concept invention into some of its core components. It thus focuses on presenting the key facets that a computational framework for concept invention should possess. A central theme in our narrative is the notion of refinement, understood as ways of specialising or generalising concepts, an idea that can be seen as providing conceptual uniformity to a number of theoretical constructs as well as implementation efforts underlying computational versions of conceptual blending. Particular elements underlying our reconstruction effort include ontologies and ontology-based reasoning, image schema theory, spatio-temporal reasoning, abstract specification, social choice theory, and axiom pinpointing. We overview and analyse adopted solutions and then focus on open perspectives that address two core problems in computational approaches to conceptual blending: searching for the shared semantic structure between concepts-the so-called generic space in conceptual blending-and concept evaluation, i.e., to determine the value of newly found blends.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		479	516		10.1007/s10472-019-09654-6													
J								Acronyms: identification, expansion and disambiguation	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Acronyms; Modern Hebrew; Natural language processing	ABBREVIATION DISAMBIGUATION; SENSE	Acronyms-words formed from the initial letters of a phrase-are important for various natural language processing applications, including information retrieval and machine translation. While hand-crafted acronym dictionaries exist, they are limited and require frequent updates. We present a new machine-learning-based approach to automatically build an acronym dictionary from unannotated texts. This is the first such technique that specifically handles non-local acronyms, i.e., that can determine an acronym's expansion even when the expansion does not appear in the same document as the acronym. Our approach automatically enhances the dictionary with contextual information to help address the acronym disambiguation task (selecting the most appropriate expansion for a given acronym in context), outperforming dictionaries built using prior techniques. We apply the approach to Modern Hebrew, a language with a long tradition of using acronyms, in which the productive morphology and unique orthography adds to the complexity of the problem.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		517	532		10.1007/s10472-018-9608-8													
J								Dual embeddings and metrics for word and relational similarity	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Word embeddings; Dual embeddings; Word similarity; Relational similarity; 68		Word embedding models excel in measuring word similarity and completing analogies. Word embeddings based on different notions of context trade off strengths in one area for weaknesses in another. Linear bag-of-words contexts, such as in word2vec, can capture topical similarity better, while dependency-based word embeddings better encode functional similarity. By combining these two word embeddings using different metrics, we show how the best aspects of both approaches can be captured. We show state-of-the-art performance on standard word and relational similarity benchmarks.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		533	547		10.1007/s10472-019-09636-8													
J								A computer agent that develops visual compositions based on the ER-model	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Computational creativity; Engagement and reflection; Intelligent systems; Visual composition; Autonomous design		This paper describes a computer agent for the automatic generation of visual compositions based on the Engagement-Reflection Model of creative writing (Perez y Perez Cogn. Syst. Res. 8, 89-109, 2007; Perez y Perez and Sharples J. Exp. Theor. Artif. Intell. 13, 119-139, 2001). During engagement the system progresses the composition; during reflection the agent evaluates, and if necessary modifies, the material produced so far and generates a set of guidelines that constrains the production of material during engagement. The final output is the result of a constant interplay between these two states. We offer details of the model and describe a prototype that provides the users with the possibility of adding compositions to the knowledge-base. Then, we show how through engagement and reflection cycles, the system is capable of generating novel outputs. Using a questionnaire, we asked a group of volunteers to describe the features of pieces produced by the program and the features of pieces produced by human designers. The results suggest that our agent provides an adequate novel framework to study the generation of automatic visual compositions.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		549	588		10.1007/s10472-019-9616-3													
J								Spatial reasoning about qualitative shape compositions Composing Qualitative Lengths and Angles	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Qualitative representation; Qualitative reasoning; Spatial reasoning; Shapes; Qualitative angles; Qualitative lengths; Composition tables; Correctness; Geometry	SIMILARITY; KNOWLEDGE; CALCULUS	Shape composition is a challenge in spatial reasoning. Qualitative Shape Descriptors (QSD) have proven to be rotation and location invariant, which make them useful in spatial reasoning tests. QSD uses qualitative representations for angles and lengths, but their composition operations have not been defined before. In this paper, the Qualitative Model for Angles (QMAngles) and the Qualitative Model for Lengths (QMLengths) are presented in detail by describing their arity, reference systems and operators. Their operators are defined taking the well-known temporal model by Allen (Commun. ACM 26(11), 832-843 (1983). 10.1145/182.358434) as a reference. Moreover, composition tables are built, and the composition relations of qualitative angles and lengths are proved using their geometric counterparts. The correctness of these composition tables is also proved computationally using a logic program implemented using Swi-Prolog.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		589	621		10.1007/s10472-019-09637-7													
J								Non-terminating processes in the situation calculus	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Knowledge representation; Reasoning about actions; Situation calculus; Inductive definitions; Formal verification of Golog and ConGolog programs	PROGRAMMING LANGUAGE; GOLOG PROGRAMS	By their very design, many robot control programs are non-terminating. This paper describes a situation calculus approach to expressing and proving properties of non-terminating programs expressed in Golog, a high level logic programming language for modeling and implementing dynamical systems. Because in this approach actions and programs are represented in classical (second-order) logic, it is natural to express and prove properties of Golog programs, including non-terminating ones, in the very same logic. This approach to program proofs has the advantage of logical uniformity and the availability of classical proof theory.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		623	640		10.1007/s10472-019-09643-9													
J								Discovering state constraints for planning with conditional effects in Discoplan (part I)	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Automated planning; Inference of state constraints for planning; Planning domain analysis; State invariants in planning; Planning with conditional effects; Knowledge discovery for planning; Automatic inductive proofs	INVARIANTS; SEARCH	Discoplan is a durable and efficient system for inferring state constraints (invariants) in planning domains, specified in the PDDL language. It is exceptional in the range of constraint types it can discover and verify, and it directly allows for conditional effects in action operators. However, although various aspects of Discoplan have been previously described and its utility in planning demonstrated, the underlying methodology, the algorithms for the discovery and inductive verification of constraints, and the proofs of correctness of the algorithms and their complexity analysis have never been laid out in adequate detail. The purpose of this paper is to remedy these lacunae.																	1012-2443	1573-7470				JUN	2020	88	5-6			SI		641	686		10.1007/s10472-019-09618-w													
J								Automated MRI-Based Deep Learning Model for Detection of Alzheimer's Disease Process	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Deep learning; convolution neural networks; three-dimensional magnetic resonance imaging; support vector machine; Alzheimer's disease; mild cognitive impairment	MILD COGNITIVE IMPAIRMENT; EEG-BASED DIAGNOSIS; CLASSIFICATION; NETWORK; CONNECTIVITY; REPRESENTATION; METHODOLOGY; PREDICTION; CONVERSION; COHERENCE	In the context of neuro-pathological disorders, neuroimaging has been widely accepted as a clinical tool for diagnosing patients with Alzheimer's disease (AD) and mild cognitive impairment (MCI). The advanced deep learning method, a novel brain imaging technique, was applied in this study to evaluate its contribution to improving the diagnostic accuracy of AD. Three-dimensional convolutional neural networks (3D-CNNs) were applied with magnetic resonance imaging (MRI) to execute binary and ternary disease classification models. The dataset from the Alzheimer's disease neuroimaging initiative (ADNI) was used to compare the deep learning performances across 3D-CNN, 3D-CNN-support vector machine (SVM) and two-dimensional (2D)-CNN models. The outcomes of accuracy with ternary classification for 2D-CNN, 3D-CNN and 3D-CNN-SVM were 82.57 +/- 7.35%, 89.76 +/- 8.67% and 95.74 +/- 2.31% respectively. The 3D-CNN-SVM yielded a ternary classification accuracy of 93.71%, 96.82% and 96.73% for NC, MCI and AD diagnoses, respectively. Furthermore, 3D-CNN-SVM showed the best performance for binary classification. Our study indicated that 'NC versus MCI' showed accuracy, sensitivity and specificity of 98.90%, 98.90% and 98.80%; 'NC versus AD' showed accuracy, sensitivity and specificity of 99.10%, 99.80% and 98.40%; and 'MCI versus AD' showed accuracy, sensitivity and specificity of 89.40%, 86.70% and 84.00%, respectively. This study clearly demonstrates that 3D-CNN-SVM yields better performance with MRI compared to currently utilized deep learning methods. In addition, 3D-CNN-SVM proved to be efficient without having to manually perform any prior feature extraction and is totally independent of the variability of imaging protocols and scanners. This suggests that it can potentially be exploited by untrained operators and extended to virtual patient imaging data. Furthermore, owing to the safety, noninvasiveness and nonirradiative properties of the MRI modality, 3D-CNN-SMV may serve as an effective screening option for AD in the general population. This study holds value in distinguishing AD and MCI subjects from normal controls and to improve value-based care of patients in clinical practice.																	0129-0657	1793-6462				JUN	2020	30	6							2050032	10.1142/S012906572050032X													
J								Single-Trial EEG Responses Classified Using Latency Features	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Longitudinal covert attention training; EEG; machine learning classification; latency features	EVENT-RELATED POTENTIALS; WORKING-MEMORY; COGNITIVE PLASTICITY; SELECTIVE ATTENTION; BRAIN POTENTIALS; ERP; YOUNG; OLD; COMPONENT; GAINS	Covert attention has been repeatedly shown to impact on EEG responses after single and repeated practice sessions. Machine learning techniques are increasingly adopted to classify single-trial EEG responses thereby primarily relying on amplitude-based features instead of latency-based features. In this study, we investigated changes in EEG response signatures of nine healthy older subjects when performing 10 sessions of covert attention training. We show that, when we trained classifiers to distinguish recorded EEC patterns between the two experimental conditions (a target stimulus is "present" or "not present"), latency-based classifiers outperform the amplitude-based ones and that classification accuracy improved along with behavioral accuracy, providing supportive evidence of brain plasticity.																	0129-0657	1793-6462				JUN	2020	30	6							2050033	10.1142/S0129065720500331													
J								Temporal Backpropagation for Spiking Neural Networks with One Spike per Neuron	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Spiking neural network; supervised learning; temporal backpropagation; single spike coding	VISUAL FEATURES; INPUT	We propose a new supervised learning rule for multilayer spiking neural networks (SNNs) that use a form of temporal coding known as rank-order-coding. With this coding scheme, all neurons fire exactly one spike per stimulus, but the firing order carries information. In particular, in the readout layer, the first neuron to fire determines the class of the stimulus. We derive a new learning rule for this sort of network, named S4NN, akin to traditional error backpropagation, yet based on latencies. We show how approximated error gradients can be computed backward in a feedforward network with any number of layers. This approach reaches state-of-the-art performance with supervised multi-fully connected layer SNNs: test accuracy of 97.4% for the MNIST dataset, and 99.2% for the ('altech Face/Motorbike dataset. Yet, the neuron model that we use, nonleaky integrate-and-fire, is much simpler than the one used in all previous works. The source codes of the proposed S4NN are publicly available at https://github.com/SRKH/S4NN.																	0129-0657	1793-6462				JUN	2020	30	6							2050027	10.1142/S0129065720500276													
J								The Effect of Breath Pacing on Task Switching and Working Memory	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										HRV; neurovisceral integration model; cognitive functions; breath control	HEART-RATE-VARIABILITY; RESPIRATORY MODULATION; COGNITIVE FUNCTION; BIOFEEDBACK; PSYTOOLKIT; EMOTION; MODEL	The cortical and subcortical circuit regulating both cognition and cardiac autonomic interactions are already well established. This circuit has mainly been analyzed from cortex to heart. Thus, the heart rate variability (HRV) is usually considered a reflection of cortical activity. In this paper, we investigate whether HRV changes affect cortical activity. Short-term local autonomic changes were induced by three breathing strategies: spontaneous (Control), normal (NB) and slow paced breathing (SB). We measured the performance in two cognition domains: executive functions and processing speed. Breathing maneuvres produced three clearly differentiated autonomic states, which preconditioned the cognitive tasks. We found that the SB significantly increased the HRV low frequency (LF) power and lowered the power spectral density (PSD) peak to 0.1 Hz. Meanwhile, executive function was assessed by the working memory test, whose accuracy significantly improved after SB, with no significant changes in the response times. Processing speed was assessed by a multitasking test. Consistently, the proportion of correct answers (success rate) was the only dependent variable affected by short-term and long-term breath pacing. These findings suggest that accuracy, and not timing of these two cognitive domains would benefit from short-term SB in this study population.																	0129-0657	1793-6462				JUN	2020	30	6							2050028	10.1142/S0129065720500288													
J								3D-Convolutional Neural Network with Generative Adversarial Network and Autoencoder for Robust Anomaly Detection in Video Surveillance	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Video anomaly detection; machine learning; transfer learning; generative adversarial network; 3D CNN	EVENT DETECTION	As the surveillance devices proliferate, various machine learning approaches for video anomaly detection have been attempted. We propose a hybrid deep learning model composed of a video feature extractor trained by generative adversarial network with deficient anomaly data and an anomaly detector boosted by transferring the extractor. Experiments with UCSD pedestrian dataset show that it achieves 94.4% recall and 86.4% precision, which is the competitive performance in video anomaly detection.																	0129-0657	1793-6462				JUN	2020	30	6							2050034	10.1142/S0129065720500343													
J								Nonspecific Visuospatial Imagery as a Novel Mental Task for Online EEG-Based BCI Control	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										BCI; electroencephalography; visuospatial; imagery; online	BRAIN-COMPUTER INTERFACE; VISUAL-ATTENTION; COMMUNICATION; COMPLEX; CLASSIFICATION; DYNAMICS; OBJECT; SIGNAL	Brain-computer interfaces (BCIs) can provide a means of communication to individuals with severe motor disorders, such as those presenting as locked-in. Many BCI paradigms rely on motor neural pathways, which are often impaired in these individuals. However, recent findings suggest that visuospatial function may remain intact. This study aimed to determine whether visuospatial imagery, a previously unexplored task, could be used to signify intent in an online electroencephalography (EEG)-based BCI. Eighteen typically developed participants imagined checkerboard arrow stimuli in four quadrants of the visual field in 5-s trials, while signals were collected using 16 dry electrodes over the visual cortex. In online blocks, participants received graded visual feedback based on their performance. An initial BCI pipeline (visuospatial imagery classifier I) attained a mean accuracy of 71.67 +/- 12.32% classifying rest against visuospatial imagery in online trials. This BCI pipeline was further improved using restriction to alpha band features (visuospatial imagery classifier II), resulting in a mean pseudo-online accuracy of 75.05 +/- 11.90%. Accuracies exceeded the threshold for practical BCIs in 12 participants. This study supports the use of visuospatial imagery as a real-time, binary EEG-BCI control paradigm.																	0129-0657	1793-6462				JUN	2020	30	6							2050026	10.1142/S0129065720500264													
J								Learning Multi-level Deep Representations for Image Emotion Classification	NEURAL PROCESSING LETTERS										Deep learning; Multi-level; Image emotion classification; Image semantics; Image aesthetics	AUDIO	In this paper, we propose a new deep network that learns multi-level deep representations for image emotion classification (MldrNet). Image emotion can be recognized through image semantics, image aesthetics and low-level visual features from both global and local views. Existing image emotion classification works using hand-crafted features or deep features mainly focus on either low-level visual features or semantic-level image representations without taking all factors into consideration. The proposed MldrNet combines deep representations of different levels, i.e. image semantics, image aesthetics and low-level visual features to effectively classify the emotion types of different kinds of images, such as abstract paintings and web images. Extensive experiments on both Internet images and abstract paintings demonstrate the proposed method outperforms the state-of-the-art methods using deep features or hand-crafted features. The proposed approach also outperforms the state-of-the-art methods with at least 6% performance improvement in terms of overall classification accuracy.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2043	2061		10.1007/s11063-019-10033-9													
J								Visual Sentiment Analysis by Combining Global and Local Information	NEURAL PROCESSING LETTERS										Visual sentiment analysis; Salient objects; Local information; Global information		With the development of visual social networks, the sentiment analysis of images has quickly emerged for opinion mining. Based on the observation that the sentiments conveyed by some images are related to salient objects in them, we propose a scheme for visual sentiment analysis that combines global and local information. First, the sentiment is predicted from the entire images. Second, it is judged whether there are salient objects in an image or not. If there are, sub-images are cropped from the entire image based on the detection window of the salient objects. Moreover, a CNN model is trained for the set of sub-images. Predictions of sentiments from entire images and sub-images are then fused together to obtain the final results. If no salient object is detected in the images, the sentiment predicted directly from entire images is used as the final result. The compared experimental results show that the proposed approach is superior to state-of-the-art algorithms. It also demonstrates that reasonably utilizing the local information could improve the performance for visual sentiment analysis.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2063	2075		10.1007/s11063-019-10027-7													
J								Deep Transfer Learning for Image Emotion Analysis: Reducing Marginal and Joint Distribution Discrepancies Together	NEURAL PROCESSING LETTERS										Image emotion analysis; Transfer learning; Deep learning; Convolutional neural network		A lot of research attentions have been paid to image emotion analysis in recent years. Meanwhile, as convolutional neural networks (CNNs) have made great successful in computer vision, many researchers start to employ CNN to discriminate image emotions. However, the training procedure of CNNs depends on sufficient labeled data. Therefore, a CNN is hard to perform well in an image domain with scant labeled information. In this paper, we propose a deep transfer learning method for image emotion analysis. The method can leverage rich emotion knowledge from a source domain to the target domain. Our method reduces both marginal and joint domain distribution discrepancies at fully-connected layers. Through this way, we can effectively extract more transferable features and advance the performance of CNNs on poor-label emotion-image domains.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2077	2086		10.1007/s11063-019-10035-7													
J								Multi-layer Attention Based CNN for Target-Dependent Sentiment Classification	NEURAL PROCESSING LETTERS										Target-dependent; Sentiment classification; Multi-layer CNN; Attention mechanism	TRACKING; SENSOR	Target-dependent sentiment classification aims at identifying the sentiment polarities of targets in a given sentence. Previous approaches utilize recurrent neural network with attention mechanism incorporated to model the context and learn key sentiment intermediate representation in relation to a given target. However, such methods are incapable either of modeling complex contexts or of processing data parallelly. To address these problems, we propose, in this paper, a new model that employs a multi-layer convolutional neural network to process the context parallelly and model the context multiple times, where the neural network is able to explicitly learn the sentiment intermediate representation via an attention mechanism. Eventually, we integrate these features to form a final sentiment representation, which will be fed into the classifier. Experiments show that our model surpasses the existing approaches on several datasets.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2089	2103		10.1007/s11063-019-10017-9													
J								Inferring Personality Traits from Attentive Regions of User Liked Images Via Weakly Supervised Dual Convolutional Network	NEURAL PROCESSING LETTERS										Attentive image regions; Multi-personality class activation map; Personality prediction; Weakly supervised dual convolutional network	REGRESSION	In social media, users usually unconsciously their preferences on images, which can be considered as the personal cues for inferring their personality traits. Existing methods map the holistic image features into personality traits. However, users' attention on their liked images is typically localized, which should be taken into account in modeling personality traits. In this paper, we propose an end-to-end weakly supervised dual convolutional network (WSDCN) for personality prediction, which consists of a classification network and a regression network. The classification network captures personality class-specific attentive image regions while only requiring the image-level personality class labels. The regression network is used for predicting personality traits. Firstly, the users' Big-Five (BF) traits are converted into ten personality class labels for their liked images. Secondly, the Multi-Personality Class Activation Map (MPCAM) is generated based on the classification network and utilized as the localized activation to produce local deep features, which are then combined with the holistic deep features for the regression task. Finally, the user liked images and the associated personality traits are used to train the end-to-end WSDCN model. The proposed method is able to predict the BF personality traits simultaneously by training the WSDCN network only once. Experimental results on the annotated PsychoFlickr database show that the proposed method is superior to the state-of-the-art approaches.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2105	2121		10.1007/s11063-019-09987-7													
J								An End-to-End Perceptual Quality Assessment Method via Score Distribution Prediction	NEURAL PROCESSING LETTERS										Image quality assessment; Label distribution learning; Convolutional neural network; ROI pooling; Cross-entropy loss	NATURAL SCENE; IMAGE; REPRODUCTION	Image quality assessment (IQA) has become a rapidly growing field of technology as it automatically predicts the perceptual quality, which is of vital importance for consumer-centric services. However, most existing IQA algorithms focus on predicting the mean opinion score regardless of the inevitable opinion diversity. To address this shortcoming, in this paper, we propose to predict the distribution of opinion scores via an end-to-end convolutional neural network. The network is based on a pre-trained ResNet with 50 layers and a novel Statistical Region-of-Interest (ROI) Pooling layer is introduced for lower model complexity, which enables effective training with few datum. Meanwhile, instead of using traditional mean-square-error as loss function, our model is trained with cross-entropy loss, which is more suitable for probability distribution learning. Extensive experiments have been carried out on ESPL-LIVE HDR datasets with highly diverse opinion scores. It is shown that the statistical ROI Pooling is more efficient than traditional ROI Pooling layers and classical dimensionality reduction of principle component analysis. And the proposed algorithm achieves superior performance than state-of-the-art label distribution learning methods in terms of six representative evaluation metrics.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2123	2137		10.1007/s11063-019-10057-1													
J								Blind Image Deconvolution via Enhancing Significant Segments	NEURAL PROCESSING LETTERS										Blind deconvolution; Latent image prior; Proximal operator	RESTORATION	Blind image deconvolution aims to estimate both a blur kernel and a sharp image from a blurry observation. It is not only a classical problem in image processing, but also serves as preprocessing in many advanced tasks including affective image content analysis. In terms of statistical inference, this problem can be viewed as maximizing the probability of latent image and kernel, given the observed blurry image. Proper formulation of latent image prior is crucial to the success of blind deconvolution methods. A novel latent image prior is proposed to penalize low contrast and dense gradients, thus playing the role of enhancing significant segments. Our latent image prior is based on a one-dimensional regularizer, which involves normalizing reciprocals of absolute differences between two neighbouring unequal components. To solve the resulting optimization problem, a dynamic programming based method is derived to approximately evaluate the proximal operator associated with the proposed regularizer. Both quantitative and qualitative experiments illustrate that our method is comparable to the top-performing algorithms.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2139	2154		10.1007/s11063-019-10123-8													
J								State Estimation of Quaternion-Valued Neural Networks with Leakage Time Delay and Mixed Two Additive Time-Varying Delays	NEURAL PROCESSING LETTERS										Quaternion-valued neural networks; Linear matrix inequalities; State estimation; Additive time-varying delays	GLOBAL EXPONENTIAL STABILITY; SYNCHRONIZATION; SYSTEMS; DISCRETE	In this paper, the state estimation of quaternion-valued neural networks (QVNNs) with leakage time delay, both discrete and distributed two additive time-varying delays is studied. By considering the QVNNs as a whole, instead of decomposing it into two complex-valued neural networks or four real-valued neural networks. Via constructing suitable Lyapunov-Krasovskii functionals, combining free weight matrix, reciprocally convex approach, and matrix inequalities, the sufficient criteria for time delays are given in the form of quaternion-valued linear matrix inequalities and complex-valued linear matrix inequalities. Some observable output measurements are used to estimate the state of neurons, which ensures the global asymptotic stability of the error-state system. Finally, the effectiveness of theoretical analysis is illustrated by a numerical simulation.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2155	2178		10.1007/s11063-019-10178-7													
J								A Neural Network Study of Blasius Equation	NEURAL PROCESSING LETTERS										Boundary layer problem; Blasius equation; Artificial neural network	PERIODIC-SOLUTIONS; STABILITY; ALGORITHM; FLOW	In this work we applied a feed forward neural network to solve Blasius equation which is a third-order nonlinear differential equation. Blasius equation is a kind of boundary layer flow. We solved Blasius equation without reducing it into a system of first order equation. Numerical results are presented and a comparison according to some studies is made in the form of their results. Obtained results are found to be in good agreement with the given studies.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2179	2194		10.1007/s11063-019-10184-9													
J								On Impulsive Synchronization Control for Coupled Inertial Neural Networks with Pinning Control	NEURAL PROCESSING LETTERS										Coupled inertial neural networks; Synchronization; Impulsive control; Pinning control; Hybrid couplings	FIXED-TIME SYNCHRONIZATION; FINITE-TIME; DISSIPATIVITY ANALYSIS; EXPONENTIAL STABILITY; DYNAMICAL NETWORKS; VARYING DELAYS; SYSTEMS	The impulsive control for the synchronization problem of coupled inertial neural networks involved distributed-delay coupling is investigated in the present paper. A novel impulsive pinning control method is introduced to obtain the complete synchronization of the coupled inertial neural networks with three different coupling structures. At each impulsive control instant, the pinning-controlled nodes can be selected according to our selection strategy which is dependent on the lower bound of the pinning control ratio. Our criteria can be utilized to declare the synchronization of the coupled neural networks with asymmetric and reducible coupling structures. The effectiveness of our control strategy is exhibited by typical numerical examples.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2195	2210		10.1007/s11063-019-10189-4													
J								Stepanov-Like Pseudo Almost Periodic Solution of Quaternion-Valued for Fuzzy Recurrent Neural Networks with Mixed Delays	NEURAL PROCESSING LETTERS										Stepanov-like pseudo almost periodic function; Quaternion-valued fuzzy recurrent neural networks; Existence and uniqueness; Asymptotic and exponential stability	NICHOLSONS BLOWFLIES MODEL; FIXED-POINT THEOREMS; EXPONENTIAL STABILITY; LIMIT-CYCLES; SYSTEMS; SYNCHRONIZATION; CONVERGENCE; BOUNDEDNESS	Real-valued neural networks or complex-valued neural networks are sometimes inappropriate for some engineering and research problems for instance where the data is multi-dimensional, such as 4-D signals, color images and body images. Hence, researchers explored recently a more general and sophisticated model than the previous one, which is the quaternion-valued neural networks. The quaternions, which can also be defined as 2x2 documentclass matrix of complex numbers, have the capacity to analyze three or more dimensional signals and represent spatial transformations. In this paper, some sufficient conditions are given for the existence and various kinds of stability for the unique Stepanov-like pseudo almost periodic solution of quaternion-valued fuzzy recurrent neural networks. The results are established by employing Lyapunov functionals, Nemytskii's operator and Banach fixed point theorem. Also, a new direct method is used to establish our theoretical results in order to avoid the decomposition of the considered model into real-valued or complex-valued system. Finally, a numerical example is given to illustrate the validity of the obtained results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2211	2243		10.1007/s11063-020-10193-z													
J								Superpixels Features Extractor Network (SP-FEN) for Clothing Parsing Enhancement	NEURAL PROCESSING LETTERS										Semantic segmentation; Deep convolutional networks; Clothing parsing; Scene parsing; Superpixels	IMAGE SEGMENTATION	In this paper, the research looks at improving clothing parsing using superpixels features extractor network (SP-FEN). Clothing parsing using a fully convolutional network has two parts: an encoder and decoder. The encoder lowers the dimensionality and produces a low-resolution prediction, while the decoder tries to upscale the prediction and returns it to the size of the input image. Typically, fine-grained details get lost in the encoding part of the model is not recovered well in the decoder part. To fix this issue, skip connections are typically used in recovering and adding more fine-grained details to the final prediction. A new method is proposed to introduce superpixels features to the decoder by adding a side network (SP-FEN) that extracts features from superpixels representation of the input image using the SLIC Algorithm. SP-FEN then produces a meaningful superpixels features to be injected into the decoder. The SP-FEN is learning to choose specific features to be fed to the decoder part to boost the outputs overall quality. The proposed method has shown to enhance the MIoU accuracy using the refined Fashionista V1.0 dataset and CFPD dataset. The results showed that the proposed approach achieved superior performance with pixel-wise segmentation and clothing parsing.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2245	2263		10.1007/s11063-019-10173-y													
J								An Evaluation of RetinaNet on Indoor Object Detection for Blind and Visually Impaired Persons Assistance Navigation	NEURAL PROCESSING LETTERS										Indoor object recognition; Visually impaired people (VIP); Deep convolutional neural network (DCNN); Deep learning; Indoor object detection and recognition dataset (IODR)	RECOGNITION	Indoor object detection presents a computer vision task that deals with the detection of specific indoor classes. This task attracts a lot of attention, especially in the last few years. The strong interest related to this field can be explained by the big importance of this task for indoor assistance navigation for visually impaired people and also by the phenomenal development of the deep convolutional neural networks (Deep CNN). In this paper, an effort is made to perform a new indoor object detector using the deep convolutional neural network-based framework. The framework is built based on the deep convolutional neural network "RetinaNet". Evaluation is done by using various backbones as ResNet, DenseNet, and VGGNet in order to improve detection performances and processing time. We obtained very encouraging results coming up to 84.61% mAP as detection precision.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2265	2279		10.1007/s11063-020-10197-9													
J								Asymptotic Stability and Polynomial Stability of Impulsive Cohen-Grossberg Neural Networks with Multi-proportional Delays	NEURAL PROCESSING LETTERS										Global polynomial stability; Global asymptotic stability; Cohen-Grossberg neural networks; Proportional delay; Impulsive effect	GLOBAL EXPONENTIAL CONVERGENCE; FINITE-TIME STABILITY; LEAKAGE DELAYS; CRITERIA; SYNCHRONIZATION; DISSIPATIVITY; EQUATIONS; SYSTEM	This paper is concerned with the global asymptotic stability (GAS) and global polynomial stability (GPS) of impulsive Cohen-Grossberg neural networks (ICGNNs) with multi-proportional delays. The concept of GPS of the ICGNNs considered is first proposed and it is pointed out that the GPS is also one of the dynamics of recurrent neural networks with proportional delays. The GPS criteria depending on proportional delay factors are made by introducing tunable parameters, skillfully designing Lyapunov functionals and using inequality skills. The application scope of the ICGNNs considered parameters is extended by introducing tunable parameters. And the relationship of exponential stability, polynomial stability and asymptotic stability of the ICGNNs considered is revealed. These criteria proposed are checked by two numerical examples and simulations.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2607	2627		10.1007/s11063-020-10209-8													
J								Regularized Negative Label Relaxation Least Squares Regression for Face Recognition	NEURAL PROCESSING LETTERS										Least squares regression (LSR); Negative; math; mi; mi; math; documentclass; dragging technique; Manifold learning; Class compactness graph; Label relaxation	DIMENSION REDUCTION; LINEAR-REGRESSION; CROWD EVACUATION; REPRESENTATION; CLASSIFICATION; GRAPH; UNCERTAINTY; MODEL	Least squares regression (LSR) is widely used for pattern classification. Some variants based on it try to enlarge the margin between different classes to achieve better performance. However, the large margin classifier doesn't work well when it deals with the complex applications in the real world, such as face recognition, where images are captured with different facial expressions, lighting conditions or background. To address this problem, we propose a regularized negative label relaxation least squares regression method with the following characteristics. First, we introduce a negative epsilon\documentclass dragging technique to relax the strict binary label matrix into a slack label matrix, which has more freedom to fit the labels and reduces the class margins at the same time. Second, we introduce manifold learning and class compactness graph to devise a regularization item to preserve the intrinsic structure of data and avoid the problem of overfitting. The class compactness graph can enable samples from the same class to be kept close together after they are transformed into the slack label space. The algorithm based on L2-norm loss function is devised. The experimental results show that our algorithm achieves better classification accuracy.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2629	2647		10.1007/s11063-020-10219-6													
J								Stacked Fusion Supervised Auto-encoder with an Additional Classification Layer	NEURAL PROCESSING LETTERS										Deep learning; Supervised learning; Auto-encoder; Adaptive weighted voting fusion	DEEP; RECOGNITION; ENSEMBLE; AUTOENCODER	Auto-encoders are unsupervised deep learning models, which try to learn hidden representations to reconstruct the inputs. While the learned representations are suitable for applications related to unsupervised reconstruction, they may not be optimal for classification. In this paper, we propose a supervised auto-encoder (SupAE) with an addition classification layer on the representation layer to jointly predict targets and reconstruct inputs, so it can learn discriminative features specifically for classification tasks. We stack several SupAE and apply a greedy layer-by-layer training approach to learn the stacked supervised auto-encoder (SSupAE). Then an adaptive weighted majority voting algorithm is proposed to fuse the prediction results of SupAE and the SSupAE, because each individual SupAE and the final SSupAE can both get the posterior probability information of samples belong to each class, we introduce Shannon entropy to measure the classification ability for different samples based on the posterior probability information, and assign high weight to sample with low entropy, thus more reasonable weights are assigned to different samples adaptively. Finally, we fuse the different results of classification layer with the proposed adaptive weighted majority voting algorithm to get the final recognition results. Experimental results on several classification datasets show that our model can learn discriminative features and improve the classification performance significantly.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2649	2667		10.1007/s11063-020-10223-w													
J								Gaussian Pyramid of Conditional Generative Adversarial Network for Real-World Noisy Image Denoising	NEURAL PROCESSING LETTERS										Image denoising; Real-world noisy images; Gaussian pyramid; Generative model	SPARSE	Image denoising is an essential and important pre-processing step in digital imaging systems. However, most of existing methods are not adaptive in real-world applications due to the complexity of real noise. To address this problem, a novel pyramidal generative structural network (PGSN) is proposed for robust and efficient real-world noisy image denoising. Specifically, we consider the denoising problem as a process of image generation. The procedure is to first build a Gaussian pyramid where a cascade of encoder-decoder networks are used to adaptively capture multi-scale image features and progressively reconstruct the corresponding noise-free image from coarse to fine granularity. Then, we train a conditional form of GAN at each pyramid level. By integrating the conditional GAN approach into the Gaussian pyramid, the proposed network can well combine the image features from different pyramid levels, and an incremental distinction between the real noise and image details is dynamically built up, hence greatly boosting the denoising performance. Extensive experimental results demonstrate that our PGSN gives satisfactory denoising results, and achieves superior performance against the state-of-the-arts.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2669	2684		10.1007/s11063-020-10215-w													
J								Impulsive-Interaction-Driven Synchronization in an Array of Coupled Neural Networks	NEURAL PROCESSING LETTERS										Coupled neural networks; Impulsive interactions; Average impulsive interval; Signed graph; Impulsive synchronization	BOOLEAN NETWORKS; CONSENSUS; STABILIZATION; CRITERIA	This paper deals with the problem of globally exponential synchronization and bipartite synchronization of coupled neural networks with impulsive interactions. Impulsive interaction means that a number of neural networks only communicate with each other at impulsive instants, while the array of neural networks are independent from each other at the remaining time. The advantage of the scheme is that communication cost can be largely reduced when only discrete communication is required. Moreover the communication links between nodes can be either positive or negative at impulsive instants. Using the Lyapunov method combined with some mathematical analysis and average impulsive interval, some efficient criteria are obtained to guarantee synchronization of impulsive coupled neural networks. Finally, the validity of our theoretical results is demonstrated by two numerical examples.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2685	2700		10.1007/s11063-020-10214-x													
J								Intuitionistic Fuzzy Proximal Support Vector Machines for Pattern Classification	NEURAL PROCESSING LETTERS										Machine learning; Support vector machines; Fuzzy sets; Kernel functions; Quadratic programming	SVM	Support vector machine is a powerful technique for classification and regression problems. In the binary data problems, it classifies the points by assigning them to one of the two disjoint halfspaces. However, this method fails to handle the noises and outliers present in the dataset and the solution of a large-sized quadratic programming problem is required to obtain the decision surface in input or in feature space. We propose the intuitionistic fuzzy proximal support vector machine (IFPSVM) which classifies the patterns according to its proximity with the two parallel planes that are kept as distant as possible from each other. These two parallel 'proximal' planes can be obtained by solving a system of linear equations only. There is an intuitionistic fuzzy number associated with each training point which is framed by its degree of membership and non-membership. The membership degree of a pattern considers its distance from the corresponding class center and the degree of non-membership of a pattern is given by the ratio of the number of heterogeneous points to the number of total points in its neighborhood. The proposed technique effectively reduces the impact of noises and distinguishes the edge support vectors and outliers. Computational simulations on an artificial and eleven UCI benchmark datasets using linear, polynomial and Gaussian kernel functions, show the effectiveness of the proposed IFPSVM method. The experiments prove that it can handle large datasets with less computational time and yields better accuracy.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2701	2735		10.1007/s11063-020-10222-x													
J								RSDCN: A Road Semantic Guided Sparse Depth Completion Network	NEURAL PROCESSING LETTERS										Depth completion; Lidar & image processing; Semantic guided; Convolutional neural network; Asymmetric multiscale convolution structure	IMAGE	Laser radar (Lidar) plays an indispensable role in lots of security critical applications such as autonomous driving. However, the high sparsity and non-uniformity nature of the raw laser data brings large difficulties to reliable 3D scene understanding. Traditional depth completion methods suffer from the highly ill-conditioned nature of the problem. A novel end-to-end road semantic guided depth completion neural network with a special designed Asymmetric Multiscale Convolution (AMC) structure is proposed in this paper. The whole network is composed of two parts: semantic part and depth completion part. The semantic part is constructed by an image-Lidar joint segmentation sub-network which produces semantic masks (ground or object) to the following network. The depth completion part is composed of a series of AMC convolution structure. By combining the semantic masks and treating the ground and non-ground objects separately, the proposed AMC structure can well fit the depth distribution pattern implied in road scene. The experiments carried on both synthesized and real datasets demonstrate that our method can effectively improve the accuracy of depth completion results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2737	2749		10.1007/s11063-020-10226-7													
J								Task-Independent Spiking Central Pattern Generator: A Learning-Based Approach	NEURAL PROCESSING LETTERS										Central pattern generators; Spiking neural networks; Learning; Robotics locomotion; Neurorobotics	NETWORKS	Legged locomotion is a challenging task in the field of robotics but a rather simple one in nature. This motivates the use of biological methodologies as solutions to this problem. Central pattern generators are neural networks that are thought to be responsible for locomotion in humans and some animal species. As for robotics, many attempts were made to reproduce such systems and use them for a similar goal. One interesting design model is based on spiking neural networks. This model is the main focus of this work, as its contribution is not limited to engineering but also applicable to neuroscience. This paper introduces a new general framework for building central pattern generators that are task-independent, biologically plausible, and rely on learning methods. The abilities and properties of the presented approach are not only evaluated in simulation but also in a robotic experiment. The results are very promising as the used robot was able to perform stable walking at different speeds and to change speed within the same gait cycle.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2751	2764		10.1007/s11063-020-10224-9													
J								Dual Global Structure Preservation Based Supervised Feature Selection	NEURAL PROCESSING LETTERS										Feature selection; Structure preservation; Sparse representation; Dual-structure preservation	UNSUPERVISED FEATURE-SELECTION; REPRESENTATION	The recent literature indicates that the global structure preservation is very important for sparse representation based supervised feature selection. However, the selected features in preserving different global structures are often different and which global structure is the best not yet known. As a result, which feature selection result we should trust is confusing. The reason may be that each global structure does not carry enough information for the data, as the distribution of a real life data is very complex. To overcome the above problem, in this paper, a dual global structure preservation based supervised feature selection (DGSPSFS) method is proposed. In DGSPSFS, the supervised dimensional reduction method based on manifold learning is used to calculate the response matrix, which can contain more information of the data. And a new sparse representation framework that can preserve two global structures in the same time is proposed, which can comprehensively use two response matrices to fully utilize the information of the data. As a result, the features that can carry more information are selected. A comprehensive experimental study is then conducted in order to compare our feature selection algorithms with many state-of-the art ones in supervised learning scenarios. The conducted experiments validate the effectiveness of our feature selection.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2765	2787		10.1007/s11063-020-10225-8													
J								A Balanced Feature Fusion SSD for Object Detection	NEURAL PROCESSING LETTERS										Object detection; SSD; Feature fusion; Localization loss		Single shot multibox detector (SSD) takes several feature layers for object detection, but each layer is used independently. This structure may ignore some context information and is not conducive to improving the detection accuracy of small objects. Moreover, the imbalances of samples and multi-tasks during SSD training process can lead to inefficient training and model degradation. In order to improve the performance of SSD, this paper proposes a balanced feature fusion SSD (BFSSD) algorithm. Firstly, a feature fusion module is proposed to fuse and refine different layers of the feature pyramid. Then, a more balanced L1 loss function is proposed to further solve these imbalances. Finally, our model is trained with Pascal VOC2007 and VOC2012 trainval datasets and tested on Pascal VOC2007 test datasets. Simulation results show that, for the input size of 300 x 300, BFSSD exceeds the best results provided by the conventional SSD and other advanced object detection algorithms.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2789	2806		10.1007/s11063-020-10228-5													
J								Models of Trust in Human Control of Swarms With Varied Levels of Autonomy	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Task analysis; Robot sensing systems; Robot kinematics; Predictive models; Automation; Computational modeling; Human-robot interaction; human-swarm interaction; multirobot systems; swarm robotics; trust	AUTOMATION	In this paper, we study human trust and its computational models in supervisory control of swarm robots with varied levels of autonomy (LOA) in a target foraging task. We implement three LOAs: manual, mixed-initiative (MI), and fully autonomous LOA. While the swarm in the MI LOA is controlled by a human operator and an autonomous search algorithm collaboratively, the swarms in the manual and autonomous LOAs are fully directed by the human and the search algorithm, respectively. From user studies, we find that humans tend to make their decisions based on physical characteristics of the swarm rather than its performance since the task performance of swarms is not clearly perceivable by humans. Based on the analysis, we formulate trust as a Markov decision process whose state space includes the factors affecting trust. We develop variations of the trust model for different LOAs. We employ an inverse reinforcement learning algorithm to learn behaviors of the operator from demonstrations where the learned behaviors are used to predict human trust. Compared to an existing model, our models reduce the prediction error by at most 39.6%, 36.5%, and 28.8% in the manual, MI, and auto-LOA, respectively.																	2168-2291	2168-2305				JUN	2020	50	3			SI		194	204		10.1109/THMS.2019.2896845													
J								Influence of Culture, Transparency, Trust, and Degree of Automation on Automation Use	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Automation; Thermostats; Cultural differences; Face; Furnaces; Task analysis; Temperature measurement; Automation transparency; cultural differences; degree of automation (DOA); trust in automation	CONFIDENCE; RELIANCE	The reported study compares groups of 120 participants each, from the United States (U.S.), Taiwan (TW), and Turkey (TK), interacting with versions of an automated path planner that vary in transparency and degree of automation. The nationalities were selected in accordance with the theory of cultural syndromes as representatives of Dignity (U.S.), Face (TW), and Honor (TK) cultures, and were predicted to differ in readiness to trust automation, degree of transparency required to use automation, and willingness to use systems with high degrees of automation. Three experimental conditions were tested. In the first, highlight, path conflicts were highlighted leaving rerouting to the participant. In the second, replanner made requests for permission to reroute when a path conflict was detected. The third combined condition increased transparency of the replanner by combining highlighting with rerouting to make the conflict on which decision was based visible to the user. A novel framework relating transparency, stages of automation, and trust in automation is proposed in which transparency plays a primary role in decisions to use automation but is supplemented by trust where there is insufficient information otherwise. Hypothesized cultural effects and framework predictions were confirmed.																	2168-2291	2168-2305				JUN	2020	50	3			SI		205	214		10.1109/THMS.2019.2931755													
J								Transparency for a Workload-Adaptive Cognitive Agent in a Manned-Unmanned Teaming Application	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Task analysis; Planning; Helicopters; Tools; Proposals; Man-machine systems; Automation; Adaptive associate systems; cognitive agents; human factors; human-agent teaming; scalable autonomy; situation awareness (SA); transparency	SITUATION AWARENESS	This study focuses on the transparent design of a cognitive agent to enhance situation awareness in two aspects of a human-agent teaming application: assisted system management and mixed-initiative mission planning. Adaptive and complex agent behavior might result in the failure to comprehend resulting interventions, a decrease in trust, and a loss of overall situation awareness. This study describes and validates a concept for transparent agent design by adopting the transparency strategies proposed by the "situation awareness-based agent transparency model." The overall objective was to improve the human operator's perception, comprehension, and projection of the agent's support. The concept was applied to the prototype of a workload-adaptive cognitive agent, which supports a helicopter crew during mission planning and execution in complex and dynamically changing multi-vehicle missions. A human-in-the-loop experiment revealed enhancements in situation awareness and performance. Subjective trust measures implied an increase in human-like characteristics of the cognitive agent. The results and the potential for further research are discussed.																	2168-2291	2168-2305				JUN	2020	50	3			SI		225	233		10.1109/THMS.2019.2914667													
J								Individual Differences in Trust in Autonomous Robots: Implications for Transparency	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Cognitive science; Robot kinematics; Tools; Task analysis; Automation; Psychology; Autonomous systems; individual differences; robots; transparency; trust	SITUATION AWARENESS; AUTOMATION; METAANALYSIS; ANTHROPOMORPHISM; PREDICTION; LIKABILITY; MODEL	The introduction of increasingly intelligent and autonomous systems raises novel human factors challenges for human-machine teaming. People utilize differing mental models in understanding the functioning of complex systems that may be capable of social agency. Operators may perceive the machine as either a complex tool or a humanlike teammate. When the "advanced tool" mental model is adopted, operator trust may reflect individual differences in expectations of automation. By contrast, when the "teammate" mental model is activated, trust may depend on evaluative attitudes to robots. This article investigates predictors of trust in an autonomous robot detecting threat on either a physics-based or psychological basis. Distinct dimensions of physics-based and psychological trust are identified, corresponding to advanced tool and team mental models, respectively. Dispositional perceptions of automation, measured with the perfect automation schema scale, are associated with both aspects of trust. By contrast, the negative attitudes toward robots scale is specifically associated with lower psychological trust. The findings suggest that transparency information should be designed for compatibility with the operator's mental model in order to support accurate trust calibration and situation awareness. Transparency may be personalized to emphasize either the machine's data-analytic capabilities (advanced tool) or its humanlike social functioning (teammate).																	2168-2291	2168-2305				JUN	2020	50	3			SI		234	244		10.1109/THMS.2019.2947592													
J								The IMPACT of Agent Transparency on Human Performance	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Decision making; Uncertainty; Task analysis; Calibration; Atmospheric measurements; Particle measurements; Time factors; Human-agent team; human-machine system; performance; response time; trust; workload	SITUATION AWARENESS; AUTOMATION; TRUST	The primary purpose of this article is to determine the impact of a simulated agent's transparency on human performance and related variables, such as response time, workload, and trust calibration. The agent supports participants as they complete a base defense task by managing a team of heterogeneous unmanned vehicles and serves as a decision aid to the human. Three conditions of transparency are explored. In condition 1, the agent displays only the basic information (map of vehicle location and proposed routes). In condition 2, the agent displays the basic information and an explanation of its reasoning. In condition 3, the agent displays the basic information, reasoning, and uncertainties involved in the plans. Results show that participants exhibit better performance and trust calibration in the high-transparency conditions without perceiving a significant increase in workload. However, response time also increased, likely due to the additional processing time needed for conditions with more information. Overall, our findings indicate that the increased agent transparency can improve human-agent decision making and performance, but with a small cost of the efficiency (timeliness) of task completion.																	2168-2291	2168-2305				JUN	2020	50	3			SI		245	253		10.1109/THMS.2020.2978041													
J								Agent Transparency and Reliability in Human-Robot Interaction: The Influence on User Confidence and Perceived Reliability	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Reliability; Robots; Task analysis; Monitoring; Object detection; Cognition; Decision making; Agent transparency; confidence; human-robot interaction; reliability; situation awareness (SA); trust	SITUATION AWARENESS; AUTOMATION; TRUST; AUTONOMY; PERFORMANCE; INFORMATION; MISUSE; DISUSE; MODEL	Agent transparency is an important contributor to human performance, situation awareness (SA), and trust in human-agent teaming. However, agent transparency's effects on human performance when the agent is unreliable have yet to be examined. This paper examined how the transparency and reliability of an autonomous robotic squad member (ASM) affected a human observer's task performance, workload, SA, trust in the robot, and perceptions of the robot. In a 2 (ASM transparency) x 2 (ASM reliability) within-subject design experiment, participants monitored a simulated soldier squad that included an ASM as it traversed a simulated training environment, while concurrently monitoring the environment for targets. There was no difference in participants' performance on the target detection task, workload, or SA due to either ASM transparency or reliability. ASM reliability influenced participant trust and perceptions of the robot. Results suggest that reliability may be a stronger influence on the human's perceptions of the robot than transparency. Robot errors had a profound and lasting effect on the participants' perception of the robot's future reliability and resulted in reduced confidence in their assessments of the robot's reliability. These findings could have important implications for the continued use of automated systems when the user is aware of system errors.																	2168-2291	2168-2305				JUN	2020	50	3			SI		254	263		10.1109/THMS.2019.2925717													
J								Demand-Driven Transparency for Monitoring Intelligent Agents	IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS										Task analysis; Cognition; Monitoring; Intelligent agents; Robots; Predictive models; Australia; Decision support systems; intelligent systems	SITUATION AWARENESS; AUTOMATION; TRUST; PERFORMANCE; SEARCH; MISUSE; IMPACT	In autonomous multiagent or multirobotic systems, the ability to quickly and accurately respond to threats and uncertainties is important for both mission outcomes and survivability. Such systems are never truly autonomous, often operating as part of a human-agent team. Artificial intelligent agents (IAs) have been proposed as tools to help manage such teams; e.g., proposing potential courses of action to human operators. However, they are often underutilized due to a lack of trust. Designing transparent agents, who can convey at least some information regarding their internal reasoning processes, is considered an effective method of increasing trust. How people interact with such transparency information to gain situation awareness while avoiding information overload is currently an unexplored topic. In this article, we go part way to answering this question, by investigating two forms of transparency: sequential transparency, which requires people to step through the IA's explanation in a fixed order; and demand-driven transparency, which allows people to request information as needed. In an experiment using a multivehicle simulation, our results show that demand-driven interaction improves the operators' trust in the system while maintaining, and at times improving, performancehttp://www.ieee.org/documents/taxonomy_v101.pdf."?> and usability.																	2168-2291	2168-2305				JUN	2020	50	3			SI		264	275		10.1109/THMS.2020.2988859													
J								LiMM-PCA: Combining ASCA(+) and linear mixed models to analyse high-dimensional designed data	JOURNAL OF CHEMOMETRICS										ASCA; chemometrics; linear mixed models; PCA; random effects	PRINCIPAL COMPONENT ANALYSIS; PLS-REGRESSION; VARIANCE; TESTS; TOOL; PARAFASCA	Nowadays, life science experiments-and especially "omics" fields-often imply a high volume of information from high throughput technologies that is gathered in the form of a wide and short multivariate response. These data are intrinsically correlated and generally produced by another multivariate set of factors or continuous variables, collected in what is defined as the design matrix. Such design factors usually involve the presence of a treatment, but other sources of biological or technical variability in the data are often measured as well. The ASCA framework, based on ANOVA and PCA, leads to promising results. By combining dimension reduction projection methods and classic statistical modelling, it enables to decipher the main sources of variability in the produced response and offers attractive graphical representations of the factors' effect. However, this approach has not yet been extended to more advanced designs involving random factors, being typically involved in longitudinal, hierarchical, or repeatability/reproducibility studies. This paper has its roots in the GLM version of ASCA, called ASCA(+), that leads to unbiased estimators of the factors' effects for unbalanced data. It is here extended by replacing GLM by LMM and adapting the methodology. Taking into account the error structure of the data indeed leads to more accurate data modelling and more generalisable results. The suggested methodology is applied to two experimental case studies that highlight the benefits of this approach as it leads to a refined data analysis with interesting inferential properties, while keeping the powerful visualisation outputs produced by ASCA.																	0886-9383	1099-128X				JUN	2020	34	6							e3232	10.1002/cem.3232													
J								Chemometrics-based vibrational spectroscopy for Juglandis semen extracts investigation	JOURNAL OF CHEMOMETRICS										AHC; extract; Juglandis semen; PCA; vibrational spectroscopy	RED WINE TANNINS; REGIA L.; RAMAN-SPECTRA; IDENTIFICATION; TOCOPHEROLS; WALNUTS; RISK; OILS; CONSUMPTION; PHENOLICS	In this study, the chemical composition of Juglandis semen extracts obtained by different extraction solvents and methods were determined by Fourier transform infrared (FTIR) spectroscopy and Raman spectroscopy. The multivariate analyses, principal component analysis and hierarchical cluster analysis of the FTIR and Raman spectral datasets, were performed to determine the differences in the chemical composition according to the solvent (e.g., mixture of water with alcohol, glycerole, and propylene glycol) and the extraction method (i.e., ultrasound-assisted extraction, rapid pressurized extraction, and subcritical fluid extraction). The obtained results reveal that the J. semen extracts with water-alcohol solvent have equivalent chemical composition. A well-defined differentiation based on extraction method could not be highlighted. Based on FTIR data compilation, it can be observed that a first classification can be made based on the solvent, acidic or basic, used for extraction methods (except subcritical fluid extraction - SFE, clustering is clearly assigned by the solvent). Multivariate analysis on FTIR and Raman spectral data was able to differentiate SFE extract as a unique cluster, which means HFC134a solvent allowed a specific extraction (from organic constituents' point of view) due to the selected conditions of extraction method.																	0886-9383	1099-128X				JUN	2020	34	6							e3234	10.1002/cem.3234													
J								Power to the Oracle? Design Principles for Interactive Labeling Systems in Machine Learning	KUNSTLICHE INTELLIGENZ										Interactive labeling; Interactive machine learning; Training data		Labeling is the process of enclosing information to some object. In machine learning it is required as ground truth to leverage the potential of supervised techniques. A key challenge in labeling is that users are not necessarily eager to behave as simple oracles, that is, repeatedly answering questions whether a label is right or wrong. In this respect, scholars acknowledge designing interactivity in labeling systems as a promising area for further improvements. In recent years, a considerable number of articles focusing on interactive labeling systems have been published. However, there is a lack of consolidated principles how to design these systems. In this article, we identify and discuss five design principles for interactive labeling systems based on a literature review and offer a frame for detecting common ground in the implementation of corresponding solutions. With these guidelines, we strive to contribute design knowledge for the increasingly important class of interactive labeling systems.																	0933-1875	1610-1987				JUN	2020	34	2			SI		131	142		10.1007/s13218-020-00634-1													
J								Measuring the Quality of Explanations: The System Causability Scale (SCS) Comparing Human and Machine Explanations	KUNSTLICHE INTELLIGENZ										System causability scale (SCS); Explainable AI; Human-AI interfaces	NEURAL-NETWORKS; DEEP	Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human-AI interfaces for explainable AI. In order to build effective and efficient interactive human-AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.																	0933-1875	1610-1987				JUN	2020	34	2			SI		193	198		10.1007/s13218-020-00636-z													
J								ITP: Inverse Trajectory Planning for Human Pose Prediction	KUNSTLICHE INTELLIGENZ												Tracking and predicting humans in three dimensional space in order to know the location and heading of the human in the environment is a difficult task. Though if solved it will allow a robotic agent to know where it can safely be and navigate the environment without imposing any danger to the human that it is interacting with. We propose a novel probabilistic framework for robotic systems in which multiple models can be fused into a circular probabilitymap to forecast human poses. We developed and implemented the framework and tested it on Toyota's HSR robot and Waymo Open Dataset. Our experiments show promising results.																	0933-1875	1610-1987				JUN	2020	34	2			SI		209	225		10.1007/s13218-020-00658-7													
J								Mutual Explanations for Cooperative Decision Making in Medicine	KUNSTLICHE INTELLIGENZ										Human-AI partnership; Inductive Logic Programming; Explanations as constraints	FACIAL EXPRESSIONS	Exploiting mutual explanations for interactive learning is presented as part of an interdisciplinary research project on transparent machine learning for medical decision support. Focus of the project is to combine deep learning black box approaches with interpretable machine learning for classification of different types of medical images to combine the predictive accuracy of deep learning and the transparency and comprehensibility of interpretable models. Specifically, we present an extension of the Inductive Logic Programming system Aleph to allow for interactive learning. Medical experts can ask for verbal explanations. They can correct classification decisions and in addition can also correct the explanations. Thereby, expert knowledge can be taken into account in form of constraints for model adaption.																	0933-1875	1610-1987				JUN	2020	34	2			SI		227	233		10.1007/s13218-020-00633-2													
J								Dealing with Mislabeling via Interactive Machine Learning	KUNSTLICHE INTELLIGENZ										Interactive learning; Knowledge and learning; Managing annotator mistakes		We propose an interactive machine learning framework where the machine questions the user feedback when it realizes it is inconsistent with the knowledge previously accumulated. The key idea is that the machine uses its available knowledge to check the correctness of its own and the user labeling. The proposed architecture and algorithms run through a series of modes with progressively higher confidence and features a conflict resolution component. The proposed solution is tested in a project on university student life where the goal is to recognize tasks like user location and transportation mode from sensor data. The results highlight the unexpected extreme pervasiveness of annotation mistakes and the advantages provided by skeptical learning.																	0933-1875	1610-1987				JUN	2020	34	2			SI		271	278		10.1007/s13218-020-00630-5													
J								A particle swarm optimisation-trained feedforward neural network for predicting the maximum power point of a photovoltaic array	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Artificial neural network (ANN); Fuzzy logic control (FLC); Maximum power point tracking (MPPT); Photovoltaic (PV); Perturb and observe (P&O); Efficiency of MPPT (eta MPPT)	FUZZY-LOGIC; PV MODULE; MPPT; CONTROLLER; ALGORITHM; SYSTEMS; IMPLEMENTATION; EFFICIENCY; DESIGN	In this paper, a feedforward Artificial Neural Network (ANN) technique using experimental data is designed for predicting the maximum power point of a photovoltaic array. An ANN model training strategy is challenging due to the variations in the training and the operation conditions of a photovoltaic system. In order to improve ANN model accuracy, the Particle Swarm Optimisation (PSO) algorithm is utilised to find the best topology and to calculate the optimum initial weights of the ANN model. Hence, the dilemma between computational time and the best-fitting regression of the ANN model is addressed, as well as the mean squared error being minimised. To evaluate the proposed method, a MATLAB/Simulink model for an installed photovoltaic system is developed. Experimental data of a sunny and cloudy day are utilised to determine the average efficiency of this proposed method under varying atmospheric conditions. The results show that the optimised feedforward ANN technique based on the PSO algorithm using real data predicts the maximum power point accurately, achieving hourly average efficiencies of more than 99.67% and 99.30% on the sunny and cloudy day, respectively.																	0952-1976	1873-6769				JUN	2020	92								103688	10.1016/j.engappai.2020.103688													
J								Decision making for energy investments by using neutrosophic present worth analysis with interval-valued parameters	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Renewable energy; Solar production economics; Investment analysis; Present worth analysis; Fuzzy logic; Neutrosophic sets	FUZZY	Due to the increasing energy consumption of the world, new alternative energy sources are needed. The alternative energy resources can be classified into three categories such as fossil fuels energy resources, renewable energy resources, and nuclear energy resources. Solar energy is a very commonly used renewable energy resource for generating electricity and heating water. In this paper, establishing a solar energy system is handled as an investment analysis problem whose parameters are defined under uncertainty. Therefore, neutrosophic sets as a mean of dealing with uncertainty have been preferred to capture this vagueness and impreciseness. Neutrosophic sets are one of the extensions of intuitionistic fuzzy sets, which use an indeterminacy function unlike the other extensions of fuzzy sets. This paper proposes a new neutrosophic investment analysis method by using interval-valued parameters to evaluate solar energy systems. In the application section, three different types of solar energy systems are evaluated by the proposed method. It is observed that the proposed method presents big flexibility to experts and it gives effective and efficient results.																	0952-1976	1873-6769				JUN	2020	92								103639	10.1016/j.engappai.2020.103639													
J								A novel strategy for classifying perceived video quality using electroencephalography signals	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Video quality; Electroencephalography; Quality of experience; Affective computing; Classification	P300; EEG	Video streaming through the Internet is abundant nowadays. While video quality is continuously demanded, monitoring users' quality of experience (QoE) is essential when watching video contents. QoE can be evaluated directly through subjective assessment which is the human ground truths; however, such assessment is generally expensive and time consuming, and cannot be implemented in real time. QoE can also be evaluated by video quality models; however, the evaluation is fully based on video contents but human physical states cannot be taken into account. To tackle the limitations, detection of a prominent electroencephalography (EEG) signal feature namely P300 correlated to QoE can be used, when users are viewing videos. P300 is a positive deflection pulse that appears around 300 ms after a significant video distortion appears. QoE can be indicated by P300 pulses. However, the captured EEG signal is generally contaminated with noise. Strong noise generates P300 although video carries no distortion. Hence, detections of P300 patterns are not accurate. In this paper, a double classifier consisting of a first and second classifier is proposed. The first classifier attempts to determine whether the captured EEG feature is abnormal or not, where the abnormal caption behaves opposite to the normal P300 characteristic when showing the distorted video. The second classifier is developed to perform classifications for either normal or abnormal features. We evaluate the performance of the proposed double classifier based on the EEG samples, which are captured when showing video stimuli to participants. The proposed classifier is implemented by the support vector machine and logistic regression, which are commonly used for detection of EEG patterns and are computationally much simpler than deep learning. The performance of the proposed classifier is compared to those of the single classifiers, which determine the QoE directly when the EEG signal is given. Cross-validations showed that generally more than 5% improvement can be achieved by the proposed double classifier. Statistical tests indicate that the proposed double classifier can generally obtain better classification rates than solely using the single classifier at a 97.5% confidence level.																	0952-1976	1873-6769				JUN	2020	92								103692	10.1016/j.engappai.2020.103692													
J								DEACO: Adopting dynamic evaporation strategy to enhance ACO algorithm for the traveling salesman problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Ant colony algorithm; Traveling salesman problem; Local optimum problem; Swarm intelligence; Convergence speed	ANT COLONY OPTIMIZATION; INTELLIGENCE; SEARCH; DESIGN	AM Colony Optimization (ACO) algorithm is one of the effective solutions to solve the problem of combination optimization like traveling salesman problem (TSP) which belongs to NP-hard problem. However, this algorithm is robust and has a strong ability for solution discovery, but the convergence speed of that is low and stuck into local optimum. Therefore, for overcoming the drawbacks of ACO, we proposed a self-adaptive ACO with unique strategies to improve uncertain convergence time and random decisions of this algorithm. The proposed technique (DEACO) adjusting the ACO parameters dynamically. In this mechanism, main idea is how to select the first city (start point) to achieve the shortest path based on clustering. In this approach, DEACO finds the minimum cost/shortest path for each cluster. The data that used for this experiment is from TSPLIB library under MATLAB simulation with 10 TSP instances. The experiment outcome illustrates better performance of the proposed method than the conventional ACO in term of faster convergence speed and higher search accuracy.																	0952-1976	1873-6769				JUN	2020	92								103649	10.1016/j.engappai.2020.103649													
J								Potential, challenges and future directions for deep learning in prognostics and health management applications	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep learning; Prognostics and health management; GAN; Domain adaptation; Fleet PHM; Deep reinforcement learning; Physics-induced machine learning	NEURAL-NETWORKS; FAULT-DETECTION; ANOMALY DETECTION; SYSTEMS; CLASSIFICATION; DIAGNOSIS; MODELS; KERNEL; LSTM	Deep learning applications have been thriving over the last decade in many different domains, including computer vision and natural language understanding. The drivers for the vibrant development of deep learning have been the availability of abundant data, breakthroughs of algorithms and the advancements in hardware. Despite the fact that complex industrial assets have been extensively monitored and large amounts of condition monitoring signals have been collected, the application of deep learning approaches for detecting, diagnosing and predicting faults of complex industrial assets has been limited. The current paper provides a thorough evaluation of the current developments, drivers, challenges, potential solutions and future research needs in the field of deep learning applied to Prognostics and Health Management (PHM) applications.																	0952-1976	1873-6769				JUN	2020	92								103678	10.1016/j.engappai.2020.103678													
J								A novel and effective optimization algorithm for global optimization and its engineering applications: Turbulent Flow of Water-based Optimization (TFWO)	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Real-parameter global optimization; Classical optimization algorithms; Turbulent Flow of Water-based Optimization (TFWO); Economic Load Dispatch (ELD)	ECONOMIC-DISPATCH PROBLEM; IMPERIALIST COMPETITIVE ALGORITHM; DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; SEARCH ALGORITHM; DESIGN	In this study we present a new and effective grouping optimization algorithm (namely, the Turbulent Flow of Water-based Optimization (TFWO)), inspired from a nature search phenomenon, i.e. whirlpools created in turbulent flow of water, for global real-world optimization problems. In the proposed algorithm, the problem of selecting control parameters is eliminated, the convergence power is increased and the algorithm have a fixed structure. The proposed algorithm is used to find the global solutions of real-parameter benchmark functions with different dimensions. Besides, in order to further investigate the effectiveness of TFWO, it was used to solve various types of nonlinear Economic Load Dispatch (ELD) optimization problems in power systems and Reliability-Redundancy Allocation Optimization (RRAO) for the overspeed protection system of a gas turbine, as two real-world engineering optimization problems. The results of TFWO are compared with other algorithms, which provide evidence for efficient performance with superior solution quality of the proposed TFWO algorithm in solving a great range of real-parameter benchmark and real-world engineering problems. Also, the results prove the competitive performance and robustness of TFWO algorithm compared to other state-of-the-art optimization algorithms in this study. The source codes of the TFWO algorithm are publicly available at http://github.com/ebrahimakbary/TFWO.																	0952-1976	1873-6769				JUN	2020	92								103666	10.1016/j.engappai.2020.103666													
J								Improving physical activity recognition using a new deep learning architecture and post-processing techniques	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep learning structure; Physical activity recognition; Inertial signals; Convolutional neural networks; Classification	ACCELEROMETER; MOBILE; ALGORITHM; SENSORS; MOTION	This paper proposes a Human Activity Recognition system composed of three modules. The first one segments the acceleration signals into overlapped windows and extracts information from each window in the frequency domain. The second module detects the performed activity at each window using a deep learning structure based on Convolutional Neural Networks (CNNs). The first part of this structure has several layers associated to each sensor independently and the second part combines the outputs from all sensors in order to classify the physical activity. The third module integrates the window-level decision in longer periods of time, obtaining a significant performance improvement (from 89.83% to 96.62%). These are the best classification results on the PAMAP2 dataset with a Leave-One-Subject-Out (LOSO) evaluation.																	0952-1976	1873-6769				JUN	2020	92								103679	10.1016/j.engappai.2020.103679													
J								An iterated greedy algorithm for the obnoxious p-median problem	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Obnoxious p-median problem; Iterated greedy; Metaheuristics; Combinatorial optimization		The obnoxious p-median problem (OpM) is one of the NP-hard combinatorial optimization problems, in which the goal is to find optimal places to facilities that are undesirable (e.g. noisy, dangerous, or pollutant) such that the sum of the minimum distances between each non-facility location and its nearest facility is maximized. In this paper, for the first time in the literature, Iterated Greedy (IG) metaheuristic has been applied at a higher level to solve this problem. A powerful composite local search method has also been developed by combining two fast and effective local search algorithms, namely RLS1 and RLS2, which were previously used to solve the OpM. Comprehensive experiments have been conducted to test the performance of the proposed algorithm using a common benchmark for the problem. The computational results show the effectiveness of the IG algorithm that it can find high-quality solutions in a short time. Based on the set of selected instances, the results also reveal that the developed IG algorithm outperforms most of the state-of-the-art algorithms and contributes to the literature with 5 new best-known solutions.																	0952-1976	1873-6769				JUN	2020	92								103674	10.1016/j.engappai.2020.103674													
J								An accelerator for online SVM based on the fixed-size KKT window	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Online support vector machine; KKT conditions; Window technology; Accelerator	SUPPORT VECTOR MACHINE; ALGORITHM; MODEL	Support vector machine (SVM), as a general and useful supervised learning tool, is facing with some challenges such as low learning efficiency, poor generalization performance, noise sensitivity, etc. when it is applied to online learning tasks. To overcome these limitations, an accelerator model based on window technology and the KKT conditions for online SVM learning is proposed in this paper. The proposed model is not an independent online algorithm but may be regarded as an accelerator for other online SVM learning algorithms, and it constructs working set of SVM by a fixed-size window with the samples which violate the KKT conditions. The relationship between Lagrangain multipliers in dual problem of SVM and KKT conditions are analyzed in the case of online learning. On this basis, a fixed-size KKT window can be constructed according to whether the samples violate KKT conditions or not. Then, it takes the samples that violate the KKT conditions as the training window, which not only makes the training samples with the same size each time, but also ensures that all samples are useful for the hyperplane updating (it means that the classifier can be updated more smoothly). Two typical and specific online SVM algorithms are used as baseline, and the corresponding speeding online SVM learning algorithms with "X+accelerator" models are proposed to testing the performance of the proposed accelerator. Comprehensive experiments clearly show that the proposed model can accelerate the online learning process effectively and has good robustness and generalization performance.																	0952-1976	1873-6769				JUN	2020	92								103637	10.1016/j.engappai.2020.103637													
J								Detecting, locating and recognising human touches in social robots with contact microphones	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Acoustic sensing; Social robots; Touch gesture recognition; Touch localisation; Human-robot interaction; Machine learning applications	COMMUNICATION; RECOGNITION; EMOTION	There are many situations in our daily life where touch gestures during natural human-human interaction take place: meeting people (shaking hands), personal relationships (caresses), moments of celebration or sadness (hugs), etc. Considering that robots are expected to form part of our daily life in the future, they should be endowed with the capacity of recognising these touch gestures and the part of its body that has been touched since the gesture's meaning may differ. Therefore, this work presents a learning system for both purposes: detect and recognise the type of touch gesture (stroke, fickle, tap and slap) and its localisation. The interpretation of the meaning of the gesture is out of the scope of this paper. Different technologies have been applied to perceive touch by a social robot, commonly using a large number of sensors. Instead, our approach uses 3 contact microphones installed inside some parts of the robot. The audio signals generated when the user touches the robot are sensed by the contact microphones and processed using Machine Learning techniques. We acquired information from sensors installed in two social robots, Maggie and Mini (both developed by the RoboticsLab at the Carlos III University of Madrid), and a real-time version of the whole system has been deployed in the robot Mini. The system allows the robot to sense if it has been touched or not, to recognise the kind of touch gesture, and its approximate location. The main advantage of using contact microphones as touch sensors is that by using just one, it is possible to "cover" a whole solid part of the robot. Besides, the sensors are unaffected by ambient noises, such as human voice, TV, music etc. Nevertheless, the fact of using several contact microphones makes possible that a touch gesture is detected by all of them, and each may recognise a different gesture at the same time. The results show that this system is robust against this phenomenon. Moreover, the accuracy obtained for both robots is about 86%.																	0952-1976	1873-6769				JUN	2020	92								103670	10.1016/j.engappai.2020.103670													
J								Modified Sauvola binarization for degraded document images	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Binarization techniques; Degraded document images; Historical documents; Stroke width transform	ENTROPY; ALGORITHM	The binarization of historical documents is a difficult job due to the presence of many degradations. Many existing local binarization techniques use certain manually adjusted parameters. The output of these techniques is much dependent on the value of these parameters. One of such parameters is window size which is kept fixed for the whole text image. The fixed window size will not be able to perform well for images having variable stroke widths and text sizes. The proposed binarization technique (Modified Sauvola) is the modification of state of art Sauvola's binarization technique. It automatically computes window size dynamically across the image pixel to pixel using the stroke width transform (SWT). This led to reduction in number of manually adjusted parameters. The results are compared with the nine existing techniques using the quantitative measures: FM, PSNR, NRM, MPM, and DRD. The results show that the proposed method outperforms existing methods for images having variable stroke widths and text sizes.																	0952-1976	1873-6769				JUN	2020	92								103672	10.1016/j.engappai.2020.103672													
J								A new W-SVM kernel combining PSO-neural network transformed vector and Bayesian optimized SVM in GDP forecasting	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Artificial intelligence; Bayesian optimization; Gross domestic product; Support vector machines; Swarm intelligence	PARTICLE SWARM OPTIMIZATION; ARTIFICIAL-INTELLIGENCE; SUPPORT; PREDICTION; ALGORITHM; CLASSIFICATION; MACHINES; DESIGN	Considering that in the literature there is a very limited number of studies proposing new SVM kernels especially in regression problems, the scope of this research is to investigate the development of a novel Support Vector Machine Kernel. The proposed new W-SVM (Weighted-SVM) kernel was developed by applying a suitably transformed weight vector derived from particle swarm optimized neural networks in order to satisfy the kernel conditions of Mercer's theorem and then incorporated to a Bayesian Optimized (BO) kernel for building the new proposed W-SVM kernel. The proposed SVM kernel was applied in Gross Domestic Product growth forecasting. The new kernel has led to significantly improved forecasting results compared to all the other conventional ANN, SVM, and optimized BO-SVM, PSO-ANN machine learning models.																	0952-1976	1873-6769				JUN	2020	92								103650	10.1016/j.engappai.2020.103650													
J								A novel three-way decision method in a hybrid information system with images and its application in medical diagnosis	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Three-way decisions; Medical diagnosis; Hybrid information system with images; Hybrid distance; Loss function	THEORETIC ROUGH SET; MODEL; GRANULATION	Three-way decisions are effective and heuristic methods in information processing, moreover, it provides a trisecting-and-acting framework for complex problem solving. In this paper, combining with the practical application scenario, we propose a novel three-way decisions approach and apply it to medical diagnosis. First, we build an information system which is called a hybrid information system with images by considering the characteristics of the examination items of nephritis, including urinary color, urinary tuberculosis, pH, red blood cell count, urinary irritation, computed tomography, white blood cell count and so on. Second, to describe two objects of the conditional attribute set in a hybrid information system with images, we propose the hybrid distance based on Euclidean distance. Then, the tolerance relation induced by this system is constructed. In addition, considering that missing values exist in a hybrid information system with images, interval-valued numbers are used to obtain the loss function. Given different types of parameters can respond the level of the tolerance relation and the risk preference of decision makers, and the decision rules are shown in tabular forms. Finally, an illustration is showed to verify the feasibility and reasonability of the proposed method.																	0952-1976	1873-6769				JUN	2020	92								103651	10.1016/j.engappai.2020.103651													
J								Granular space, knowledge-encoded deep learning architecture and remote sensing image classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep auto-encoder; Remote sensing image classification; Information granulation; Wavelet transform; Rough sets; Knowledge-encoding	SPECTRAL-SPATIAL CLASSIFICATION; NEURAL-NETWORKS; HYPERSPECTRAL IMAGES; REDUCTION; FRAMEWORK	Hand-crafted features of remotely sensed (RS) image require the involvement of expensive human experts for classification. This factor motivates for designing the classification model with representative feature learning-based deep architecture to automate the feature extraction process and improve the generalization capability of the model. With this reasoning, we propose a deep auto-encoder neural network (NN) architecture with knowledge-encoded granular space for the classification of RS images. The network works with wavelet-rough granulated spaces and its architecture is designed with the encoded domain knowledge that strategically initializes the network parameters. Mostly, the learning time and performance of deep auto-encoders are persuaded by randomly selected weights and thus, we aim here to minimize these efforts with the domain knowledge. Neighborhood rough sets (NRS) are used to encode the domain knowledge and explore the contextual information for improved decision. We perform the knowledge-encoding operation for all stages of the auto-encoder. The proposed model thus exploits the mutual merits of deep network, wavelet-rough granular space and knowledge-encoding method. Comparative experimental results with multispectral and hyperspectral RS images demonstrate the superiority of our model to the related advanced methods.																	0952-1976	1873-6769				JUN	2020	92								103647	10.1016/j.engappai.2020.103647													
J								Multiple model extended continuous ant colony filter applied to real-time wind estimation in a fixed-wing UAV	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multiple model filter; Heuristic filter; Extended Continuous Ant Colony Filter; Multiple model wind estimation; Hardware in the loop	STRONG OBSERVABILITY; NONLINEAR-SYSTEMS; PARTICLE FILTERS; STATE ESTIMATION; KALMAN FILTER; OPTIMIZATION; ALGORITHM; TRACKING	In this study, a new heuristic multiple model filter, called Multiple Model Extended Continuous Ant Colony Filter, is proposed to solve a nonlinear multiple model state estimation problem. In this filter, a bank of extended continuous ant colony filters are run in parallel to solve the multiple model estimation problem. The probability of each model is continually updated and consequently both the true model and the states of the nonlinear system are updated based on the weighted sum of the fillers. The new multiple model filter is tested on an engineering problem. The problem is to estimate simultaneously the states of a fixed-wing unmanned aerial vehicle as well as the wind model, applied to the system. Four different wind models are considered and the proposed filter is unaware of the wind type. Then, observability of the states and the wind components are analyzed. Four new propositions are introduced and proved for unknown input observability, state and unknown input observability, the effect of time-varying unknown input matrix on the unknown input observability, and the effect of linearization errors on the state observability. Moreover, observability of the wind parameters is analyzed based on the nonlinear systems observability theory. Performance of the proposed filter is also evaluated in maneuvering flight and compared to a single extended continuous ant colony filter and a multiple model extended Kalman filter. A hardware-in-the-loop experiment is also performed to verify the real-time implementation capability of the suggested architecture.																	0952-1976	1873-6769				JUN	2020	92								103629	10.1016/j.engappai.2020.103629													
J								A novel approach to solve AI planning problems in graph transformations	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Bayesian Optimization Algorithm; AI planning; Graph transformation system; Bayesian network; Refinement	MODEL CHECKING; HEURISTIC SOLUTION; SEARCH	The aim of AI planning is to solve the problems with no exact solution available. These problems usually have a big search space, and planning may not find plans with the least actions and in the shortest time. Recent researches show that using suitable heuristics can help to find desired plans. In planning problems specified formally through graph transformation system (GTS), there are dependencies between applied rules (actions) in the search space. This fact motivates us to solve the planning problem for a small goal (instead of the main goal), extract dependencies from the searched space, and use these dependencies to solve the planning problem for the main goal. In GTS based systems, the nodes of a state (really is a graph) can be grouped due to their type. To create a small (refined) goal, we use a refinement technique to remove the predefined percent of nodes from each group of the main goal. Bayesian Optimization Algorithm (BOA) is then used to solve the planning problem for the refined goal. BOA is an Estimation of Distribution Algorithm (EDA) in which Bayesian networks are used to evolve the solution populations. Actually, a Bayesian network is learned from the current population, and then this network is employed to generate the next population. Since the last Bayesian network learned in BOA has the knowledge about dependencies between applied rules, this network can be used to solve the planning problem for the main goal. Experimental results on four well-known planning domains confirm that the proposed approach finds plans with the least actions and in the lower time compared with the state-of-the-art approaches.																	0952-1976	1873-6769				JUN	2020	92								103684	10.1016/j.engappai.2020.103684													
J								A bibliometric analysis and cutting-edge overview on fuzzy techniques in Big Data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Big data; Fuzzy sets; Type-2 fuzzy sets; Bibliometric study; Web of science; Scopus	CLASSIFICATION SYSTEMS; CLUSTERING-ALGORITHM; FEATURE-SELECTION; INFORMATION; REPRESENTATION; OPERATIONS; MANAGEMENT; INDEX; SETS	Over the last few years, Big Data has gained a tremendous attention from the research community. The data being generated in huge quantity from almost every field is unstructured and unprocessed. Extracting knowledge base and useful information from the big raw data is one of the major challenges, present today. Various computational intelligence and soft computing techniques have been proposed for efficient big data analytics. Fuzzy techniques are one of the soft computing approaches which can play a very crucial role in current big data challenges by pre-processing and reconstructing data. There is a wide spread application domains where traditional fuzzy sets (type-1 fuzzy sets) and higher order fuzzy sets (type-2 fuzzy sets) have shown remarkable outcomes. Although, this research domain of "fuzzy techniques in Big Data" is gaining some attention, there is a strong need for a motivation to encourage researchers to explore more in this area. In this paper, we have conducted bibliometric study on recent development in the field of "fuzzy techniques in big data". In bibliometric study, various performance metrics including total papers, total citations, and citation per paper are calculated. Further, top 10 of most productive and highly cited authors, discipline, source journals, countries, institutions, and highly influential papers are also evaluated. Later, a comparative analysis is performed on the fuzzy techniques in big data after analysing the most influential works in this field.																	0952-1976	1873-6769				JUN	2020	92								103625	10.1016/j.engappai.2020.103625													
J								DLCSS: A new similarity measure for time series data mining	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Time series; Data mining; Similarity measurement; Longest common subsequence; Dynamic time warping; Developed longest common subsequence		The Longest Common Subsequence (LCSS) is considered as a classic problem in computer science. In most studies related to time series data mining, LCSS had been mentioned as the best and the most usable similarity measurement method. The results of time series data mining under LCSS strongly depend on the similarity threshold, because the similarity measurement approach in LCSS is a zero-one approach. Since there is no knowledge about the data, and it is very difficult to determine the right amount of similarity threshold, using LCSS can actually lead to poor results. In this research, a new similarity measurement method named Developed Longest Common Subsequence (DLCSS) has been suggested for time series data mining based on LCSS. In DLCSS, by defining two similarity thresholds and determining their values, LCSS' shortcoming was eliminated. The performance of DLCSS was compared with performance of LCSS and Dynamic Time Warping (DTW) using 1-Nearest neighbor and k-medoids clustering techniques. This evaluation was carried out on 63 time series datasets of UCR collection. Using these results, it could be claimed that the 1-NN accuracy and clustering accuracy under DLCSS is better than that of under LCSS and DTW with at least 99.5% and 99% confidence, respectively. Also, DLCSS has better effect in correctly predicting the number of clusters compared to LCSS and DTW. In addition, the effect of DLCSS in determining the better cluster representatives is greater than that of under LCSS and DTW with at least 99.95% confidence.																	0952-1976	1873-6769				JUN	2020	92								103664	10.1016/j.engappai.2020.103664													
J								A novel machine learning technique for computer-aided diagnosis	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Computer-aided diagnosis; Artificial neural network; Artificial bee colony algorithm; Pruning; Logic circuit	NEURON MODEL; MEDICAL DIAGNOSIS; ENSEMBLE METHODS; LUNG-CANCER; PREDICTION; DISEASE; ALGORITHM; SELECTION; NETWORKS	The primary motivation of this paper is twofold: first, to employ a heuristic optimization algorithm to optimize the dendritic neuron model (DNM) and second, to design a tidy visual classifier for computer-aided diagnosis that can be easily implemented on a hardware system. Considering that the backpropagation (BP) algorithm is sensitive to the initial conditions and can easily fall into local minima, we propose an evolutionary dendritic neuron model (EDNM), which is optimized by the gbest-guided artificial bee colony (GABC) algorithm. The experiments are performed on the Liver Disorders Data Set, the Wisconsin Breast Cancer Data Set, the Haberman's Survival Data Set, the Diabetic Retinopathy Debrecen Data Set and Hepatitis Data Set, and the effectiveness of our model was rigorously validated in terms of the classification accuracy, the sensitivity, the specificity, the F_measure, Cohen's Kappa, the area under the receiver operating characteristic curve (AUC), convergence speed and the statistical analysis of the Wilcoxon signed-rank test. Moreover, after training, the EDNM can simplify its neural structure by removing redundant synapses and superfluous dendrites by the neuronal pruning mechanism. Finally, the simplified structural morphology of the EDNM can be replaced by a logic circuit (LC) without sacrificing accuracy. It is worth emphasizing that once implemented by an LC, the model has a significant advantage over other classifiers in terms of speed when handling big data. Consequently, our proposed model can serve as an efficient medical classifier with excellent performance.																	0952-1976	1873-6769				JUN	2020	92								103627	10.1016/j.engappai.2020.103627													
J								A weighted corrective fuzzy reasoning spiking neural P system for fault diagnosis in power systems with variable topologies	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Fault diagnosis; Power system; Spiking neural P system; Fuzzy reasoning; Membrane computing; Cause-effect network	SECTION ESTIMATION; EXPERT-SYSTEM; PETRI-NETS; NETWORK	This paper focuses on power system fault diagnosis based on Weighted Corrective Fuzzy Reasoning Spiking Neural P Systems with real numbers (rWCFRSNPSs) to propose a graphic fault diagnosis method, called FD-WCFRSNPS. In the FD-WCFRSNPS, an rWCFRSNPS is proposed to model the logical relationships between faults and potential warning messages triggered by the corresponding protective devices. In addition, a matrix-based reasoning algorithm for the rWCFRSNPS is devised to reason about the fault alarm messages using parallel representations. Besides, a layered modeling method based on rWCFRSNPSs is developed to adapt to topological changes in power systems and a Temporal Order Information Processing Method based on Cause-Effect Networks is designed to correct fault alarm messages before the fault reasoning. Finally, in a case study considering a local subsystem of a 220kV power system, the diagnosis results of five test cases prove that the proposed FD-WCFRSNPS is viable and effective.																	0952-1976	1873-6769				JUN	2020	92								103680	10.1016/j.engappai.2020.103680													
J								A T-S fuzzy model identification approach based on evolving MIT2-FCRM and WOS-ELM algorithm	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										T-S fuzzy model identification; Evolving MIT2-FCRM; Gravitational search algorithm (GSA); Hyper-plane-shaped membership function (MF); WOS-ELM algorithm	SYSTEMS; REGRESSION; NETWORK; CLASSIFICATION; PREDICTION	Inter type-2 fuzzy model has been confirmed to be more effective in Takagi-Sugeno (T-S) fuzzy model identification compared to type-1 fuzzy model. It is indisputable that some algorithms based on inter type-2 fuzzy model have already been developed and shown remarkable modeling performance. To further improve the modeling accuracy, the optimization methods and the neural network are taken into consideration. In this paper, an evolving modified inter type-2 fuzzy c-regression model (MIT2-FCRM) algorithm based on gravitational search algorithm (GSA) and a consequent parameter identification method based on extreme learning machine algorithm with forgetting factor for processing online sequences (namely WOS-ELM) were proposed. Then a novel approach for T-S fuzzy modeling was presented, in which, the coefficients of the upper and lower hyperplanes were obtained by evolving MIT2-FCRM algorithm based on GSA, a hyper-plane-shaped membership function (MF) was utilized to identify the antecedent parameters of the T-S fuzzy model, and WOS-ELM was employed to identify the consequent parameters. The modeling results of six examples indicate that the proposed approach is superior to other studies in terms of identification accuracy, compact fuzzy rules and noise resistance ability.																	0952-1976	1873-6769				JUN	2020	92								103653	10.1016/j.engappai.2020.103653													
J								TextTricker: Loss-based and gradient-based adversarial attacks on text classification models	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Adversarial attacks; Text classification; The loss-based implementation; The gradient-based implementation		Adversarial examples are generated by adding infinitesimal perturbations to legitimate inputs so that incorrect predictions can be induced into deep learning models. They have received increasing attention recently due to their significant values in evaluating and improving the robustness of neural networks. While adversarial attack algorithms have achieved notable advancements in the continuous data of images, they cannot be directly applied for discrete symbols such as text, where all the semantic and syntactic constraints in languages are expected to be satisfied. In this paper, we propose a while-box adversarial attack algorithm, TextTricker, which supports both targeted and non-targeted attacks on text classification models. Our algorithm can be implemented in either a loss-based way, where word perturbations are performed according to the change in loss, or a gradient-based way, where the expected gradients are computed in the continuous embedding space to restrict the perturbations towards a certain direction. We perform extensive experiments on two publicly available datasets and three state-of-the-art text classification models to evaluate our algorithm. The empirical results demonstrate that TextTricker performs notably better than baselines in attack success rate. Moreover, we discuss various aspects of TextTricker in details to provide a deep investigation and offer suggestions for its practical use.																	0952-1976	1873-6769				JUN	2020	92								103641	10.1016/j.engappai.2020.103641													
J								Fractional-order cuckoo search algorithm for parameter identification of the fractional-order chaotic, chaotic with noise and hyper-chaotic financial systems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Noise; Benchmak; Optimization; Cuckoo search; Fractional calculus; Genetic Algorithm; Chaotic financial systems; Particle Swarm Optimization; Hyper-chaotic financial systems; Fractional-order optimization algorithms	BEE COLONY ALGORITHM; OPTIMIZATION ALGORITHM; NONLINEAR DYNAMICS; DESIGN; CONTROLLER; VARIANTS; CALCULUS	Identifying the parameters of the chaos phenomena in the economic-financial systems is a critical issue to control and avoid the financial crises and bogging the market down. Therefore, in this paper, an efficient and reliable optimization algorithm is developed to identify the corresponding parameters of that chaotic dynamical behavior in the fractional-order chaotic, chaotic with noise, and hyper-chaotic financial systems. The introduced algorithm is a cooperation among the fractional calculus (FC) perspective and the basic cuckoo search algorithm to enhance the stochastic cuckoo's walk via considering the cuckoo's earlier behaviors from memory. The developed fractional-order cuckoo search (FO-CS) is validated with twenty-eight functions of CEC2017 with different dimensions. Several measures and non-parametric statistical tests are presented to demonstrate the superiorly of the introduced algorithm while compared with the CS and the state-of-theart techniques. The results show that merging of FC properties magnifies CS's efficiency, convergence speed, and robustness against the complexly of the considered CEC benchmarks suite and the non-linearly of the fractional-order chaotic, chaotic with noise, and hyper-chaotic financial systems.																	0952-1976	1873-6769				JUN	2020	92								103662	10.1016/j.engappai.2020.103662													
J								Distributed gas concentration prediction with intelligent edge devices in coal mine	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Gas disaster; Intelligent edge system; Distributed gas concentration prediction; Industrial multidimensional data	DEPLOYMENT; ALGORITHM; MACHINE	Gas disaster can be triggered by gas concentrations exceeding standard levels, and gas concentration prediction system can reduce the occurrence of gas disaster by predicting the trend of gas concentration and alerting engineers to take necessary measures whenever needed. With the increasing use of intelligent edge devices in coal mines and the limitations of some existing systems, developing a new gas concentration prediction system for large-scale intelligent edge devices has become an important issue. This work proposes to address the issue through a novel method for predicting gas concentrations by taking full advantage of multidimensional data in an intelligent edge system. Specifically, 1) it proposed a Single hidden layer Random Weights Neural Network (SRWNN) as the prediction model, which is based on interval prediction rather than point prediction; 2) It employs a Non-dominated Sorting Genetic Algorithm II (NSGA-II) to train SRWNN; 3) To significantly reduce the time consumed during model training and facilitate real-time predictions, it proposes a distributed gas concentration prediction scheme based on an intelligent edge system; and 4) it conducts extensive experiments by using actual industrial data collected from a company to demonstrate the superior performance of the proposed method.																	0952-1976	1873-6769				JUN	2020	92								103643	10.1016/j.engappai.2020.103643													
J								Transmission Dynamics Model of Coronavirus COVID-19 for the Outbreak in Most Affected Countries of the World	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Coronavirus COVID-19; Stability; Sensitivity Analysis; Statistical Inference; Basic Reproductive Number		The wide spread of coronavirus (COVID-19) has threatened millions of lives and damaged the economy worldwide. Due to the severity and damage caused by the disease, it is very important to fore-tell the epidemic lifetime in order to take timely actions. Unfortunately, the lack of accurate information and unavailability of large amount of data at this stage make the task more difficult. In this paper, we used the available data from the mostly affected countries by COVID-19, (China, Iran, South Korea and Italy) and fit this with the SEIR type model in order to estimate the basic reproduction number R-0. We also discussed the development trend of the disease. Our model is quite accurate in predicting the current pattern of the infected population. We also performed sensitivity analysis on all the parameters used that are affecting the value of R-0.																	1989-1660					JUN	2020	6	2					7	10		10.9781/ijimai.2020.04.001													
J								COVID-19 Detection in Chest X-ray Images using a Deep Learning Approach	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										COVID-19; Deep Learning; Object Detection; X-ray		The Corona Virus Disease (COVID-19) is an infectious disease caused by a new virus that has not been detected in humans before. The virus causes a respiratory illness like the flu with various symptoms such as cough or fever that, in severe cases, may cause pneumonia. The COVID-19 spreads so quickly between people, affecting to 1,200,000 people worldwide at the time of writing this paper (April 2020). Due to the number of contagious and deaths are continually growing day by day, the aim of this study is to develop a quick method to detect COVID-19 in chest X-ray images using deep learning techniques. For this purpose, an object detection architecture is proposed, trained and tested with a public available dataset composed with 1500 images of non-infected patients and infected with COVID-19 and pneumonia. The main goal of our method is to classify the patient status either negative or positive COVID-19 case. In our experiments using SDD300 model we achieve a 94.92% of sensibility and 92.00% of specificity in COVID-19 detection, demonstrating the usefulness application of deep learning models to classify COVID-19 in X-ray images.																	1989-1660					JUN	2020	6	2					11	14		10.9781/ijimai.2020.04.003													
J								An Extreme Learning Machine-Relevance Feedback Framework for Enhancing the Accuracy of a Hybrid Image Retrieval System	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Color Moment; Extreme Learning Machine; Gray Level Co-occurrence Matrix; Relevance Feedback; Region Props Procedure	FEATURE-SELECTION; COLOR; TEXTURE; HISTOGRAMS; WAVELET	The process of searching, indexing and retrieving images from a massive database is a challenging task and the solution to these problems is an efficient image retrieval system. In this paper, a unique hybrid Content-based image retrieval system is proposed where different attributes of an image like texture, color and shape are extracted by using Gray level co-occurrence matrix (GLCM), color moment and various region props procedure respectively. A hybrid feature matrix or vector (HFV) is formed by an integration of feature vectors belonging to three individual visual attributes. This HFV is given as an input to an Extreme learning machine (ELM) classifier which is based on a solitary hidden layer of neurons and also is a type of feed-forward neural system. ELM performs efficient class prediction of the query image based on the pre-trained data. Lastly, to capture the high level human semantic information, Relevance feedback (RF) is utilized to retrain or reformulate the training of ELM. The advantage of the proposed system is that a combination of an ELM-RF framework leads to an evolution of a modified learning and intelligent classification system. To measure the efficiency of the proposed system, various parameters like Precision, Recall and Accuracy are evaluated. Average precision of 93.05%, 81.03%, 75.8% and 90.14% is obtained respectively on Corel-1K, Corel-5K, Corel-10K and GHIM-10 benchmark datasets. The experimental analysis portrays that the implemented technique outmatches many state-of-the-art related approaches depicting varied hybrid CBIR system.																	1989-1660					JUN	2020	6	2					15	27		10.9781/ijimai.2020.01.002													
J								Adjectives Grouping in a Dimensionality Affective Clustering Model for Fuzzy Perceptual Evaluation	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Affective Computing; Product Evaluation; Fuzzy Set; Clustering; Valence-Arousal-Dominance	EMOTION RECOGNITION; SENTIMENT ANALYSIS; K-MEDOIDS; DESIGN; IMPROVE; SYSTEMS	More and more products are no longer limited to the satisfaction of the basic needs, but reflect the emotional interaction between people and environment. The characteristics of user emotions and their evaluation scales are relatively simple. This paper proposes a three-dimensional space model valence-arousal-dominance (VAD) based on the theory of psychological dimensional emotions. It studies the clustering and evaluation of emotional phrases, called VAdC (VAD-dimensional clustering), which is a kind of the affective computing technology. Firstly, a Gaussian Mixture Model (GMM) based information presentation system was introduced, including the type of the presentation, such as single point, plain, and sphere. Subsequently, the border of the presentation was defined. To increase the ability of the proposed algorithm to handle a high dimensional affective space, the distance and inference mechanics were addressed to avoid lacking of local measurement by using fuzzy perceptual evaluation. By comparing the performance of the proposed method with fuzzy c-mean (FCM), k-mean, hard -c-mean (HCM), extra fuzzy c-mean (EFCM), the proposed VADdC performs high effectiveness in fitness, inter-distance, intra-distance, and accuracy. The results were based on the dataset created from a questionnaire on products of the Ming style chairs online evaluation system.																	1989-1660					JUN	2020	6	2					28	37		10.9781/ijimai.2020.05.002													
J								An Experimental Study on Microarray Expression Data from Plants under Salt Stress by using Clustering Methods	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Clustering Methods; Clustering Validity Indices; Gene Chips Analysis; Gene Expression; Plant Datasets	GENE-EXPRESSION; ALGORITHMS	Current Genome-wide advancements in Gene chips technology provide in the "Omics (genomics, proteomics and transcriptomics) research", an opportunity to analyze the expression levels of thousand of genes across multiple experiments. In this regard, many machine learning approaches were proposed to deal with this deluge of information. Clustering methods are one of these approaches. Their process consists of grouping data (gene profiles) into homogeneous clusters using distance measurements. Various clustering techniques are applied, but there is no consensus for the best one. In this context, a comparison of seven clustering algorithms was performed and tested against the gene expression datasets of three model plants under salt stress. These techniques are evaluated by internal and relative validity measures. It appears that the AGNES algorithm is the best one for internal validity measures for the three plant datasets. Also, K-Means profiles a trend for relative validity measures for these datasets.																	1989-1660					JUN	2020	6	2					38	47		10.9781/ijimai.2020.05.004													
J								A Holistic Methodology for Improved RFID Network Lifetime by Advanced Cluster Head Selection using Dragonfly Algorithm	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Wireless Sensor Networks (WSN); Sensor Nodes (SN); CH Node; Cluster; Cluster Head Selection		Radio Frequency Identification (RFID) networks usually require many tags along with readers and computation facilities. Those networks have limitations with respect to computing power and energy consumption. Thus, for saving energy and to make the best use of the resources, networks should operate and be able to recover in an efficient way. This will also reduce the energy expenditure of RFID readers. In this work, the RFID network life span will be enlarged through an energy-efficient cluster-based protocol used together with the Dragonfly algorithm. There are two stages in the processing of the clustering system: the cluster formation from the whole structure and the election of a cluster leader. After completing those procedures, the cluster leader controls the other nodes that are not leaders. The system works with a large energy node that provides an amount of energy while transmitting aggregated data near a base station.																	1989-1660					JUN	2020	6	2					48	55		10.9781/ijimai.2020.05.003													
J								Incremental Hierarchical Clustering driven Automatic Annotations for Unifying IoT Streaming Data	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										IoT Sensor Data; Semantics; Automatic Annotation; Incremental Hierarchical Clustering; Healthcare; Agent; SPARQL		In the Internet of Things (IoT), Cyber-Physical Systems (CPS), and sensor technologies huge and variety of streaming sensor data is generated. The unification of streaming sensor data is a challenging problem. Moreover, the huge amount of raw data has implied the insufficiency of manual and semi-automatic annotation and leads to an increase of the research of automatic semantic annotation. However, many of the existing semantic annotation mechanisms require many joint conditions that could generate redundant processing of transitional results for annotating the sensor data using SPARQL queries. In this paper, we present an Incremental Clustering Driven Automatic Annotation for IoT Streaming Data (IHC-AA-IoTSD) using SPARQL to improve the annotation efficiency. The processes and corresponding algorithms of the incremental hierarchical clustering driven automatic annotation mechanism are presented in detail, including data classification, incremental hierarchical clustering, querying the extracted data, semantic data annotation, and semantic data integration. The IHC-AA-IoTSD has been implemented and experimented on three healthcare datasets and compared with leading approaches namely-Agent-based Text Labelling and Automatic Selection (ATLAS), Fuzzy-based Automatic Semantic Annotation Method (FBASAM), and an Ontology-based Semantic Annotation Approach (OBSAA), yielding encouraging results with Accuracy of 86.67%, Precision of 87.36%, Recall of 85.48%, and F-score of 85.92% at 100k triple data.																	1989-1660					JUN	2020	6	2					56	70		10.9781/ijimai.2020.03.001													
J								NFC and VLC based Mobile Business Information System for Registering Class Attendance	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Class Attendance; Visible Light Communications; NFC; Presence Information Systems	VISIBLE-LIGHT COMMUNICATION; POSITIONING SYSTEM; HIGHER-EDUCATION	This work proposes a Mobile Information System for class attendance control using Visible Light Communications (VLC), and the students' own mobile devices for automatic clocking in and clocking out. The proposed information system includes (a) VLC physical infrastructure, (b) native Android and iOS apps for the students, and (c) a web application for classroom attendance management. A proof of concept has been developed, setting up a testbed representing a real-world classroom environment for experimentation, using two VLC-enabled LED lighting sources. After three rounds of testing (n=225) under different conditions, it has been concluded that the system is viable and shows consistent positive detections when the smartphones are on the classroom desk within non-overlapped areas of the light circles generated by the LED lighting sources on the table surface. The performed tests also show that if mobile devices are placed within those overlapping areas, the likelihood of a detection error could increase up to nearly 10%, due to multipath effects, and actions can be taken should it happen. Finally, it has to be highlighted that the proposed autonomous class attendance system allows lecturers to focus on making the most of their time in class, transferring knowledge instead of spending time in attendance management task.																	1989-1660					JUN	2020	6	2					71	77		10.9781/ijimai.2020.05.001													
J								On Improvement of Speech Intelligibility and Quality: A Survey of Unsupervised Single Channel Speech Enhancement Algorithms	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Unsupervised Speech Enhancement; Speech Quality; Speech Intelligibility; Noise	SIGNAL SUBSPACE APPROACH; SPECTRAL SUBTRACTION; NORMAL-HEARING; MMSE ESTIMATION; NOISE; SUPPRESSION; AMPLITUDE; STRATEGY; MASKING; ENERGY	Many forms of human communication exist; for instance, text and nonverbal based. Speech is, however, the most powerful and dexterous form for the humans. Speech signals enable humans to communicate and this usefulness of the speech signals has led to a variety of speech processing applications. Successful use of these applications is, however, significantly aggravated in presence of the background noise distortions. These noise signals overlap and mask the target speech signals. To deal with these overlapping background noise distortions, a speech enhancement algorithm at front end is crucial in order to make noisy speech intelligible and pleasant. Speech enhancement has become a very important research and engineering problem for the last couple of decades. In this paper, we present an all-inclusive survey on unsupervised single-channel speech enhancement (U-SCSE) algorithms. A taxonomy based review of the U-SCSE algorithms is presented and the associated studies regarding improving the intelligibility and quality are outlined. The studies on the speech enhancement algorithms in unsupervised perspective are presented. Objective experiments have been performed to evaluate the potential of the U-SCSE algorithms in terms of improving the speech intelligibility and quality. It is found that unsupervised speech enhancement improves the speech quality but the speech intelligibility improvement is deprived. To finish, several research problems are identified that require further research.																	1989-1660					JUN	2020	6	2					78	89		10.9781/ijimai.2019.12.001													
J								A Collaborative Filtering Probabilistic Approach for Recommendation to Large Homogeneous and Automatically Detected Groups	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Collaborative Filtering Clustering; Dimensionality Reduction; Group Recommendation; Homogenous Groups; Recommender Systems	MATRIX FACTORIZATION; CLUSTERING-ALGORITHM; NONNEGATIVE MATRIX; SYSTEMS; MODEL; INFORMATION; NETWORK	In the collaborative filtering recommender systems (CFRS) field, recommendation to group of users is mainly focused on stablished, occasional or random groups. These groups have a little number of users: relatives, friends, colleagues, etc. Our proposal deals with large numbers of automatically detected groups. Marketing and electronic commerce are typical targets of large homogenous groups. Large groups present a major difficulty in terms of automatically achieving homogeneity, equilibrated size and accurate recommendations. We provide a method that combines diverse machine learning algorithms in an original way: homogeneous groups are detected by means of a clustering based on hidden factors instead of ratings. Predictions are made using a virtual user model, and virtual users are obtained by performing a hidden factors aggregation. Additionally, this paper selects the most appropriate dimensionality reduction for the explained RS aim. We conduct a set of experiments to catch the maximum cumulative deviation of the ratings information. Results show an improvement on recommendations made to large homogeneous groups. It is also shown the desirability of designing specific methods and algorithms to deal with automatically detected groups.																	1989-1660					JUN	2020	6	2					90	100		10.9781/ijimai.2020.03.002													
J								Tree Growth Algorithm for Parameter Identification of Proton Exchange Membrane Fuel Cell Models	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Proton Exchange Membrane Fuel Cell; Parameters Estimation; Tree Growth Algorithm; Total of the Squared Deviations (TSD)	OPTIMAL ALLOCATION; PEMFC MODEL; OPTIMIZATION; FLOW; PERFORMANCE; TRANSPORT; STRATEGY	Demonstrating an accurate mathematical model is a mandatory issue for realistic simulation, optimization and performance evaluation of proton exchange membrane fuel cells (PEMFCs). The main goal of this study is to demonstrate a precise mathematical model of PEMFCs through estimating the optimal values of the unknown parameters of these cells. In this paper, an efficient optimization technique, namely, Tree Growth Algorithm (TGA) is applied for extracting the optimal parameters of different PEMFC stacks. The total of the squared deviations (TSD) between the experimentally measured data and the estimated ones is adopted as the objective function. The effectiveness of the developed parameter identification algorithm is validated through four case studies of commercial PEMFC stacks under various operating conditions. Moreover, comprehensive comparisons with other optimization algorithms under the same study cases are demonstrated. Statistical analysis is presented to evaluate the accuracy and reliability of the developed algorithm in solving the studied optimization problem.																	1989-1660					JUN	2020	6	2					101	111		10.9781/ijimai.2020.03.003													
J								Time-Dependent Performance Prediction System for Early Insight in Learning Trends	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										E-learning; Education; Learning Analytics; Learning Management Systems; Prediction; Support Vector Machine		Performance prediction systems allow knowing the learning status of students during a term and produce estimations on future status, what is invaluable information for teachers. The majority of current systems statically classify students once in time and show results in simple visual modes. This paper presents an innovative system with progressive, time-dependent and probabilistic performance predictions. The system produces by-weekly probabilistic classifications of students in three groups: high, medium or low performance. The system is empirically tested and data is gathered, analysed and presented. Predictions are shown as point graphs over time, along with calculated learning trends. Summary blocks are with latest predictions and trends are also provided for teacher efficiency. Moreover, some methods for selecting best moments for teacher intervention are derived from predictions. Evidence gathered shows potential to give teachers insights on students' learning trends, early diagnose learning status and selecting best moment for intervention.																	1989-1660					JUN	2020	6	2					112	124		10.9781/ijimai.2020.05.006													
J								Two-Stage Human Activity Recognition Using 2D-ConvNet	INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL INTELLIGENCE										Activities Recognition; Random Forest; 2D Convolution Neural Network; Intelligent Monitoring System	HUMAN MOTION ANALYSIS; VIDEO SURVEILLANCE	There is huge requirement of continuous intelligent monitoring system for human activity recognition in various domains like public places, automated teller machines or healthcare sector. Increasing demand of automatic recognition of human activity in these sectors and need to reduce the cost involved in manual surveillance have motivated the research community towards deep learning techniques so that a smart monitoring system for recognition of human activities can be designed and developed. Because of low cost, high resolution and ease of availability of surveillance cameras, the authors developed a new two-stage intelligent framework for detection and recognition of human activity types inside the premises. This paper, introduces a novel framework to recognize single-limb and multi-limb human activities using a Convolution Neural Network. In the first phase single-limb and multi-limb activities are separated. Next, these separated single and multi-limb activities have been recognized using sequence-classification. For training and validation of our framework we have used the UTKinect-Action Dataset having 199 actions sequences performed by 10 users. We have achieved an overall accuracy of 97.88% in real-time recognition of the activity sequences.																	1989-1660					JUN	2020	6	2					125	135		10.9781/ijimai.2020.04.002													
J								Designing hybrid classifiers based on general type-2 fuzzy logic and support vector machines	SOFT COMPUTING										alpha-planes; Type-2 fuzzy logic; General type-2 fuzzy logic; Support vector machines	INTERVAL TYPE-2; DATA CLASSIFICATION; FAULT-DETECTION; SYSTEMS; CONSTRUCTION; REDUCTION; ALGORITHM; ANFIS	This paper describes two alternatives for hybridizing general type-2 fuzzy logic with the Support Vector Machine (SVM), which is one of the best classification methods in the literature. The main idea of using type-2 fuzzy logic is providing SVM with the ability for uncertainty handling in real-world situations, which suffer from dynamic changes and multiple sources of uncertainty. Two approaches for general type-2 fuzzy hybrid classifiers are proposed, tested and compared based on benchmark data sets. In order to find the best hybrid combination of these methods a comparison has been realized with different experiments using diagnosis benchmark datasets by measuring the classifier accuracy. The first approach consists on using fuzzy rules as additional features to the SVM in order to increase the separability of the data. On the other hand, the second approach consists on defining the Sugeno coefficients for a general type-2 fuzzy classifier as elements of the optimal hyperplane obtained by the SVM method. The motivation for proposing these hybrid approaches is finding the best classifier combining the abilities of the original methods, which are robustness and uncertainty handling. The conclusion based on the experimental results is that the hybrid combination of both methods produces a classifier that is better than the original individual approaches.																	1432-7643	1433-7479															10.1007/s00500-020-05052-x		JUN 2020											
J								Machine learning's limitations in avoiding automation of bias	AI & SOCIETY										Machine learning; Bias; Bias automation; Artificial intelligence	PREDICTION	The use of predictive systems has become wider with the development of related computational methods, and the evolution of the sciences in which these methods are applied Solon and Selbst (Calif L REV 104: 671-732, 2016) and Pedreschi et al. (2007). The referred methods include machine learning techniques, face and/or voice recognition, temperature mapping, and other, within the artificial intelligence domain. These techniques are being applied to solve problems in socially and politically sensitive areas such as crime prevention and justice management, crowd management, and emotion analysis, just to mention a few. However, dissimilar predictions can be found nowadays as the result of the application of these methods resulting in misclassification, for example for the case of conviction risk assessment Office of Probation and Pretrial Services (2011) or decision-making process when designing public policies Lange (2015). The goal of this paper is to identify current gaps on fairness achievement within the context of predictive systems in artificial intelligence by analyzing available academic and scientific literature up to 2020. To achieve this goal, we have gathered available materials at the Web of Science and Scopus from last 5 years and analyzed the different proposed methods and their results in relation to the bias as an emergent issue in the Artificial Intelligence field of study. Our tentative conclusions indicate that machine learning has some intrinsic limitations which are leading to automate the bias when designing predictive algorithms. Consequently, other methods should be explored; or we should redefine the way current machine learning approaches are being used when building decision making/decision support systems for crucial institutions of our political systems such as the judicial system, just to mention one.																	0951-5666	1435-5655															10.1007/s00146-020-00996-y		JUN 2020											
J								Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning	APPLIED INTELLIGENCE										Multi-objective reinforcement learning; Multi-policy reinforcement learning; Pareto frontier; Sampling efficiency	COVARIANCE-MATRIX UPDATE; KRILL HERD ALGORITHM; OPTIMIZATION; SELECTION; REPRESENTATION; GAME; GO	Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method.																	0924-669X	1573-7497				OCT	2020	50	10					3301	3317		10.1007/s10489-020-01702-7		JUN 2020											
J								Taking stock of legal ontologies: a feature-based comparative analysis	ARTIFICIAL INTELLIGENCE AND LAW										Legal ontologies; Semantic web; Modelling legal knowledge		Ontologies represent the standard way to model the knowledge about specific domains. This holds also for the legal domain where several ontologies have been put forward to model specific kinds of legal knowledge. Both for standard users and for law scholars, it is often difficult to have an overall view on the existing alternatives, their main features and their interlinking with the other ontologies. To answer this need, in this paper, we address an analysis of the state-of-the-art in legal ontologies and we characterise them along with some distinctive features. This paper aims to guide generic users and law experts in selecting the legal ontology that better fits their needs and in understanding its specificity so that proper extensions to the selected model could be investigated.																	0924-8463	1572-8382				JUN	2020	28	2					207	235		10.1007/s10506-019-09252-1													
J								Using machine learning to predict decisions of the European Court of Human Rights	ARTIFICIAL INTELLIGENCE AND LAW										Machine learning; Case law; European Court of Human Rights; Natural language processing; Judicial decisions	NETWORK ANALYSIS; LAW; CITATIONS; JUDICIARY; BEHAVIOR; JUDGES	When courts started publishing judgements, big data analysis (i.e. large-scale statistical analysis of case law and machine learning) within the legal domain became possible. By taking data from the European Court of Human Rights as an example, we investigate how natural language processing tools can be used to analyse texts of the court proceedings in order to automatically predict (future) judicial decisions. With an average accuracy of 75% in predicting the violation of 9 articles of the European Convention on Human Rights our (relatively simple) approach highlights the potential of machine learning approaches in the legal domain. We show, however, that predicting decisions for future cases based on the cases from the past negatively impacts performance (average accuracy range from 58 to 68%). Furthermore, we demonstrate that we can achieve a relatively high classification performance (average accuracy of 65%) when predicting outcomes based only on the surnames of the judges that try the case.																	0924-8463	1572-8382				JUN	2020	28	2					237	266		10.1007/s10506-019-09255-y													
J								ICAIL Doctoral Consortium, Montreal 2019	ARTIFICIAL INTELLIGENCE AND LAW										ICAIL 2019; Doctoral Consortium; Conference report	PRIVACY	This is a report on the Doctoral Consortium co-located with the 17th International Conference on Artificial Intelligence and Law in Montreal.																	0924-8463	1572-8382				JUN	2020	28	2					267	280		10.1007/s10506-020-09267-z													
J								Triangular interval type-2 fuzzy soft set and its application	COMPLEX & INTELLIGENT SYSTEMS										Triangular interval type-2 fuzzy soft set; Aggregation operators; Properties; Decision-making problem	DECISION-MAKING; LOGIC SYSTEMS; EXPERT-SYSTEM; T-NORMS	Decision-making is an essential task in Science and Engineering. Since most of the real-world problems have uncertainty in nature, making the decision is challengeable one for the decision makers. Soft set has the advantage of free from the deficiency of the parameterization tools of existing theories, namely probability, fuzzy theory and the theory of rough sets. Linguistic terms mean different things to different people, so variability in expert's acceptance degree is possible. Here usage of type-1 fuzzy leads to noisy and uncertain, and the parameters also may be noisy and hence type-2 fuzzy sets may be used to address the mentioned issues. Therefore, a triangular interval type-2 fuzzy soft set has been considered in the present work by combining triangular interval type-2 fuzzy set and soft set. In this paper, a triangular interval type-2 fuzzy soft weighted arithmetic operator (TIT2FSWA) has been proposed with its desired mathematical properties; also applied the proposed methodology in a decision-making problem for profit analysis. Further comparative analysis has been made with the existing methods to show the effectiveness of the proposed method.																	2199-4536	2198-6053				OCT	2020	6	3					531	544		10.1007/s40747-020-00151-6		JUN 2020											
J								Simultaneous feature selection and clustering of micro-array and RNA-sequence gene expression data using multiobjective optimization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Gene expression data clustering; Feature selection; Point symmetry based distance; Multiobjective optimization; Cluster validity index	ALGORITHM; DISTANCE	In this paper, we have devised a multiobjective optimization solution framework for solving the problem of gene expression data clustering in reduced feature space. Here clustering problem is viewed from two different aspects: clustering of genes in reduced sample space or clustering of samples in reduced gene space. Three objective functions: two internal cluster validity indices and the count on the number of features are optimized simultaneously by a popular multiobjective simulated annealing based approach, namely AMOSA. Here, point symmetry based distance is used for the assignment of gene data points to different clusters. Seven publicly available benchmark gene expression data sets are used for experimental purpose. Both aspects of clustering in reduced feature space is demonstrated. The proposed gene expression clustering technique outperforms the existing nine clustering techniques. Apart from this, also some statistical and biological significant tests have been carried out to show that the proposed FSC-MOO technique is more statistically and biologically enriched																	1868-8071	1868-808X				NOV	2020	11	11					2541	2563		10.1007/s13042-020-01139-x		JUN 2020											
J								A Hybrid Multilingual Fuzzy-Based Approach to the Sentiment Analysis Problem Using SentiWordNet	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Twitter; opinion mining; Sentiment analysis; fuzzy logic; SentiWordNet; Big Data	SOCIAL NETWORKS; CLASSIFICATION	Sentiment Analysis or in particular social network analysis (SNA) is a new research area which is increased explosively. This domain has become a very active research issue in data mining and natural language processing. Sentiment analysis (opinion mining) consists in analyzing and extracting emotions, opinions or attitudes from product's reviews, movie's reviews, etc., and classify them into classes such as positive, negative and neutral, or extract the degree of importance (polarity). In this paper, we propose a new hybrid approach for classifying tweets into classes based on fuzzy logic and a lexicon based approach using SentiWordnet. Our approach consists in classifying tweets according to three classes: positive, negative or neutral, using SentiWordNet and the fuzzy logic with its three important steps: Fuzzification, Rule Inference/aggregation, and Defuzzification. The dataset of tweets to classify and the result of the classification are stored in the Hadoop Distributed File System (HDFS), and we use the Hadoop MapReduce for the application of our proposal.																	0218-4885	1793-6411				JUN	2020	28	3					361	390		10.1142/S0218488520500154													
J								Self-Adaptive Optimization for Improved Data Sanitization and Restoration	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Data sanitization; sensitive rules; privacy-preserving data mining; SAFF algorithm; restoration	ASSOCIATION RULES; GENETIC ALGORITHMS; ANOMALY DETECTION; PLAINTEXT ATTACK; PRIVACY; PSO; NOISE; CLOUD	Nowadays, Data Sanitization is considered as a highly demanded area for solving the issue of privacy preservation in Data mining. Data Sanitization, means that the sensitive rules given by the users with the specific modifications and then releases the modified database so that, the unauthorized users cannot access the sensitive rules. Promisingly, the confidentiality of data is ensured against the data mining methods. The ultimate goal of this paper is to build an effective sanitization algorithm for hiding the sensitive rules given by users/experts. Meanwhile, this paper concentrates on minimizing the four sanitization research challenges namely, rate of hiding failure, rate of Information loss, rate of false rule generation and degree of modification. Moreover, this paper proposes a heuristic optimization algorithm named Self-Adaptive Firefly (SAFF) algorithm to generate the small length key for data sanitization and also to adopt lossless data sanitization and restoration. The generated optimized key is used for both data sanitation as well as the data restoration process. The proposed SAFF-based algorithm is compared and examined against the other existing sanitizing algorithms like Fire Fly (FF), Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and Differential Evolution algorithm (DE) algorithms and the results have shown the excellent performance of proposed algorithm. The proposed algorithm is implemented in JAVA. The data set used are Chess, Retail, T10, and T40.																	0218-4885	1793-6411				JUN	2020	28	3					391	420		10.1142/S0218488520500166													
J								A Lexicographic Approach to Fuzzy Linear Assignment Problems with Different Types of Fuzzy Numbers	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Fuzzy assignment; lexicographic assignment; fuzzy numbers; ranking of fuzzy numbers	RANKING; ALGORITHM; SET	The fuzzy linear assignment problem (FLAP) is an extension of the classical linear assignment problem (LAP) to situations in which uncertainty in the cost coefficients is represented by fuzzy numbers. FLAP applications range from the assignment of workers to tasks to multiple-criteria decision analysis in fuzzy environments and many other engineering applications. Most FLAP formulations assume that all cost coefficients are fuzzy numbers of the same type (e.g. triangular, trapezoidal). The standard solution approach is the defuzzification of the cost coefficients, thus transforming the FLAP into a crisp LAP that can be solved by classical assignment algorithms such as the Hungarian method. It is known that defuzzification methods suffer from lack of discrimination when comparing fuzzy numbers which may lead to suboptimal assignments. The solution approach proposed in this paper is based on the theory of algebraic assignment problems and total orderings in the set of all fuzzy numbers, and it allows to solve FLAPs with different types of fuzzy numbers. More specifically, the FLAP is transformed into a lexicographic linear assignment problem (LLAP) which is solved in its place. We show, both theoretically and numerically, how this transformation overcomes the limitations present in existing approaches.																	0218-4885	1793-6411				JUN	2020	28	3					421	441		10.1142/S0218488520500178													
J								Hyperintensional Reasoning Based on Natural Language Knowledge Base	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										transparent intensional logic; hyperintensional logic; natural language analysis; context recognition; knowledge based system		The success of automated reasoning techniques over large natural-language texts heavily relies on a fine-grained analysis of natural language assumptions. While there is a common agreement that the analysis should be hyperintensional, most of the automatic reasoning systems are still based on an intensional logic, at the best. In this paper, we introduce the system of reasoning based on a fine-grained, hyperintensional analysis. To this end we apply Tichy's Transparent Intensional Logic (TIL) with its procedural semantics. TIL is a higher-order, hyperintensional logic of partial functions, in particular apt for a fine-grained natural-language analysis. Within TIL we recognise three kinds of context, namely extensional, intensional and hyperintensional, in which a particular natural-language term, or rather its meaning, can occur. Having defined the three kinds of context and implemented an algorithm of context recognition, we are in a position to develop and implement an extensional logic of hyperintensions with the inference machine that should neither over-infer nor under-infer.																	0218-4885	1793-6411				JUN	2020	28	3					443	468		10.1142/S021848852050018X													
J								Numerical Linear Programming under Non-Probabilistic Uncertainty Models - Interval and Fuzzy Sets	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Imprecise probability; linear programming; imprecise decision theory; maximinity and maximality	DECISION-MAKING; CONSTRAINTS	This paper considers a linear optimisation problem under uncertainty with at least one element modelled as a non-probabilistic uncertainty. The uncertainty is expressed in the coefficient matrices of constraints and/or coefficients of goal function. Previous work converts such problems to classical (linear) optimisation problems and eliminates uncertainty by converting the linear programming under uncertainty problem to a decision problem using imprecise probability and imprecise decision theory. Our aim here is to generalise this approach numerically and present three methods to calculate the solution. We investigate what numerical results can be obtained for interval and fuzzy types of uncertainty models and compare them to classical probabilistic cases - for two different optimality criteria: maximinity and maximality. We also provide an efficient method to calculate the maximal solutions in the fuzzy set model. A numerical example is considered for illustration of the results.																	0218-4885	1793-6411				JUN	2020	28	3					469	495		10.1142/S0218488520500191													
J								A New Fine-Kinney Method Based on Clustering Approach	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Fine Kinney method; clustering algorithms; K-means algorithms	RISK-ASSESSMENT APPROACH; EXTENDED MULTIMOORA; FMEA; AHP; CONSTRUCTION; SAFETY	In this study, a new approach to Fine-Kinney risk assessment method is developed in order to overcome the limitations of the conventional method with clustering algorithms. New risk level of classes are attempted to determine with K-Means and Hierarchical clustering algorithms with using two different distance functions which are Euclidean and Manhattan distances. According to the results, K-Means algorithms have provided accurate and sensitive cluster of classes. Classes from conventional and K-Means algorithms are applied and compared to the identified risks of a workshop of a medium sized textile company. Results of the study indicate that clustering techniques are new, original and applicable way to define new classes in order to prioritize risks by overcoming the drawbacks of conventional Fine-Kinney method.																	0218-4885	1793-6411				JUN	2020	28	3					497	512		10.1142/S0218488520500208													
J								On the Structure of the Classes of Copulas and Quasi-Copulas with a Given Diagonal Section	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										1-Lipschitz condition; copula; diagonal section; quasi-copula	CONSTRUCTIONS	In this paper we provide an alternative proof to that given in Ref. 1 to the fact that the class of multivariate copulas with a given diagonal section delta - denoted by C-delta - is a singleton if, and only if, the unique element of the class is the copula of comonotonicity among random variables. This will be used to prove that, for any diagonal delta different from the diagonal of the comotonic copula, C-delta is strictly included in the class of all multivariate quasi-copulas whose diagonal section is delta.																	0218-4885	1793-6411				JUN	2020	28	3					513	524		10.1142/S021848852050021X													
J								HoBAC: fundamentals, principles, and policies	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Security; Access Control; ABAC and HoBAC	ACCESS-CONTROL; THREATS; RBAC	Access Control (AC) is a critical and challenging security aspect within an IT infrastructure. Different AC models have been proposed to define AC policies that dictate the conditions under which a resource may be accessed by a subject. Attribute- Based Access Control (ABAC) is one of the most promising of those models and has received meaningful attention in recent years. Higher-order Attribute-Based Access Control (HoBAC) is a new AC model we recently proposed as a generalization of ABAC that offers more flexibility when designing AC policies. In this paper, theoretical foundations of HoBAC are further developed and an Access Control System (ACS) and an AC policy framework are presented. An application example related to the Internet of Things (IoT) is used to illustrate the different concepts of HoBAC.																	1868-5137	1868-5145															10.1007/s12652-020-02102-y		JUN 2020											
J								Improved performance of cloud servers using LBSDD factors of private cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Load balancing, security, and data deduplication (LBSDD); Distributed key based modified elliptic curve cryptographic with Caesar cipher (DKME4C); Hybrid grasshopper optimization algorithm (HGOA); Encrypted-hashed indexed technique (E-HIT); secure hash algorithm-512 (SHA-512)	DEDUPLICATION; SECURITY	The performance of service in cloud environment is a dominant factor which impacts the performance of the entire cloud system. However, the most organizations maintain their own private clouds to maintain their data and enable the access for the users of their own organization. There exist several load balancing and security protocols to access the services and maintain their data in the cloud environment but suffer to achieve higher performance. To handle this issue, Load Balancing, Security, and Data Deduplication (LBSDD) algorithm is presented. Initially, the request received from various users and various locations. Next, request and server related features are extracted. Then, select the best features from the extracted features using HGOA algorithm. After that the LBSDD factors are evaluated for the cloud performance. In evaluation user request is balanced by using Dropbox-NGINX tool with selected features. Next, the user may upload the files to the cloud server, so for providing security is an important factor here the security is maintained by using DKME4C algorithm. Then, the third factor is Data Deduplication evaluated using hashed indexes, tables, here the hash code is generated using the SHA-512 algorithm in this proposed method Data Deduplication is named as E-HIT. The proposed LBSDD algorithm achieves higher performance in server efficiency than other methods.																	1868-5137	1868-5145															10.1007/s12652-020-02125-5		JUN 2020											
J								A provenance based defensive technique to determine malevolent selective forwarding attacks in multi-hop wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Inter-packet delay; Provenance; Security; Selective forwarding attack; Sensor nodes	TRANSMISSION; SCHEME	Multi-hop wireless sensor networks are being implemented in various domains. The collected data are used to make important decisions for censorious infrastructures. Data are processed by a huge quantity of sensor nodes as well as handled on intermediate nodes which are known as hops on the way to the Base Station (BS) which performs the resolution accomplishing. Information have been collected from various sources and intermediate hopes will process and aggregate information. A malevolent opponent may interfere through the information via proposing extra node within the web infrastructure. There are chances to compromising the existing nodes. Malevolent Selective Forwarding attacks are most important protection problem toward the information forwarding within wireless sensor networks (WSN), as this might hinder broadcast of susceptible information as well as it reduces network throughput. Assuring data trustworthiness in this situation has been critical in support of accurate resolution accomplishing. Information provenance signifies a main aspect into analyzing the trustworthiness of wireless sensor data. In this paper we introduce a provenance based technique to determine the attack. Three phases are there in this scheme, detecting packet-loss in the first phase, second phase dealing with identifying attacked sensor node and isolating malevolent sensor node in the third phase. We have included the detailedanalysis of the investigational outcomes for showing the accurateness as well as effectiveness of the proposed system.																	1868-5137	1868-5145															10.1007/s12652-020-02079-8		JUN 2020											
J								SFSH: a novel smart factory SDN-layer handoff scheme in 5G-enabled mobile networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Industrial networks; Cross layer handoff; 5G mobile network; Software defined networks; Smart factory networking	MANAGEMENT; ALGORITHMS	A smart factory network integrates various wireless networks to provide administrators with advanced factory network management capabilities and optimized services. Supporting smooth handoff management is an important issue in network architecture. Existing handoff management protocols are not sufficient to ensure handoff support in smart factories. This work was conducted to achieve the agility required by the smart factory network. The Smart Factory SDN-layer Handoff (SFSH) technique is proposed to guarantee Quality of Service (QoS) by supporting efficient handoff of devices in a smart factory. We proposes the SDN framework called SFSH for smart factory and a handover algorithm considering and the Received Signal Strength (RSS) and speed for SFSH. Through simple simulation, the SDN framework can flexibly cope with the conditions required by the smart factory, and it can be seen that the performance of SFSH considering RSS is improved over the existing handover process. SFSH uses the location and handoff signaling-delay information of devices in the factory to support mobility management and improve handoff performance in a wireless network environment. The sensitivity of the link layer and network layer is analyzed for handoff performance. Theoretical analysis shows that SFSH works better performance than other solutions.																	1868-5137	1868-5145															10.1007/s12652-020-02101-z		JUN 2020											
J								MCAMO: multi constraint aware multi-objective resource scheduling optimization technique for cloud infrastructure services	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-objective scheduling; Resource constraints; Infrastructure services; Optimization; Cloud services	IAAS; ALGORITHM	In cloud computing infrastructure-based services, resource scheduling is still an open issue. Normally resource scheduling involves multi-objective fulfillment but often developed as single-objective problems and solutions are proposed. For dealing with multi-objective problems, optimization techniques come in-aid to develop various techniques as cloud resource scheduling is a soft computing problem. The ultimate aim of cloud resource scheduling is to reduce the billing cost of users and to increase the revenue of cloud service providers. In this paper, the MCAMO technique is proposed for cloud resource scheduling especially dealing with infrastructure-based cloud services. This method deals with multi-objective by applying multi constraints while resource scheduling in infrastructure cloud services. The proposed method is novel as it deals with the constraints of the submitted jobs along with fulfilling the objectives of the cloud service client. For a powerful arrangement, the fitness value worth takes a base worth value and the improved determination of the asset resources relies upon the MCAMO calculation. The performance of the MCAMO technique is assessed by comparing through few existing multi-objective constraints applied VM machines scheduling techniques using the cloudsim simulator. The comparison proves that the proposed MCAMO technique provides optimized resource scheduling than other methods.																	1868-5137	1868-5145															10.1007/s12652-020-02138-0		JUN 2020											
J								Modeling, learning, and simulating human activities of daily living with behavior trees	KNOWLEDGE AND INFORMATION SYSTEMS										Behavior tree; Machine learning; Visualization; Human activity modeling	ACTIVITY RECOGNITION	Autonomy is a key factor in the quality of life of a person. With the aging of the population, an increasing number of people suffers from a reduced level of autonomy. That compromises their capacity of performing their daily activities and causes safety issues. The new concept of ambient assisted living (AAL), and more specifically its application in smart homes for supporting elderly people, constitutes a great avenue of the solution. However, to be able to automatically assist a user carrying out is activities, researchers and engineers face three main challenges in the development of smart homes: (i) how to represent the activity models, (ii) how to automatically construct theses models based on historical data and (iii) how to be able to simulate the user behavior for tests and calibration purpose. Most of recent works addressing these challenges exploit simple models of activity with no semantic, or use logically complex ones or else use probabilistically rigid representations. In this paper, we propose a global approach to address the three challenges. We introduce a new way of modeling human activities in smart homes based on behavior trees which are used in the video game industry. We then present an algorithmic way to automatically learn these models with sensors logs. We use a simulator that we have developed to validate our approach.																	0219-1377	0219-3116				OCT	2020	62	10					3881	3910		10.1007/s10115-020-01476-x		JUN 2020											
J								Research on disaster evolution process in open-pit mining area based on space fault network	NEURAL COMPUTING & APPLICATIONS										Safety system engineering; Open-pit mining area; Disaster evolution process; Space fault network; Disaster mode and measurement	DIAGNOSIS; SYSTEMS	To study the space fault network theory and the area risk of open-pit mining area, the theory and method of space fault network are applied to the study of the disaster evolution process in open-pit mining area. The open-pit mine is close to a city and has complex hydrogeological conditions. The characteristics of disasters occurring in different areas are quite different, which lead to the diversity of the disaster evolution processes. It makes disaster analysis, prevention and management more difficult. Space fault network is a theory to study the evolution process of system faults. The evolution process can be studied from the perspective of macro-evolution process and micro-event relationship. The area characteristics of the open-pit mining area are studied and the area division is carried out. Based on the existing data, the space fault networks of disaster evolution processes in different areas are established. Space fault network is transformed into space fault tree, and disaster evolution models in different areas are simplified. Three indicators, structure importance, structure complexity and structure accessibility, are proposed to measure the possibility that the edge event will lead to the target event. The characteristics and relationships of these indicators are described. Finally, taking the evolution process of south wall disaster as an example, the above three indicators are calculated, and the edge events are sorted and the significance of sort is explained. This is the beginning of the application of space fault network to the study of disaster evolution process of open-pit mine.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16737	16754		10.1007/s00521-020-05025-z		JUN 2020											
J								RS-HeRR: a rough set-based Hebbian rule reduction neuro-fuzzy system	NEURAL COMPUTING & APPLICATIONS										Pattern classification; Neuro-fuzzy system; Hebbian-based rule reduction; Rough set; Rule reduction	PSEUDO-OUTER-PRODUCT; MULTIOBJECTIVE GENETIC OPTIMIZATION; COMPLEX-SYSTEMS; CLASSIFICATION; EVOLUTION; ACCURATE; MODEL; MLP	Interpretabilty is one of the desired characteristics in various classification task. Rule-based system and fuzzy logic can be used for interpretation in classification. The main drawback of rule-based system is that it may contain large complex rules for classification and sometimes it becomes very difficult in interpretation. Rule reduction is also difficult for various reasons. Removing important rules may effect in classification accuracy. This paper proposes a hybrid fuzzy-rough set approach named RS-HeRR for the generation of effective, interpretable and compact rule set. It combines a powerful rule generation and reduction fuzzy system, called Hebbian-based rule reduction algorithm (HeRR) and a novel rough-set-based attribute selection algorithm for rule reduction. The proposed hybridization leverages upon rule reduction through reduction in partial dependency as well as improvement in system performance to significantly reduce the problem of redundancy in HeRR, even while providing similar or better accuracy. RS-HeRR demonstrates these characteristics repeatedly over four diverse practical classification problems, such as diabetes identification, urban water treatment monitoring, sonar target classification, and detection of ovarian cancer. It also demonstrates excellent performance for highly biased datasets. In addition, it competes very well with established non-fuzzy classifiers and outperforms state-of-the-art methods that use rough sets for rule reduction in fuzzy systems.																	0941-0643	1433-3058															10.1007/s00521-020-04997-2		JUN 2020											
J								GMM discriminant analysis with noisy label for each class	NEURAL COMPUTING & APPLICATIONS										Gaussian mixture models; Label noise; Discriminant analysis; Maximum likelihood estimate	CLASSIFICATION	Real-world datasets often contain noisy labels, and learning from such datasets using standard classification approaches may not produce the desired performance. In this paper, we propose a Gaussian Mixture Discriminant Analysis (GMDA) with noisy label for each class. We introduce flipping probability and class probability and use EM algorithms to solve the discriminant problem with label noise. We also provide the detail proofs of convergence. Experimental results on synthetic and real-world datasets show that the proposed approach notably outperforms other four state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-05038-8		JUN 2020											
J								Trends in graph -based representations for Pattern Recognition	PATTERN RECOGNITION LETTERS										Graph-based representations; Graph matching; Graph edit distance; Graph kernels	KERNELS	In this paper we try to examine recent trends on the use of graph-based representations in Pattern Recognition, using as a vantage point the 11th IAPR-TC15 Workshop GbR2017, dedicated to this topic. A survey of the paper presented at GbR2017 will give us the opportunity to reflect on the directions where the interest of the research community working on this subject is moving. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						3	9		10.1016/j.patrec.2018.03.016													
J								Error-tolerant graph matching in linear computational cost using an initial small partial matching	PATTERN RECOGNITION LETTERS										Graph Edit Distance; Sub-optimal algorithm; Linear computational cost	EDIT DISTANCE	Error-tolerant graph matching has been demonstrated to be an NP-problem, therefore, its exact computation has an exponential computational cost and several sub-optimal algorithms have been presented with the aim of making the runtime acceptable in some applications. Some well-known sub-optimal algorithms have sixth, cubic or quadratic computational costs with respect to the order of the graphs. Although these computational costs could be considered very low, when applications deal with large graphs (for instance in social networks), the quadratic cost continues to be unacceptable. For this reason, we present an error-tolerant graph-matching algorithm that has a O(d(3.5) .) computational cost, d being the number of output edges per node and n the order of the graphs. Note that, usually, in social networks, it holds that d << n and for this reason we consider the cost to be linear, in other words O(k .), k being a low constant. Our method needs an initial seed, which is composed of one or several node-to-node mappings. The algorithm has been applied to analyse the evolution of social networks. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						10	19		10.1016/j.patrec.2018.04.003													
J								Graph edit distance: Accuracy of local branching from an application point of view	PATTERN RECOGNITION LETTERS										Graph matching; Graph edit distance; Local Branching Heuristic; Application of graph edit distance	LINEAR-PROGRAMMING FORMULATION; ASSIGNMENT; ALGORITHMS	In the context of graph-based representations, comparing and measuring the dissimilarities between graphs can be done by solving the Graph Edit Distance (GED) problem. It is well known and embedded in many application fields such as Computer Vision and Cheminformatics. GED is a NP-hard minimization problem, therefore the optimal solution cannot be found in reasonable time. The GED problem has been addressed by exact approaches like Mixed Integer Linear Programs (MILP) formulations and heuristic approaches like beam-search, bipartite graph matching among others. Recently, a novel heuristic, called local branching (LocBra) for the GED problem, has been proposed and shown to be efficient. In this work, the focus is on evaluating LocBra with other competitive heuristics available in the literature from an application point of view. Moreover, it tries to answer the following question: is it important to compute an accurate GED regarding the final applications? Similarity search and graph matching are considered as final applications. Three experiments are conducted to evaluate the accuracy and efficiency of the heuristics. The quality of the obtained solutions and matching w.r.t. optimal solutions and ground-truth matching is studied. The results of those experiments show that LocBra has a high correlation with the optimal solutions and the ground-truth matching. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						20	28		10.1016/j.patrec.2018.03.033													
J								Correspondence edit distance to obtain a set of weighted means of graph correspondences	PATTERN RECOGNITION LETTERS										Graph correspondence; Hamming distance; Edit distance; Weighted mean; Generalised median	MEDIAN GRAPHS; COMPUTATION; PAIR	Given a pair of data structures, such as strings, trees, graphs or sets of points, several correspondences (also referred in literature as labellings, matchings or assignments) can be defined between their local parts. The Hamming distance has been largely used to define the dissimilarity of a pair of correspondences between two data structures. Although it has the advantage of being simple in computation, it does not consider the data structures themselves, which the correspondences relate to. In this paper, we extend the definitions of a recently presented distance between correspondences based on the concept of the edit distance, which we called Correspondence edit distance. Moreover, we present an algorithm to compute the set of weighted means between a pair of graph correspondences. Both the Correspondence edit distance and the computation of the set of weighted means are necessary for the calculation of a more representative prototype between a set of correspondences. In the validation section, we show how the use of the Correspondence edit distance increases the quality of the set of weighted means compared to using the Hamming distance. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						29	36		10.1016/j.patrec.2018.08.027													
J								Fast linear sum assignment with error-correction and no cost constraints	PATTERN RECOGNITION LETTERS										Inexact graph matching; Linear assignment; Graph edit distance	GRAPH EDIT DISTANCE; COMPUTATION; ALGORITHM; SEARCH	We propose an algorithm that efficiently solves the linear sum assignment problem with error-correction and no cost constraints. This problem is encountered for instance in the approximation of the graph edit distance. The fastest currently available solvers for the linear sum assignment problem require the pair-wise costs to respect the triangle inequality. Our algorithm is as fast as these algorithms, but manages to drop the cost constraint. The main technical ingredient of our algorithm is a cost-dependent factorization of the node substitutions. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						37	45		10.1016/j.patrec.2018.03.032													
J								On the exact computation of the graph edit distance	PATTERN RECOGNITION LETTERS										Graph edit distance; Exact algorithms; Depth-first search; Best-first search; Integer programming	LINEAR-PROGRAMMING FORMULATION; ASSIGNMENT; DATABASE	The graph edit distance is a widely used distance measure for labelled graph. However, A* - GED, the standard approach for its exact computation, suffers from huge runtime and memory requirements. Recently, three better performing algorithms have been proposed: The general algorithms DF - GED and BIP - GED, and the algorithm CSI - GED, which only works for uniform edit costs. All newly proposed algorithms outperform the standard approach A* - GED. However, cross-comparisons are lacking. This paper consolidates and extends these recent advances. To this purpose, we present all existing algorithms in a unified way and show that the slightly different definitions of the graph edit distance underlying A* - GED and DF - GED, on the one side, and CSI - GED, on the other side, can be harmonised. This harmonisation allows us to develop a generalisation of CSI - GED to non-uniform edit cost. Moreover, we present a speed-up of A* - GED and DF - GED for uniform edit costs, which build upon the fact that, in the uniform case, a continuously used subroutine can be implemented to run in linear rather than cubic time. We also suggest an algorithm MIP - GED which builds upon a very compact new mixed integer linear programming formulation. Finally, we carry out a thorough empirical evaluation, which, for the first time, compares all existing exact algorithms. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						46	57		10.1016/j.patrec.2018.05.002													
J								Comparing performance of graph matching algorithms on huge graphs	PATTERN RECOGNITION LETTERS										Graphs; Graph matching; Subgraph isomorphism; Large graphs; Biographs; Social graphs; Comparison	SUBGRAPH	Graph matching algorithms are gaining more and more interest in the last years from different scientific communities; indeed, they allow comparing any kind of objects represented using their intrinsic structure, represented in terms of attributed relational graphs. The challenge is to make these algorithms able to provide solutions over huge graphs, with many thousands of nodes, and in a time that is adequate for practical applications; in this paper, we propose a comparison among the best performing algorithms available in the literature on a variety of very large graph databases used for performance assessment. The chosen datasets vary in terms of graph structure, size, density, presence of symmetric or repetitive substructures; this variability makes such datasets very challenging. The aim of this paper is to characterize the performance of the compared algorithms with respect to the typology, the size and other structural properties of the graphs; in this way, the user may consciously select the best suited algorithm for a given purpose. The results of an impressive experimentation that required 556 days of machine time are here presented and extensively discussed. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						58	67		10.1016/j.patrec.2018.06.025													
J								Learning error-correcting graph matching with a multiclass neural network	PATTERN RECOGNITION LETTERS										Learning graph matching; Graph classification; Graph edit distance	EDIT DISTANCE; REPOSITORY; COSTS	Many tasks in computer vision and pattern recognition are formulated as graph matching problems. Despite the NP-hard nature of such problems, fast and accurate approximations have led to significant progress in a wide range of applications. However, learning graph matching from observed data, remains a challenging issue. In practice, the node correspondences ground truth is rarely available. This paper presents an effective scheme for optimizing the graph matching problem in a classification context. For this, we propose a representation that is based on a parametrized model graph, and optimize the associated parameters. The objective of the optimization problem is to increase the classification rate. Experimental results on seven public datasets demonstrate the effectiveness (in terms of accuracy and speed) of our approach compared to four reference graph classifiers. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						68	76		10.1016/j.patrec.2018.03.031													
J								Efficient k-nearest neighbors search in graph space	PATTERN RECOGNITION LETTERS										Graph classification; Graph Edit Distance; K-Nearest Neighbors; Branch-and-Bound; Optimization	EDIT DISTANCE; COMPUTATION	The k-nearest neighbors classifier has been widely used to classify graphs in pattern recognition. An unknown graph is classified by comparing it to all the graphs in the training set and then assigning it the class to which the majority of the nearest neighbors belong. When the size of the database is large, the search of k-nearest neighbors can be very time consuming. On this basis, researchers proposed optimization techniques to speed up the search for the nearest neighbors. However, to the best of our knowledge, all the existing works compared the unknown graph to each train graph separately and thus none of them considered finding the k nearest graphs from a query as a single problem. In this paper, we define a new problem called multi graph edit distance to which k-nearest neighbor belongs. As a first algorithm to solve this problem, we take advantage of a recent exact branch-and-bound graph edit distance approach in order to speed up the classification stage. We extend this algorithm by considering all the search spaces needed for the dissimilarity computation between the unknown and the training graphs as a single search space. Results showed that this approach drastically outperformed the original approach under limited time constraints. Moreover, the proposed approach outperformed fast graph edit distance algorithms in terms of average execution time especially when the number of graphs is tremendous. (C) 2018 Published by Elsevier B.V.																	0167-8655	1872-7344				JUN	2020	134						77	86		10.1016/j.patrec.2018.05.001													
J								Local-global nested graph kernels using nested complexity traces	PATTERN RECOGNITION LETTERS										Graph kernels; Depth-based complexity traces; Nested kernels		In this paper, we propose two novel local-global nested graph kernels, namely the nested aligned kernel and the nested reproducing kernel, drawing on depth-based complexity traces. Both of the nested kernels gauge the nested depth complexity trace through a family of K-layer expansion subgraphs rooted at the centroid vertex, i.e., the vertex with minimum shortest path length variance to the remaining vertices. Specifically, for a pair of graphs, we commence by computing the centroid depth-based complexity traces rooted at the centroid vertices. The first nested kernel is defined by measuring the global alignment kernel, which is based on the dynamic time warping framework, between the complexity traces. Since the required global alignment kernel incorporates the whole spectrum of alignment costs between the complexity traces, this nested kernel can provide rich statistic measures. The second nested kernel, on the other hand, is defined by measuring the basic reproducing kernel between the complexity traces. Since the associated reproducing kernel only requires time complexity O(1), this nested kernel has very low computational complexity. We theoretically show that both of the proposed nested kernels can simultaneously reflect the local and global graph characteristics in terms of the nested complexity traces. Experiments on standard graph datasets abstracted from bioinformatics and computer vision databases demonstrate the effectiveness and efficiency of the proposed graph kernels. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						87	95		10.1016/j.patrec.2018.06.016													
J								Protein function prediction as a graph-transduction game	PATTERN RECOGNITION LETTERS										Protein function prediction; Graph transduction; Game theory	NETWORK INTEGRATION; PRIORITIZATION; CLASSIFICATION; ALGORITHM; HOMOLOGY; KERNEL; TOOL	Motivated by the observation that network-based methods for the automatic prediction of protein functions can greatly benefit from exploiting both the similarity between proteins and the similarity between functional classes (as encoded, e.g., in the Gene Ontology), in this paper we propose a novel approach to the problem, based on the notion of a "graph transduction game." We envisage a (non-cooperative) game, played over a graph, where the players (graph vertices) represent proteins, the functional classes correspond to the (pure) strategies, and protein- and function-level similarities are combined into a suitable payoff function. Within this formulation, Nash equilibria turn out to provide consistent functional labelings of proteins, and we use classical replicator dynamics from evolutionary game theory to find them. To test the effectiveness of our approach we conducted experiments on five different organisms and three ontologies, and the results obtained show that our method compares favorably with state-of-the-art algorithms. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						96	105		10.1016/j.patrec.2018.04.002													
J								A two-step hypergraph reduction based fitting method for unbalanced data	PATTERN RECOGNITION LETTERS										Hypergraph reduction; Hypergraph construction; Unbalanced data; Model fitting	ROBUST; CONSENSUS	In this paper, we propose a two-step hypergraph reduction based fitting for unbalanced data. A hypergraph effectively characterizes the relationship between model hypotheses and data points for model fitting. However, a hypergraph-based fitting method often suffers from the problem of high computational cost due to the complex relationship between hyperedges and vertices. Hypergraph reduction algorithms are used to alleviate this problem, but they cannot work well for unbalanced data. To deal with the unbalanced model fitting problem, we first locally remove hyperedges corresponding to the same model instances in data, and then globally remove hyperedges corresponding to the bad model hypotheses. Moreover, we extend the binary incident matrix of a normal hypergraph to a continuous (soft) generalization, to improve the accuracy of hypergraph partition for model fitting. Experimental results on both synthetic data and real images demonstrate that the proposed method has significant superiority over several other state-of-the-art fitting methods. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						106	115		10.1016/j.patrec.2018.11.003													
J								Hierarchical graphs for coarse-to-fine error tolerant matching	PATTERN RECOGNITION LETTERS										Hierarchical graph representation; Coarse-to-fine graph matching; Graph-based retrieval	SCALE-SPACE	During the last years, graph-based representations are experiencing a growing usage in visual recognition and retrieval due to their ability to capture both structural and appearance-based information. Thus, they provide a greater representational power than classical statistical frameworks. However, graph-based representations leads to high computational complexities usually dealt by graph embeddings or approximated matching techniques. Despite their representational power, they are very sensitive to noise and small variations of the input image. With the aim to cope with the time complexity and the variability present in the generated graphs, in this paper we propose to construct a novel hierarchical graph representation. Graph clustering techniques adapted from social media analysis have been used in order to contract a graph at different abstraction levels while keeping information about the topology. Abstract nodes attributes summarise information about the contracted graph partition. For the proposed representations, a coarse-to-fine matching technique is defined. Hence, small graphs are used as a filtering before more accurate matching methods are applied. This approach has been validated in real scenarios such as classification of colour images or retrieval of handwritten words (i.e. word spotting). (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						116	124		10.1016/j.patrec.2019.02.001													
J								Filters for graph-based keyword spotting in historical handwritten documents	PATTERN RECOGNITION LETTERS										Handwritten keyword spotting; Graph representation; Bipartite graph matching; Filter methods; Fast rejection	DISTANCE; RECOGNITION; MODELS	The accessibility to handwritten historical documents is often constrained by the limited feasibility of automatic full transcriptions. Keyword Spotting (KWS), that allows to retrieve arbitrary query words from documents, has been proposed as alternative. In the present paper, we make use of graphs for representing word images. The actual keyword spotting is thus based on matching a query graph with all documents graphs. However, even with relative fast approximation algorithms the shear amount of matchings might limit the practical application of this approach. For this reason we present two novel filters with linear time complexity that allow to substantially reduce the number of graph matchings actually required. In particular, these filters estimate a graph dissimilarity between a query graph and all document graphs based on their node and edge distribution in a polar coordinate system. Eventually, all graphs from the document with distributions that differ to heavily from the query's node/edge distribution are eliminated. In an experimental evaluation on four different historical documents, we show that about 90% of the matchings can be omitted, while the KWS accuracy is not negatively affected. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						125	134		10.1016/j.patrec.2018.03.030													
J								Directed and undirected network evolution from Euler-Lagrange dynamics	PATTERN RECOGNITION LETTERS										Dynamic networks; Euler-Lagrange equation; Approximate von neumann entropy	TIME-SERIES	In this paper, we investigate both undirected and directed network evolution using the Euler-Lagrange equation. We use the Euler-Lagrange equation to develop a variational principle based on the von Neumann entropy for time-varying network structure. Commencing from recent work to approximate the von Neumann entropy using simple degree statistics, the changes in entropy between different time epochs are determined by correlations in the degree difference in the edge connections. Our Euler-Lagrange equation minimises the change in entropy and allows to develop a dynamic model to simulate the changes of node degree with time. We first explore the effect of network dynamics on the three widely studied complex network models, namely (a) Erdos-Renyi random graphs, (b) Watts-Strogatz small-world networks, and (c) Barabasi-Albert scale-free networks. Our model effectively captures both undirected and directed structural transitions in the dynamic network models. We apply our model to a network time sequence representing the evolution of stock prices on the New York Stock Exchange(NYSE) and sequences of Drosophila gene regulatory networks containing different developmental phases of the organism from embryo to adult. Here we use the model to differentiate between periods of stable and unstable stock price trading and to detect periods of anomalous network evolution. Our experiments show that the presented model not only provides an accurate simulation of the degree statistics in time-varying networks but also captures the topological variations taking place when the structure of a network changes violently. (C) 2018 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				JUN	2020	134						135	144		10.1016/j.patrec.2018.03.029													
J								Fusion of visual salience maps for object acquisition	IET COMPUTER VISION										feature extraction; object detection; probability; computer vision; CCD image sensors; infrared imaging; visual salience maps; object acquisition; computer vision applications; saliency-based visual attention algorithm; feature saliency maps; saliency map; specific feature domain; feature selection; repeatability criteria; feature combination; extracted PVA; visually salient regions; visual attention approach; low false alarm rate	TARGET DETECTION; ATTENTION; MODEL; SCALE; PERFORMANCE; ALGORITHMS	The paradigm of visual attention has been widely investigated and applied to many computer vision applications. In this study, the authors propose a new saliency-based visual attention algorithm applied to object acquisition. The proposed algorithm automatically extracts points of visual attention (PVA) in the scene, based on different feature saliency maps. Each saliency map represents a specific feature domain, such as textural, contrast, and statistical-based features. A feature selection, based on probability of detection and false alarm rate and repeatability criteria, is proposed to choose the most efficient feature combination for saliency map. Motivated by the assumption that the extracted PVA represents the most visually salient regions in the image, they suggest using the visual attention approach for object acquisition. A comparison with other well-known algorithms for point of interest detection shows that the proposed algorithm performs better. The proposed algorithm was successfully tested on synthetic, charge-coupled device (CCD), and infrared (IR) images. Evaluation of the algorithm for object acquisition, based on ground truth, is carried out using synthetic images, which contain multiple examples of objects, with various sizes and brightness levels. A high probability of correct detection (greater than 90%) with a low false alarm rate (about 20 false alarms per image) was achieved.																	1751-9632	1751-9640				JUN	2020	14	4			SI		113	121		10.1049/iet-cvi.2019.0624													
J								Locally lateral manifolds of normalised Gabor features for face recognition	IET COMPUTER VISION										face recognition; feature extraction; learning (artificial intelligence); image classification; Gabor filters; normalised Gabor features; local variations; projected features; multiscale orientation; practical face recognition; relatively lengthy classification process; extensive local classifier; local cosine similarity classifier; manifold learning method; locally linear embedding; locally lateral normalised local Gabor feature vector; publicly available face datasets; feature compression; LGFV features; locally lateral manifolds	MODEL; CLASSIFICATION; REPRESENTATION; HISTOGRAM; PATTERNS; COPULA; LBP	Due to inherent characteristics of multiscale and orientation, normalised Gabor features have been successfully used in face recognition. Various previous works have showcased the strength and feasibility of this approach, especially on its robustness against local variations. However, the projected features are numerous and substantial in dimension, which is largely due to the convolution of multiscale and orientation of wavelets. Such features, when used in practical face recognition, would require relatively lengthy classification process, particularly when it involves computationally extensive local classifier or experts, such as ensembles of local cosine similarity (ELCS) classifier. The authors address this issue by simultaneously reducing the size of Gabor features laterally and locally using a manifold learning method called locally linear embedding (LLE). This method is thus denoted as locally lateral normalised local Gabor feature vector with LLE (LGFV/LN/LLE). Results on several publicly available face datasets reveal the superiority of the authors' approach in terms of improvements in feature compression of LGFV features by up to a reduction of 95% of total dimensionality while increasing the average classification accuracy by 26%. Altogether, the authors show that their LGFV/LN/LLE augmented by ELCS classifiers delivers equivalent result when compared against the state-of-the-art.																	1751-9632	1751-9640				JUN	2020	14	4			SI		122	130		10.1049/iet-cvi.2019.0531													
J								SRP-AKAZE: an improved accelerated KAZE algorithm based on sparse random projection	IET COMPUTER VISION										image resolution; nonlinear filters; image matching; VLSI; feature extraction; pipeline processing; transforms; image registration; SRP-AKAZE; improved accelerated KAZE algorithm; sparse random projection; typical image registration algorithm; high computational efficiency; nonlinear diffusion; scale-invariant feature transformation algorithm; new version; AKAZE algorithm; SIFT descriptor; feature descriptor	SELECTION; FEATURES	The AKAZE algorithm is a typical image registration algorithm that has the advantage of high computational efficiency based on non-linear diffusion. However, it is weaker than the scale-invariant feature transformation (SIFT) algorithm in terms of robustness and stability. We propose a new and improved version of the AKAZE algorithm by using the SIFT descriptor based on sparse random projection (SRP). The proposed method not only retains the advantage of high efficiency of the AKAZE algorithm in feature detection but also has the stability of the SIFT descriptor. Moreover, the computational complexity due to the high dimension of the SIFT descriptor, which limits the speed of feature matching, is drastically reduced by the SRP strategy. Experiments on several benchmark image datasets demonstrate that the proposed algorithm can significantly improve the stability of the AKAZE algorithm, and the results suggest the better matching performance and robustness of the feature descriptor.																	1751-9632	1751-9640				JUN	2020	14	4			SI		131	137		10.1049/iet-cvi.2019.0622													
J								RootsGLOH2: embedding RootSIFT 'square rooting' in sGLOH2	IET COMPUTER VISION										feature extraction; image matching; transforms; descriptor vectors; extended descriptor; nonplanar scenes; matching accuracy; deep descriptors; classical norm-based distances; matching distance design; RootSIFT square rooting; state-of-the-art nondeep descriptors; RootsGLOH2; shifting gradient local orientation histogram doubled local image descriptor; sGLOH2 local image descriptor; planar scene; scale invariant feature transform; suboptimal solutions	SELECTION; REPRESENTATION; DESCRIPTORS	This study introduces an extension of the sGLOH2 local image descriptor inspired by RootSIFT 'square rooting' as a way to indirectly alter the matching distance used to compare the descriptor vectors. The extended descriptor, named RootsGLOH2, achieved the best results in terms of matching accuracy and robustness among the latest state-of-the-art non-deep descriptors in recent evaluation contests dealing with both planar and non-planar scenes. RootsGLOH2 also achieves a matching accuracy very close to that obtained by the best deep descriptors to date. Beside confirming that 'square rooting' has beneficial effects on sGLOH2 as it happens on SIFT, experimental evidence shows that classical norm-based distances, such as the Euclidean and Manhattan distances, only provide suboptimal solutions to the problem of local image descriptor matching. This suggests matching distance design as a topic to investigate further in the near future.																	1751-9632	1751-9640				JUN	2020	14	4			SI		138	143		10.1049/iet-cvi.2019.0716													
J								Local descriptor for retinal fundus image registration	IET COMPUTER VISION										transforms; image enhancement; eye; image registration; feature extraction; medical image processing; image matching; biomedical optical imaging; diseases; retinal fundus image registration; retinal image registration; align multiple fundus images; feature-based RIR technique; geometrical transformation; scaling intensity; image enhancement; feature descriptor method; scale invariant feature; feature-based RIR techniques; SIFT-FiSP; Harris-partial intensity invariant feature; public fundus image registration dataset; statistical properties; D-saddle feature point extraction methods; scale invariant feature transform; Ghassabi feature point extraction	LONGITUDINAL REGISTRATION; SCALE; CLASSIFICATION; ACCURACY	A feature-based retinal image registration (RIR) technique aligns multiple fundus images and composed of pre-processing, feature point extraction, feature descriptor, matching and geometrical transformation. Challenges in RIR include difference in scaling, intensity and rotation between images. The scale and intensity differences can be minimised with consistent imaging setup and image enhancement during the pre-processing, respectively. The rotation can be addressed with feature descriptor method that robust to varying rotation. Therefore, a feature descriptor method is proposed based on statistical properties (FiSP) to describe the circular region surrounding the feature point. From the experiments on public Fundus Image Registration dataset, FiSP established 99.227% average correct matches for rotations between 0 degrees and 180 degrees. Then, FiSP is paired with Harris corner, scale-invariant feature transform (SIFT), speeded-up robust feature (SURF), Ghassabi's and D-Saddle feature point extraction methods to assess its registration performance and compare with the existing feature-based RIR techniques, namely generalised dual-bootstrap iterative closet point (GDB-ICP), Harris-partial intensity invariant feature descriptor (PIIFD), Ghassabi's-SIFT, H-M 16, H-M 17 and D-Saddle-histogram of oriented gradients (HOG). The combination of SIFT-FiSP registered 64.179% of the image pairs and significantly outperformed other techniques with mean difference between 25.373 and 60.448% (p=<0.001*).																	1751-9632	1751-9640				JUN	2020	14	4			SI		144	153		10.1049/iet-cvi.2019.0623													
J								SGHs for 3D local surface description	IET COMPUTER VISION										interpolation; object recognition; image representation; feature extraction; SGH descriptor; SGHs; 3D local surface description; distinctive spatial; robust spatial; three-dimensional local surface description; local reference frame; spatial partition; interpolation strategies; state-of-the-art descriptors	OBJECT RECOGNITION; UNIQUE SIGNATURES; REPRESENTATION; REGISTRATION; HISTOGRAMS; FEATURES; IMAGES	This study proposes a distinctive and robust spatial and geometric histograms (SGHs) feature descriptor for three-dimensional (3D) local surface description. The authors also introduce a new local reference frame for the generation of their SGH descriptor. To fully describe a local surface, the SGH descriptor considers both spatial distribution and geometrical characteristics in its underlying support region. To encode neighbourhood information, the SGH descriptor is constructed using histogram statistics with spatial partition and interpolation strategies. The performance of the SGH descriptor was rigorously tested on six public datasets for applications of both 3D object recognition and registration. Compared to eight state-of-the-art descriptors, experimental results show that SGH achieves the best performance on noise-free data. It also produces the best results even under different nuisances. The promising descriptiveness and robustness of their SGH descriptor have been fully demonstrated.																	1751-9632	1751-9640				JUN	2020	14	4			SI		154	161		10.1049/iet-cvi.2019.0601													
J								Directional dense-trajectory-based patterns for dynamic texture recognition	IET COMPUTER VISION										image recognition; image texture; image motion analysis; image representation; computer vision; video signal processing; image sequences; dense trajectories; spatio-temporal features; motion points; dense-trajectory-based descriptors; DT recognition; directional dense-trajectory-based patterns; dynamic texture recognition; moving textures; video analysis; motion features; computer vision; DT description; beneficial properties; substantial extensions; local vector pattern operator; directional features; directional beams; directional dense trajectory patterns	LOCAL BINARY PATTERN; VECTOR PATTERN; VIDEO; CLASSIFICATION; SPACE; COUNT; SCALE; FLOW	Representation of dynamic textures (DTs), well-known as a sequence of moving textures, is a challenging problem in video analysis due to the disorientation of motion features. Analysing DTs to make them 'understandable' plays an important role in different applications of computer vision. In this study, an efficient approach for DT description is proposed by addressing the following novel concepts. First, the beneficial properties of dense trajectories are exploited for the first time to efficiently describe DTs instead of the whole video. Second, two substantial extensions of local vector pattern operator are introduced to form a completed model which is based on complemented components to enhance its performance in encoding directional features of motion points in a trajectory. Finally, the authors present a new framework, called directional dense trajectory patterns, which takes advantage of directional beams of dense trajectories along with spatio-temporal features of their motion points in order to construct dense-trajectory-based descriptors with more robustness. Evaluations of DT recognition on different benchmark datasets (i.e. UCLA, DynTex, and DynTex++) have verified the interest of the authors' proposal.																	1751-9632	1751-9640				JUN	2020	14	4			SI		162	176		10.1049/iet-cvi.2019.0455													
J								Finite-time synchronization of fractional-order gene regulatory networks with time delay	NEURAL NETWORKS										Fractional-order; Gene regulatory networks; Feedback control; Finite-time synchronization	CELLULAR NEURAL-NETWORKS; LIMIT-CYCLES; STABILITY; SYSTEMS; BIFURCATION	As multi-gene networks transmit signals and products by synchronous cooperation, investigating the synchronization of gene regulatory networks may help us to explore the biological rhythm and internal mechanisms at molecular and cellular levels. We aim to induce a type of fractional-order gene regulatory networks to synchronize at finite-time point by designing feedback controls. Firstly, a unique equilibrium point of the network is proved by applying the principle of contraction mapping. Secondly, some sufficient conditions for finite-time synchronization of fractional-order gene regulatory networks with time delay are explored based on two kinds of different control techniques and fractional Lyapunov function approach, and the corresponding setting time is estimated. Finally, some numerical examples are given to demonstrate the effectiveness of the theoretical results. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						1	10		10.1016/j.neunet.2020.02.004													
J								Global exponential stabilization and lag synchronization control of inertial neural networks with time delays	NEURAL NETWORKS										Stabilization; Lag synchronization; Inertial neural networks (INNs); Time delays	ASYMPTOTIC STABILITY; VARYING DELAYS; MIXED DELAYS; DISCRETE; CRITERIA	The global exponential stabilization and lag synchronization control of delayed inertial neural networks (INNs) are investigated. By constructing nonnegative function and employing inequality techniques, several new results about exponential stabilization and exponential lag synchronization are derived via adaptive control. And the theoretical outcomes are developed directly from the INNs themselves without variable substitution. In addition, the synchronization results are also applied to image encryption and decryption. Finally, an example is presented to illustrate the validity of the derived results. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						11	20		10.1016/j.neunet.2020.03.006													
J								NeuroBayesSLAM: Neurobiologically inspired Bayesian integration of multisensory information for robot navigation	NEURAL NETWORKS										Bayesian; Multisensory integration; Attractor dynamics; Head direction cells; Grid cells; Monocular SLAM	HEAD-DIRECTION CELLS; GRID CELLS; PLACE CELLS; SIMULTANEOUS LOCALIZATION; SPATIAL MAP; REPRESENTATION; SPACE; SLAM; HIPPOCAMPUS; MODELS	Spatial navigation depends on the combination of multiple sensory cues from idiothetic and allothetic sources. The computational mechanisms of mammalian brains in integrating different sensory modalities under uncertainty for navigation is enlightening for robot navigation. We propose a Bayesian attractor network model to integrate visual and vestibular inputs inspired by the spatial memory systems of mammalian brains. In the model, the pose of the robot is encoded separately by two sub-networks, namely head direction network for angle representation and grid cell network for position representation, using similar neural codes of head direction cells and grid cells observed in mammalian brains. The neural codes in each of the sub-networks are updated in a Bayesian manner by a population of integrator cells for vestibular cue integration, as well as a population of calibration cells for visual cue calibration. The conflict between vestibular cue and visual cue is resolved by the competitive dynamics between the two populations. The model, implemented on a monocular visual simultaneous localization and mapping (SLAM) system, termed NeuroBayesSLAM, successfully builds semi-metric topological maps and self-localizes in outdoor and indoor environments of difference characteristics, achieving comparable performance as previous neurobiologically inspired navigation systems but with much less computation complexity. The proposed multisensory integration method constitutes a concise yet robust and biologically plausible method for robot navigation in large environments. The model provides a viable Bayesian mechanism for multisensory integration that may pertain to other neural subsystems beyond spatial cognition. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						21	35		10.1016/j.neunet.2020.02.023													
J								Modeling coherence by ordering paragraphs using pointer networks	NEURAL NETWORKS										Ordering paragraphs; Pointer networks; Paragraph embeddings		Coherence is a distinctive feature in well-written documents. One method to study coherence is to analyze how sentences are ordered in a document. In Multi-document Summarization, sentences from different sources need to be ordered. Cluster-based ordering algorithms aim to study various themes or topics that are present in a set of sentences. After the clusters of sentences have been identified, sentences are ordered within each cluster in isolation. One challenge that remains is to order these clusters or paragraphs to obtain a coherent ordering of information. Inspired by the success of deep neural networks in several NLP tasks, we propose an RNN-based encoder-decoder system to predict order for a given set of loose clusters or paragraphs. Universal Sentence Encoder (USE) is used to encode paragraphs into high dimensional embeddings, which are then fed into an LSTM encoder and consecutively passed to a pointer network, which finally outputs the paragraph order. Since Wikipedia is a source of well-structured articles, it is used to generate multiple datasets. Based on our experimental results, the proposed model satisfactorily outperforms the baseline model across multiple datasets. We observe a two-fold increase in Kendall's tau values for the final paragraph orderings. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						36	41		10.1016/j.neunet.2020.02.022													
J								Probabilistic inference of binary Markov random fields in spiking neural networks through mean-field approximation	NEURAL NETWORKS										Probabilistic inference; Markov Random Fields (MRFs); Spiking Neural Networks (SNNs); Recurrent Neural Networks (RNNs); Mean-field approximation	BAYESIAN-INFERENCE; BELIEF PROPAGATION; UNCERTAINTY; CONFIDENCE; VISION	Recent studies have suggested that the cognitive process of the human brain is realized as probabilistic inference and can be further modeled by probabilistic graphical models like Markov random fields. Nevertheless, it remains unclear how probabilistic inference can be implemented by a network of spiking neurons in the brain. Previous studies have tried to relate the inference equation of binary Markov random fields to the dynamic equation of spiking neural networks through belief propagation algorithm and reparameterization, but they are valid only for Markov random fields with limited network structure. In this paper, we propose a spiking neural network model that can implement inference of arbitrary binary Markov random fields. Specifically, we design a spiking recurrent neural network and prove that its neuronal dynamics are mathematically equivalent to the inference process of Markov random fields by adopting mean-field theory. Furthermore, our mean-field approach unifies previous works. Theoretical analysis and experimental results, together with the application to image denoising, demonstrate that our proposed spiking neural network can get comparable results to that of mean-field inference. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						42	51		10.1016/j.neunet.2020.03.003													
J								Recommendation via Collaborative Autoregressive Flows	NEURAL NETWORKS										Collaborative recommendation; Variational inference; Normalizing flows; Autoregressive flows; Generative models		Although it is one of the most widely used methods in recommender systems, Collaborative Filtering (CF) still has difficulties in modeling non-linear user-item interactions. Complementary to this, recently developed deep generative model variants (e.g., Variational Autoencoder (VAE)) allowing Bayesian inference and approximation of the variational posterior distributions in these models, have achieved promising performance improvement in many areas. However, the choices of variation distribution - e.g., the popular diagonal-covariance Gaussians - are insufficient to recover the true distributions, often resulting in biased maximum likelihood estimates of the model parameters. Aiming at more tractable and expressive variational families, in this work we extend the flow-based generative model to CF for modeling implicit feedbacks. We present the Collaborative Autoregressive Flows (CAF) for the recommender system, transforming a simple initial density into more complex ones via a sequence of invertible transformations, until a desired level of complexity is attained. CAF is a non-linear probabilistic approach allowing uncertainty representation and exact tractability of latent-variable inference in item recommendations. Compared to the agnostic-presumed prior approximation used in existing deep generative recommendation approaches, CAF is more effective in estimating the probabilistic posterior and achieves better recommendation accuracy. We conducted extensive experimental evaluations demonstrating that CAF can capture more effective representation of latent factors, resulting in a substantial gain on recommendation compared to the state-of-the-art approaches. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						52	64		10.1016/j.neunet.2020.03.010													
J								Variational approximation error in non-negative matrix factorization	NEURAL NETWORKS										Non-negative matrix factorization (NMF); Real log canonical threshold (RLCT); Learning coefficient; Bayesian inference; Variational Bayesian method; Variational inference	STOCHASTIC COMPLEXITIES; INFORMATION CRITERION; MARGINAL LIKELIHOOD; ASYMPTOTIC-BEHAVIOR; MODEL SELECTION; BAYES; SINGULARITIES	Non-negative matrix factorization (NMF) is a knowledge discovery method that is used in many fields. Variational inference and Gibbs sampling methods for it are also well-known. However, the variational approximation error has not been clarified yet, because NMF is not statistically regular and the prior distribution used in variational Bayesian NMF (VBNMF) has zero or divergence points. In this paper, using algebraic geometrical methods, we theoretically analyze the difference in negative log evidence (a.k.a. free energy) between VBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the variational posterior and the true posterior. We derive an upper bound for the learning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF. By using the upper bound, we find a lower bound for the approximation error, asymptotically. The result quantitatively shows how well the VBNMF algorithm can approximate Bayesian NMF; the lower bound depends on the hyperparameters and the true non-negative rank. A numerical experiment demonstrates the theoretical result. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						65	75		10.1016/j.neunet.2020.03.009													
J								AdaEn-Net: An ensemble of adaptive 2D-3D Fully Convolutional Networks for medical image segmentation	NEURAL NETWORKS										Medical image segmentation; Deep learning; Neural architecture search; Hyperparameter optimization; Multiobjective optimization	SEARCH; ALGORITHM	Fully Convolutional Networks (FCNs) have emerged as powerful segmentation models but are usually designed manually, which requires extensive time and can result in large and complex architectures. There is a growing interest to automatically design efficient architectures that can accurately segment 3D medical images. However, most approaches either do not fully exploit volumetric information or do not optimize the model's size. To address these problems, we propose a self-adaptive 2D-3D ensemble of FCNs called AdaEn-Net for 3D medical image segmentation that incorporates volumetric data and adapts to a particular dataset by optimizing both the model's performance and size. The AdaEn-Net consists of a 2D FCN that extracts intra-slice information and a 3D FCN that exploits inter-slice information. The architecture and hyperparameters of the 2D and 3D architectures are found through a multiobjective evolutionary based algorithm that maximizes the expected segmentation accuracy and minimizes the number of parameters in the network. The main contribution of this work is a model that fully exploits volumetric information and automatically searches for a high-performing and efficient architecture. The AdaEn-Net was evaluated for prostate segmentation on the PROMISE12 Grand Challenge and for cardiac segmentation on the MICCAI ACDC challenge. In the first challenge, the AdaEn-Net ranks 9 out of 297 submissions and surpasses the performance of an automatically-generated segmentation network while producing an architecture with 13x fewer parameters. In the second challenge, the proposed model is ranked within the top 8 submissions and outperforms an architecture designed with reinforcement learning while having 1.25x fewer parameters. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						76	94		10.1016/j.neunet.2020.03.007													
J								Dynamic resource allocation during reinforcement learning accounts for ramping and phasic dopamine activity	NEURAL NETWORKS										Prediction error; Salience; Temporal-difference learning model; Pearce-Hall model; Habit; Striatum	PREDICTION ERRORS; NEURON ACTIVITY; WORKING-MEMORY; REWARD; ATTENTION; SIGNALS; MECHANISMS; TRANSIENTS; PROXIMITY; STRIATUM	For an animal to learn about its environment with limited motor and cognitive resources, it should focus its resources on potentially important stimuli. However, too narrow focus is disadvantageous for adaptation to environmental changes. Midbrain dopamine neurons are excited by potentially important stimuli, such as reward-predicting or novel stimuli, and allocate resources to these stimuli by modulating how an animal approaches, exploits, explores, and attends. The current study examined the theoretical possibility that dopamine activity reflects the dynamic allocation of resources for learning. Dopamine activity may transition between two patterns: (1) phasic responses to cues and rewards, and (2) ramping activity arising as the agent approaches the reward. Phasic excitation has been explained by prediction errors generated by experimentally inserted cues. However, when and why dopamine activity transitions between the two patterns remain unknown. By parsimoniously modifying a standard temporal difference (TD) learning model to accommodate a mixed presentation of both experimental and environmental stimuli, we simulated dopamine transitions and compared them with experimental data from four different studies. The results suggested that dopamine transitions from ramping to phasic patterns as the agent focuses its resources on a small number of reward-predicting stimuli, thus leading to task dimensionality reduction. The opposite occurs when the agent re-distributes its resources to adapt to environmental changes, resulting in task dimensionality expansion. This research elucidates the role of dopamine in a broader context, providing a potential explanation for the diverse repertoire of dopamine activity that cannot be explained solely by prediction error. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						95	107		10.1016/j.neunet.2020.03.005													
J								Chimera states in hybrid coupled neuron populations	NEURAL NETWORKS										Chimera state; Hybrid coupling; Chaotic population behavior	ELECTRICAL SYNAPSES; PHASE SYNCHRONIZATION; CHEMICAL SYNAPSES; GLOBUS-PALLIDUS; NETWORKS; OSCILLATIONS; MODEL; DESYNCHRONIZATION; PATHOPHYSIOLOGY; RELEVANCE	Here we study the emergence of chimera states, a recently reported phenomenon referring to the coexistence of synchronized and unsynchronized dynamical units, in a population of Morris-Lecar neurons which are coupled by both electrical and chemical synapses, constituting a hybrid synaptic architecture, as in actual brain connectivity. This scheme consists of a nonlocal network where the nearest neighbor neurons are coupled by electrical synapses, while the synapses from more distant neurons are of the chemical type. We demonstrate that peculiar dynamical behaviors, including chimera state and traveling wave, exist in such a hybrid coupled neural system, and analyze how the relative abundance of chemical and electrical synapses affects the features of chimera and different synchrony states (i.e. incoherent, traveling wave and coherent) and the regions in the space of relevant parameters for their emergence. Additionally, we show that, when the relative population of chemical synapses increases further, a new intriguing chaotic dynamical behavior appears above the region for chimera states. This is characterized by the coexistence of two distinct synchronized states with different amplitude, and an unsynchronized state, that we denote as a chaotic amplitude chimera. We also discuss about the computational implications of such state. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						108	117		10.1016/j.neunet.2020.03.002													
J								Deep neural networks with a set of node-wise varying activation functions	NEURAL NETWORKS										Deep network; Principal component analysis; Pruning; Varying activation		In this study, we present deep neural networks with a set of node-wise varying activation functions. The feature-learning abilities of the nodes are affected by the selected activation functions, where the nodes with smaller indices become increasingly more sensitive during training. As a result, the features learned by the nodes are sorted by the node indices in order of their importance such that more sensitive nodes are related to more important features. The proposed networks learn input features but also the importance of the features. Nodes with lower importance in the proposed networks can be pruned to reduce the complexity of the networks, and the pruned networks can be retrained without incurring performance losses. We validated the feature-sorting property of the proposed method using both shallow and deep networks as well as deep networks transferred from existing networks. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						118	131		10.1016/j.neunet.2020.03.004													
J								Cross-modal dual subspace learning with adversarial network	NEURAL NETWORKS										Cross-modal retrieval; Adversarial network; Subspace learning	RETRIEVAL	Cross-modal retrieval has recently attracted much interest along with the rapid development of multimodal data, and effectively utilizing the complementary relationship of different modal data and eliminating the heterogeneous gap as much as possible are the two key challenges. In this paper, we present a novel network model termed cross-modal Dual Subspace learning with Adversarial Network (DSAN). The main contributions are as follows: (1) Dual subspaces (visual subspace and textual subspace) are proposed, which can better mine the underlying structure information of different modalities as well as modality-specific information. (2) An improved quadruplet loss is proposed, which takes into account the relative distance and absolute distance between positive and negative samples, together with the introduction of the idea of hard sample mining. (3) Intra-modal constrained loss is proposed to maximize the distance of the most similar cross-modal negative samples and their corresponding cross-modal positive samples. In particular, feature preserving and modality classification act as two antagonists. DSAN tries to narrow the heterogeneous gap between different modalities, and distinguish the original modality of random samples in dual subspaces. Comprehensive experimental results demonstrate that, DSAN significantly outperforms 9 state-of-the-art methods on four cross-modal datasets. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						132	142		10.1016/j.neunet.2020.03.015													
J								SDARE: A stacked denoising autoencoder method for game dynamics network structure reconstruction	NEURAL NETWORKS											RECOVERY																		0893-6080	1879-2782				JUN	2020	126						143	152															
J								NFN plus : A novel network followed network for retinal vessel segmentation	NEURAL NETWORKS										Retinal vessel segmentation; Deep learning; Cascaded networks; Skip connections	BLOOD-VESSELS; IMAGES; CLASSIFICATION; VALIDATION; ALGORITHM; LEVEL	In the early diagnosis of diabetic retinopathy, the morphological attributes of blood vessels play an essential role to construct a retinal computer-aided diagnosis system. However, due to the challenges including limited densely annotated data, inter-vessel differences and structured prediction problem, it remains challenging to segment accurately the retinal vessels, particularly the capillaries on color fundus images. To address these issues, in this paper, we propose a novel deep learning-based model called NFN+ to effectively extract multi-scale information and make full use of deep feature maps. In NFN+, the front network converts an image patch into a probabilistic retinal vessel map, and the followed network further refines the map to achieve a better post-processing module, which helps represent the vessel structures implicitly. We employ the inter-network skip connections to unite two identical multi-scale backbones, which enables the useful multi-scale features to be directly transferred from shallow layers to deeper layers. The refined probabilistic retinal vessel maps produced from the augmented images are then averaged to construct the segmentation results. We evaluated this model on the digital retinal images for vessel extraction (DRIVE), structured analysis of the retina (STARE), and the child heart and health study (CHASE) databases. Our results indicate that the elaborated cascaded designs can produce performance gain and the proposed NFN+ model, to our best knowledge, achieved the state-of-the-art retinal vessel segmentation accuracy on color fundus images (AUC: 98.30%, 98.75% and 98.94%, respectively). (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						153	162		10.1016/j.neunet.2020.02.018													
J								Exponential synchronization of memristive neural networks with time-varying delays via quantized sliding-mode control	NEURAL NETWORKS										Memristive neural network; Time-varying delay; Super-twisting algorithm; Quantization function; Exponential synchronization	PASSIVITY; SYSTEMS	In the paper, exponential synchronization issue is considered for memristive neural networks (MNNs) with time-varying delays via quantized sliding-mode algorithm. Quantized Sliding-mode controller is introduced to ensure the slave system can be exponentially synchronized with the host system via the super-twisting algorithm, which has been proved in the main results. Quantization function consists of uniform quantizer and logarithmic quantizer. Simulation results are given with comparisons between two quantizers in the end. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						163	169		10.1016/j.neunet.2020.03.014													
J								Prediction of admission in pediatric emergency department with deep neural networks and triage textual data	NEURAL NETWORKS										Deep neural networks; Emergency department admission; Gradient boosting; Prediction model; Triage; Unstructured data	HOSPITAL ADMISSIONS	Emergency department (ED) overcrowding is a global condition that severely worsens attention to patients, increases clinical risks and affects hospital cost management. A correct and early prediction of ED's admission is of high value and a motivation to adopt machine learning models. However, several of these studies do not consider data collected in textual form, which is a feature set that contains detailed information about patients and presents great potential for medical health care improvement. To this end, we propose and compare predictive models for admission that use both structured and unstructured data available at triage time. In total, our dataset comprised 499,853 pediatric ED's presentations (with an admission rate of 5.76%) of patients with age up to 18 years old observed over 3.5 years. Our best model consists of a 2-stage architecture with a deep neural network (DNN) to extract information from textual data followed by a gradient boosting classifier. This combined model achieved a value of 0.892 for the Area Under the Curve (AUC) in the test data. We highlight the importance of DNN-based text processing for better prediction, since the absence of text features resulted in AUC reduction of approximately two percentage points. Also, the feature importance of text was higher than that of the Manchester Triage System (MTS), which is a widely used risk classification protocol. These results suggest that activations from a trained DNN should be used in transfer learning setups in future studies. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						170	177		10.1016/j.neunet.2020.03.012													
J								Summation Detector for False Data-Injection Attack in Cyber-Physical Systems	IEEE TRANSACTIONS ON CYBERNETICS										Attack detection; cyber-physical systems (CPSs); false data-injection attack; summation (SUM) detector	RESILIENT CONTROL	In this paper, from the perspectives of defenders, we consider the detection problems of false data-injection attacks in cyber-physical systems (CPSs) with white noise. The false data-injection attacks usually modify the sensor data to make CPSs unstable and keep stealth for the X-2 detector. To guarantee system security, a novel detector, that is, the summation (SUM) detector, is proposed to detect the false data-injection attacks. Different from the X-2 detector, the SUM detector not only utilizes the current compromise information but also collects all historical information to reveal the threat. Its evaluation value also satisfies X-2 distribution when no attacks compromise the systems, and the false alarm rate can be restricted to less than any given value by choosing the proper threshold value. Furthermore, an improved false data-injection attack with a time-variable increment coefficient is introduced based on the existing approaches. The effects of the SUM detector are also verified for the traditional and the improved false data-injection attacks, respectively. Finally, some simulation results are given to demonstrate the effectiveness and superiority of the SUM detector.																	2168-2267	2168-2275				JUN	2020	50	6					2338	2345		10.1109/TCYB.2019.2915124													
J								Online Reinforcement Learning Control for the Personalization of a Robotic Knee Prosthesis	IEEE TRANSACTIONS ON CYBERNETICS										Knee; Prosthetics; Impedance; Robots; Tuning; Kinematics; Dynamic programming; Approximate dynamic programming (ADP); direct heuristic dynamic programming (dHDP); reinforcement learning (RL); robotic knee prosthesis	IMPEDANCE CONTROL; EXOSKELETON	Robotic prostheses deliver greater function than passive prostheses, but we face the challenge of tuning a large number of control parameters in order to personalize the device for individual amputee users. This problem is not easily solved by traditional control designs or the latest robotic technology. Reinforcement learning (RL) is naturally appealing. The recent, unprecedented success of AlphaZero demonstrated RL as a feasible, large-scale problem solver. However, the prosthesis-tuning problem is associated with several unaddressed issues such as that it does not have a known and stable model, the continuous states and controls of the problem may result in a curse of dimensionality, and the human-prosthesis system is constantly subject to measurement noise, environmental change and human-body-caused variations. In this paper, we demonstrated the feasibility of direct heuristic dynamic programming, an approximate dynamic programming (ADP) approach, to automatically tune the 12 robotic knee prosthesis parameters to meet individual human users' needs. We tested the ADP-tuner on two subjects (one able-bodied subject and one amputee subject) walking at a fixed speed on a treadmill. The ADP-tuner learned to reach target gait kinematics in an average of 300 gait cycles or 10 min of walking. We observed improved ADP tuning performance when we transferred a previously learned ADP controller to a new learning session with the same subject. To the best of our knowledge, our approach to personalize robotic prostheses is the first implementation of online ADP learning control to a clinical problem involving human subjects.																	2168-2267	2168-2275				JUN	2020	50	6					2346	2356		10.1109/TCYB.2019.2890974													
J								Unified Graph-Based Multicue Feature Fusion for Robust Visual Tracking	IEEE TRANSACTIONS ON CYBERNETICS										Target tracking; Adaptation models; Feature extraction; Visualization; Reliability; Object tracking; Encoding; Adaptive appearance model; feature fusion; outlier detection; visual tracking	OBJECT TRACKING; CLASSIFICATION; SCALE	Visual tracking is a complex problem due to unconstrained appearance variations and a dynamic environment. The extraction of complementary information from the object environment via multiple features and adaption to the target's appearance variations are the key problems of this paper. To this end, we propose a robust object tracking framework based on the unified graph fusion (UGF) of multicue to adapt to the object's appearance. The proposed cross-diffusion of sparse and dense features not only suppresses the individual feature deficiencies but also extracts the complementary information from multicue. This iterative process builds robust unified features which are invariant to object deformations, fast motion, and occlusion. Robustness of the unified feature also enables the random forest classifier to precisely distinguish the foreground from the background, adding resilience to background clutter. In addition, we present a novel kernel-based adaptation strategy using outlier detection and a transductive reliability metric. The adaptation strategy updates the appearance model to accommodate variations in scale, illumination, and rotation. Both qualitative and quantitative analyses on benchmark video sequences from OTB-50, OTB-100, VOT2017/18, and UAV123 show that the proposed UGF tracker performs favorably against 18 other state-of-the-art trackers under various object tracking challenges.																	2168-2267	2168-2275				JUN	2020	50	6					2357	2368		10.1109/TCYB.2019.2920289													
J								Fuzzy Output Tracking Control and Filtering for Nonlinear Discrete-Time Descriptor Systems Under Unreliable Communication Links	IEEE TRANSACTIONS ON CYBERNETICS										Descriptor systems; filtering; output tracking control; packet dropouts; Takagi-Sugeno (T-S) fuzzy approximation	NETWORKED CONTROL-SYSTEMS; MARKOVIAN JUMP SYSTEMS; SLIDING MODE CONTROL; LINEAR-SYSTEMS; DESIGN; STABILIZATION; STABILITY; DELAYS	In this paper, the problems of output tracking control and filtering are investigated for Takagi-Sugeno fuzzy-approximation-based nonlinear descriptor systems in the discrete-time domain. Especially, the unreliability of the communication links between the sensor and actuator/filter is taken into account, and the phenomenon of packet dropouts is characterized by a binary Markov chain with uncertain transition probabilities, which may reflect the reality more accurately than the existing description processes. A novel bounded real lemma (BRL), which ensures the stochastic admissibility with H8 performance for fuzzy discrete-time descriptor systems despite the uncertain Markov packet dropouts, is presented based on a fuzzy basis-dependent Lyapunov function. By resorting to the dual conditions of the obtained BRL, a solution for the designed fuzzy output tracking controller is given. A design method for the fullorder fuzzy filter is also provided. Finally, two examples are finally adopted to show the applicability of the achieved design strategies.																	2168-2267	2168-2275				JUN	2020	50	6					2369	2379		10.1109/TCYB.2019.2920709													
J								Synchronization in Kuramoto Oscillator Networks With Sampled-Data Updating Law	IEEE TRANSACTIONS ON CYBERNETICS										Oscillators; Synchronization; Couplings; Topology; Power systems; Lyapunov methods; Frequency synchronization; Asynchronous coupling; Kuramoto oscillators; sampled-data control; synchronization	EXPONENTIAL SYNCHRONIZATION; TIME SYNCHRONIZATION; MULTIAGENT SYSTEMS; FINITE-TIME; STABILITY; CONSENSUS; MODELS	In this article, we are concerned with the synchronization problem of Kuramoto oscillators under the sampled-data updating law. This article is motivated by the needs of synchronization of Kuramoto oscillators in the presence of periodic and asynchronous coupling updates. Based on the periodical sampled-data method, a sufficient condition ensuring synchronization under periodic updates is derived. In order to relax the requirement of having all data updated simultaneously, an event-triggered law is designed to implement asynchronous coupling updates. Our synchronization analysis does not rely on any linearization technique around equilibrium points. Instead, we employ the Lyapunov stability theory and nonsmooth analysis technique to deduce the synchronization conditions and estimate the region of attraction. The effectiveness of the proposed sampled-data coupling is illustrated by numerical simulations.																	2168-2267	2168-2275				JUN	2020	50	6					2380	2388		10.1109/TCYB.2019.2940987													
J								Dissipativity-Based Control for Fuzzy Systems With Asynchronous Modes and Intermittent Measurements	IEEE TRANSACTIONS ON CYBERNETICS										Hidden Markov models; Output feedback; Switched systems; Switches; Symmetric matrices; Stochastic processes; Asynchronous output feedback control; fuzzy switched systems; hidden Markov model (HMM); intermittent measurements	TIME SWITCHED SYSTEMS; STOCHASTIC-SYSTEMS; NETWORKED CONTROL; INFINITY CONTROL; DELAY SYSTEMS; JUMP SYSTEMS; STABILITY; DESIGN; STABILIZATION; CONVERGENCE	In this paper, the problem of asynchronous output feedback control is investigated for a class of Takagi-Sugeno fuzzy switched systems subject to intermittent measurements. The Bernoulli process is employed to model the phenomenon of stochastic intermittent measurements. Based on the hidden Markov model and output measurements, an asynchronous controller is designed. Then, sufficient conditions for the existence of an asynchronous controller are proposed, which ensure the stochastic stability of the closed-loop system with desired extended dissipative performance. Finally, an example is presented to illustrate the effectiveness and advantages of the proposed new design techniques.																	2168-2267	2168-2275				JUN	2020	50	6					2389	2399		10.1109/TCYB.2018.2887060													
J								Ternary Adversarial Networks With Self-Supervision for Zero-Shot Cross-Modal Retrieval	IEEE TRANSACTIONS ON CYBERNETICS										Semantics; Correlation; Knowledge transfer; Standards; Task analysis; Training; Feature extraction; Adversarial learning; cross-modal retrieval; self-supervision; zero-shot learning (ZSL)	FUSION	Given a query instance from one modality (e.g., image), cross-modal retrieval aims to find semantically similar instances from another modality (e.g., text). To perform cross-modal retrieval, existing approaches typically learn a common semantic space from a labeled source set and directly produce common representations in the learned space for the instances in a target set. These methods commonly require that the instances of both two sets share the same classes. Intuitively, they may not generalize well on a more practical scenario of zero-shot cross-modal retrieval, that is, the instances of the target set contain unseen classes that have inconsistent semantics with the seen classes in the source set. Inspired by zero-shot learning, we propose a novel model called ternary adversarial networks with self-supervision (TANSS) in this paper, to overcome the limitation of the existing methods on this challenging task. Our TANSS approach consists of three paralleled subnetworks: 1) two semantic feature learning subnetworks that capture the intrinsic data structures of different modalities and preserve the modality relationships via semantic features in the common semantic space; 2) a self-supervised semantic subnetwork that leverages the word vectors of both seen and unseen labels as guidance to supervise the semantic feature learning and enhances the knowledge transfer to unseen labels; and 3) we also utilize the adversarial learning scheme in our TANSS to maximize the consistency and correlation of the semantic features between different modalities. The three subnetworks are integrated in our TANSS to formulate an end-to-end network architecture which enables efficient iterative parameter optimization. Comprehensive experiments on three cross-modal datasets show the effectiveness of our TANSS approach compared with the state-of-the-art methods for zero-shot cross-modal retrieval.																	2168-2267	2168-2275				JUN	2020	50	6					2400	2413		10.1109/TCYB.2019.2928180													
J								Intermittent Discrete Observation Control for Synchronization of Stochastic Neural Networks	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Stochastic processes; Biological neural networks; Feedback control; Lyapunov methods; Graph theory; Discrete-time state observations; exponential synchronization; periodically intermittent control (PIC); stochastic neural networks	EXPONENTIAL SYNCHRONIZATION; DIFFERENTIAL-EQUATIONS; COUPLED SYSTEMS; FUZZY MODEL; STABILIZATION; STABILITY; DELAYS; NOISE	In this paper, to investigate the exponential synchronization of stochastic neural networks, a new periodically intermittent discrete observation control (PIDOC) is first proposed. Different from the existing periodically intermittent control, our control in control time is feedback control based on discrete-time state observations (FCDSOs) instead of a continuous-time one. By employing the Lyapunov method, graph theory, and theory of differential inclusions, the exponential synchronization of stochastic neural networks with a discontinuous right-hand side is realized by PIDOC and some sufficient conditions are presented. Especially, when control width tends to control period, PIDOC will be reduced to a general FCDSO and we give some detailed discussions. Then, we provide some corollaries about synchronization in mean square, asymptotical synchronization in mean square, and exponential synchronization of stochastic neural networks under FCDSO. Finally, some numerical simulations are provided to demonstrate our analytical results.																	2168-2267	2168-2275				JUN	2020	50	6					2414	2424		10.1109/TCYB.2019.2930579													
J								Hybrid Artificial Bee Colony Algorithm for a Parallel Batching Distributed Flow-Shop Problem With Deteriorating Jobs	IEEE TRANSACTIONS ON CYBERNETICS										Artificial bee colony algorithm; Production facilities; Job shop scheduling; Optimization; Parallel machines; Artificial bee colony (ABC); deteriorating job; distributed flow shop (DFS); parallel batching	ITERATED GREEDY ALGORITHM; SHOP SCHEDULING PROBLEM; PERMUTATION FLOWSHOP; OPTIMIZATION ALGORITHM; NEIGHBORHOOD SEARCH; MINIMIZE; TIME; HEURISTICS; MAKESPAN; MACHINES	In this article, we propose a hybrid artificial bee colony (ABC) algorithm to solve a parallel batching distributed flow-shop problem (DFSP) with deteriorating jobs. In the considered problem, there are two stages as follows: 1) in the first stage, a DFSP is studied and 2) after the first stage has been completed, each job is transferred and assembled in the second stage, where the parallel batching constraint is investigated. In the two stages, the deteriorating job constraint is considered. In the proposed algorithm, first, two types of problem-specific heuristics are proposed, namely, the batch assignment and the right-shifting heuristics, which can substantially improve the makespan. Next, the encoding and decoding approaches are developed according to the problem constraints and objectives. Five types of local search operators are designed for the distributed flow shop and parallel batching stages. In addition, a novel scout bee heuristic that considers the useful information that is collected by the global and local best solutions is investigated, which can enhance searching performance. Finally, based on several well-known benchmarks and realistic industrial instances and via comprehensive computational comparison and statistical analysis, the highly effective performance of the proposed algorithm is favorably compared against several algorithms in terms of both solution quality and population diversity.																	2168-2267	2168-2275				JUN	2020	50	6					2425	2439		10.1109/TCYB.2019.2943606													
J								Exponential H infinity Filtering for Continuous-Time Switched Neural Networks Under Persistent Dwell-Time Switching Regularity	IEEE TRANSACTIONS ON CYBERNETICS										H-infinity filtering; global uniform exponential stability; persistent dwell-time (PDT) switching strategy; switched neural networks (NNs)	LINEAR-SYSTEMS; STABILITY; SYNCHRONIZATION	This paper focuses on the exponential H8 filtering issue for a class of continuous-time switched neural networks (NNs). Our aim is to design a mode-dependent filter acquiring the state of the investigated system, and ensuring the global uniform exponential stability of the resulting filtering error system. The persistent dwell-time (PDT) switching strategy is employed to represent the switching among NNs. By utilizing a suitable Lyapunov function and the switched system theory, some criteria for the solvability of the addressed problem are presented under the full consideration of switching frequency. Finally, the filter gains are derived by a straightforward decoupling method, and with the aid of the algorithm of the continuous-time PDT switching regularity, the availability of the filter is expounded through a numerical example.																	2168-2267	2168-2275				JUN	2020	50	6					2440	2449		10.1109/TCYB.2019.2901867													
J								Hierarchical Controller-Estimator for Coordination of Networked Euler-Lagrange Systems	IEEE TRANSACTIONS ON CYBERNETICS										Topology; Switches; Task analysis; Germanium; Transient response; Decentralized control; Coordination; hierarchical controller-estimator algorithms (HCEAs); networked Euler-Lagrange systems (NELSs); sampled-data interactions	MULTIAGENT SYSTEMS; DISTRIBUTED COORDINATION; ROBOTIC SYSTEMS; CONSENSUS; TRACKING; AGENTS; COMMUNICATION; INFORMATION; ALGORITHMS; DELAYS	This paper proposes several hierarchical controller-estimator algorithms (HCEAs) to solve the coordination problem of networked Euler-Lagrange systems (NELSs) with sampled-data interactions and switching interaction topologies, where the cases with both discontinuous and continuous signals are successfully addressed in a unified framework. The HCEAs comprise two main layers (i.e., a control layer and an estimator layer) and one optional layer (i.e., a filter layer), in which the coordination problem is tackled in the main layers and the transient response can be optionally smoothed in the filter layer. For stabilizing the corresponding cascade closed-loop systems, several sufficient conditions on the upper bound of the aperiodic sampling intervals and the lower bound of the control parameters are established. In addition, the HCEAs are extended to address the task-space coordination problem of networked heterogeneous robotic systems, which shows the versatility of the HCEAs. Finally, comparison studies and simulation results are provided to demonstrate the effectiveness, significance, and advantages of the presented algorithms.																	2168-2267	2168-2275				JUN	2020	50	6					2450	2461		10.1109/TCYB.2019.2914861													
J								Network-Based Modeling and Proportional-Integral Control for Direct-Drive-Wheel Systems in Wireless Network Environments	IEEE TRANSACTIONS ON CYBERNETICS										Direct-drive-wheel (DDW) systems; height adjusting; network-based proportional-integral (PI) control; network-induced delays; stochastic packet dropouts	OUTPUT-FEEDBACK CONTROL; PREDICTIVE CONTROL; MULTIAGENT SYSTEM; NEURAL-NETWORKS; PID CONTROLLER; FUZZY-SYSTEMS; STABILITY; DELAYS	This paper focuses on the network-based modeling and proportional-integral (PI) control for a continuous-time direct-drive-wheel system in a wireless network environment. The developed system can simplify configuration, reduce bus cables, and realize vehicle height adjustment. A novel networkbased model is first established by constructing a PI control system and taking network-induced delays and stochastic packet dropouts into account. By using two different artificial delays to characterize the update of proportional and integral control signals, the network-based PI control system is modeled as a stochastic impulsive system with two input delays and reset equations at updating instants. Then, through involving the reset states and the relationship among two delayed states and the current state in the discontinuous Lyapunov-Krasovskii functional and actively introducing the upper bounds of nonzero network-induced delays, some exponential mean-square stability and H8 performance conditions with less conservatism are derived in terms of tractable linear matrix inequalities. An algorithm is presented to determine the minimum H8 performance and the corresponding PI control parameters by combining a particle swarm optimization technique with the performance condition. These results can be extended to a networkbased PI control of general continuous-time linear systems. A ZigBee-based network simulation platform is finally built and some simulation results are provided to validate the proposed methods.																	2168-2267	2168-2275				JUN	2020	50	6					2462	2474		10.1109/TCYB.2019.2924450													
J								Learning Graph Embedding With Adversarial Training Methods	IEEE TRANSACTIONS ON CYBERNETICS										Task analysis; Training; Clustering algorithms; Generators; Convolutional codes; Decoding; Data models; Adversarial regularization; graph autoencoder; graph clustering; graph convolutional networks (GCNs); graph embedding; link prediction		Graph embedding aims to transfer a graph into vectors to facilitate subsequent graph-analytics tasks like link prediction and graph clustering. Most approaches on graph embedding focus on preserving the graph structure or minimizing the reconstruction errors for graph data. They have mostly overlooked the embedding distribution of the latent codes, which unfortunately may lead to inferior representation in many cases. In this article, we present a novel adversarially regularized framework for graph embedding. By employing the graph convolutional network as an encoder, our framework embeds the topological information and node content into a vector representation, from which a graph decoder is further built to reconstruct the input graph. The adversarial training principle is applied to enforce our latent codes to match a prior Gaussian or uniform distribution. Based on this framework, we derive two variants of the adversarial models, the adversarially regularized graph autoencoder (ARGA) and its variational version, and adversarially regularized variational graph autoencoder (ARVGA), to learn the graph embedding effectively. We also exploit other potential variations of ARGA and ARVGA to get a deeper understanding of our designs. Experimental results that compared 12 algorithms for link prediction and 20 algorithms for graph clustering validate our solutions.																	2168-2267	2168-2275				JUN	2020	50	6					2475	2487		10.1109/TCYB.2019.2932096													
J								Flexible Linguistic Expressions and Consensus Reaching With Accurate Constraints in Group Decision-Making	IEEE TRANSACTIONS ON CYBERNETICS										Accuracy; consensus; flexible linguistic expression (FLE); group decision-making (GDM); linguistic decision-making; minimum preference-loss	MINIMUM-COST; TERM SETS; DISTRIBUTION ASSESSMENTS; REPRESENTATION MODEL; METHODOLOGY; INFORMATION; QUALITY; WORDS	Various linguistic expressions have been presented to model the flexibility of linguistic preference expressions and to support the consensus reaching in linguistic group decision-making (GDM). In this paper, we propose the concept of flexible linguistic expressions (FLEs) as a general linguistic preference expression format to improve the flexibility of the construction of complex linguistic expressions and the elicitation of linguistic preferences and, then, we develop a new linguistic GDM model with FLEs, referred to as FLE-based GDM (FLEGDM). In the FLEGDM, an FLE aggregation process with accurate constraints is developed to improve the quality (i.e., accuracy) of the collective result as well as guarantee the principle of minimum preference-loss through a mixed 0-1 linear programming model. Meanwhile, the consensus rules with minimum preference-loss are designed to support the consensus reaching process (CRS) in the FLEGDM. Finally, we present the detailed comparative analysis involving different linguistic GDM models to show the advantages of the FLEGDM.																	2168-2267	2168-2275				JUN	2020	50	6					2488	2501		10.1109/TCYB.2019.2906318													
J								A Consensus Community-Based Particle Swarm Optimization for Dynamic Community Detection	IEEE TRANSACTIONS ON CYBERNETICS										Sociology; Statistics; Heuristic algorithms; Clustering algorithms; Image edge detection; Particle swarm optimization; Minimization; Community detection; consensus community; dynamic network; particle swarm optimization (PSO)	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; NETWORKS	The community detection in dynamic networks is essential for important applications such as social network analysis. Such detection requires simultaneous maximization of the clustering accuracy at the current time step while minimization of the clustering drift between two successive time steps. In most situations, such objectives are often in conflict with each other. This article proposes the concept of consensus community. Knowledge from the previous step is obtained by extracting the intrapopulation consensus communities from the optimal population of the previous step. Subsequently, the intrapopulation consensus communities of the previous step obtained is voted by the population of the current time step during the evolutionary process. A subset of the consensus communities, which receives a high support rate, will be recognized as the interpopulation consensus communities of the previous and current steps. Interpopulation consensus communities are the knowledge that can be transferred from the previous to the current step. The population of the current time step can evolve toward the direction similar to the population in the previous time step by retaining such interpopulation consensus community during the evolutionary process. Community structure is subjected to evaluation, update, and mutation events, which are directed by using interpopulation consensus community information during the evolutionary process. The experimental results over many artificial and real-world dynamic networks illustrate that the proposed method produces more accurate and robust results than those of the state-of-the-art approaches.																	2168-2267	2168-2275				JUN	2020	50	6					2502	2513		10.1109/TCYB.2019.2938895													
J								Asynchronous Distributed Algorithms for Seeking Generalized Nash Equilibria Under Full and Partial-Decision Information	IEEE TRANSACTIONS ON CYBERNETICS										Games; Convergence; Heuristic algorithms; Distributed algorithms; Linear programming; Estimation; Couplings; Asynchronous computation; distributed algorithm; generalized Nash equilibrium (GNE); monotone game; multiagent systems; operator splitting methods	AGGREGATIVE GAMES; CONVERGENCE; NETWORKS	This paper investigates asynchronous algorithms for distributedly seeking generalized Nash equilibria with delayed information in multiagent networks. In the game model, a shared affine constraint couples all players' local decisions. Each player is assumed to only access its private objective function, private feasible set, and a local block matrix of the affine constraint. We first give an algorithm for the case when each agent is able to fully access all other players' decisions. By using auxiliary variables related to communication links and the edge Laplacian matrix, each player can carry on its iteration asynchronously with only private data and possibly delayed information from its neighbors. Then, we consider the case when agents cannot know all other players' decisions, called a partial-decision information case. We introduce a local estimation of the overall agents' decisions and incorporate consensus dynamics on these local estimations. The two algorithms do not need any centralized clock coordination, fully exploit the local computation resource, and remove the idle time due to waiting for the "slowest" agent. Both algorithms are developed by preconditioned forward-backward operator splitting, and their convergence is shown by relating them to asynchronous fixed-point iterations, under proper assumptions and fixed and nondiminishing step-size choices. Numerical studies verify the algorithms' convergence and efficiency.																	2168-2267	2168-2275				JUN	2020	50	6					2514	2526		10.1109/TCYB.2019.2908091													
J								Output-Feedback Cooperative Formation Maneuvering of Autonomous Surface Vehicles With Connectivity Preservation and Collision Avoidance	IEEE TRANSACTIONS ON CYBERNETICS										Collision avoidance; Observers; Uncertainty; Stability analysis; Vehicle dynamics; Marine vehicles; Output feedback; Autonomous surface vehicles (ASVs); collision avoidance; connectivity preservation; state observer; time-varying formation maneuvering	FOLLOWER FORMATION CONTROL; UNDERWATER VEHICLES; CONTAINMENT CONTROL; TRACKING CONTROL; SYSTEMS; VESSELS; DISTURBANCES; SUBJECT	In this paper, a cooperative time-varying formation maneuvering problem with connectivity preservation and collision avoidance is investigated for a fleet of autonomous surface vehicles (ASVs) with position-heading measurements. Each vehicle is subject to unknown kinetics induced by internal model uncertainty and external disturbances. At first, a nonlinear state observer is used to recover the unmeasured linear velocity and yaw rate as well as unknown uncertainty and disturbances. Then, observer-based cooperative time-varying formation maneuvering control laws are designed based on artificial potential functions, nonlinear tracking differentiators, and a backstepping technique. The stability of closed-loop distributed formation control system is analyzed based on input-to-state stability and cascade stability. The salient features of the proposed method are as follows. First, cooperative time-varying formation maneuvering with the capability of connectivity preservation and collision avoidance can be achieved in the absence of velocity measurements. Second, the complexity of the cooperative time-varying formation maneuvering control laws is reduced without resorting to dynamic surface control. Third, the uncertainty and disturbance are actively rejected in the presence of position-heading measurements. Simulation results are given to substantiate the proposed output feedback control method for cooperative time-varying formation maneuvering of ASVs with connectivity preservation and collision avoidance.																	2168-2267	2168-2275				JUN	2020	50	6					2527	2535		10.1109/TCYB.2019.2914717													
J								Adaptive Neural Command Filtering Control for Nonlinear MIMO Systems With Saturation Input and Unknown Control Direction	IEEE TRANSACTIONS ON CYBERNETICS										Adaptive systems; MIMO communication; Backstepping; Control systems; Artificial neural networks; Complexity theory; Adaptive neural control; command filtering control; multiple-input multiple-output (MIMO) nonlinear systems; Nussbaum functions	DYNAMIC SURFACE CONTROL; OUTPUT-FEEDBACK CONTROL; SINGULARLY PERTURBED SYSTEMS; VARYING DELAY SYSTEMS; TRACKING CONTROL; CONTROL DESIGN; DEAD-ZONE; NETWORKS; STATE	In this paper, the tracking control problem is considered for a class of multiple-input multiple-output (MIMO) nonlinear systems with input saturation and unknown direction control gains. A command filtered adaptive neural networks (NNs) control method is presented with regard to the MIMO systems by designing the virtual controllers and error compensation signals. First, the command filtering is used to solve the "explosion of complexity" problem in the conventional backstepping design and the nonlinearities are approximated by NNs. Then, the error compensation signals are developed to conquer the shortcoming of the dynamic surface method. In addition, the Nussbaum-type functions are utilized to cope with the unknown direction control gains. The effectiveness of the proposed new design scheme is illustrated by simulation examples.																	2168-2267	2168-2275				JUN	2020	50	6					2536	2545		10.1109/TCYB.2019.2901250													
J								Containment Control of Asynchronous Discrete-Time General Linear Multiagent Systems With Arbitrary Network Topology	IEEE TRANSACTIONS ON CYBERNETICS										Network topology; Vehicle dynamics; Multi-agent systems; Information exchange; Clocks; Matrix converters; Arbitrary network topology; asynchronous setting; containment control; linear multiagent systems (MASs)	SWITCHING TOPOLOGY; DYNAMIC LEADERS; CONSENSUS; AGENTS; COORDINATION; STATIONARY; BEHAVIOR	In this contribution, we propose and investigate the containment control issue for general linear multiagent systems (MASs) under the asynchronous setting, where the network topology is not subjected to any structural restrictions and the roles of the leaders and the followers are entirely determined by the network topology. It is assumed that the interaction time instants of each agent, at which this agent interacts with its neighbors, are independent of the other agents' and can be unevenly distributed. An asynchronous distributed algorithm is proposed to implement the control strategy of linear MASs. The non-negative matrix theory and the composition of binary relations are utilized to handle the asynchronous containment control issue. It is shown that the leaders in each closed and strongly connected component of the network topology will reach a common state and the followers will gradually enter the dynamic convex hull constructed by the leaders. Moreover, it is also proved that the system matrix can be strictly unstable, and the upper bound of the system matrix's spectral radius is explicitly stated. Finally, two simulation examples are also provided to verify the efficacy of our theoretical results.																	2168-2267	2168-2275				JUN	2020	50	6					2546	2556		10.1109/TCYB.2019.2915941													
J								Composite Learning Adaptive Dynamic Surface Control of Fractional-Order Nonlinear Systems	IEEE TRANSACTIONS ON CYBERNETICS										Backstepping; Parameter estimation; Convergence; Complexity theory; Adaptive systems; Explosions; Adaptive control; backstepping control; composite learning; dynamic surface control; fractional-order nonlinear system; interval excitation (IE)	BACKSTEPPING CONTROL; NEURAL-NETWORK; LYAPUNOV FUNCTIONS; TRACKING CONTROL; NONEXISTENCE; ALGORITHM; ROBUST	Adaptive dynamic surface control (ADSC) is effective for solving the complexity problem in adaptive backstepping control of integer-order nonlinear systems. This article focuses on the ADSC design for parametric uncertain fractional-order nonlinear systems (FONSs). In each backstepping step, the virtual controller is driven to pass through a fractional dynamic surface whose fractional-order derivative can be calculated easily. An ADSC law that ensure tracking error convergence is designed. The proposed ADSC requires a stringent condition called persistent excitation (PE) to achieve parameter convergence. To relax this limitation, a prediction error is defined by using online recorded data and instantaneous data, and a composite learning law is proposed to utilize both the prediction error and the tracking error. Then, a composite learning ADSC (CLADSC) method is developed to guarantee tracking error convergence and accurate parameter estimation under an interval excitation condition that is weaker than the PE one. Finally, an illustrative example is presented to show the performance of our methods.																	2168-2267	2168-2275				JUN	2020	50	6					2557	2567		10.1109/TCYB.2019.2938754													
J								Finite-Time Convergence Adaptive Neural Network Control for Nonlinear Servo Systems	IEEE TRANSACTIONS ON CYBERNETICS										Adaptive control; finite-time (FT) convergence; neural networks (NNs); parameter estimation; servomechanisms	PURE-FEEDBACK SYSTEMS; PARAMETER-ESTIMATION; TRACKING CONTROL; MOTION CONTROL; INPUT	Although adaptive control design with function approximators, for example, neural networks (NNs) and fuzzy logic systems, has been studied for various nonlinear systems, the classical adaptive laws derived based on the gradient descent algorithm with s-modification or e-modification cannot guarantee the parameter estimation convergence. These nonconvergent learning methods may lead to sluggish response in the control system and make the parameter tuning complex. The aim of this paper is to propose a new learning strategy driven by the estimation error to design the alternative adaptive laws for adaptive control of nonlinear servo systems. The parameter estimation error is extracted and used as a new leakage term in the adaptive laws. By using this new learning method, the convergence of both the estimated parameters and the tracking error can be achieved simultaneously. The proposed learning algorithm is further tailored to retain finite-time convergence. To handle unknown nonlinearities in the servomechanisms, an augmented NN with a new friction model is used, where both the NN weights and some friction model coefficients are estimated online via the proposed algorithms. Comparisons with the s-modification algorithm are addressed in terms of convergence property and robustness. Simulations and practical experiments are given to show the superior performance of the suggested adaptive algorithms.																	2168-2267	2168-2275				JUN	2020	50	6					2568	2579		10.1109/TCYB.2019.2893317													
J								Stability and Stabilization of T-S Fuzzy Systems With Time-Varying Delays via Delay-Product-Type Functional Method	IEEE TRANSACTIONS ON CYBERNETICS										Delays; Fuzzy systems; Linear matrix inequalities; Symmetric matrices; Stability criteria; Time-varying systems; Delay-product-type functional; extended reciprocally convex matrix inequality; stabilization; time-varying delay; T-S fuzzy system	OUTPUT-FEEDBACK CONTROL; INEQUALITY	This paper is concerned with the stability and stabilization problems of T-S fuzzy systems with time-varying delays. The purpose is to develop a new state-feedback controller design method with less conservatism. First, a novel Lyapunov-Krasovskii functional is constructed by combining delay-product-type functional method together with the state vector augmentation. By utilizing Wirtinger-based integral inequality and an extended reciprocally convex matrix inequality, a less conservative delay-dependent stability condition is developed. Then, the corresponding controller design method for the closed-loop delayed fuzzy system is derived based on parallel distributed compensation scheme. Finally, two classic numerical examples are given to show the effectiveness and merits of the proposed approaches.																	2168-2267	2168-2275				JUN	2020	50	6					2580	2589		10.1109/TCYB.2018.2890425													
J								Ultra-Wideband and Odometry-Based Cooperative Relative Localization With Application to Multi-UAV Formation Control	IEEE TRANSACTIONS ON CYBERNETICS										Estimation; Distance measurement; Ultra wideband technology; Sensors; Unmanned aerial vehicles; Global Positioning System; Time measurement; Cooperative relative localization (RL); distributed formation control; GPS-denied environments; ultra-wideband ranging and communication (RCM) network; unmanned aerial vehicles (UAVs)	SYSTEMS	This puts forth an infrastructure-free cooperative relative localization (RL) for unmanned aerial vehicles (UAVs) in global positioning system (GPS)-denied environments. Instead of estimating relative coordinates with vision-based methods, an onboard ultra-wideband (UWB) ranging and communication (RCM) network is adopted to both sense the inter-UAV distance and exchange information for RL estimation in 2-D spaces. Without any external infrastructures prepositioned, each agent cooperatively performs a consensus-based fusion, which fuses the obtained direct and indirect RL estimates, to generate the relative positions to its neighbors in real time despite the fact that some UAVs may not have direct range measurements to their neighbors. The proposed RL estimation is then applied to formation control. Extensive simulations and real-world flight tests corroborate the merits of the developed RL algorithm.																	2168-2267	2168-2275				JUN	2020	50	6					2590	2603		10.1109/TCYB.2019.2905570													
J								Dimensionality Reduction of Hyperspectral Imagery Based on Spatial-Spectral Manifold Learning	IEEE TRANSACTIONS ON CYBERNETICS										Manifolds; Hyperspectral imaging; Feature extraction; Dimensionality reduction; Germanium; Image reconstruction; Dimensionality reduction; discriminant features; hyperspectral remote sensing; manifold learning; spatial-spectral combined distance (SSCD)	CLASSIFICATION; REPRESENTATION; INFORMATION; EXTRACTION	The graph embedding (GE) methods have been widely applied for dimensionality reduction of hyperspectral imagery (HSI). However, a major challenge of GE is how to choose the proper neighbors for graph construction and explore the spatial information of HSI data. In this paper, we proposed an unsupervised dimensionality reduction algorithm called spatial-spectral manifold reconstruction preserving embedding (SSMRPE) for HSI classification. At first, a weighted mean filter (WMF) is employed to preprocess the image, which aims to reduce the influence of background noise. According to the spatial consistency property of HSI, SSMRPE utilizes a new spatial-spectral combined distance (SSCD) to fuse the spatial structure and spectral information for selecting effective spatial-spectral neighbors of HSI pixels. Then, it explores the spatial relationship between each point and its neighbors to adjust the reconstruction weights to improve the efficiency of manifold reconstruction. As a result, the proposed method can extract the discriminant features and subsequently improve the classification performance of HSI. The experimental results on the PaviaU and Salinas hyperspectral data sets indicate that SSMRPE can achieve better classification results in comparison with some state-of-the-art methods.																	2168-2267	2168-2275				JUN	2020	50	6					2604	2616		10.1109/TCYB.2019.2905793													
J								Finite-Time Fuzzy Control of Stochastic Nonlinear Systems	IEEE TRANSACTIONS ON CYBERNETICS										Nonlinear systems; Stability criteria; Fuzzy logic; Asymptotic stability; Fuzzy control; Adaptive systems; Adaptive fuzzy control; finite-time stability; square stability; stochastic nonlinear systems	OUTPUT-FEEDBACK CONTROL; ADAPTIVE NEURAL-CONTROL; STABILIZATION; STABILITY; TRACKING; DESIGN	This paper studies the finite-time stabilization of a class of stochastic nonlinear systems. Different from functions which are necessarily known in the conventional finite-time control of nonlinear systems, the nonlinear functions can be completely unknown in this paper. By applying fuzzy-logic systems to approximate the unknown nonlinearities, a novel adaptive finite-time control strategy is proposed. However, due to the existence of approximation errors, the existing finite-time stability criterion cannot be used to analyze the stability of stochastic nonlinear systems. To deal with this difficulty, a finite-time stability criterion, by utilizing the mean value theorem of integrals, is first established in Lemma 5, which plays a significant role in the finite-time stability analysis of stochastic nonlinear systems. Then, the finite-time mean square stability of a stochastic nonlinear system is proved by combining Lemma 3 with Jensen's inequality.																	2168-2267	2168-2275				JUN	2020	50	6					2617	2626		10.1109/TCYB.2019.2925573													
J								Distributed Fixed-Time Consensus Tracking Control of Uncertain Nonlinear Multiagent Systems: A Prioritized Strategy	IEEE TRANSACTIONS ON CYBERNETICS										Nickel; Topology; Switches; Protocols; Multi-agent systems; Network topology; Distributed control; fixed-time consensus; nonlinear multiagent systems (MASs); prioritized strategy; switching topology	LEADER-FOLLOWING CONSENSUS; STABILIZATION; DESIGN; ALGORITHM; NETWORKS; AGENTS	This paper focuses on the fixed-time consensus tracking problem for uncertain high-order nonlinear multiagent systems (MASs) under switching topologies. Unlike the traditional methods using the given original topology directly, a prioritized strategy is first proposed to assign a priority to the topological network such that the information from the leader is delivered to each agent in a least transit route (LTR). For each agent, only parts of its received information are used to design controller, which reduces the computing burden and complexity in the design process. According to the proposed prioritized strategy, the consensus tracking problem is transformed into general tracking problem. By utilizing the power integration technique, the uncertainties of nonlinear functions in MASs are dealt with. Finally, the distributed fixed-time control protocols are developed to ensure all the followers achieve the consensus with the leader under different topological cases. Rigorous stability analysis has been given and simulation results verify the effectiveness of the proposed method.																	2168-2267	2168-2275				JUN	2020	50	6					2627	2638		10.1109/TCYB.2019.2925123													
J								Adaptive Finite-Time Fuzzy Control of Nonlinear Active Suspension Systems With Input Delay	IEEE TRANSACTIONS ON CYBERNETICS										Active suspension systems; adaptive control; finite-time (FT) convergence; fuzzy logic systems (FLSs); input time delay	TRACKING CONTROL; PARAMETER-ESTIMATION; ROBUST-CONTROL; VARYING INPUT; APPROXIMATION; COMPENSATION; PERFORMANCE	This paper presents a new adaptive fuzzy control scheme for active suspension systems subject to control input time delay and unknown nonlinear dynamics. First, a predictor-based compensation scheme is constructed to address the effect of input delay in the closed-loop system. Then, a fuzzy logic system (FLS) is employed as the function approximator to address the unknown nonlinearities. Finally, to enhance the transient suspension response, a novel parameter estimation error-based finite-time (FT) adaptive algorithm is developed to online update the unknown FLS weights, which differs from traditional estimation methods, for example, gradient algorithm with e-modification or s-modification. In this framework, both the suspension and estimation errors can achieve convergence in FT. A Lyapunov-Krasovskii functional is constructed to prove the closed-loop system stability. Comparative simulation results based on a dynamic simulator built in a professional vehicle simulation software, Carsim, are provided to demonstrate the validity of the proposed control approach, and show its effectiveness to operate active suspension systems safely and reliably in various road conditions.																	2168-2267	2168-2275				JUN	2020	50	6					2639	2650		10.1109/TCYB.2019.2894724													
J								New Super-Twisting Zeroing Neural-Dynamics Model for Tracking Control of Parallel Robots: A Finite-Time and Robust Solution	IEEE TRANSACTIONS ON CYBERNETICS										Convergence; Parallel robots; Real-time systems; Robustness; Heuristic algorithms; Legged locomotion; Finite-time convergence; robot manipulators; robustness; super-twisting (ST); tracking control; zeroing neural-dynamics (ZNDs)	MANIPULATORS	Parallel robots are usually required to perform real-time tracking control tasks in the presence of external disturbances in the complex environment. Conventional zeroing neural-dynamics (ZNDs) provide an alternative solution for the real-time tracking control of parallel robots due to its capacity of parallel processing and nonlinearity handling. However, it is still a challenge for the solution in a unified framework of the ZND to deal with the external disturbances, and simultaneously possess a finite-time convergence property. In this paper, a novel ZND model by exploring the super-twisting (ST) algorithm, named ST-ZND model, is proposed. The theoretical analyses on the global stability, finite-time convergence, as well as the robustness against the external disturbances are rigorously presented. Finally, the effectiveness and superiority of the ST-ZND model for the real-time tracking control of parallel robots are demonstrated by two illustrative examples, comparisons, and convergence tests.																	2168-2267	2168-2275				JUN	2020	50	6					2651	2660		10.1109/TCYB.2019.2930662													
J								Impulsive Control of Nonlinear Systems With Time-Varying Delay and Applications	IEEE TRANSACTIONS ON CYBERNETICS										Delays; Delay effects; Neural networks; Synchronization; Perturbation methods; Stability analysis; Time-varying systems; Delay systems; exponential stability; impulsive control; neural networks; synchronization control; unbounded time-varying delay	GLOBAL EXPONENTIAL STABILITY; TO-STATE STABILITY; NEURAL-NETWORKS; PERIODIC-SOLUTIONS; ASYMPTOTIC STABILITY; SYNCHRONIZATION; STABILIZATION; DISCRETE	Impulsive control of nonlinear delay systems is studied in this paper, where the time delays addressed may be the constant delay, bounded time-varying delay, or unbounded time-varying delay. Based on the impulsive control theory and some analysis techniques, a new theoretical result for global exponential stability is derived from the impulsive control point of view. The significance of the presented result is that the stability can be achieved via the impulsive control at certain impulse points despite the existence of impulsive perturbations which causes negative effect to the control. That is, the impulsive control provides a super performance to allow the existence of impulsive perturbations. In addition, we apply the theoretical result to the problem of impulsive control of delayed neural networks. Some results for global exponential stability and synchronization control of neural networks with time delays are derived via impulsive control. Three illustrated examples are given to show the effectiveness and distinctiveness of the proposed impulsive control schemes.																	2168-2267	2168-2275				JUN	2020	50	6					2661	2673		10.1109/TCYB.2019.2896340													
J								Temporally Identity-Aware SSD With Attentional LSTM	IEEE TRANSACTIONS ON CYBERNETICS										Detectors; Videos; Feature extraction; Visualization; Task analysis; Real-time systems; Proposals; Object detection; sequential learning; tracking-by-detection; video processing	NEURAL-NETWORKS	Temporal object detection has attracted significant attention, but most popular detection methods cannot leverage rich temporal information in videos. Very recently, many algorithms have been developed for video detection task, yet very few approaches can achieve real-time online object detection in videos. In this paper, based on the attention mechanism and convolutional long short-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD) for real-world detection. Distinct from the previous methods, we take aim at temporally integrating pyramidal feature hierarchy using ConvLSTM, and design a novel structure, including a low-level temporal unit as well as a high-level one for multiscale feature maps. Moreover, we develop a creative temporal analysis unit, namely, attentional ConvLSTM, in which a temporal attention mechanism is specially tailored for background suppression and scale suppression, while a ConvLSTM integrates attention-aware features across time. An association loss and a multistep training are designed for temporal coherence. Besides, an online tubelet analysis (OTA) is exploited for identification. Our framework is evaluated on ImageNet VID dataset and 2DMOT15 dataset. Extensive comparisons on the detection and tracking capability validate the superiority of the proposed approach. Consequently, the developed TSSD-OTA achieves a fast speed and an overall competitive performance in terms of detection and tracking. Finally, a real-world maneuver is conducted for underwater object grasping.																	2168-2267	2168-2275				JUN	2020	50	6					2674	2686		10.1109/TCYB.2019.2894261													
J								Cooperative Deep Reinforcement Learning for Large-Scale Traffic Grid Signal Control	IEEE TRANSACTIONS ON CYBERNETICS										Deep reinforcement learning (DRL); intelligent transportation systems; traffic signal control	INTELLIGENCE	Exploiting reinforcement learning (RL) for traffic congestion reduction is a frontier topic in intelligent transportation research. The difficulty in this problem stems from the inability of the RL agent simultaneously monitoring multiple signal lights when taking into account complicated traffic dynamics in different regions of a traffic system. Such challenge is even more outstanding when forming control decisions on a large-scale traffic grid, where the RL action space grows exponentially with the number of intersections within the traffic grid. In this paper, we tackle such a problem by proposing a cooperative deep reinforcement learning (Coder) framework. The intuition behind Coder is to decompose the original difficult RL task as a number of subproblems with relatively easy RL goals. Accordingly, we implement Coder with multiple regional agents and a centralized global agent. Each regional agent learns its own RL policy and value functions over a small region with limited actions. Then, the centralized global agent hierarchically aggregates RL achievements from different regional agents and forms the final ${Q}$ -function over the entire large-scale traffic grid. The experimental investigations demonstrate that the proposed Coder could reduce on average 30% congestions in terms of the number of waiting vehicles during high density traffic flows in simulations.																	2168-2267	2168-2275				JUN	2020	50	6					2687	2700		10.1109/TCYB.2019.2904742													
J								Bionic Face Sketch Generator	IEEE TRANSACTIONS ON CYBERNETICS										Face; Generators; Learning systems; Entertainment industry; Probabilistic logic; Graphics; Biological system modeling; Bionic face sketch generator; convolutional neural network; facial entertainment; generative adversarial network (GAN); probabilistic graphic model	RECOGNITION; MODEL	Face sketch synthesis is a crucial technique in digital entertainment. However, the existing face sketch synthesis approaches usually generate face sketches with coarse structures. The fine details on some facial components fail to be generated. In this paper, inspired by the artists during drawing face sketches, we propose a bionic face sketch generator. It includes three parts: 1) a coarse part; 2) a fine part; and 3) a finer part. The coarse part builds the facial structure of a sketch by a generative adversarial network in the U-Net. In the middle part, the noise produced by the coarse part is erased and the fine details on the important face components are generated via a probabilistic graphic model. To compensate for the fine sketch with distinctive edge and area of shadows and lights, we learn a mapping relationship at the high-frequency band by a convolutional neural network in the finer part. The experimental results show that the proposed bionic face sketch generator can synthesize the face sketch with more delicate and striking details, satisfy the requirement of users in the digital entertainment, and provide the students with the coarse, fine, and finer face sketch copies when learning sketches. Compared with the state-of-the-art methods, the proposed approach achieves better results in both visual effects and quantitative metrics.																	2168-2267	2168-2275				JUN	2020	50	6					2701	2714		10.1109/TCYB.2019.2924589													
J								Dynamic Group Learning Distributed Particle Swarm Optimization for Large-Scale Optimization and Its Application in Cloud Workflow Scheduling	IEEE TRANSACTIONS ON CYBERNETICS										Cloud computing; Task analysis; Optimization; Sociology; Statistics; Processor scheduling; Dynamic scheduling; Adaptive renumber strategy (ARS); dynamic group learning distributed particle swarm optimization (DGLDPSO); dynamic group learning strategy; large-scale cloud workflow scheduling; master-slave multigroup distributed	COOPERATIVE COEVOLUTION; ALGORITHM	Cloud workflow scheduling is a significant topic in both commercial and industrial applications. However, the growing scale of workflow has made such a scheduling problem increasingly challenging. Many current algorithms often deal with small- or medium-scale problems (e.g., less than 1000 tasks) and face difficulties in providing satisfactory solutions when dealing with the large-scale problems, due to the curse of dimensionality. To this aim, this article proposes a dynamic group learning distributed particle swarm optimization (DGLDPSO) for large-scale optimization and extends it for the large-scale cloud workflow scheduling. DGLDPSO is efficient for large-scale optimization due to its following two advantages. First, the entire population is divided into many groups, and these groups are coevolved by using the master-slave multigroup distributed model, forming a distributed PSO (DPSO) to enhance the algorithm diversity. Second, a dynamic group learning (DGL) strategy is adopted for DPSO to balance diversity and convergence. When applied DGLDPSO into the large-scale cloud workflow scheduling, an adaptive renumber strategy (ARS) is further developed to make solutions relate to the resource characteristic and to make the searching behavior meaningful rather than aimless. Experiments are conducted on the large-scale benchmark functions set and the large-scale cloud workflow scheduling instances to further investigate the performance of DGLDPSO. The comparison results show that DGLDPSO is better than or at least comparable to other state-of-the-art large-scale optimization algorithms and workflow scheduling algorithms.																	2168-2267	2168-2275				JUN	2020	50	6					2715	2729		10.1109/TCYB.2019.2933499													
J								Edge-Semantic Learning Strategy for Layout Estimation in Indoor Environment	IEEE TRANSACTIONS ON CYBERNETICS										Layout; Semantics; Estimation; Image edge detection; Training; Decoding; Deconvolution; Deep neural network; indoor environment; layout estimation; scene understanding; visual cognition		Visual cognition of the indoor environment can benefit from the spatial layout estimation, which is to represent an indoor scene with a 2-D box on a monocular image. In this paper, we propose to fully exploit the edge and semantic information of a room image for layout estimation. More specifically, we present an encoder-decoder network with shared encoder and two separate decoders, which are composed of multiple deconvolution (transposed convolution) layers, to jointly learn the edge maps and semantic labels of a room image. We combine these two network predictions in a scoring function to evaluate the quality of the layouts, which are generated by ray sampling and from a predefined layout pool. Guided by the scoring function, we apply a novel refinement strategy to further optimize the layout hypotheses. Experimental results show that the proposed network can yield accurate estimates of edge maps and semantic labels. By fully utilizing the two different types of labels, the proposed method achieves the state-of-the-art layout estimation performance on the benchmark datasets.																	2168-2267	2168-2275				JUN	2020	50	6					2730	2739		10.1109/TCYB.2019.2895837													
J								Intelligent Critic Control With Robustness Guarantee of Disturbed Nonlinear Plants	IEEE TRANSACTIONS ON CYBERNETICS										Robustness; Adaptive systems; Cost function; Neural networks; Optimal control; Stability analysis; Adaptation models; Adaptive learning; disturbance; intelligent critic control; neural identifier; robustness	POLICY-ITERATION; SYSTEMS; DESIGN; SUM	In this paper, the author focuses on establishing an intelligent critic control framework with robustness guarantee for disturbed nonlinear systems. Combining the neural network learning ability with adaptive critic designs, a general structure of intelligent critic control is developed to address the robustness problems, which broadens the application scope of adaptive dynamic programming and the related learning control methods. First, the problem transformation is conducted for changing the robust stabilization problem into optimal control design, where a special discounted cost function is well defined. Then, a recurrent neural network is constructed to learn the unknown nominal plant with stability proof. Moreover, the critic network implementation is presented with the help of the obtained neural identifier and the adaptive learning architecture. In addition, extension discussions and several simulation examples are provided to display the robustness verification results of the intelligent critic strategy.																	2168-2267	2168-2275				JUN	2020	50	6					2740	2748		10.1109/TCYB.2019.2903117													
J								A Hierarchical Recurrent Neural Network for Symbolic Melody Generation	IEEE TRANSACTIONS ON CYBERNETICS										Bars; Rhythm; Recurrent neural networks; Generators; Gallium nitride; Melody generation; recurrent neural network (RNN)		In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty to design a good model. In this article, we present a hierarchical recurrent neural network (HRNN) for melody generation, which consists of three long-short-term-memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles, and notes, in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance to generate the finer time-scale melody components in the low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the recently proposed models MidiNet and MusicVAE, the HRNN produces better melodies evaluated by humans.																	2168-2267	2168-2275				JUN	2020	50	6					2749	2757		10.1109/TCYB.2019.2953194													
J								Anti-Synchronization in Fixed Time for Discontinuous Reaction-Diffusion Neural Networks With Time-Varying Coefficients and Time Delay	IEEE TRANSACTIONS ON CYBERNETICS										Neurons; Synchronization; Biological neural networks; Lyapunov methods; Convergence; Delay effects; Mathematical model; Discontinuous reaction-diffusion neural networks (DRDNNs); fixed-time anti-synchronization (FTAS); integral state-feedback control algorithm; novel state-feedback control algorithm; time-varying coefficient	FINITE-TIME; STABILITY; SYSTEMS; PASSIVITY; DESIGN	This paper studies the fixed-time anti-synchronization (FTAS) of discontinuous reaction-diffusion neural networks (DRDNNs) with both time-varying coefficients and time delay. First, differential inclusion theory is used to deal with the influence caused by discontinuous activations. In addition, a new fixed-time convergence theorem is used to handle the time-varying coefficients. Second, a novel state-feedback control algorithm and integral state-feedback control algorithm are proposed to realize FTAS of DRDNNs. During the generalized (adaptive) pinning control strategy, a guideline is proposed to select neurons to pin the designed controller. Furthermore, we present several criteria on FTAS by using the generalized Lyapunov function method. Different from the traditional Lyapunov function with negative definite derivative, the derivative of the Lyapunov function can be positive in this paper. Finally, we give two numerical simulations to substantiate the merits of the obtained results.																	2168-2267	2168-2275				JUN	2020	50	6					2758	2769		10.1109/TCYB.2019.2913200													
J								New Criteria on Global Stabilization of Delayed Memristive Neural Networks With Inertial Item	IEEE TRANSACTIONS ON CYBERNETICS										Artificial neural networks; Circuit stability; Stability analysis; Synchronization; Asymptotic stability; Adaptive control; Adaptive control; memristor; neural networks (NNs); stabilization; time-varying delays	FINITE-TIME STABILIZATION; EXPONENTIAL SYNCHRONIZATION; DISSIPATIVITY ANALYSIS; ASYMPTOTIC STABILITY; LAG SYNCHRONIZATION; VARYING DELAYS; BIFURCATION; MODEL; CHAOS	In this paper, we are concerned with global stabilization for a kind of delayed memristive neural network with an inertial term. By building a new Lyapunov functional and designing a feedback controller, we obtain some new results on global stabilization of the addressed delayed memristive inertial neural networks (MINNs). An adaptive control strategy is also designed to realize the global stabilization. Compared with the reduced-order method used in the existing literature, we consider the stabilization directly from the MINNs themselves without a reduced-order method. In addition, the new results proposed here are shown as algebraic criteria, which are easy to test. At last, some simulations are given to show the validity of the derived criteria.																	2168-2267	2168-2275				JUN	2020	50	6					2770	2780		10.1109/TCYB.2018.2889653													
J								High-Performance Visual Tracking With Extreme Learning Machine Framework	IEEE TRANSACTIONS ON CYBERNETICS										Feature extraction; Visualization; Target tracking; Support vector machines; Adaptation models; Computational modeling; Extreme learning machine autoencoder (ELM-AE); extreme learning machine (ELM); feature classification; feature learning; online sequential ELM (OS-ELM); robust visual tracking	OBJECT TRACKING	In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker.																	2168-2267	2168-2275				JUN	2020	50	6					2781	2792		10.1109/TCYB.2018.2886580													
J								Robust Partial-Nodes-Based State Estimation for Complex Networks Under Deception Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Delays; State estimation; Couplings; Security; Symmetric matrices; Complex networks; Uncertainty; Deception attacks; finite-distributed delays; partial-nodes-based estimation; state estimation; uncertain complex networks (CNs); uncertain inner-coupling matrix	H-INFINITY CONTROL; STABILITY ANALYSIS; NEURAL-NETWORKS; CONTROL-SYSTEMS; TIME; SYNCHRONIZATION	In this paper, the partial-nodes-based state estimators (PNBSEs) are designed for a class of uncertain complex networks subject to finite-distributed delays, stochastic disturbances, as well as randomly occurring deception attacks (RODAs). In consideration of the likely unavailability of the output signals in harsh environments from certain network nodes, only partial measurements are utilized to accomplish the state estimation task for the addressed complex network with norm-bounded uncertainties in both the network parameters and the inner couplings. The RODAs are taken into account to reflect the compromised data transmissions in cyber security. We aim to derive the gain parameters of the estimators such that the overall estimation error dynamics satisfies the specified security constraint in the simultaneous presence of stochastic disturbances and deception signals. Through intensive stochastic analysis, sufficient conditions are obtained to guarantee the desired security performance for the PNBSEs, based on which the estimator gains are acquired by solving certain matrix inequalities with nonlinear constraints. A simulation study is carried out to testify the security performance of the presented state estimation method.																	2168-2267	2168-2275				JUN	2020	50	6					2793	2802		10.1109/TCYB.2019.2918760													
J								Torus-Event-Based Fault Diagnosis for Stochastic Multirate Time-Varying Systems With Constrained Fault	IEEE TRANSACTIONS ON CYBERNETICS										Ellipsoid-constrained fault (ECF); fault detection and isolation (FDI); multirate time-varying systems; torus-event-triggered communication; variance-constrained estimation	NETWORKED CONTROL-SYSTEMS; FUZZY-SYSTEMS; QUANTIZATION; CONSENSUS	In this paper, the torus-event-based fault detection and isolation (FDI) problem is investigated for a class of time-varying multirate systems. An ellipsoidal constraint is first adopted to describe the fault in a more practical pattern, and a novel torus-event-triggering scheme is proposed to improve the unilateral triggering mechanism. The aim is to design the torus-event-based fault detection filter and fault isolation estimators such that both the prescribed variance constraint on the estimation error and the desired ${H}_{\infty }$ performance on the disturbance are guaranteed over the finite horizon. Especially, the residual evaluation function is employed to detect the fault, and the residual matching function is developed to isolate the fault. Furthermore, three optimization problems are provided to seek separately the minimal parameters on the ${H}_{\infty }$ performance level, the upper bound of the estimation error variance, and the triggering torus. Finally, two simulation examples are utilized to show the effectiveness of the FDI scheme proposed in this paper.																	2168-2267	2168-2275				JUN	2020	50	6					2803	2813		10.1109/TCYB.2019.2895238													
J								A Scalable Test Suite for Continuous Dynamic Multiobjective Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Heuristic algorithms; Benchmark testing; Optimization; Shape; Scalability; Cybernetics; Computer science; Adversarial examples; dynamic multiobjective optimization (DMO); dynamics; Pareto front; scalable test problems	PREDICTION STRATEGY; ALGORITHMS	Dynamic multiobjective optimization (DMO) has gained increasing attention in recent years. Test problems are of great importance in order to facilitate the development of advanced algorithms that can handle dynamic environments well. However, many of the existing dynamic multiobjective test problems have not been rigorously constructed and analyzed, which may induce some unexpected bias when they are used for algorithmic analysis. In this paper, some of these biases are identified after a review of widely used test problems. These include poor scalability of objectives and, more important, problematic overemphasis of static properties rather than dynamics making it difficult to draw accurate conclusion about the strengths and weaknesses of the algorithms studied. A diverse set of dynamics and features is then highlighted that a good test suite should have. We further develop a scalable continuous test suite, which includes a number of dynamics or features that have been rarely considered in literature but frequently occur in real life. It is demonstrated with empirical studies that the proposed test suite is more challenging to the DMO algorithms found in the literature. The test suite can also test algorithms in ways that existing test suites cannot.																	2168-2267	2168-2275				JUN	2020	50	6					2814	2826		10.1109/TCYB.2019.2896021													
J								Semiglobal Observer-Based Non-Negative Edge Consensus of Networked Systems With Actuator Saturation	IEEE TRANSACTIONS ON CYBERNETICS										Actuators; Network topology; Observers; Kinetic theory; Heuristic algorithms; Graph theory; Cybernetics; Actuator saturation; edge consensus; low-gain output feedback; non-negative constraint; observer	LEADER-FOLLOWING CONSENSUS; LINEAR MULTIAGENT SYSTEMS; INPUT SATURATION; CONTROLLABILITY	The observer-based edge-consensus problem of networked continuous-time dynamical systems with edge state non-negative constraint and actuator saturation is considered in this paper. Based on line graph theory, low-gain output-feedback technique and algebraic Riccati equation (ARE)-based method, two edge-consensus algorithms are designed to achieve the observer-based edge consensus, in which the specific mathematical expressions of the two algorithms are obtained. Meanwhile, sufficient conditions are obtained to meet the bounded inputs and the non-negative edge states by combining with the ARE-based low-gain output-feedback technique and the positive system theory. Moreover, the feedback-gain and observer-gain matrices which must meet the sufficient conditions at the same time are existed and easy to obtain. Finally, two simulation cases are introduced to show the effectiveness of the theoretical results.																	2168-2267	2168-2275				JUN	2020	50	6					2827	2836		10.1109/TCYB.2019.2917006													
J								Hybrid Noise-Oriented Multilabel Learning	IEEE TRANSACTIONS ON CYBERNETICS										Correlation; Predictive models; Noise measurement; Labeling; Matrices; Training data; Learning systems; Bi-sparsity; hybrid noise; label enrichment; multilabel learning	LABEL CLASSIFICATION; LOW-RANK	For real-world applications, multilabel learning usually suffers from unsatisfactory training data. Typically, features may be corrupted or class labels may be noisy or both. Ignoring noise in the learning process tends to result in an unreasonable model and, thus, inaccurate prediction. Most existing methods only consider either feature noise or label noise in multilabel learning. In this paper, we propose a unified robust multilabel learning framework for data with hybrid noise, that is, both feature noise and label noise. The proposed method, hybrid noise-oriented multilabel learning (HNOML), is simple but rather robust for noisy data. HNOML simultaneously addresses feature and label noise by bi-sparsity regularization bridged with label enrichment. Specifically, the label enrichment matrix explores the underlying correlation among different classes which improves the noisy labeling. Bridged with the enriching label matrix, the structured sparsity is imposed to jointly handle the corrupted features and noisy labeling. We utilize the alternating direction method (ADM) to efficiently solve our problem. Experimental results on several benchmark datasets demonstrate the advantages of our method over the state-of-the-art ones.																	2168-2267	2168-2275				JUN	2020	50	6					2837	2850		10.1109/TCYB.2019.2894985													
J								Output Feedback Stabilization of Networked Control Systems Under a Stochastic Scheduling Protocol	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Output feedback; Delays; Scheduling; Communication networks; Stochastic processes; Networked control systems; Independent and identically distributed (i; i; d) scheduling protocol; networked control systems (NCSs); output feedback control	H-INFINITY CONTROL; STABILITY ANALYSIS; ROUND-ROBIN; COMMUNICATION; CONSTRAINTS; ALGORITHM; DELAYS	This paper investigates the output feedback stabilization problem for networked control systems under a stochastic scheduling protocol. First, an independent and identically distributed (i.i.d) scheduling protocol is introduced to orchestrate the signal transmission via a communication network. Taking into account the i.i.d protocol, network-induced delay, and packet dropout, a stochastic impulsive delayed model is presented for the studied system. Second, by use of the Lyapunov-Krasovskii functional approach, sufficient conditions for guaranteeing the stability of the studied system in the mean-square sense are obtained in the form of matrix inequalities. Moreover, an optimization algorithm is investigated to obtain the suitable dynamic output feedback controller and optimal i.i.d protocol parameters simultaneously. Finally, two numerical examples are presented to show the validity of the proposed method.																	2168-2267	2168-2275				JUN	2020	50	6					2851	2860		10.1109/TCYB.2019.2894294													
J								Reinforcement Learning-Based Optimal Sensor Placement for Spatiotemporal Modeling	IEEE TRANSACTIONS ON CYBERNETICS										Spatiotemporal phenomena; Optimization; Linear programming; Modeling; Mathematical model; Nonlinear dynamical systems; Distributed parameter systems (DPSs); Karhunen-Loeve decomposition (KLD); optimal sensor placement; reinforcement learning (RL); spatiotemporal modeling	LOCATION; DESIGN; SELECTION; IDENTIFICATION; OBSERVABILITY; OPTIMIZATION; TEMPERATURE; STATE	A reinforcement learning-based method is proposed for optimal sensor placement in the spatial domain for modeling distributed parameter systems (DPSs). First, a low-dimensional subspace, derived by Karhunen-Loeve decomposition, is identified to capture the dominant dynamic features of the DPS. Second, a spatial objective function is proposed for the sensor placement. This function is defined in the obtained low-dimensional subspace by exploiting the time-space separation property of distributed processes, and in turn aims at minimizing the modeling error over the entire time and space domain. Third, the sensor placement configuration is mathematically formulated as a Markov decision process (MDP) with specified elements. Finally, the sensor locations are optimized through learning the optimal policies of the MDP according to the spatial objective function. The experimental results of a simulated catalytic rod and a real snap curing oven system are provided to demonstrate the feasibility and efficiency of the proposed method in solving the combinatorial optimization problems, such as optimal sensor placement.																	2168-2267	2168-2275				JUN	2020	50	6					2861	2871		10.1109/TCYB.2019.2901897													
J								Transfer Clustering Ensemble Selection	IEEE TRANSACTIONS ON CYBERNETICS										Clustering algorithms; Partitioning algorithms; Task analysis; Micromechanical devices; Redundancy; Indexes; Cybernetics; Clustering ensemble selection (CES); machine learning; multiobjective; transfer learning	FRAMEWORK; DIVERSITY; CONSENSUS; QUALITY; SCHEME	Clustering ensemble (CE) takes multiple clustering solutions into consideration in order to effectively improve the accuracy and robustness of the final result. To reduce redundancy as well as noise, a CE selection (CES) step is added to further enhance performance. Quality and diversity are two important metrics of CES. However, most of the CES strategies adopt heuristic selection methods or a threshold parameter setting to achieve tradeoff between quality and diversity. In this paper, we propose a transfer CES (TCES) algorithm which makes use of the relationship between quality and diversity in a source dataset, and transfers it into a target dataset based on three objective functions. Furthermore, a multiobjective self-evolutionary process is designed to optimize these three objective functions. Finally, we construct a transfer CE framework (TCE-TCES) based on TCES to obtain better clustering results. The experimental results on 12 transfer clustering tasks obtained from the 20newsgroups dataset show that TCE-TCES can find a better tradeoff between quality and diversity, as well as obtaining more desirable clustering results.																	2168-2267	2168-2275				JUN	2020	50	6					2872	2885		10.1109/TCYB.2018.2885585													
J								HDec-POSMDPs MRS Exploration and Fire Searching Based on IoT Cloud Robotics	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Multi-robot systems; hybrid decentralized partially observable semi-Markov decision process (HDec-POSMDPs); multi-robot systems (MRS) exploration and fire searching; cloud robotics h]cloud computing	REAL-TIME; MOBILE ROBOT; SYSTEM; ARCHITECTURE; DESIGN	The multi-robot systems (MRS) exploration and fire searching problem is an important application of mobile robots which require massive computation capability that exceeds the ability of traditional MRS's. This paper propose a cloud-based hybrid decentralized partially observable semi-Markov decision process (HDec-POSMDPs) model. The proposed model is implemented for MRS exploration and fire searching application based on the Internet of things (IoT) cloud robotics framework. In this implementation the heavy and expensive computational tasks are offloaded to the cloud servers. The proposed model achieves a significant improvement in the computation burden of the whole task relative to a traditional MRS. The proposed model is applied to explore and search for fire objects in an unknown environment; using different sets of robots sizes. The preliminary evaluation of this implementation demonstrates that as the parallelism of computational instances increase the delay of new actuation commands which will be decreased, the mean time of task completion is decreased, the number of turns in the path from the start pose cells to the target cells is minimized and the energy consumption for each robot is reduced.																	1476-8186	1751-8520				JUN	2020	17	3					364	377		10.1007/s11633-019-1187-6													
J								Research on End-force Output of 8-cable Driven Parallel Manipulator	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Cable driven parallel manipulators; low gravity environment; end-force output; cable force control; mix control strategy	DESIGN	The return capsule needs to be launched to the moon and return back to earth in the third stage of the Chinese lunar exploration project. Therefore, it is necessary to perform simulations on the ground. This paper presents an 8-cable-driven parallel manipulator to achieve end-force output in a low-gravity environment. End-force output refers to the vector sum of the external force on the end-effector. A model of end-force output is established based on a kinematics model, a dynamic model, and a force analysis of an 8-cable driven parallel manipulator. To obtain end-force output in a low-gravity environment, the cable force has to be controlled to counteract gravity. In addition, a force-position mix control strategy is proposed to proactively control the cable force according to the force optimal distribution given by the closed-form force distribution method. Furthermore, a suitable choice for an end-force output is obtained by modeling the effect of cable force on end-force output. Experimental results show that the actual cable force agrees well with the calculated force distribution, indicating that it is feasible to realize end-force output in a low gravity environment.																	1476-8186	1751-8520				JUN	2020	17	3					378	389		10.1007/s11633-019-1195-6													
J								Item Ownership Relationship Semantic Learning Strategy for Personalized Service Robot	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Service robot; ownership relationship; convolutional neural network; human-object spatial relationship; personalized service		In order to satisfy the robotic personalized service requirements that can select exclusive items to perform inference and planning according to different service individuals, the service robots need to have the ability to independently obtain the ownership relationship between humans and their carrying items. In this work, we present a novel semantic learning strategy for item ownership. Firstly, a human-carrying-items detection network based on human posture estimation and object detection model is used. Then, the transferred convolutional neural network is used to extract the characteristics of the objects and the back-end classifier to recognize the object instance. At the same time, the face detection and recognition model are used to identify the service individual. Finally, on the basis of the former two, the active learning of ownership items is completed. The experimental results show that the proposed ownership semantic learning strategy can determine the ownership relationship of private goods accurately and efficiently. The solution of this problem can improve the intelligence level of robot life service.																	1476-8186	1751-8520				JUN	2020	17	3					390	402		10.1007/s11633-019-1206-7													
J								Hybrid Dynamic Neural Network and PID Control of Pneumatic Artificial Muscle Using the PSO Algorithm	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Dynamic neural network (DNN) control; hybrid control; pneumatic muscle; particle swarm optimization; sliding mode control	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; TRACKING; DRIVEN; DESIGN; SYSTEM	Pneumatic artificial muscles (PAM) have been recently considered as a prominent challenge regarding pneumatic actuators specifically for rehabilitation and medical applications. Since accomplishing accurate control of the PAM is comparatively complicated due to time-varying behavior, elasticity and ambiguous characteristics, a high performance and efficient control approach should be adopted. Besides of the mentioned challenges, limited course length is another predicament with the PAM control. In this regard, this paper proposes a new hybrid dynamic neural network (DNN) and proportional integral derivative (PID) controller for the position of the PAM. In order to enhance the proficiency of the controller, the problem under study is designed in the form of an optimization trend. Considering the potential of particle swarm optimization, it has been applied to optimally tune the PID-DNN parameters. To verify the performance of the proposed controller, it has been implemented on a real-time system and compared to a conventional sliding mode controller. Simulation and experimental results show the effectiveness of the proposed controller in tracking the reference signals in the entire course of the PAM.																	1476-8186	1751-8520				JUN	2020	17	3					428	438		10.1007/s11633-019-1196-5													
J								Low-latency Data Gathering with Reliability Guaranteeing in Heterogeneous Wireless Sensor Networks	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Heterogeneous wireless sensor networks (HWSNs); data gathering tree; multi-channel; power assignment; link scheduling	DATA-COLLECTION; ALGORITHM; PROTOCOLS	In order to achieve low-latency and high-reliability data gathering in heterogeneous wireless sensor networks (HWSNs), the problem of multi-channel-based data gathering with minimum latency (MCDGML), which associates with construction of data gathering trees, channel allocation, power assignment of nodes and link scheduling, is formulated as an optimization problem in this paper. Then, the optimization problem is proved to be NP-hard. To make the problem tractable, firstly, a multi-channel-based low-latency (MCLL) algorithm that constructs data gathering trees is proposed by optimizing the topology of nodes. Secondly, a maximum links scheduling (MLS) algorithm is proposed to further reduce the latency of data gathering, which ensures that the signal to interference plus noise ratio (SINR) of all scheduled links is not less than a certain threshold to guarantee the reliability of links. In addition, considering the interruption problem of data gathering caused by dead nodes or failed links, a robust mechanism is proposed by selecting certain assistant nodes based on the defined one-hop weight. A number of simulation results show that our algorithms can achieve a lower data gathering latency than some comparable data gathering algorithms while guaranteeing the reliability of links, and a higher packet arrival rate at the sink node can be achieved when the proposed algorithms are performed with the robust mechanism.																	1476-8186	1751-8520				JUN	2020	17	3					439	452		10.1007/s11633-017-1074-y													
J								Performance Improvement of Discrete-time Linear Control Systems Subject to Varying Sampling Rates Using the Tikhonov Regularization Method	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Discrete-time switched systems; Tikhonov regularization; varying sampling rates; discrete-time control systems; networked control systems	STOCHASTIC-SYSTEMS; PARAMETER CHOICE; STABILITY	Methods to stabilize discrete-time linear control systems subject to variable sampling rates, i.e., using state feedback controllers, are well known in the literature. Several recent works address the use of the Tikhonov regularization method, originally designed to attenuate the noise effects on ill-posed problems, with the aim of improving performance and stabilizing approximately controllable dynamical systems. Inspired by these works, we propose the use of a feedback controller designed using the Tikhonov method to regularize discrete-time linear systems subject to varying sampling rates. The goal is to minimize an error function, thus improving the performance of the closed loop system and reducing the possibility of instability. Illustrative examples show the effectiveness of the proposed method.																	1476-8186	1751-8520				JUN	2020	17	3					453	463		10.1007/s11633-019-1205-8													
J								A Practical Approach to Representation of Real-time Building Control Applications in Simulation	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Distributed dynamic simulation; networked control systems; building performance applications; smart buildings; building automation and control systems (BACS) architecture		Computer based automation and control systems are becoming increasingly important in smart sustainable buildings, of- ten referred to as automated buildings (ABs), in order to automatically control, optimize and supervise a wide range of building performance applications over a network while minimizing energy consumption and associated green house gas emission. This technology generally refers to building automation and control systems (BACS) architecture. Instead of costly and time-consuming experiments, this paper focuses on development and design of a distributed dynamic simulation environment with the capability to represent BACS architecture in simulation by run-time coupling two or more different software tools over a network. This involves using distributed dynamic simulations as means to analyze the performance and enhance networked real-time control systems in ABs and improve the functions of real BACS technology. The application and capability of this new dynamic simulation environment are demonstrated by an experimental design, in this paper.																	1476-8186	1751-8520				JUN	2020	17	3					464	478		10.1007/s11633-018-1131-1													
J								On Relation Between Linear Temporal Logic and Quantum Finite Automata	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Quantum finite automata; Linear temporal logic; Measure-once one-way quantum finite automata; Latvian quantum finite automata; Measure-many one-way quantum finite automata	COMPLEXITY	Linear temporal logic is a widely used method for verification of model checking and expressing the system specifications. The relationship between theory of automata and logic had a great influence in the computer science. Investigation of the relationship between quantum finite automata and linear temporal logic is a natural goal. In this paper, we present a construction of quantum finite automata on finite words from linear-time temporal logic formulas. Further, the relation between quantum finite automata and linear temporal logic is explored in terms of language recognition and acceptance probability. We have shown that the class of languages accepted by quantum finite automata are definable in linear temporal logic, except for measure-once one-way quantum finite automata.																	0925-8531	1572-9583				JUN	2020	29	2					109	120		10.1007/s10849-019-09302-6													
J								Truth Diagrams Versus Extant Notations for Propositional Logic	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Diagrams; Notations; Propositional logic; Sentential calculus; Truth diagrams; Formula notation; Truth-tables; Frege conceptual notation; Wittgenstein Tractatus; Venn diagrams; Gardner shuttle networks; Pierce existential graphs	REPRESENTATIONS	Truth diagrams (TDs) are introduced as a novel graphical representation for propositional logic (PL). To demonstrate their epistemic efficacy a set of 28 concepts are proposed that any comprehensive representation for PL should encompass. TDs address all the criteria whereas seven other existing representations for PL only provide partial coverage. These existing representations are: the linear formula notation, truth tables, a PL specific interpretation of Venn Diagrams, Frege's conceptual notation, diagrams from Wittgenstein's Tractatus, Pierce's alpha graphs and Gardner's shuttle diagrams. The comparison of the representations succeeds in distinguishing ideas that are fundamental to PL from features of common PL representations that are somewhat arbitrary.																	0925-8531	1572-9583				JUN	2020	29	2					121	161		10.1007/s10849-019-09299-y													
J								A Revised Projectivity Calculus for Inclusion and Exclusion Reasoning	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Inclusion; Exclusion; Opposition properties; Projectivity signatures; Natural Logic		We present a Revised Projectivity Calculus (denoted RC) that extends the scope of inclusion and exclusion inferences derivable under the Projectivity Calculus (denoted C) developed by Icard (Stud Log 100(4):705-725, 2012). After pointing out the inadequacies of C, we introduce four opposition properties (OPs) which have been studied by Chow (in: Aloni et al (eds) Proceedings of the 18th Amsterdam Colloquium, Springer, Berlin, 2012; Beziau, Georgiorgakis (eds) New dimensions of the square of opposition, Philosophia Verlag GmbH, Munchen, 2017) and are more appropriate for the study of exclusion reasoning. Together with the monotonicity properties (MPs), the OPs will form the basis of RC instead of the additive/multiplicative properties used in C. We also prove some important results of the OPs and their relation with the MPs. We then introduce a set of projectivity signatures together with the associated operations and conditions for valid inferences, and develop RC by inheriting the key features of C. We then show that under RC, we can derive some inferences that are not derivable under C. We finally discuss some properties of RC and point to possible directions of further studies.																	0925-8531	1572-9583				JUN	2020	29	2					163	195		10.1007/s10849-019-09292-5													
J								Confused Terms in Ordinary Language	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Proper names; Empty names; Ambiguity; Confusion; Supervaluation; Experimental logic	REALISM BIT; PHILOSOPHY; INTUITIONS; SEMANTICS; NAMES	Confused terms appear to signify more than one entity. Carnap (Meaning and necessity, University of Chicago Press, Chicago, 1956) maintained that any putative name that is associated with more than one object in a relevant universe of discourse fails to be a genuine name. Although many philosophers have agreed with Carnap, they have not always agreed among themselves about the truth-values of atomic sentences containing such terms. Some hold that such atomic sentences are always false, and others claim they are always truth-valueless. Field (J Philos 70:462-481, 1973) maintained that confused terms can still refer, albeit partially, and offered a supervaluational account of their semantic properties on which some atomic sentences with confused terms can be true. After outlining many of the most important theoretical considerations for and against various semantic theories for such terms, we report the results of a study designed to investigate which of these accounts best accords with the truth-value judgments of ordinary language users about sentences containing these terms. We found that naive participants view confused names as capable of successfully referring to one or more objects. Thus, semantic theories that judge them to involve total reference failure do not comport well with patterns of ordinary usage.																	0925-8531	1572-9583				JUN	2020	29	2					197	219		10.1007/s10849-019-09300-8													
J								Formal Semantics and Applied Mathematics: An Inferential Account	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Formal semantics; Applied mathematics; Scientific modelling; Philosophy of linguistics	LANGUAGE; CONCEPTION	In this paper, I utilise the growing literature on scientific modelling to investigate the nature of formal semantics from the perspective of the philosophy of science. Specifically, I incorporate the inferential framework proposed by Bueno and Colyvan (Nous 45(2): 345-374, 2011) in the philosophy of applied mathematics to offer an account of how formal semantics explains and models its data. This view produces a picture of formal semantic models as involving an embedded process of inference and representation applying indirectly to linguistic phenomena. The final aim of the paper is directed at proposing a novel account of the syntax-semantics interface while shedding light on empty categories, semantically null forms, underspecified content and compositionality as a whole.																	0925-8531	1572-9583				JUN	2020	29	2					221	253		10.1007/s10849-019-09298-z													
J								Residual Contraction	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Belief change; Belief bases; Contraction; Axiomatic characterizations; Residual contractions; Residuums	ENSCONCEMENT	In this paper, we propose and axiomatically characterize residual contractions, a new kind of contraction operators for belief bases. We establish that the class of partial meet contractions is a strict subclass of the class of residual contractions. We identify an extra condition that may be added to the definition of residual contractions, which is such that the class of residual contractions that satisfy it coincides with the class of partial meet contractions. We investigate the interrelations in the sense of (strict) inclusion among the class of residual contractions and other classes of well known contraction operators for belief bases.																	0925-8531	1572-9583				JUN	2020	29	2					255	274		10.1007/s10849-019-09296-1													
J								Cross-domain aspect/sentiment-aware abstractive review summarization by combining topic modeling and deep reinforcement learning	NEURAL COMPUTING & APPLICATIONS										Domain adaptation; Abstractive review summarization; Reinforcement learning; Weakly supervised LDA	SPARSE REPRESENTATION; MULTITASK	Review text has been widely studied in traditional tasks such as sentiment analysis and aspect extraction. However, to date, no work is toward the end-to-end abstractive review summarization that is essential for business organizations and individual consumers to make informed decisions. This study takes the lead to study the aspect/sentiment-aware abstractive review summarization in domain adaptation scenario. Our novel model Abstractive review Summarization with Topic modeling and Reinforcement deep learning (ASTR) leverages the benefits of the supervised deep neural networks, reinforcement learning, and unsupervised probabilistic generative model to strengthen the aspect/sentiment-aware review representation learning. ASTR is a multi-task learning system, which simultaneously optimizes two coupled objectives: domain classification (auxiliary task) and abstractive review summarization (primary task), in which a document modeling module is shared across tasks. The main purpose of our multi-task model is to strengthen the representation learning of documents and safeguard the performance of cross-domain abstractive review summarization. Specifically, ASTR consists of two key components: (1) a domain classifier, working on datasets of both source and target domains to recognize the domain information of texts and transfer knowledge from the source domain to the target domain. In particular, we propose a weakly supervised LDA model to learn the domain-specific aspect and sentiment lexicon representations, which are then fed into the neural hidden states of given reviews to form aspect/sentiment-aware review representations; (2) an abstractive review summarizer, sharing the document modeling module with the domain classifier. The learned aspect/lexicon-aware review representations are fed into a pointer-generator network to generate aspect/sentiment-aware abstractive summaries of given reviews by employing a reinforcement learning algorithm. We conduct extensive experiments on real-life Amazon reviews to evaluate the effectiveness of our model. Quantitatively, ASTR achieves better performance than the state-of-the-art summarization methods in terms of ROUGE score and human evaluation in both out-of-domain and in-domain setups. Qualitatively, our model can generate better sentiment-aware summarization for reviews with different categories and aspects.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6421	6433		10.1007/s00521-018-3825-2													
J								Mutual kNN based spectral clustering	NEURAL COMPUTING & APPLICATIONS										Mutual kNN; Affinity matrix; Spectral clustering; Standardized processing; Normalization	SPARSE; CLASSIFICATION; INFORMATION; REGRESSION; ALGORITHM; GRAPH	The key step of spectral clustering is learning the affinity matrix to measure the similarity among data points. This paper proposes a new spectral clustering method, which uses mutual k nearest neighbor to obtain the affinity matrix by removing the influence of noise. Then, the characteristics of high-dimensional data are self-represented to ensure local important information of data by using affinity matrix in standardized processing. Furthermore, we also use the normalization method to further improve the performance of clustering. Experimental analysis on eight benchmark data sets showed that our proposed method outperformed the state-of-the-art clustering methods in terms of clustering performance such as cluster accuracy and normalized mutual information.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6435	6442		10.1007/s00521-018-3836-z													
J								Unsupervised nonlinear feature selection algorithm via kernel function	NEURAL COMPUTING & APPLICATIONS										Feature selection; Kernel function; Sparse regularization factor		Feature selection is one of the important methods of data preprocessing, but the general feature selection algorithm has the following shortcomings: (1) Noise and outliers cannot be ruled out so that the algorithm does not work well. (2) They only consider the linear relationship between data without considering the nonlinear relationship between data. For this reason, an unsupervised nonlinear feature selection algorithm via kernel function is proposed in this paper. First, each data feature is mapped to a kernel space by a kernel function. In this way, nonlinear feature selection can be performed. Secondly, the low-rank processing of the kernel coefficient matrix is used to eliminate the interference of noise samples. Finally, the feature selection is performed through a sparse regularization factor in the kernel space. Experimental results show that our algorithm has better results than contrast algorithms.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6443	6454		10.1007/s00521-018-3853-y													
J								Single-image low-light enhancement via generating and fusing multiple sources	NEURAL COMPUTING & APPLICATIONS										Low-light image; Image enhancement; Fusion	CONTRAST ENHANCEMENT; HISTOGRAM EQUALIZATION; DETAIL ENHANCEMENT; PHOTOGRAPH	Imperfect lightness conditions usually lower the visual quality of an image by bringing in unclear image details and poor image contrast. Traditional low-light enhancement models based on one single input are often limited in avoiding the effect of over-enhancement or under-enhancement. Models based on fusing multiple input sources usually perform well in relieving this issue, as they can harmonize the complementary visual appearances of a same scene provided by different sources. Nevertheless, these models still have difficulty in dealing with the situation that only one input is at hand, which usually happens in many practical situations. In this paper, we propose a low-light enhancement model that artificially enriches input sources and then seamlessly fuses them. Specifically, with an input image, we first generate multiple enhanced images based on a lightness-aware camera response model. These images are then fused at mid-level based on a patch-based image decomposition model. To validate our model, we conduct qualitative and quantitative comparisons with several state-of-the-art single-source and multi-source models on a collection of real-world images. Experimental results show that our model better improves the image quality in terms of visual naturalness and aesthetics.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6455	6465		10.1007/s00521-018-3893-3													
J								Multi-task learning using a hybrid representation for text classification	NEURAL COMPUTING & APPLICATIONS										Text classification; Deep learning; Multi-task learning; Feature representation; LSTM; CNN; Big data	CONVOLUTIONAL NEURAL-NETWORK; SENTIMENT CLASSIFICATION; EXTRACTION	Text classification is an important task in machine learning. Specifically, deep neural network has been shown strong capability to improve performance in different fields, for example speech recognition, objects recognition and natural language processing. However, in most previous work, the extracted feature models do not achieve the relative text tasks well. To address this issue, we introduce a novel multi-task learning approach called a hybrid representation-learning network for text classification tasks. Our method consists of two network components: a bidirectional gated recurrent unit with attention network module and a convolutional neural network module. In particular, the attention module allows for the task learning private feature representation in local dependence from training texts and that the convolutional neural network module can learn the global representation on sharing. Experiments on 16 subsets of Amazon review data show that our method outperforms several baselines and also proves the effectiveness of joint learning multi-relative tasks.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6467	6480		10.1007/s00521-018-3934-y													
J								Big data analytics for MOOC video watching behavior based on Spark	NEURAL COMPUTING & APPLICATIONS										MOOC; Big data; Video watching behavior; Spark		The purpose of this study is to measure the effectiveness of courses delivered using MOOCs in China Agricultural University. Video watching is considered to be the most important way to disseminate knowledge in Massive Open Online Course (MOOC). Its mission is to understand the degree of students' learning engagement and to provide suggestions for teachers to construct courses. This paper proposes the analysis methods of students' video watching behavior in MOOCs platform and verifies it with the data of the cauX platform. Initially, a detailed statistical analysis of video watching data and behavior was performed. Later, data preprocessing algorithms based on Spark platform were developed and used to calculate the number of video watching behaviors in every hour and every minute. Then, the entropy weight method was used to calculate the weight of pause video, seek video and speed change video. Finally, we analyze and discuss the results of experiment. The results show that the proposed method based on Spark platform can quickly and accurately analyze the characteristics of video watching behavior.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6481	6489		10.1007/s00521-018-03983-z													
J								A semi-structured information semantic annotation method for Web pages	NEURAL COMPUTING & APPLICATIONS										Semantic annotation; Semi-structured information; Back-propagation neural network; Domain description model	BAYESIAN NETWORK; NEURAL-NETWORK; CLASSIFICATION; EXTRACTION	There is a large amount of semi-structured information on Web pages. Comprehensive and accurate annotation of Web page information with uniform semantics can enhance the use value of information and provide support for Web site information integration. According to the characteristics of semi-structured information on Web pages, a semantic annotation method based on header recognition and data item classification is proposed. Firstly, a description model is constructed for the domain to be annotated. Secondly, header recognition is used to annotate data items on extracted pages. For those data items fail to be annotated by header recognition, feature vectors are constructed based on the feature sets in the domain description model and semantics of those data items are annotated by the classification results of back-propagation neural network. The proposed method is tested on 19,657 data items in the domain of agricultural product price and 8089 data items in the domain of recruitment information. The annotation precision is 97.39% and 95.67% respectively, and the annotation recall is 95.41% and 95.67%, respectively. These results show that the proposed method can annotate semi-structured information on Web pages accurately and completely.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6491	6501		10.1007/s00521-018-03999-5													
J								Adaptive graph learning and low-rank constraint for supervised spectral feature selection	NEURAL COMPUTING & APPLICATIONS										Graph learning; Low-rank constraint; Spectral feature selection	UNSUPERVISED FEATURE-SELECTION; DIMENSIONALITY; REGRESSION	Spectral feature selection (SFS) effectively improves performance of feature selection by introducing a graph matrix to preserve information of data. However, conventional SFS (1) generally preserves either global structure or local structure of data in selected subset, which is not capable of providing comprehensive information for model to output a robust result; (2) constructs graph matrix by original data, which usually lead to a suboptimal graph matrix because of redundant information; (3) conducts feature selection task depending on the fixed graph matrix, which is easily trapped in local optimization. Thus, we have proposed a novel SFS to (1) preserve both local information and global information of original data in feature-selected subset to provide comprehensive information for learning model; (2) integrate graph construction and feature selection to propose a robust spectral feature selection easily obtaining global optimization of feature selection. Besides, for the proposed problem, we further provide a optimization algorithm to effectively tackle the problem with a fast convergence. The extensive experimental results showed that our proposed method outperforms state-of-the-art feature selection methods, in terms of classification performance.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6503	6512		10.1007/s00521-018-04006-7													
J								Sparsity-regularized feature selection for multi-class remote sensing image classification	NEURAL COMPUTING & APPLICATIONS										Feature extraction; Bag of features; Sparsity-regularized feature selection; Remote sensing image classification	BAG	Remote sensing image classification plays an important role in a wide range of applications and has caused widely concerns. During the last few years, great efforts have been made to develop a number of scene classification methods for remote sensing images. However, the existing remote sensing image classification methods do not perform satisfactorily in dealing with multi-class classification problems and rely heavily on the quality of data sets. These disadvantages seriously restrict the application of remote sensing image, including industrial research, analysis and calculation of land use and land coverage. To this end, this paper proposes a remote sensing image classification algorithm based on the sparse regularized feature learning method. Specifically, after constructing bag of features by using speeded up robust features extraction algorithm, direct sparsity optimization-based feature selection method is applied for selecting discriminative features, which is used for constructing support vector machine classifier model. The proposed algorithm has been evaluated and compared with other advanced feature selection methods on four public remote sensing image data sets. The experimental results demonstrate the effectiveness of our proposed image classification algorithm, which has been successfully applied to remote sensing image classification tasks.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6513	6521		10.1007/s00521-019-04046-7													
J								Inferring region significance by using multi-source spatial data	NEURAL COMPUTING & APPLICATIONS										Region; Trajectory; Density; Recommendation; Ranking; Spatial data mining		The ranking and recommendation of regions of interest are increasingly important in recent years. In this light, we propose and study a novel and interesting problem of inferring region significance using multi-source spatiotemporal data. In our study, POIs, locations, regions, trajectories, and spatial networks are taken into account. Given a set of regions R and a set of trajectories T, we seek for the top-k most attractive regions to users, i.e., regions with the top-k highest spatial-density correlations to the trajectories of travelers. This study is useful in many mobile applications such as urban computing, region recommendation, and location-based service in general. This problem is challenging due to two reasons: (1) how to model the spatial-density correlation effectively and practically and (2) how to process the problem in interactive time. To overcome the challenges, we design a novel spatial-density correlation function to evaluate the relationship between regions and trajectories, and the density of POIs and network distance are taken into account. Then, we develop a series of optimization techniques to accelerate the query efficiency. Furthermore, we develop a parallel mechanism to support big spatial data. Finally, we conduct extensive experiments on real and synthetic spatial data sets to show the efficiency and effectiveness of developed algorithms.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6523	6531		10.1007/s00521-019-04070-7													
J								Traffic sign detection and recognition based on pyramidal convolutional networks	NEURAL COMPUTING & APPLICATIONS										Traffic sign; Object detection; Feature pyramid		With the development of driverless technology, we are in dire need of a method to understand traffic scenes. However, it is still a difficult task to detect traffic signs because of the tiny scale of signs in real-world images. In complex scenarios, some traffic signs could be very elusive due to the awful weather and lighting conditions. To implement a more comprehensive detection and recognition system, we develop a two-stage network. At the region proposal stage, we adopt a deep feature pyramid architecture with lateral connections, which makes the semantic feature of small object more sensitive. At the classification stage, densely connected convolutional network is used to strengthen the feature transmission and multiplexed, which leads to more accurate classification with less number of parameters. We test on GTSDB detection benchmark, as well as the challenging Tsinghua-Tencent 100K benchmark which is pretty difficult for most traditional networks. Experiments show that our proposed method achieves a very great performance and surpasses the other state-of-the-art methods. Implementation source code is available at https://github.com/derderking/Traffic-Sign.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6533	6543		10.1007/s00521-019-04086-z													
J								Deep convolutional neural network for automatically segmenting acute ischemic stroke lesion in multi-modality MRI	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Acute ischemic stroke; Residual unit; Multi-modality; Lesion segmentation	MULTIPLE-SCLEROSIS; SEGMENTATION; CLASSIFICATION; IMAGES; VOLUME	Correct segmentation of stroke lesions from magnetic resonance imaging (MRI) is crucial for neurologists and patients. However, manual segmentation relies on expert experience and is time-consuming. The complicated stroke evolution phase and the limited samples pose challenges for automatic segmentation. In this study, we propose a novel deep convolutional neural network (Res-CNN) to automatically segment acute ischemic stroke lesions from multi-modality MRIs. Our network draws on U-shape structure, and we embed residual unit into network. In Res-CNN, we use residual unit to alleviate the degradation problem and use multi-modality to exploit the complementary information in MRIs. Before training the model, we use data fusion and data augmentation methods to increase the number of training images. Seven neural networks are extensively evaluated on two acute ischemic stroke datasets. Res-CNN shows good performance compared with other six networks both in single modality and multi-modality. Furthermore, compared with the gold standard segmentation manually labeled by two neurologists on a local test dataset, our network achieves the best results in seven neural networks. The average Dice coefficient and Hausdorff distance of our method are 74.20% and 2.33 mm, respectively. Our proposed network may provide a useful tool for segmentation lesion of acute ischemic stroke.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6545	6558		10.1007/s00521-019-04096-x													
J								A weighted KNN-based automatic image annotation method	NEURAL COMPUTING & APPLICATIONS										Image annotation; CNN; Multi-label linear discriminant; Semantic extension		Automatic image annotation becomes a hot research area because of its efficiency on shrinking the semantic gap between images and their semantic meanings. We present a model referred as weight-KNN which firstly introduces the CNN feature to address the problem that traditional models only work well with well-designed manual feature representations. Additionally, in order to employ the simplicity and generality of the KNN-based model for annotation, the proposed model incorporates a multi-label linear discriminant approach to compute the weighting which improves the accuracy in the subsequent procedures of distance calculation. Moreover, we take the advantage of the KNN-based model to acquire the test image's k-nearest neighbors in each label category and get the prediction of the image according to the contribution of its neighbors. At last, the experiments are performed on three typical image data sets, corel 5k, esp game and laprtc12, which verify the effectiveness of the proposed model.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6559	6570		10.1007/s00521-019-04114-y													
J								Stability property of impulsive inertial neural networks with unbounded time delay and saturating actuators	NEURAL COMPUTING & APPLICATIONS										Stability property; Unbounded delay; Impulsive effect; Inertial neural networks; Saturating actuators	GLOBAL ASYMPTOTIC STABILITY; EXPONENTIAL STABILITY; SYNCHRONIZATION; SYSTEMS; STABILIZATION; PERIODICITY	This paper considers the stability property of impulsive inertial neural networks with unbounded delay and saturating actuators. Based on polytopic representation approach, some sufficient conditions to ensure global asymptotic stability are obtained for impulsive inertial neural networks. By using Lyapunov function with the matrix form of 2-norm, we obtain some conditions to ensure the stability of impulsive inertial neural networks. Finally, the validity of this method is verified by several simulation examples.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6571	6580		10.1007/s00521-019-04115-x													
J								Unsupervised feature selection based on joint spectral learning and general sparse regression	NEURAL COMPUTING & APPLICATIONS										Spectral selection; General sparse regression; Unsupervised feature selection	FRAMEWORK	Unsupervised feature selection is an important machine learning task since the manual annotated data are dramatically expensive to obtain and therefore very limited. However, due to the existence of noise and outliers in different data samples, feature selection without the help of discriminant information embedded in the annotated data is quite challenging. To relieve these limitations, we investigate the embedding of spectral learning into a general sparse regression framework for unsupervised feature selection. Generally, the proposed general spectral sparse regression (GSSR) method handles the outlier features by learning the joint sparsity and the noisy features by preserving the local structures of data, jointly. Specifically, GSSR is conducted in two stages. First, the classic sparse dictionary learning method is used to build the bases of original data. After that, the original data are project to the basis space by learning a new representation via GSSR. In GSSR, robust loss function l2,r-norm(0<r <= 2) and l2,p-norm(0<p <= 1) instead of the traditional F norm and least square loss function are simultaneously considered as the reconstruction term and sparse regularization term for sparse regression. Furthermore, the local topological structures of the new representations are preserved by spectral learning based on the Laplacian term. The overall objective function in GSSR is optimized and proved to be converging. Experimental results on several publicly datasets have demonstrated the validity of our algorithm, which outperformed the state-of-the-art feature selections in terms of classification performance.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6581	6589		10.1007/s00521-019-04117-9													
J								Color-depth multi-task learning for object detection in haze	NEURAL COMPUTING & APPLICATIONS										Multi-task learning; Object detection; Haze environment; Depth feature	MOVING-OBJECTS; UNDERWATER; RESTORATION; TRACKING; WEATHER; IMAGES	Haze environments pose serious challenges for object detection, making existing methods difficult to generate satisfied results. However, there is no escape from haze environments in real-world applications, especially in water and bad weather. Hence, it is necessary to enable object detection methods to conquer the difficulties caused by the haze effect. In spite of the diversity between various conditions, haze environments share a common characteristic that the haze concentration is changed with the scene depth. Hence, this haze concentration feature can be used as a representation of the scene depth. This provides us a novel cue available for object detection in haze that the object-background depth contrast can be identified. In this paper, we propose a multi-task learning-based object detection method by jointly using the color and depth features. A pair of background models is built separately with the color and depth features, forming two streams of our multi-task learning framework. The final object detection results are generated by fusing the results given by color and depth features. In contrast to existing object detection methods, the novelty of our method lies in the combination of the color and depth features under a unified multi-task learning mechanism, which is experimentally demonstrated to be robust against challenging haze environments.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6591	6599		10.1007/s00521-018-3732-6													
J								EEG classification using sparse Bayesian extreme learning machine for brain-computer interface	NEURAL COMPUTING & APPLICATIONS										Brain-computer interface; Electroencephalogram; Motor imagery; Extreme learning machine; Sparse Bayesian learning	CANONICAL CORRELATION-ANALYSIS; MOTOR-IMAGERY; REGRESSION; SIGNALS; TASKS	Mu rhythm is a spontaneous neural response occurring during a motor imagery (MI) task and has been increasingly applied to the design of brain-computer interface (BCI). Accurate classification of MI is usually rather difficult to be achieved since mu rhythm is very weak and likely to be contaminated by other background noises. As an extension of the single layer feedforward network, extreme learning machine (ELM) has recently proven to be more efficient than support vector machine that is a benchmark for MI-related EEG classification. With probabilistic inference, this study introduces a sparse Bayesian ELM (SBELM)-based algorithm to improve the classification performance of MI. SBELM is able to automatically control the model complexity and exclude redundant hidden neurons by combining advantageous of both ELM and sparse Bayesian learning. The effectiveness of SBELM for MI-related EEG classification is validated on a public dataset from BCI Competition IV IIb in comparison with several other competing algorithms. Superior classification accuracy confirms that the proposed SBELM-based algorithm is a promising candidate for performance improvement of an MI BCI.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6601	6609		10.1007/s00521-018-3735-3													
J								Spectral clustering algorithm combining local covariance matrix with normalization	NEURAL COMPUTING & APPLICATIONS										Spectral clustering; Local covariance; Affinity matrix learning; Clusters intersection	UNSUPERVISED FEATURE-SELECTION	Affinity matrix construction is a key step in the spectral clustering. However, traditional spectral clustering methods usually ignore the intersection problem that may exist between the different clusters of data, so the resulting matrix could be unreliable. This paper proposes a new local covariance-based method to solve the above problem. Specifically, we first learn an initial affinity matrix by adding the local covariance into traditional matrix construction step, which could guarantee the obtained matrix avoids the impact of the intersection point while preserving the neighborhood relationship of data. We then employ the normalized Laplacian on the obtained matrix to further improve the clustering performance. The ACC and NMI of the proposed method increased by 6.40% and 5.33% on average compared with six classical spectral clustering methods. Experimental evaluation on eight benchmark data sets shows that the proposed method has better clustering performance.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6611	6618		10.1007/s00521-018-3852-z													
J								Minimal weighted infrequent itemset mining-based outlier detection approach on uncertain data stream	NEURAL COMPUTING & APPLICATIONS										Minimal infrequent itemset mining; Outlier detection; Uncertain weighted data stream; Deviation index	FREQUENT ITEMSETS; EFFICIENT; PATTERNS; ALGORITHMS; DATABASES	Outliers are a critical factor that affects the accuracy of data-based predictions and some other data-based processing; thus, outliers must be effectively detected as soon as possible to improve the credibility of the data. In recent years, massive outlier detection approaches have been proposed for static data and precise data; however, the uncertainty and weight information of each item was not considered in this prior work. Moreover, traditional outlier detection approaches only take the deviation degree of each data element as the standard for determining outliers; therefore, the detected outliers do not fit the definition of an outlier (i.e., rarely appearing and different from most of the other data). Aimed at these problems, a minimal weighted infrequent itemset mining-based outlier detection approach that can be applied to an uncertain data stream, called MWIFIM-OD-UDS, is proposed in this paper to effectively detect implicit outliers, which have a rarely occurring frequency, uncertainty and a certain weight of the itemset, while the characteristics of the data stream are considered. In particular, a matrix structure-based approach that is called MWIFIM-UDS is proposed to mine the minimal weighted infrequent itemsets (MWiFIs) from an uncertain data stream, and then, the MWIFIM-OD-UDS method is proposed based on the mined MWiFIs and the designed deviation indexes. Experimental results show that the proposed MWIFIM-OD-UDS method outperforms the frequent itemset mining-based outlier detection methods, FindFPOF and LFP, in terms of its runtime and detection accuracy.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6619	6639		10.1007/s00521-018-3876-4													
J								Operational neural networks	NEURAL COMPUTING & APPLICATIONS										Operational neural network; Heterogeneous and nonlinear neural networks; Convolutional neural networks	NEURONAL DIVERSITY	Feed-forward, fully connected artificial neural networks or the so-called multi-layer perceptrons are well-known universal approximators. However, their learning performance varies significantly depending on the function or the solution space that they attempt to approximate. This is mainly because of their homogenous configuration based solely on the linear neuron model. Therefore, while they learn very well those problems with a monotonous, relatively simple and linearly separable solution space, they may entirely fail to do so when the solution space is highly nonlinear and complex. Sharing the same linear neuron model with two additional constraints (local connections and weight sharing), this is also true for the conventional convolutional neural networks (CNNs) and it is, therefore, not surprising that in many challenging problems only the deep CNNs with a massive complexity and depth can achieve the required diversity and the learning performance. In order to address this drawback and also to accomplish a more generalized model over the convolutional neurons, this study proposes a novel network model, called operational neural networks (ONNs), which can be heterogeneous and encapsulate neurons with any set of operators to boost diversity and to learn highly complex and multi-modal functions or spaces with minimal network complexity and training data. Finally, the training method to back-propagate the error through the operational layers of ONNs is formulated. Experimental results over highly challenging problems demonstrate the superior learning capabilities of ONNs even with few neurons and hidden layers.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6645	6668		10.1007/s00521-020-04780-3													
J								Applying depthwise separable and multi-channel convolutional neural networks of varied kernel size on semantic trajectories	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Depthwise separable convolution; Multi-channel convolution; Semantic trajectories; Location prediction	SEQUENCES; LOCATIONS; GPS	Convolutional neural networks (CNN) have become due to their outstanding performance in the past few years rapidly the standard approach when it comes to processing 2D data as these can be found in the image recognition and classification domain. Recent research shows that CNN models can handle 1D data, such as temporal sequences (e.g., speech and text), with a similar high performance as well. This fact motivated our present idea to apply convolutional networks for modeling human semantic trajectories and predicting future locations. Our work consists of three parts. The first part evaluates the performance of a standard spatial CNN in comparison with a vanilla feed-forward, a recurrent and a long short-term memory network (LSTM) at two different semantic representation levels. In the second part, we explore in depth the impact of the kernel size and propose a multi-channel convolutional approach based on kernels of varied size. Finally, part three investigates the depthwise factorization of the convolutional layer with regard to training time and test accuracy. Altogether, it can be shown that convolutional networks are able to outperform the competition, with the channel number as well as the kernel size being the most significant hyperparameters.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6685	6698		10.1007/s00521-019-04603-0													
J								Identifying data streams anomalies by evolving spiking restricted Boltzmann machines	NEURAL COMPUTING & APPLICATIONS										Big Data; Data streams analysis; Evolving spiking neural networks; Restricted Boltzmann machines; Deep learning; Real-time anomaly detection	SYSTEM	Data streams are characterized by high volatility, and they drastically change in an unpredictable way over time. In the typical case, newer data are the most important, as the concept of aging is based on their timing. These flows require real-time processing in order to extract meaningful information that will allow for essential and targeted responses to changing circumstances. Knowledge mining is a real-time process performed on a subset of the data streams, which contains a small but recent part of the observations. Timely security requirements call for further quest of optimal approaches, capable of improving the reliability and the accuracy of the employed classifiers. This research introduces a real-time evolving spiking restricted Boltzmann machine approach, for efficient anomaly detection in data streams. Testing has proved that the proposed algorithm maximizes the classification accuracy and at the same time minimizes the computational resources requirements. A comparative analysis has shown that it outperforms other data flow analysis algorithms.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6699	6713		10.1007/s00521-019-04288-5													
J								Real-time diameter of the fetal aorta from ultrasound	NEURAL COMPUTING & APPLICATIONS											INTIMA-MEDIA THICKNESS; ARTERY; IMAGE	The automatic analysis of ultrasound sequences can substantially improve the efficiency of clinical diagnosis. This article presents an attempt to automate the challenging task of measuring the vascular diameter of the fetal abdominal aorta from ultrasound images. We propose a neural network architecture consisting of three blocks: a convolutional neural network (CNN) for the extraction of imaging features, a convolution gated recurrent unit (C-GRU) for exploiting the temporal redundancy of the signal, and a regularized loss function, called CyclicLoss, to impose our prior knowledge about the periodicity of the observed signal. The solution is investigated with a cohort of 25 ultrasound sequences acquired during the third-trimester pregnancy check, and with 1000 synthetic sequences. In the extraction of features, it is shown that a shallow CNN outperforms two other deep CNNs with both the real and synthetic cohorts, suggesting that echocardiographic features are optimally captured by a reduced number of CNN layers. The proposed architecture, working with the shallow CNN, reaches an accuracy substantially superior to previously reported methods, providing an average reduction of the mean squared error from 0.31 (state-of-the-art) to 0.09 mm2, and a relative error reduction from 8.1 to 5.3%. The mean execution speed of the proposed approach of 289 frames per second makes it suitable for real-time clinical use.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6735	6744		10.1007/s00521-019-04646-3													
J								Sparse coding predicts optic flow specificities of zebrafish pretectal neurons	NEURAL COMPUTING & APPLICATIONS										Optic flow; Sparse coding; Optimality; pretectum; Egomotion detection	FIELD; MOTION	Zebrafish pretectal neurons exhibit specificities for large-field optic flow patterns associated with rotatory or translatory body motion. We investigate the hypothesis that these specificities reflect the input statistics of natural optic flow. Realistic motion sequences were generated using computer graphics simulating self-motion in an underwater scene. Local retinal motion was estimated with a motion detector and encoded in four populations of directionally tuned retinal ganglion cells, represented as two signed input variables. This activity was then used as input into one of three learning networks: a sparse coding network (competitive learning), PCA whitening with subsequent sparse coding, and a backpropagation network (supervised learning). All simulations developed specificities for optic flow which are comparable to those found in a neurophysiological study (Kubo et al. in Neuron 81(6):1344-1359, 2016. 10.1016/j.neuron.2014.02.043), but relative frequencies of the various neuronal responses were best modeled by the sparse coding approach without whitening. We conclude that the optic flow neurons in the zebrafish pretectum do reflect the optic flow statistics. The predicted vectorial receptive fields show not only typical optic flow fields but also "Gabor" and dipole-shaped patterns that likely reflect difference fields needed for reconstruction by linear superposition.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6745	6754		10.1007/s00521-019-04500-6													
J								Modular domain-to-domain translation network	NEURAL COMPUTING & APPLICATIONS										Unsupervised learning; Auto-encoder; Feature mapping; Neural networks		Domain-to-domain translation methods map images from a source domain to corresponding images from a target domain. The two domains contain images from the same classes, but these images look different. Recent approaches use generative adversarial networks in various configurations and architectures to perform the translation. By using GANs, they inevitably inherit their problems like training instability and mode collapse. We propose a novel approach to the problem that does not use a GAN. Instead, it relies on an hierarchical architecture that encapsulates information of the target domain by using individually trained networks. This hierarchical architecture is then trained as one unified deep network. Using this approach, we show that images from the original domain are translated to the target domain both for the case when there is a one-to-one correspondence between the images of the two domains and for the case that such correspondence information is absent. We visualize and evaluate the translation from one information domain to the other and discuss the proposed model's relation to the conditional generative adversarial networks. We further argue that deep learning can benefit from the proposed hierarchical architecture.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6779	6791		10.1007/s00521-019-04358-8													
J								A deep learning classifier for sentence classification in biomedical and computer science abstracts	NEURAL COMPUTING & APPLICATIONS										Bidirectional gated recurrent unit; Abstract sentence classification; Deep learning; Crowdsourcing		The automatic classification of abstract sentences into its main elements (background, objectives, methods, results, conclusions) is a key tool to support scientific database querying, to summarize relevant literature works and to assist in the writing of new abstracts. In this paper, we propose a novel deep learning approach based on a convolutional layer and a bidirectional gated recurrent unit to classify sentences of abstracts. First, the proposed neural network was tested on a publicly available repository containing 20 thousand abstracts from the biomedical domain. Competitive results were achieved, with weight-averaged Precision, Recall and F1-score values around 91%, and an area under the ROC curve (AUC) of 99%, which are higher when compared to a state-of-the-art neural network. Then, a crowdsourcing approach using gamification was adopted to create a new comprehensive set of 4111 classified sentences from the computer science domain, focused on social media abstracts. The results of applying the same deep learning modeling technique trained with 3287 (80%) of the available sentences were below the ones obtained for the larger biomedical dataset, with weight-averaged Precision, Recall and F1-score values between 73 and 76%, and an AUC of 91%. Considering the dataset dimension as a likely important factor for such performance decrease, a data augmentation approach was further applied. This involved the use of text mining to translate sentences of the computer science abstract corpus while retaining the same meaning. Such approach resulted in slight improvements (around 2 percentage points) for the weight-averaged Recall and F1-score values.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6793	6807		10.1007/s00521-019-04334-2													
J								MCK-ELM: multiple composite kernel extreme learning machine for hyperspectral images	NEURAL COMPUTING & APPLICATIONS										Multiple kernel learning; Composite kernels; Hybrid kernels; Extreme learning machines; Hyperspectral images	CLASSIFICATION; AGREEMENT	Multiple kernel (MK) learning (MKL) methods have a significant impact on improving the classification performance. Besides that, composite kernel (CK) methods have high capability on the analysis of hyperspectral images due to making use of the contextual information. In this work, it is aimed to aggregate both CKs and MKs autonomously without the need of kernel coefficient adjustment manually. Convex combination of predefined kernel functions is implemented by using multiple kernel extreme learning machine. Thus, complex optimization processes of standard MKL are disposed of and the facility of multi-class classification is profited. Different types of kernel functions are placed into MKs in order to realize hybrid kernel scenario. The proposed methodology is performed over Pavia University, Indian Pines, and Salinas hyperspectral scenes that have ground-truth information. Multiple composite kernels are constructed using Gaussian, polynomial, and logarithmic kernel functions with various parameters, and then the obtained results are presented comparatively along with the state-of-the-art standard machine learning, MKL, and CK methods.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6809	6819		10.1007/s00521-019-04044-9													
J								HPFE: a new secure framework for serving multi-users with multi-tasks in public cloud without violating SLA	NEURAL COMPUTING & APPLICATIONS										Scheduling; HPFE; SLA; Security; Public cloud	ALGORITHM; INTERNET; ARCHITECTURE; THINGS	Efficient resource distribution for multiple users is one of the most challenging problems of the cloud service provider (CSP). This problem is more difficult when each user submits multi-tasks to the cloud environment. Hence, CSP needs an efficient framework to optimize multiple objectives, e.g., cost, time, and load balancing as well as to manage tasks of different users in a secure way. This paper presents a new efficient scheduling framework, called highest priority first execute (HPFE), for serving multiple users with multiple tasks in a secure and optimized way. Computational results show that the proposed HPFE reduces the makespan and increases the load balancing degree comparing to the most recent algorithms; Min-Min, Max-Min, minimum completion time, first come first serve, genetic algorithm and simulated annealing.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6821	6841		10.1007/s00521-019-04091-2													
J								Fully automatic alpha matte extraction using artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Alpha matting; Digital compositing; Green screen; Machine learning; Backpropagation algorithm	IMAGE	The alpha matte is a two-dimensional map that is used to combine two images, one containing a foreground and the other containing a background. Alpha matte extraction is performed on green-screen images and requires user interaction to tune parameters in different preprocessing and postprocessing stages to refine an alpha matte. This paper tackles the problem of fully automatic extraction of the foreground on green-screen images with extraction of the corresponding alpha matte. The method is based on a multi-layer perceptron that assigns an alpha value, from a discrete set of ten alpha values, to each patch on a green-screen image. The approach for assigning an alpha value to an image patch is based on a set of features that enhance discrimination between foreground and background. The classifier is trained to learn to separate foreground objects from green-screen backgrounds as well as to generate the corresponding alpha matte map required for subsequent digital compositing. To test how the proposed approach handles alpha matte extraction under unsuitable conditions, a 64-image dataset was generated. The main contribution is that our method overcomes two challenges publicly posed within a dataset of green-screen image sequences, donated by Hollywood Camera Work LLC. Tests with this dataset generate high-quality visual results for those two cases. These results are confirmed by comparing the proposed fully automatic alpha matte extraction with that based on the use of Adobe After Effects Creative Cloud, an application which heavily depends on user interaction.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6843	6855		10.1007/s00521-019-04154-4													
J								Modeling of electricity demand forecast for power system	NEURAL COMPUTING & APPLICATIONS										Electricity demand forecasting; Adaptive Fourier decomposition; Seasonal adjustment; Moth-flame optimization algorithm; Machine learning method	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; UNIT COMMITMENT; ENERGY DEMAND; PREDICT	The emerging complex circumstances caused by economy, technology, and government policy and the requirement of low-carbon development of power grid lead to many challenges in the power system coordination and operation. However, the real-time scheduling of electricity generation needs accurate modeling of electricity demand forecasting for a range of lead times. In order to better capture the nonlinear and non-stationary characteristics and the seasonal cycles of future electricity demand data, a new concept of the integrated model is developed and successfully applied to research the forecast of electricity demand in this paper. The proposed model combines adaptive Fourier decomposition method, a new signal preprocessing technology, for extracting useful element from the original electricity demand series through filtering the noise factors. Considering the seasonal term existing in the decomposed series, it should be eliminated through the seasonal adjustment method, in which the seasonal indexes are calculated and should multiply the forecasts back to restore the final forecast. Besides, a newly proposed moth-flame optimization algorithm is used to ensure the suitable parameters of the least square support vector machine which can generate the forecasts. Finally, the case studies of Australia demonstrated the efficacy and feasibility of the proposed integrated model. Simultaneously, it can provide a better concept of modeling for electricity demand prediction over different forecasting horizons.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6857	6875		10.1007/s00521-019-04153-5													
J								A comparison of modified tree-seed algorithm for high-dimensional numerical functions	NEURAL COMPUTING & APPLICATIONS										Tree-seed algorithm; Metaheuristic algorithms; Benchmark functions; Optimization	OPTIMIZATION ALGORITHM; DIFFERENTIAL EVOLUTION; TSA	Optimization methods are used to solve many problems and, under certain constraints, can provide the best possible results. They are inspired by the behavior of living things in nature and called metaheuristic algorithms. The population-based tree-seed algorithm (TSA) is an example of these algorithms and is used to solve continuous optimization problems that have recently emerged. This method, inspired by the relationship between trees and seeds, produces a certain number of seeds for each tree during each iteration. In this study, during seed formation in the TSA, trees were selected using the tournament selection method rather than by random means. Efforts were also made to enhance high-dimensional solutions, utilizing problem dimensions, D, of 20, 50, 100 and 1000 by optimizing the search tendency parameter within the structure of the algorithm, resulting in a modified TSA (MTSA). Empirical test data, convergence graphs and box plots were obtained by applying the MTSA to numerical benchmark functions. In addition, the results of the current algorithms in the literature were compared with the MTSA and the statistical test results were presented. The results from this analysis demonstrated that the MTSA could achieve superior results to the original TSA.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6877	6911		10.1007/s00521-019-04155-3													
J								Mining hidden non-redundant causal relationships in online social networks	NEURAL COMPUTING & APPLICATIONS										Online social network; Causal discovery; Transfer entropy; Non-redundant causal relationships	CONNECTIVITY; MODEL	Causal discovery is crucial to obtain a deep understanding of the actual mechanism behind the online social network, e.g., identifying the influential individuals and understanding the interaction among user behavior sequences. However, detecting causal directions and pruning causal redundancy of online social networks are still the great challenge of existing research. This paper proposed a constraint-based approach, minimal causal network (MCN), to mine hidden non-redundant causal relationships behind user behavior sequences. Under the MCN, the transfer entropy with the adaptive causal time lag is used to detect causal directions and find causal time lags, while a permutation-based significance test is proposed to prune redundant edges. Experiments on simulated data verify the effectiveness of our proposed method. We also apply our approach to real-world data from Sina Weibo and reveal some interesting discoveries.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6913	6923		10.1007/s00521-019-04161-5													
J								Parameters optimization of support vector machines for imbalanced data using social ski driver algorithm	NEURAL COMPUTING & APPLICATIONS										Optimization algorithms; Support vector machine (SVM); Parameter optimization; Imbalanced data	FEATURE-SELECTION; CLASSIFICATION; SVM	The parameters of support vector machines (SVMs) such as kernel parameters and the penalty parameter have a great influence on the accuracy and complexity of the classification models. In the past, different evolutionary optimization algorithms were employed for optimizing SVMs; in this paper, we propose a social ski-driver (SSD) optimization algorithm which is inspired from different evolutionary optimization algorithms for optimizing the parameters of SVMs, with the aim of improving the classification performance. To cope with the problem of imbalanced data which is one of the challenging problems for building robust classification models, the proposed algorithm (SSD-SVM) was enhanced to deal with imbalanced data. In this study, eight standard imbalanced datasets were used for testing our proposed algorithm. For verification, the results of the SSD-SVM algorithm are compared with grid search, which is a conventional method of searching parameter values, and particle swarm optimization (PSO). The experimental results show that the SSD-SVM algorithm is capable of finding near-optimal values of SVMs parameters. The results also demonstrated high classification performance compared to the PSO algorithm.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6925	6938		10.1007/s00521-019-04159-z													
J								Intelligent schemes for fault classification in mutually coupled series-compensated parallel transmission lines	NEURAL COMPUTING & APPLICATIONS										Artificial neural network; Series compensation; Mutual coupling; Fault phase identification; Fault classification; Decision tree; Support vector machine	PROTECTION SCHEME; WAVELET TRANSFORM; IDENTIFICATION; LOCATION; MACHINE	The protection of mutually coupled series capacitor-compensated (SCC) parallel transmission lines is a more complicated task than uncompensated lines due to the effect of mutual coupling, inter-circuit faults, and non-linearity of effective impedance of SCC line. A method that can overcome these issues and still work efficiently is a supervised learning-based method which is an adaptive technique. Hence, in this work, various supervised learning-based intelligent schemes like artificial neural network (ANN), support vector machines (SVM), and decision tree (DT) are employed to find a suitable method for the protection of series capacitor-compensated lines. Discrete wavelet transform has been used to process the three-phase current signals of the parallel lines measured at one terminal only. A moving window of 20 samples is selected, and approximate wavelet coefficient is calculated up to level 1 using DB-4 mother wavelet. The resultant is then given as input to the intelligent schemes (ANN, SVM, and DT). The proposed intelligent schemes have been tested with variety of fault conditions such as inter-circuit faults, cross-country faults, transforming faults, single-circuit operation, and high resistance faults. A large number of fault simulation studies corroborate that DT-based fault classification method is better than ANN and SVM. The accuracy of faulty phase and ground identification scheme is 100% for all the tested fault cases. Hence, the proposed supervised learning-based intelligent method can be implemented in real power system network effectively.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6939	6956		10.1007/s00521-019-04185-x													
J								Path-based reasoning with constrained type attention for knowledge graph completion	NEURAL COMPUTING & APPLICATIONS										Neural network; Knowledge graph completion; Multi-hop reasoning; Attention mechanism		Multi-hop reasoning over paths in knowledge graphs has attracted rising research interest in the field of knowledge graph completion. Entity types and relation types both contain various kinds of information content though only a subset of them are helpful in the specific triples. Although significant progress has been made by existing models, they have two major shortcomings. First, these models seldom learn an explicit representation of entities and relations with semantic information. Second, they reason without discriminating distinct role types that the same entity with multiple types plays in different triples. To address these issues, we develop a novel path-based reasoning with constrained type attention model, which tries to identify entity types by leveraging relation type constraints in the corresponding triples. Our experimental evaluation shows that the proposed model outperforms the state of the art on a real-world dataset. Further analyses also confirm that both word-level and triple-level attention mechanisms of our model are effective.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6957	6966		10.1007/s00521-019-04181-1													
J								Opposition-based antlion optimizer using Cauchy distribution and its application to data clustering problem	NEURAL COMPUTING & APPLICATIONS										Optimization; Cauchy distribution; Opposition-based learning; Data clustering; Intra-cluster variance	DIFFERENTIAL EVOLUTION; SWARM OPTIMIZER; COLONY APPROACH; ALGORITHM	This paper proposes an improved version of antlion optimizer (ALO) to solve data clustering problem. In this work, Cauchy distribution-based random walk is employed in place of uniform distribution to jump out of local optima as a first strategy. Then opposition-based learning model is utilized in conjunction with acceleration coefficient to overcome the slow convergence of classical ALO as second strategy to propose opposition-based ALO using Cauchy distribution (OB-C-ALO). The performance of the proposed OB-C-ALO is evaluated over a set of benchmark problems of different varieties of characteristics and analysed statistically by performing Wilcoxon rank-sum test. The proposed version then utilizes K-means clustering by refining the clusters formed using K-means as objective function. The algorithm is evaluated on six data sets of UCI machine learning repository and compared with classical ALO and recently developed version of ALO, namely OB-L-ALO, over benchmark test problems as well as data clustering problem and proved to be better in terms of performance achieved.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6967	6995		10.1007/s00521-019-04174-0													
J								Evaluation of effective stiffness of RC column sections by support vector regression approach	NEURAL COMPUTING & APPLICATIONS										Support vector regression; Effective stiffness ratio; Reinforced concrete columns; Nonlinear dynamic analysis	PERFORMANCE-BASED DESIGN; CONCRETE; PREDICTION; RELIABILITY; MEMBERS	Effective stiffness of reinforced concrete (RC) members has a very important role in the performance evaluation of RC frame buildings through nonlinear dynamic analyses. The beam effective stiffness can be readily computed using mechanics, but the evaluation of column stiffness is a complicated process and the use of support vector regression helps in this regard. Therefore, in this study, an attempt is made to predict the effective stiffness ratio of reinforced concrete columns using support vector regression (SVR) approach. A data set of 208 samples, which are collected through nonlinear dynamic analysis of reinforced concrete buildings using SAP2000 software, is utilized to develop the SVR model. The input parameters considered are reinforcement percentage, axial load and depth of the column section in both the perpendicular directions, and the output parameter is the effective stiffness ratio of columns. Three different kernel parameters are used, namely exponential radial basis function (ERBF), Gaussian radial basis function and polynomial function for SVR modelling, among which ERBF is found to be the most suitable one. The obtained results indicate that the statistical performance of the SVR-ERBF model is better than the models with other two kernels in predicting the effective stiffness ratio of reinforced concrete columns. Performance of the SVR model is compared with the results of multi-variable regression analysis. In addition to that, a sensitivity analysis is also performed to check the influence of each input parameter on output responses.																	0941-0643	1433-3058				JUN	2020	32	11			SI		6997	7007		10.1007/s00521-019-04190-0													
J								SP-J48: a novel optimization and machine-learning-based approach for solving complex problems: special application in software engineering for detecting code smells	NEURAL COMPUTING & APPLICATIONS										Optimization; Bio-inspired metaheuristic techniques; Machine-learning; Code smells	SPOTTED HYENA OPTIMIZER; ALGORITHM	This paper presents a novel hybrid algorithm based on optimization and machine-learning approaches for solving real-life complex problems. The optimization algorithm is inspired from the searching and attacking behaviors of sandpipers, called as Sandpiper Optimization Algorithm (SPOA). These two behaviors are modeled and implemented computationally to emphasize intensification and diversification in the search space. A comparison of the proposed SPOA algorithm is performed with nine competing optimization algorithms over 23 benchmark test functions. The proposed SPOA is further hybridized with B-J48 pruned machine-learning approach for efficiently detecting the code smells from the data set. The results reveal that the proposed technique is able to solve challenging problems and outperforms the other well-known approaches.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7009	7027		10.1007/s00521-019-04175-z													
J								Digital mammogram classification using 2D-BDWT and GLCM features with FOA-based feature selection approach	NEURAL COMPUTING & APPLICATIONS										Computer-aided diagnosis; Discrete wavelet transform (DWT); Forest optimization algorithm (FOA); Matthews correlation coefficient (MCC)	FEATURE-EXTRACTION METHOD; COMPUTER-AIDED DETECTION; BREAST-CANCER; DIAGNOSIS; BENIGN; IMAGES; MASSES	This paper proposes an improved computer-aided diagnosis model to identify mammographic images as normal or abnormal, and further, benign or malignant. The proposed scheme employs all the steps associated with other classification schemes; however, the contribution of the suggested scheme is fourfold. Initially, a fusion-based feature extraction method is employed to obtain the features using a combination of 2-D block discrete wavelet transform (2D-BDWT) and gray-level co-occurrence matrix (GLCM). Next, principal component analysis (PCA) is utilized to reduce the large dimension of the feature vector. Furthermore, to select the most optimal features from the reduced set of features, forest optimization algorithm (FOA) is used. The FOA-based feature selection algorithm is utilized as a wrapper-based technique which includes both feature selection and classification. In the proposed framework, several classifiers, namely support vector machine (SVM), k-nearest neighbor (k-NN), and decision tree (C4.5), are applied. The proposed method is compared with the benchmark schemes on two standard datasets, namely Mammographic Image Analysis Society (MIAS) and Digital Database for Screening Mammography (DDSM). Simulation results and analysis confirm that the proposed scheme brings potential improvements with respect to classification accuracy, sensitivity, specificity, area under curve, F-score, and Matthews correlation coefficient. The classification accuracy is measured with respect to normal versus abnormal and further, benign versus malignant. The proposed scheme with different combinations of classifiers, namely 2D-BDWT + GLCM + PCA + FOA + SVM, 2D-BDWT + GLCM + PCA + FOA + k-NN, and 2D-BDWT + GLCM + PCA + FOA + C4.5, achieves a maximum classification accuracy of 100% for both the MIAS and DDSM datasets.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7029	7043		10.1007/s00521-019-04186-w													
J								Designing secure substitution boxes based on permutation of symmetric group	NEURAL COMPUTING & APPLICATIONS										Substitution boxes; S-box; Projective general linear group; Cybersecurity; Algebraic attacks	S-BOXES; NONLINEAR COMPONENT; CONSTRUCTION; COMBINATION; SCHEME	The strength of cryptosystems heavily relies on the substitution boxes. Cryptosystems with weak substitution boxes cannot resist algebraic attacks, linear and differential cryptanalysis. In this paper, first, we propose a strong algebraic structure for the construction of substitution boxes. The proposed substitution boxes have good algebraic properties and are able to resist against algebraic attacks. Second, we propose a new method for creating multiple substitution boxes with the same algebraic properties using permutation of symmetric group on a set of size 8 and bitwise XOR operation. Third, the proposed substitution boxes with the same algebraic properties are then applied to images and it is observed that the statistical properties of substituted images are different from each other. The simulation results and statistical and security analysis for the proposed substitution boxes are very competitive. Also, it is shown in this work that the proposed substitution boxes can resist differential and linear cryptanalysis and sustain algebraic attacks.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7045	7056		10.1007/s00521-019-04207-8													
J								Guaranteed-consensus posterior-aggregation fuzzy analytic hierarchy process method	NEURAL COMPUTING & APPLICATIONS										Fuzzy analytic hierarchy process; Decision maker; Consensus; Posterior aggregation	AHP; PRIORITIES; JUDGMENTS; FRAMEWORK; OPINIONS; SYSTEM; HOT	Current group decision-making fuzzy analytic hierarchy processes (FAHPs) have two major problems. First, inconsistent fuzzy pairwise comparison results, rather than compromised fuzzy weights, are aggregated. Second, a consensus among decision makers (DMs) cannot be guaranteed. To address these problems, in this study, the guaranteed-consensus posterior-aggregation FAHP (GCPA-FAHP) method was proposed. In the proposed methodology, the membership functions of the linguistic terms for performing fuzzy pairwise comparisons were designed to guarantee a consensus among the DMs and can be modified afterward to enhance the estimation precision. In addition, fuzzy intersection and center of gravity were used to aggregate and defuzzify the estimated fuzzy weights. The GCPA-FAHP method was applied to a real case to evaluate its effectiveness. The experimental results revealed that the GCPA-FAHP method guaranteed consensus among the DMs and improved the precision of estimating fuzzy weights.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7057	7068		10.1007/s00521-019-04211-y													
J								A novel three-coil wireless power transfer system and its optimization for implantable biomedical applications	NEURAL COMPUTING & APPLICATIONS										Finite element method; Optimal design; Three-coil; Wireless power transfer; Biomedical application	SMOOTHING NEWTON METHOD; ALGORITHM; UNIQUENESS; EXISTENCE	The technology of wireless power transfer (WPT) has broad applications especially in implantable biomedical devices. Because of the limitation of the size of the receiver coil, how to lengthen the power transfer distance is crucial in biomedical applications. In order to address this problem, in this paper, a novel three-coil WPT system is proposed and analyzed. In the system, there are two transmitter coils and one receiver coil. Based on the Biot-Savart's law, the electromagnetic property of the square coil is analyzed using finite element method. Moreover, the structural design of the system is optimized by a memetic algorithm. The memetic algorithm combines features of the artificial bee colony method and the covariance matrix adaption evolutionary strategy method. The simulation and experiment results show that the receiver coil can receive about dozens of millivolt when the power transfer distance is about 15 cm. This means the proposed system is suitable for implantable biomedical devices. Compared with the two-coil WPT system, in addition, the power received in the receiver coil of three-coil WPT system can increase about 48%.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7069	7078		10.1007/s00521-019-04214-9													
J								Antlion optimization algorithm for pairwise structural alignment with bi-objective functions	NEURAL COMPUTING & APPLICATIONS										Protein structure; Bi-objective functions; RMSD; TM score; Gene ontology functions	PROTEIN-STRUCTURE ALIGNMENT; DATABASE; CLASSIFICATION; SCOP	This research work concentrates on the pairwise protein structure alignment of numerous protein structures. This is considered to be a problematic job which is an NP-hard problem. This work proposed a bi-objective-based antlion optimization algorithm (BO-ALO) which is aimed to improve the geometrical and evolutionary relationship among the protein structures. The bi-objective functions are the RMSD and TM scores which aim to minimize the RMSD score and maximize the TM score. The BO-ALO method has been compared with various widespread existing methods such as SPalignNS, UniAlign, DALI, MICAN, GANGSTA, DeepAlign, TM-align, CE, ant colony optimization and artificial bee colony optimization methods. The experiments were taken from the benchmark datasets, namely SCOPe and CATH. Also, the predicted alignments were compared with gold standard benchmark databases such as CDD, MALIDUP, MALISAM and HOMSTRAD. The outcomes of the proposed method have better bi-objective function scores and performance measures than other well-known existing approaches. This work also evaluates the proposed method with its biological significance of predicting the common gene ontology functions among the aligned protein structures. Finally, the statistical significance of the BO-ALO is computed by employing the Wilcoxon matched-signed rank test. Also, the statistical significant results of the proposed methods give better results when compared to other existing approaches.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7079	7096		10.1007/s00521-019-04176-y													
J								Quadratic programming over ellipsoids with applications to constrained linear regression and tensor decomposition	NEURAL COMPUTING & APPLICATIONS										Quadratic programming over a single sphere; Quadratic programming over ellipsoids; Linear regression with bound constraint; Generalised eigenvalue decomposition with tucker or tensor train structure	OPTIMIZATION; RELAXATION; ALGORITHMS	A novel algorithm to solve the quadratic programming (QP) problem over ellipsoids is proposed. This is achieved by splitting the QP problem into two optimisation sub-problems, (1) quadratic programming over a sphere and (2) orthogonal projection. Next, an augmented-Lagrangian algorithm is developed for this multiple constraint optimisation. Benefitting from the fact that the QP over a single sphere can be solved in a closed form by solving a secular equation, we derive a tighter bound of the minimiser of the secular equation. We also propose to generate a new positive semidefinite matrix with a low condition number from the matrices in the quadratic constraint, which is shown to improve convergence of the proposed augmented-Lagrangian algorithm. Finally, applications of the quadratically constrained QP to bounded linear regression and tensor decomposition paradigms are presented.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7097	7120		10.1007/s00521-019-04191-z													
J								Design of nature-inspired heuristic paradigm for systems in nonlinear electrical circuits	NEURAL COMPUTING & APPLICATIONS										Electronic circuits; Artificial neural networks; Particle swarm optimization; Interior-point algorithm; Hybrid computing	NEURAL-NETWORK; ALGORITHM; INTELLIGENCE; HISTORY; MODELS	In the present study, a novel application of nature-inspired heuristics is presented for problems in nonlinear circuit analysis using neural networks, particle swarm optimization (PSO), and interior-point algorithm (IPA) as well as integrated approach PSO-IPA. The governing system models of resistor-capacitor circuits with nonlinear capacitance as well as resistor-inductor circuits with nonlinear inductance are mathematically modeled through competency of neural networks and weights of these networks are trained for global search with PSO hybrid with IPA for speedy refinements. The designed technique is applied on a number of scenarios by taking different values of resistance, current, voltage inductance, and capacitance parameters in nonlinear electrical circuit models. Comparative study with Adams numerical solvers having matching of the order 10(-04)-10(-07) and consistently attaining near-optimal gauges of performance indices based on root-mean-squared error, Theil's inequality coefficient, and Nash-Sutcliffe efficiency metrics validate and verify the efficacy of the scheme.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7121	7137		10.1007/s00521-019-04197-7													
J								Transfer learning and feature fusion for kinship verification	NEURAL COMPUTING & APPLICATIONS										Kinship verification; Multiple deep features; Feature selection; Multi-view repulsive metric learning; Fusion; Classifier	FACIAL IMAGES; FACE	Facial image analysis has been an important subject of study in the communities of pattern recognition and computer vision. Facial images contain much information about the person they belong to like identity, age, gender, ethnicity, and expression. This paper introduces a new framework that exploits facial deep features for kinship verification. The framework integrates efficient feature selection and kinship-oriented discriminant data projection. The resulting framework incorporates three levels of fusion: (1) an early fusion of descriptors where the filter selection selects the most relevant deep features, (2) a middle-stage fusion which exploits a kinship-based multi-view metric learning method, and (3) a late-stage fusion that merges classifiers responses. In our study, face features are provided by the pre-trained deep convolutional neural networks VGG-F and VGG-Face that were originally proposed for discriminating categories of objects and identities, respectively. Experimental results on two benchmarked datasets for kinship verification in the wild (KinFaceW-I and KinFaceW-II) show that the proposed framework outperforms state-of-the-art techniques without the use of external data or data augmentation that are tailored for the kinship verification problem. These experiments show that the proposed scheme can outperform feature fusion obtained by deep multi-metric learning.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7139	7151		10.1007/s00521-019-04201-0													
J								Optimized metamaterial-loaded fractal antenna using modified hybrid BF-PSO algorithm	NEURAL COMPUTING & APPLICATIONS										Fractal antenna; Metamaterial; Modified hybrid bacterial foraging-particle swarm optimization; Split-ring resonator	SPLIT-RING RESONATORS; EQUIVALENT-CIRCUIT MODELS; DESIGN OPTIMIZATION; TRANSMISSION	The paper proposes optimization of square split-ring resonator (SRR) metamaterial unit cell using modified hybrid bacterial foraging-particle swarm optimization (BF-PSO). Optimized metamaterial unit cells are loaded into novel designed square fractal antenna for its bandwidth enhancement. The presented research is alienated in three phases: Novel design of microstrip line-fed square fractal antenna with defected ground structure is proposed in the initial phase that provides dual band performance. In second phase, with the aim of bandwidth enhancement, quasi-static model of SRR unit cell is used to optimize its structural parameters so that optimized structure resonates at desired frequency region. Modifications are included in hybrid BF-PSO algorithm as per size constraints of SRR unit cell to be optimized and for improving the convergence behavior of algorithm. The performance of modified hybrid BF-PSO algorithm is assessed against four other evolutionary techniques named as classical BFO, chaos PSO, IWO and ABC. In later phase, optimized SRR unit cells are loaded into initially designed square fractal antenna that results in broadband performance suitable for upper S-band and lower C-band wireless applications. The designed square fractal antenna without and with SRR unit cells is fabricated and tested to verify the experimental results.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7153	7169		10.1007/s00521-019-04202-z													
J								A novel adaptive model predictive controller for load frequency control of power systems integrated with DFIG wind turbines	NEURAL COMPUTING & APPLICATIONS										Adaptive model predictive control; Load frequency control; Power system inertia; Online model estimation; Wind turbines; Power system control	MANAGEMENT; DESIGN	With the rapid growth of renewable energy resources, wind energy system is getting more interest everywhere throughout the world. However, its extensive use in power systems prompts many power system dynamics and stability problems. Load variation and anomalous operating conditions prompt inconsistencies in frequency and planned power trades. These inconsistencies should be remedied by load frequency control. This paper introduces a novel frequency control system utilizing a mix of adaptive model predictive controller (AMPC) and recursive polynomial model estimator (RPME) integrated with double fed induction generator wind turbines. Inside each control duration, the RPME is identifying a discrete-time online autoregressive exogenous model. The latter is used through the AMPC to update the interior plant model in order to achieve a successful nonlinear control. The performance of the proposed system has been verified and contrasted with the conventional MPC system through a computer simulation-based MATLAB/SIMULINK. The simulation results demonstrated the superiority of the proposed system as for the conventional MPC system.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7171	7181		10.1007/s00521-019-04205-w													
J								Adaptive over-sampling method for classification with application to imbalanced datasets in aluminum electrolysis	NEURAL COMPUTING & APPLICATIONS										Class imbalance problem; Multi-objective optimization; State transition algorithm; SMOTE; Aluminum electrolysis	STATE TRANSITION ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; SMOTE; PREDICTION	The class imbalance problem often appears in practical applications, where one class has numerous instances and the other has only a few instances. Synthetic Minority Over-sampling TEchnique (SMOTE) is the most popular and commonly used sampling method to solve this problem. It has two important parameters: over-sampling rate N and number of nearest neighbors k. However, the two parameters that are arbitrarily chosen by users are not optimal in practical applications. In addition, the imbalance ratios of these datasets are absolutely different, which makes parameter selection in SMOTE more difficult. To overcome the problem, an adaptive over-sampling method is proposed in this study based on SMOTE. It transforms the parameter selection problem in SMOTE to a multi-objective optimization problem. Then, a new selection strategy named absolute dominance-based selection is proposed to obtain the current optimal solution. Finally, the state transition algorithm is used to search the best parameter values of SMOTE to achieve the optimal objectives. Four imbalanced benchmark datasets and four class-imbalanced aluminum electrolysis datasets are used to verify the validity of the proposed method. In comparison with other methods, the proposed method has the advantage of good classification performance. Numerical results also show that the proposed method can successfully solve the class imbalance problem in aluminum electrolysis.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7183	7199		10.1007/s00521-019-04208-7													
J								Stability analysis for model-based study of nanofluid flow over an exponentially shrinking permeable sheet in presence of slip	NEURAL COMPUTING & APPLICATIONS										Nanofluid; Boundary-layer flow; Exponentially shrinking sheet; Velocity slip; Thermal slip; Suction; Dual solutions; Stability analysis	MICROPOLAR DUSTY FLUID; BOUNDARY-LAYER-FLOW; STAGNATION-POINT FLOW; HEAT-TRANSFER; THERMAL-RADIATION; STRETCHING SHEET; SQUEEZING FLOW; VISCOUS-FLOW; CU-WATER; SURFACE	This article aims to present the nanofluid flow over an exponentially porous shrinking sheet in the presence of velocity slip and thermal slip. Single-phase fluid model for nanofluid has been used. In this investigation, the effects of silver (Ag) nanoparticle in two different types of base fluids (water and kerosene oil) are investigated. Using similarity transformations, the governing boundary-layer equations and the boundary conditions are reduced to the system of coupled nonlinear ordinary differential equations and then solved numerically with the help of shooting method. Dual solutions exist for some particular range of values of the governing parameters. A stability analysis has been performed to find out the stable solution. A comparison is made between the boundary-layer flow of Ag-water and Ag-kerosene. Impacts of various parameters on velocity, temperature profiles, skin friction coefficient, and Nusselt number are computed and presented in graphs and tables. The fluid velocity and temperature both increase with the increasing nanoparticle volume fraction.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7201	7211		10.1007/s00521-019-04221-w													
J								Stability analysis and dynamic output feedback control for fuzzy networked control systems with mixed time-varying delays and interval distributed time-varying delays	NEURAL COMPUTING & APPLICATIONS										Stability analysis; Lyapunov functional; T-S fuzzy model; LMIs; Mixed time-varying delays; Distributed time-varying delays	NONLINEAR-SYSTEMS; ROBUST STABILITY; NEURAL-NETWORKS; STABILIZATION	This paper addresses the stability analysis and dynamic output feedback control for a class of T-S fuzzy networked control systems with mixed time-varying delays and interval distributed time-varying delays. Firstly, the T-S fuzzy model is employed to approximate the networked control system based on the system model. Secondly, the dynamic output feedback controller with hybrid intermittent feedback control law is designed for the T-S fuzzy networked control system. Thirdly, the delay-dependent stability conditions are derived by introducing the Lyapunov functional with Leibniz-Newton formula. Compared with previous works, both the mixed time-varying delays and interval distributed time-varying delays are considered in the T-S fuzzy networked control system. The control design conditions are relaxed because of the proposed controller. By introducing the Lyapunov functional with Leibniz-Newton formula, the linear matrix inequalities are solved effectively by the standard convex optimization algorithm. Finally, the simulation examples and some computing results are given to show the effectiveness and advantage of the proposed method, respectively.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7213	7234		10.1007/s00521-019-04204-x													
J								Fractional-order self-tuned fuzzy PID controller for three-link robotic manipulator system	NEURAL COMPUTING & APPLICATIONS										Self-tuning fuzzy PID controller; Fractional-order operator; Cuckoo search algorithm; Three-link manipulator system	TRACKING CONTROL; SEARCH; DESIGN; LOGIC; CAUCHY; PD	With increasing links in a manipulator system, efficacy and usefulness are enhanced along with the complexity. In this paper, a fractional-order self-tuned fuzzy PID (FOSTFPID) controller is investigated to control a highly nonlinear, coupled multi-input multi-output, three-link rigid robotic manipulator system. The performance of FOSTFPID controller is investigated for trajectory tracking, disturbance rejection, noise suppression and model uncertainty. The comparative analysis between the performances of FOSTFPID, fractional-order fuzzy PID and integer-order self-tuning fuzzy PID controllers, all tuned for minimum weighted sum of integral of absolute error and integral of absolute change in controller output using cuckoo search algorithm, revealed a clear superiority of FOSTFPID for trajectory tracking, disturbance rejection, noise suppression and model uncertainty.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7235	7257		10.1007/s00521-019-04215-8													
J								An efficient hybrid approach of improved adaptive neural fuzzy inference system and teaching learning-based optimization for design optimization of a jet pump-based thermoacoustic-Stirling heat engine	NEURAL COMPUTING & APPLICATIONS										Jet pump; Acoustic streaming; Optimization; Taguchi method; ANFIS; Teaching-learning-based optimization; Statistical analysis	PARTICLE SWARM OPTIMIZATION; RESPONSE-SURFACE METHODOLOGY; TAGUCHI METHOD; STATISTICAL COMPARISONS; PHASE ADJUSTER; ANFIS; PERFORMANCE; PREDICTION; MODEL; PARAMETERS	The acoustic streaming is a key drawback and eliminates the performance of a jet pump-based thermoacoustic-Stirling heat engine. The present study deals with a new hybrid optimization approach to reduce the acoustic streaming energy. The proposed work is an integration of Taguchi method (TM), adaptive neural fuzzy inference system (ANFIS), and teaching-learning-based optimization (TLBO). The Taguchi method plays three important roles. The first role is to layout the number of experiments. The second role is to identify the most appropriate parameters for ANFIS structure regarding the number of input membership functions (MFs), types of input MFs, optimal learning method, and types of output MFs. In order to determine the optimum parameters for the ANFIS structure, the root-mean-squared error, a performance criterion, is minimized by using the TM. The final role of TM is to optimize the controllable parameters of the TLBO. Subsequently, modeling between geometric parameters and acoustic streaming is established by the built ANFIS structure. Finally, the TLBO is adopted by optimizing the design parameters. The outcomes of study revealed that the acoustic streaming is relatively reduced. Based on Wilcoxon signed-rank test and Friedman test, it proves that the effectiveness of the proposed hybrid approach is better to other evolutionary algorithms. The current approach is an efficient optimizer for complex optimization problems.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7259	7273		10.1007/s00521-019-04249-y													
J								Cross-view gait recognition through ensemble learning	NEURAL COMPUTING & APPLICATIONS										Gait recognition; Cross-view; Ensemble learning; Gait classification	FUSION; PERFORMANCE; FEATURES	Gait has been well known as an unobtrusive promising biometric to identify a person from a distance. However, the effectiveness of silhouette-based approaches in gait recognition is diluted due to variations of view angles. In this paper, we put forward a novel and effective method of gait recognition: cross-view gait recognition based on ensemble learning. The proposed method greatly enhances the effectiveness and reduces the sensitivity of gait recognition under various view angles conditions. Furthermore, in this paper we will introduce a novel algorithm based on ensemble learning for combining several gait learners together, which utilizes a well-designed gait feature based on area average distance. Through experimental evaluations on the well-known CASIA gait database and OU-ISIR gait database, our paper demonstrates the advantages of the proposed method in comparison with others. The contribution of this research work is to resolve the multiview angles problem of gait recognition through assembling several gait learners.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7275	7287		10.1007/s00521-019-04256-z													
J								Predicting ultimate bond strength of corroded reinforcement and surrounding concrete using a metaheuristic optimized least squares support vector regression model	NEURAL COMPUTING & APPLICATIONS										Ultimate bond strength; Steel reinforcement; Least squares support vector regression; Differential flower pollination; Machine learning	ARTIFICIAL NEURAL-NETWORK; PARTICLE SWARM OPTIMIZATION; SHEAR-STRENGTH; FIREFLY ALGORITHM; STEEL BARS; MACHINE; CORROSION; BEHAVIOR; CRACKING; TREE	The ultimate bond strength of corroded steel reinforcement and surrounding concrete critically affects the load carrying capacity and eventually serviceability of the reinforced concrete structures. This study constructs and verifies a data-driven method for estimating ultimate bond strength. The proposed method is a hybridization of least squares support vector regression (LSSVR) and differential flower pollination (DFP) computational intelligence approaches. Since the problem of ultimate bond strength prediction involves nonlinear and multivariate data modeling, the LSSVR is employed to infer the mapping function between ultimate bond strength and its influencing factors of concrete compressive strength, concrete cover, steel type, diameter of steel bar, bond length, and corrosion level. Moreover, in order to overcome the very challenging task of fine-tuning the LSSVR model training, the DFP algorithm, as a population-based metaheuristic, is utilized to optimize the performance of the LSSVR prediction model. A dataset including 218 experimental tests has been collected from the literature to construct and verify the proposed hybrid method. Experimental results supported by the Wilcoxon signed-rank test point out that the hybridization of LSSVR and DFP can deliver predictive results (root-mean-square error=2.39, mean absolute percentage error=33.82%, and coefficient of determination=0.84) superior to those of benchmark models including the artificial neural network, the multivariate adaptive regression splines, and the regression tree. Additionally, a software program based on the LSSVR model and the DFP optimization result has also been developed and compiled in Visual C#.Net to ease the model implementation. Hence, the hybrid model of DFP and LSSVR can be a promising alternative to assist engineers in the task of evaluating the health of reinforced concrete structures.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7289	7309		10.1007/s00521-019-04258-x													
J								Crosstalk modeling in high-speed transmission lines by multilayer perceptron neural networks	NEURAL COMPUTING & APPLICATIONS										Crosstalk; Microstrip; Multilayer perceptron; Stripline	FEEDFORWARD NETWORKS; MICROSTRIP LINES; PARAMETERS; REDUCTION; DESIGN	Signal degradation due to crosstalk-related issues has become increasingly important particularly in high-speed signal transmissions. Conventional analysis of crosstalk requires a full electromagnetic modeling of the signal transmission path along with a time-domain transient simulation which is computationally demanding. In this work, we apply a multilayer perceptron neural network for crosstalk prediction in coupled transmission lines. The well-trained neural networks can be used to predict the time-domain crosstalk directly, thereby replacing complex circuit simulations. Numerical results show a high degree of generalization of the neural networks, which are able to produce accurate results and can be trained to include effects such as reflections and input mismatches.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7311	7320		10.1007/s00521-019-04252-3													
J								Weak, modified and function projective synchronization of Cohen-Grossberg neural networks with mixed time-varying delays and parameter mismatch via matrix measure approach	NEURAL COMPUTING & APPLICATIONS										Neural networks; Mixed time-varying delays; Halanay inequality; Matrix measure; Modified function projective synchronization	EXPONENTIAL SYNCHRONIZATION; GENERALIZED SYNCHRONIZATION; ADAPTIVE SYNCHRONIZATION; STABILITY; CONVERGENCE	This paper is concerned with the modified function projective synchronization of Cohen-Grossberg neural networks systems with parameter mismatch and mixed time-varying delays. Due to the existence of parameter mismatch between the drive and slave systems, complete modified function projective synchronization is not possible to achieve. So a new concept, viz., weak modified function projective synchronization, is discussed up to a small error bound. Several generic criteria are derived to show weak modified function projective synchronization between the systems. The estimation of error bound is done using matrix measure and Halanay inequality. Simulation results are proposed graphically for different particular cases to show the synchronization between parameter-mismatched systems, which validate the effectiveness of our proposed theoretical results.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7321	7332		10.1007/s00521-019-04227-4													
J								Generative image completion with image-to-image translation	NEURAL COMPUTING & APPLICATIONS										Image completion; Generative adversarial networks; U-net		Though many methods have been proposed, image completion still remains challenge; besides textured patterns completion, it often requires high-level understanding of scenes and objects being completed. More recently, deep convolutional generative adversarial networks have been turned into an efficient tool for image completion. Manually specified transformation methods are having been replaced with training neural nets. Hand-engineered loss calculations for training the generator are replaced by the loss function provided by the discriminator. With existing deep learning-based approaches, image completion results in high quality but may still lack high-level feature details or contain artificial appearance. In our completion architecture, we leverage a fully convolutional generator with two subnetworks as our basic completion approach and divide the problem into two steps: The first subnetwork generates the outline of a completed image in a new domain, and the second subnetwork translates the outline to a visually realistic output with image-to-image translation. The feedforward fully convolutional network can complete images with holes of any size at any location. We compare our method with several existing ones on representative datasets such as CelebA, ImageNet, Places2 and CMP Facade. The evaluations demonstrate that our model significantly improves the completion results.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7333	7345		10.1007/s00521-019-04253-2													
J								Comprehensive learning gravitational search algorithm for global optimization of multimodal functions	NEURAL COMPUTING & APPLICATIONS										Gravitational search algorithm; Optimization; Soft computing; Artificial intelligence	PARTICLE SWARM OPTIMIZATION; CENTRAL FORCE OPTIMIZATION; DIFFERENTIAL EVOLUTION; PARAMETERS	In this paper, a new comprehensive learning gravitational search algorithm (CLGSA) is proposed to enhance the performance of basic GSA. The proposed algorithm is a new kind of intelligent optimization algorithm which has better ability to choose good elements. An intensive comprehensive learning methodology is proposed to enrich the optimization ability of the GSA. The efficiency of the proposed algorithm was evaluated by 28 benchmark functions which have been proposed in IEEE-CEC 2013 sessions. The results are compared with eight state-of-the-art algorithms IPOP, BIPOP, NIPOP, NBIPOP, DE/rand, SPSRDEMMS, SPSO-2011 and GSA. A variety of ways are considered to examine the ability of the proposed technique in terms of convergence ability, success rate and statistical behavior of algorithm over dimensions 10, 30 and 50. Apart from experimental studies, theoretical stability of the proposed CLGSA is also proved. It was concluded that the proposed algorithm performed efficiently with good results.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7347	7382		10.1007/s00521-019-04250-5													
J								Extraction of non-functional requirement using semantic similarity distance	NEURAL COMPUTING & APPLICATIONS										Machine learning; Non-functional requirement; Natural language processing; Semantic similarity; Word2Vec		Functional and non-functional requirements are important equally in software development. Usually, the requirements are expressed in natural languages. The functional and non-functional requirements are written inter-mixed in software requirement document. The extraction of requirement from the software requirement document is a challenging task. Most of the recent studies adopted a supervised learning approach for the extraction of non-functional requirements. However, there is a drawback of supervised learning such as training of model and retrain if the domain changed. The proposed approach manipulates the textual semantic of functional requirements to identify the non-functional requirements. The semantic similarity is calculated based on co-occurrence of patterns in large human knowledge repositories of Wikipedia. This study finds the similarity distance between the popular indicator keywords and requirement statements to identify the type of non-functional requirement. The proposed approach is applied to PROMISE "NFR dataset." The performance of the proposed approach is measured in terms of precision, recall and F-measure. Furthermore, the research applies three pre-processing approaches (traditional, part of speech tagging and word augmentation) to increase the performance of NFR extraction. The proposed approach outperforms the results of existing studies.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7383	7397		10.1007/s00521-019-04226-5													
J								An energy-efficient stable clustering approach using fuzzy-enhanced flower pollination algorithm for WSNs	NEURAL COMPUTING & APPLICATIONS										FPA; EFPA; Fuzzy logic; WSN; Stability period; Network lifetime	WIRELESS SENSOR NETWORKS; ROUTING PROTOCOL	Due to advancement in the technology and need for machine-to-machine connectivity, wireless sensor network (WSN) overplays the role compared to other wireless networks. In this context, different applications based on WSNs need to be executed efficiently in terms of energy and communication. To achieve this, there is a need to collaborate among various devices at various levels. This can be achieved by the grouping of these devices, that is, through the clustering. Clustering-based routing is the most suitable approach to support for load balancing, fault tolerance and reliable communication to prolong performance parameters of WSN. These performance parameters are achieved at the cost of reduced lifetime of cluster head (CH). To overcome such limitations in clustering-based hierarchical approach, efficient CH selection algorithm and optimized routing algorithm are essential to design efficient solution for larger scale networks. In this paper, fuzzy-enhanced flower pollination algorithm-based threshold-sensitive energy-efficient clustering protocol is proposed to prolong the stability period of the network. Analysis and simulation results show that the proposed algorithm significantly outperforms competitive clustering algorithms in the context of energy consumption, stability period and system lifetime.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7399	7419		10.1007/s00521-019-04251-4													
J								Network design for resilience in supply chains using novel crazy elitist TLBO	NEURAL COMPUTING & APPLICATIONS										Supply network design; Resilience; Decision-making; TLBO	LEARNING-BASED OPTIMIZATION; PI CONTROLLER-DESIGN; MULTIOBJECTIVE OPTIMIZATION; PARAMETER OPTIMIZATION; ALGORITHM APPROACH; MODEL; DISRUPTIONS; RISK; UNCERTAINTY; LOCATION	A resilient plant location model is proposed in this research and has been evaluated for a case problem. The model considers three major indicators of network resilience, viz. node density, node complexity and node criticality. A resilient design could ensure for cost efficiency, apart from that the likelihood of potential disruptions due to bottlenecks could be minimized. The results were optimized using a novel crazy elitist TLBO algorithm. The algorithm has been presented to solve the case problem and has been pretested for a constrained and unconstrained test function. A multi-objective decision-making model has been constructed with the flow of products as variables and was effectively solved using the meta-heuristic. The solution to the case brings insights into the design of supply network for resilience, and the managers are recommended to incorporate the concepts of resilience from the design phase itself.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7421	7437		10.1007/s00521-019-04260-3													
J								A new method for time series classification using multi-dimensional phase space and a statistical control chart	NEURAL COMPUTING & APPLICATIONS										Time series; Quality control charts; Classification; Phase space reconstruction; Dynamic time warping	EMPIRICAL MODE DECOMPOSITION; INDUCTION-MOTORS; FAULT-DIAGNOSIS; RECONSTRUCTION; RECOGNITION; FRAMEWORK; MACHINE	Since large amounts of data were collected over time in many different areas, the classification of these data according to their similarities was an important problem. The methods used to classify time series are a combination of classifiers in different domains such as time, autocorrelation, frequency spectrum, and phase space. The weakest point of these methods is that they require high computational burden and the obtained features lead to misclassifications. When the phase space of the time series is modeled by the Gaussian mixture model, different conditions can be easily classified. However, this technique fails when the phase spaces of time series representing different conditions are similar. In this study, a new method for time series classification using multi-dimensional phase space is proposed using quality control charts that constructed from the phase space of a time series. It aims obtaining a new feature signal from the phase space, providing a faster method for classification of time series, and effectively detecting minor changes in time series. The method is consisted of six stages such as time series inputs, selecting an appropriate time delay and embedding dimension for each time series, construction of phase space, obtaining new time series from phase space using T-2 control chart, alignment of time series with dynamic time warping, and classification with the nearest neighbor. The constructed time series is guaranteed to be a complete representation of a system where the phase space parameters are properly chosen. With the proposed new representation, the time series that belongs to different classes and whose phase spaces are similar can be easily distinguished. The k-nearest neighbor classifier is implemented for time series classification, and the datasets from two different domains are used for validation, including motor current signals and nine benchmark datasets from the UCR time series repository. The results show that the proposed method enhances the time series classification performance with new time series representation across these diverse domains.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7439	7453		10.1007/s00521-019-04270-1													
J								Computationally efficient MPC for path following of underactuated marine vessels using projection neural network	NEURAL COMPUTING & APPLICATIONS										Underactuated marine vessels; Model predictive control; Projection neural network; Path following	MODEL-PREDICTIVE CONTROL; LINE-OF-SIGHT; TRACKING CONTROL; SURFACE VESSELS; SHIPS; CONTROLLER; DESIGN	A practical model predictive control (MPC) for path following of underactuated marine vessels, which is a representative marine application, is presented in this paper. Taking advantage of the capability of dealing with multivariable system and input saturation, the MPC method is used to transform the underactuated control problem into the optimization problem with incorporation of input (rudder) constraints. Considering the implementation obstacle of solving optimization problem formulated by the MPC method efficiently, the projection neural network, which is known as parallel computational capability, is employed here to improve the computational efficiency. The full information of ship motion is normally difficult to obtain directly due to the lack of enough measurements; therefore, the state observer is also included. A simple linear model represented the main dynamics of path following of underactuated marine vessels is conceived as predictive (control design) model; meanwhile, in order to demonstrate the effectiveness of proposed control design, all the comparative studies are conducted on a nonlinear high-fidelity simulation model. The simulation results validate that the proposed control design is effective and efficient.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7455	7464		10.1007/s00521-019-04273-y													
J								A new multi-criteria group decision-making approach based on q-rung orthopair fuzzy interaction Hamy mean operators	NEURAL COMPUTING & APPLICATIONS										q-Rung orthopair fuzzy set; Hamy mean; q-Rung orthopair fuzzy interaction Hamy mean; Multi-attribute group decision-making	INTERACTION AGGREGATION OPERATORS; SIMILARITY MEASURES; SETS; EXTENSION; TOPSIS	The recently proposed q-rung orthopair fuzzy set (q-ROFS) is a powerful and effective tool to describe uncertainty and vagueness, and Hamy mean (HM) has a significant advantage of capturing the interrelationship among aggregated arguments. In order to take full advantage of q-ROFS and HM, and consider the interactions between membership and non-membership degrees at the same time, in this paper, we propose a family of q-rung orthopair fuzzy Hamy mean operators based on interaction operations. First, we define interaction operational rules for q-rung orthopair fuzzy numbers. Based on the new operational rules, q-rung orthopair fuzzy interaction HM and q-rung orthopair fuzzy interaction weighted HM operators are proposed. Further, we propose a dual Hamy mean (DHM) operator and extend it to accommodate q-rung orthopair fuzzy environment. Based on interaction operational rules and DHM, q-rung orthopair fuzzy interaction DHM operator and its weighted form are also developed. Then, a novel multi-attribute group decision-making approach based on proposed operators is introduced. Finally, a numerical instance, as well as some comparative analyses, is provided to illustrate the validity and advantages of the new approach.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7465	7488		10.1007/s00521-019-04269-8													
J								Deep joint two-stream Wasserstein auto-encoder and selective attention alignment for unsupervised domain adaptation	NEURAL COMPUTING & APPLICATIONS										Domain adaptation; Two-stream Wasserstein auto-encoder; Selective attention alignment		Domain adaptation refers to the process of utilizing the labeled source domain data to learn a model that can perform well in the target domain with limited or missing labels. Several domain adaptation methods combining image translation and feature alignment have been recently proposed. However, there are two primary drawbacks of such methods. First, the majority of the methods assume that synthetic target images have the same distribution as real target images, and thus, only the synthetic target images are employed for training the target classifier, which makes the model's performance significantly dependent on the quality of the generated images. Second, most of the methods blindly align the discriminative content information by merging spatial and channel-wise information, thereby ignoring the relationships among channels. To address these issues, a two-step approach that joints two-stream Wasserstein auto-encoder (WAE) and selective attention (SA) alignment, named J2WSA, is proposed in this study. In the pre-training step, the two-stream WAE is employed for mapping the four domains to a shared nice manifold structure by minimizing the Wasserstein distance between the distribution of each domain and the corresponding prior distribution. During the fine-tuning step, the SA alignment model initialized by the two-stream WAE is applied for automatically selecting the style part of channels for alignment, while simultaneously suppressing the content part alignment using the SA block. Extensive experiments indicate that the combination of the aforementioned two models can achieve state-of-the-art performance on the Office-31 and digital domain adaptation benchmarks.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7489	7502		10.1007/s00521-019-04262-1													
J								Single-image de-raining using low-rank matrix approximation	NEURAL COMPUTING & APPLICATIONS										Image de-raining; Nuclear norm; Low-rank matrix; Matrix approximation	REMOVAL; SPARSE	Most existing image de-raining methods are based on the assumption that rain has certain shape or chromatic priors, which may be inaccurate for some rain cases and may lead to unsatisfactory de-rained results. In this paper, the proposed image de-raining method is built on the priors of clear images that many self-similar patches exist in natural images. And similar patches satisfy the low-rank property when grouped together. This property has led to many powerful image/video denoising schemes with impressive performance but has never been used for single-image de-raining. We formulate the problem of removing rain as a low-rank matrix approximation problem by adopting this self-similarity image prior with a designed patch matching strategy applicable for rain images. The resulting nuclear norm minimization problem can be efficiently solved by many recently developed methods. Experiments on both real rain images and synthetic rain images show that the proposed method is competitive against other state-of-the-art methods.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7503	7514		10.1007/s00521-019-04271-0													
J								Fabrication of a portable device for stress monitoring using wearable sensors and soft computing algorithms	NEURAL COMPUTING & APPLICATIONS										Machine learning; Stress; Photoplethysmography; Galvanic skin response; Portable device; Emotion recognition; Physiological signals; Arousal detection	HEART-RATE-VARIABILITY; MENTAL STRESS; TIME; ANALYZE; SYSTEM	Stress is an issue that everyone experiences in today's modern life. Prolonged exposure to stress can cause many mental and physical diseases. Accordingly, the stress management issue has become popular, and the need for personal healthcare devices has increased in recent years. Therefore, the aim of this research is to design and manufacture a portable stress monitoring system, based on photoplethysmography (PPG) and galvanic skin response (GSR) physiological signals, acquired by wearable sensors. To do so, we proposed a novel algorithm for continuous measurement of the stress index (SI) as well as the classification of stress levels. In order to estimate an accurate value for SI, various soft computing algorithms such as support vector regression, artificial neural networks (ANN), and adaptive neuro-fuzzy inference system (ANFIS) were adopted for modeling the stress based on the features extracted from normalized and non-normalized types of PPG and GSR signals and their combinations. Furthermore, K-nearest neighbor (KNN), ANNs, Naive Bayes, and support vector machine (SVM) were utilized to discriminate different levels of stress in subjects. The obtained results indicate that the ANFIS algorithm can estimate the SI training output with the correlation coefficient (CC) of 0.9281 and the average relative error of 0.23 on a subset of the combined features of PPG and GSR signals. Also, the best classification performance was for KNN (K=3) algorithm, with 85.3% accuracy. To evaluate the developed system, data of 16 subjects, out of the training dataset, participated in the experiment in the presence of the experts and psychologists, were used. The average CC of 0.81 and classification accuracy of 75% were obtained, using the implemented ANFIS model and KNN classifier.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7515	7537		10.1007/s00521-019-04278-7													
J								Local bit-plane decoded convolutional neural network features for biomedical image retrieval	NEURAL COMPUTING & APPLICATIONS										Biomedical images; Convolutional neural networks; AlexNet; Image retrieval; Local bit-plane decoding	FEATURE DESCRIPTOR; PATTERNS; CLASSIFICATION; ROTATION; MRI	Biomedical image retrieval is a challenging problem due to the varying contrast and size of structures in the images. The approaches for biomedical image retrieval generally rely on the feature descriptors to characterize the images. The feature descriptor of query image is compared with the descriptors of images from the database, to find the best matches. Several hand-crafted feature descriptors have been proposed so far for biomedical image retrieval by exploiting the local relationship of neighboring image pixels. It is observed in the literature that the local bit-plane decoded features are well suited for this retrieval task. Moreover, in recent past, it is also observed that the convolutional neural network-based features such as AlexNet, Vgg16, GoogleNet and ResNet perform well in many computer vision-related tasks. Motivated by the success of the deep learning-based approaches, this paper proposes a local bit-plane decoding-based AlexNet descriptor (LBpDAD) for biomedical image retrieval. The proposed LBpDAD is computed by max-fusing the ReLU operated feature maps of pre-trained AlexNet at a particular layer, obtained from the original and local bit-plane decoded images. The proposed approach is also compared with Vgg16, GoogleNet and ResNet models. The experiments on the proposed method over three benchmark biomedical databases of different modalities such as MRI, CT and microscopic show the efficacy of the proposed descriptor.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7539	7551		10.1007/s00521-019-04279-6													
J								DenseNet with Up-Sampling block for recognizing texts in images	NEURAL COMPUTING & APPLICATIONS										Chinese recognition; DenseNet; Up-Sampling block; Feature information restorement	NETWORKS	The Convolutional Recurrent Neural Networks (CRNN) have achieved a great success for the study of OCR. But existing deep models usually apply the down-sampling in pooling operation to reduce the size of features by dropping some feature information, which may cause the relevant characters with small occupancy rate to be missed. Moreover, all hidden layer units in the cyclic module need to be connected in cyclic layer, which may result in a heavy computation burden. In this paper, we explore to improve the results potentially using Dense Convolutional Network (DenseNet) to replace the convolution network of the CRNN to connect and combine multiple features. Also, we use the up-sampling function to construct an Up-Sampling block to reduce the negative effects of down-sampling in pooling stage and restore the lost information to a certain extent. Thus, informative features can also be extracted with deeper structure. Besides, we also directly use the output of inner convolution parts to describe the label distribution of each frame to make the process efficient. Finally, we propose a new OCR framework, termed DenseNet with Up-Sampling block joint with the connectionist temporal classification, for Chinese recognition. Results on Chinese string dataset show that our model delivers the enhanced performance, compared with several popular deep frameworks.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7553	7561		10.1007/s00521-019-04285-8													
J								A data ensemble approach for real-time air quality forecasting using extremely randomized trees and deep neural networks	NEURAL COMPUTING & APPLICATIONS										Machine learning; Urban air quality; Atmospheric chemistry; Statistical analysis; Extra-trees method	SIMULATED NOX; ALGORITHMS; EMISSIONS; IMPACT; MODEL; O-3	Six generalized machine learning (ML) ensemble models were developed to predict the real-time hourly ozone concentration of the following day. These models were used to forecast hourly ozone concentrations of the following day for all of 2017 in the city of Seoul, South Korea. To prepare the training dataset, it was referred to observed meteorology and air pollution parameters of the 2014-2016 period. The ensemble models fuse two regression models: a low-ozone peak model and a high-ozone model. For both, extremely randomized trees and deep neural networks were used. A regularization approach was also adopted that adjusts the model toward capturing higher ozone peaks by resampling the training dataset based on the peaks. Adopting the proposed ML ensemble forecasting method over single-model ML techniques as a part of mainstream practice for air quality forecasting will be beneficial for several reasons. For one, the proposed method, which captures daily maximum ozone concentrations during the high-ozone season (April-September), reduces the ozone peak prediction error by 5 to 30 ppb. In addition, compared to station-specific (independent) ML models with more frequent low-ozone values, models are trained with a uniformly distributed dataset, so they are more generalizable in nature. As a result, unlike station-specific models, they retain their accuracy (yearly IOA=0.84-0.89) in all stations with an IOA increment. Proposed models also make predictions several times faster, requiring only one-time training for predicting an entire station network. Based on a categorical analysis of the training dataset, an algorithm was proposed for selecting the most suitable model for each month. The "best" model further improves the accuracy of both the ML ensemble and individual models by up to 2.4%. This study shows that the ML ensemble modeling approach is a fast, reliable, and robust technique that can benefit environmental decision-makers in urban regions.																	0941-0643	1433-3058				JUN	2020	32	11			SI		7563	7579		10.1007/s00521-019-04287-6													
J								New optimization algorithms for neural network training using operator splitting techniques	NEURAL NETWORKS										Neural network; MNIST; CIFAR10; Splitting; Nesterov; Dynamical system		In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems. Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						178	190		10.1016/j.neunet.2020.03.018													
J								Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics	NEURAL NETWORKS										Time series forecasting; RNN, LSTM, GRU; Reservoir Computing; Kuramoto-Sivashinsky; Lorenz-96; Complex systems	TIME-SERIES; LYAPUNOV EXPONENTS; SYSTEMS; MPI	We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing (RC) and Backpropagation through time (BPTT) for gated network architectures. We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures. We quantify the relative prediction accuracy of these algorithms for the long-term forecasting of chaotic systems using as benchmarks the Lorenz-96 and the Kuramoto-Sivashinsky (KS) equations. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and in capturing of the long-term statistics, while at the same time requiring much less training time. However, in the case of reduced order data, large scale RC models can be unstable and more likely than the BPTT algorithms to diverge. In contrast, RNNs trained via BPTT show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the KS equation with BPTT, achieving similar accuracy as RC. This study establishes that RNNs are a potent computational framework for the learning and forecasting of complex spatiotemporal systems. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						191	217		10.1016/j.neunet.2020.02.016													
J								Brain MRI analysis using a deep learning based evolutionary approach	NEURAL NETWORKS										3D-CNN; Genetic algorithm; Deep learning; Interpretable classifier; Brain MRI classification	AUTISM SPECTRUM DISORDER; MULTI-ATLAS SEGMENTATION; FUNCTIONAL CONNECTIVITY; CLASSIFICATION; DEPRESSION; DIAGNOSIS; SYMPTOMS; CHILDREN; VOLUME; MODEL	Convolutional neural network (CNN) models have recently demonstrated impressive performance in medical image analysis. However, there is no clear understanding of why they perform so well, or what they have learned. In this paper, a three-dimensional convolutional neural network (3D-CNN) is employed to classify brain MRI scans into two predefined groups. In addition, a genetic algorithm based brain masking (GABM) method is proposed as a visualization technique that provides new insights into the function of the 3D-CNN. The proposed GABM method consists of two main steps. In the first step, a set of brain MRI scans is used to train the 3D-CNN. In the second step, a genetic algorithm (GA) is applied to discover knowledgeable brain regions in the MRI scans. The knowledgeable regions are those areas of the brain which the 3D-CNN has mostly used to extract important and discriminative features from them. For applying GA on the brain MRI scans, a new chromosome encoding approach is proposed. The proposed framework has been evaluated using ADNI (including 140 subjects for Alzheimer's disease classification) and ABIDE (including 1000 subjects for Autism classification) brain MRI datasets. Experimental results show a 5-fold classification accuracy of 0.85 for the ADNI dataset and 0.70 for the ABIDE dataset. The proposed GABM method has extracted 6 to 65 knowledgeable brain regions in ADNI dataset (and 15 to 75 knowledgeable brain regions in ABIDE dataset). These regions are interpreted as the segments of the brain which are mostly used by the 3D-CNN to extract features for brain disease classification. Experimental results show that besides the model interpretability, the proposed GABM method has increased final performance of the classification model in some cases with respect to model parameters. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						218	234		10.1016/j.neunet.2020.03.017													
J								Learning in the machine: To share or not to share?	NEURAL NETWORKS										Deep learning; Convolutional neural networks; Weight-sharing; Biologically plausible architectures	NEURAL-NETWORKS; DROPOUT	Weight-sharing is one of the pillars behind Convolutional Neural Networks and their successes. However, in physical neural systems such as the brain, weight-sharing is implausible. This discrepancy raises the fundamental question of whether weight-sharing is necessary. If so, to which degree of precision? If not, what are the alternatives? The goal of this study is to investigate these questions, primarily through simulations where the weight-sharing assumption is relaxed. Taking inspiration from neural circuitry, we explore the use of Free Convolutional Networks and neurons with variable connection patterns. Using Free Convolutional Networks, we show that while weight-sharing is a pragmatic optimization approach, it is not a necessity in computer vision applications. Furthermore, Free Convolutional Networks match the performance observed in standard architectures when trained using properly translated data (akin to video). Under the assumption of translationally augmented data, Free Convolutional Networks learn translationally invariant representations that yield an approximate form of weight-sharing. (c) 2020 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	0893-6080	1879-2782				JUN	2020	126						235	249		10.1016/j.neunet.2020.03.016													
J								Multi-way backpropagation networks for training compact deep neural networks	NEURAL NETWORKS										Backpropagation; Supervision vanishing; Compact model		Depth is one of the key factors behind the success of convolutional neural networks (CNNs). Since ResNet (He et al., 2016), we are able to train very deep CNNs as the gradient vanishing issue has been largely addressed by the introduction of skip connections. However, we observe that, when the depth is very large, the intermediate layers (especially shallow layers) may fail to receive sufficient supervision from the loss due to severe transformation through long backpropagation path. As a result, the representation power of intermediate layers can be very weak and the model becomes very redundant with limited performance. In this paper, we first investigate the supervision vanishing issue in existing backpropagation (BP) methods. And then, we propose to address it via an effective method, called Multi-way BP (MW-BP), which relies on multiple auxiliary losses added to the intermediate layers of the network. The proposed MW-BP method can be applied to most deep architectures with slight modifications, such as ResNet and MobileNet. Our method often gives rise to much more compact models (denoted by "Mw+Architecture") than existing methods. For example, MwResNet-44 with 44 layers performs better than ResNet-110 with 110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models even outperform the light models obtained by state-of-the-art model compression methods. Last, our method inherently produces multiple compact models with different depths at the same time, which is helpful for model selection. Extensive experiments on both image classification and face recognition demonstrate the superiority of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						250	261		10.1016/j.neunet.2020.03.001													
J								Crowding in humans is unlike that in convolutional neural networks	NEURAL NETWORKS										Convolutional neural networks; Object recognition; Crowding	OBJECT RECOGNITION; SPATIAL INTERACTION; MASKING	Object recognition is a primary function of the human visual system. It has recently been claimed that the highly successful ability to recognise objects in a set of emergent computer vision systems-Deep Convolutional Neural Networks (DCNNs)-can form a useful guide to recognition in humans. To test this assertion, we systematically evaluated visual crowding, a dramatic breakdown of recognition in clutter, in DCNNs and compared their performance to extant research in humans. We examined crowding in three architectures of DCNNs with the same methodology as that used among humans. We manipulated multiple stimulus factors including inter-letter spacing, letter colour, size, and flanker location to assess the extent and shape of crowding in DCNNs. We found that crowding followed a predictable pattern across architectures that was different from that in humans. Some characteristic hallmarks of human crowding, such as invariance to size, the effect of target-flanker similarity, and confusions between target and flanker identities, were completely missing, minimised or even reversed. These data show that DCNNs, while proficient in object recognition, likely achieve this competence through a set of mechanisms that are distinct from those in humans. They are not necessarily equivalent models of human or primate object recognition and caution must be exercised when inferring mechanisms derived from their operation. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						262	274		10.1016/j.neunet.2020.03.021													
J								Learning visual features under motion invariance	NEURAL NETWORKS										Convolutional networks; Invariance of visual features; Information-based learning; Neural differential equations; Principle of least cognitive action	SLOW FEATURE ANALYSIS; FUNCTIONAL ARCHITECTURE; RECEPTIVE-FIELDS	Humans are continuously exposed to a stream of visual data with a natural temporal structure. However, most successful computer vision algorithms work at image level, completely discarding the precious information carried by motion. In this paper, we claim that processing visual streams naturally leads to formulate the motion invariance principle, which enables the construction of a new theory of learning that originates from variational principles, just like in physics. Such principled approach is well suited for a discussion on a number of interesting questions that arise in vision, and it offers a well-posed computational scheme for the discovery of convolutional filters over the retina. Differently from traditional convolutional networks, which need massive supervision, the proposed theory offers a truly new scenario for the unsupervised processing of video signals, where features are extracted in a multi-layer architecture with motion invariance. While the theory enables the implementation of novel computer vision systems, it also sheds light on the role of information-based principles to drive possible biological solutions. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						275	299		10.1016/j.neunet.2020.03.013													
J								Extracting boolean and probabilistic rules from trained neural networks	NEURAL NETWORKS										Neural networks; Boolean functions; Rule extraction; Dynamic programming		This paper presents two approaches to extracting rules from a trained neural network consisting of linear threshold functions. The first one leads to an algorithm that extracts rules in the form of Boolean functions. Compared with an existing one, this algorithm outputs much more concise rules if the threshold functions correspond to 1-decision lists, majority functions, or certain combinations of these. The second one extracts probabilistic rules representing relations between some of the input variables and the output using a dynamic programming algorithm. The algorithm runs in pseudo-polynomial time if each hidden layer has a constant number of neurons. We demonstrate the effectiveness of these two approaches by computational experiments. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						300	311		10.1016/j.neunet.2020.03.024													
J								Automata complete computation with Hodgkin-Huxley neural networks composed of synfire rings	NEURAL NETWORKS										Neural computation; Recurrent neural networks; Cell assemblies; Synfire rings; Hodgkin-Huxley equations; Finite state automata	SYNCHRONOUS SPIKING; ASSOCIATIVE MEMORY; EXPRESSIVE POWER; DYNAMICS; ANALOG; MODEL; PROPAGATION; LANGUAGE; CHAINS; TERMS	Synfire rings are neural circuits capable of conveying synchronous, temporally precise and selfsustained activities in a robust manner. We propose a cell assembly based paradigm for abstract neural computation centered on the concept of synfire rings. More precisely, we empirically show that Hodgkin-Huxley neural networks modularly composed of synfire rings are automata complete. We provide an algorithmic construction which, starting from any given finite state automaton, builds a corresponding Hodgkin-Huxley neural network modularly composed of synfire rings and capable of simulating it. We illustrate the correctness of the construction on two specific examples. We further analyze the stability and robustness of the construction as a function of changes in the ring topologies as well as with respect to cell death and synaptic failure mechanisms, respectively. These results establish the possibility of achieving abstract computation with bio-inspired neural networks. They might constitute a theoretical ground for the realization of biological neural computers. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						312	334		10.1016/j.neunet.2020.03.019													
J								Multi-view projected clustering with graph learning	NEURAL NETWORKS										Multi-view; Subspace learning; Clustering; Feature selection; Local structure		Graph based multi-view learning is well known due to its effectiveness and good clustering performance. However, most existing methods directly construct graph from original high-dimensional data which always contain redundancy, noise and outlying entries in real applications, resulting in unreliable and inaccurate graph. Moreover, they do not effectively select some useful features which are important for graph learning and clustering. To solve these limits, we propose a novel model that combines dimensionality reduction, manifold structure learning and feature selection into a framework. We map high-dimensional data into low-dimensional space to reduce the complexity of the algorithm and reduce the effect of noise and redundance. Therefore, we can adaptively learn a more accurate graph. Further more, l(21)-norm regularization is adopted to adaptively select some important features which help improve clustering performance. Finally, an efficiently algorithm is proposed to solve the optimal solution. Extensive experimental results on some benchmark datasets demonstrate the superiority of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						335	346		10.1016/j.neunet.2020.03.020													
J								A fast conformal predictive system with regularized extreme learning machine	NEURAL NETWORKS										Conformal predictive system; Cross-conformal predictive system; Cumulative distribution function; Asymptotic validity; Regularized extreme learning machine	REGRESSION; KERNEL	A conformal predictive system(CPS) is based on the learning framework of conformal prediction, which outputs cumulative distribution functions(CDFs) for labels in regression problems. The CDFs output by a CPS provide useful information for users, as they not only provide probability for the events related to the test labels, but also can be transformed to prediction intervals with the corresponding quantiles. Moreover, CPSs have the property of validity since the distributions and intervals they output have statistical compatibility with the realizations. This property is very useful for many risk-sensitive applications such as financial time series forecast and weather forecast. However, as based on conformal predictors, CPSs inherit the computational issue. To build a fast CPS, in this paper, we propose a CPS with regularized extreme learning machine as the underlying algorithm. To be specific, we combine the leave-one-out cross-conformal predictive system(Leave-One-Out CCPS), a variant of the original CPS, with regularized extreme learning machine(RELM), which is named as LOO-CCPS-RELM. We analyse the computational complexity of it and prove its asymptotic validity based on some regularity assumptions. We also prove that the error rate of the prediction interval output by LOO-CCPS-RELM is under control in the asymptotic setting. Experiments with 20 public data sets were conducted to test LOO-CCPS-RELM and the results showed that LOO-CCPS-RELM is empirically valid and compared favourably with the other CPSs. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						347	361		10.1016/j.neunet.2020.03.022													
J								Hyperlink regression via Bregman divergence	NEURAL NETWORKS										Hypernetwork; Bregman divergence; Neural network; Graph embedding	LINK-PREDICTION; ROBUST; CLASSIFICATION; CONVERGENCE; 1ST	A collection of U (is an element of N) data vectors is called a U-tuple, and the association strength among the vectors of a tuple is termed as the hyperlink weight, that is assumed to be symmetric with respect to permutation of the entries in the index. We herein propose Bregman hyperlink regression (BHLR), which learns a user-specified symmetric similarity function such that it predicts the tuple's hyperlink weight from data vectors stored in the U-tuple. BHLR is a simple and general framework for hyper-relational learning, that minimizes Bregman-divergence (BD) between the hyperlink weights and estimated similarities defined for the corresponding tuples; BHLR encompasses various existing methods, such as logistic regression (U = 1), Poisson regression (U = 1), link prediction (U = 2), and those for representation learning, such as graph embedding (U = 2), matrix factorization (U = 2), tensor factorization (U >= 2), and their variants equipped with arbitrary BD. Nonlinear functions (e.g., neural networks), can be employed for the similarity functions. However, there are theoretical challenges such that some of different tuples of BHLR may share data vectors therein, unlike the i.i.d. setting of classical regression. We address these theoretical issues, and proved that BHLR equipped with arbitrary BD and U is an element of N is (P-1) statistically consistent, that is, it asymptotically recovers the underlying true conditional expectation of hyperlink weights given data vectors, and (P-2) computationally tractable, that is, it is efficiently computed by stochastic optimization algorithms using a novel generalized minibatch sampling procedure for hyper-relational data. Consequently, theoretical guarantees for BHLR including several existing methods, that have been examined experimentally, are provided in a unified manner. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						362	383		10.1016/j.neunet.2020.03.026													
J								Automatic detection of tympanic membrane and middle ear infection from oto-endoscopic images via convolutional neural networks	NEURAL NETWORKS										Otoscope; Tympanic membrane; Otitis media; Convolutional neural networks	DIABETIC-RETINOPATHY; OTOSCOPY; VALIDATION; SYSTEM; SKILLS	Convolutional neural networks (CNNs), a popular type of deep neural network, have been actively applied to image recognition, object detection, object localization, semantic segmentation, and object instance segmentation. Accordingly, the applicability of deep learning to the analysis of medical images has increased. This paper presents a novel application of state-of-the-art CNN models, such as DenseNet, to the automatic detection of the tympanic membrane (TM) and middle ear (ME) infection. We collected 2,484 oto-endoscopic images (OEIs) and classified them into one of three categories: normal, chronic otitis media (COM) with TM perforation, and otitis media with effusion (OME). Our results indicate that CNN models have significant potential for the automatic recognition of TM and ME infections, demonstrating a competitive accuracy of 95% in classifying TM and middle ear effusion (MEE) from OEIs. In addition to accuracy measurement, our approach achieves nearly perfect measures of 0.99 in terms of the average area under the receiver operating characteristics curve (AUROC). All these results indicate robust performance when recognizing TM and ME effusions in OEIs. Visualization through a class activation mapping (CAM) heatmap demonstrates that our proposed model performs prediction based on the correct region of OEIs. All these outcomes ensure the reliability of our method; hence, the study can aid otolaryngologists and primary care physicians in real-world scenarios. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUN	2020	126						384	394		10.1016/j.neunet.2020.03.023													
J								Anomaly detection in surveillance video based on bidirectional prediction	IMAGE AND VISION COMPUTING										Anomaly detection; Bidirectional prediction; Sliding window; U-Net	ABNORMAL EVENT DETECTION; HISTOGRAMS	With the development of information technology and the popularization of monitoring network, how to quickly and automatically detect abnormal behaviors in surveillance video is becoming more and more important for public security and smart city. The emergence of deep learning has greatly promoted the development of anomaly detection and much remarkable work has been presented on this topic. However, the existing approaches for anomaly detection generally encounter problems such as insufficient utilization of motion patterns and instability on different datasets. To improve the performance of anomaly detection in surveillance video, we propose a framework based on bidirectional prediction, which predicts the same target frame by the forward and the backward prediction subnetworks, respectively. Then the loss function is constructed based on the real target frame and its bidirectional prediction frame. Furthermore, we also propose an anomaly score estimation method based on the sliding window scheme which focuses on the foregrounds of the prediction error map. The comparison with the state-of-the-art shows that the proposed model outperforms most competing models on different video surveillance datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103915	10.1016/j.imavis.2020.103915													
J								Learning visual variation for object recognition	IMAGE AND VISION COMPUTING										Object recognition; Multi-task learning; Convolutional neural network		We propose visual variation learning to improve object recognition with convolutional neural networks (CNN). While a typical CNN regards visual variations as nuisances and marginalizes them from the data, we speculate that some variations are informative. We study the impact of visual variation as an auxiliary task, during training only, on classification and similarity embedding problems. To train the network, we introduce the iLab-20M dataset, a large-scale controlled parametric dataset of toy vehicle objects under systematic annotated variations of viewpoint, lighting, focal setting, and background. After training, we strip out the network components related to visual variations, and test dassification accuracy on images with no visual variation labels. Our experiments on 1.75 million images from Rab-20M show significant improvement in object recognition accuracy, i.e., AlexNet: 84.49% to 91.15%; ResNet: 86.14% to 90.70%; and DenseNet: 85.56% to 91.55%. Our key contribution is that, at the cost of visual variation annotation during training only, CNN enhanced with visual variation learning is able to focus its attention on distinctive features and learn better object representations, reducing classification error rate of Alexnet by 42%, ResNet by 32%, and DenseNet by 41%, without significant sacrificing of training time and model complexity. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103912	10.1016/j.imavis.2020.103912													
J								A contextual conditional random field network for monocular depth estimation	IMAGE AND VISION COMPUTING										Monocular depth estimation; Deep neural network; Skip connection; Conditional random field		Monocular depth estimation plays a crucial role in understanding 3D scene geometry and is a challenging computer vision task. Recently, deep convolutional neural networks have been applied to solve this problem. However, existing methods either directly exploiting RGB pixels which can introduce much noise into the depth map or utilizing over smoothed internal representation features which can cause blur in the depth map. In this paper, we propose a contextual CRF network (CCN) to tackle these issues. The new CCN adopts the popular encoder-decoder architecture with a new contextual CRF module (CCM) which is guided by the depth features and regularizes the information flow from the encoder layer to the corresponding layer in the decoder, thus can reduce the mismatch between the RGB pixel and the depth map cue while at the same time retain detail features to output a fine-grained depth map. Moreover, we propose a depth-guided loss function which pays a balanced attention to near and far pixels thus addressing the long-tailed distribution of depth information. We have conducted extensive experiments on three public datasets for monocular depth estimation. Results demonstrate that our proposed CCN achieves superior performances in terms of visual quality and competitive quantitative results when compared with state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103922	10.1016/j.imavis.2020.103922													
J								Convolutional prototype learning for zero-shot recognition	IMAGE AND VISION COMPUTING										Zero-shot recognition; Prototype learning; Image recognition; Deep learning		Zero-shot learning (ZSL) has received increasing attention in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. The key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary class attribute vectors. However, the popularly learned projection functions in previous works cannot generalize well since they assume the distribution consistency between seen and unseen domains at sample-level. Besides, the provided non-visual and unique class attributes can significantly degrade the recognition performance in semantic space. In this paper, we propose a simple yet effective convolutional prototype learning (CPL) framework for zero-shot recognition. By assuming distribution consistency at task-level, our CPL is capable of transferring knowledge smoothly to recognize unseen samples. Furthermore, inside each task, discriminative visual prototypes are learned via a distance based training mechanism. Consequently, we can perform recognition in visual space, instead of semantic space. An extensive group of experiments are then carefully designed and presented, demonstrating that CPL obtains more favorable effectiveness, over currently available alternatives under various settings. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103924	10.1016/j.imavis.2020.103924													
J								EDS pooling layer	IMAGE AND VISION COMPUTING										Feature pooling layer; Convolutional neural network; Deep learning; Object recognition		Convolutional neural networks (CNNs) have been the source of recent breakthroughs in many vision tasks. Feature pooling layers are being widely used in CNNs to reduce the spatial dimensions of the feature maps of the hidden layers. This gives CNNs the property of spatial invariance and also results in speed-up and reduces overfitting. However, this also causes significant information loss. All existing feature pooling layers follow a one-step procedure for spatial pooling, which affects the overall performance due to significant information loss. Not much work has been done to do efficient feature pooling operation in CNNs. To reduce the loss of information at this critical operation of the CNNs, we propose a new EDS layer (Expansion Downsampling learnable-Scaling) to replace the existing pooling mechanism. We propose a two-step procedure to minimize the information loss by increasing the number of channels in pooling operation. We also use feature scaling in the proposed EDS layer to highlight the most relevant channels/feature-maps. Our results show a significant improvement over the generally used pooling methods such as MaxPool, AvgPool, and StridePool (strided convolutions with stride >1). We have done the experiments on image classification and object detection task. ResNet-50 with our proposed EDS layer has performed comparably to ResNet-152 with stride pooling on the ImageNet dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103923	10.1016/j.imavis.2020.103923													
J								Energy clustering for unsupervised person re-identification	IMAGE AND VISION COMPUTING										Person re-identification; Fully unsupervised method; Hierarchical clustering; Energy distance		Due to the high cost of data annotation in supervised person re-identification (re-ID) methods, unsupervised methods become more attractive in the real world. Recently, the hierarchical clustering serves as a promising unsupervised method. One key factor of hierarchical clustering is the distance measurement strategy. Ideally, a good distance measurement should consider both inter-cluster and intra-cluster distance of all samples. To solve this problem, we propose to use the energy distance to measure inter-cluster distance in hierarchical clustering (E-cluster), and use the sum of squares of deviations (SSD) as a regularization term to measure intra-cluster distance for further performance promotion. We evaluate our method on Market-1501 and DukeMTMC-reID. Extensive experiments show that E-cluster obtains significant improvements over the state-of-the-arts fully unsupervised methods. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JUN	2020	98								103913	10.1016/j.imavis.2020.103913													
J								Fast Cloud-Paillier homomorphic schemes for protecting confidentiality of sensitive data in cloud computing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing (CC); Cloud-Paillier scheme; Confidentiality; Homomorphic encryption (HE); Additive homomorphism (AH); Fast decryption; Chinese remainder theorem (CRT)	ACCESS-CONTROL; ENCRYPTION; RSA; COMPUTATION; PRIVACY; DESIGN	With homomorphic encryption (HE), data can be processed in its encrypted form in the cloud computing (CC). This HE property can be considered as a useful solution to get over some concerns limiting the widespread adoption of CC services. Nevertheless, since CC environments are threatened by outsider and insider security attacks and since cloud consumers oftentimes access to CC services using resource-limited devices, the HE schemes need to be promoted in terms of security level and running time to work effectively. In El Makkaoui et al. ( 2016 international conference on big data and advanced wireless technologies, BDAW 2016, 2016b), we boosted the main Paillier's scheme at security level by proposing a variant of the scheme called Cloud-Paillier. The proposed scheme addresses an exception of the Paillier's scheme, supports the additive homomorphism over the integers and withstands more confidentiality attacks. For fast decryption, herein, we propose two fast variants of the Cloud-Paillier scheme. The proposed variants use moduli formed of k = 2 distinct primes. The first variant utilizes the Chinese remainder theorem to decrypt. Whereas, the second variant sightly modifies the from of the Cloud-Paillier's encryption algorithm and decrypts as in the Cloud-Paillier. Theoretical and simulation outcomes show that the suggested variants give a large decryption speed-up over the Cloud-Paillier while preserving a recommended security level.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2205	2214		10.1007/s12652-019-01366-3													
J								A game-theoretic approach for channel security against active time-varying attacks based on artificial noise	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Active time-varying attacks; Artificial noise; Stackelberg game; Stackelberg equilibrium; Channel security	ALLOCATION	To penetrate sensitive communication systems, attackers can attack the channel using an active time-varying (ATV) way, which will lead to a great information loss. The conventional approach is to encrypt the original signal making it difficult for attackers to get information. However, this technology is constrained by the limited wireless terminal equipment. In this paper, we choose to insert artificial noise into the channel, which aims at disturbing the attackers and reducing the loss of the system once attacks occur. However, this technology would produce some side effects and there is a tradeoff between inserting artificial noise and minimizing information loss. In this paper, we deal with this issue and propose a game-theoretic framework to minimize the total losses. We model the problem as a Stackelberg security game between the attacker and the defender. Furthermore, we propose a novel method to reduce the searching space of computing the Strong Stackelberg Equilibrium which is the optimal defense strategy. This algorithm reduces a M-dimensional problem to M 1-dimensional problems so that the complexity is lowered. The experimental results show that our proposed algorithm significantly outperforms other non-strategic strategies in terms of decreasing the total losses against ATV attacks.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2215	2224		10.1007/s12652-019-01350-x													
J								Adaptive packet scheduling in IoT environment based on Q-learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Packet scheduling; Gateway; Q-learning; Markov decision process	SCHEME; NETWORKS; INTERNET; QOS	In the internet of things (IoT) environment consisting of various devices the traffic condition dynamically changes. Failure to process them in complying with the QoS requirement can significantly degrade the reliability and quality of the system. Therefore, the gateway collecting the data needs to quickly establish a new scheduling policy according to the changing traffic condition. The traditional packet scheduling schemes are not effective for IoT since the data transmission pattern is not identified in advance. Q-learning is a type of reinforcement learning that can establish a dynamic scheduling policy without any prior knowledge on the network condition. In this paper a novel Q-learning scheme is proposed which updates the Q-table and reward table based on the condition of the queues in the gateway. Computer simulation reveals that the proposed scheme significantly increases the number of packets satisfying the delay requirement while decreasing the processing time compared to the existing scheme based on Q-learning with stochastic learning automaton. And the processing time is also minimized by omitting unnecessary computation steps in selecting the action in the iterative Q-learning operations.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2225	2235		10.1007/s12652-019-01351-w													
J								Toward a user-centric classification scheme for extended reality paradigms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mixed reality; Augmented reality; Extended reality; Classification; 3iVClass; User-centric	VIRTUAL-REALITY; MIXED REALITY	For several years, augmented and virtual reality technologies have attracted increasing interest in all areas. In the midst of this universe, the concept, already well known, of mixed reality has established itself as a distinct paradigm. However, and contrary to augmented and virtual realities, there is not a clear and standard definition of what it is exactly, what it is not and why it is different from the other paradigms. In this paper, we attempt to provide a new user-centered classification scheme to standardize the definition of virtual, augmented and mixed realities. First, a quick overview of existing taxonomies of augmented, virtual realities is made, then we present our user-centered classification which is based on three criteria we called 3iVClass (immersion, interaction, information). After that, a weighting system is proposed, the objective of this method is to give a simple way for anyone, expert or not, to quickly and simply characterize or define the best user experiences. Finally, to test our classification, we tried to characterize different user experiences through the study of two use cases.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2237	2249		10.1007/s12652-019-01352-9													
J								An effective hyper-dense deployment algorithm via search economics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile communication; Deployment problem; Metaheuristic algorithm; Search economics	5G; ARCHITECTURE	Developing an effective strategy for deploying base stations of a mobile communication environment has been a critical issue for years because it typically needs to take into account several conflict factors, such as coverage ratio and interference. Since 5G cellular services are expected to be commercially available in 2020, a "good deployment strategy" for the hyper-dense deployment problem (HDDP) has attracted the attention of researchers from different disciplines in recent years. To enhance the performance of a 5G mobile communication environment, an effective search algorithm for solving the HDDP, called search economics for hyper-dense deployment problem (SE-HDDP), is presented in this paper. A distinctive feature of the proposed algorithm is that it divides the search space into a set of subspaces and dynamically allocates the computing resources to these subspaces based on their potentials during the convergence process. The simulation results show that the proposed algorithm is able to find a better result of HDDP for a 5G mobile communication environment than all the other metaheuristic and rule-based algorithms compared in this paper.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2251	2262		10.1007/s12652-019-01353-8													
J								Facilitating research through serendipity of recommendations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Serendipity; Transparency; User interface; Recommender system	SYSTEM	Recommender systems are used to suggest items that are useful to users. The recommendations can be surprising and may be categorized as serendipitous recommendations. One of the limitations with serendipitous recommendations is that the user interface of such a recommender system rarely supports the user to switch from accuracy orientation to serendipity facilitations. Using serendipitous recommendations can be challenging. This is because the user might not fully benefit from and understand the serendipitous recommendations. One main advantage of this type of system is that a serendipity-oriented recommender system can be used for the supervision of research students. It can help them to find a novel topic in the area of their research interests. This paper reports on a novel user interface design for facilitating serendipitous recommendations generation in educational environments. The user interface of this recommender system provides students with user controls and visualization in order to explore research articles. This research comprises user experience experiments conducted in an academic environment and evaluated by means of a user centered design evaluation. It involves research articles recommender system named JabRef. The recommender systems were used by students at the undergraduate level. Users reported an enhanced user experience while using the user controls and visualization and serendipitous resource discovery. It was found that user interface design can facilitate a serendipity recommender system in the learning environment. University professors supervising students during the research can also benefit from the recommender system.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2263	2275		10.1007/s12652-019-01354-7													
J								What data are smartphone users willing to share with researchers? Designing and evaluating a privacy model for mobile data collection apps	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ubiquitous computing; Context-aware computing; Mobile computing; Psychometrics; Sensor data; Privacy	PERSONALITY-TRAITS	Context-aware applications stemming from diverse fields like mobile health, recommender systems, and mobile commerce potentially benefit from knowing aspects of the user's personality. As filling out personality questionnaires is tedious, we propose the prediction of the user's personality from smartphone sensor and usage data. In order to collect data for researching the relationship between smartphone data and personality, we developed the Android app track your daily routine (TYDR), which tracks and records smartphone data and utilizes psychometric personality questionnaires. With TYDR, we track a larger variety of smartphone data than many other existing apps, including metadata on notifications, photos taken, and music played back by the user. Based on the development of TYDR, we introduce a general context data model consisting of four categories that focus on the user's different types of interactions with the smartphone: physical conditions and activity, device status and usage, core functions usage, and app usage. On top of this, we developed the Privacy Model for Mobile Data Collection Applications (PM-MoDaC) specifically tailored for apps that are related to the collection of mobile data, consisting of nine proposed privacy measures. We present the implementation of all of those measures in TYDR. Our experimental evaluation is based on data collected with TYDR during a two-month period. We find evidence that our users accept our proposed privacy model. Based on data about granting TYDR all or no Android system permissions, we find evidence that younger users tend to be less willing to share their data (average age of 30 years compared to 35 years). We also observe that female users tend to be less willing to share data compared to male users. We did not find any evidence that education or personality traits are a factor related to data sharing. TYDR users score higher on the personality trait openness to experience than the average of the population, which we assume to be evidence that the type of app influences the user base it attracts in terms of average personality traits.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2277	2289		10.1007/s12652-019-01355-6													
J								Design and performance evaluation of cost-effective function-distributed mobility management scheme for software-defined smart factory networking	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Function-distributed mobility management; Future industrial internet; Proxy mobile IP; Software-defined networks; Smart factory networking	5G; IMPLEMENTATION; OPTIMIZATION	With the proliferation of mobile devices, Internet of Things (IoT), and edge commuting, and the advancement of ICT, mobile computing has become the norm, rather than the exception. Current network technologies and distributed system architectures make it difficult to meet users' network trace, and incur high processing costs. To solve this problem, this paper proposes new functional distributed mobility management technologies for the application of the industrial future Internet. This implies the need for an efficient distributed mobility system to apply 5G or TSN (time sensitive network) technologies to the future industry Internet. Thus, it is assumed that various analytical techniques will be applied to provide superior performance in cost analyses, such as location update costs and packet transport costs. In this paper, we propose a cost-effective function-distribution mobility management system that will build a future industrial Internet to meet the needs and convenience of users. It also uses computer simulation to create the mobility provision system, and the electiveness of the cost analysis usage and rental management function models. Software-defined networking in smart factories (SFs) enables them to easily adapt the communication network to changing requirements. Similar to cloud-based systems, such SFs could be seen as producing clusters that could be rented and configured as needed. To achieve the required flexibility, the SF network uses software defined networking combined with network function virtualization. Despite the fact that the technology is not yet ready for deployment in today's manufacturing networks, a novel network architecture for SFs based on software-defined networking and network virtualization is here proposed, to support smart services, especially for Industry 4.0.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2291	2307		10.1007/s12652-019-01356-5													
J								A hybrid GPU-FPGA based design methodology for enhancing machine learning applications performance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; High performance computing; Heterogeneous computing; Hybrid platform; GPU computing; FPGA computing; CNN; DNN; Model converting; PUE		The high-density computing requirements of machine learning (ML) is a challenging performance bottleneck. Limited by the sequential instruction execution system, traditional general purpose processors are not suitable for efficient ML. In this work, we present an ML system design methodology based on GPU and FPGA to tackle this problem. The core idea of our proposal is when designing an ML platform, we leverage the graphics processing unit (GPU)'s high-density computing to perform model training and exploit field programmable gate array (FPGA)'s low-latency to perform model inferencing. In between, we define a model converter, which enable transforming the model used by the training module to one that is used by inferencing module. We evaluated our approach through two use cases. The first is a handwritten digit recognition with convolutional neural network while the second use case is for predicting data center's power usage effectiveness with deep neural network regression algorithm. The experimental results indicate that our solution can take advantages of GPU and FPGA's parallel computing capacity to improve the efficiency of training and inferencing significantly. Meanwhile, the solution preserves the accuracy and the mean square error while converting the models between the different frameworks.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2309	2323		10.1007/s12652-019-01357-4													
J								EyeCom: an IoT based affordable wearable solution for paralyzed people to interact with machines	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Wearable technology		The world is innovating rapidly, and there is a need for continuous interaction with technology. Sadly, there do not exist promising options for paralyzed people to interact with the machines, i.e., laptops, smartphones, and tabs. A few commercial solutions such as Google Glasses are costly and cannot be afforded by every paralyzed person for such interaction. Towards this end, the paper proposes a retina-controlled device called EyeCom. The proposed device is constructed from off-the-shelf cost-effective yet robust IoT devices (i.e., Arduino microcontrollers, Xbee wireless sensors, IR diodes, and accelerometer). The device can easily be mounted on to the glasses; the paralyzed person using this device can interact with the machine using simple head movement and eye blinks. The IR detector is located in front of the eye to illuminate the eye region. As a result of illumination, the eye reflects IR light which includes electrical signals. As the eyelids close, the reflected light over the eye surface is disrupted, and the change in reflected value is recorded. Further to enable cursor movement onto the computer screen for the paralyzed person a device named accelerometer is used. The accelerometer is a small device, with the size of phalanges, a human thumb bone. The device operates on the principle of axis-based motion sensing and it can be worn as a ring by a paralyzed person. A microcontroller processes the inputs from the IR sensors, accelerometer and transmits them wirelessly via Xbee wireless sensor (i.e., a radio) to another microcontroller attached to the computer. With the help of a proposed algorithm, the microcontroller attached to the computer, on receiving the signals moves the cursor onto the computer screen and facilitate performing actions, as simple as opening a document to operating a word-to-speech software. EyeCom has features which can help paralyzed persons to continue their contributions towards the technological world and become an active part of the society. Resultantly, they will be able to perform a number of tasks without depending upon others from as simple as reading a newspaper on the computer to activate word-to-voice software.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2325	2336		10.1007/s12652-019-01358-3													
J								An improved energy efficient system for IoT enabled precision agriculture	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy efficient systems; Internet of things (IoT); Duty cycling; Precision agriculture		Energy efficiency in wireless sensor network is a well studied research problem. A recent research study on IoT based precision agriculture has reported that designing the energy efficient data aggregation at the base station is an open research problem. The paper is motivated by recent developments in the energy efficient models and algorithms to maintain energy requirement at the base station. The base station is energy constrained device and it has to survive maintaining the energy requirements from various sensors and gateway modules. Therefore base station uses energy harvesting to maintain energy neutrality. There are two important research questions to study; how to estimate the solar energy to maintain the energy neutrality at the base station having solar backup for energy harvesting and how to estimate the energy requirements at the base station at random point of time. The first question has been answered quite extensively. In this paper, we are trying to address the second question of how to estimate the energy requirements at the base station of IoT enabled precision agriculture. In precision agriculture, numerous types of sensors including soil, moisture, temperature, wind direction, wind speed, camera, drone etc. are used to continuously monitor the field and connect to the base station. Hence base station has different modes of power requirements such as low power, medium power, and high power modes based on different types of wireless communication medium at random point of time. The paper proposes a novel product density model to estimate the energy requirements at the base station. Moreover, an Improved Duty Cycling algorithm is proposed using residual energy parameter. The performance of the proposed Improved Duty Cycling is compared with two other algorithms and the proposed algorithm shows an improvement in terms of average energy consumption, residual energy performance, throughput etc.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2337	2348		10.1007/s12652-019-01359-2													
J								Image gradient orientations embedded structural error coding for face recognition with occlusion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Unconstrained face recognition; Face occlusion; Image gradient orientations; Structural error coding; Markov random field	REPRESENTATION; DECOMPOSITION; REGRESSION	Partially occluded faces are very common in automatic face recognition (FR) in the real world. We explore the problem of FR with occlusion by embedding Image Gradient Orientations (IGO) into robust error coding. The existing works usually put stress on the error distribution in the non-occluded region but neglect the one in the occluded region due to its unpredictability incurred by irregular occlusion. However, in the IGO domain, the error distribution in the occluded region can be built simply and elegantly by a uniform distribution in the interval [- , ), and the one in the occluded region can be well built by a weight-conditional Gaussian distribution. By incorporating the two error distributions and a Markov random field for the priori distribution of the occlusion support, we propose a joint probabilistic generative model for a novel IGO-embedded Structural Error Coding (IGO-SEC) model. Two methods, a new reconstruction method and a new robust structural error metric, are further presented to boost the performance of IGO-SEC. Extensive experiments on 8 popular robust FR methods and 4 benchmark face databases demonstrate the effectiveness and robustness of IGO-SEC in dealing with facial occlusion and occlusion-like variations.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2349	2367		10.1007/s12652-019-01257-7													
J								CNN-SkelPose: a CNN-based skeleton estimation algorithm for clinical applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Skeleton model; CNN; Clinical application; Actigraphy	MOTION ANALYSIS; DELIRIUM; ACTIGRAPHY; SLEEP	Computer vision based patient activity monitoring systems can be attractive for various unobtrusive clinical applications. Such a monitoring system can be developed using movement information derived from the skeleton model of the current body pose, e.g. obtained using a depth camera. Earlier research using estimated skeleton models have been focused mostly on gaming applications. In this paper, we propose CNN-SkelPose as a skeleton model estimation method for clinical applications. CNN-SkelPose uses a trained Convolutional Neural Network to extract both the local and global information from the depth image. CNN-SkelPose outperforms the baseline model of Skeltrack for reliable skeleton model estimation in patient monitoring scenarios. Our results show the inadequacy of existing methods for skeleton model estimation when applied to a clinical scenario and suggests CNN-SkelPose as an improvement towards this application.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2369	2380		10.1007/s12652-019-01259-5													
J								A novel method for failure mode and effects analysis using fuzzy evidential reasoning and fuzzy Petri nets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Failure mode and effects analysis (FMEA); Fuzzy evidential reasoning; Fuzzy Petri net (FPN); Ship fire-safety system	SHAFER EVIDENCE THEORY; KNOWLEDGE REPRESENTATION; CRITICALITY ANALYSIS; DECISION-MAKING; RISK-EVALUATION; FMECA; PRIORITIZATION; SYSTEM	Failure mode and effects analysis (FMEA) has been broadly used in various industries to ensure the safety and reliability of high-risk systems. As a meritorious risk management tool, it can identify, evaluate and eliminate potential failure modes in a system for remedial actions. Nevertheless, the traditional FMEA has suffered from many deficiencies, especially in the assessment of failure modes, the weighting of risk factors, and the calculation of RPN. Therefore, this paper presents a novel FMEA method based on fuzzy evidential reasoning and fuzzy Petri nets (FPNs) to improve the classical FMEA. In this model, belief structures are used to capture the uncertainty and fuzziness of the subjective assessments given by experts and a rule-based FPN model is established to determine the risk priority of the failure modes identified in FMEA. An empirical case concerning the risk evaluation of a ship fire-safety system is provided to illustrate the practicality and effectiveness of the proposed FMEA. The results show that the new risk assessment method can produce more reliable risk ranking results of failure modes.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2381	2395		10.1007/s12652-019-01262-w													
J								Adaptable QoS provisioning for efficient traffic-to-resource control in software defined wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Quality of service; Software defined networking; Software defined wireless sensor networks; QoS path selection and resource-associating		Realizing efficient network control requires intelligent methods of network management, especially for sensitive network data. Different network types implement diverse methods to control and administer network traffic as well as effectively manage network resources. As with wireless sensor networks (WSNs), communication traffic and network resource control are performed depending on independently employed mechanisms to deal with events occurring on different levels of the network. It is therefore challenging to realize efficient network performance with guaranteed quality of service (QoS) in WSNs, given their computing limitations. Software defined networking (SDN) carries the potential to improve and evolve WSNs in terms of capacity and application. A means to apply SDN strategies to these compute-limited WSNs, formulates software defined wireless sensor networks (SDWSN). This work proposes a QoS Path Selection and Resource-associating (Q-PSR) scheme for adaptive load balancing and intelligent resource control for optimal network performance. Our experimental results indicate better performance in terms of computation with load balancing and efficient resource alignment for different networking tasks when compared with other competing schemes.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2397	2405		10.1007/s12652-019-01263-9													
J								An effective real time gender recognition system for smart cameras	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart camera; Gender recognition; Gender recognition from video; Real-time; Face analysis; Video; Embedded vision	EMBEDDED VISION SYSTEM; IDENTIFICATION; CLASSIFICATION; PEOPLE; FUSION	In recent years we have assisted to a growing interest for embedded vision, due to the availability of low cost hardware systems, effective for energy consumption, flexible for their size at the cost of limited (compared to the server) computing resources. Their use is boosted by the simplicity of their positioning in places where energy or network bandwidth is limited. Smart cameras are digital cameras embedding computer systems able to host video applications; due to the cost and the performance, they are progressively gaining popularity and conquering large amount of the market. Smart cameras are now able to host on board video applications, even if this imposes an heavy reformulation of the algorithms and of the software design so as to make them compliant with the limited CPUs and the small RAM and flash memory (typically of a few megabytes). In this paper we propose a method for gender recognition on video sequences, specifically designed for making it suited to smart cameras; although the algorithm uses very limited resources (in terms of RAM and CPU), it is able to run on smart cameras available today, presenting at the same time an high accuracy on unrestricted videos taken in real environments (malls, shops, etc.).																	1868-5137	1868-5145				JUN	2020	11	6			SI		2407	2419		10.1007/s12652-019-01267-5													
J								Adaptive technique for PQ analysis in renewable sources with grid integrated SSFC	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PQ; PV; Micro grid; WT; FACTS; SSFC; PID controller; CFA; RNN algorithm	SERIES COMPENSATOR SSSC; CONTROL STRATEGY; POWER-SYSTEM; HYBRID; ALGORITHM; STABILIZATION; OPTIMIZATION; IMPROVEMENT; CONTROLLER; MANAGEMENT	In power quality (PQ) problems, the flexible AC transmission system (FACTS) devices have been developed the real power and reducing the PQ issues of the power system. Here the PQ analysis in photovoltaic (PV), wind turbine (WT) system with grid integrated static switched filter compensator (SSFC) is controlled by adaptive technique. The adaptive technique works with the combined process of Cuttle fish algorithm (CFA) and recurrent neural network (RNN) algorithm. The proposed adaptive technique is controlling the irregular switching process of the shunt capacitor banks of a tuned arm filter in SSFC device. The novelty of the proposed technique is improved for energy efficiency and demand management in grid utilization process. Here a power injection model of SSFC is considered to control the real power flow control in terms of grid utilization. The switching process is controlled by balancing control signal generated with the help of pulse width modulation (PWM) process. To modulate the PWM is used by two regulators, which is based on a tri-loop real error managed combined weighted adjusted proportional integral derivative controller. The proposed SSFC system is designed and implemented in MATLAB/Simulink platforms and the performance is validated with some existing devices such as unified power flow controller and SSFC devices.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2421	2434		10.1007/s12652-019-01283-5													
J								A fuzzy logic approach to influence maximization in social networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social networks; Community detection; Influence propagation; Fuzzy logic		Within a community, social relationships are paramount to profile individuals' conduct. For instance, an individual within a social network might be compelled to embrace a behaviour that his/her companion has recently adopted. Such social attitude is labelled social influence, which assesses the extent by which an individual's social neighbourhood adopt that individual's behaviour. We suggest an original approach to influence maximization using a fuzzy-logic based model, which combines influence-weights associated with historical logs of the social network users, and their favourable location in the network. Our approach uses a two-phases process to maximise influence diffusion. First, we harness the complexity of the problem by partitioning the network into significantly-enriched community-structures, which we then use as modules to locate the most influential nodes across the entire network. These key users are determined relatively to a fuzzy-logic based technique that identifies the most influential users, out of which the seed-set candidates to diffuse a behaviour or an innovation are extracted following the allocated budget for the influence campaign. This way to deal with influence propagation in social networks, is different from previous models, which do not compare structural and behavioural attributes among members of the network. The performance results show the validity of the proposed partitioning-approach of a social network into communities, and its contribution to "activate" a higher number of nodes overall. Our experimental study involves both empirical and real contemporary social-networks, whereby a smaller seed set of key users, is shown to scale influence to the high-end compared to some renowned techniques, which employ a larger seed set of key users and yet they influence less nodes in the social network.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2435	2451		10.1007/s12652-019-01286-2													
J								Developing FIA(5) to FSTPR25 for modeling spatio-temporal relevancy in context-aware wayfinding systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Context-awareness; Spatio-temporal relevancy; Customization; Spatio-temporal prism; Fuzzy interval algebra; Tourist	FUZZY-LOGIC	Wayfinding or leading a moving user from an origin to a target is one of the main research focuses in urban context-aware systems. Space and time are two dominant properties of the context-aware wayfinding process and spatio-temporal relevancy between the fixed urban entities and the moving users determine whether an entity is related to the moving user or not. This paper specifically concentrates on the development of customized fuzzy interval algebra (FIA(5)) for detecting spatio-temporally relevant contexts to the user. This paper integrates fuzzy spatial and temporal intervals and customizes the spatio-temporal relations between the new data models-called fuzzy spatio temporal prism relevancy (FSTPR25) model-based on Allen's fuzzy multi interval algebra. In this implementation, the FSTPR25 helps the tourist to find his/her preferred areas that are spatio-temporally relevant with two optimistic and pessimistic strategies. The experimental results in a scenario of tourist navigation are evaluated with respect to the accuracy of the model in 450 iterations of the algorithm in 15 different routes based on the statistical quantifiers in Tehran, Iran. The evaluation process demonstrated the high accuracy and user satisfaction of the optimistic strategy in real-world applications.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2453	2466		10.1007/s12652-019-01287-1													
J								Interactive spaces for children: gesture elicitation for controlling ground mini-robots	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Elicitation study; Participatory design; Natural user interface; Child computer interaction; Interactive space; Robot	DEFINED BODY GESTURES; NATURAL INTERFACE; YOUNG-CHILDREN; HUMANOID ROBOT; DESIGN; PLAY; GAME	Interactive spaces for education are emerging as a mechanism for fostering children's natural ways of learning by means of play and exploration in physical spaces. The advanced interactive modalities and devices for such environments need to be both motivating and intuitive for children. Among the wide variety of interactive mechanisms, robots have been a popular research topic in the context of educational tools due to their attractiveness for children. However, few studies have focused on how children would naturally interact and explore interactive environments with robots. While there is abundant research on full-body interaction and intuitive manipulation of robots by adults, no similar research has been done with children. This paper therefore describes a gesture elicitation study that identified the preferred gestures and body language communication used by children to control ground robots. The results of the elicitation study were used to define a gestural language that covers the different preferences of the gestures by age group and gender, with a good acceptance rate in the 6-12 age range. The study also revealed interactive spaces with robots using body gestures as motivating and promising scenarios for collaborative or remote learning activities.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2467	2488		10.1007/s12652-019-01290-6													
J								Using Therbligs to embed intelligence in workpieces for digital assistive assembly	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart workpiece; Smart factory; Industry 4; 0; Assembly operation		Current OEM (Original Equipment Manufacturer) facilities tend to be highly integrated and are often situated on one site. While providing scale of production such centralisation may create barriers to the achievement of fully flexible, adaptable, and reconfigurable factories. The advent of Industry 4.0 opens up opportunities to address these barriers by decentralising information and decision-making in manufacturing systems through CPS (Cyber Physical Systems) use. This research presents a qualitative study that investigates the possibility of distributing information and decision-making logic into 'smart workpieces' which can actively participate in assembly operations. To validate the concept, a use-case demonstrator, corresponding to the assembly of a 'flat-pack' table, was explored. Assembly parts in the demonstrator, were equipped with computation, networking, and interaction capabilities. Ten participants were invited to evaluate the smart assembly method and compare its results to the traditional assembly method. The results showed that in its current configuration the smart assembly was slower. However, it made the assembly process more flexible, adaptable and reconfigurable.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2489	2503		10.1007/s12652-019-01294-2													
J								An immune clone selection based power control strategy for alleviating energy hole problems in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster-based coronal model; Energy hole problem; Immune clone selection; Power control; Transmission range adjustment; Wireless sensor network	TRANSMISSION RANGE ADJUSTMENT; LIFETIME; EFFICIENT; ARCHITECTURE; ALGORITHM; PROTOCOL	In wireless sensor networks (WSNs), the creation of energy holes is extremely difficult to be avoided because the data flow usually follows a many-to-one and multi-hop pattern. Since energy holes exhaust their energy faster than other nodes, network partitions might be created, which might lead to failure of the network. Cluster-based WSNs have been widely used because of their good performance, and power control strategies are an effective way to improve energy efficiency in WSNs. In this paper, we first propose a power-based energy consumption model and a cluster-based coronal model for analyzing the energy hole problem in WSNs. Then, on the basis of the proposed models, we investigate the feasibility and effectiveness of the existing approaches for solving the energy hole problem in WSNs. Furthermore, an immune clone selection-based power control (ICSPC) strategy for alleviating the energy hole problem in WSNs is proposed. In the ICSPC strategy, the immune clone selection algorithm is used to optimize the transmission ranges of sensors in various coronas to balance the energy consumption rates of the coronas. Finally, simulation results are analyzed to show that the energy hole problem in WSNs has been largely alleviated by the ICSPC strategy, and the network lifetime is greatly prolonged.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2505	2518		10.1007/s12652-019-01300-7													
J								A fault monitoring approach using model-based and neural network techniques applied to input-output feedback linearization control induction motor	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Induction motor (IM); Input-output feedback linearization (IOFL) control; Fault monitoring; Residual speed; Neural network (NN); Hilbert transform (HT)	DIRECT TORQUE CONTROL; BROKEN ROTOR BARS; TOLERANT CONTROL; DIAGNOSIS; DRIVE; SPEED; SIMULATION; TRANSFORM; OBSERVERS; HILBERT	This paper presents a contribution to the fault monitoring approach and input-output feedback linearization control of the induction motor (IM) in the closed-loop drive. Two kinds of faults are considered in the machine, particularly the broken rotor bars and stator inter-turn short circuit faults. This approach has been employed to detect and identify simple and mixed defects during motor operation by utilizing advanced techniques. To achieve it, two procedures are applied for the fault monitoring: The model-based strategy, which used to generate a residual speed signal to indicate the presence of possible failures, by means the high gain observer in the closed-loop drive. However, this strategy is not able to recognise the type of faults but it can be affected by the disturbances. Therefore, the neural network (NN) technique is applied in order to identify the faults and distinguish them. However, the NN required a relevant database to achieve satisfactory results. Hence, the stator current analysis based on the HFFT combination of the Hilbert transform and fast Fourier transform is applied to extract the amplitude of the harmonics and used them as an input data set for NN. The obtained results show the efficiency of the fault monitoring system and its ability to detect and diagnosis any minor faults in a closed loop of the IM.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2519	2538		10.1007/s12652-019-01307-0													
J								Maximizing the earned benefit in an incentivized social networking environment: a community-based approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Social network; Influence maximization; Earned benefit; Influence probability; Effective nodes; Community structure	INFLUENCE MAXIMIZATION; ALGORITHM; SPREAD	Given a social network of users represented as a directed graph with edge weight as diffusion probability, the Social Influence Maximization Problem asks for selecting a set of highly influential users for initial activation to maximize the influence in the network. In this paper, we study a variant of this problem, where nodes are associated with a selection cost signifying the incentive demand; a fixed budget is allocated for the seed set selection process; a subset of the nodes is designated as the target nodes, and each of them is associated with a benefit value that can be earned by influencing the corresponding target user; and the goal is to choose a seed set within the allocated budget for maximizing the earned benefit. Formally, we call this problem as the Earned Benefit Maximization in Incentivized Social Networking Environment or Earned Benefit Maximization Problem (EBM Problem), in short. For this problem, we develop a priority-based ranking methodology having three steps. First, marking the effective nodes for the given target nodes; second, priority computation of the effective nodes and the third is to choose the seed nodes based on this priority value within the budget. We implement the proposed methodology with two publicly available social network datasets and observe that the proposed methodology can achieve more benefit compared to the baseline methods. To improve the proposed methodology, we exploit the community structure of the network. Experimental results show that the incorporation of community structure helps the proposed methodology to achieve more benefit without much increase in computational burden.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2539	2555		10.1007/s12652-019-01308-z													
J								Shannon entropy and fuzzy C-means weighting for AI-based diagnosis of vertebral column diseases	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vertebral column diseases; Herniated disk; Spondylolisthesis; Fuzzy C-means; Shannon entropy	CLASSIFICATION; CLASSIFIERS	Degenerative vertebral column diseases are becoming increasingly common and computer-aided decision-making and diagnosis systems are gaining popularity. In this paper, we propose a machine learning decision-making model based on noninvasive panoramic radiographs to tackle the problem of automated diagnosis of two common vertebral column diseases; disc prolapse and spondylolisthesis. We collected raw data from real X-ray images of 422 subjects (i.e., 201 disc prolapse, 111 spondylolisthesis, and 110 healthy). We used five biomechanical parameters as input to the model representing the pelvic incidence, pelvic tilt, lumbar lordosis angle, sacral slope, and degree spondylolisthesis. To obtain more meaningful features, we preprocessed each vertebral column dataset by weighting every vertebral feature using a set of weights computed based on Shannon entropy and the fuzzy C-means clustering algorithm. Then, the new weighted set of features was fed to an artificial neural network classifier. Our proposed method was able to classify the subjects into three classes with 99.5% overall accuracy. This reflects a strong ability to predict the patient vertebral column dysfunction using the biomechanical attributes and with an accuracy satisfying clinical requirements. This approach represents a feasible system that facilitates the diagnosis of vertebral column disorders. It can help the physician to take the correct decision very early, which will prevent the development of the pathology into a chronic level and reduce the need for surgical treatment.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2557	2566		10.1007/s12652-019-01312-3													
J								FGAF-CDG: fuzzy geographic routing protocol based on compressive data gathering in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Compressive sensing; Compressive data gathering; Fuzzy based routing; Wireless sensor networks	ENERGY-EFFICIENT; CLUSTERING-ALGORITHM	In the wireless sensor networks (WSNs), energy consumption is one of the significant factors. Most of the energy in a WSN is consumed by communication between nodes. To minimize energy consumption, routing protocols can be merged with data aggregation techniques. The geographic adaptive fidelity (GAF) protocol is one of the prominent geographic routing protocols which is proposed in order to reduce energy consumption in WSNs. Moreover, compressive sensing (CS) theory presented as an alternative method for data gathering in WSNs, known as compressive data gathering (CDG). CDG reduces the cost of communications and balances the energy load in the network without imposing heavy computation or transmission overhead. With CDG, instead of receiving all readings from the sensors, the sink may receive few weighted sums of all the readings by which original data can be recovered by the sink. In this paper, we propose a GAF-based routing protocol based on CDG technique named fuzzy GAF based on CDG (FGAF-CDG). In this work, we partition the sensors area into virtual hexagonal grid cells firstly and then we lay the cells according to their geographic locations. In each sampling round, cluster head (CH) sensor in each grid cell is selected based on a fuzzy logic-based algorithm. Then, CH readings will be forwarded to the sink in a multi-hop path based on a fuzzy-based routing algorithm in the CDG form. Simulation results show that the proposed method results in superior efficiency as compared to other competitive GAF-based methods. For example, the proposed model offers about 50% reduction in energy consumption as compared to FTGAF-HEX method depending on the dimensions of the sensors area.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2567	2589		10.1007/s12652-019-01314-1													
J								Analysis of a retrial queue with group service of impatient customers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Markovian arrival process; Phase-type distribution of the service time; Group service; Impatient customers; Retrial	MARKOVIAN ARRIVAL PROCESS; SYSTEM; MODEL; TIME	A single-server retrial queue with a MAP flow, PH service times and a pool of finite capacity for accumulation of the customers and their group service is considered. Service to the next group is not provided until the number of customers in the pool will reach a certain predefined threshold value. The service time of a group depends on its size and it is less than the sum of the individual service times. The dependencies of the basic performance measures of the system on the capacity of the pool and the threshold are obtained. Numerical results are presented.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2591	2599		10.1007/s12652-019-01318-x													
J								Shilling attack detection in binary data: a classification approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Shilling attack; Detection; Collaborative filtering; Classification; Binary ratings	DEFENDING RECOMMENDER SYSTEMS; PROFILE INJECTION ATTACKS; UNSUPERVISED METHOD; MODEL	Reliability of a recommender system is extremely substantial for the continuity of the system. Malicious users may harm the reliability of predictions by injecting fake profiles called shilling attacks into the system. Therefore, the detection of such attacks is vital for a recommender system. Thus, many shilling attack detection methods have been studied. However, the proposed solutions work only on numerical rating based recommender systems. On the other hand, it has been shown that collaborative filtering systems utilizing binary ratings are also vulnerable to shilling attacks. In this work, we propose a detection method, which finds out six well-known shilling attack models against binary ratings-based collaborative filtering systems. Besides deriving generic attributes from user profiles, we generate additional model-specific attributes in order to deal with fake profiles. Our empirical results show that the proposed method successfully detects attack profiles even with low attack size and filler size values.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2601	2611		10.1007/s12652-019-01321-2													
J								Covariance of uncertain random variables and its application to portfolio optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chance theory; Uncertain random variable; Chance distribution; Covariance; Portfolio selection; Mean-variance model	ENTROPY; DEFINITION; MOMENTS	Covariance is a device to measure the joint variability of two uncertain random variables. If the greater values of one uncertain random variable associated with the greater values of the other uncertain random variable, and the same state holds for the lesser values, i.e., the uncertain random variables have similar behavior, the covariance is positive. On the other hand, when the greater values of one uncertain random variable associate with the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. Since, interpretation of covariance is not easy, we consider the concept of correlation coefficient for two uncertain random variables as a normalized version of covariance. For calculating the covariance of uncertain random variables, some formulas are provided through the inverse uncertainty distribution. As an application of variance-covariance, portfolio selection problem is optimized by mean-variance model. The main results are explained by using several examples.																	1868-5137	1868-5145				JUN	2020	11	6			SI		2613	2624		10.1007/s12652-019-01323-0													
J								Self-assembly of 3-D structures using 2-D folding tiles	NATURAL COMPUTING										Self-assembly; 3-Dimensional; Folding; Reconfiguration	DNA; COMPLEXITY	Self-assembly is a process which is ubiquitous in natural, especially biological systems. It occurs when groups of relatively simple components spontaneously combine to form more complex structures. While such systems have inspired a large amount of research into designing theoretical models of self-assembling systems, and even laboratory-based implementations of them, these artificial models and systems often tend to be lacking in one of the powerful features of natural systems (e.g. the assembly and folding of proteins), which is dynamic reconfigurability of structures. In this paper, we present a new mathematical model of self-assembly, based on the abstract Tile Assembly Model (aTAM), called the Flexible Tile Assembly Model (FTAM). In the FTAM, the individual components are 2-dimensional tiles as in the aTAM, but in the FTAM, bonds between the edges of tiles can be flexible, allowing bonds to flex and entire structures to reconfigure, thus allowing 2-dimensional components to form 3-dimensional structures. We analyze the powers and limitations of FTAM systems by (1) demonstrating how flexibility can be controlled to carefully build desired structures, and (2) showing how flexibility can be beneficially harnessed to form structures which can "efficiently" reconfigure into many different configurations and/or greatly varying configurations. We also show that with such power comes a heavy burden in terms of computational complexity of simulation and prediction by proving that for important properties of FTAM systems, determining their existence is intractable, even for properties which are easily computed for systems in less dynamic models.																	1567-7818	1572-9796				JUN	2020	19	2			SI		337	355		10.1007/s11047-019-09751-9													
J								CRN plus plus : Molecular programming language	NATURAL COMPUTING											CHEMICAL IMPLEMENTATION; NETWORKS; DNA	Synthetic biology is a rapidly emerging research area, with expected wide-ranging impact in biology, nanofabrication, and medicine. A key technical challenge lies in embedding computation in molecular contexts where electronic micro-controllers cannot be inserted. This necessitates effective representation of computation using molecular components. While previous work established the Turing-completeness of chemical reactions, defining representations that are faithful, efficient, and practical remains challenging . This paper introduces CRN++, a new language for programming deterministic (mass-action) chemical kinetics to perform computation. We present its syntax and semantics, and build a compiler translating CRN++ programs into chemical reactions, thereby laying the foundation of a comprehensive framework for molecular programming. Our language addresses the key challenge of embedding familiar imperative constructs into a set of chemical reactions happening simultaneously and manipulating real-valued concentrations. Although some deviation from ideal output value cannot be avoided, we develop methods to minimize the error, and implement error analysis tools. We demonstrate the feasibility of using CRN++on a suite of well-known algorithms for discrete and real-valued computation. CRN++ can be easily extended to support new commands or chemical reaction implementations, and thus provides a foundation for developing more robust and practical molecular programs.																	1567-7818	1572-9796				JUN	2020	19	2			SI		391	407		10.1007/s11047-019-09775-1													
J								A survey of cellular automata: types, dynamics, non-uniformity and applications	NATURAL COMPUTING										Cellular automata (CAs); Types; Characterization tools; Dynamics; Non-uniformity; Technology	PSEUDORANDOM NUMBER GENERATORS; DENSITY CLASSIFICATION; STATISTICAL-MECHANICS; LYAPUNOV EXPONENTS; PERIODIC BOUNDARY; LEADER ELECTION; LINEAR-TIME; COMPUTATION; REVERSIBILITY; COMPLEXITY	Cellular automata (CAs) are dynamical systems which exhibit complex global behavior from simple local interaction and computation. Since the inception of cellular automaton (CA) by von Neumann in 1950s, it has attracted the attention of several researchers over various backgrounds and fields for modeling different physical, natural as well as real-life phenomena. Classically, CAs are uniform. However, non-uniformity has also been introduced in update pattern, lattice structure, neighborhood dependency and local rule. In this survey, we tour to the various types of CAs introduced till date, the different characterization tools, the global behavior of CAs, like universality, reversibility, dynamics etc. Special attention is given to non-uniformity in CAs and especially to non-uniform elementary CAs, which have been very useful in solving several real-life problems.																	1567-7818	1572-9796				JUN	2020	19	2			SI		433	461		10.1007/s11047-018-9696-8													
J								Application of surrogate models in estimation of storm surge: A comparative assessment	APPLIED SOFT COMPUTING										Surrogate models; Storm surge prediction error structure; Artificial neural network; Gaussian process regression; Support vector regression	ENSEMBLE PREDICTION SYSTEM; RESPONSE FUNCTION-APPROACH; NEURAL-NETWORK MODEL; WAVE; APPROXIMATION; METHODOLOGY; FORECAST	Coastal storm surge hazard assessment has received increased attention due to major hurricane events in the last two decades. Robust hazard assessment requires accurate and efficient storm surge prediction models; however, existing numerical models are either high-fidelity but computationally demanding or low-fidelity but with real-time forecasting application. This dichotomy has prompted the development of surrogate models that leverage available synthetic/historical storm databases to build prediction models intended to better balance efficiency and accuracy. Despite numerous studies that have examined the application of various surrogate modeling methods in coastal response prediction, no study is available that compares all of the frequently used methods for storm surge prediction. Furthermore, in most of these studies, the discussion of the performance of these models is bounded to aggregated error metrics (e.g., RMSE, R). This study aims to provide a comprehensive framework for comparison and assessment of the performance of surrogate models based on Artificial Neural Network (ANN), Gaussian Process Regression (GPR) and Support Vector Regression (SVR) for predicting storm surge. The United States Army Corp of Engineers' North Atlantic Coast Comprehensive Study (NACCS) database is used for developing the models at representative coastal locations. In this study, the performance of the models is explored by investigating the stability of performance across training sample sizes, identifying systematic trends in errors, assessing performance in predicting large target response quantities, and characterizing the distribution of error. The results indicate that the performance of surrogate models may be improved by use of physically-motivated parameter scaling and that the selection of a surrogate modeling method should be informed by factors such as consistency in performance under a range of target surge elevations. In particular, the results suggest that accuracy of the tested surrogate models may be a function of the target surge elevation. Furthermore, results suggest that model performance should be assessed using factors beyond aggregate error metrics because such measures (particularly when used without modification to focus on risk-significant events) may give incomplete information about performance of surrogate models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106184	10.1016/j.asoc.2020.106184													
J								Dynamic ensemble mechanisms to improve particulate matter forecasting	APPLIED SOFT COMPUTING										Particulate matter forecasting; Extreme learning machines; Data streams; Concept drift	NEURAL-NETWORKS; MODELS; REGRESSION; MACHINE	Respirable solid particles and liquid droplets suspended in the air, known as particulate matter (PM), may have a significant impact on human health, urban infrastructure, and natural and agricultural systems. The adverse effects of PM have raised public concern, especially in heavily polluted areas in the world, making it imperative the development of strategies to keep the concentration levels of these pollutants below harmful thresholds. Traditional machine learning approaches have been used to forecast PM concentrations. However, complex chemical processes may be involved in the composition of PM in the atmosphere and influenced by many meteorological parameters. Thus, underlying data distributions of PM data, uninterruptedly collected, may evolve over time. This phenomenon, known as concept drift, implies an important challenge for traditional machine learning techniques since they do not have mechanisms to handle changes on data distribution at the running time, thus limiting their forecasting capabilities. The overall goal of this work is to evaluate whether the incorporation of mechanisms to deal with concept drift, together with online sequential learning approaches, can improve the accuracy of PM forecasting. To do so, new mechanisms that enable online dynamic ensembles to handle and retain knowledge from different concepts for more time were proposed and adapted to EOS and DOER algorithms, resulting in three approaches: EOS-rank, EOS-D and DOER-rank. These ensemble strategies, which were based on Online Sequential Extreme Learning Machines (OS-ELM), were compared with five algorithms from the literature. To evaluate their performance, real-world and artificial datasets, with known dynamic behaviors, and PM concentration datasets from different cities of the State of Sao Paulo, Brazil, were used in the experiments. The obtained results showed that the proposed approaches can handle dynamic environments with different rates of drift and that EOS-rank was capable of outperforming most approaches from the literature in scenarios with higher rates of drift. The results also indicate that PM data distributions slowly evolve over time and, consequently, the proposed mechanisms that keep information of past concepts and slowly adapt the ensemble tend to present better results when applied to forecast PM concentration. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				JUN	2020	91								106123	10.1016/j.asoc.2020.106123													
J								A Soft Computing Approach for group decision making: A supply chain management application	APPLIED SOFT COMPUTING										Soft Computing; Neuro-Fuzzy Analytic Network Process (NFANP); Fuzzy judgments; Group decision-making; Supply chain management; ANNs	ANALYTIC NETWORK PROCESS; FUZZY HIERARCHICAL ANALYSIS; SELECTION; AHP; ALTERNATIVES; CONSISTENCY; EXTENSION; TOPSIS; MODEL	This paper presents a novel Soft Computing Approach called "Neuro-Fuzzy Analytical Network Process (NFANP)'' for the group decision-making problems based on the conventional Analytic Network Process (ANP) method. The proposed approach deals with the interval values of judgments in a fuzzy environment using mobile, not fixed, trapezoidal and triangular membership functions, as well as the interval numerical ratio defined by alpha-cuts and the decision maker's confidence levels. The consistency problem of the fuzzy reciprocal matrices is addressed in the proposed paper by allowing a certain tolerance deviation to be less than 0.20. Furthermore, trained Artificial Neural Networks (ANNs) are included in the proposed approach to reduce the large number of computations of the arithmetic operations required to correlate decision factors with the alternatives. In the proposed implementation, the selection problem is defined into three main decision groups: Supplier Characteristics, On-Going Performance, and Project Management Capabilities. The supplier alternatives are classified by the decision makers corresponding to company size, quality system implementation, and cost management. The application of the proposed approach shows a great accuracy in the final utility values and a significant reduction in the calculation requirements. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106201	10.1016/j.asoc.2020.106201													
J								Developing the seismic fragility analysis with fuzzy random variables using Mouth Brooding Fish algorithm	APPLIED SOFT COMPUTING										Uncertainty; Fuzzy sets; Seismic fragility analysis; Modified genetic algorithm; Genetic expression programming; Mouth Brooding Fish algorithm	COMPRESSIVE STRENGTH; GENETIC ALGORITHM; NEURAL-NETWORKS; FORMULATION; CONCRETE; PREDICT; CURVES; MODELS	The present work proposes a fuzzy-random fragility assessment framework for evaluating 2D reinforced concrete moment frame buildings in the presence of various sources of uncertainty, most notably aleatory and epistemic. Such uncertainties exert a powerful influence on the nonlinear behavior of structural systems and consequently affect the seismic response of these structures. For this reason, a number of effective techniques including Latin Hypercube Sampling (LHS) simulation, fuzzy set theory, and a well-known alpha-cut approach have been used to quantify the median of the collapse fragility curve as the fuzzy-random response. As a major step in the alpha-cut approach, metaheuristic evolutionary algorithms including modified genetic algorithm (MGA), and a novel global optimization algorithm inspired by Mouth Brooding Fish (MBF) in nature have been adopted to explore the maximum and minimum of such median in each membership degree, alpha. The results demonstrate that the merit of new MBF algorithm is its greater efficiency in specifying the alpha-cut boundaries compared with MGA. Herein, for the sake of more simplicity and efficiency, a new equation is proposed for the prediction of the median of collapse fragility curve of the case-study building, using the gene expression programming (GEP) methodology. This median is formulated in terms of several effective parameters such as steel modulus of elasticity E-s, steel yield stress f(y), and concrete strength f(c)', which regarded as the input fuzzy-random variables. The performance and validity of the GEP model are further tested using several criteria. The results indicate that analyzing the proposed GEP model in terms of fuzzy-random variables thru the MBF algorithm significantly improves efficiency and reduces computational time by 75%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106190	10.1016/j.asoc.2020.106190													
J								OHDA: An Opposition based High Dimensional optimization Algorithm	APPLIED SOFT COMPUTING										High Dimensional optimization; Evolutionary algorithms; Opposition-based learning; Constraint optimization	BIOGEOGRAPHY-BASED OPTIMIZATION; SEARCH	One of the challenging problems to-date is to deal with high dimensional data. This problem is getting more severe as the data gathering tools are progressing. This paper proposes an opposition-based optimization algorithm suitable for high dimensions. Its novelty is the angular movement according to a few selected dimensions that makes it effective to search in high dimensions. Accordingly, the Opposition-based High Dimensional optimization Algorithm (OHDA) is proposed. Its performance is studied using functions from CEC2005 for high dimensional data including 1000D and 2000D. In addition, the performance of the proposed algorithm is tested with CEC2014, which is more complicated than CEC2005 and CEC2013. The performance of OHDA is also examined using CEC2017 constraint optimization test suit. The comparing algorithms are CA, ICA, AAA, ABC, KH, MVO, WOA, RW-GWO, B-BBO, LX-BBO and LSHADE44-IEpsilon. The results verify that the proposed algorithm outperforms some conventional optimization algorithms in terms of their accuracies. The efficiency of employing opposite points in optimization is also validated in this paper. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106185	10.1016/j.asoc.2020.106185													
J								On the rectangular fuzzy complex linear systems	APPLIED SOFT COMPUTING										Generalized fuzzy complex set; Rectangular fuzzy complex number; Rectangular fuzzy linear system	EQUATIONS	In this paper, we focus on generalized fuzzy complex numbers and provide several examples of membership functions and graphical representations of these numbers. Then, a specific type of fuzzy complex linear systems, called the rectangular fuzzy complex linear system, is considered and its algebraic and general solutions are defined. Finally an approach based on restricting the general solution is presented to solve a rectangular fuzzy complex linear system. It is illustrated that the proposed method gives a unique algebraic solution to a rectangular fuzzy complex linear system if it exists. To show the ability and efficiency of the method and for more illustration, three numerical examples including a real application in electrical engineering are modeled and solved. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106196	10.1016/j.asoc.2020.106196													
J								Sarcasm detection in mash-up language using soft-attention based bi-directional LSTM and feature-rich CNN	APPLIED SOFT COMPUTING										Sarcasm; Code-switch; Mash-up; Social media; Deep learning	SENTIMENT	Analyzing explicit and clear sentiment is challenging owing to the growing use of emblematic and multilingual language constructs. This research proposes sarcasm detection using deep learning in code-switch tweets, specifically the mash-up of English with Indian native language, Hindi. The proposed model is a hybrid of bidirectional long short-term memory with a softmax attention layer and convolution neural network for real-time sarcasm detection. To evaluate the performance of the proposed model, real-time mash-up tweets are extracted on the trending political (#government) and entertainment (#cricket, #bollywood) posts on Twitter. The randomly sampled dataset contains 3000 sarcastic and 3000 non-sarcastic bilingual Hinglish (Hindi + English) tweets. Feature engineering is done using pre-trained GloVe word embeddings to extract English semantic context vector, hand-crafted features using subjective lexicon Hindi-SentiWordNet to generate the SentiHindi feature vector and an auxiliary pragmatic feature vector depicting the count of pragmatic markers in tweet. Performance analysis is done to compare and validate the proposed (BiLSTM)-Bi-softAtt-feature-richCNN model. The model outperforms the baseline deep learning models with a superior classification accuracy of 92.71% and F-measure of 89.05%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106198	10.1016/j.asoc.2020.106198													
J								Influence of initialization on the performance of metaheuristic optimizers	APPLIED SOFT COMPUTING										Initialization; Differential evolution; Particle swarm optimization; Cuckoo search; Probability distribution	DIFFERENTIAL EVOLUTION ALGORITHM; BEE COLONY ALGORITHM; OPTIMIZATION METHOD; CUCKOO SEARCH; POPULATION	All metaheuristic optimization algorithms require some initialization, and the initialization for such optimizers is usually carried out randomly. However, initialization can have some significant influence on the performance of such algorithms. This paper presents a systematic comparison of 22 different initialization methods on the convergence and accuracy of five optimizers: differential evolution (DE), particle swarm optimization (PSO), cuckoo search (CS), artificial bee colony (ABC) and genetic algorithm (GA). We have used 19 different test functions with different properties and modalities to compare the possible effects of initialization, population sizes and the numbers of iterations. Rigorous statistical ranking tests indicate that 43.37% of the functions using the DE algorithm show significant differences for different initialization methods, while 73.68% of the functions using both PSO and CS algorithms are significantly affected by different initialization methods. The simulations show that DE is less sensitive to initialization, while both PSO and CS are more sensitive to initialization. In addition, under the condition of the same maximum number of fitness evaluations (FEs), the population size can also have a strong effect. Particle swarm optimization usually requires a larger population, while the cuckoo search needs only a small population size. Differential evolution depends more heavily on the number of iterations, a relatively small population with more iterations can lead to better results. Furthermore, ABC is more sensitive to initialization, while such initialization has little effect on GA. Some probability distributions such as the beta distribution, exponential distribution and Rayleigh distribution can usually lead to better performance. The implications of this study and further research topics are also discussed in detail. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106193	10.1016/j.asoc.2020.106193													
J								Cooperative localization for multiple AUVs based on the rough estimation of the measurements	APPLIED SOFT COMPUTING										Cooperative localization; Autonomous underwater vehicle; Bayesian filtering; Underwater acoustic transmission; Measurement rough estimation; Common observation environment; Simulation	NAVIGATION; FILTER	The accuracy of cooperative localization for multiple autonomous underwater vehicles (AUVs) equipped with low precise proprioceptive localization sensors can be improved by using relative location information between individuals and Bayesian filtering. However, when the relative location measurement errors are high, its accuracy will be reduced. Two measurement for rough estimation algorithms under the constraint environment of the cooperative structure are developed in this paper: the first algorithm is based on the underwater acoustic isotropic transmission. And the second algorithm is based on the common observation environment. In the first algorithm, it builds under the assumption that the distance errors calculated from the simultaneous omnidirectional response signals from the same transmitting source have correlated. Similarly, in the second algorithm, the assumption that "common observation environment'' is correlated is made. First, the correlation between the errors is used roughly to estimate the measurement of information. Then, a suitable filter is applied to fuse the rough estimation measurement with dead-reckoning estimation that improves the location estimation accuracy. The final simulation, by changing the AUV formation navigation paths and the sensor observation noises, shows the proposed processing methods have effectiveness and consistency compared to the traditional algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106197	10.1016/j.asoc.2020.106197													
J								Local segmentation of images using an improved fuzzy C-means clustering algorithm based on self-adaptive dictionary learning	APPLIED SOFT COMPUTING										Dictionary learning; Fuzzy C-means clustering; Algorithm of the noise reduction; Image segmentation; MRI and CT	SPARSE; CLASSIFICATION; REPRESENTATION; MODEL	Image segmentation is an active research topic in image processing. The Fuzzy C-means (FCM) clustering analysis has been widely used in image segmentation. As there is a large amount of delicate tissues such as blood vessels and nerves in medical images, noise generated during imaging process can easily affect successful segmentation of these tissues. The traditional FCM algorithm is not ideal for segmentation of images containing strong noise. In this study, we proposed an improved FCM algorithm with anti-noise capability. We first discussed the algorithm of dictionary learning for noise reduction. Then we developed a new image segmentation algorithm as a combination of the dictionary learning for noise reduction and the improved fuzzy C-means clustering. Lastly we used the algorithm of the improved FCM to segment images, during which we removed the nontarget areas making use of the grayscale features of images and extracted accurately the areas of interests. The algorithm was tested using synthetic Shepp-Logan images and real medical magnetic resonance imaging (MRI) and computed tomography (CT) images. Compared to the synthetic data and real medical images segmented by the fuzzy C-means (FCM) clustering algorithm, the Kernel Fuzzy C-mean (KFCM) clustering algorithm, spectral clustering algorithm, the sparse learning based fuzzy C-means (SL_FCM) clustering algorithm, and the modified spatial KFCM (MSFCM) algorithm, the images segmented by the dictionary learning Fuzzy C-mean clustering (DLFCM) algorithm have higher partition coefficient, lower partition entropy, better visual perception, better clustering accuracy, and clustering purity. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106200	10.1016/j.asoc.2020.106200													
J								A universal strengthened searching module for multi-objective optimization based on variable properties	APPLIED SOFT COMPUTING										Multi-objective; Variable clustering; Strengthened searching; Evolutionary algorithm	EVOLUTIONARY ALGORITHM; DECOMPOSITION	Numerous algorithms have been introduced in solving multi-/many-objective optimization problems. However, some rigorous elite definitions may result in the side-effect that new solutions are hard to be admitted by the elite archive, which will restrain the searching ability. In this paper, a universal strengthened searching (SS) module is proposed as an accessorial searching procedure to improve the performance of existing algorithms. By concentrating part of the computational resources, this strategy can enhance the convergence and diversity searching strengths in separate areas. These areas are determined by classifying decision variables according to a dynamic spread-based procedure. Moreover, incorporating SS with existing algorithms can improve the overall performance of original algorithms while persisting their inherent advantages. In this paper, the structure of an SS-embedded algorithm is illustrated and the comparison experiments have been established among several couples of algorithms. The results demonstrate that the algorithms with the SS module have better overall performance compared to the original algorithms towards different problem properties. Meanwhile, the proposed strategy can accelerate the searching procedure for the time-consuming algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106199	10.1016/j.asoc.2020.106199													
J								BG-SAC: Entity relationship classification model based on Self-Attention supported Capsule Networks	APPLIED SOFT COMPUTING										Entity relationship classification; Self-Attention mechanism; Capsule Networks; Dynamic routing algorithm		To date, deep learning techniques, especially the combination of convolutional neural networks and recurrent neural networks with the attention mechanism, have been the state-of-the-art solutions for processing relation extraction and classification tasks. However, the neural network model constructed by this method cannot make full use of the labeled entities and their positional information in the relation classification, or even performs poorly on the small sample dataset. To address these issues, this paper proposes an entity relationship classification model BG-SAC, which combines BiGRU, Self-Attention mechanism and Capsule Networks. BG-SAC primarily uses BiGRU to obtain sentence sequential information and context-based semantic information, and then is coupled with the Self-Attention mechanism to get the correlation between words. Capsule Networks are used to acquire the positional information of entities. Eventually the probability that entities belong to a certain relationship category is calculated through the length of a capsule, so as to determine the relationship between entities and realize the classification of entity relationship. The experimental results show that the proposed model can effectively capture the word positional information and improve the classification effect with small sample datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106186	10.1016/j.asoc.2020.106186													
J								Frame-by-frame Wi-Fi attack detection algorithm with scalable and modular machine-learning design	APPLIED SOFT COMPUTING										Network attacks; Malicious traffic detection; Traffic classification; Dissimilarity measure; Clustering; Genetic optimization	INTERNET; NETWORKS; FLOWS	The popularity of Wi-Fi networks coupled with the intrinsic vulnerability of wireless interfaces has promoted the investigation and proposal of traffic analysis and anomaly detection algorithms targeted to that application. We propose a scalable and modular algorithm architecture to set up a lightweight classifier, able to detect malicious frames with high reliability, allowing a simple implementation and suitable for real-time operations. We compare two design alternatives, based on either an optimized neuro-fuzzy classifier or a k-Nearest Neighbor classifier wrapped into a genetic optimization procedure. Both designs exploit a dissimilarity measure able to handle both numerical and non-numerical features. Scalability and modularity are obtained by considering an array of binary classifiers tuned to identify one specific attack against any other type of traffic. We exploit the Aegean Wi-Fi Intrusion Detection (AWID) dataset to assess the accuracy of the proposed algorithm, finding up to twelve out of the fourteen attack classes of the dataset can be identified with high reliability based just on the inspection of a single frame, provided the right features are observed. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106188	10.1016/j.asoc.2020.106188													
J								A decomposition-based multi-objective optimization approach for extractive multi-document text summarization	APPLIED SOFT COMPUTING										Multi-document summarization; Multi-objective optimization; Artificial bee colony; Decomposition-based	MAXIMUM COVERAGE; DIFFERENTIAL EVOLUTION; ALGORITHM; MOEA/D; SELECTION	Currently, due to the overflow of textual information on the Internet, automatic text summarization methods are becoming increasingly important in many fields of knowledge. Extractive multi-document text summarization approaches are intended to automatically generate summaries from a document collection, covering the main content and avoiding redundant information. These approaches can be addressed through optimization techniques. In the scientific literature, most of them are single-objective optimization approaches, but recently multi-objective approaches have been developed and they have improved the single-objective existing results. In addition, in the field of multi-objective optimization, decomposition-based approaches are being successfully applied increasingly. For this reason, a Multi-Objective Artificial Bee Colony algorithm based on Decomposition (MOABC/D) is proposed to solve the extractive multi-document text summarization problem. An asynchronous parallel design of MOABC/D algorithm has been implemented in order to take advantage of multi-core architectures. Experiments have been carried out with Document Understanding Conferences (DUC) datasets, and the results have been evaluated with Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. The obtained results have improved the existing ones in the scientific literature for ROUGE-1, ROUGE-2, and ROUGE-L scores, also reporting a very good speedup. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106231	10.1016/j.asoc.2020.106231													
J								Multiobjective multi-verse optimization algorithm to solve combined economic, heat and power emission dispatch problems	APPLIED SOFT COMPUTING										Economic dispatch; Emission dispatch; Cogeneration; Multiobjective optimization; Pareto optimality; Meta heuristic algorithms	ARTIFICIAL BEE COLONY; FLOWER POLLINATION ALGORITHM; PARTICLE SWARM OPTIMIZATION; LEARNING BASED OPTIMIZATION; IMPROVED GENETIC ALGORITHM; LOAD DISPATCH; BAT ALGORITHM; EVOLUTIONARY; IMPLEMENTATION; OBJECTIVES	This study implements a potent Multiobjective Multi-Verse Optimization algorithm to solve the highly complicated combined economic emission dispatch and combined heat and power economic emission dispatch problems. Solving these problems operates the power system integrated with cogeneration plants economically and reduces the environmental impacts caused by the pollutants of fossil fuel-fired power plants. A chaotic opposition based strategy is proposed to explore the search space extensively and to generate the initial populations for the multiobjective optimization algorithm. An effective constraint handling mechanism is also proposed to enable the population to remain within the bounds and in the feasible operating region of the cogeneration plants. The algorithm is applied to standard test functions, four test systems including a large 140 bus system considering valve-point effects, ramp limits, transmission power losses, and the feasible operating region of cogeneration units. The Pareto Optimal solutions obtained by the algorithm are well spread and diverse when compared with other optimization algorithms. The statistical analysis and various performance metrics used indicate the algorithm converges to true POF and is a viable alternative to solve the highly complicated combined economic emission dispatch and combined heat and power economic emission dispatch problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106195	10.1016/j.asoc.2020.106195													
J								A hybrid algorithm based optimal placement of DG units for loss reduction in the distribution system	APPLIED SOFT COMPUTING										Cuckoo Search (CS); Distribution networks; Distribution generation; Grasshopper Optimization Algorithm (GOA); Loss reduction	DISTRIBUTION NETWORKS; RECONFIGURATION; OPTIMIZATION; ALLOCATION; GENERATORS; LOAD; INTEGRATION; MODELS	Distributed generation (DG) has been utilized in some electric power networks. Power loss reduction, environmental friendliness, voltage improvement, postponement of system upgrading, and increasing reliability are some advantages of DG-unit application This paper uses a hybrid technique to optimize the position and size of DG units to reduce losses in the distribution system. The hybrid technique is the joined execution of both the Grasshopper Optimization Algorithm (GOA) and Cuckoo Search (CS) technique. Here, the GOA optimization behavior is upgraded by utilizing the CS technique. Here, the perfect position of the DG unit is settled with respect to the power loss, line power flow and voltage profile using the proposed system. For improving the dynamic execution, the limit of DG is directed by the proposed technique with respect to the cost work. The motivation behind the proposed system is to produce optimal capacity to lessen the aggregate power loss and enhance the voltage profiles of power distribution networks. The proposed hybrid technique is executed in MATLAB/Simulink working platform and the dynamic dependability execution is tested and considered with IEEE 33-bus distribution networks and IEEE 69-bus system. The stability by diminishing loss of the distribution system is investigated by executed different load state of the system. The execution of the proposed system is analyzed and compared with different existing techniques. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106191	10.1016/j.asoc.2020.106191													
J								A hybrid algorithm for task scheduling on heterogeneous multiprocessor embedded systems	APPLIED SOFT COMPUTING										Power consumption; Real-time embedded systems; Evolutionary algorithms; Task graph	OPTIMIZATION ALGORITHM; ENERGY-CONSUMPTION; ASSIGNMENT; TIME; PERFORMANCE; VOLTAGE	Most of the scheduling algorithms proposed for real-time embedded systems, with energy constraints, try to reduce power consumption. However, reducing the power consumption may decrease the computation speed and impact the makespan. Therefore, for real-time embedded systems, makespan and power consumption need to be considered simultaneously. Since task scheduling is an NP-hard problem, most of the proposed scheduling algorithms are not able to find the multi-objective optimal solution. In this paper, we propose a two-phase hybrid task scheduling algorithm based on decomposition of the input task graph, by applying spectral partitioning. The proposed algorithm, called G-SP, assigns each part of the task graph to a low power processor in order to minimize power consumption. Through experiments, we compare the makespan and power consumption of the G-SP against well-known algorithms of this area for a large set of randomly generated and real-world task graphs with different characteristics. The obtained results show that the G-SP outperforms other algorithms in both metrics, under various conditions, involving different numbers of processors and considering several system configurations. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				JUN	2020	91								106202	10.1016/j.asoc.2020.106202													
J								A novel bilateral impedance controls for underwater tele-operation	APPLIED SOFT COMPUTING										Underwater tele-operation; Time delay; Adaptive neural fuzzy inference system; Disturbance observer-based; Impedance control	MANIPULATOR; SYSTEMS; DESIGN	Owing to characteristics of the flow and the variability, it is extremely difficult to achieve the stability and the transparency of the underwater tele-operation system. In practice, the accurate force may not easily be acquired due to model uncertainties, the time delay and external disturbances. In order to enhance the stability and the transparency of the underwater tele-operation, an adaptive neural fuzzy inference system disturbance observer-based impedance control is proposed to both the master side and slave side. The learning algorithm of the adaptive neural fuzzy inference system network and the disturbance observer may simultaneously suppress model uncertainties of the nonlinear system and disturbances of external underwater environment. Concerning the time delay, the stability is analyzed by Lyapunov theorem. Numerical simulations are performed and results demonstrate the effective performance of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106194	10.1016/j.asoc.2020.106194													
J								A fuzzy rough number-based AHP-TOPSIS for design concept evaluation under uncertain environments	APPLIED SOFT COMPUTING										Design concept evaluation; Fuzzy rough number; Uncertainty and subjectivity; Group decision-making; AHP and TOPSIS	GROUP DECISION-MAKING; CONCEPT SELECTION; FAILURE MODE; PERFORMANCE; EXTENSION; DEMATEL; ANP	Design concept evaluation in the early phase of product design plays a crucial role in new product development as it considerably determines the direction of subsequent design activities. However, it is a process involving uncertainty and subjectivity. The evaluation information mainly relies on expert's subjective judgment, which is imprecise and uncertain. How to effectively and objectively evaluate the design concept under such subjective and uncertain environments remains an open question. To fill this gap, this paper proposes a fuzzy rough number-enhanced group decision-making framework for design concept evaluation by integrating a fuzzy rough number-based AHP (analytic hierarchy process) and a fuzzy rough number-based TOPSIS (technique for order preference by similarity to ideal solution). First of all, a fuzzy rough number is presented to aggregate personal risk assessment information and to manipulate the uncertainty and subjectivity during the decision-making. Then a fuzzy rough number-based AHP is developed to determine the criteria weights. A fuzzy rough number-based TOPSIS is proposed to conduct the alternative ranking. A practical case study is put forward to illustrate the applicability of the proposed decision-making framework. Experimental results and comparative studies demonstrate the superiority of the fuzzy rough number-based method in dealing with the uncertainty and subjectivity in design concept evaluation under group decision-making environment. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106228	10.1016/j.asoc.2020.106228													
J								A minimum centre distance rule activation method for extended belief rule-based classification systems	APPLIED SOFT COMPUTING										Extended belief-rule-based system; Rule activation; Minimum centre distance; Classification; Filtering and selecting	EXPERT-SYSTEM; WEIGHT CALCULATION; REASONING APPROACH; OPTIMIZATION	Originating from the belief-rule-based (BRB) system, the extended belief rule-based (EBRB) system combined the advantages of the rule-based method and those of data-driven methods. By transforming the data set into extended belief rules and using evidential reasoning (ER), the EBRB system has expanded the application of BRB systems and demonstrated their capability in addressing classification problems. Nevertheless, the problem of activating nearly the entire rule base in every classification process is embedded in the EBRB scheme. There have been advances in rule activation for the EBRB system; however, the introduction of subjective information into the classification, high computational costs and long response times are common problems facing existing rule activation methods. To solve the problems facing rule activation for EBRB systems, a minimum centre distance rule activation (MCDRA) method for EBRB systems is proposed. In MCDRA, no subjective information is required, and no time-consuming iteration procedure is necessary. Two components of the proposed MCDRA, i.e., the filtering procedure and the selection procedure, are designed to eliminate unrelated samples of input query data and to select and activate the highly related samples to the input query data. A total of 12 benchmark data sets are used to test the performance of EBRB with MCDRA (M-EBRB). The experimental results show that compared with other rule activation methods, the proposed method obtains satisfactory rule activation ratios, accuracies and response times. Additionally, M-EBRB performs well on noisy data and comparatively with both the fuzzy-rule-based classification system (FRBCS) and several machine learning classification algorithms. In addition, MCDRA can be utilized as a generic rule activation method and can be used to optimize other rule-based classification systems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106214	10.1016/j.asoc.2020.106214													
J								Iris anti-spoofing through score-level fusion of handcrafted and data-driven features	APPLIED SOFT COMPUTING										Iris anti-spoofing; DenseNet121; DCCNet; Iris presentation attack detection; Contact lens classification; Score-level fusion	TEXTURED CONTACT-LENSES; LIVENESS DETECTION; CLASSIFICATION; RECOGNITION	In the past two decads, iris spoofing detection has occupied an ample space in the literature of iris biometics. The textured lens may be used to spoof the Iris Recognition (IR) system by exploiting its external texture. Besides, the soft lens may cause an upsurge in the false rejection rate as it blurs the iris texture. Therefore, it is foremost to identify contact lens in human eyes before accessing an IR system. This paper proposes a novel fusion-based approach to discriminate live iris from contact lens images that combines handcrafted and data-driven features. It also demonstrates a Densely-connected Contact-lens Classification Network (DCCNet) as a data-driven model that is basically a customized Densenet121 framework. The DCCNet features are-pooled with handcrafted counterparts to create a combined feature set. However, the optimal features are identified by top-k feature selection using the Friedman test and are fused through score-level fusion. The assessment of the proposed approach includes several experiments simulated on three iris databases, i.e. Notre Dame (ND) Contact Lens 2013, IIIT-Delhi Contact Lens (IIITD), and Clarkson Databases. The equal error rate (EER) and the detection error tradeoff (DET) curve are used as performance metrics. Further, the statistical analysis is performed using Nemenyi and Bonferroni-Dunn tests, where the proposed approach significantly improves the state of the arts. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106206	10.1016/j.asoc.2020.106206													
J								Spam filtering using a logistic regression model trained by an artificial bee colony algorithm	APPLIED SOFT COMPUTING										Spam filtering; Artificial bee colony; Naive Bayes; Support vector machines; Logistic regression; Turkish emails	NEGATIVE SELECTION ALGORITHM	Email spam is a serious problem that annoys recipients and wastes their time. Machine-learning methods have been prevalent in spam detection systems owing to their efficiency in classifying mail as solicited or unsolicited. However, existing spam detection techniques usually suffer from low detection rates and cannot efficiently handle high-dimensional data. Therefore, we propose a novel spam detection method that combines the artificial bee colony algorithm with a logistic regression classification model. The empirical results on three publicly available datasets (Enron, CSDMC2010, and TurkishEmail) show that the proposed model can handle high-dimensional data thanks to its highly effective local and global search abilities. We compare the proposed model's spam detection performance to those of support vector machine, logistic regression, and naive Bayes classifiers, in addition to the performance of the state-of-the-art methods reported by previous studies. We observe that the proposed method outperforms other spam detection techniques considered in this study in terms of classification accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106229	10.1016/j.asoc.2020.106229													
J								Numerical sensitive data recognition based on hybrid gene expression programming for active distribution networks	APPLIED SOFT COMPUTING										Function mining; Active distribution network; Sensitive data recognition; Gene expression programming; Rough set	SMART; SECURITY; CLASSIFICATION; EFFICIENT	Complex and flexible access mode, and frequent data interaction bring about large security risks to data transmission for active distribution networks. How to ensure data security is critical to the safe and stable operation of active distribution networks. Traditional methods, like access control, data encryption, and text filtering based on intelligent algorithms, are difficult to ensure the security of dynamically increased and high-dimensional numerical data transmission in active distribution networks. In this paper, we first propose a rough feature selection algorithm based on the average importance measurement (RFS-AIM) to simplify the complexity of data recognition. Then, we propose a sensitive data recognition function mining algorithm based on RFS-AIM and improved gene expression programming (SDR-IGEP) where population update operation is constructed by chromosome similarity based on the Jaccard coefficient. The operation avoids local convergence of the gene express programming by increasing individual diversity in the new population. Finally, we present a new incremental mining algorithm for a sensitive data recognition function based on global function fitting (ISDR-GFF) by using a grain granulation model for incremental datasets. The experimental results on IEEE benchmark datasets and real datasets show that the algorithms proposed in this paper outperform the state-of-the-art algorithms in terms of the average running time, precision, recall, F-1 index, accuracy, specificity and speedup on all experimental datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106213	10.1016/j.asoc.2020.106213													
J								Extracting Combinatorial Test parameters and their values using model checking and evolutionary algorithms	APPLIED SOFT COMPUTING										Covering array; Model checking; Genetic algorithm; GROOVE	PARTICLE SWARM OPTIMIZATION; TEST SUITE; STRATEGY; GENERATION	Combinatorial Testing (CT) is one of the popular testing approaches for generating a minimum test suite to detect defects caused by interactions between subsystems. One of the most practical CT methods is the Covering Array (CA). While generating CA, there are at least two main groups of challenges. The first one is extracting information about parameters, identifying constraints and detecting interactions between subsystems automatically. In most of the existing approaches, this information is fed to the system manually which makes it difficult or even impossible for testing modern software systems. The second one is the speed and the array size. Even though most of the existing approaches are concentrated on this challenge, their results show that there is still room for improvement. In this paper, we propose an idea to cope with both challenges. At first, we represent a method to extract information about the system under test (SUT) from its model using model checking (MC) techniques. MC is a method that scans all possible states of the system for detecting errors. After that, we propose another new approach using genetic algorithm to generate the optimal CA in terms of speed and size. To evaluate the results, we implemented the proposed strategy along with several other metaheuristic algorithms in the GROOVE tool, an open toolset for designing and model checking graph transformation specifications. The results represent that the proposed strategy performs better than others. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106219	10.1016/j.asoc.2020.106219													
J								Learning the retinal anatomy from scarce annotated data using self-supervised multimodal reconstruction	APPLIED SOFT COMPUTING										Deep learning; Eye fundus; Self-supervised learning; Optic disc; Blood vessels; Fovea; Medical imaging; Transfer learning	CONVOLUTIONAL NEURAL-NETWORKS; OPTIC DISC; VESSEL SEGMENTATION; BLOOD-VESSELS; IMAGE; FOVEA	Deep learning is becoming the reference paradigm for approaching many computer vision problems. Nevertheless, the training of deep neural networks typically requires a significantly large amount of annotated data, which is not always available. A proven approach to alleviate the scarcity of annotated data is transfer learning. However, in practice, the use of this technique typically relies on the availability of additional annotations, either from the same or natural domain. We propose a novel alternative that allows to apply transfer learning from unlabelled data of the same domain, which consists in the use of a multimodal reconstruction task. A neural network trained to generate one image modality from another must learn relevant patterns from the images to successfully solve the task. These learned patterns can then be used to solve additional tasks in the same domain, reducing the necessity of a large amount of annotated data. In this work, we apply the described idea to the localization and segmentation of the most important anatomical structures of the eye fundus in retinography. The objective is to reduce the amount of annotated data that is required to solve the different tasks using deep neural networks. For that purpose, a neural network is pre-trained using the self-supervised multimodal reconstruction of fluorescein angiography from retinography. Then, the network is fine-tuned on the different target tasks performed on the retinography. The obtained results demonstrate that the proposed self-supervised transfer learning strategy leads to state-of-the-art performance in all the studied tasks with a significant reduction of the required annotations. (C) 2020 The Authors. Published by Elsevier B.V.																	1568-4946	1872-9681				JUN	2020	91								106210	10.1016/j.asoc.2020.106210													
J								An efficient Long Short-Term Memory model based on Laplacian Eigenmap in artificial neural networks	APPLIED SOFT COMPUTING										Recurrent neural network; Laplacian Eigenmap; Long Short-Term Memory; Artificial intelligence	STOCHASTIC OZONE DAYS	A new algorithm for data prediction based on the Laplacian Eigenmap (LE) is presented. We construct the Long Short-Term Memory model with the application of the LE in artificial neural networks. The new Long Short-Term Memory model based on Laplacian Eigenmap (LE-LSTM) reserves the characteristics of original data using the eigenvectors derived from the Laplacian matrix of the data matrix. LE-LSTM introduces the projection layer embedding data into a lower dimension space so that it improves the efficiency. With the implementation of LE, LE-LSTM provides higher accuracy and less running time on various simulated data sets with characteristics of multivariate, sequential, and time-series. In comparison with previously reported algorithms such as stochastic gradient descent and artificial neural network with three layers, LE-LSTM leads to many more successful runs and learns much faster. The algorithm provides a computationally efficient approach to most of the artificial neural network data sets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106218	10.1016/j.asoc.2020.106218													
J								A novel hybrid MCDM model for machine tool selection using fuzzy DEMATEL, entropy weighting and later defuzzification VIKOR	APPLIED SOFT COMPUTING										Machine tool selection; Hybrid MCDM model; Fuzzy DEMATEL; Entropy weighting; Later defuzzification VIKOR	DECISION-MAKING APPROACH; CRITICAL SUCCESS FACTORS; GOAL-PROGRAMMING MODEL; INTEGRATED APPROACH; SUPPORT-SYSTEM; OPERATION ALLOCATION; AHP; TOPSIS; BENEFIT; COPRAS	Machine tool selection has been an important issue in the manufacturing industry because improper machine tool selection can have a negative effect on productivity, accuracy, flexibility, and the responsive manufacturing capabilities of a company. The current multi-criteria decision making (MCDM) approach of machine tool selection mostly focuses on the subjective perspective. However, as the objective evaluation represents the actual performance of machine tools, both subjective and objective perspectives need to be considered when choosing an appropriate machining tool. Therefore, this study proposes a machine tool selection method based on a novel hybrid MCDM model. Firstly, the presented method employs a comprehensive weight technique integrating subjective weights obtained using fuzzy decision-making trial and evaluation laboratory (FDEMATEL) with objective weights obtained using entropy weighting (EW). Secondly, later defuzzification VIKOR (LDVIKOR) is put forward to rank the optional alternatives. Finally, a case application verifies the effectiveness of the proposed method. The evaluation results indicate that the best and worst selected machine tool of the proposed method keeps high conformance with the actual ranking in real factory. Additionally, sensitivity analysis results of the effect of parameters. on the decision outcome show that irrespective of the variations in this parameter, the best decision outcome will be not influenced. These indicate that the presented hybrid model has advantages in granting flexibility to the preferences of decision makers. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106207	10.1016/j.asoc.2020.106207													
J								An integrated framework of deep learning and knowledge graph for prediction of stock price trend: An application in Chinese stock exchange market	APPLIED SOFT COMPUTING										Stock trend prediction; Deep neural network; Knowledge graph	NEURAL-NETWORK; MOVEMENT PREDICTION; MODE DECOMPOSITION; MACHINE; SUPPORT	Many studies have been carried out on stock price trend prediction, but most of them focused on the public market data and did not utilize the trading behaviors owing to the unavailability of real transaction records data. In fact, trading behaviors can better reflect the market movements, and the fusion of trading information and market information can further improve the prediction accuracy. In this paper, we propose a deep neural network model using the desensitized transaction records and public market information to predict stock price trend. Considering the correlation between stocks, our method utilizes the knowledge graph and graph embeddings techniques to select the relevant stocks of the target for constructing the market and trading information. Given the considerable number of investors and the complexity of transaction records data, the investors are clustered to reduce the dimensions of the trading feature matrices, and then the matrices are fed into the convolutional neural network to unearth the investment patterns. Eventually, the attention-based bidirectional long short-term memory network can predict the stock price trends for financial decision support. The experiments on the price movement direction and trend prediction show that our method achieves the best performance in comparison with other prediction baselines. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106205	10.1016/j.asoc.2020.106205													
J								Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning	APPLIED SOFT COMPUTING										Flexible job shop scheduling; New job insertion; Dispatching rules; Deep reinforcement learning; Deep Q network	DISPATCHING RULES; ALGORITHMS; SUBJECT	In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state-action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a "softmax" action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106208	10.1016/j.asoc.2020.106208													
J								DSTARS: A multi-target deep structure for tracking asynchronous regressor stacking	APPLIED SOFT COMPUTING										Machine learning; Multi-target; Multi-output; Regression analysis	SUPPORT VECTOR REGRESSION; ENSEMBLES; INDEX	Several applications of supervised learning involve the prediction of multiple continuous target variables from a dataset. When the target variables exhibit statistical dependencies among them, a multi-target regression (MTR) modelling permits to improve the predictive performance in comparison to induce a separate model for each target. Apart from describing the dependencies among the targets, the MTR methods could offer better performance and less overfitting than traditional single-target (ST) methods. A group of MTR methods have addressed this demand, but there are still many possibilities for further improvements. This paper presents a novel MTR method called Deep Structure for Tracking Asynchronous Regressor Stacking (DSTARS), which overcomes some existing gaps in the current solutions. DSTARS extends the Stacked Single-Target (SST) approach by combining multiple stacked regressors into a deep structure. In this sense, it is able to boost the predictive performance by successively improving the predictions for the targets. Besides, DSTARS exploits the dependency of each target individually by tracking an asynchronous number of stacked regressors. Additionally, our proposal explores the inter-targets dependencies by exposing and measuring them through a nonlinear metric of variable importance. We compared DSTARS to SST, Ensemble of Regressor Chains (ERC) and Multi-objective Random Forest (MORF). Also, the ST strategy with different algorithms was used to compute independent regressions for each target. We used Random Forest (RF) and Support Vector Machine (SVM) as base-learners to investigate the prediction capability of algorithms belonging to different machine learning paradigms. The experiments carried out on eighteen diverse datasets showed that the proposed method was significantly better than the other compared approaches.																	1568-4946	1872-9681				JUN	2020	91								106215	10.1016/j.asoc.2020.106215													
J								A multi-objective methodology for multi-criteria engineering design	APPLIED SOFT COMPUTING										MOMICA; Sorting non-dominated strategy; Attraction and repulsion concept; Engineering optimization; High dimension	IMPERIALIST COMPETITIVE ALGORITHM; ARTIFICIAL NEURAL-NETWORK; OPTIMIZATION ALGORITHM; GENETIC ALGORITHM; ICA; PREDICTION; PSO	Optimization is more and more significant due to its application in the real engineering problems. The recently proposed imperialist competitive algorithm (ICA) is a successful method in mono-objective optimization. Nevertheless, ICA cannot handle simultaneously the conflicting objectives in multi-objective design problem. In addition, the ICA has the drawback of trapping in local optimum solutions when used for high-dimensional or complex multimodal functions. In order to deal with these situations, in this work, an improved ICA, named modified multi-objective imperialist competitive algorithm (MOMICA) is proposed. In MOMICA, an attraction and repulsion (AR) concept is implemented in the assimilation phase to improve the performances of the algorithm to reach the global optimal position. Moreover, in contrast to ICA, the proposed algorithm integrates the sorting non-dominated strategy (SND) to store the Pareto optimal solutions of multiple conflicting functions. Three performance metrics are used to evaluate the performance of the proposed algorithm: (a) convergence to the true Pareto-optimal set, (b) solutions diversity and (c) robustness, characterized by the variance over 10 runs. The results presented in this paper show that the MOMICA algorithm outperforms the other popular techniques in terms of convergence characteristics and global search ability, for both benchmark functions optimization and multi-objective engineering optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106204	10.1016/j.asoc.2020.106204													
J								Co-reconfiguration of product family and supply chain using leader-follower Stackelberg game theory: Bi-level multi-objective optimization	APPLIED SOFT COMPUTING										Product family; Supply chain; Reconfiguration; Bi-level programming; Stackelberg game; MOPSO algorithm	JOINT OPTIMIZATION; PLATFORM PRODUCTS; MASS CUSTOMIZATION; CONFIGURATION; DESIGN; SELECTION; ARCHITECTURE	This article deals with product Family: a group of products with similar modules produced based on assembling to order (ATO) approach to cover diverse customer needs. The demand level and the customer requirements of these products are dynamically changing, which necessitate a novel model for co-reconfiguration of the product family (PF) and the supply chain (SC). Therefore, this article aims to apply Leader-follower Stackelberg Game Theory in order to present a co-reconfiguration of PF and SC based on three objectives: maximizing the total profit, maximizing customer utility and minimizing the supply chain cost in a bi-level structure. Maximizing the total profit and maximizing customer utility are the two objectives of the Leader problem for product family reconfiguration, which results in the optimal selection of components, modules, and product variants. The upper-level problem is considered as a multi-objective problem with the aforementioned two objectives. The lower level of this problem intends to reconfigure the supply chain with the objective of minimizing the supply chain costs, and therefore, to reach the optimal selection of suppliers, manufacturers, assembly plant, distribution centers, and retailers. A bi-level multi-objective linear programming problem (B-MOLP) is used to model the game of the leader-follower. A new particle swarm optimization algorithm, called bi-level multi-objective PSO (B-MOPSO), is developed to solve the proposed bi-level multi-objective model. To show the validity of the proposed model and the efficiency of our algorithm, a case study at a mountain bike industry is investigated. Finally, results in some managerial implications are obtained through sensitivity analysis. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106203	10.1016/j.asoc.2020.106203													
J								Fuzzy-rough assisted refinement of image processing procedure for mammographic risk assessment	APPLIED SOFT COMPUTING										Image processing; Feature extraction; Fuzzy-rough sets; Mammographic risk assessment	BREAST-CANCER RISK; CLASSIFICATION; CURVE	The use of computer aided diagnosis (CAD) systems, which are computer based tools for the automatic analysis of medical images such as mammogram and prostate MRI, can assist in the early detection and diagnosis of developing cancer. In the process of CAD for mammogram, the task of image processing (IP) plays a fundamental role in providing promising diagnostic results, by exploiting high-quality features extracted from the mammographic images. Normally, an IP procedure for mammographic images involves three mechanisms: region of interest (ROI) extraction, image enhancement (IE) and feature extraction (FE). However, an improper utilisation of IE may lead to an inferior composition of the features due to unexpected enhancement of any irrelevant or useless information in ROI. In order to overcome this problem, a fuzzy-rough refined IP (FRIP) framework is presented in this paper to improve the quality of mammographic image features hierarchically. Following the proposed framework, the ROI of each mammographic image is segmented and enhanced locally in the area of the block which is of the highest value of fuzzy positive region (FPR). Here, FPR implies a positive dependency relationship between the block and the decision with regard to the given feature set. The higher a block's FPR value the more certain its underlying image category. To attain a high quality of the image enhancement procedure, the winner block will be further improved by a multi-round strategy to create a pool of IE results. As such, for a mammographic image, after embedding the candidate enhanced blocks into the original ROI, the respectively extracted features from the locally enhanced ROI are compared against each other on the basis of the value of FPR. A given image is therefore represented by a set of features which are supported by the premier FPR among all of the resulting extracted features. The quality of the extracted features by FRIP is compared against that of those directly extracted from the original images, from the globally enhanced images or from the randomly locally enhanced images in performing classification tasks. The experimental results demonstrate that the mammographic risk assessment results based on the features achieved by the proposed framework are much improved over those by the alternatives. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106230	10.1016/j.asoc.2020.106230													
J								Autonomic performance prediction framework for data warehouse queries using lazy learning approach	APPLIED SOFT COMPUTING										Data warehouse; Autonomic computing; Decision support system; Lazy learning; Case-based reasoning	MANAGEMENT	Information is one of the most important assets of an organization. In recent years, the volume of data stored in organizations, varying user requirements, time constraints, and query management complexities have grown exponentially. Due to these problems, the performance modeling of queries in data warehouses (DWs) has assumed a key role in organizations. DWs make relevant information available to decision-makers; however, DW administration is becoming increasingly difficult and time-consuming. DW administrators spend too much time managing queries, which also affects data warehouse performance. To enhance the performance of overloaded data warehouses with varying queries, a prediction-based framework is required that forecasts the behavior of query performance metrics in a DW. In this study, we propose a cluster-based autonomic performance prediction framework using a case-based reasoning approach that determines the performance metrics of the data warehouse in advance by incorporating autonomic computing characteristics. This prediction is helpful for query monitoring and management. For evaluation, we used metrics for precision, recall, accuracy, and relative error rate. The proposed approach is also compared with existing lazy learning techniques. We used the standard TPC-H dataset. Experiments show that our proposed approach produce better results compared to existing techniques. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106216	10.1016/j.asoc.2020.106216													
J								A compact neuromorphic architecture with dynamic routing to efficiently simulate the FXECAP-L algorithm for real-time active noise control	APPLIED SOFT COMPUTING										Affine projection-like algorithm; Spiking neural P systems; Rules on the synapses; Synaptic weights	NEURAL P SYSTEMS; AFFINE PROJECTION ALGORITHM; VARIABLE STEP-SIZE; ASTROCYTE-LIKE CONTROL; DENDRITIC BEHAVIOR; IMPLEMENTATION; RULES; MULTIPLIER; SYNAPSES; SPIKES	In this work, we introduce, for the first time, the design of a compact neuromorphic architecture to efficiently support a filtered-x error-coded affine projection-like (FXECAP-L) algorithm that is based on affine projection (AP) algorithms for active noise cancellation (ANC) in an acoustic duct. To date, few practical ANC implementations have used AP algorithms because of their high computational complexity, despite providing fast convergence speeds. One of the main factors that increases their computational complexity is linked to the dimensions of the matrix used in the AP algorithm's computations. Evidently, the largest dimensions of the matrix increase the convergence speed of the AP algorithms by paying a penalty in terms of area consumption. However, convergence speed is crucial in ANC applications since this factor determines the speed at which the noise is canceled. Recently, an FXECAP-L algorithm with evolving order has been proposed to dynamically reduce the dimensions of the matrix by maintaining the convergence speed of AP algorithms. Here, we propose a compact neuromorphic architecture with a dynamic routing mechanism to efficiently implement the evolutionary method of the FXECAP-L algorithm by creating a virtual matrix, whose dimensions can be modified over the filter processing. In this way, we avoid spending a large amount of memory to save the largest matrix elements. In addition, the inclusion of the dynamic routing mechanism in the proposed neuromorphic architecture has allowed us to guarantee low area consumption since the neuromorphic architecture is capable of simulating different adaptive structures without modifying its structure. Here, the neuromorphic architecture has been configured as the system identification and ANC controller for practical noise cancellation in an acoustic duct. Our results have demonstrated that the combination of the properties of the FXECAP-L algorithm and the implementation techniques generate a versatile signal processing development tool that can be used in practical real-time ANC applications. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106233	10.1016/j.asoc.2020.106233													
J								A decision-theoretic rough set model with q-rung orthopair fuzzy information and its application in stock investment evaluation	APPLIED SOFT COMPUTING										Decision making; Decision-theoretic rough sets; q-Rung orthopair fuzzy sets; Loss function	3-WAY DECISIONS; SELECTION; DEA; NUMBERS; TOPSIS	Stock investment is characterized by high risk and massive profit, so it is necessary to propose a scientific and accurate stock assessment and selection method for avoiding investment risks and obtaining high returns. Stock investment evaluation and selection can be regarded as a three-way decision (3WD) problem. Decision-theoretic rough sets (DTRSs) are an excellent tool to cope with 3WDs under risks and uncertainty. Due to the increasing complexity and high uncertainty of decision environments, the loss functions involved in DTRSs are not always expressed with real numbers. As a novel generalized form of Pythagorean fuzzy sets (PFSs) and intuitionistic fuzzy sets (IFSs), qrung orthopair fuzzy sets (q-ROFSs) depict uncertain information more widely and flexibly. Thus, it is a significant innovation to combine q-ROFSs with DTRSs and construct a new 3WD model for stock investment evaluation. More specifically, we first extend q-rung orthopair fuzzy numbers (q-ROFNs) to DTRSs, which can offer a novel illustration for loss functions. Then, we establish a novel q-rung orthopair fuzzy DTRS (q-ROFDTRS) model and explore some fundamental properties of the expected losses. Additionally, we propose two methods to handle q-ROFNs and obtain 3WDs. These two methods are compared, and their characteristics and applicability are analysed. Finally, a practical case concerning stock investment evaluation is supplied to illustrate the effectiveness and the superiority of the developed approaches over existing methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106212	10.1016/j.asoc.2020.106212													
J								Structural improved regular simplex support vector machine for multiclass classification	APPLIED SOFT COMPUTING										K-class classification; Structural information; Regular simplex support vector machine; Sequential minimization optimization	MINIMAL OPTIMIZATION ALGORITHM; REGRESSION; CONVERGENCE; CLASSIFIERS	Although the structural regularized support vector machine (SRSVM) can enhance the generalization capability of the standard support vector machine (SVM), its current version is used only for binary classification. To make SRSVM adapt to the K-class classification, the most direct approach is combining it with partitioning strategies, which may however lead to the following shortcomings: (1) Extracting structural information repeatedly for individual classifiers based on different class partitions increases the computational complexity. (2) Individual classifiers can hardly utilize complete data structural information. Under the basic framework of regular simplex support vector machine (RSSVM), we developed a novel structural improved regular simplex support vector machine (SIRSSVM). SIRSSVM generates only a single primal optimization problem, into which the data structural information within all classes is embedded, rather than using only partial structural information to construct individual classifiers as partitioning strategies do. Additionally, we modified the sequential minimization optimization (SMO)-type solver for RSSVM to adapt the proposed SIRSSVM model. Experimental results verified that our SIRSSVM could achieve excellent performance on both generalization capability and training efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106235	10.1016/j.asoc.2020.106235													
J								Mixed spatial pyramid pooling for semantic segmentation	APPLIED SOFT COMPUTING										Semantic segmentation; Convolutional neural network; Spatial pyramid pooling; Dilated convolution; Encoder-decoder architecture; Scene understanding		Semantic segmentation is a challenging task as each pixel should be labeled accurately in the image. To improve the performance of semantic segmentation, some Fully Convolutional Network (FCN) based semantic segmentation methods adopt a spatial pyramid pooling structure to enrich contextual information. Others employ an encoder-decoder architecture to recover object details gradually. In this paper, we propose a semantic segmentation framework which combines the benefits of these approaches. Specifically, we propose a Mixed Spatial Pyramid Pooling (MSPP) module based on region-based average pooling and dilated convolution to obtain dense multi-level contextual priors. To further refine the details of objects more effectively, we also propose a Global-Attention Fusion (GAF) module to provide global context as guidance for low-level features. Our proposed method achieves mIoU of 84.1% on PASCAL VOC 2012 dataset and 80.4% on Cityscapes dataset without using any post-processing or additional datasets for pretrained model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106209	10.1016/j.asoc.2020.106209													
J								Cleaning decision model of MBR membrane based on Bandelet neural network optimized by improved Bat algorithm	APPLIED SOFT COMPUTING										MBR; Membrane cleaning; Bandelet neural network; Improved Bat algorithm; Prediction	ULTRAFILTRATION MEMBRANE; FOULING CHARACTERISTICS; BIOREACTORS MBRS; WAVELET; CLASSIFICATION; ENSEMBLE; SYSTEM; PRODUCTS; SMP	The membrane fouling is an important factor of restricting wide application of MBR (Membrane Bio-Reactor), which causes the fall of membrane flux and reduces the membrane cleaning period. So the Bandelet neural network is proposed through combining Bandelet transform and neural network, which predicts membrane flux and its recovery rate for making proper membrane cleaning decision. Firstly, the main affecting factors of membrane fouling are discussed. Secondly, the architecture of Bandelet neural network is designed with Bandelet function and its scale function as activation functions of hidden and output layers respectively. Thirdly, the improved Bat algorithm is established, which is applied to improve the optimization effect of parameters of Bandelet neural network. Finally, the simulation analysis is carried out, the improved bat algorithm has higher performance than the traditional bat algorithm through analyzing the single objective optimization problem from 2018 CEC competition, the optimal number of nodes in hidden layer is confirmed based on comparison analysis and statistical tests. The proposed BNN-IBA has obvious superiority in prediction accuracy and speed according to prediction simulation results of membrane fouling of MBR, which has better prediction results than other state-of-art prediction models optimized by the novel optimal algorithms. In addition, the proper membrane cleaning period and method are confirmed according to the prediction results of membrane flux and its recovery rate. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106211	10.1016/j.asoc.2020.106211													
J								Particle filter and Levy flight-based decomposed multi-objective evolution hybridized particle swarm for flexible job shop greening scheduling with crane transportation	APPLIED SOFT COMPUTING										Greening scheduling; Particle filter; Levy flight; Decomposed multi-objective evolutionary algorithm; Particle swarm optimization	ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; LOCAL SEARCH; COMPLEXITY; SYSTEM; ENVIRONMENT; PERFORMANCE; MACHINES; NETWORK; MOEA/D	Since greening scheduling is arousing increasing attention from many manufacturing enterprises, this paper focuses on a flexible job shop greening scheduling problem with crane transportation (FJSGSP-CT). Distinguished from the traditional scheduling model which merely concentrates on machining processes, FJSGSP-CT takes the comprehensive effect of machining and crane transportation processes into consideration. Due to the NP-hard nature of the problem, an efficient hybrid algorithm, particle filter and Levy flight-based decomposed multi-objective evolution hybridized with particle swarm (PLMEAPS), is developed to find feasible solutions. The proposed PLMEAPS benefits from the synergy of decomposed multi-objective evolutionary algorithm (MOEA/D) and particle swarm optimization (PSO). Particle filter and Levy flights are then creatively fused into the framework of PLMEAPS to enhance the computational performance of the algorithm. The introduction of particle filter enriches the diversity of the population and makes it possible to predict the near optimal solutions at each iteration, and the combination of Levy flights has beneficial effect on escaping from local optimum and accelerating convergence speed. The performance of the proposed PLMEAPS is evaluated by comparing with two other high-performing intelligent optimization algorithms, the multi-objective genetic local search (MOGLS) and the multi-objective grey wolf optimizer (MOGWO). The computational results reveal that the proposed PLMEAPS outperforms the other two algorithms both in solutions' quality and convergence rate when solving FJSGSP-CT. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUN	2020	91								106217	10.1016/j.asoc.2020.106217													
J								Fuzzy-statistical prediction intervals from crisp regression models	EVOLVING SYSTEMS										Prediction intervals; Fuzzy neural networks; Fuzzy linear regression; PICP; FuzzyPICP		Most prediction outputs from regression models are either point estimates or interval estimates. Point estimates from a model are useful for making conclusions about model accuracy. Interval estimates on the other-hand are used to evaluate the uncertainty in the model predictions. These two approaches only produce either point or a single interval and thus do not fully represent the uncertainties in the model prediction. In this paper, previous works on constructing fuzzy numbers from arbitrary statistical intervals are extended by first constructing fuzzy-statistical prediction intervals which combines point and prediction interval estimates into a single fuzzy number which fully represents the uncertainties in the model. Then two simple metrics are introduced that can evaluate the quality of the proposed fuzzy-statistical prediction intervals. The proposed metrics are simple to calculate and use same ideas from the well-known metrics for evaluating interval estimates. To test the applicability of the proposed method, two types of scenarios are adopted. In the first scenario, the models are calibrated and then the proposed method is used to get the fuzzy-statistical prediction interval. In the second scenario, the point estimate and prediction intervals are given as output from a model by another researcher, then the proposed approach is used to get the fuzzy-statistical prediction intervals without prior knowledge of the model calibration process. The first scenario is tested by calibrating linear regression and neural network models using a well-known data set of automobile fuel consumption (auto-MPG). The second scenario is tested using outputs from point and interval estimates of two time series models (ARIMA, Kalman Filter) calibrated from a real traffic flow data set.																	1868-6478	1868-6486				JUN	2020	11	2			SI		201	213		10.1007/s12530-019-09285-6													
J								Study on population dynamics for triple-linked food chain using a simulation-based approach	EVOLVING SYSTEMS										Population dynamics; Ecological simulation; Agent-based modeling; Predator-prey relation; Triple-linked food chain; Evolution-inspired optimization	CELLULAR-AUTOMATA; BIODIVERSITY; STABILITY	The procedures based on simulation have become a feasible testing method that does not require investing valuable resources to create a concrete prototype, especially with the increasing computational power of computers. Thus, design changes can be adopted and design errors can be fixed before it is too late. Simulation turns to be a cheap, safe and often more acceptable from an ethical perspective. In our work we summarize the results from the analysis with the help of a computational simulation of an elementary, yet analytically intractable problem scenario from the field of ecology. Our main goal is to confirm that even with a seemingly simple agent-based model and simulation, one could obtain plausible results regarding a system's real life behavior. As a last point, we propose an efficient alternative for analysis, rather than the expensive simulation process.																	1868-6478	1868-6486				JUN	2020	11	2			SI		215	226		10.1007/s12530-019-09298-1													
J								EDUC8 pathways: executing self-evolving and personalized intra-organizational educational processes	EVOLVING SYSTEMS										Semantic meta-modeling; Educational process; Learning pathways; Academic advising; Personalization		One of the main challenges to be confronted by modern tertiary sector, so as to improve quality is the personalization of learning, which has to be combined with a minimization of the respective costs. However, personalization requires continuous reconfiguration of the academic plans since the academic status of each student, educational options and circumstances inside a Higher Educational Institution constantly change. In this paper, we present EDUC8 (EDUCATE) software environment that provides an integrated information technology solution concerning the dynamic recommendation and execution of personalized education processes. The implemented EDUC8 prototype aggregates a process execution engine, a rule engine and a semantic infrastructure for reconfiguring the learning pathways for each student. The semantic infrastructure consists of an ontology enclosing the required knowledge and a semantic rule-set. During the execution of learning pathways, the system reasons over the rules and reconfigures the next steps of the learning process. At the same time, new knowledge and facts originated from both the rule base and the learning pathway meta-models that are established during their execution are created, which constitute the evolving knowledge base of EDUC8 platform. The completeness and performance of the implemented infrastructure was tested for the modeling and selection of a set of appropriate academic recommendations regarding the Network Engineering specialization field of the Computer Science program.																	1868-6478	1868-6486				JUN	2020	11	2			SI		227	240		10.1007/s12530-019-09287-4													
J								Optimization of future charging infrastructure for commercial electric vehicles using a multi-objective genetic algorithm and real travel data	EVOLVING SYSTEMS										Electric mobility; Electric vehicles; Multi objective optimization; Genetic algorithm	LOCATION PROBLEM; MODEL	Electric mobility has gained much interest in the automotive industry and among commercial customers. A well-developed charging infrastructure is a fundamental requirement to meet the rising demand for electricity. The aim of this contribution is to demonstrate how optimization can be used for the extension of public charging infrastructure for electric vehicles (EVs). The suitability of conversion from combustion engines to EVs for commercial customers is evaluated for different scenarios. The impact of an expanded charging infrastructure is measured by a multi-objective genetic algorithm. The location and type of charging stations is optimized with respect to the number of failed trips, due to empty batteries, and the total cost of infrastructure. Assuming that the usage of commercial vehicles ist unaltered when switching to EVs, travel data from commercial vehicles with combustion engines may serve as a starting point for the optimization of the charging infrastructure. The resulting pareto front may support decision makers in placing optimal public charging stations.																	1868-6478	1868-6486				JUN	2020	11	2			SI		241	254		10.1007/s12530-019-09295-4													
J								A hybrid probabilistic bi-sector fuzzy regression based methodology for normal distributed hydrological variable	EVOLVING SYSTEMS										Fuzzy regression; Bi-sector regression; Statistical sample; Normal distribution; Cumulative annual precipitation; Hydrological variable	LINEAR-REGRESSION	An advantage of the probabilistic approach is the exploitation of the observed probability values in order to test the goodness-of-fit for the examined theoretical probability distribution function (pdf). Since, in fact, the interest of the engineers is to achieve a relation between the hydrological variable and the corresponding probability which corresponds to a selected return period, a fuzzy linear relation between the standardized normal variable Z and the examined hydrologic random variable is achieved in condition that the hydrological variable is normally distributed. In this article, primary, the implementation of the fuzzy linear regression of Tanaka is proposed regarding the annual cumulative precipitation. Thus, all the historical data will be included in the produced fuzzy band. However, since many times the question is about the inverse process, that is, the determination of the return period for a given hydrological value, then, for this purpose, a fuzzy bi-sector regression is developed. The proposed bi-sector fuzzy regression incorporates the inclusion property regarding the produced fuzzy band as the fuzzy regression of Tanaka does. The proposed innovative methodology provides the opportunity to achieve simultaneously a fuzzy assessment of the mean value and the standard deviation based on the solution of the fuzzy linear regression. To test the suitability of the produced fuzzy band, several measures are proposed which incorporate the magnitude of the produced fuzzy band and the comparison between the estimated fuzzy mean value and standard deviation with the unbiased crisp estimation of the same variables and the median of the sample.																	1868-6478	1868-6486				JUN	2020	11	2			SI		255	268		10.1007/s12530-019-09284-7													
J								A plug 'n' play approach for dynamic data acquisition from heterogeneous IoT medical devices of unknown nature	EVOLVING SYSTEMS										Internet of Things; Heterogeneous devices; Unknown device; Data acquisition; Data collection	INTEGRATION	With the rapid development of Information Technology, the existence of Cyber-Physical Systems (CPSs) has revealed, which are slowly emerging to dominate our world through their tight integration between the computational and physical components. Especially the physical components, consist of various devices, known as Internet of Things (IoT) devices, which are responsible for collecting and producing CPSs' data, whereas simultaneously are able to sense, monitor and interpret the different occasions and environments that are used. However, these devices are typically characterized by a high degree of heterogeneity, emerging the need for programming applications to deal with each specific new device in order to use its data. To address this problem, in this manuscript a generic plug 'n' play approach is proposed for connecting and recognizing heterogeneous IoT devices of both known and unknown nature, and integrating them to finally gather their data, focusing mainly in the healthcare domain. This approach is based upon a 4-step mechanism, where in the first stage the mechanism discovers and connects all the available devices of both known and unknown nature, gathering various information of them. The latter is then used in the second, third, and fourth stage, so as to identify the API methods that are responsible for collecting devices' data, and integrate them into a unified API, for finally gathering the data from all the both known and unknown devices. The proposed mechanism is evaluated through a specific use case, producing reliable results, thus being considered as a reference value of high quality and accuracy.																	1868-6478	1868-6486				JUN	2020	11	2			SI		269	289		10.1007/s12530-019-09286-5													
J								Employing query disambiguation using clustering techniques	EVOLVING SYSTEMS										Query disambiguation; Information retrieval; Query reformulation; Clustering; Containment; Semantics	EXTRACTING KNOWLEDGE	Due to the boundless expansion of the Web in the last decade, the research community has paid significant attention to the problem of effective searching in the vast information available. In this paper, we introduce a novel framework for improving information retrieval results. Initially, relevant documents are organized in clusters utilizing several metrics combined with language modelling tools. In following, a produced ranked list of the documents is returned to the user for a specific query. This is implemented as the scores between the clusters and the query representations are extracted; next in line, the internal rankings of the documents, per cluster, using these scores as weighting factor, are combined. Our proposed methodology is based on the exploitation of the inter-documents similarities (lexical and/or semantics) after a sophisticated pre-processing step. Our experimental evaluation demonstrates that the proposed algorithm can efficiently improve the quality of the retrieved results.																	1868-6478	1868-6486				JUN	2020	11	2			SI		305	315		10.1007/s12530-019-09292-7													
J								Learning of operator hand movements via least angle regression to be teached in a manipulator	EVOLVING SYSTEMS										Manipulator; Least angle regression; Reference; Kinematics; Model; Embedded platform	FUZZY; SYSTEM	In this document, a control system is developed to allow a manipulator to learn and plan references from demonstrations given by an operator hand. Data entry is acquired by a sensor and is learned by the generalized learning model with least angle regression to create a desired reference in three dimensional space. A fifth reference profile is employed to smooth the desired reference. Direct and inverse kinematics are gotten to represent the transformation between the three dimensional space and each of the manipulator links. A dynamic model is gotten using Newton-Euler formulation. An evolving proportional derivative (PD) control is applied to get that the manipulator end effector follows the operator hand movements. The monitoring and control systems are implemented in an embedded platform for testing purposes.																	1868-6478	1868-6486				JUN	2020	11	2			SI		317	332		10.1007/s12530-018-9224-1													
J								Danger theory inspired micro-population immune optimization for probabilistic constrained programming	EVOLVING SYSTEMS										Probabilistic constrained optimization; Danger theory; Micro immune optimization; Adaptive sampling; Life cycle		This work solves the problem of a general kind of single-objective probabilistic constrained programming without any a prior stochastic distribution information, after probing into an adaptive sampling-based micro immune optimization approach inspired by the danger theory in immunology. In the whole design of the algorithm, the current population is divided into uninfected, susceptible and infected sub-populations based on the version of individuals' dominance, relying upon the schemes of sample-dependent constraint handling and objective evaluation. Those uninfected and susceptible sub-populations proliferate their clones and execute adaptive mutation with small variable mutation rates, whereas the infected sub-population directly participates in mutation at a large and variable mutation rate. Two mutation strategies, together with the version of life cycle on individual, are simultaneously designed to evolve those sub-populations along different directions in order to explore those diverse and high-quality solutions. It is shown that the complexity of the algorithm depends mainly on the number of iterations. Experimental results have validated that one such approach is a competitive and potential optimizer because of few parameters, fast convergence and its ability of effective noise suppression.																	1868-6478	1868-6486				JUN	2020	11	2			SI		333	348		10.1007/s12530-019-09277-6													
J								General controllability and observability tests for Takagi-Sugeno fuzzy systems	EVOLVING SYSTEMS										Takagi-Sugeno fuzzy models; Fuzzy controllability; Fuzzy observability	IDENTIFICATION; DESIGN; MODELS	An approach for investigating controllability and observability properties in Takagi-Sugeno (TS) fuzzy systems is given. The proposed method is independent of the number of fuzzy rules acting at the same instant and independent of the number of inputs and outputs included in the TS fuzzy model. Therefore, it can be applied to a wide class of fuzzy systems. The analysis relies on the solution of a set of symbolic simultaneous equations with the fuzzy weights as the unknowns of such equations.																	1868-6478	1868-6486				JUN	2020	11	2			SI		349	358		10.1007/s12530-019-09281-w													
J								A Disocclusion Inpainting Framework for Depth-Based View Synthesis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image reconstruction; Correlation; Reconstruction algorithms; Cameras; Three-dimensional displays; Motion compensation; Two dimensional displays; Depth image based rendering; disocclusion inpainting; foreground extraction; improved background reconstruction	IMAGE COMPLETION; RANDOM-WALKS; VIDEO	This paper proposes a disocclusion inpainting framework for depth-based view synthesis. It consists of four modules: foreground extraction, motion compensation, improved background reconstruction, and inpainting. The foreground extraction module detects the foreground objects and removes them from both depth map and rendered video; the motion compensation module guarantees the background reconstruction model to suit for moving camera scenarios; the improved background reconstruction module constructs a stable background video by exploiting the temporal correlation information in both 2D video and its corresponding depth map; and the constructed background video and inpainting module are used to eliminate the holes in the synthesized view. The analysis and experiment indicate that the proposed framework has good generality, scalability and effectiveness, which means most of the existing background reconstruction methods and image inpainting methods can be employed or extended as the modules in our framework. Our comparison results have demonstrated that the proposed framework achieves better synthesized quality, temporal consistency, and has lower running time compared to the other methods.																	0162-8828	1939-3539				JUN 1	2020	42	6					1289	1302		10.1109/TPAMI.2019.2899837													
J								Absent Multiple Kernel Learning Algorithms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Kernel; Optimization; Signal processing algorithms; Clustering algorithms; Classification algorithms; Pattern analysis; Absent data learning; multiple kernel learning; max-margin classification	MATRIX; RECOGNITION; MARGIN	Multiple kernel learning (MKL) has been intensively studied during the past decade. It optimally combines the multiple channels of each sample to improve classification performance. However, existing MKL algorithms cannot effectively handle the situation where some channels of the samples are missing, which is not uncommon in practical applications. This paper proposes three absent MKL (AMKL) algorithms to address this issue. Different from existing approaches where missing channels are first imputed and then a standard MKL algorithm is deployed on the imputed data, our algorithms directly classify each sample based on its observed channels, without performing imputation. Specifically, we define a margin for each sample in its own relevant space, a space corresponding to the observed channels of that sample. The proposed AMKL algorithms then maximize the minimum of all sample-based margins, and this leads to a difficult optimization problem. We first provide two two-step iterative algorithms to approximately solve this problem. After that, we show that this problem can be reformulated as a convex one by applying the representer theorem. This makes it readily be solved via existing convex optimization packages. In addition, we provide a generalization error bound to justify the proposed AMKL algorithms from a theoretical perspective. Extensive experiments are conducted on nine UCI and six MKL benchmark datasets to compare the proposed algorithms with existing imputation-based methods. As demonstrated, our algorithms achieve superior performance and the improvement is more significant with the increase of missing ratio.																	0162-8828	1939-3539				JUN 1	2020	42	6					1303	1316		10.1109/TPAMI.2019.2895608													
J								End-to-End Active Object Tracking and Its Real-World Deployment via Reinforcement Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Object tracking; Cameras; Target tracking; Reinforcement learning; Robot vision systems; Active object tracking; reinforcement learning; environment augmentation	VISUAL TRACKING; NETWORKS	We study active object tracking, where a tracker takes visual observations (i.e., frame sequences) as input and produces the corresponding camera control signals as output (e.g., move forward, turn left, etc.). Conventional methods tackle tracking and camera control tasks separately, and the resulting system is difficult to tune jointly. These methods also require significant human efforts for image labeling and expensive trial-and-error system tuning in the real world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates good generalization behaviors in the case of unseen object moving paths, unseen object appearances, unseen backgrounds, and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer, via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.																	0162-8828	1939-3539				JUN 1	2020	42	6					1317	1332		10.1109/TPAMI.2019.2899570													
J								Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric Fits	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Quadrics; surface fitting; implicit surfaces; point clouds; 3D surface detection; primitive fitting; minimal problems	EFFICIENT RANSAC; SEGMENTATION; ROBUST; PLANAR; EXTRACTION; SURFACES; CURVES; SPACE	We present a novel and effective method for detecting 3D primitives in cluttered, unorganized point clouds, without axillary segmentation or type specification. We consider the quadric surfaces for encapsulating the basic building blocks of our environments - planes, spheres, ellipsoids, cones or cylinders, in a unified fashion. Moreover, quadrics allow us to model higher degree of freedom shapes, such as hyperboloids or paraboloids that could be used in non-rigid settings. We begin by contributing two novel quadric fits targeting 3D point sets that are endowed with tangent space information. Based upon the idea of aligning the quadric gradients with the surface normals, our first formulation is exact and requires as low as four oriented points. The second fit approximates the first, and reduces the computational effort. We theoretically analyze these fits with rigor, and give algebraic and geometric arguments. Next, by re-parameterizing the solution, we devise a new local Hough voting scheme on the null-space coefficients that is combined with RANSAC, reducing the complexity from $O(N<^>4)$O(N4) to $O(N<^>3)$O(N3) (three points). To the best of our knowledge, this is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes without segmentation. Our extensive qualitative and quantitative results show that our method is efficient and flexible, as well as being accurate.																	0162-8828	1939-3539				JUN 1	2020	42	6					1333	1347		10.1109/TPAMI.2019.2900309													
J								Hierarchical Surface Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Geometry; Image color analysis; Shape; Octrees; Color; Surface reconstruction; Single view reconstruction; high resolution; voxel grid; geometry prediction	RECONSTRUCTION; OPTIMIZATION	Recently, Convolutional Neural Networks have shown promising results for 3D geometry prediction. They can make predictions from very little input data such as a single color image. A major limitation of such approaches is that they only predict a coarse resolution voxel grid, which does not capture the surface of the objects well. We propose a general framework, called hierarchical surface prediction (HSP), which facilitates prediction of high resolution voxel grids. The main insight is that it is sufficient to predict high resolution voxels around the predicted surfaces. The exterior and interior of the objects can be represented with coarse resolution voxels. This allows us to predict significantly higher resolution voxel grids around the surface, from which triangle meshes can be extracted. Additionally it allows us to predict properties such as surface color which are only defined on the surface. Our approach is not dependent on a specific input type. We show results for geometry prediction from color images and depth images. Our analysis shows that our high resolution predictions are more accurate than low resolution predictions.																	0162-8828	1939-3539				JUN 1	2020	42	6					1348	1361		10.1109/TPAMI.2019.2896296													
J								Hyperbolic Wasserstein Distance for Shape Indexing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Shape; Three-dimensional displays; Measurement; Space vehicles; Indexing; Face; Shape space; hyperbolic conformal geometry; Wasserstein distance; shape indexing	MILD COGNITIVE IMPAIRMENT; OPTIMAL MASS-TRANSPORT; ALZHEIMERS-DISEASE; FACE RECOGNITION; REARRANGEMENT; SEGMENTATION; REGISTRATION; MORPHOMETRY; FRAMEWORK; SURFACES	Shape space is an active research topic in computer vision and medical imaging fields. The distance defined in a shape space may provide a simple and refined index to represent a unique shape. This work studies the Wasserstein space and proposes a novel framework to compute the Wasserstein distance between general topological surfaces by integrating hyperbolic Ricci flow, hyperbolic harmonic map, and hyperbolic power Voronoi diagram algorithms. The resulting hyperbolic Wasserstein distance can intrinsically measure the similarity between general topological surfaces. Our proposed algorithms are theoretically rigorous and practically efficient. It has the potential to be a powerful tool for 3D shape indexing research. We tested our algorithm with human face classification and Alzheimer's disease (AD) progression tracking studies. Experimental results demonstrated that our work may provide a succinct and effective shape index.																	0162-8828	1939-3539				JUN 1	2020	42	6					1362	1376		10.1109/TPAMI.2019.2898400													
J								Joint Rain Detection and Removal from a Single Image with Contextualized Deep Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Rain; Shape; Atmospheric modeling; Image restoration; Deep learning; Degradation; Computer vision; Rain removal; rain detection; deep learning; rain accumulation; contextualized dilated network	QUALITY ASSESSMENT; SUPERRESOLUTION; VISION; MODEL	Rain streaks, particularly in heavy rain, not only degrade visibility but also make many computer vision algorithms fail to function properly. In this paper, we address this visibility problem by focusing on single-image rain removal, even in the presence of dense rain streaks and rain-streak accumulation, which is visually similar to mist or fog. To achieve this, we introduce a new rain model and a deep learning architecture. Our rain model incorporates a binary rain map indicating rain-streak regions, and accommodates various shapes, directions, and sizes of overlapping rain streaks, as well as rain accumulation, to model heavy rain. Based on this model, we construct a multi-task deep network, which jointly learns three targets: the binary rain-streak map, rain streak layers, and clean background, which is our ultimate output. To generate features that can be invariant to rain steaks, we introduce a contextual dilated network, which is able to exploit regional contextual information. To handle various shapes and directions of overlapping rain streaks, our strategy is to utilize a recurrent process that progressively removes rain streaks. Our binary map provides a constraint and thus additional information to train our network. Extensive evaluation on real images, particularly in heavy rain, shows the effectiveness of our model and architecture.																	0162-8828	1939-3539				JUN 1	2020	42	6					1377	1393		10.1109/TPAMI.2019.2895793													
J								Measuring Shapes with Desired Convex Polygons	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Shape; Shape measurement; Extraterrestrial measurements; Linearity; Area measurement; Tuning; Rotation measurement; Shape; shape descriptors; shape measure; shape convexity; image processing; pattern recognition	PATTERN-RECOGNITION; ELLIPTICITY; CIRCULARITY; DESCRIPTOR; RETRIEVAL; ORIENTATION; INVARIANT; MOMENTS	In this paper we have developed a family of shape measures. All the measures from the family evaluate the degree to which a shape looks like a predefined convex polygon. A quite new approach in designing object shape based measures has been applied. In most cases such measures were defined by exploiting some shape properties. Such properties are optimized (e.g., maximized or minimized) by certain shapes and based on this, the new shape measures were defined. An illustrative example might be the shape circularity measure derived by exploiting the well-known result that the circle has the largest area among all the shapes with the same perimeter. Of course, there are many more such examples (e.g., ellipticity, linearity, elongation, and squareness measures are some of them). There are different approaches as well. In the approach applied here, no desired property is needed and no optimizing shape has to be found. We start from a desired convex polygon, and develop the related shape measure. The method also allows a tuning parameter. Thus, there is a new 2-fold family of shape measures, dependent on a predefined convex polygon, and a tuning parameter, that controls the measure's behavior. The measures obtained range over the interval (0,1] and pick the maximal possible value, equal to 1, if and only if the measured shape coincides with the selected convex polygon that was used to develop the particular measure. All the measures are invariant with respect to translations, rotations, and scaling transformations. An extension of the method leads to a family of new shape convexity measures.																	0162-8828	1939-3539				JUN 1	2020	42	6					1394	1407		10.1109/TPAMI.2019.2898830													
J								Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Optical flow; pyramid; warping; cost volume; and convolutional neural network (CNN)		We investigate two crucial and closely-related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11 percent more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure for PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56 percent more accurate on Sintel final than the previously trained one and even 5 percent more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10 percent and on KITTI 2012 and 2015 by 20 percent. Our newly trained model parameters and training protocols are available on https://github.com/NVlabs/PWC-Net.																	0162-8828	1939-3539				JUN 1	2020	42	6					1408	1423		10.1109/TPAMI.2019.2894353													
J								Progressive Representation Adaptation for Weakly Supervised Object Localization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Proposals; Detectors; Training; Feature extraction; Clutter; Noise measurement; Adaptation models; Weakly supervised learning; object localization; domain adaptation		We address the problem of weakly supervised object localization where only image-level annotations are available for training object detectors. Numerous methods have been proposed to tackle this problem through mining object proposals. However, a substantial amount of noise in object proposals causes ambiguities for learning discriminative object models. Such approaches are sensitive to model initialization and often converge to undesirable local minimum solutions. In this paper, we propose to overcome these drawbacks by progressive representation adaptation with two main steps: 1) classification adaptation and 2) detection adaptation. In classification adaptation, we transfer a pre-trained network to a multi-label classification task for recognizing the presence of a certain object in an image. Through the classification adaptation step, the network learns discriminative representations that are specific to object categories of interest. In detection adaptation, we mine class-specific object proposals by exploiting two scoring strategies based on the adapted classification network. Class-specific proposal mining helps remove substantial noise from the background clutter and potential confusion from similar objects. We further refine these proposals using multiple instance learning and segmentation cues. Using these refined object bounding boxes, we fine-tune all the layer of the classification network and obtain a fully adapted detection network. We present detailed experimental validation on the PASCAL VOC and ILSVRC datasets. Experimental results demonstrate that our progressive representation adaptation algorithm performs favorably against the state-of-the-art methods.																	0162-8828	1939-3539				JUN 1	2020	42	6					1424	1438		10.1109/TPAMI.2019.2899839													
J								Rolling Shutter Camera Absolute Pose	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Cameras; Mathematical model; Optimization; Computational modeling; Data models; Analytical models; Standards; Computer vision; camera absolute pose; rolling shutter; minimal problems		We present minimal, non-iterative solutions to the absolute pose problem for images from rolling shutter cameras. The absolute pose problem is a key problem in computer vision and rolling shutter is present in a vast majority of today's digital cameras. We discuss several camera motion models and propose two feasible rolling shutter camera models for a polynomial solver. In previous work a linearized camera model was used that required an initial estimate of the camera orientation. We show how to simplify the system of equations and make this solver faster. Furthermore, we present a first solution of the non-linearized camera orientation model using the Cayley parameterization. The new solver does not require any initial camera orientation estimate and therefore serves as a standalone solution to the rolling shutter camera pose problem from six 2D-to-3D correspondences. We show that our algorithms outperform P3P followed by a non-linear refinement using a rolling shutter model.																	0162-8828	1939-3539				JUN 1	2020	42	6					1439	1452		10.1109/TPAMI.2019.2894395													
J								Skeleton-Based Online Action Prediction Using Scale Selection Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Microsoft Windows; Skeleton; Three-dimensional displays; Task analysis; Videos; Real-time systems; Pattern recognition; Action prediction; scale selection; sliding window; dilated convolution; skeleton data	ACTION RECOGNITION; MODEL	Action prediction is to recognize the class label of an ongoing activity when only a part of it is observed. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the temporal axis. Since there are significant temporal scale variations in the observed part of the ongoing action at different time steps, a novel window scale selection method is proposed to make our network focus on the performed part of the ongoing action and try to suppress the possible incoming interference from the previous actions at each step. An activation sharing scheme is also proposed to handle the overlapping computations among the adjacent time steps, which enables our framework to run more efficiently. Moreover, to enhance the performance of our framework for action prediction with the skeletal input data, a hierarchy of dilated tree convolutions are also designed to learn the multi-level structured semantic representations over the skeleton joints at each frame. Our proposed approach is evaluated on four challenging datasets. The extensive experiments demonstrate the effectiveness of our method for skeleton-based online action prediction.																	0162-8828	1939-3539				JUN 1	2020	42	6					1453	1467		10.1109/TPAMI.2019.2898954													
J								Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms, and Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Optimization; Machine learning; Principal component analysis; Videos; Standards; Calcium; Imaging; Low-rank matrix factorization; non-convex optimization; calcium imaging; hyperspectral compressed recovery	THRESHOLDING ALGORITHM; OPTIMIZATION	Convex formulations of low-rank matrix factorization problems have received considerable attention in machine learning. However, such formulations often require solving for a matrix of the size of the data matrix, making it challenging to apply them to large scale datasets. Moreover, in many applications the data can display structures beyond simply being low-rank, e.g., images and videos present complex spatio-temporal structures that are largely ignored by standard low-rank methods. In this paper we study a matrix factorization technique that is suitable for large datasets and captures additional structure in the factors by using a particular form of regularization that includes well-known regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is non-convex, we show that if the size of the factors is large enough, under certain conditions, any local minimizer for the factors yields a global minimizer. A few practical algorithms are also provided to solve the matrix factorization problem, and bounds on the distance from a given approximate solution of the optimization problem to the global optimum are derived. Examples in neural calcium imaging video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets.																	0162-8828	1939-3539				JUN 1	2020	42	6					1468	1482		10.1109/TPAMI.2019.2900306													
J								Training Faster by Separating Modes of Variation in Batch-Normalized Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Batch normalization; convolutional neural networks; generative probability models; Gaussian mixture model; fisher vector		Batch Normalization (BN) is essential to effectively train state-of-the-art deep Convolutional Neural Networks (CNN). It normalizes the layer outputs during training using the statistics of each mini-batch. BN accelerates training procedure by allowing to safely utilize large learning rates and alleviates the need for careful initialization of the parameters. In this work, we study BN from the viewpoint of Fisher kernels that arise from generative probability models. We show that assuming samples within a mini-batch are from the same probability density function, then BN is identical to the Fisher vector of a Gaussian distribution. That means batch normalizing transform can be explained in terms of kernels that naturally emerge from the probability density function that models the generative process of the underlying data distribution. Consequently, it promises higher discrimination power for the batch-normalized mini-batch. However, given the rectifying non-linearities employed in CNN architectures, distribution of the layer outputs show an asymmetric characteristic. Therefore, in order for BN to fully benefit from the aforementioned properties, we propose approximating underlying data distribution not with one, but a mixture of Gaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM), reveals that batch normalization can be improved by independently normalizing with respect to the statistics of disentangled sub-populations. We refer to our proposed soft piecewise version of batch normalization as Mixture Normalization (MN). Through extensive set of experiments on CIFAR-10 and CIFAR-100, using both a 5-layers deep CNN and modern Inception-V3 architecture, we show that mixture normalization reduces required number of gradient updates to reach the maximum test accuracy of the batch-normalized model by $\sim 31\%-47\%$similar to 31%-47% across a variety of training scenarios. Replacing even a few BN modules with MN in the 48-layers deep Inception-V3 architecture is sufficient to not only obtain considerable training acceleration but also better final test accuracy. We show that similar observations are valid for 40 and 100-layers deep DenseNet architectures as well. We complement our study by evaluating the application of mixture normalization to the Generative Adversarial Networks (GANs), where "mode collapse" hinders the training process. We solely replace a few batch normalization layers in the generator with our proposed mixture normalization. Our experiments using Deep Convolutional GAN (DCGAN) on CIFAR-10 show that mixture-normalized DCGAN not only provides an acceleration of $\sim 58\%$similar to 58% but also reaches lower (better) "Frechet Inception Distance" (FID) of 33.35 compared to 37.56 of its batch-normalized counterpart.																	0162-8828	1939-3539				JUN 1	2020	42	6					1483	1500		10.1109/TPAMI.2019.2895781													
J								Unsupervised Video Matting via Sparse and Low-Rank Representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Optical imaging; Dictionaries; Image color analysis; Streaming media; Topology; Geometrical optics; Benchmark testing; Video matting; image matting; sparse representation; low-rank; unsupervised; discriminative dictionary	IMAGE SUPERRESOLUTION	A novel method, unsupervised video matting via sparse and low-rank representation, is proposed which can achieve high quality in a variety of challenging examples featuring illumination changes, feature ambiguity, topology changes, transparency variation, dis-occlusion, fast motion and motion blur. Some previous matting methods introduced a nonlocal prior to search samples for estimating the alpha matte, which have achieved impressive results on some data. However, on one hand, searching inadequate or excessive samples may miss good samples or introduce noise; on the other hand, it is difficult to construct consistent nonlocal structures for pixels with similar features, yielding video mattes with spatial and temporal inconsistency. In this paper, we proposed a novel video matting method to achieve spatially and temporally consistent matting result. Toward this end, a sparse and low-rank representation model is introduced to pursue consistent nonlocal structures for pixels with similar features. The sparse representation is used to adaptively select best samples and accurately construct the nonlocal structures for all pixels, while the low-rank representation is used to globally ensure consistent nonlocal structures for pixels with similar features. The two representations are combined to generate spatially and temporally consistent video mattes. We test our method on lots of dataset including the benchmark dataset for image matting and dataset for video matting. Our method has achieved the best performance among all unsupervised matting methods in the public alpha matting evaluation dataset for images.																	0162-8828	1939-3539				JUN 1	2020	42	6					1501	1514		10.1109/TPAMI.2019.2895331													
J								Joint Segmentation and Path Classification of Curvilinear Structures	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image segmentation; Image edge detection; Roads; Task analysis; Decoding; Feature extraction; Computer architecture; Deep convolutional neural networks; multi-task learning; segmentation; delineation; curvilinear structures; road detection; neuron tracing	CENTERLINE EXTRACTION; NETWORKS	Detection of curvilinear structures in images has long been of interest. One of the most challenging aspects of this problem is inferring the graph representation of the curvilinear network. Most existing delineation approaches first perform binary segmentation of the image and then refine it using either a set of hand-designed heuristics or a separate classifier that assigns likelihood to paths extracted from the pixel-wise prediction. In our work, we bridge the gap between segmentation and path classification by training a deep network that performs those two tasks simultaneously. We show that this approach is beneficial because it enforces consistency across the whole processing pipeline. We apply our approach on roads and neurons datasets.																	0162-8828	1939-3539				JUN 1	2020	42	6					1515	1521		10.1109/TPAMI.2019.2921327													
J								Learning Local Metrics and Influential Regions for Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Measurement; Task analysis; Learning systems; Mathematical model; Fasteners; Artificial neural networks; Clustering algorithms; Distance-based classification; distance metric; metric learning; local metric		The performance of distance-based classifiers heavily depends on the underlying distance metric, so it is valuable to learn a suitable metric from the data. To address the problem of multimodality, it is desirable to learn local metrics. In this short paper, we define a new intuitive distance with local metrics and influential regions, and subsequently propose a novel local metric learning algorithm called LMLIR for distance-based classification. Our key intuition is to partition the metric space into influential regions and a background region, and then regulate the effectiveness of each local metric to be within the related influential regions. We learn multiple local metrics and influential regions to reduce the empirical hinge loss, and regularize the parameters on the basis of a resultant learning bound. Encouraging experimental results are obtained from various public and popular data sets.																	0162-8828	1939-3539				JUN 1	2020	42	6					1522	1529		10.1109/TPAMI.2019.2914899													
J								Multilabel Deep Visual-Semantic Embedding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Semantics; Computational modeling; Visualization; Training; Task analysis; Convolutional neural networks; Redundancy; Multilabel classification; visual semantic embedding; convolutional neural networks		Inspired by the great success from deep convolutional neural networks (CNNs) for single-label visual-semantic embedding, we exploit extending these models for multilabel images. We propose a new learning paradigm for multilabel image classification, in which labels are ranked according to its relevance to the input image. In contrast to conventional CNN models that learn a latent vector representation (i.e., the image embedding vector), the developed visual model learns a mapping (i.e., a transformation matrix) from an image in an attempt to differentiate between its relevant and irrelevant labels. Despite the conceptual simplicity of our approach, the proposed model achieves state-of-the-art results on three public benchmark datasets.																	0162-8828	1939-3539				JUN 1	2020	42	6					1530	1536		10.1109/TPAMI.2019.2911065													
J								Subspace Clustering via Good Neighbors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Clustering algorithms; Sparse matrices; Correlation; Clustering methods; Optimization; Minimization; Matching pursuit algorithms; Spectral-based subspace clustering; post-processing; good neighbors; sparsity; graph connectivity	SEGMENTATION; ROBUST	Finding the informative subspaces of high-dimensional datasets is at the core of numerous applications in computer vision, where spectral-based subspace clustering is arguably the most widely studied method due to its strong empirical performance. Such algorithms first compute an affinity matrix to construct a self-representation for each sample using other samples as a dictionary. Sparsity and connectivity of the self-representation play important roles in effective subspace clustering. However, simultaneous optimization of both factors is difficult due to their conflicting nature, and most existing methods are designed to address only one factor. In this paper, we propose a post-processing technique to optimize both sparsity and connectivity by finding good neighbors. Good neighbors induce key connections among samples within a subspace and not only have large affinity coefficients but are also strongly connected to each other. We reassign the coefficients of the good neighbors and eliminate other entries to generate a new coefficient matrix. We show that the few good neighbors can effectively recover the subspace, and the proposed post-processing step of finding good neighbors is complementary to most existing subspace clustering algorithms. Experiments on five benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods with negligible additional computation cost.																	0162-8828	1939-3539				JUN 1	2020	42	6					1537	1544		10.1109/TPAMI.2019.2913863													
J								An Overview of the Fuzzy Data Envelopment Analysis Research and Its Successful Applications	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy data envelopment analysis; Literature review; Retrospective analysis; Theory development; Method application	DECISION-MAKING UNITS; MATHEMATICAL-PROGRAMMING APPROACH; DEA MODEL; PERFORMANCE EVALUATION; SUPPLY CHAIN; EFFICIENCY MEASURES; CROSS-EFFICIENCY; PRODUCTIVE EFFICIENCY; LOCATION OPTIMIZATION; ENERGY TECHNOLOGIES	Data envelopment analysis (DEA) is a prominent technique to make decisions and improve alternatives based on non-parameter modeling and ratio calculation. However, an obvious difficulty to use this method is how to obtain accurate input and output data in the real application. To address this issue, the fuzzy DEAs (FDEAs) are proposed which have been successfully applied in many real fields. The FDEAs hold two aforementioned advantages; meanwhile, it can conveniently present uncertain evaluation information. Therefore, the FDEAs have received much attention from researchers. To summarize the current status, development trends, and further studies of the FDEA research, this paper investigates the related publications from two perspectives of description analysis and literature review, which includes the following details: (1) the literature retrospective analysis of the FDEA with bibliometric technique. Based on it, the publication overview, the cluster network, the emerging trend, and the burst detection are demonstrated in detail. (2) The method review of the basic FDEAs and two kinds of extended FDEAs. These FDEAs are proposed by introducing different fuzzy inputs and outputs, developing different theoretical DEAs, and integrating different mathematical models, respectively. (3) The application review of the FDEAs in some real-life situations. The obtained results provide some clues for researchers who are interested in the FDEA research to do further investigations on theory development and practical applications.																	1562-2479	2199-3211				JUN	2020	22	4					1037	1055		10.1007/s40815-020-00853-6													
J								Multi-parameter Portfolio Selection Model with Some Novel Score-Deviation Under Dual Hesitant Fuzzy Environment	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Dual hesitant fuzzy set; Portfolio selection; Score-deviation; Information preference; Risk appetite	ATTRIBUTE DECISION-MAKING; CORRELATION-COEFFICIENT; AGGREGATION OPERATORS; SIMILARITY MEASURES; SETS; INVESTMENT; DISTANCE	In risk investment, investors have to rely on uncertain information when it is difficult to obtain enough precise data. Dual hesitant fuzzy set (DHFS) is more applicable to deal with uncertain information because it involves membership degrees and non-membership degrees, which can validly describe positive and negative information, respectively. Although there has been research on decision-making based on the DHFS, the focus still remains on ranking the alternatives and choosing the best one, which cannot help investors to find the optimal portfolios. Therefore, to solve this problem, we mainly propose two novel portfolio selection models based on the DHFS in this paper. Firstly, we propose a Max-score dual hesitant fuzzy portfolio selection model with information preference (Model 3) for investors focusing on returns regardless of risks. Secondly, to consider the risks of portfolios, we improve Model 3 and develop a score-deviation dual hesitant fuzzy portfolio selection model with information preference and risk appetite (Model 5). Finally, a case study is conducted to highlight the effectiveness of the proposed models. A detailed sensitivity analysis and an efficient frontier analysis show that Model 5 can validly capture investors' information preferences and risk appetites. Furthermore, compared with the hesitant fuzzy portfolio model, Model 5 can offer more options to the investors with different information preferences.																	1562-2479	2199-3211				JUN	2020	22	4					1123	1141		10.1007/s40815-020-00835-8													
J								Deadbeat Predictive Power Control with Fuzzy PI Compound Controller and Power Predictive Corrector for PWM Rectifier Under Unbalanced Grid Conditions	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										PWM rectifier; Fuzzy PI compound controller; Power predictive corrector; Deadbeat predictive power control	CONVERTER; MODEL	Under unbalanced grid conditions, the DC side voltage of pulse width modulation (PWM) rectifier will overshoot during start-up and reference voltage transients, which may lead to system instability. In this paper, a deadbeat predictive power control (DPPC) with fuzzy PI compound controller (FPCC) and power predictive corrector (PPC) is proposed to solve that problem. Firstly, the parameters of the PI controller are adjusted online by the fuzzy control rule of the FPCC to eliminate the overshoot of the DC side voltage and contribute to faster dynamic responses, which thereby could correct the reference active power. Secondly, the static error between the reference and system active powers is reduced by accumulative predictive errors in the PPC. The simulation is carried out under ideal and unbalanced grid conditions. The result shows that the proposed control scheme can effectively eliminate the overshoot of DC voltage, reduce the static error of active power, and improve the dynamic response and anti-interference ability of the rectifier.																	1562-2479	2199-3211				JUN	2020	22	4					1277	1288		10.1007/s40815-020-00847-4													
J								Design of Non-fragile Controller for Singular Fractional Order Takagi-Sugeno Fuzzy Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Singular systems; Fractional order systems; Takagi-Sugeno fuzzy systems; Non-fragile output feedback control	ROBUST-CONTROL; STABILIZATION; ADMISSIBILITY	This paper investigates the stabilization problem of a class of singular fractional order fuzzy systems with order 0<alpha<1$$0. Firstly, by the method of full rank decomposition of matrix, equality constraint of admissibility criterion in most existing literature is eliminated. Next, we present a stabilization criterion for singular fractional order Takagi-Sugeno (T-S) fuzzy systems by designing non-fragile state feedback controllers. Then, applying an equivalent form of original system in admissibility, the non-fragile output feedback controllers are designed to guarantee the closed-loop systems stable. And for fractional order 1<alpha<2 $$1 case, using similar designing approaches, strict linear matrix inequality (LMI) stabilization criteria based on non-fragile state and output feedback controller are obtained. Finally, two numerical simulation examples are given to illustrate the effectiveness of the proposed method.																	1562-2479	2199-3211				JUN	2020	22	4					1289	1298		10.1007/s40815-020-00822-z													
J								Takagi-Sugeno Fuzzy Neural Network Hysteresis Modeling for Magnetic Shape Memory Alloy Actuator Based on Modified Bacteria Foraging Algorithm	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Magnetic shape memory alloy; Hysteresis modeling; Fuzzy neural network; Modified bacteria foraging algorithm	GENETIC ALGORITHM; GA ALGORITHM; OPTIMIZATION; IDENTIFICATION	The magnetic shape memory alloy (MSMA)-based actuator, as a new type of actuator, has a great application prospect in the micro-precision positioning field. However, the input-to-output hysteresis nonlinearity largely hinders its wide application. In this paper, a Takagi-Sugeno fuzzy neural network (TSFNN) model based on the modified bacteria foraging algorithm (MBFA) is innovatively utilized to describe the complex hysteresis nonlinearity of the MSMA-based actuator, and the parameters of TSFNN are optimized by the MBFA. The TSFNN is a combination of the fuzzy-logic system and neural network; thus, it has the capability of approximating the nonlinear mapping function and self-adjustment and is suitable for hysteresis modeling. The MBFA, which can obtain better optimization values, is employed for the parameter identification procedure. To demonstrate the effectiveness of the proposed model, a TSFNN based on the gradient descent algorithm (GDA) is used for comparison. Experimental results clearly show that the proposed modeling method can accurately describe the hysteresis nonlinearity of the MSMA-based actuator and has significance for its future application.																	1562-2479	2199-3211				JUN	2020	22	4					1314	1329		10.1007/s40815-020-00826-9													
J								Robust Deep Neural Network Using Fuzzy Denoising Autoencoder	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Deep neural network; Fuzzy denoising autoencoder; Compact parameter strategy; Adaptive back-propagation algorithm	CLASSIFICATION; OPTIMIZATION; PREDICTION; ARCHITECTURE	Deep neural network (DNN) has been applied in many fields and achieved great successes. However, DNN suffers from poor robustness for uncertainties because of its characteristic of the deterministic representation. To overcome this problem, a novel robust DNN (RDNN) is designed in this paper. First, a fuzzy denoising autoencoder (FDA) is developed to replace the general base-building unit in DNN. Then, the proposed RDNN is able to extract the robust representations to weaken the uncertainties. Second, a compact parameter strategy (CPS) is designed to reconstruct the parameters of FDA. Then, the computational burden of FDA can be alleviated to speed up the learning process. Third, an adaptive back-propagation (ABP) algorithm, with an adaptive learning rate strategy, is proposed to update the parameters of RDNN. Then, the performance of RDNN can be improved. Finally, the results on the benchmark problems and real applications demonstrate the effectiveness of RDNN.																	1562-2479	2199-3211				JUN	2020	22	4					1356	1375		10.1007/s40815-020-00845-6													
J								Detecting the presence of anterior cruciate ligament injury based on gait dynamics disparity and neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Anterior cruciate ligament; Nonlinear gait dynamics; Kinematics; Cyclogram; Phase portrait; Neural networks	ACL-DEFICIENT; MOVEMENT VARIABILITY; JOINT KINEMATICS; HUMAN KNEE; PATTERN; WALKING; MODEL; STABILITY; QUANTIFICATION; CLASSIFICATION	The aim of this study is to develop a new pattern recognition-based method to model and discriminate gait dynamics disparity between anterior cruciate ligament (ACL) deficient (ACL-D) knee and contralateral ACL-intact (ACL-I) knee in patients with unilateral ACL deficiency by using kinematic features and neural networks. Thereby the capabilities of these features to detect the presence of injury can be assessed. The proposed method consists of two stages. In the first (training) stage, gait analysis is performed. A two-dimensional five-link biped model used for imitating human gait locomotion is employed to demonstrate that functions containing kinematic data of lower extremities, including knee and hip flexion/extension angles and angular velocities, characterize the gait system dynamics. Knee angle-hip angle cyclograms, knee and hip angle-angular velocity phase portraits visually demonstrate the significant disparity of gait dynamics between the lower extremities of patients with unilateral ACL deficiency. Gait dynamics underlying gait patterns of ACL-D and ACL-I knees are locally accurately modeled and approximated by radial basis function (RBF) neural networks via deterministic learning theory. The derived knowledge of approximated gait dynamics is preserved in constant RBF networks. In the second (classification) stage, a bank of dynamical estimators is constructed using the preserved constant RBF networks to represent the learned training gait patterns. By comparing the set of estimators with a test gait pattern, the generated average L1norms of errors are taken as the disparity and classification measure between the training and test gait patterns to differentiate between ACL-D and ACL-I knees. Finally, experiments are carried out on forty-three patients to assess the effectiveness of the proposed method. By using the leave-one-out cross-validation style under normal and fast walking speed conditions, the correct classification rates for discriminating between ACL-D and ACL-I knees are reported to be 95.61% espectively. Compared with other state-of-the-art methods, the results demonstrate that gait alterations in the presence of chronic ACL deficiency can be visualized through cyclograms and phase portraits, and can be detected with superior performance.																	0269-2821	1573-7462				JUN	2020	53	5					3153	3176		10.1007/s10462-019-09758-9													
J								An integrating genetic algorithm and modified Newton method for tracking control and vibration suppression	ARTIFICIAL INTELLIGENCE REVIEW										Genetic algorithm; Modified Newton method; Tracking control; Vibration control	EXTRACT HEURISTIC KNOWLEDGE; NEURAL-NETWORK CONTROLLER; ACTIVE STRUCTURAL CONTROL; MASS-DAMPER; INDUCTION-MOTORS; INCIPIENT FAULTS; FUZZY SYSTEM; IDENTIFICATION; MODEL; FRICTION	With the trend toward taller and more flexible building structures, a mass-damper shaking table system has been considered as means for vibration suppression to external excitation and disturbances in recent years. However, there are few researches on the control of nonlinear structure using active mass damper under earthquake excitation, especially for high-rise building. This study presents a model combining the advantages of adaptive genetic algorithm and modified Newton method is developed for system identification and vibration suppression of a building structure with an active mass damper. The genetic algorithm with adaptive reproduction, crossover, and mutation operators is to search for initial weight and bias of the neural network, while the modified Newton method, similar to BFGS algorithm, is to increase network training performance. Experimental results show that the controller performance is strongly influenced by the accuracy of system identification. The controller is also shown to be robust to variations in system parameters.																	0269-2821	1573-7462				JUN	2020	53	5					3177	3199		10.1007/s10462-019-09759-8													
J								Machine learning in telemetry data mining of space mission: basics, challenging and future directions	ARTIFICIAL INTELLIGENCE REVIEW										Satellite telemetry data mining; Satellite health monitoring; Satellite ground control operations; Machine learning; Deep learning; Debris; Aerospace engineering	NEURAL-NETWORK; FAULT-DETECTION; SATELLITE; ALGORITHM; MODEL	The development of an intelligent artificial satellite health monitoring system is a key issue in aerospace engineering that determines satellite health status and failure using telemetry data. The modern design of data mining and machine learning technologies allows the use of satellite telemetry data and the mining of integrated information to produce an advanced health monitoring system. This paper reviews the current status and presents a framework of necessary processes on data mining to solving various problems in telemetry data such as error detection, prediction, summarization, and visualization of large quantities, and help them understand the health status of the satellite and detect the symptoms of anomalies. Machine learning technologies that include neural networks, fuzzy sets, rough sets, support vector machines, Naive Bayesian, swarm optimization, and deep learning are also presented. Also, this paper reviews a wide range of existing satellite health monitoring solutions and discusses them in the framework of remote data mining techniques. In addition, we are discussing the analysis of space debris flow analysis and the prediction of low earth orbit collision based on our orbital Petri nets model. Challenges to be addressed and future directions of research are identified and an extensive bibliography is also included.																	0269-2821	1573-7462				JUN	2020	53	5					3201	3230		10.1007/s10462-019-09760-1													
J								Detecting the presence of anterior cruciate ligament deficiency based on a double pendulum model, intrinsic time-scale decomposition (ITD) and neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Anterior cruciate ligament (ACL); Nonlinear gait dynamics; Double pendulum; Intrinsic time-scale decomposition (ITD); Neural networks	KNEE OSTEOARTHRITIS; FAULT-DIAGNOSIS; MOVEMENT VARIABILITY; GAIT CHARACTERISTICS; INJURY; WALKING; CLASSIFICATION; RISK; RECONSTRUCTION; INDIVIDUALS	The anterior cruciate ligament (ACL) possesses the function of stabilizing the knee joint through limiting anterior tibial translation and controlling tibial rotation. Patients with unilateral ACL deficiency often demonstrate alterations of knee kinematics, kinetics and gait patterns in the deficient side in comparison to the unaffected contralateral side. This also leads to the early onset of osteoarthritis. In order to detect and monitor the progression of ACL deficiency over time, various classification approaches using spatiotemporal gait variables have been presented. In this study we propose a novel method for classifying gait patterns between ACL-deficient (ACLD) knee and unaffected contralateral ACL-intact (ACLI) knee based upon gait system dynamics, intrinsic time-scale decomposition (ITD) and neural networks. First, human leg is modeled as a double-pendulum to imitate and simplify the human walking. Since the lower extremities act as a kinetic chain during dynamic tasks, control of the hip joint will interact with knee motion. Related gait kinematic parameters including knee and hip joint angle and angular velocity are decomposed into a series of proper rotation components (PRCs) and a baseline signal by using the ITD method. The first PRCs of knee and hip joint angle and angular velocity are extracted, which contain most of the kinematic signals' vibration energy and are considered to be the predominant PRCs. Third, neural networks are then used as the classifier with feature vectors as the input to distinguish between ACLD and ACLI knees based on the difference of gait system dynamics between the two groups. Finally, experiments are carried out on forty-three patients to assess the effectiveness of the proposed method. By using the leave-one-out cross-validation style under normal and fast walking speed conditions, the correct classification rates are reported to be 95.12% respectively. In comparison to other state-of-the-art methods, the results demonstrate superior performance and the proposed method may serve as a potential assistant tool for the automatic detection of ACL deficiency in the clinical application.																	0269-2821	1573-7462				JUN	2020	53	5					3231	3253		10.1007/s10462-019-09761-0													
J								An efficient intrusion detection technique based on support vector machine and improved binary gravitational search algorithm	ARTIFICIAL INTELLIGENCE REVIEW										Network security; Intrusion detection; Support vector machine; Hypergraph; Optimization	DETECTION SYSTEM; FEATURE-SELECTION; SVM; SET; GA; INTELLIGENCE; CLASSIFIERS; ROBUST; MODEL	'Curse of Dimensionality' and the trade-off between high detection rate and less false alarm rate make the design of an efficient and robust Intrusion Detection System, an open research challenge. In this way, we present Hyper Clique-Improved Binary Gravitational Search Algorithm based Support Vector Machine (HC-IBGSA SVM), an efficient and adaptive intrusion detection technique to improve the performance of SVM in terms of detection rate and false alarm rate. HC-IBGSA SVM employs hyper clique property of hypergraph, novel mutation operator, and Newton-Raphson inspired position update function to fasten the search for an optimal solution and to prevent premature convergence. Further, HC-IBGSA uses a weighted objective function to maintain the trade-off between maximizing detection rate and minimizing the false alarm rate and the optimal number of features. The experimental evaluations were carried out using two benchmark intrusion datasets, namely NSL-KDD CUP dataset and UNSW-NB15 dataset under two scenarios (1) SVM trained with all features, and (2) SVM trained with the optimal feature subset and model parameters obtained from HC-IBGSA in terms of various quality metrics, stability analysis and statistical test.																	0269-2821	1573-7462				JUN	2020	53	5					3255	3286		10.1007/s10462-019-09762-z													
J								Neural networks for facial age estimation: a survey on recent advances	ARTIFICIAL INTELLIGENCE REVIEW										Artificial neural networks; Facial age estimation; Biometrics; Soft biometrics; Fusion; Survey	FACE; RECOGNITION; IMAGES; CLASSIFICATION; FEATURES; FUSION; MODELS	Soft biometrics has emerged out to be a new area of interest for the researchers due to its growing real-world applications. It includes the estimation of demographic traits like age, gender, scars, ethnicity. Moreover, researchers are trying to develop models which can accurately estimate the age or the age group of a person using different biometric traits. Presently, neural networks proves out to give the best classification results for age estimation using human faces. Hence, in this paper, we have surveyed and compared all the neural network models developed and implemented for facial age estimation from 2010 to 2019. We have precisely compared all twenty-three different research works done so far to estimate age from human faces using neural networks. Most of the works are based on convolutional neural networks and a few are based on feed forward back propagation and autoencoders. Important details, issues and results of each work are thoroughly discussed for better knowledge of interested researchers. This paper also includes details on other classification techniques for facial age estimation to give an overall idea of all additional techniques adopted by the scientists till date. Details like neural network model names, datasets used, main contributions, evaluation metrics and results are adopted for a tabular and easy to understand comparison study. Finally, the paper concludes by mentioning the other relevant future research tasks that can be done in this challenging area of research.																	0269-2821	1573-7462				JUN	2020	53	5					3299	3347		10.1007/s10462-019-09765-w													
J								Cancelable Biometrics: a comprehensive survey	ARTIFICIAL INTELLIGENCE REVIEW										Taxonomy; Performance; Databases; Survey; Attacks	CANCELLABLE BIOMETRICS; RANDOM PROJECTION; FINGERPRINT; TEMPLATE; FILTERS; SECURITY; PRIVACY; IMAGE; CODE	Biometric recognition is a challenging research field but suffers from privacy and security concerns. To address this concern, Cancelable Biometrics is suggested in literature in which a Biometric image of a sample is distorted or transformed in such a manner that it becomes difficult to obtain the original Biometric image from the distorted one. Another important characteristic of Cancelable Biometrics is that it can be reissued if compromised. In this research paper, we present a comprehensive survey of more than 120 techniques suggested by various researchers from time to time for Cancelable Biometrics and a novel taxonomy for the same is developed. Further, various performance measures used in Cancelable Biometrics are reviewed and their mathematical formulations are given. Cancelable Biometrics also suffer from various security attacks as given in literature. A review of these security attacks is carried out. We have also performed a review of databases used in literature for nine different Cancelable Biometrics viz. Face, Iris, Speech, Fingerprint, Signature, Palmprint, ECG, Palmvein and Fingervein. Lastly, we have also given future research directions in this field. This study shall be useful for the researchers and practitioners working in this fascinating research area.																	0269-2821	1573-7462				JUN	2020	53	5					3403	3446		10.1007/s10462-019-09767-8													
J								Analyzing rare event, anomaly, novelty and outlier detection terms under the supervised classification framework	ARTIFICIAL INTELLIGENCE REVIEW										Rare event detection; Anomaly detection; Novelty detection; Outlier detection; Supervised classification	PROBABILITY ESTIMATION; FAILURES; FEATURES; SVM	In recent years, a variety of research areas have contributed to a set of related problems with rare event, anomaly, novelty and outlier detection terms as the main actors. These multiple research areas have created a mix-up between terminology and problems. In some research, similar problems have been named differently; while in some other works, the same term has been used to describe different problems. This confusion between terms and problems causes the repetition of research and hinders the advance of the field. Therefore, a standardization is imperative. The goal of this paper is to underline the differences between each term, and organize the area by looking at all these terms under the umbrella of supervised classification. Therefore, a one-to-one assignment of terms to learning scenarios is proposed. In fact, each learning scenario is associated with the term most frequently used in the literature. In order to validate this proposal, a set of experiments retrieving papers from Google Scholar, ACM Digital Library and IEEE Xplore has been carried out.																	0269-2821	1573-7462				JUN	2020	53	5					3575	3594		10.1007/s10462-019-09771-y													
J								Novel aggregation operators and ranking method for complex intuitionistic fuzzy sets and their applications to decision-making process	ARTIFICIAL INTELLIGENCE REVIEW										Aggregation operators; MCDM; Uncertainty; Complex IFS; Possibility degree measures		Complex intuitionistic fuzzy set (CIFS) is a distinctive intuitionistic fuzzy set (IFS) in which the membership degrees are determined on the unit disc of the complex plane and can more clearly express the imprecision and ambiguity in the data. The prevailing studies on IFS deal with the data over the subset of a real number and hence there is a sacrifice of some information during the method under certain conditions. As an alteration to these, CIFS characterized with supplementary terms in membership degrees called as phase terms and hence examine two-dimensional data concurrently in a single set. To get full utilization of these assets, in this paper, the aim of the practice is classified into two turns: (i) to define the possibility degree measure to order the numbers, and (ii) to define some novel operational laws and aggregation operators (AOs) to aggregate the various choices over CIFS environment. The beneficial features of the proposed weighted averaging and geometric AOs are addressed. Finally, a decision-making approach is extended for the multicriteria decision-making problem with complex intuitionistic fuzzy information, in which weights are managed objectively. A practical illustration is furnished to address the availability and advantages of the proposed method by comparison with some existing methods.																	0269-2821	1573-7462				JUN	2020	53	5					3595	3620		10.1007/s10462-019-09772-x													
J								Opportunities and challenges in enhancing access to metadata of cultural heritage collections: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Metadata; Information retrieval; Data retrieval; Cultural heritage	STRUCTURED DOCUMENT-RETRIEVAL; QUERY EXPANSION; INFORMATION-RETRIEVAL; MODEL; WEB; PRESERVATION; PERFORMANCE; ENVIRONMENT; EXTRACTION; FEEDBACK	Machine processable data that narrate digital/non-digital resources are termed as metadata. Different metadata standards exist for describing various types of digital objects. Several researches have reported on how to address issues related to accessing of metadata resources. Most studies on metadata involve cultural heritage domain, and this is an indication of the importance of this domain in metadata research and development. Research on metadata in cultural heritage mainly revolves around three fundamental issues: (1) lack of quality in metadata contents in most of the cases, (2) difficulty in accessing metadata contents due largely to limited user's knowledge on the content of the metadata, and (3) heterogeneity of the data at the level of schemas which makes the access even more difficult. The lack of quality in metadata makes it difficult for the users to retrieve and explore information that satisfies their needs. So, in order to make its contents more accessible, enhancing the metadata content is required, especially for cultural heritage collections which consist of digital objects (structured documents) described by a variety of metadata schemas. This paper presents issues and challenges in enhancing access to metadata by reviewing the existing approaches in metadata environment with a particular emphasis on cultural heritage collections. In this paper, firstly, we look at the classification of metadata which is divided into two categories namely data retrieval and information retrieval. Then, we present the analysis, findings and suggestions on how to address issues in enhancing access to metadata contents especially in cultural heritage collections. A detailed comparison is given between information retrieval and data retrieval, and it focuses on the applicability of one approach over the other. A framework that aims to improve the effectiveness of retrieval when searching metadata is also proposed and tested. The proposed framework consists of approaches and methods that are expected to enhance access to metadata especially in cultural heritage collections and be useful for those with limited knowledge on cultural heritage. The experiments were conducted on CHiC2013 which is a collection on cultural heritage. The results show a considerable enhancement over other IR approaches that use the expansion methods.																	0269-2821	1573-7462				JUN	2020	53	5					3621	3646		10.1007/s10462-019-09773-w													
J								Iris recognition in unconstrained environment on graphic processing units with CUDA	ARTIFICIAL INTELLIGENCE REVIEW										Segmentation; Hough; Matching; GPU; CUDA	SEGMENTATION; GPU; NETWORK	Newly introduced Iris recognition systems (IRSs) run on serial processors. In this paper, an alternative method has been introduced for parallel processing on Graphic processing unit (GPU) with Compute unified device architecture (CUDA) in order to increase the speed of the system. The IRS has two main parallel processing criteria, which include the division of computations into hundreds of independent units and the time of calculation more than the time of transferring from the GPU. The IRS is divided into six stages including imaging, pre-processing, segmentation, normalization, feature extraction, and matching. In order to increase speed and accuracy, two stages of iris segmentation and matching play an important role in the IRS. In this paper parallel execution of an identical algorithm for these two stages has been used. The reason for paralleling the iris segmentation stage and their low speed matching is due to a great amount of information in the iris database, plenty of calculations and lack of data dependency in these two stages. For parallelism at the segmentation stage, for each radius, the Hough transform (HT) is a processor, and in the matching stage two parts are considered: The first part consists of 32 actions comparing the input code with the database code in parallel and in the second part 2048 bits with the use of threads on each processor is performed in two sub-sections in pairs of bits and in parallel with each other. Finally, the two-way coding is achieved. In compare of existing methods, this method has rather more accurate and is also superior in terms of processing time on the GPU with CUDA. The results of the implementation of the above method on the images in UBIRIS, BATH, CASIA and MMUI databases show that the proposed method has a precision accuracy of 99.12%, 97.98%, 98.80% and 98.34%, respectively, and the average speedup for parallel processing of images in the database in the proposed method on the GPU with CUDA are 18.8, 14.7, 18, and 19 times, respectively.																	0269-2821	1573-7462				JUN	2020	53	5					3705	3729		10.1007/s10462-019-09776-7													
J								Data-based composite control design with critic intelligence for a wastewater treatment platform	ARTIFICIAL INTELLIGENCE REVIEW										Adaptive critic; Data-based composite control; Intelligent systems; Optimal feedback; Wastewater treatment	ALGORITHMS	In this paper, by integrating neural network approximators, a data-based composite control technique is developed with critic learning implementation and wastewater treatment verification. The iterative adaptive critic framework is established involving dual heuristic dynamic programming (DHP), so as to obtain an intelligent optimal controller. Besides, a steady control input is computed with the help of the neural identifier. Then, by combining the DHP controller and the steady control input, an effective composite control strategy is derived and applied to the proposed wastewater treatment platform. Through conducting experiments, it is observed that the dissolved oxygen concentration and the nitrate level can be maintained at setting points successfully, which results in an intelligent wastewater treatment system.																	0269-2821	1573-7462				JUN	2020	53	5					3773	3785		10.1007/s10462-019-09778-5													
J								Adapted a novel similarity and its application in fuzzy risk analysis	EVOLUTIONARY INTELLIGENCE										Generalized trapezoidal fuzzy number (GTFN); Similarity measure; Amending value; Fuzzy risk analysis; Reciprocating pump system (RPS)	NUMBERS; SETS; FAILURES	Generalized trapezoidal fuzzy numbers (GTFNs) and their similarity measures have been widely applied in fuzzy risk analysis. However, some existing similarity measures of GTFNs cannot identify the similarity of some special GTFNs properly. In this study, we introduce the exponential distance of center of gravity (COG) and an amending value to adapted a novel similarity measure between GTFNs. Then, seven properties of this new method are investigated and proved. In addition, in order to verify the superiority of the new method, fifteen special testing sets are given to compare the performance of seven existing similarity measures with the new method. Moreover, the presented new similarity measure is used to solve a case study about the failure risk analysis of six major subsystems of the reciprocating pump system (RPS) in which different parameters are expressed by generalized trapezoidal fuzzy linguistic term.																	1864-5909	1864-5917				JUN	2020	13	2			SI		147	158		10.1007/s12065-019-00286-7													
J								Hybrid genetic algorithm and a fuzzy logic classifier for heart disease diagnosis	EVOLUTIONARY INTELLIGENCE										Disease classification; Adaptive genetic algorithm; Rough set theory; Feature reduction; Membership function	PREDICTION; SYSTEM	For the past two decades, most of the people from developing countries are suffering from heart disease. Diagnosing these diseases at earlier stages helps patients reduce the risk of death and also in reducing the cost of treatment. The objective of adaptive genetic algorithm with fuzzy logic (AGAFL) model is to predict heart disease which will help medical practitioners in diagnosing heart disease at early stages. The model consists of the rough sets based heart disease feature selection module and the fuzzy rule based classification module. The generated rules from fuzzy classifiers are optimized by applying the adaptive genetic algorithm. First, important features which effect heart disease are selected by rough set theory. The second step predicts the heart disease using the hybrid AGAFL classifier. The experimentation is performed on the publicly available UCI heart disease datasets. Thorough experimental analysis shows that our approach has outperformed current existing methods.																	1864-5909	1864-5917				JUN	2020	13	2			SI		185	196		10.1007/s12065-019-00327-1													
J								Mining high influence co-location patterns from instances with attributes	EVOLUTIONARY INTELLIGENCE										High influence co-location pattern; Influence index; Spatial instances with attributes; Information entropy	RULES	A spatial co-location pattern describes coexistence of spatial features whose instances frequently appear together in geographic space. Numerous studies have been proposed to discover interesting co-location patterns from spatial data sets, but most of them only use the location information of instances. As a result, they cannot adequately reflect the influence between instances. In this paper, we take additional attributes of instances into account in the process of co-location pattern mining, and propose a new approach for discovering the high influence co-location patterns. In our approach, we consider the spatial neighboring relationships and the similarity of instances simultaneously, and utilize the information entropy approach to measure the influence of any instance exerting on its neighbors and the influence of any feature in a co-location pattern. Then, an influence index for measuring the interestingness of a co-location pattern is proposed and we prove the influence index measure satisfies the downward closure property that can be used for pruning the search space, and thus an efficient high influence co-location pattern mining algorithm is designed. At last, extensive experiments are conducted on synthetic and real spatial data sets. Experimental results reveal the effectiveness and efficiency of our method.																	1864-5909	1864-5917				JUN	2020	13	2			SI		197	210		10.1007/s12065-019-00321-7													
J								Structural hole detection based on weighted meta path in heterogeneous networks	EVOLUTIONARY INTELLIGENCE										Heterogeneous information networks; Structural holes detection; Weighted meta path; Edge weight		With the rapid development of online social networks, the detection of structural holes, i.e. identifying the key nodes that can bridge with individuals or groups without direct relationship in social networks, has attracted more attention of a large number of researches. The existing researches mainly focus on the influence of a homogeneous network structure, ignoring the importance of node types and different edges in online social networks. In this paper, an algorithm based on weighted meta paths for detecting structural hole in heterogeneous networks (SH_WMP) is proposed. SH_WMP not only flexibly integrates rich semantic information of heterogeneous networks, but also utilizes edge weight and potential link information to improve the performance. Experimental results show that the proposed method outperforms the comparison methods.																	1864-5909	1864-5917				JUN	2020	13	2			SI		211	220		10.1007/s12065-019-00342-2													
J								Estimation of spreading sequences in LC-DS-CDMA signals based on sparse auto-encoder	EVOLUTIONARY INTELLIGENCE										Direct-sequence code-division multiple access; Deep learning; Sparse auto-encoder; Blind source separation; Signal analysis	BLIND; NETWORK	A method based on a sparse auto-encoder (SAE) network for the estimation of spreading sequences in long-code direct-sequence code-division multiple access (LC-DS-CDMA) signals is proposed. First, a network classification model based on SAE and softmax classifier is established. Next, the effectiveness of the proposed method is verified by estimating Walsh sequences and m sequences. To estimate the spreading sequences, the LC-DS-CDMA signal is divided into fragments. Then, each user's spreading sequence is separated by the fast independent component analysis (Fast-ICA) algorithm, and the amplitude fuzziness is eliminated by the delay-and-multiply method. Finally, the spreading sequences are estimated by the SAE model. Experimental results showed that the proposed algorithm could effectively estimate the spreading sequences of LC-DS-CDMA signals. Compared to the existing matching algorithm and Fast-ICA algorithm, the estimation time required by the proposed algorithm was shorter, and its estimation performance at low signal-to-noise ratios was superior.																	1864-5909	1864-5917				JUN	2020	13	2			SI		235	246		10.1007/s12065-019-00298-3													
J								Performance evaluation of Botnet DDoS attack detection using machine learning	EVOLUTIONARY INTELLIGENCE										Botnet detection; Command and control channel; Distributed Denial of service attack; Machine learning; Unsupervised learning	MITIGATION; BEHAVIOR; TRENDS; PEER	Botnet is regarded as one of the most sophisticated vulnerability threats nowadays. A large portion of network traffic is dominated by Botnets. Botnets are conglomeration of trade PCs (Bots) which are remotely controlled by their originator (BotMaster) under a Command and-Control (C&C) foundation. They are the keys to several Internet assaults like spams, Distributed Denial of Service Attacks (DDoS), rebate distortions, malwares and phishing. To over the problem of DDoS attack, various machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) (K-means, X-means etc.) were proposed. With the increasing popularity of Machine Learning in the field of Computer Security, it will be a remarkable accomplishment to carry out performance assessment of the machine learning methods given a common platform. This could assist developers in choosing a suitable method for their case studies and assist them in further research. This paper performed an experimental analysis of the machine learning methods for Botnet DDoS attack detection. The evaluation is done on the UNBS-NB 15 and KDD99 which are well-known publicity datasets for Botnet DDoS attack detection. Machine learning methods typically Support Vector Machine (SVM), Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT), and Unsupervised Learning (USML) are investigated for Accuracy, False Alarm Rate (FAR), Sensitivity, Specificity, False positive rate (FPR), AUC, and Matthews correlation coefficient (MCC) of datasets. Performance of KDD99 dataset has been experimentally shown to be better as compared to the UNBS-NB 15 dataset. This validation is significant in computer security and other related fields.																	1864-5909	1864-5917				JUN	2020	13	2			SI		283	294		10.1007/s12065-019-00310-w													
J								Prediction of corn drying performance for a combined IRC dryer with a genetically-optimized SVR algorithm	EVOLUTIONARY INTELLIGENCE										Infrared radiation drying; Grain drying; Support vector regression; Genetic algorithm; Modelling	MODEL; SCHEME	Grain drying process is a complex nonlinear system which is characterized by long delay process, multi disturbance and strong coupling. In order to explore the modelling of an uncertain system, such as those used in grain drying, and to study the application of the support vector regress algorithm, a corn drying process conducted in a side-heat Infrared Radiation and Convection dryer was modelled by using a support vector regress algorithm combined with a genetic algorithm which is abbreviated as GA-SVR. The algorithm was trained by using the input and output data collected from the practical experiment of corn drying. The predicted performance comparisons between the GA-SVR modelling method and the other two modelling methods (the neural network of BP model and the SVR model based on the grid search algorithm) were also made. Moreover, we successfully used the method to design a model of concurrent-counter flow drying. The designed GA-SVR model has achieved higher modelling prediction accuracy according to the prediction results which have verified the feasibility of the proposed modelling algorithm for modelling the grain drying. The modelling method can also realize the performance prediction of different drying techniques and can be applied in the model prediction control of the grain drying.																	1864-5909	1864-5917				JUN	2020	13	2			SI		295	307		10.1007/s12065-019-00347-x													
J								A Face Fairness Framework for 3D Meshes	INTERNATIONAL JOURNAL OF COMPUTER VISION										Face fairness; 3D meshes; Mesh-denoising; Mesh-normal fusion	MULTIVIEW; STEREO	In this paper, we present a face fairness framework for 3D meshes that preserves the regular shape of faces and is applicable to a variety of 3D mesh restoration tasks. Specifically, we present a number of desirable properties for any mesh restoration method and show that our framework satisfies them. We then apply our framework to two different tasks-mesh-denoising and mesh-refinement, and present comparative results for these two tasks showing improvement over other relevant methods in the literature.																	0920-5691	1573-1405				JUN	2020	128	6					1565	1579		10.1007/s11263-019-01268-z													
J								GMS: Grid-Based Motion Statistics for Fast, Ultra-robust Feature Correspondence	INTERNATIONAL JOURNAL OF COMPUTER VISION										Feature matching; Epipolar geometry; Visual SLAM; Structure-from-motion; GMS	EPIPOLAR GEOMETRY; MODEL	Feature matching aims at generating correspondences across images, which is widely used in many computer vision tasks. Although considerable progress has been made on feature descriptors and fast matching for initial correspondence hypotheses, selecting good ones from them is still challenging and critical to the overall performance. More importantly, existing methods often take a long computational time, limiting their use in real-time applications. This paper attempts to separate true correspondences from false ones at high speed. We term the proposed method (GMS) grid-based motion Statistics, which incorporates the smoothness constraint into a statistic framework for separation and uses a grid-based implementation for fast calculation. GMS is robust to various challenging image changes, involving in viewpoint, scale, and rotation. It is also fast, e.g., take only 1 or 2 ms in a single CPU thread, even when 50K correspondences are processed. This has important implications for real-time applications. What's more, we show that incorporating GMS into the classic feature matching and epipolar geometry estimation pipeline can significantly boost the overall performance. Finally, we integrate GMS into the well-known ORB-SLAM system for monocular initialization, resulting in a significant improvement.																	0920-5691	1573-1405				JUN	2020	128	6					1580	1593		10.1007/s11263-019-01280-3													
J								Real-Time Multi-person Motion Capture from Multi-view Video and IMUs	INTERNATIONAL JOURNAL OF COMPUTER VISION										Pose estimation; Motion capture; IMU; Multi-view video; Real-time; Multi-person	SHAPE ESTIMATION; POSE	A real-time motion capture system is presented which uses input from multiple standard video cameras and inertial measurement units (IMUs). The system is able to track multiple people simultaneously and requires no optical markers, specialized infra-red cameras or foreground/background segmentation, making it applicable to general indoor and outdoor scenarios with dynamic backgrounds and lighting. To overcome limitations of prior video or IMU-only approaches, we propose to use flexible combinations of multiple-view, calibrated video and IMU input along with a pose prior in an online optimization-based framework, which allows the full 6-DoF motion to be recovered including axial rotation of limbs and drift-free global position. A method for sorting and assigning raw input 2D keypoint detections into corresponding subjects is presented which facilitates multi-person tracking and rejection of any bystanders in the scene. The approach is evaluated on data from several indoor and outdoor capture environments with one or more subjects and the trade-off between input sparsity and tracking performance is discussed. State-of-the-art pose estimation performance is obtained on the Total Capture (mutli-view video and IMU) and Human 3.6M (multi-view video) datasets. Finally, a live demonstrator for the approach is presented showing real-time capture, solving and character animation using a light-weight, commodity hardware setup.																	0920-5691	1573-1405				JUN	2020	128	6					1594	1611		10.1007/s11263-019-01270-5													
J								Representation Learning on Unit Ball with 3D Roto-translational Equivariance	INTERNATIONAL JOURNAL OF COMPUTER VISION										Convolution neural networks; 3D moments; Volumetric convolution; Zernike polynomials; Deep learning	INVARIANT; DESCRIPTORS; RECOGNITION; HISTOGRAMS; ROTATION	Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces-such as a sphere (S2\or a unit ball (B3-entails unique challenges. In this work, we propose a novel 'volumetric convolution' operation that can effectively model and convolve arbitrary functions in B3 We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.																	0920-5691	1573-1405				JUN	2020	128	6					1612	1634		10.1007/s11263-019-01278-x													
J								Fine-Grained Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION										Person re-identification; Fine-grained cross-view matching; Visual surveillance		Person re-identification (re-id) plays a critical role in tracking people via surveillance systems by matching people across non-overlapping camera views at different locations. Although most re-id methods largely depend on the appearance features of a person, such methods always assume that the appearance information (particularly color) is distinguishable. However, distinguishing people who dress in very similar clothes (especially the same type of clothes, e.g. uniform) is ineffective if relying only on appearance cues. We call this problem the fine-grained person re-identification (FG re-id) problem. To solve this problem, rather than relying on clothing color, we propose to exploit two types of local dynamic pose features: motion-attentive local dynamic pose feature and joint-specific local dynamic pose feature. They are complementary to each other and describe identity-specific pose characteristics, which are found to be more unique and discriminative against similar appearance between people. A deep neural network is formed to learn these local dynamic pose features and to jointly quantify motion and global visual cues. Due to the lack of a suitable benchmark dataset for evaluating the FG re-id problem, we also contribute a fine-grained person re-identification (FGPR) dataset, which contains 358 identities. Extensive evaluations on the FGPR dataset show that our proposed model achieves the best performance compared with related person re-id and fine-grained recognition methods for FG re-id. In addition, we verify that our method is still effective for conventional video-based person re-id.																	0920-5691	1573-1405				JUN	2020	128	6					1654	1672		10.1007/s11263-019-01259-0													
J								Siamese Dense Network for Reflection Removal with Flash and No-Flash Image Pairs	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep learning; Reflection removal; Image restoration; Flash; no-flash; Image fusion; Layer separation; Depth filling	SEPARATION; PHOTOGRAPHY; COMPLETION	This work addresses the reflection removal with flash and no-flash image pairs to separate reflection from transmission. When objects are covered by glass, the no-flash image usually contains reflection, and thus flash is used to enhance transmission details. However, the flash image suffers from the specular highlight on the glass surface caused by flash. In this paper, we propose a siamese dense network (SDN) for reflection removal with flash and no-flash image pairs. SDN extracts shareable and complementary features via concatenated siamese dense blocks. We utilize an image fusion block for the SDN to fuse the intermediate output of two branches. Since severe information loss occurs in the specular highlight, we detect the specular highlight in the flash image based on gradient of the maximum chromaticity. Through observations, flash causes various artifacts such as tone distortion and inhomogeneous brightness. Thus, with synthetic datasets we collect 758 pairs of real flash and no-flash image pairs (including their ground truth) by different cameras to gain generalization. Various experiments show that the proposed method successfully removes reflections using flash and no-flash image pairs and outperforms state-of-the-art ones in terms of visual quality and quantitative measurements. Besides, we apply the SDN to color/depth image pairs and achieve both color reflection removal and depth filling.																	0920-5691	1573-1405				JUN	2020	128	6					1673	1698		10.1007/s11263-019-01276-z													
J								Gated Fusion Network for Degraded Image Super Resolution	INTERNATIONAL JOURNAL OF COMPUTER VISION										Super resolution; Image restoration; Deep learning		Single image super resolution aims to enhance image quality with respect to spatial content, which is a fundamental task in computer vision. In this work, we address the task of single frame super resolution with the presence of image degradation, e.g., blur, haze, or rain streaks. Due to the limitations of frame capturing and formation processes, image degradation is inevitable, and the artifacts would be exacerbated by super resolution methods. To address this problem, we propose a dual-branch convolutional neural network to extract base features and recovered features separately. The base features contain local and global information of the input image. On the other hand, the recovered features focus on the degraded regions and are used to remove the degradation. Those features are then fused through a recursive gate module to obtain sharp features for super resolution. By decomposing the feature extraction step into two task-independent streams, the dual-branch model can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. We evaluate the proposed method in three degradation scenarios. Experiments on these scenarios demonstrate that the proposed method performs more efficiently and favorably against the state-of-the-art approaches on benchmark datasets.																	0920-5691	1573-1405				JUN	2020	128	6					1699	1721		10.1007/s11263-019-01285-y													
J								Multi-level context extraction and attention-based contextual inter-modal fusion for multimodal sentiment analysis and emotion classification	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Attention model; Inter-modal fusion; Multi-level contextual information; Bidirectional recurrent neural network		The recent advancements in the Internet technology and its associated services, led the users to post a large amount of multimodal data into social media Web sites, online shopping portals, video repositories, etc. The availability of the huge amount of multimodal content, multimodal sentiment classification, and affective computing has become the most researched topic. The extraction of context among the neighboring utterances and considering the importance of inter-modal utterances before multimodal fusion are the most important research issues in this field. This article presents a novel approach to extract the context at multiple levels and to understand the importance of inter-modal utterances in sentiment and emotion classification. Experiments are conducted on two publically accepted datasets such as CMU-MOSI for sentiment analysis and IEMOCAP for emotion classification. By incorporating the utterance-level contextual information and importance of inter-modal utterances, the proposed model outperforms the standard baselines by over 3% in classification accuracy.																	2192-6611	2192-662X				JUN	2020	9	2			SI		103	112		10.1007/s13735-019-00185-8													
J								Learning visual features for relational CBIR	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										CLEVR; Content-based image retrieval; Deep learning; Relational reasoning; Relation networks; Deep features		Recent works in deep-learning research highlighted remarkable relational reasoning capabilities of some carefully designed architectures. In this work, we employ a relationship-aware deep learning model to extract compact visual features used relational image descriptors. In particular, we are interested in relational content-based image retrieval (R-CBIR), a task consisting in finding images containing similar inter-object relationships. Inspired by the relation networks (RN) employed in relational visual question answering (R-VQA), we present novel architectures to explicitly capture relational information from images in the form of network activations that can be subsequently extracted and used as visual features. We describe a two-stage relation network module (2S-RN), trained on the R-VQA task, able to collect non-aggregated visual features. Then, we propose the aggregated visual features relation network (AVF-RN) module that is able to produce better relationship-aware features by learning the aggregation directly inside the network. We employ an R-CBIR ground-truth built by exploiting scene-graphs similarities available in the CLEVR dataset in order to rank images in a relational fashion. Experiments show that features extracted from our 2S-RN model provide an improved retrieval performance with respect to standard non-relational methods. Moreover, we demonstrate that the features extracted from the novel AVF-RN can further improve the performance measured on the R-CBIR task, reaching the state-of-the-art on the proposed dataset.																	2192-6611	2192-662X				JUN	2020	9	2			SI		113	124		10.1007/s13735-019-00178-7													
J								A retrieval-based approach for diverse and image-specific adversary selection	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Adversarial attack; Deep CNN; Diversity; Determinantal point process		While deep neural network-based models have demonstrated compelling performance on various tasks in computer vision and other fields, they have been found to be vulnerable to adversarial attacks. Particularly, deep convolutional neural network (CNN)-based models can be easily fooled by adding a small quasi-imperceptible perturbation to the input, thus resulting in significant drop in prediction accuracies. While most of the previous works have focused on generating one adversary/perturbation per model, it was recently shown that it is possible to learn a continuous distribution over adversarial perturbations for a model. Building upon this work, in this paper, we propose a new technique for image-specific adversary selection and treat it as a retrieval task. The proposed technique utilizes a learned model that ranks the perturbations in a given set of perturbations based on their ability to fool with respect to a given sample. This model is a conditional determinantal point process model that also explicitly induces diversity among the retrieved perturbations. We conduct experiments on the ImageNet dataset using four popular deep CNN image classification models, and demonstrate that the proposed method consistently achieves state-of-the-art fooling rates.																	2192-6611	2192-662X				JUN	2020	9	2			SI		125	133		10.1007/s13735-019-00177-8													
J								MA-CRNN: a multi-scale attention CRNN for Chinese text line recognition in natural scenes	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Chinese text line recognition; Multiple scales; Attention mechanism; Chinese text line dataset (CTLD)		The recognition methods for Chinese text lines, as an important component of optical character recognition, have been widely applied in many specific tasks. However, there are still some potential challenges: (1) lack of open Chinese text recognition dataset; (2) challenges caused by the characteristics of Chinese characters, e.g., diverse types, complex structure and various sizes; (3) difficulties brought by text images in different scenes, e.g., blur, illumination and distortion. In order to address these challenges, we propose an end-to-end recognition method based on convolutional recurrent neural networks (CRNNs), i.e., multi-scale attention CRNN, which adds three components on the basis of a CRNN: asymmetric convolution, feature reuse network and attention mechanism. The proposed model is mainly aimed at scene text recognition including Chinese characters. Then the model is trained and tested on two Chinese text recognition datasets, i.e., the open dataset MTWI and our constructed large-scale Chinese text line dataset collected from various scenes. The experimental results demonstrate that the proposed method achieves better performance than other methods.																	1433-2833	1433-2825				JUN	2020	23	2					103	114		10.1007/s10032-019-00348-7													
J								An adaptive document recognition system for lettrines	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Drop caps; Indexing; Relevance feedback; Human centered	RETRIEVAL; SEARCH; IMAGES	In this paper, we propose an approach to interactively propagate annotations representing the historians' knowledge on a database of lettrine images manually populated by historians (with annotations). Based on a novel document indexing processing scheme which combines the use of the Zipf law and the use of bag of patterns, our approach extends the bag-of-words model to represent the knowledge by visual features through relevance feedback. Then, annotation propagation is automatically performed to propagate knowledge to the lettrine database. Our approach is presented together with preliminary experimental results and an illustrative example.																	1433-2833	1433-2825				JUN	2020	23	2					115	128		10.1007/s10032-019-00346-9													
J								A general framework for the recognition of online handwritten graphics	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Graphics recognition; Online handwriting recognition; Graph parsing; Graph grammar; Mathematical expression; Flowchart	LANGUAGES; LINE	We revisit graph grammar and graph parsing as tools for recognizing graphics. A top-down approach for parsing families of handwritten graphics containing different kinds of symbols and of structural relations is proposed. It has been tested on two distinct domains, namely the recognition of handwritten mathematical expressions and of handwritten flowcharts. In the proposed approach, a graphic is considered as a labeled graph generated by a graph grammar. The recognition problem is translated into a graph parsing problem: Given a set of strokes (input data), a parse tree which represents the best interpretation is extracted. The graph parsing algorithm generates multiple interpretations (consistent with the grammar) that can be ranked according to a global cost function that takes into account the likelihood of symbols and structures. The parsing algorithm consists in recursively partitioning the stroke set according to rules defined in the graph grammar. To constrain the number of partitions to be evaluated, we propose the use of a hypothesis graph, built from data-driven machine learning techniques, to encode the most likely symbol and relation hypotheses. Within this approach, it is easy to relax the stroke ordering constraint allowing interspersed symbols, as opposed to some previous works. Experiments show that our method obtains accuracy comparable to methods specifically developed to recognize domain-dependent data.																	1433-2833	1433-2825				JUN	2020	23	2					143	160		10.1007/s10032-019-00349-6													
J								Learning similarity measures from data	PROGRESS IN ARTIFICIAL INTELLIGENCE										Similarity measure; Data science; Neural networks; Data analytics; Case-based reasoning; Similarity function; Siamese networks; Similarity metrics; Distance metrics	NETWORK	Defining similarity measures is a requirement for some machine learning methods. One such method is case-based reasoning (CBR) where the similarity measure is used to retrieve the stored case or a set of cases most similar to the query case. Describing a similarity measure analytically is challenging, even for domain experts working with CBR experts. However, datasets are typically gathered as part of constructing a CBR or machine learning system. These datasets are assumed to contain the features that correctly identify the solution from the problem features; thus, they may also contain the knowledge to construct or learn such a similarity measure. The main motivation for this work is to automate the construction of similarity measures using machine learning. Additionally, we would like to do this while keeping training time as low as possible. Working toward this, our objective is to investigate how to apply machine learning to effectively learn a similarity measure. Such a learned similarity measure could be used for CBR systems, but also for clustering data in semi-supervised learning, or one-shot learning tasks. Recent work has advanced toward this goal which relies on either very long training times or manually modeling parts of the similarity measure. We created a framework to help us analyze the current methods for learning similarity measures. This analysis resulted in two novel similarity measure designs: The first design uses a pre-trained classifier as basis for a similarity measure, and the second design uses as little modeling as possible while learning the similarity measure from data and keeping training time low. Both similarity measures were evaluated on 14 different datasets. The evaluation shows that using a classifier as basis for a similarity measure gives state-of-the-art performance. Finally, the evaluation shows that our fully data-driven similarity measure design outperforms state-of-the-art methods while keeping training time low.																	2192-6352	2192-6360				JUN	2020	9	2					129	143		10.1007/s13748-019-00201-2													
J								Option valuation with IG-GARCH model and a U-shaped pricing kernel	SOFT COMPUTING										Option valuation; Pricing kernel; VIX; IG-GARCH; S&P500	RISK-AVERSION; VOLATILITY; VARIANCE; RETURNS; VIX	Empirical and theoretical studies have attempted to establish the U-shape of the log-ratio of conditional risk-neutral and physical probability density functions. The main subject of this paper is to question the use of such a U-shaped pricing kernel to improve option pricing performances in a non-Gaussian setting. Starting from the so-called inverse Gaussian GARCH model (IG-GARCH), known to provide semi-closed-form formulas for classical European derivatives when an exponential-affine pricing kernel is used, we build a new pricing kernel that is non-monotonic and that still has this remarkable property. Using a daily dataset of call options written on the S&P500 index, we compare the pricing performances of these two IG-GARCH models proving, in this framework, that the new exponential U-shaped stochastic discount factor clearly outperforms the classical exponential-affine one. What is more, several estimation strategies including options or VIX information are evaluated taking advantage of the analytical tractability of these models. We prove that the parsimonious estimation approach using returns and VIX historical data remains competitive without having to work with the cross section of options.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8505	8522		10.1007/s00500-019-04236-4													
J								A Quantile Regression approach for the analysis of the diversification in non-life premium risk	SOFT COMPUTING										Quantile Regression; Generalized linear model; Premium principles; Risk margin; Ratemaking; Diversification		This paper concerns the study of the diversification effect involved in a portfolio of non-life policies priced via traditional premium principles when individual pure premiums are calculated via Quantile Regression. Our aim is to use Quantile Regression to estimate the individual conditional loss distribution given a vector of rating factors. To this aim, we make a comparison of the outcomes obtained via Quantile Regression with the widely used industry standard method based on generalized linear models. Then, considering a specific premium principle, we calculate individual pure premium by means of a specific functional of the conditional loss distribution, the standard deviation. We determine the portfolio risk margin according to the Solvency 2 framework and then we allocate it over each policy in a way consistent with his/her riskiness. Indeed, considering a portfolio of heterogeneous policies, we determine the individual reduction of the safety loading, due to the diversification, and we measure the risk contribution of each individual.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8523	8534		10.1007/s00500-019-04291-x													
J								Polynomial goal programming and particle swarm optimization for enhanced indexation	SOFT COMPUTING										Enhanced indexation; Cardinality; Turnover constraint; Polynomial goal programming; Particle swarm optimization; Constraint handling	PORTFOLIO OPTIMIZATION; TRACKING-ERROR; ALGORITHM; EVOLUTIONARY; INVESTORS; MODEL	Enhanced indexation is an investment strategy that aims to generate moderate and consistent excess returns with respect to a tracked benchmark index. In this work, we introduce an optimization approach where the risk of under-performing the benchmark is separated from the potential over-performance, and the Sharpe ratio measures the profitability of the active management. In addition, a cardinality constraint controls the number of active positions in the portfolio, while a turnover threshold limits the transaction costs. We adopt a polynomial goal programming approach to combine these objectives with the investor's preferences. An improved version of the particle swarm optimization algorithm with a novel constraint-handling mechanism is proposed to solve the optimization problem. A numerical example, where the Euro Stoxx 50 Index is used as the benchmark, shows that our method consistently produces larger returns, with reduced costs and risk exposition, than the standard indexing strategies over a 10-year backtesting period.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8535	8551		10.1007/s00500-019-04378-5													
J								Tackling longevity risk by means of financial compensation	SOFT COMPUTING										Longevity risk; Life annuities; Financial compensation	MORTALITY	The remarkable increases in life expectancy observed over the last decades have posed a major challenge to pension funds and annuity providers because of the related systematic longevity risk. This article proposes a variable payout life annuity where benefits have to follow the observed mortality and the interest rates obtained. This scheme is effective and efficient for annuity providers, who always have a fund that matches exactly the undertaken commitments to annuitants. On the other hand, potential reductions in the benefit payments can be felt by annuitants more bearable than those that include a safety loading. Specifically, the concept of observed survival probabilities is introduced and applied to: (a) translate a demographic change into the related financial adjustment; (b) decompose a demographic change into two effects, one stemming from the survival probability observations and the other from life table updates; (c) show that the financial compensation mechanism should run for single cohorts to avoid creating inequalities for older and smaller cohorts.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8583	8597		10.1007/s00500-019-04433-1													
J								Determination of the financial minimum in a municipal budget to deal with crisis situations	SOFT COMPUTING										Crisis management; Population protection; Resource allocation for crisis situations; Municipal budget	VULNERABILITY; CAPABILITY; RISK	This paper offers an analysis of the mission and role of municipalities in the Czech national crisis management system. The paper clarifies the nature of population protection within the national security system accentuating resource availability. The significance of local municipal budgets as the baseline to crisis solution support within the administered area is discussed. The problem is not the system of budgets or the budgeting system itself at the level of local government. The key issue, however, is the determination of budget resources, which are primarily predetermined ex ante for the crisis prevention resolution phase. Current legislation does not address this requirement. The authors also point out the fundamental influence of local policy in the allocation of resources in the municipal budget in relation to their political preferences. Therefore, the authors of the article propose the calculation method for the standard minimum amount of financial resources allocated to coping with crises in the municipal budget at their level, as appropriate. The submitted proposal will contribute to guaranteeing a minimum level of preparedness for crisis management in ensuring the safety of all residents.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8607	8616		10.1007/s00500-019-04527-w													
J								Testing for non-chaoticity under noisy dynamics using the largest Lyapunov exponent	SOFT COMPUTING										Chaoticity; Largest Lyapunov exponent; Kalman filter; Simulation smoother; Noise; Confidence intervals	CHAOS	In this paper, we introduce a robust procedure to test for non-chaoticity when data are contaminated by an additive noise. Under the Kalman filter framework, our procedure first amounts to compute the largest Lyapunov exponent of the extracted signal. The exponent describes the log-divergence of a dynamical system (Rosenstein et al. in Phys D 65(1-2):117-134, 1993. 10.1016/0167-2789(93)90009-p). Then, using the so-called simulation smoother, we generate a high number of trajectories of the state-vector, conditional on the observed series, and compute the empirical distribution of the largest Lyapunov exponent. The distribution allows for computing confidence intervals. We can thus test if the largest Lyapunov exponent is not significantly greater than zero. Using Monte Carlo simulations, we show the validity of such an approach. We provide an illustration using toy models, which depict several dynamical systems. Finally, we implement tests of non-chaoticity on financial time series. We find no empirical evidence of chaotic patterns. Our approach is simple, efficient, and tests for chaos when data are measured with errors (i.e., noisy dynamics).																	1432-7643	1433-7479				JUN	2020	24	12			SI		8617	8626		10.1007/s00500-019-04595-y													
