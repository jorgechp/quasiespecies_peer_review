PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Scalable Real-Time Attributes Responsive Extreme Learning Machine	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Extreme learning machine; Attributes scalable; Cropping strategy	RECOGNITION	Extreme learning machine (ELM) has recently attracted many researchers interest due to its very fast learning speed, and ease of implementation. Its many applications, such as regression, binary and multiclass classification, acquired better results. However, When some attributes of the dataset have been lost, this fixed network structure will be less than satisfactory. This article suggests a Scalable Real-Time Attributes Responsive Extreme Learning Machine (Star-ELM), is such can grow its appropriate structure with nodes autonomous coevolution based on the different dataset. Its hidden nodes can be merged to more effectively adjust structure and. Weight In the experiments of classical data sets we compare with other relevant variants of ELM, Star-ELM makes better performance on classification learning with loss of dataset attributes in some situations. (C) 2020 The Authors. Published by Atlatatis Press B.V.																	1875-6891	1875-6883					2020	13	1					1101	1107		10.2991/ijcis.d.200731.001													
J								Sustainable Supplier Selection Based on Regret Theory and QUALIFLEX Method	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Sustainable supplier selection; Bounded ran Lionality; 2-dimension at uncertain linguistic variable; Regret theory; QUALIFLEX	GROUP DECISION-MAKING; LINGUISTIC TERM SETS; INTEGRATED APPROACH; GREEN SUPPLIERS; PROSPECT-THEORY; SITE SELECTION; DEMATEL METHOD; POWER-PLANT; FUZZY; PROMETHEE	Sustainable supplier selection is the essential core of sustainable supply chain management, which cm directly influence the manufacturer's performance and can enormously enhance the manufacturer's competitiveness in Is international market. However, most of the previous studies concerning sustainable supplier selection have less focused on the reliability of the decision-makers judgments and the application of regret theory. To fill this gap, we presented an integrated sustainable supplier selection model based on regret theory and QUALItative FLEXible multiple criteria method (QUALIFLEX) under a 2-dimensional uncertain linguistic variable (2-DULV) environment. In the proposed model, 2-DULV including the reliability of evaluation information is employed to handle the uncertainty and vagueness of decision-makers judgments. A similarity-based method is used to derive the decision-makers' weight, and a maximizing deviation model is established to calculate the weights of evaluation criteria. Then an improved QUALIFLEX method based on regret theory is presented to obtain the ranking Order of sustainable suppliers. The proposed approach integrates both the superiority of 2-DULV in effectively handling the uncertainty, vagueness, and reliability of evaluation information and the merit of regret theory in dealing wall decision-maker's bounded rationality. Finally, a numerical example concerning an automobile manufacturer is provided to validate the effectiveness and feasibility of the presented model. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1120	1133		10.2991/ijcis.d.200730.001													
J								A Novel Two-Stage DEA Model in Fuzzy Environment: Application to Industrial Workshops Performance Measurement	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Effidency; Fuzzy data envelopment analysis; Two-stage DEA model; Industrial-workshops	DATA ENVELOPMENT ANALYSIS; EFFICIENCY DECOMPOSITION; PROGRAMMING PROBLEMS; NETWORK DEA; WEIGHTS	One of the paramount mathematical methods to compute the general performance of organizations is data envelopment analysis (DEA). Nevertheless, in some cases, the decision-making units (DMUs) have middle values. Furthermore, the conventional DEA models have been originally formulated solely for crisp data and cannot handle the problems with uncertain information. To tackle the above issues, this paper presents a two-stage DEA model with fuzzy data. The recommended technique is based on the fuzzy arithmetic and has a simple construction. Furthermore, to illustrate the new model, we investigate the efficiency of some industrial workshops in Iran. The results show the effectiveness and robustness of the new model. (C) 2020 The Authors. Published by Atlantis Press B.V																	1875-6891	1875-6883					2020	13	1					1134	1152		10.2991/ijcis.d.200731.002													
J								YOLOv3: Face Detection in Complex Environments	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Face detection; Complex environment; Priori box; Multiple score values		Face detection has been well studied for many years. However, the problem of face detection in complex environments is still being studied. In complex environments, faces is often blocked and blurred. This article proposes applying YOLOv3 to face detection problems in complex environments. first, we will re-cluster the data set in order to find the most suitable a priori box. Then we set multiple score values to make it possible to predict the results of multiple sets of images and rind the optimal score value. Experimental results show that after adjustment, the model has more advantages in face detection than the original model in complex environments. the average accuracy is more than 10% higher than that of aggregate channel feature (ACE), Tow-stage convolutional neural network (CNN) and multi-scale Cascade CNN in face detection benchmarks WIDER FACE. Our code is available in:git@github.com:Mrtake/-complex scenes-faceYOLOv3.git (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1153	1160		10.2991/ijcis.d.200805.002													
J								A Search-Based Test Data Generation Method for Concurrent Programs	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Software testing; Test data generation; Concurrent program; Hybrid meta-heuristic algorithm	ANT COLONY OPTIMIZATION; FROG-LEAPING ALGORITHM; GENETIC ALGORITHM; EVOLUTION; COVERAGE	Concurrent programs are being widely adopted]. a development of multi-core and many-core processors. However., these types of programs present some features such as concurrency, communication and synchronization which make their testing more challenging than sequential programs. Search-based techniques, which use meta-heuristic search algorithms, have frequently been used for testing sequential programs, especially in the test data generation activity. However, application of search-based techniques in test data generation for concurrent programs has seldom been covered in the literature. The first contribution of this paper is to present a search-based test data generation framework for concurrent programs. Additionally, a hybrid meta-heuristic algorithm, called SFLA-VND, is proposed, which could be used in the mentioned framework as well as other meta-heuristic algorithms. SFLA-VND is a combination of the shuffled frog leaping algorithm (SFLA) and the variable neighborhood descent (VND). The proposed framework has been experimented on five concurrent benchmark programs by applying genetic algorithm (GA), ant colony optimization (ACO), particle swarm optimization (PSO), SFLA and SFLA-VND. Experimental results demonstrate the effectiveness and efficiency of this framework. Also, the results confirm the superiority of SFLA-VND in comparison With some popular meta-heuristic algorithms, When they are used for test data generation. (C) 2020 The authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1161	1175		10.2991/ijcis.d.200805.003													
J								Group Decision Algorithm for Aged Healthcare Product Purchase Under q-Rung Picture Normal Fuzzy Environment Using Heronian Mean Operator	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Aged healthcare product purchase; Group decision-making; q rung picture normal fuzzy sets; Heronian mean operators	SENTIMENT ANALYSIS; AGGREGATION OPERATORS; RANKING PRODUCTS; ONLINE REVIEWS; FRAMEWORK; SELECTION; NUMBERS; MEDIA; MODEL; RISK	With the intensification of the aging, the health issue of the elderly is amusing public concern increasingly Various healthcare products for the elderly are emerging from the market, this how to select suitable aged healthcare product is critical to the wellbeing of the elderly. In the literature, nonetheless, a comprehensive and standardized evaluation framework to support healthcare product purchase decision for the aged is currently lacking. This paper proposes a novel group decision-making method to aid the decision-making of aged healthcare product purchase based on q-rung picture normal fuzzy Heronian mean (q-RPtNoFHM) operators. In it, firstly, a new fuzzy variable called the q-rung picture normal fuzzy set (q-RPtNoFS) is defined to reasonably describe different responses to healthcare product evaluation, for which, some definitions including operational laws, a score function, and an accuracy function of q-RPtNoFSs are introduced. Then, two q-RPtNoFIIM operators are presented to aggregate group decision information. In addition, some properties of q-RPtNoFHM operators, such as monotonicity, commutativity, and idempotency, are discussed. Finally, an example on antihypertensive drugs purchase is gave to illustrate the practicality of the proposed method, and conduct sensitivity analysis to analyze the effectiveness and flexibility of proposed methods. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1176	1197		10.2991/ijcis.d.200803.001													
J								Flow Measurement of Natural Gas in Pipeline Based on 1D-Convolutional Neural Network	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Flow measurement; Ultrasonic signal; Arrival time; Feature recognitin; Convolutional neural network		Time-difference method is a vitally signlicant algorithm for measuring natural gas flow csrith Ultrasonic gas flowmeter. The key (i)f this algorithm is to accurately measure the annual time of ultrasonic signal. However, it is difficult to determine the 'feature points corresponding to the arrival time stably and accurately. To solve this problem, based on great feature recognition ability of deep learning, one-dimensional-convolutional neural network (ID-CNN) is utilized to determine the arrival time of ultrasonic signal according to the feature of the arrival time. First of all, a dataset, Which includes different features such as different arrival time, different signal-to-noises (SNRs), etc., is used as a training set to train the ID CNN. Then, based on the size of the training set, an 1 D-CNN is designed which includes three convolution and pooling layers and one fully connected layer to determine the arrival time, and the gas flow rate is calculated. To verify this method, an experimental ultrasonic gas flowmeter system is developed. By comparing With the typical method of determining arrival time, most of the deviations distribute close to zero and less than is using the proposed ID-CNN, which verifies the effectiveness of the proposed ID CNN method. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1198	1206		10.2991/ijcis.d.200803.002													
J								Predicting Cards Using a Fuzzy Multiset Clustering of Decks	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Fuzzy multisets; Clustering; Deck analysis; Hearthstone	EFFICIENT ALGORITHM	Search-based agents have shown to perform well in many game-based applications. In the context of partially, scenarios agent's require the state to be fully determinized. Especially in case of collectible cards games, the sheer number of decks constructed by players hinder an agent to reliably estimate the game's current state, and therefore, renders the search ineffective. In this paper, we propose the use of a (fuzzy) multiset representation to describe frequently played decks. Extracted deck prototypes have shown to match human expert labels well and seem to serve as an efficient abstraction of the deck space. We further show that such deck prototypes allow the agent to predict upcoming cards with high accuracy, therefore, allowing more accurate sampling procedures for search-based agents. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1207	1217		10.2991/ijcis.d.200805.001													
J								Tolerance Rough Set-Based Bag-of-Words Model for Document Representation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Document representation; Rblerance rough set; Brag-of-words		Document representation foundation.)f. natural language processing. The bag-oEwo ds (BoW) model, as the representative of document representation models, is a method with the properties of simplicity and validity. However, the traditional el has the drawbacks of sparsity and lacking of latent semantic relations. In this paper, to solve these mentioned propose two tolerance rough set -based BOW models, called as TRBoW1 and TRBoW2 according to different weight calculation methods. Different from the popular representation methods of supervision, they are unsupervised and no prior knowledge required. Extending each document to its upper approximation with TREBoW1 or TRBoW2, the semantic relations among documents are mined and document vectors become denser. Comparative experiments on various document representation methods for text classification on different datasets have verified optimal performance of our methods. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1218	1226		10.2991/ijcis.d.200808.001													
J								Bid Evaluation for Major Construction Projects Under Large-Scale Group Decision-Making Environment and Characterized Expertise Levels	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Bid evaluation; Expert classification; Consensus reaching processes; ELECTRE HI; Multi-attribute group decision-making	LINGUISTIC TERM SETS; PERSONALIZED INDIVIDUAL SEMANTICS; GREEN SUPPLIER SELECTION; CONTRACTOR SELECTION; CONSENSUS MODEL; ELECTRE III; CRITERIA; HIERARCHY; INFORMATION; ATTRIBUTES	Rapid growth and development of civil engineering in recent years inspire building enterprises to concentrate on construction contractor selection for achieving more construction quality and lower construction cost. The existing studies generally regard the process of selecting the best contractor as a multi-criteria at decision making problem. Few research studies addressed the contractor selection problem in the context of large-scale out decision making, which is common in practical scenarios in terms of major construction projects as a number of experts with diverse backgrounds am usually involved. On this basis, we establish a contractor selection framework under large-scale group decision making environment, which covers expert classification, consensus reaching process, collective decision matrix generation, and the ranking-oriented decision making moth (A. We cluster expert group with K-means clustering method based on expertise levels, Which are depicted by six features generated with an expertise identification approach. The consensus model manages consensus reaching process from both infra- and interlayers and takes into account the interactions between them. After reaching agreements among experts, this paper utilizes the concept of proportional hesitant fuzzy linguistic term set to assemble intra-subgroup assessments for the reduction of information loss or distortion. Then, an aggregation process carries on as to gather subgroup assessments in which the subgroup weights are derived from their cluster Centers and sizes in the use of the TOPSIS method. Finally, the well-established decision making tool integrating qualitative and quantitative criteria, ELECTRE III, is adapted to elicit the ranking of bidders. An illustrative study and a comparative analysis are performed to demonstrate the 'feasibility and effectiveness of the established multi-criteria group decision making approach. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1227	1242		10.2991/ijcis.d.200801.002													
J								A Formal Concept Analysis Approach to Cooperative Conversational Recommendation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Formal concept analysis; Simplification logic; Conversational recommender systems; Group recommender system	FEATURE-SELECTION; SYSTEMS; RETRIEVAL; CLOSURE	We focus on the development of a method to guide the choice of a set of users in an environment where the nunlber of features describing the items is high and user interaction becomes laborious. Using the framework of formal concept analysis, particularly the notion of implication between attributes, we pr pose a method strongly based on logic which allows to manage the users' preferences by following a conversational paradigm. Concerning complexity, to build the conversation and provide updated information based on the users' previous actions (choices) the method has polyilonUal delay. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1243	1252		10.2991/ijcis.d.200806.001													
J								Discovering Potential Partners via Projection-Based Link Prediction in the Supply Chain Network	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Supply chain network; Resilience; Potential partners; Link prediction	SELECTION; RECOMMENDATION; COLLABORATION; RESILIENCE; ALGORITHM; MODEL	As reserving a certain number of poteittial partners plays a significant role in alleviating existing partners' collaborative interruption risks, we investigate the process of discovering potential partners to improve the supply chain network's resilience. Most of the existing research confines its locus on discovering potential partners in the supply chain on the basic of sufficient partners' information, but very few works consider discovering potential partners in the supply chain network according to the structure of the supply chain network when the partner information is insufficient In tins situation, a novel model Which applies projection-based link prediction method to discover potential partners in the supply chain network is proposed. The prcposed model is composed of three stages. The first stage is predicting the candidate partnerships links based on the projection one model graph which is trans.formed from the supply chain network according to its structure. The second stage is discovering potential partners by comparing the acquired connectivity of candidate partnership links with the maximal connectivity of existent partnerships. Iiihe third stage, a resilience evaluation framework considering the both connectiity and flexibility indexes is presented to determine whether the supply chain network's agility is improved. In the experimental design, a supply thain network is is formed from a real dataset containing mobile phone suppliers, manufacturers and packers is used to evaluate the proposed algorithnfs prediction accuracy. The results reveal that the algorithm achieves highest area under curve (AUG) scores and the supply chain network's resilience is improved by discovering potential partners. (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1253	1264		10.2991/ijcis.d.20083.001													
J								Cubic Graphs and their Application to a Traffic Flow Problem	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Cubic set; Interval-valued fuzzy set; Cubic graph; (complete, strong) cubic graph; Cubic (bridge, cutvertex); Traffic flows	IDEALS; SETS	A graph structure is a useful tool in solving the combinatorial problems in different areas of computer science and computational intelligence systems. In this paper, we introduce the concept of cubic graph, which is different from the notion of cubic graph in S. Rashid, N. Yactoob, M. Akram, M. Gulistan, Cubic graphs with application, Int. J. Anal. Appl. 16 (2018), 733-750, and investigate some of their interesting properties. Then we define the notions of cubic path, cubic cycle, cubic diameter, strength of cubic graph, complete cubic graph, strong cubic graph and illustrate these notions by several examples. We prove that any cubic bridge is strong and we investigate equivalent condition for cubic cutvertex. Finally, we use the concept of cubic graphs in traffic flows to get the least time to reach the destination. Copyright (C) 2020 The Authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1265	1280		10.2991/ijcis.d.200730.002													
J								A Deng-Entropy-Based Evidential Reasoning Approach for Multi-expert Multi-criterion Decision-Making with Uncertainty	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multi expert multi criterion decision making; Evidential reasoning; Deng entropy; Incertainly; Lung cancer	LUNG-CANCER; COMBINATION; DIAGNOSIS	The evidential reasoning (ER) approach has been widely applied to aggregate evaluation information in multi-expert multi criterion decision-making (MEMCDM) problems with uncertainties. However, the comprehensive results derived by the ER approach remain uncertain. In this study, we propose a Deng-entropy-based ER approach for MEMCDM problems to reduce the uncertainty. Firstly, we reassign the remaining belief of the uncertain evaluation information to the focal elements of the given evaluations. Afterward, we introduce the Deng entropy to respectively calculate the objective weights of criteria and those of experts, so as to reduce the subjective uncertainty in MEMCDM. Then, the ER approach is applied twice to generate the comprehensive evaluations of alternatives. A method is introduced to rank alternatives corresponding to their comprehensive evaluations, forming a Deng-entropy-based ER approach for MEMCDM problems with uncertainty. An illustrative example of screening the people at high risk of lung cancer is provided, and comparative analyses are given to show the rationality and superiority of the proposed method. (C) 2020 The authors. Published by Atlantis Press B.V.																	1875-6891	1875-6883					2020	13	1					1281	1294		10.2991/ijcis.d.200514.001													
J								Unobtrusive Activity Recognition and Position Estimation for Work Surfaces Using RF-Radar Sensing	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Activity recognition; retail; deskwork; sensing; radio frequency radar sensor; IMU		Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users' privacy concerns associated with cameras and is useful for a wide range of intelligent systems.																	2160-6455	2160-6463				JAN	2020	10	1			SI				11	10.1145/3241383													
J								A Bandit-Based Ensemble Framework for Exploration/Exploitation of Diverse Recommendation Components: An Experimental Study within E-Commerce	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										E-commerce recommender systems; streaming recommendations; multi-arm bandit ensembles; session-based recommendations; Thompson Sampling; reinforcement learning	RANKED BANDITS	This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. In addition, we detail the construction of a personalized predictor based on k-Nearest Neighbors (kNN), with temporal decay capabilities and event weighting. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.																	2160-6455	2160-6463				JAN	2020	10	1			SI				4	10.1145/3237187													
J								A Roadmap to User-Controllable Social Exploratory Search	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Exploratory search; social search; user-controllable interface	GAIN-BASED EVALUATION; WEB; VISUALIZATION; INFORMATION; SYSTEM	Information-seeking tasks with learning or investigative purposes are usually referred to as exploratory search. Exploratory search unfolds as a dynamic process where the user, amidst navigation, trial and error, and on-the-fly selections, gathers and organizes information (resources). A range of innovative interfaces with increased user control has been developed to support the exploratory search process. In this work, we present our attempt to increase the power of exploratory search interfaces by using ideas of social search- for instance, leveraging information left by past users of information systems. Social search technologies are highly popular today, especially for improving ranking. However, current approaches to social ranking do not allow users to decide to what extent social information should be taken into account for result ranking. This article presents an interface that integrates social search functionality into an exploratory search system in a user-controlled way that is consistent with the nature of exploratory search. The interface incorporates control features that allow the user to (i) express information needs by selecting keywords and (ii) to express preferences for incorporating social wisdom based on tag matching and user similarity. The interface promotes search transparency through color-coded stacked bars and rich tooltips. This work presents the full series of evaluations conducted to, first, assess the value of the social models in contexts independent to the user interface, in terms of objective and perceived accuracy. Then, in a study with the full-fledged system, we investigated system accuracy and subjective aspects with a structural model revealing that when users actively interacted with all of its control features, the hybrid system outperformed a baseline content-based-only tool and users were more satisfied.																	2160-6455	2160-6463				JAN	2020	10	1			SI				8	10.1145/3241382													
J								A Data-Driven Approach to Designing for Privacy in Household IoT	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Designing for IoT; privacy	PERSONALIZATION; PARADOX; WILLINGNESS; AWARENESS; IMPACT	In this article, we extend and improve upon a previously developed data-driven approach to design privacy-setting interfaces for users of household IoT devices. The essence of this approach is to gather users' feedback on household IoT scenarios before developing the interface, which allows us to create a navigational structure that preemptively maximizes users' efficiency in expressing their privacy preferences, and develop a series of 'privacy profiles' that allow users to express a complex set of privacy preferences with the single click of a button. We expand upon the existing approach by proposing a more sophisticated translation of statistical results into interface design, and by extensively discussing and analyzing the tradeoff between user-model parsimony and accuracy in developing privacy profiles and default settings.																	2160-6455	2160-6463				JAN	2020	10	1			SI				10	10.1145/3241378													
J								A Visual Analytics Approach for Interactive Document Clustering	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Interactive document clustering; key-term; visualization; document projection; user study; text; email list; seeding; deterministic		Document clustering is a necessary step in various analytical and automated activities. When guided by the user, algorithms are tailored to imprint a perspective on the clustering process that reflects the user's understanding of the dataset. More than just allow for customized adjustment of the clusters, a visual analytics approach will provide tools for the user to draw new insights on the collection. While contributing his or her perspective, the user will also acquire a deeper understanding of the data set. To that effect, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. In the proposed system, initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. A set of coordinated visualizations allow the examination of the dataset and the results of the clustering. The visualization provides the user with the highlights of individual documents and understanding of the evolution of documents over the time period to which they relate. The users then interact with the process by means of changing key-terms that drive the process according to their knowledge of the documents domain. In key-term-based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. We have improved that process with a novel algorithm for choosing proper seeds for the clustering. Results demonstrate that not only the system has improved considerably its precision, but also its effectiveness in the document-based decision making. A set of quantitative experiments and a user study have been conducted to show the advantages of the approach for document analytics based on clustering. We performed and reported on the use of the framework in a real decision-making scenario that relates users discussion by email to decision making in improving patient care. Results show that the framework is useful even for more complex data sets such as email conversations.																	2160-6455	2160-6463				JAN	2020	10	1			SI				6	10.1145/3241380													
J								FourEyes: Leveraging Tool Diversity as a Means to Improve Aggregate Accuracy in Crowdsourcing	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Crowdsourcing; human computation; multi-tool aggregation; tool diversity; semantic image segmentation; computer vision		Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and handeye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition, we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool's design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018 [46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach.																	2160-6455	2160-6463				JAN	2020	10	1			SI				3	10.1145/3237188													
J								AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Interactive machine learning; visualization; error discovery; semantic data exploration; unlabeled data; concept discovery; machine teaching	OF-THE-ART; VISUALIZATION	When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.																	2160-6455	2160-6463				JAN	2020	10	1			SI				7	10.1145/3241379													
J								Individualising Graphical Layouts with Predictive Visual Search Models	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Visual search; graphical layouts; computational design; adaptive user interfaces	MEMORY	In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) on an interface. This article contributes individualised predictive models of visual search, and a computational approach to restructure graphical layouts for an individual user such that features on a new, unvisited interface can be found quicker. It explores four technical principles inspired by the human visual system (HVS) to predict expected positions of features and create individualised layout templates: (I) the interface with highest frequency is chosen as the template; (II) the interface with highest predicted recall probability (serial position curve) is chosen as the template; (III) the most probable locations for features across interfaces are chosen (visual statistical learning) to generate the template; (IV) based on a generative cognitive model, the most likely visual search locations for features are chosen (visual sampling modelling) to generate the template. Given a history of previously seen interfaces, we restructure the spatial layout of a new (unseen) interface with the goal of making its features more easily tradable. The four HVS principles are implemented in Familiariser, a web browser that automatically restructures webpage layouts based on the visual history of the user. Evaluation of Familiariser (using visual statistical learning) with users provides first evidence that our approach reduces visual search time by over 10%, and number of eye-gaze fixations by over 20%, during web browsing tasks.																	2160-6455	2160-6463				JAN	2020	10	1			SI				9	10.1145/3241381													
J								Exploring Social Recommendations with Visual Diversity-Promoting Interfaces	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Social recommendation; user interface; user-driven exploration; diversification; diversity-promoting interface	COMMERCE; AGENTS; SEARCH	The beyond-relevance objectives of recommender systems have been drawing more and more attention. For example, a diversity-enhanced interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about how users adopt diversity-enhanced interfaces to accomplish various real-world tasks. In this article, we present two attempts at creating a visual diversity-enhanced interface that presents recommendations beyond a simple ranked list. Our goal was to design a recommender system interface to help users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two within-subject user studies in the context of social recommendation at academic conferences were conducted to compare our visual interfaces. Results from our user study show that the visual interfaces significantly reduced the exploration efforts required for given tasks and helped users to perceive the recommendation diversity. We show that the users examined a diverse set of recommended items while experiencing an improvement in overall user satisfaction. Also, the users' subjective evaluations show significant improvement in many user-centric metrics. Experiences are discussed that shed light on avenues for future interface designs.																	2160-6455	2160-6463				JAN	2020	10	1			SI				5	10.1145/3231465													
J								Exploring a Design Space of Graphical Adaptive Menus: Normal vs. Small Screens	ACM TRANSACTIONS ON INTERACTIVE INTELLIGENT SYSTEMS										Adaptation; adaptive user interfaces; adaptivity; graphical adaptive menus; intelligent user interfaces; menu selection; prediction scheme; prediction window; split interface	PULL-DOWN MENUS; PREFERENCE; USABILITY; FRAMEWORK	Graphical Adaptive Menus are Graphical User Interface menus whose predicted items of immediate use can be automatically rendered in a prediction window. Rendering this prediction window is a key question for adaptivity to enable the end-user to efficiently differentiate predicted items from normal ones and to consequently select appropriate items. Adaptivity for graphical menus has been investigated more for normal screens, such as desktops, than for small screens, such as smartphones, where real estate imposes severe rendering constraints. To address this question, this article defines and explores a design space where graphical adaptive menus are structured based on Bertin's eight visual variables (i.e., position, size, shape, value, color, orientation, texture, and motion) and their combination by comparing their rendering for small screens with respect to normal screens. Based on this design space, previously introduced graphical adaptive menus are revisited in terms of four stability properties (i.e., spatial, physical, format, and temporal), and new menu designs are introduced and discussed for both normal and small screens. The resulting set of graphical adaptive menu has been subject to a preference analysis from which a particular design emerged: the cloud menu, where predicted items are arranged in an adaptive tag cloud. We investigate empirically the effect of the cloud menu on the item selection time and the error rate with respect to a static menu and an adaptive linear menu. This article then suggests a set of usability guidelines for designers and practitioners to design graphical adaptive menus in general and cloud menus in particular.																	2160-6455	2160-6463				JAN	2020	10	1			SI				2	10.1145/3237190													
J								Accurate Long-Term Multiple People Tracking Using Video and Body-Worn IMUs	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tracking; Trajectory; Visualization; Sensors; Task analysis; Labeling; Multiple people tracking; graph labeling; sensor fusion; IMU; human motion analysis	HUMAN POSE ESTIMATION; MULTITARGET	Most modern approaches for video-based multiple people tracking rely on human appearance to exploit similarities between person detections. Consequently, tracking accuracy degrades if this kind of information is not discriminative or if people change apparel. In contrast, we present a method to fuse video information with additional motion signals from body-worn inertial measurement units (IMUs). In particular, we propose a neural network to relate person detections with IMU orientations, and formulate a graph labeling problem to obtain a tracking solution that is globally consistent with the video and inertial recordings. The fusion of visual and inertial cues provides several advantages. The association of detection boxes in the video and IMU devices is based on motion, which is independent of a person's outward appearance. Furthermore, inertial sensors provide motion information irrespective of visual occlusions. Hence, once detections in the video are associated with an IMU device, intermediate positions can be reconstructed from corresponding inertial sensor data, which would be unstable using video only. Since no dataset exists for this new setting, we release a dataset of challenging tracking sequences, containing video and IMU recordings together with ground-truth annotations. We evaluate our approach on our new dataset, achieving an average IDF1 score of 91.2%. The proposed method is applicable to any situation that allows one to equip people with inertial sensors.																	1057-7149	1941-0042					2020	29						8476	8489		10.1109/TIP.2020.3013801													
J								Visual Saliency via Embedding Hierarchical Knowledge in a Deep Neural Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature fusion; hierarchical knowledge embedding; saliency map; task-independent saliency	MODEL; ATTENTION; FIXATIONS; PREDICT	Deep neural networks (DNNs) have been extensively applied in image processing, including visual saliency map prediction of images. A major difficulty in using a DNN for visual saliency prediction is the lack of labeled ground truth of visual saliency. A powerful DNN usually contains a large number of trainable parameters. This condition can easily lead to model over-fitting. In this study, we develop a novel method that overcomes such difficulty by embedding hierarchical knowledge of existing visual saliency models in a DNN. We achieve the objective of exploiting the knowledge contained in the existing visual saliency models by using saliency maps generated by local, global, and semantic models to tune and fix about 92.5% of the parameters in our network in a hierarchical manner. As a result, the number of trainable parameters that need to be tuned by the ground truth is considerably reduced. This reduction enables us to fully utilize the power of a large DNN and overcome the issue of over-fitting at the same time. Furthermore, we introduce a simple but very effective center prior in designing the learning cost function of the DNN by attaching high importance to the errors around the image center. We also present extensive experimental results on four commonly used public databases to demonstrate the superiority of the proposed method over classical and state-of-the-art methods on various evaluation metrics.																	1057-7149	1941-0042					2020	29						8490	8505		10.1109/TIP.2020.3016464													
J								Bayesian Adversarial Spectral Clustering With Unknown Cluster Number	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spectral clustering; Bayesian learning; low rank; variational inference; generative adversarial network	INFERENCE	Spectral clustering is a popular tool in many unsupervised computer vision and machine learning tasks. Recently, due to the encouraging performance of deep neural networks, many conventional spectral clustering methods have been extended to the deep framework. Although these deep spectral clustering methods are quite powerful and effective, learning the cluster number from data is still a challenge. In this article, we aim to tackle this problem by integrating the spectral clustering, generative adversarial network and low rank model within a unified Bayesian framework. First, we adapt the low rank method to the cluster number estimation problem. Then, an adversarial-learning-based deep clustering method is proposed and incorporated. When introducing the spectral clustering method into our model clustering procedure, a hidden space structure preservation term is proposed. Via a Bayesian framework, the structure preservation term is embedded into the generative process, which can then be used to deduce a spectral clustering in the optimization procedure. Finally, we derive a variational-inference-based method and embed it into the network optimization and learning procedure. Experiments on different datasets prove that our model has the cluster number estimation capability and show that our method can outperform many similar graph clustering methods.																	1057-7149	1941-0042					2020	29						8506	8518		10.1109/TIP.2020.3016491													
J								Universal Face Photo-Sketch Style Transfer via Multiview Domain Translation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Style transfer; domain translation; face synthesis	RECOGNITION	Face photo-sketch style transfer aims to convert a representation of a face from the photo (or sketch) domain to the sketch (respectively, photo) domain while preserving the character of the subject. It has wide-ranging applications in law enforcement, forensic investigation and digital entertainment. However, conventional face photo-sketch synthesis methods usually require training images from both the source domain and the target domain, and are limited in that they cannot be applied to universal conditions where collecting training images in the source domain that match the style of the test image is unpractical. This problem entails two major challenges: 1) designing an effective and robust domain translation model for the universal situation in which images of the source domain needed for training are unavailable, and 2) preserving the facial character while performing a transfer to the style of an entire image collection in the target domain. To this end, we present a novel universal face photo-sketch style transfer method that does not need any image from the source domain for training. The regression relationship between an input test image and the entire training image collection in the target domain is inferred via a deep domain translation framework, in which a domain-wise adaption term and a local consistency adaption term are developed. To improve the robustness of the style transfer process, we propose a multiview domain translation method that flexibly leverages a convolutional neural network representation with hand-crafted features in an optimal way. Qualitative and quantitative comparisons are provided for universal unconstrained conditions of unavailable training images from the source domain, demonstrating the effectiveness and superiority of our method for universal face photo-sketch style transfer.																	1057-7149	1941-0042					2020	29						8519	8534		10.1109/TIP.2020.3016502													
J								An Architecture for Restful Web Service Discovery Using Semantic Interfaces	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Restful Web Service; Semantic Interface; Semantic Web; SERIN; Web Service Discovery	OF-THE-ART; WSMO-LITE; ANNOTATIONS	In the internet environment, web services based on representational state transfer (REST) have become the de facto standard. The addition of semantics is intended to enhance the description of web services with information that enables automatic agents to understand their data. However, the existence of different languages to semantically describe services makes it difficult to discover and select the service that best meets a requirement. Furthermore, relatively few proposals have a RESTful service semantic description, making the discovery process for RESTful services more difficult. This work proposes a RESTful semantic web service discovery architecture based on semantic interfaces (SERIN). SERIN is an ontology with annotations that semantically describe RESTful web services. This architecture enables software agents to automatically discover and make service calls in order to execute a determined task.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					1	24		10.4018/IJSWIS.2020010101													
J								A Status Property Classifier of Social Media User's Personality for Customer-Oriented Intelligent Marketing Systems: Intelligent-Based Marketing Activities	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Crowdsourcing; DISC Theory; Facebook; Personality Prediction; Social Media; Text Semantic Mining	FACEBOOK	Enterprises need to obtain information about not only specific customer preferences, but also, more importantly, customers' psychological characteristics that significantly influence their consumption behaviors and response to intelligent-based marketing activities. If enterprises want to implement more precise intelligent selling activities for customers, customers' personality information will serve as a highly valued reference. The automatic detection method proposed in this study is based on techniques such as text semantic mining and machine learning to conduct personality type prediction on the target by collecting and analyzing the target's social media data. In the test, 5,858 statuses were obtained, 815 of which were labeled, with 122 effective tags. In general, when n = 5, the labeling rate can reach 60-80%. The status property classifier (SPC) proposed in this study can predict the personality type (PT) of the user publishing the status set with a high degree of accuracy by conducting text semantic mining on the status set.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					25	46		10.4018/IJSWIS.2020010102													
J								A Survey of Ontology Benchmarks for Semantic Web Ontology Tools	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Benchmarks Requirements; Ontology Alignment; Ontology Authoring; Ontology Benchmark Recommender; Ontology Reasoning; Ontology Tool Category; Ontology Visualization; OWL	FRAMEWORK	Software engineering employs different benchmarks for a software evaluation. This enables software developers to continuously improve their product. The same needs are intrinsic for software tools in the semantic web field. While there are many different benchmarks already available, there has not been their overview and categorization yet. This work provides such an overview and categorization of benchmarks specifically oriented on benchmarks where an ontology plays an important role. Benchmarks are naturally categorized in line with ontology tool categorization along with an indication which activities those benchmarks are deliberate and which are non-deliberative. Although the article itself can already navigate a reader to an adequate benchmark, we moreover automatically designed a flexible rule-based recommendation tool based on the analysis of existing benchmarks.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					47	68		10.4018/IJSWIS.2020010103													
J								Internet Data Analysis Methodology for Cyberterrorism Vocabulary Detection, Combining Techniques of Big Data Analytics, NLP and Semantic Web	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Big Data Analytics; cyberterrorism; Internet Data Analysis; Machine Learning; Natural Language Processing; Parallel Processing; Semantic Web; Text Mining		This article presents a methodology for the analysis of data on the Internet, combining techniques of Big Data analytics, NLP and semantic web in order to find knowledge about large amounts of information on the web. To test the effectiveness of the proposed method, webpages about cyberterrorism were analyzed as a case study. The procedure implemented a genetic strategy in parallel, which integrates (Crawler to locate and download information from the web; to retrieve the vocabulary, using techniques of NLP (tokenization, stop word, TF, TFIDF), methods of stemming and synonyms). For the pursuit of knowledge was built a dataset through the description of a linguistic corpus with semantic ontologies, considering the characteristics of cyber-terrorism, which was analyzed with the algorithms, Random Forests (parallel), Boosting, SVM, neural network, K-nn and Bayes. The results reveal a percentage of the 95.62% accuracy in the detection of the vocabulary of cyber-terrorism, which were approved through cross validation, reaching 576% time savings with parallel processing.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					69	86		10.4018/IJSWIS.2020010104													
J								A Tool for Transforming Semantic Web Rule Language to SPARQL Infererecing Notation	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Ontologies; OWL; Prolog; Rules; Semantic Web; SPARQL; SPIN; SWRL; Transformation	SWRL RULES; OWL-DL; SYSTEM	Semantic web rule language (SWRL) combines web ontology language (OWL) ontologies with horn logic rules of the rule markup language (Ru1eML) family. Being supported by ontology editors, rule engines and ontology reasoners, it has become a very popular choice for developing rule-based applications on top of ontologies. However, SWRL is probably not going to become a WWW Consortium standard, prohibiting industrial acceptance. On the other hand, SPARQL Inferencing Notation (SPIN) has become a de-facto industry standard to represent SPARQL rules and constraints on semantic web models, building on the widespread acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this article, we argue that the life of existing SWRL rule-based ontology applications can be prolonged by converting them to SPIN. To this end, we have developed the SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules, considering the object-orientation of SPIN, i.e. linking rules to the appropriate ontology classes and optimizing them, as derived by analysing the rule conditions.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					87	115		10.4018/IJSWIS.2020010105													
J								Hybrid Approach for Sentiment Analysis of Twitter Posts Using a Dictionary-based Approach and Fuzzy Logic Methods: Study Case on Cloud Service Providers	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Cloud Service; Dictionary; Fuzzy-Logic; Hashtag; Sentiment Analysis; Tweet; Twitter; Word Polarity		Recently, sentiment analysis of social media has become a hot topic because of the huge amount of information that is provided in these networks. Twitter is a popular social media application offers businesses and government the opportunities to share and acquire information. This article proposes a technique that aims at measuring customers' satisfaction with cloud service providers, based on their tweets. Existing techniques focused on classifying sentimental text as either positive or negative, while the proposed technique classifies the tweets into five categories to provide better information. A hybrid approach of dictionary-based and Fuzzy Inference Process (FIP) is developed for this purpose. This direction was selected for its advantages and flexibility in addressing complex problems, using terms that reflect on human behaviors and experiences. The proposed hybrid-based technique used fuzzy systems in order to accurately identify the sentiment of the input text while addressing the challenges that are facing sentiment analysis using various fuzzy parameters.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					116	145		10.4018/IJSWIS.2020010106													
J								Information Systems Workforce and Innovative Work Behavior: The Role of Participatory Management, Affective Trust and Guanxi	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Affective Trust; Information Systems; Innovative Work Behavior; Participatory Management; Supervisor-Subordinate Guanxi	SUPERVISOR-SUBORDINATE GUANXI; MEDIATING ROLE; MODERATING ROLE; CHINESE; LEADERSHIP; ORGANIZATION; PERFORMANCE; COMMITMENT; OUTCOMES	This research examined the mediating role of affective trust along with the moderating role of supervisor-subordinate guanxi on the relationship between innovative work behavior and participatory management. Utilizing data gathered from information workforce consisting of 333 employees in addition to 38 leaders of six firms operating in China, the research affirms that trust mediated the relationship between innovative work behavior and participatory management. This study concludes that a strong relation between participatory management and innovative work behavior exists provided high-quality supervisor-subordinate guanxi is present among employees. The findings and implications for this research are discussed at the end.																	1552-6283	1552-6291				JAN-MAR	2020	16	1					146	165		10.4018/IJSWIS.2020010107													
J								Tone Mapping Beyond the Classical Receptive Field	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tone mapping; classical receptive field; surround modulation; biologically inspired computer vision	IMAGE-ENHANCEMENT; SUPERIOR COLLICULUS; QUALITY ASSESSMENT; CONTRAST GAIN; MACAQUE V1; MECHANISMS; RETINEX; PATHWAY; FUSION; ILLUMINATION	Some neurons in the primary visual cortex (V1) of human visual system (HVS) conduct dynamic center-surround computation, which is thought to contribute to compress the high dynamic range (HDR) scene and preserve the details. We simulate this dynamic receptive field (RF) property of V1 neurons to solve the so-called tone mapping (TM) task in this paper. The novelties of our method are as follows. (1) Cortical processing mechanisms of HVS are modeled to build a local TM operation based on two Gaussian functions whose kernels and weights adapt according to the center-surround contrast, thus reducing halo artifacts and effectively enhancing the local details of bright and dark parts of image. (2) Our method uses an adaptive filter that follows the contrast levels of the image, which is computationally very efficient. (3) The local fusion between the center and surround responses returned by a cortical processing flow and the global signals returned by a sub-cortical processing flow according to the local contrast forms a dynamic mechanism that selectively enhances the details. Extensive experiments show that the proposed method can efficiently render the HDR scenes with good contrast, clear details, and high structural fidelity. In addition, the proposed method can also obtain promising performance when applied to enhance the low-light images. Furthermore, by modeling these biological solutions, our technique is simple and robust considering that our results were obtained using the same parameters for all the datasets (e. g., HDR images or low-light images), that is, mimicking how HVS operates.																	1057-7149	1941-0042					2020	29						4174	4187		10.1109/TIP.2020.2970541													
J								Revisiting Anchor Mechanisms for Temporal Action Localization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Temporal action localization; default anchor; anchor free; complementarity		Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.																	1057-7149	1941-0042					2020	29						8535	8548		10.1109/TIP.2020.3016486													
J								Self-Supervised Agent Learning for Unsupervised Cross-Domain Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Self-supervised learning; unsupervised cross-domain person re-identification; hard negative mining	NETWORK	Unsupervised person re-identification (Re-ID) has better scalability and practicability than supervised Re-ID in the actual deployment. However, it is difficult to learn a discriminative Re-ID model without annotations. To address the above issue, we propose an end-to-end Self-supervised Agent Learning (SAL) algorithm by exploiting a set of agents as a bridge to reduce domain gaps for unsupervised cross-domain person Re-ID. The proposed SAL model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit self-supervised learning for unsupervised person Re-ID. Second, our model has designed three effective learning mechanisms including supervised label learning in source domain, similarity consistency learning in target domain, and self-supervised learning in cross domain, which can learn domain-invariant yet discriminative representations through the principled lens of agent learning by reducing domain discrepancy adaptively. Extensive experimental results on three standard benchmarks demonstrate that the proposed SAL performs favorably against state-of-the-art unsupervised person Re-ID methods.																	1057-7149	1941-0042					2020	29						8549	8560		10.1109/TIP.2020.3016869													
J								Image Restoration via Simultaneous Nonlocal Self-Similarity Priors	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; structural sparse representation; simultaneous nonlocal self-similarity; Gaussian mixture model; adaptive parameter adjusting	SPARSE REPRESENTATION; SUPERRESOLUTION; REGULARIZATION; ALGORITHM; SMOOTHNESS; REDUCTION; ARTIFACTS; NOISE; DCT	Through exploiting the image nonlocal self-similarity (NSS) prior by clustering similar patches to construct patch groups, recent studies have revealed that structural sparse representation (SSR) models can achieve promising performance in various image restoration tasks. However, most existing SSR methods only exploit the NSS prior from the input degraded (internal) image, and few methods utilize the NSS prior from external clean image corpus; how to jointly exploit the NSS priors of internal image and external clean image corpus is still an open problem. In this article, we propose a novel approach for image restoration by simultaneously considering internal and external nonlocal self-similarity (SNSS) priors that offer mutually complementary information. Specifically, we first group nonlocal similar patches from images of a training corpus. Then a group-based Gaussian mixture model (GMM) learning algorithm is applied to learn an external NSS prior. We exploit the SSR model by integrating the NSS priors of both internal and external image data. An alternating minimization with an adaptive parameter adjusting strategy is developed to solve the proposed SNSS-based image restoration problems, which makes the entire algorithm more stable and practical. Experimental results on three image restoration applications, namely image denoising, deblocking and deblurring, demonstrate that the proposed SNSS produces superior results compared to many popular or state-of-the-art methods in both objective and perceptual quality measurements.																	1057-7149	1941-0042					2020	29						8561	8576		10.1109/TIP.2020.3015545													
J								Restoration of Fast Moving Objects	IEEE TRANSACTIONS ON IMAGE PROCESSING										Blind deconvolution; motion deblurring; object deblurring; motion estimation; image matting; shape estimation; alternating direction method of multipliers	BLIND DECONVOLUTION; IMAGE	If an object is photographed at motion in front of a static background, the object will be blurred while the background sharp and partially occluded by the object. The goal is to recover the object appearance from such blurred image. We adopt the image formation model for fast moving objects and consider objects undergoing 2D translation and rotation. For this scenario we formulate the estimation of the object shape, appearance, and motion from a single image and known background as a constrained optimization problem with appropriate regularization terms. Both similarities and differences with blind deconvolution are discussed with the latter caused mainly by the coupling of the object appearance and shape in the acquisition model. Necessary conditions for solution uniqueness are derived and a numerical solution based on the alternating direction method of multipliers is presented. The proposed method is evaluated on a new dataset.																	1057-7149	1941-0042					2020	29						8577	8589		10.1109/TIP.2020.3016490													
J								Findable and reusable workflow data products: A genomic workflow case study	SEMANTIC WEB										FAIR; Linked Data; scientific workflows; provenance; bioinformatics; data summaries	SUITE	While workflow systems have improved the repeatability of scientific experiments, the value of the processed (intermediate) data have been overlooked so far. In this paper, we argue that the intermediate data products of workflow executions should be seen as first-class objects that need to be curated and published. Not only will this be exploited to save time and resources needed when re-executing workflows, but more importantly, it will improve the reuse of data products by the same or peer scientists in the context of new hypotheses and experiments. To assist curator in annotating (intermediate) workflow data, we exploit in this work multiple sources of information, namely: (i) the provenance information captured by the workflow system, and (ii) domain annotations that are provided by tools registries, such as Bio.Tools. Furthermore, we show, on a concrete bioinformatics scenario, how summarising techniques can be used to reduce the machine-generated provenance information of such data products into concise human- and machine-readable annotations.																	1570-0844	2210-4968					2020	11	5					751	763		10.3233/SW-200374													
J								Continuous top-k approximated join of streaming and evolving distributed data	SEMANTIC WEB										Continuous top-k join; RDF data stream; distributed dataset; RSP engine	QUERIES; MAINTENANCE; SEMANTICS	Continuously finding the most relevant (shortly, top-k) answer of a query that joins streaming and distributed data is getting a growing attention. In recent years, this is in particular happening in Social Media and IoT. It is well known that, in those settings, remaining reactive can be challenging, because accessing the distributed data can be highly time consuming as well as rate-limited. In this paper, we investigate the problem of continuous top-k query evaluation over a data stream joined with a distributed dataset in even a more extreme situation: the distributed data evolves. We propose the Topk+N algorithm and the AcquaTop framework. They keep up to date a local replica of the distributed dataset and guarantees reactiveness by construction, but to do so they may need to approximate the result. Therefore, we propose two maintenance policies to update the replica: the Top Selection Maintenance (AT-TSM) policy maximizes the relevancy, while the Border Selection Maintenance (AT-BSM) policy maximizes the accuracy of the top-k result. We contribute a theoretical proof of the correctness of Topk+N algorithm and we study its complexity. Moreover, we provide empirical evidence that the proposed policies within AcquaTop framework produce more relevant and accurate results than the state of the art.																	1570-0844	2210-4968					2020	11	5					767	799		10.3233/SW-190367													
J								Automatic detection of relation assertion errors and induction of relation constraints	SEMANTIC WEB										SHACL; SPARQL; ontology learning; error detection; knowledge graph	KNOWLEDGE; OWL	Although the link prediction problem, where missing relation assertions are predicted, has been widely researched, error detection did not receive as much attention. In this paper, we investigate the problem of error detection in relation assertions of knowledge graphs, and we propose an error detection method which relies on path and type features used by a classifier for every relation in the graph exploiting local feature selection. Furthermore, we propose an approach for automatically correcting detected errors originated from confusions between entities. Moreover, we present an approach that translates decision trees trained for relation assertion error detection into SHACL-SPARQL relation constraints. We perform an extensive evaluation on a variety of datasets comparing our error detection approach with state-of-the-art error detection and knowledge completion methods, backed by a manual evaluation on DBpedia and NELL. We evaluate our error correction approach results on DBpedia and NELL and show that the relation constraint induction approach benefits from the higher expressiveness of SHACL and can detect errors which could not be found by automatically learned OWL constraints.																	1570-0844	2210-4968					2020	11	5					801	830		10.3233/SW-200369													
J								Data-driven assessment of structural evolution of RDF graphs	SEMANTIC WEB										Data evolution; data management; pattern mining; similarity measure; Semantic Web	EFFICIENT ALGORITHM; SEMANTIC WEB; LINKED DATA; QUALITY; DBPEDIA	Since the birth of the Semantic Web, numerous knowledge bases have appeared. The applications that exploit them rely on the quality of their data through time. In this regard, one of the main dimensions of data quality is conformance to the expected usage of the vocabulary. However, the vocabulary usage (i.e., how classes and properties are actually populated) can vary from one base to another. Moreover, through time, such usage can evolve within a base and diverge from the previous practices. Methods have been proposed to follow the evolution of a knowledge base by the observation of the changes of their intentional schema (or ontology); however, they do not capture the evolution of their actual data, which can vary greatly in practice. In this paper, we propose a data-driven approach to assess the global evolution of vocabulary usage in large RDF graphs. Our proposal relies on two structural measures defined at different granularities (dataset vs update), which are based on pattern mining techniques. We have performed a thorough experimentation which shows that our approach is scalable, and can capture structural evolution through time of both synthetic (LUBM) and real knowledge bases (different snapshots and updates of DBpedia).																	1570-0844	2210-4968					2020	11	5					831	853		10.3233/SW-200368													
J								VocBench 3: A collaborative Semantic Web editor for ontologies, thesauri and lexicons	SEMANTIC WEB										Collaborative editing; ontologies; thesauri; lexicons; OWL; SKOS; OntoLex	KNOWLEDGE ACQUISITION; API; ENVIRONMENT	VocBench is an open source web platform for the collaborative development of datasets complying with SemanticWeb standards. Since its public release - five years ago - as an open source platform, VocBench has attracted a growing user community consisting of public organizations, companies and independent users looking for open source solutions for maintaining their thesauri, code lists and authority resources. The focus on collaboration, the differentiation of user roles and the workflow management for content validation and publication have been the strengths of the platform, especially for those organizations requiring a distributed, yet centrally controlled, publication environment. In 2017, a new, completely reengineered, version of the system has been released, broadening the scope of the platform: funded by the ISA2 programme of the European Commission, VocBench 3 offers a general-purpose collaborative environment for development of any kind of RDF dataset (with dedicated facilities for ontologies, thesauri and lexicons), improving the editing capabilities of its predecessor, while still maintaining the peculiar aspects that determined its success. In this article, we review the requirements and the new objectives set for version 3, and then introduce the new characteristics that were implemented for this new incarnation of the platform.																	1570-0844	2210-4968					2020	11	5					855	881		10.3233/SW-200370													
J								CODI: A multidimensional theory of mereotopology with closure operations	APPLIED ONTOLOGY										Spatial ontology; qualitative spatial reasoning; mereotopology; topological relations; geometric data models; GIS; first-order logic; supplementation principle	ONTOLOGY; SEMANTICS; PARTS	Geometric data models form the backbone of virtually all spatial information systems, such as GIS, CAD, and CAM. Yet a lot of spatial information from textual sources, including historical documents or social media, is predominantly of qualitative, especially mereotopological, rather than geometric-quantitative nature. While mereotopological theories have been extensively studied in Logic, Computer Science, Cognitive Science, and Geographic Information Science, most are unidimensional mereotopologies in the sense that only entities of a single dimension are permitted to co-exist. Integrating mereotopological information with geometric data requires a multidimensional mereotopology, which permits entities of different dimensions to co-exist, similarly to how geometric and algebraic topological data models permit points, simple lines, polylines, cells, polygons, and polyhedra to co-exist. It further requires complex spatial objects to be represented as sets of atomic entities such that spatial relations between complex objects can be computed from the relations of the atomic entities in their decomposition. This paper provides a comprehensive study of CODI, a first-order logic ontology of multidimensional mereotopology. An axiomatization of mereological closure operations of intersection, difference, and sums for CODI is proposed in which these operations apply to all pairs of spatial entities regardless of their dimension. It is proved that for atomic models - and thus all finite models - the extended theory is indeed able to decompose all spatial entities into a partition of atomic parts. A full representation of the models as sets of Boolean algebras verifies this. The closure operations are further shown to satisfy important mereological principles from unidimensional mereotopology and to preserve many of the mathematical properties of set intersection and set difference.																	1570-5838	1875-8533					2020	15	3					251	311		10.3233/AO-200233													
J								The FOUnt ontologies for quantities, units, and the physical world	APPLIED ONTOLOGY										Foundational ontology; ontological commitments; TUpper; units of measure; vector space		Quantities and units of measure provide an important means by which intelligent agents interact with the physical world. Although multiple ontologies for quantities and units of measure have been proposed within the Applied Ontology com-munity, they often incorporate questionable ontological commitments. Quantities are combined using notions of dimensional analysis that often conflate the combination of units with algebraic operations on real numbers. In this paper, we present an alternative approach that shifts the focus to the connection between kinds of measurements associated with a unit and the phys-ical objects and processes that are being measured. One of the key features of this approach is that it makes minimal ontological commitments with respect to the TUpper upper ontology - the only new classes that are introduced are the classes for kinds of measures and associated units. We propose axiomatizations of the intended semantics for combining measurements, and the correct axiomatization of the relationship between the quantities and units of measure and the existing upper ontology.																	1570-5838	1875-8533					2020	15	3					313	359		10.3233/AO-200231													
J								Parthood and part-whole relations in Zulu language and culture	APPLIED ONTOLOGY										Mereology; meronomy; part-whole relation; isiZulu; ontology development; multilingual ontologies	EXTRACTION; ONTOLOGY; MODEL	Part-whole relations are pervasive throughout domain ontologies and enjoy interest also in, inter alia, NLP and manufacturing, and by philosophers in the scope of mereology. There exist a stable list of part-whole relations that are assumed to be common, yet for isiZulu, among other languages, there were at least linguistic differences. This raises the question whether there are ontological differences, which would imply that the `common' list is not universal across languages and cultures. We investigated this for 18 part-whole terms in the Zulu language that we selected from an initial list of 81 terms collected. They were formalised and aligned to the well-known part-whole relations, and checked against a corpus. While there is a term for general parthood in Zulu, the main difference observed concerns relation proliferation due to very specific relata that are entities typically represented only in domain ontologies. This poses new questions for ontology engineering on how to manage the plurality of relations and for philosophy to possibly extend mereology.																	1570-5838	1875-8533					2020	15	3					361	384		10.3233/AO-200230													
J								SAREF4health: Towards IoT standard-based ontology-driven cardiac e-health systems	APPLIED ONTOLOGY										Ontology-driven healthcare; Internet of Things; ontology-driven conceptual modelling; SAREF; ECG	INTERNET; DEVICES; ECG	Recently, a number of ontology-driven healthcare systems have been leveraged by the Internet-of-Things (IoT) technologies that offer opportunities to improve abnormal situation detection when integrating medical wearables and cloud in-frastructure. Usually, these systems rely on standardised IoT ontologies to represent sensor data observations. The ETSI Smart Applications REFerence ontology (SAREF) is an extensible industry-oriented standard. In this paper, we explain the need for interoperability of IoT healthcare applications and the role of standardised ontologies to achieve semantic interoperability. In particular, we discuss the verbosity problem of SAREF when used for real-time electrocardiography (ECG), emphasizing the requirement of representing time series. We compared the main ontologies in this context, according to quality, message size (payload), IoT-orientation and standardisation. Here we describe the first attempt to extend SAREF for specific e-Health use cases related to ECG data, the SAREF4health extension, which tackles the verbosity problem. Ontology-driven conceptual modelling was applied to develop SAREF4health, in which an ECG ontology grounded in the Unified Foundational Ontology (UFO), which plays the role of a reference model. The methodology was enhanced by following a standardisation procedure and considering the RDF implementation of the HL7 Fast Healthcare Interoperability Resources (FHIR) standard. The val-idation of SAREF4health includes the responses to competency questions, as well as the development and tests of an IoT Early Warning System prototype that uses ECG data and collision identification to detect accidents with truck drivers in a port area. This prototype integrates an existing ECG wearable with a cloud infrastructure, demonstrating the performance impact of SAREF4health considering IoT constraints. Our results show that SAREF4health enables the semantic interoperability of IoT solutions that need to deal with frequency-based time series. Design decisions regarding the trade-off between ontology quality and aggregation representation are also discussed.																	1570-5838	1875-8533					2020	15	3					385	410		10.3233/AO-200232													
J								The improved ColourAnt algorithm: a hybrid algorithm for solving the graph colouring problem	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										graph colouring; metaheuristic; ant colony optimisation; ACO	ANTS	The graph colouring problem is interesting because of its application areas, ranging from register allocation, frequency association in telecommunications, timetabling and scheduling, and others. This problem is NP-complete and thus, several metaheuristic algorithms have been proposed in order to provide a good solution in an acceptable time-frame. Among several metaheuristics, this paper focuses on ant colony optimisation. In this context, a hybrid algorithm was developed called iColourAnt, which uses ant colony optimisation and an efficient local search strategy, consequently providing good solutions to the graph colouring problem. The experiment results indicate that iColourAnt outperforms its predecessor, ColourAnt.																	1758-0366	1758-0374					2020	16	1					1	12		10.1504/IJBIC.2020.109000													
J								A new replica placement strategy based on multi-objective optimisation for HDFS	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										Hadoop; Hadoop distributed file system; HDFS; replica placement; multi-objective optimisation; memetic algorithm	BIG DATA; EVOLUTIONARY; ALGORITHM; SYSTEM; DECOMPOSITION; HADOOP; MOEA/D	Distributed storage systems like the Hadoop distributed file system (HDFS) constitute the core infrastructure of cloud platforms which are well poised to deal with big-data. An optimised HDFS is critical for effective data management in terms of reduced file service time and access latency, improved file availability and system load balancing. Recognising that the file-replication strategy is key to an optimised HDFS, this paper focuses on the file-replica placement strategy while simultaneously considering storage and network load. Firstly, the conflicting relationship between storage and network load is analysed and a bi-objective optimisation model is built, following which a multi-objective optimisation memetic algorithm based on decomposition (MOMAD) and its improved version are used. Compared to the default strategy in HDFS, the file-replica placement strategies based on multi-objective optimisation provide more diverse solutions. And competitive performance could be obtained by the proposed algorithm.																	1758-0366	1758-0374					2020	16	1					13	22		10.1504/IJBIC.2020.108994													
J								A privacy-preserving recommendation method based on multi-objective optimisation for mobile users	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										recommender systems; multi-objective optimisation; differential privacy; mobile users	MATRIX FACTORIZATION; ALGORITHM; SCHEME	Recommender systems have proven to be an effective technique to deal with information overload and mislead problems by helping users get useful and valuable information or objects from massive data. However, exploiting users' preferences with recommendation algorithms lead to serious privacy risks, especially when recommender service providers are unreliable. An ideal recommender system should be both accurate, diverse and security. In this paper, we propose a private recommendation method which consists of a private collaborative filtering algorithm and a multi-objective evolutionary algorithm for mobile users. Experimental results demonstrate that even though the mobile users' preferences are significantly obfuscated, our method is effective in terms of recommendation accuracy and diversity.																	1758-0366	1758-0374					2020	16	1					23	32		10.1504/IJBIC.2020.108995													
J								Bio-inspired parameter-less heuristic for NP-hard (complete) discrete problems	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										NP-complete problem; bio-inspired heuristic; path-relinking; parameter-less heuristic	SIMULATED ANNEALING ALGORITHM; LEARNING-BASED OPTIMIZATION; GROUPING GENETIC ALGORITHM; CELL-FORMATION PROBLEM; SOLVE	In this article, a bio-inspired parameter-less heuristic employs a path-relinking approach coupled with a local search instead of moving alternatives within the search space. In addition, a mean solution has assured the exploration and exploitation phases. The proposed bio-inspired parameter-less heuristic has been compared to ZODIAC, GRAFICS, WFA-water flow, EA-evolutionary algorithm, GRASP, GATSP, HGA-hybrid genetic algorithm, GA-genetic algorithm, HGDE-differential evolution algorithm, ACO-ant colony optimisation, HGGA-grouping genetic algorithm, SACF-simulated annealing algorithm, GAA-genetic algorithm, SCF-BMCF-hybrid heuristic algorithm, EnGGA-enhanced grouping genetic algorithm, SA - simulated annealing, SA(YLL)- simulated annealing with variable on set of benchmarks. The comparative study shows that the proposed heuristic performs well on 26 benchmarks.																	1758-0366	1758-0374					2020	16	1					33	43		10.1504/IJBIC.2020.108998													
J								An effective user clustering-based collaborative filtering recommender system with grey wolf optimisation	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										recommender system; grey wolf optimisation; GWO; user clustering; venue recommendation; similarity; collaborative filtering	BAT ALGORITHM; ENSEMBLE	The enormous amount of data available today often makes it difficult for users to make decisions. Recommendation systems have become increasingly popular and mainly used in e-commerce to helping predict user preference towards particular items. The proposed system performs user cluster-based collaborative filtering for venue recommendations in which clusters are formed using a bio-inspired grey wolf optimisation algorithm. Clustering is used to eliminate the disadvantages of collaborative filtering regarding scalability, sparsity, and accuracy. In addition, we have used two similarity computation methods, namely the Pearson correlation coefficient (PCC) and cosine similarity to find the similarities between the set of users. The proposed recommendation system with the bio-inspired grey wolf optimisation algorithm has been evaluated on real-world massive volume datasets of Yelp and Trip Advisor for finding out the accuracy, precision, recall, and f-measure. We have also modelled and validated new mobile-based recommendation application frameworks for the development of urban venue recommendations in smart cities. The experimental and evaluation results demonstrate the usefulness of the newly generated recommendations and exhibit user satisfaction with the proposed recommendation technique.																	1758-0366	1758-0374					2020	16	1					44	55		10.1504/IJBIC.2020.108999													
J								Frequency-dependent synaptic plasticity model for neurocomputing applications	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										machine learning; neural networks; IoT devices; neural engineering; data classification; synaptic plasticity	NEURAL INFORMATION; SPIKING; SYNAPSES; NETWORKS; NEURONS; BURSTS; UNIT	In neuroscience, there is substantial evidence that suggests temporal filtering of stimulus by synaptic connections. In this paper, a novel frequency-dependent plasticity mechanism (FDSP) for neurocomputing applications is presented. It is proposed that synaptic junctions could be used to perform bandpass filtering on the input stimulus. The unique transfer function of a bandpass filter replaces the conventional weight value associated with synaptic connections. The proposed model has been simulated and rigorously tested with standard machine learning benchmarks such as XOR and multivariate IRIS dataset while utilising minimum resources. The proposed model offers a unique advantage and has the potential to overcome the burden of hidden layer neurons from the network. Exclusion of hidden layer from the network significantly reduces the size of the network and hence the computational effort required for classification tasks. The proposed FDSP mechanism allows for complete analogue system design with a frequency multiplexed communication scheme. The main goal of this study is to establish frequency-dependent plasticity as an alternative to existing time-domain-based techniques. The proposed method has a number of applications in neurocomputing, low power IoT devices and compute-efficient deep convolutional neural networks (DCNNs).																	1758-0366	1758-0374					2020	16	1					56	66		10.1504/IJBIC.2020.109001													
J								Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly	JOURNAL OF MACHINE LEARNING RESEARCH												Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly. github . io.																	1532-4435						2020	21																						
J								Research on the fault diagnosis method for high-speed loom using rough set and Bayesian network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										High-speed loom; fault diagnosis; rough set theory; Bayesian network	SAFETY	The textile industry has a long history and a large market scale around the world. High-speed loom belongs to the high-end production equipment of the textile industry with the characteristics of high precision, high speed and high efficiency. However, due to its expensive cost and complex structure, there might be significant loss once a high-speed loom breaks down. At present, the monitoring and troubleshooting of high-speed loom operation mainly depend on the experience of maintenance people to carry out inspections, which is inefficient, time-consuming, laborious and less efficient. In this paper, a fault diagnosis method for high-speed loom based on rough set and Bayesian network is investigated. Rough set theory is applied to reduce the attributes of fault causes and results and find the minimum reduction and classification rules. Then, a Bayesian fault diagnosis network model is built, and the probability of each fault cause is calculated to find the maximum probability. Finally, the diagnosis results are obtained. The experimental results have demonstrated the reliability and convenience of the faults diagnosis method for the high-speed loom.																	1064-1246	1875-8967					2020	39	1					1147	1161		10.3233/JIFS-192039													
J								Neighborhood preserving perceptual fidelity aware MSE for visual inspection of industrial flat surface products	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Automated visual inspection (AVI); perceptual fidelity aware mean squared error (PAMSE); extreme learning machine (ELM)	AUTOMATED DEFECT DETECTION; FABRICS; SYSTEM	Automated visual inspection is becoming an important field of computer vision in many industries. The real-time inspection of flat surface products is a task full of challenges in industrial aspects that requires fast and accurate algorithms for detection and localisation of defects. Structural, statistical and filter-based approaches, such as Gabor Filter Banks, Log-Gabor filter and Wavelets, have high computational complexity. This paper introduces a fast and accurate model for inspection and localization of industrial flat surface products: Neighborhood Preserving Perceptual Fidelity Aware Mean Squared Error (NP-PAMSE). The Extreme Learning Machine (ELM) is used for classification. ELM is found to be the perfect classifier for detecting defects. The proposed model resulted in defect detection accuracy of 99.86%, with 98.16% sensitivity, and 99.90% specificity. These results show that the proposed model outperforms many existing defect detection approaches. The discriminant power displays the efficiency of ELM in differentiation between normal and abnormal surfaces.																	1064-1246	1875-8967					2020	39	1					1183	1196		10.3233/JIFS-192071													
J								Centralized joint sparse representation for multi-view subspace clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Sparse representation; feature fusion; multi-view subspace clustering	LOW-RANK; FRAMEWORK; GRAPH	Multi-view subspace clustering arises in many computer visional tasks such as object recognition and image segmentation. The basic idea is to measure the same instance with multiple views. In this paper, we proposed two centralized joint sparse representation models, namely, Centralized Global Joint Sparse Representation (CGJSR) and Centralized Local Joint Sparse Representation (CLJSR) for multi-view subspace clustering. CGJSR and CLJSR force the concatenated representation matrix of all views and the representation matrix of each view to be sparse respectively. Both CGJSR and CLJSR allow the sparse coefficient matrix to approach a unified latent structure with an acceptable error. Noises and outliers regularization terms are included in CGJSR and CLJSR to reduce the influence of noises and outliers. Related optimization problems are solved using the alternating direction method of multipliers. Compared with seven state-of-the-art multi-view clustering algorithms, our proposed algorithms can achieve better or comparable results on four real-world datasets.																	1064-1246	1875-8967					2020	39	1					1213	1226		10.3233/JIFS-192101													
J								Some q-rung orthopair fuzzy hybrid aggregation operators and TOPSIS method for multi-attribute decision-making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										q-rung orthopair fuzzy numbers; q-rung orthopair fuzzy hybrid aggregation operators; multi-attribute decision-making; CNC machine	PYTHAGOREAN MEMBERSHIP GRADES; SOFT SETS; ROUGH SETS; ENVIRONMENT; VIEW	The q-rung orthopair fuzzy numbers (q-ROFNs) are used to deal with vague and uncertain information and they are superior to the intuitionistic fuzzy numbers (IFNs) and the Pythagorean fuzzy numbers (PFNs). In this paper, we introduce two operators namely q-rung orthopair fuzzy hybrid weighted arithmetic geometric aggregation (q-ROFHWAGA) operator and q-rung orthopair fuzzy hybrid ordered weighted arithmetic geometric aggregation (q-ROFHOWAGA) operator. The suggested operators q-ROFHWAGA and q-ROFHOWAGA are superior to the existing operators defined on q-ROFNs. We present an application of the proposed operator of q-ROFHWAGA to multiple-attribute decision-making (MADM) in computer numerical control (CNC) machine. Furthermore, we present TOPSIS method based on q-ROFNs for MADM in transport policy problem.																	1064-1246	1875-8967					2020	39	1					1227	1241		10.3233/JIFS-192114													
J								Large-scale global optimization based on hybrid swarm intelligence algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Global optimization; optimization problems; soft computing; evolutionary computing (EC); evolutionary algorithms (EAs); swarm intelligence based approaches and hybrid swarm intelligence algorithm	DIFFERENTIAL EVOLUTION ALGORITHM; BAT ALGORITHM; ENSEMBLE; MUTATION; SEARCH	There are numerous large-scale global optimization problems encountered in real-world applications including engineering, manufacturing, economics, networking fields. Over the last two decades different varieties of swarm intelligence and nature inspired based evolutionary algorithms (EAs) were developed and still. Among them, particles swarm optimization, Firefly algorithm, Ant colony optimization, Bat algorithm are the most popular and recently developed leading swarm intelligence based approaches. They are mainly inspired by the social and cooperative behaviors of swarm likewise herds of animals, flocking of birds, schooling of fish, ant colonies, herds of bisons and packs of wolves working together for their common benefit. Due to easy implementation and high capability in achieving of absolute optimum, swarm intelligence based algorithms have attained a great deal attention in both academic and industrial applications. This paper proposes a hybrid swarm intelligence (HSI) algorithm that employs the Bat Algorithm (BA) and the Practical Swarm Optimization (PSO) as constituents to perform their search process for dealing with recently designed benchmark functions in the special session of the 2017 IEEE congress of evolutionary computation (CEC'17) [3]. The approximate solutions for most of the CEC'17 benchmark functions obtained by the suggested algorithm in its twenty five independent runs of trails are much promising as compared to its competitors.																	1064-1246	1875-8967					2020	39	1					1257	1275		10.3233/JIFS-192162													
J								A novel lexicographic optimization method for solving shortest path problems with interval-valued triangular fuzzy arc weights	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Shortest path problem; interval-valued triangular fuzzy numbers; lexicographic optimization structure; multi objective linear programming; wireless sensor networks	ALGORITHM; NETWORK	Shortest path (SP) optimization problems arise in a wide range of applications such as telecommunications and transportation industries. The main purpose of these problems is to find a path between two predetermined nodes within a network as cheaply or quickly as possible. Conventional SP problems generally assume that the arc weights are defined by crisp variables, though imprecise data have been lately incorporated into the analysis. The present study formulates the SP problem in a directed interval-valued triangular fuzzy network. The resulting interval-valued fuzzy SP (IVFSP) problem is converted into a multi objective linear programming (MOLP) problem. Then, a lexicographic optimization structure is used to obtain the efficient solution of the resulting MOLP problem. The optimization process confirms that the optimum interval-valued fuzzy shortest path weight preserves the form of an interval-valued triangular fuzzy number. The applicability of the proposed approach is illustrated through an example dealing with wireless sensor networks.																	1064-1246	1875-8967					2020	39	1					1277	1287		10.3233/JIFS-192176													
J								Detection and severity of tumor cells by graded decision-making methods under fuzzy N-soft model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										N-soft sets; (F, N)-soft sets; graded TOPSIS; graded ELECTRE-I; decision-making	ELECTRE METHOD; TOPSIS; EXTENSION; OPERATORS; SETS; CHOICE	The notion of fuzzy N-soft sets is a hybrid model, which is a more generalized framework than fuzzy soft sets. To investigate the objects of a reference set in medical field, which have uncertainties in data, can be correctly captured by proposed structures of novel decision-making methods, graded TOPSIS and graded ELECTRE-I methods, based on fuzzy N-soft sets (henceforth, (F, N)-soft sets). Both the proposed methods compute the decision-maker estimations in a more flexile and affluent way, as well as improve the reliability of the decisions, that depends on star ratings or grades for the purpose of the modelization of decision-making problems in medical field. We show the importance and feasibility of proposed methods by applying them on real life example in medical field having ambiguities, that can be accurately occupied by this framework. Finally, we discuss the comparison analysis of both the proposed decision-making methods.																	1064-1246	1875-8967					2020	39	1					1303	1318		10.3233/JIFS-192203													
J								Recognition of the Parkinson's disease using a hybrid feature selection approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Relief; ant colony optimization; Parkinson's disease recognition; feature selection algorithm; classification; machine learning	OPTIMIZATION; DIAGNOSIS; SYSTEM; COLONY; ALGORITHMS; FRAMEWORK; MACHINE	Accurate and efficient recognition of Parkinson's disease is one of the prominent issues in the field of healthcare. To address this problem, different methods have been proposed in the literature. However, existing methods are lacking in accurately recognizing the Parkinson's disease and suffer from efficiency problems. To overcome these problems faced by existing models, this paper presents a machine-learning-based model for Parkinson's disease recognition. Specifically, a hybrid feature selection algorithm has been designed by integrating the Relief and ant-colony optimization algorithms to select relevant features for training the model. Moreover, the support vector machine has been trained and tested on the selected features to achieve optimal classification accuracy. Additionally, the K-fold cross-validation technique has been employed for the optimal hyper-parameters value evaluation of the model. The experimental results on a real-world dataset, i.e., Parkinson's disease dataset is revealed that the proposed system outperforms baseline competitors by accurately recognizing the Parkinson's disease and achieving 99.50% accuracy on the selected features. Due to high performance is achieved our proposed method, we are highly recommended for the recognition of PD.																	1064-1246	1875-8967					2020	39	1					1319	1339		10.3233/JIFS-200075													
J								An investigation of soft radicals	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Radical; nil radical; soft radical; soft ideal	SETS	Two important methods are used to transfer algebraic substructures to soft set theory. In the first method, the soft substructure of an algebraic structure is obtained, while in the second method a soft substructure of a soft algebraic structure is obtained. In this paper, we transfer the radical structure of an ideal to a soft set theory in a commutative ring and a semigroup by considering both methods.																	1064-1246	1875-8967					2020	39	1					1341	1346		10.3233/JIFS-200117													
J								The Z(L)-completions of fuzzy posets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy subset system; Fuzzy subset-hereditary; Delta(Gamma L)-completion; Delta(Gamma L)-continuous mapping; Delta(Gamma L)-continuously (sic)-existing; Z(Gamma L)-completion	COMPLETIONS	For a fuzzy subset system Z(L), the concepts of a Delta(Gamma L)-completion and a Z(Gamma L)-completion of a given fuzzy poset (X, e) are introduced and their universal properties are investigated. In this paper, we prove that: (1) the Delta(Gamma L)-completion Delta(Gamma L) (X) is a join-completion with the universal property; (2) the Z(Gamma L)-completion Z(Gamma L) (X) is the smallest Z(L)-complete fuzzy subposet of Delta(Gamma L) (X) in the case that Z(L) is fuzzy subset-hereditary. The results show that the Dedekind-MacNeille completion is a special case of the Z(Gamma L)-completion.																	1064-1246	1875-8967					2020	39	1					1347	1359		10.3233/JIFS-200121													
J								Folding theory applied to pseudo-hoops	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										n-fold pseudo-hoop; n-fold (positive) implicative filter; n-fold fantastic filter		In this paper, by considering the notion of pseudo-hoops, which introduced by Georgescu [10], we presented the concepts of n-fold filters in pseudo-hoop. Concerning ideas, we gave some related results. Also, we extended our definition to n-fold (positive) implicative and n-fold fantastic filters and investigated their properties and the relation among these n-fold filters. In particular, we proved that every n-fold fantastic and positive implicative filter is an n-fold implicative filter. Finally, we studied the quotient of these filters.																	1064-1246	1875-8967					2020	39	1					1381	1390		10.3233/JIFS-200179													
J								Investor sentiment index based on intuitionistic fuzzy analytic network process method and empirical analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy set; intuitionistic fuzzy analytic network process; principal component analysis; investor sentiment	DECISION-MAKING; HIERARCHY PROCESS; STOCK; INFORMATION; RETURNS; MODEL	This paper presents a network index system for assessing investor sentiment. The proposed comprehensive investor sentiment index is based on intuitionistic fuzzy analytic network process (IFANP) and regression model, and is compared with a sentiment index constructed on the basis of principal component analysis (PCA). The long-term relationship and dynamic relationship between the yields of these investor sentiment indexes and the Shanghai Composite Index (SHCI) are explored. Based on autoregressive moving average models and cointegration models, short-term and medium-term forecasts of the yields of investor sentiment index and SHCI are derived. The results of cointegration test, short-term forecasting and medium-term forecasting all show that the investor sentiment index based on IFANP is superior to that based on PCA.																	1064-1246	1875-8967					2020	39	1					19	34		10.3233/JIFS-190318													
J								New hybrid SPEA/R-deep learning to predict optimization parameters of cascade FOPID controller according engine speed in powertrain mount system control of half-car dynamic model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										SPEA/R algorithm; feed forward artificial neural network; magnetorheological MR; powertrain mounting system; FOPID controllers; PID controllers	PID CONTROLLER; ORDER; NETWORKS; DESIGN	In this article, a new methodology, hybrid genetic algorithm GA, algorithm SPEA/R with Deep Neural Network (HDNN&SPEA/R). This combination gave computing time much faster than computing time when using genetic algorithms SPEA/R. On the other hand, this combination also significantly reduces the number of samples needed for the training of deep artificial neural networks. This is the task of finding out an optimal set that changes with the engine velocity of multi-objective optimization involving 12 simultaneous optimization goals: proportional P, integral I, derivative D, additional integration n and differentiation orders m factor, displacement amplification coefficient K-Dloop, acceleration amplification coefficient K-Aloop in two controllers acceleration and displacement to enhance the ride comfort. This article has provided a control algorithm of a Cascade FOPID controller to control the acceleration and displacement of the mount. Besides, the article also offers solutions to optimize the 12 simultaneous parameters of the two controllers by the new hybrid method HDNN&SPEA/R and suitable for the speed of rotation of the engine. To increase the safety factor in operation, we use magnetorheological dampers (MR) in a powertrain mounting system and a continuous state damper controller that calculates the input voltage to the damper coil. The results of this control method are compared with traditional PID systems, optimal PID parameter adjustment using genetic algorithms (GA) and passive drive system mounts. The results are tested in both time and frequency domains, to verify the success of the proposed Cascade FOPID algorithm. The results show that the proposed Cascade FOPID controller of the MR engine mounting system gives very good results in comfort and softness when riding compared to other controllers. This proposal has reduced 335 hours for optimal computation time and reduce vibration a lot.																	1064-1246	1875-8967					2020	39	1					53	68		10.3233/JIFS-190586													
J								A real-time multi-constraints obstacle avoidance method using LiDAR	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Real-time obstacle avoidance; LiDAR; online path planning; multi-constraints; mobile robot	POTENTIAL-FIELD METHOD; SIMULTANEOUS LOCALIZATION; MOBILE ROBOTS; PERCEPTION; NAVIGATION	Obstacle avoidance is one of the essential and indispensable functions for autonomous mobile robots. Most of the existing solutions are typically based on single condition constraint and cannot incorporate sensor data in a real-time manner, which often fail to respond to unexpected moving obstacles in dynamic unknown environments. In this paper, a novel real-time multi-constraints obstacle avoidance method using Light Detection and Ranging(LiDAR) is proposed, which is able to, based on the latest estimation of the robot pose and environment, find the sub-goal defined by a multi-constraints function within the explored region and plan a corresponding optimal trajectory at each time step iteratively, so that the robot approaches the goal over time. Meanwhile, at each time step, the improved Ant Colony Optimization(ACO) algorithm is also used to re-plan optimal paths from the latest robot pose to the latest defined sub-goal position. While ensuring convergence, planning in this method is done by repeated local optimizations, so that the latest sensor data from LiDAR and derived environment information can be fully utilized at each step until the robot reaches the desired position. This method facilitates real-time performance, also has little requirement on memory space or computational power due to its nature, thus our method has huge potentials to benefit small low-cost autonomous platforms. The method is evaluated against several existing technologies in both simulation and real-world experiments.																	1064-1246	1875-8967					2020	39	1					119	131		10.3233/JIFS-190766													
J								Variational inequalities for lattice-valued fuzzy relations with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										L-fuzzy relations; fixed points; variational inequalities; iterative algorithm	DIFFERENTIAL-INCLUSIONS; POINT; EXISTENCE; SETS	In this article, we study the notion of the variational inequalities for lattice-valued fuzzy relations. In this context, a variational inequality problem has been proposed that generalizes many results in the literature. The conditions for the existence of solutions of the proposed problem have been discussed. It has been shown that the proposed variational inequality problem is equivalent to a fixed point problem. This fixed point formulation allows us to present an iterative algorithm to approximate solution of the variational inequality problem. For applications, first the existence result for the solutions of an L-fuzzy Caputo-Fabrizio fractional differential inclusion initial value problem involving a projection operator has been proved. Then the solutions of an obstacle boundary value variational inequality problem in function spaces has been obtained.																	1064-1246	1875-8967					2020	39	1					145	153		10.3233/JIFS-190894													
J								Deep neural network as deep feature learner	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image processing; relative features; deep learning; deep features	OBJECT DETECTION; CLASSIFICATION; ATTRIBUTES; RETRIEVAL; MODEL	Features play an important role in image processing. But as not all features are comparable, relative features emerged. From the beginning, low-level features, extracted by experts, have been employed to create difficult models for learning the problem of relative attribute. Knowing these models are limited in generality of their applicability, deep learning models can be employed instead of them. A deep artificial neural network framework has been suggested for the task of relative attribute prediction in this article. The paper suggests to use a convolutional artificial neural network for learning the mentioned attributes through a peripheral auxiliary layer, called also a ranking layer, which is able to learn how to rank the images. A suitable ranking cost function is used to train the whole network in an end-to-end manner. The suggested method through this paper is experimentally superior to the state of the art methods on some well-known benchmarks. The experimental results indicate that the proposed method is capable of learning the problem of relative attribute.																	1064-1246	1875-8967					2020	39	1					355	369		10.3233/JIFS-191292													
J								On bipolar complex fuzzy sets and its application	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bipolar fuzzy set; bipolar complex fuzzy distance; bipolar fuzzy complement; union and intersection; fuzzy set; complex fuzzy set	EQUILIBRIUM RELATIONS; GRAPH REPRESENTATION; SOFT SETS; LOGIC	Many factors with the perspective of bipolarity in the traditional Chinese food system "Yin and Yang food system" manipulate with types of food simultaneously to have a balanced body. This research studies the multiple attributes decision making (MADM) problem that measuring the "bipolarity of periodic" variation in bipolar information with an illustration example in order to find an optimal nutrition program for a person X. To convey this type of data to a mathematical formula and vice versa without losing the full meaning of human knowledge, we use bipolar fuzzy set in a complex geometry by extending the range of bipolar fuzzy set to the realm of a complex number. This extension needs to be successful to study and introduce intensely a new mathematical structure called a bipolar complex fuzzy set (BCFS) with its properties. Ranges of values are extended to [0, 1]e(i alpha[0,1]) and [-1, 0]e(i alpha[-1,0]) for both positive and negative membership functions, respectively, as a replacement for [-1, 0] x [0, 1], as in the bipolar fuzzy set. The main benefit of BCFS that the amplitude and phase terms of BCFSs can convey bipolar fuzzy information. Moreover, the formal definition of BCF distance measure and illustration application are introduced. Some basic mathematical operations on BCFS are also proposed and study its properties with arithmetical examples.																	1064-1246	1875-8967					2020	39	1					383	397		10.3233/JIFS-191350													
J								A comprehensive approach to avoid node selfishness and data redundancy	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										DTN; buffer management; incentive mechanism; Direct Diffusion; SERF	DTN	Delay Tolerant Network (DTN) is an array of protocols which work together to enable a standardized way to communicate between nodes is store-carry -and - forward approach. In DTN, nodes help to convey message even with connectivity problem. Instead of using the own resource to pass a message to other nodes, a relay node is preferred which discovers the sink node with a greater probability of transferring messages on its own. A condition named node's selfishness is thus raised and it surely degrades network performance, which is more obvious in urban environment. To handle such problem, an incentive-based routing algorithm is generated by using the concept Selfishness-Enhanced Reliable Forwarding (SERF) to remove node selfishness and Direct Diffusion to omit duplicate messages present in relay node. This algorithm finds the probability of messages received by the relay node with reference to the usage of resources of sender node. Concurrently, buffer management policy is maintained, by setting the threshold of message copies according to the resource consumption of the source node when it generates a message. The result reveals that the proposed algorithm is better to the existing algorithms with regard to the performance metrics such as, delivery ratio, the average delay, and network overhead.																	1064-1246	1875-8967					2020	39	1					407	419		10.3233/JIFS-191409													
J								Extreme fuzzy ideals and its applications on De Morgan residuated lattices	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										De Morgan residuated lattice; obstinate ideal; extreme fuzzy ideal	FILTERS	The variety of De Morgan residuated lattices includes important subvarieties of residuated lattices such as Boolean algebras, MV-algebras, BL-algebras, Stonean residuated lattices, MTL-algebras and involution residuated lattices (see L.C. Holdon [7]). X. Zhu, J. Yang and A. Borumand Saeid [16] used a special family of extreme fuzzy filters F on a BL-algebra L, they constructed a uniform structure (L, K) , and then the part K induced a uniform topology tau(F) in L. Also, they proved that the pair (L, tau(F)) is a topological BL-algebra, and some properties of (L, tau(F)) were investigated. Inspired by their study, in this paper, we define the family of extreme fuzzy ideals I on a De Morgan residuated lattice L, we construct a uniform structure (L, K) , and then the part K induce a uniform topology tau(I) in L. We prove that the pair (L, tau(I)) is a topological De Morgan residuated lattice, and some properties of (L, tau(I)) are investigated. In particular, we show that (L, tau(I)) is a first-countable, zero-dimensional, disconnected and completely regular space. Finally, we give some characterizations of topological properties of (L, tau(I)) . We note that, since ideals and filters are dual in BL-algebras (see C. Lele and J. B. Nganou [12]), a study on extreme fuzzy ideals in BL-algebras follows by duality, but in the framework of De Morgan residuated lattices, which is a larger class than BL-algebras, the duality between ideals and filters does not hold, so the study of extreme fuzzy ideals in De Morgan residuated lattices becomes interesting from algebraic and topological point of view, and the results of X. Zhu, J. Yang and A. Borumand Saeid [16] become particular cases of our theory.																	1064-1246	1875-8967					2020	39	1					449	461		10.3233/JIFS-191474													
J								Fuzzy measure of non-compactness with applications in fractional anti-periodic boundary value problems involving nonsingular kernel	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										fuzzy measure of noncompactness; Szulfa's theorem; K-Fan theorem; multivalued fuzzy condensing operators; multivalued fuzzy k-set contractions; Krasnoselskii's theorem; fixed points	CAUCHY-PROBLEM; EQUATIONS; UNIQUENESS; EXISTENCE	In this article we introduce the notion of fuzzy measure of noncompactness. We define the fuzzy condensing and fuzzy k-set contractions using fuzzy measure of noncompactness. An extension of Stanislaw Szulfa's fixed point theorem for a self-operator on a closed bounded and convex subset of a Banach space has been proved. Using the defined multivalued k-set contractions, generalizations of Kakutani-Fan and Krasnoselskii type theorems have been proved. For applications the existence result for solutions of a fractional Caputo-Fabrizio anti-periodic boundary value problem has been proved. We give some examples to validate our results.																	1064-1246	1875-8967					2020	39	1					463	474		10.3233/JIFS-191496													
J								Actionable knowledge discovery from social networks using causal structures of structural features	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Actionable knowledge discovery; action extraction; causal network; feature extraction		Knowledge discovery and data mining provide an array of solutions for real-world problems. When facing business requirements, the ultimate goal of knowledge discovery is not the knowledge itself but rather making the gained knowledge practical. Consequently, the models and patterns found by the mining methods often require post-processing. To this end, actionable knowledge discovery has been introduced which is developed to extract actionable knowledge from data. The output of actionable knowledge discovery is a set of actions that help the domain expert to gain the desired outcome. Such a process where a set of actions are extracted is called action extraction. One of the challenges of action extraction is to incorporate causal dependencies among the variables to find actions with higher effectiveness compared to when no such dependencies are used. The goal of this paper is to dive into the lesser studied subject of "action discovery in social networks" and intends to extract actions by utilizing the casual structures discovered from such data. Furthermore, in order to capture the underlying information within a social network, we extract the corresponding structural features. We propose a method called SF-ICE-CREAM (Social Features included Inductive Causation Enabled Causal Relationship-based Economical Action Mining) to overcome the challenges introduced above. This method uses structural features to find the underlying causal structures within a social network and incorporates them into the action extraction process.																	1064-1246	1875-8967					2020	39	1					489	501		10.3233/JIFS-191519													
J								A closed loop supply chain network design problem with multi-mode demand satisfaction in fuzzy environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Closed loop supply chain; fuzzy set theory; multi-objective optimization; credibility constrained modeling	POSSIBILISTIC PROGRAMMING APPROACH; GENETIC ALGORITHM; OPTIMIZATION; HYBRID; MODEL; UNCERTAINTY; PERFORMANCE; INVENTORY; POLICIES	A new closed loop supply chain network design problem is considered in this study. In comparison to the literature, the problem considers a more complete set of stages in the network e.g. simultaneous consideration of supplier, plant, distribution center, customer, and collection, recovering, recycling, and disposal centers. The problem makes location, capacity, allocation, demand mode and material/product flow decisions for optimizing net benefit including sales revenue, fixed establishing cost of facilities, transportation cost, material purchasing cost, production cost, and inventory holding cost. As a novelty, for the first time multi-mode demand satisfaction is considered in a closed loop supply chain problem. As another contribution, in order to be close to real-world situations, the problem is tackled in a fuzzy environment by using trapezoidal fuzzy parameters which yield a trapezoidal fuzzy objective function value. As solution methodology, considering the fuzzy objective function, the problem is reformulated as a multi-objective fuzzy mixed integer linear problem and is crisped using credibility measure of the fuzzy constraints. Finally, the crisp multi-objective version of the problem is solved by several hybrid fuzzy programming approaches to obtain a good efficient solution. Applying several numerical test problems, the SO method performs better than other approaches.																	1064-1246	1875-8967					2020	39	1					503	524		10.3233/JIFS-191528													
J								Normal wiggly hesitant fuzzy TODIM approach for multiple attribute decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Normal wiggly hesitant fuzzy sets; multiple attribute decision making; TODIM	SUPPLIER SELECTION; AGGREGATION; INFORMATION; OPERATORS; MODEL	A normal wiggly hesitant fuzzy set (NWHFS) is a powerful and useful tool to dig the potential indeterminacy of decision makers (DMs) in the process of expressing their preferences, which can be considered as an extended form of the traditional hesitant fuzzy set (HFS). The NWHFSs can not only retain the original hesitant fuzzy information completely, but also explore potential uncertainty of theses information. TODIM is an effective method to capture the psychological behavior based on prospect theory. Considering the advantages of NWHFS and TODIM method, in this paper, we define the distance measure of any two normal wiggly hesitant fuzzy elements (NWHFEs), and put forward an extended normal wiggly hesitant fuzzy TODIM (NWHF-TODIM) approach to handle multiple attribute decision making (MADM) problems with normal wiggly hesitant fuzzy (NWHF) information. Then we use the extended NWHF-TODIM method to rank alternatives and select an ideal one. Lastly, we compare it with two existing approaches to verify the rationality and validity of the proposed approach.																	1064-1246	1875-8967					2020	39	1					627	644		10.3233/JIFS-191569													
J								Assessment of effectiveness of data dependent activation method: MyAct	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial neural network; activation function; feedforward neural network; deep learning; link prediction; machine learning	NETWORK	One of the momentous transformation performed by an artificial neural network (ANN), Support Vector Machine (SVM), Radial basis Function (RBF) and many other machine learning method is the application of activation function. MyAct the proposed activation method is used here with various ANN architectures for link prediction, classification and general prediction. Statistical properties of data used here to prove the effectiveness of proposed activation function MyAct over other popular activation methods. A data dependent transfer method is developed, which is pioneer in its own way. This proves to be an unified formulation for the robust and generalised learning for the classification, link prediction and regression problem types. Classification is done with Iris dataset using ANN with different activation method and results are compared. Improved results are achieved when MyAct used with Tailored Deep Feed Forward Artificial Neural Network (TDFFANN), simple Artificial Neural Network and Deep Artificial Neural Network. Aim here is to develop a novel activation method which work with positive data, negative data, small size data, big size data, skewed data or corrupt data. An attempt is made to cover complete versatile behaviour of data. Currently not a single activation method can work well on all above mentioned data. Results obtained using MyAct on the datasets used here proves it to be a good choice in comparison to logsig, tansig and other popular activation methods for classification and link prediction. Satisfactory improvement is achieved by using data length as well as negative range values in the prediction done by proposed method. MyAct had 22% better standard deviation than ReLU (Rectified Linear unit) and 36.28% better standard deviation than ELU (Exponential linear unit). MyAct has 2.6% better accuracy in regression error than Swiss method and 2.5% better accuracy in regression error than ELU. Other results are discussed in the paper.																	1064-1246	1875-8967					2020	39	1					665	677		10.3233/JIFS-191618													
J								Co-design method for H-infinity control of quantized T-S fuzzy system over the networked system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Communication-delay; lyapunov-krasovskii functionals; quantizer; co-design method	EVENT-TRIGGERED COMMUNICATION; MARKOVIAN JUMP SYSTEMS; TIME-VARYING DELAY; NONLINEAR-SYSTEMS; TRACKING CONTROL; STABILITY; SYNCHRONIZATION	This paper deals with the problem of quantized state feedback H-infinity control for T-S fuzzy systems with appearing the communication delay under stochastic nonlinearity. To accomplish the objective a uniform framework for effective bandwidth utilization is employed to design co-design method. First of all, the co-design method is proposed such that the data can be communicated according to some logic function. Then, we implemented the measurement size-reduction scheme, using the logarithmic quantization. Additionally, we provided the impact of co-design method and quantization, on the original model of networked control systems (NCSs) is redeveloped as a new structure of hybrid-triggered NCSs with network induced delay. Moreover, Lyapunov-Krasovskii functional is considered to grantee the closed-loop for stochastic stability analysis of the T-S fuzzy system. The solvability of Lyapunov-Krasovskii functional results in the formation of Linear matrix inequalities. The solution of Linear matrix inequalities leads to the controller gains to perform simulations to validate the proposed scheme.																	1064-1246	1875-8967					2020	39	1					771	788		10.3233/JIFS-191708													
J								Multi-type resources collaborative scheduling in automated warehouse with fuzzy processing time	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Automated warehouse; fuzzy processing time; collaborative scheduling; genetic algorithm; capability coordination degree	GENETIC ALGORITHM; FLOW-SHOP; OPTIMIZATION; CRANES	The efficient operation of Intelligent Warehousing System does not rely on individual resource scheduling in stages but multi-type resources collaborative scheduling. In this paper, a collaborative scheduling model for stackers, automated guided vehicles and picking workstations in outbound process is abstracted into a hybrid flow-shop scheduling problem within an automated warehouse scene. Considering the impacts of uncertain factors related to scheduling, the objective function of this model is minimizing the makespan based on the triangular fuzzy processing time. A genetic algorithm is designed to obtain feasible solution of this model with the form of vector coding and the approach of ranking fuzzy numbers. Example analysis shows that the validity of the model and algorithm is verified. Within different resource allocation schemes, their evaluating indexes are significantly different, which are the likely completion time of system operation, the capability coordination degree and the initial investment. Furthermore, the increase of picking workstations is contributed much more to reducing the likely completion time and to improving the capability coordination degree than that of automated guided vehicles.																	1064-1246	1875-8967					2020	39	1					899	910		10.3233/JIFS-191827													
J								Hierarchical interval type-2 fuzzy path planning based on genetic optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mobile robot; path planning; interval type-2 fuzzy; hierarchical fuzzy; genetic optimization	MOBILE ROBOT NAVIGATION; SYSTEMS; DESIGN; LOGIC; STABILIZATION; CONTROLLER	This paper focuses on the path planning of mobile robot. Fuzzy logic is employed to deal with the uncertainty in the process of path planning. The hierarchical interval type-2 fuzzy method is obtained by combining the hierarchical fuzzy and interval type-2 fuzzy method, which is used in the path planning of mobile robot. Hierarchical fuzzy structure can simplify complex system and get fuzzy rules more easily. For multi input system, it can also solve the problem of rule explosion. Compared with type-1 fuzzy, interval type-2 fuzzy can better deal with the uncertainty in the process of path planning. Finally, in order to get a better path, genetic algorithm is used to optimize the membership function in the fuzzy path planner. Through the simulation experiment, the proposed hierarchical type-2 fuzzy planning method can effectively solve the path planning problem. Compared with the type-1 fuzzy method, the interval type-2 fuzzy method and the hierarchical type-1 fuzzy method, the proposed method obtains better results.																	1064-1246	1875-8967					2020	39	1					937	948		10.3233/JIFS-191864													
J								Light weight convolutional models with spiking neural network based human action recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										MobileNet; PWCNet; BindsNet; diehl and cook nodes; spiking neural network	FEATURES	Though deep learning networks have proven ability to perform video analytics in complex environments, there is an increased attention towards the development of compact networks which would facilitate edge processing and the result of which have yielded high performance compressed deep learning networks such as, MobileNet, PWCNet and BindsNet. In the work proposed herein, a dual network configuration is used for human action recognition, wherein, the MobileNet captures the spatial appearance of the action sequences and the PWCNet is used to extract the motion vectors. A novel Spiking Neural Network (SNN) based configuration is used as the classifier and the SNN implementation is based on BindsNet. The proposed configuration is experimentally validated on challenging datasets, viz., HMDB51 and UCF101. The experimental results demonstrate that the proposed work is superior to the state-of-the-art techniques and comparable in few cases.																	1064-1246	1875-8967					2020	39	1					961	973															
J								Facial Expression Recognition in Videos Using Dynamic Kernels	IEEE TRANSACTIONS ON IMAGE PROCESSING										Kernel; Hidden Markov models; Face recognition; Face; Feature extraction; Videos; Gaussian mixture model; Expression recognition; feature extraction; universal attribute model; MAP adaptation; factor analysis; Gaussian mixture model; fisher kernel; supervector kernel; mean interval kernel; intermediate matching kernel	INTERMEDIATE MATCHING KERNEL; 3D	Recognition of facial expressions across various actors, contexts, and recording conditions in real-world videos involves identifying local facial movements. Hence, it is important to discover the formation of expressions from local representations captured from different parts of the face. So in this paper, we propose a dynamic kernel-based representation for facial expressions that assimilates facial movements captured using local spatio-temporal representations in a large universal Gaussian mixture model (uGMM). These dynamic kernels are used to preserve local similarities while handling global context changes for the same expression by utilizing the statistics of uGMM. We demonstrate the efficacy of dynamic kernel representation using three different dynamic kernels, namely, explicit mapping based, probability-based, and matching-based, on three standard facial expression datasets, namely, MMI, AFEW, and BP4D. Our evaluations show that probability-based kernels are the most discriminative among the dynamic kernels. However, in terms of computational complexity, intermediate matching kernels are more efficient as compared to the other two representations.																	1057-7149	1941-0042					2020	29						8316	8325		10.1109/TIP.2020.3011846													
J								MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object segmentation; Spatiotemporal phenomena; Visualization; Motion segmentation; Decoding; Task analysis; Machine learning; Video object segmentation; zero-shot; twostream; spatiotemporal representation; neural attention; dynamic visual attention prediction	MODEL	In this paper, we present a novel end-to-end learning neural network, i.e., MATNet, for zero-shot video object segmentation (ZVOS). Motivated by the human visual attention behavior, MATNet leverages motion cues as a bottom-up signal to guide the perception of object appearance. To achieve this, an asymmetric attention block, named Motion-Attentive Transition (MAT), is proposed within a two-stream encoder network to firstly identify moving regions and then attend appearance learning to capture the full extent of objects. Putting MATs in different convolutional layers, our encoder becomes deeply interleaved, allowing for close hierarchical interactions between object apperance and motion. Such a biologically-inspired design is proven to be superb to conventional two-stream structures, which treat motion and appearance independently in separate streams and often suffer severe overfitting to object appearance. Moreover, we introduce a bridge network to modulate multi-scale spatiotemporal features into more compact, discriminative and scale-sensitive representations, which are subsequently fed into a boundary-aware decoder network to produce accurate segmentation with crisp boundaries. We perform extensive quantitative and qualitative experiments on four challenging public benchmarks, i.e., DAVIS(16), DAVIS(17), FBMS and YouTube-Objects. Results show that our method achieves compelling performance against current state-of-the-art ZVOS methods. To further demonstrate the generalization ability of our spatiotemporal learning framework, we extend MATNet to another relevant task: dynamic visual attention prediction (DVAP). The experiments on two popular datasets (i.e., Hollywood-2 and UCF-Sports) further verify the superiority of our model (our code is available at https://github.com/tfzhou/MATNet).																	1057-7149	1941-0042					2020	29						8326	8338		10.1109/TIP.2020.3013162													
J								Collaborative Filtering of Correlated Noise: Exact Transform-Domain Variance for Improved Shrinkage and Patch Matching	IEEE TRANSACTIONS ON IMAGE PROCESSING										Transforms; Noise reduction; Collaboration; Kernel; Correlation; Two dimensional displays; Three-dimensional displays; Image denoising; colored noise; correlated noise; collaborative filtering; BM3D	ROBUST ESTIMATION; POWER SPECTRUM; IMAGE; DEBLOCKING; QUALITY; SIGNAL	Collaborative filters perform denoising through transform-domain shrinkage of a group of similar patches extracted from an image. Existing collaborative filters of stationary correlated noise have all used simple approximations of the transform noise power spectrum adopted from methods which do not employ patch grouping and instead operate on a single patch. We note the inaccuracies of these approximations and introduce a method for the exact computation of the noise power spectrum. Unlike earlier methods, the calculated noise variances are exact even when noise in one patch is correlated with noise in any of the other patches. We discuss the adoption of the exact noise power spectrum within shrinkage, in similarity testing (patch matching), and in aggregation. We also introduce effective approximations of the spectrum for faster computation. Extensive experiments support the proposed method over earlier crude approximations used by image denoising filters such as Block-Matching and 3D-filtering (BM3D), demonstrating dramatic improvement in many challenging conditions.																	1057-7149	1941-0042					2020	29						8339	8354		10.1109/TIP.2020.3014721													
J								Visual Tracking With Multiview Trajectory Prediction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Target tracking; Cameras; Correlation; Trajectory; Robustness; Visualization; Deep learning; multiview; tracking; correlation filter; trajectory	OBJECT TRACKING	Recent progresses in visual tracking have greatly improved the tracking performance. However, challenges such as occlusion and view change remain obstacles in real world deployment. A natural solution to these challenges is to use multiple cameras with multiview inputs, though existing systems are mostly limited to specific targets (e.g. human), static cameras, and/or require camera calibration. To break through these limitations, we propose a generic multiview tracking (GMT) framework that allows camera movement, while requiring neither specific object model nor camera calibration. A key innovation in our framework is a cross-camera trajectory prediction network (TPN), which implicitly and dynamically encodes camera geometric relations, and hence addresses missing target issues such as occlusion. Moreover, during tracking, we assemble information across different cameras to dynamically update a novel collaborative correlation filter (CCF), which is shared among cameras to achieve robustness against view change. The two components are integrated into a correlation filter tracking framework, where features are trained offline using existing single view tracking datasets. For evaluation, we first contribute a new generic multiview tracking dataset (GMTD) with careful annotations, and then run experiments on the GMTD and CAMPUS datasets. The proposed GMT algorithm shows clear advantages in terms of robustness over state-of-the-art ones.																	1057-7149	1941-0042					2020	29						8355	8367		10.1109/TIP.2020.3014952													
J								s-LWSR: Super Lightweight Super-Resolution Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Computational modeling; Image resolution; Computer architecture; Biological system modeling; Image coding; Mobile handsets; Convolution; Super-resolution; lightweight model; multilevel information; model compression; activation operation removal	IMAGE SUPERRESOLUTION	In recent years, deep models have achieved great success in the field of single-image super-resolution (SISR) by incorporating a large number of parameters to obtain satisfactory performance. However, this achievement typically gives rise to high computational complexity, which greatly restricts deep SISR applications in deployment on mobile devices with limited computation and storage resources. To address this problem, in this article, we propose a flexibly adjustable super-lightweight SISR pipeline: s-LWSR. First, to efficiently abstract features from low-resolution images, we design a highly efficient U-shaped backbone, along with an information pool, which is constructed to mix multilevel information from the first half of our pipeline. Second, a compression mechanism based on depthwise-separable convolution is employed to further reduce the number of parameters with a negligible degradation in performance. Third, by revealing the specific role of activation in many deep models, we remove several activation layers in our super-resolution (SR) model to retain useful information, leading to a further improvement in the final performance. Extensive experiments demonstrate that our s-LWSR, with limited parameters and operations, can achieve a similar performance to that of other cumbersome but state-of-the-art (SOTA) deep SR methods.																	1057-7149	1941-0042					2020	29						8368	8380		10.1109/TIP.2020.3014953													
J								Hamming Embedding Sensitivity Guided Fusion Network for 3D Shape Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Feature extraction; Shape; Data mining; Task analysis; Sensitivity; Convolution; 3D Multimodal fusion; mesh; multi-view; hamming space		Three-dimensional multi-modal data are used to represent 3D objects in the real world in different ways. Features separately extracted from multimodality data are often poorly correlated. Recent solutions leveraging the attention mechanism to learn a joint-network for the fusion of multimodality features have weak generalization capability. In this paper, we propose a hamming embedding sensitivity network to address the problem of effectively fusing multimodality features. The proposed network called HamNet is the first end-to-end framework with the capacity to theoretically integrate data from all modalities with a unified architecture for 3D shape representation, which can be used for 3D shape retrieval and recognition. HamNet uses the feature concealment module to achieve effective deep feature fusion. The basic idea of the concealment module is to re-weight the features from each modality at an early stage with the hamming embedding of these modalities. The hamming embedding also provides an effective solution for fast retrieval tasks on a large scale dataset. We have evaluated the proposed method on the large-scale ModelNet40 dataset for the tasks of 3D shape classification, single modality and cross-modality retrieval. Comprehensive experiments and comparisons with state-of-the-art methods demonstrate that the proposed approach can achieve superior performance.																	1057-7149	1941-0042					2020	29						8381	8390		10.1109/TIP.2020.3013138													
J								Unsupervised Deep Cross-modality Spectral Hashing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Binary codes; Semantics; Optimization; Correlation; Sparse matrices; Task analysis; Training data; Cross-modal retrieval; spectral hashing; image search; constraint optimization	BINARY-CODES; SIMILARITY; QUANTIZATION; RETRIEVAL	This paper presents a novel framework, namely Deep Cross-modality Spectral Hashing (DCSH), to tackle the unsupervised learning problem of binary hash codes for efficient cross-modal retrieval. The framework is a two-step hashing approach which decouples the optimization into (1) binary optimization and (2) hashing function learning. In the first step, we propose a novel spectral embedding-based algorithm to simultaneously learn single-modality and binary cross-modality representations. While the former is capable of well preserving the local structure of each modality, the latter reveals the hidden patterns from all modalities. In the second step, to learn mapping functions from informative data inputs (images and word embeddings) to binary codes obtained from the first step, we leverage the powerful CNN for images and propose a CNN-based deep architecture to learn text modality. Quantitative evaluations on three standard benchmark datasets demonstrate that the proposed DCSH method consistently outperforms other state-of-the-art methods.																	1057-7149	1941-0042					2020	29						8391	8406		10.1109/TIP.2020.3014727													
J								RGBD Salient Object Detection via Disentangled Cross-Modal Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; Feature extraction; Object detection; Topology; Image color analysis; Machine learning; Diversity reception; Disentangle; RGBD; saliency detection	NETWORK; MODEL	Depth is beneficial for salient object detection (SOD) for its additional saliency cues. Existing RGBD SOD methods focus on tailoring complicated cross-modal fusion topologies, which although achieve encouraging performance, are with a high risk of over-fitting and ambiguous in studying cross-modal complementarity. Different from these conventional approaches combining cross-modal features entirely without differentiating, we concentrate our attention on decoupling the diverse cross-modal complements to simplify the fusion process and enhance the fusion sufficiency. We argue that if cross-modal heterogeneous representations can be disentangled explicitly, the cross-modal fusion process can hold less uncertainty, while enjoying better adaptability. To this end, we design a disentangled cross-modal fusion network to expose structural and content representations from both modalities by cross-modal reconstruction. For different scenes, the disentangled representations allow the fusion module to easily identify and incorporate desired complements for informative multi-modal fusion. Extensive experiments show the effectiveness of our designs and a large outperformance over state-of-the-art methods.																	1057-7149	1941-0042					2020	29						8407	8416		10.1109/TIP.2020.3014734													
J								Hierarchical U-Shape Attention Network for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; Fuses; Training; Feature extraction; Shape; Memory management; Heating systems; Salient object detection; convolutional neural network; attention regularization		Salient object detection aims at locating the most conspicuous objects in natural images, which usually acts as a very important pre-processing procedure in many computer vision tasks. In this paper, we propose a simple yet effective Hierarchical U-shape Attention Network (HUAN) to learn a robust mapping function for salient object detection. Firstly, a novel attention mechanism is formulated to improve the well-known U-shape network, in which the memory consumption can be extensively reduced and the mask quality can be significantly improved by the resulting U-shape Attention Network (UAN). Secondly, a novel hierarchical structure is constructed to well bridge the low-level and high-level feature representations between different UANs, in which both the intra-network and inter-network connections are considered to explore the salient patterns from a local to global view. Thirdly, a novel Mask Fusion Network (MFN) is designed to fuse the intermediate prediction results, so as to generate a salient mask which is in higher-quality than any of those inputs. Our HUAN can be trained together with any backbone network in an end-to-end manner, and high-quality masks can be finally learned to represent the salient objects. Extensive experimental results on several benchmark datasets show that our method significantly outperforms most of the state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						8417	8428		10.1109/TIP.2020.3011554													
J								Unsupervised Learning of Optical Flow With CNN-Based Non-Local Filtering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Optical imaging; Optical computing; Optical losses; Optical variables control; Estimation; Optical sensors; Optical filters; Optical flow; unsupervised learning; non-local term; loss function; occlusion map		Estimating optical flow from successive video frames is one of the fundamental problems in computer vision and image processing. In the era of deep learning, many methods have been proposed to use convolutional neural networks (CNNs) for optical flow estimation in an unsupervised manner. However, the performance of unsupervised optical flow approaches is still unsatisfactory and often lagging far behind their supervised counterparts, primarily due to over-smoothing across motion boundaries and occlusion. To address these issues, in this paper, we propose a novel method with a new post-processing term and an effective loss function to estimate optical flow in an unsupervised, end-to-end learning manner. Specifically, we first exploit a CNN-based non-local term to refine the estimated optical flow by removing noise and decreasing blur around motion boundaries. This is implemented via automatically learning weights of dependencies over a large spatial neighborhood. Because of its learning ability, the method is effective for various complicated image sequences. Secondly, to reduce the influence of occlusion, a symmetrical energy formulation is introduced to detect the occlusion map from refined bi-directional optical flows. Then the occlusion map is integrated to the loss function. Extensive experiments are conducted on challenging datasets, i.e. FlyingChairs, MPI-Sintel and KITTI to evaluate the performance of the proposed method. The state-of-the-art results demonstrate the effectiveness of our proposed method.																	1057-7149	1941-0042					2020	29						8429	8442		10.1109/TIP.2020.3013168													
J								Multimodal Deep Unfolding for Guided Image Super-Resolution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image resolution; Machine learning; Convolutional codes; Image coding; Neural networks; Imaging; Image reconstruction; Multimodal deep unfolding; multimodal image super-resolution; interpretable convolutional neural networks	LINEAR INVERSE PROBLEMS; REPRESENTATION; NETWORKS	The reconstruction of a high resolution image given a low resolution observation is an ill-posed inverse problem in imaging. Deep learning methods rely on training data to learn an end-to-end mapping from a low-resolution input to a high-resolution output. Unlike existing deep multimodal models that do not incorporate domain knowledge about the problem, we propose a multimodal deep learning design that incorporates sparse priors and allows the effective integration of information from another image modality into the network architecture. Our solution relies on a novel deep unfolding operator, performing steps similar to an iterative algorithm for convolutional sparse coding with side information; therefore, the proposed neural network is interpretable by design. The deep unfolding architecture is used as a core component of a multimodal framework for guided image super-resolution. An alternative multimodal design is investigated by employing residual learning to improve the training efficiency. The presented multimodal approach is applied to super-resolution of near-infrared and multi-spectral images as well as depth upsampling using RGB images as side information. Experimental results show that our model outperforms state-of-the-art methods.																	1057-7149	1941-0042					2020	29						8443	8456		10.1109/TIP.2020.3014729													
J								Zero-Shot Image Dehazing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Single image dehazing; unsupervised; zero-shot		In this article, we study two less-touched challenging problems in single image dehazing neural networks, namely, how to remove haze from a given image in an unsupervised and zero-shot manner. To the ends, we propose a novel method based on the idea of layer disentanglement by viewing a hazy image as the entanglement of several "simpler" layers, i.e., a hazy-free image layer, transmission map layer, and atmospheric light layer. The major advantages of the proposed ZID are two-fold. First, it is an unsupervised method that does not use any clean images including hazy-clean pairs as the ground-truth. Second, ZID is a "zero-shot" method, which just uses the observed single hazy image to perform learning and inference. In other words, it does not follow the conventional paradigm of training deep model on a large scale dataset. These two advantages enable our method to avoid the labor-intensive data collection and the domain shift issue of using the synthetic hazy images to address the real-world images. Extensive comparisons show the promising performance of our method compared with 15 approaches in the qualitative and quantitive evaluations. The source code could be found at http://www.pengxi.me.																	1057-7149	1941-0042					2020	29						8457	8466		10.1109/TIP.2020.3016134													
J								An Efficient Fire Detection Method Based on Multiscale Feature Extraction, Implicit Deep Supervision and Channel Attention Mechanism	IEEE TRANSACTIONS ON IMAGE PROCESSING										Fire detection; convolutional neural network; industrial applications; multiscale feature extraction; implicit deep supervision; channel attention mechanism	CONVOLUTIONAL NEURAL-NETWORKS; REAL-TIME FIRE; FLAME DETECTION; VIDEO FIRE; SURVEILLANCE; COLOR	Recent progress in vision-based fire detection is driven by convolutional neural networks. However, the existing methods fail to achieve a good tradeoff among accuracy, model size, and speed. In this paper, we propose an accurate fire detection method that achieves a better balance in the abovementioned aspects. Specifically, a multiscale feature extraction mechanism is employed to capture richer spatial details, which can enhance the discriminative ability of fire-like objects. Then, the implicit deep supervision mechanism is utilized to enhance the interaction among information flows through dense skip connections. Finally, a channel attention mechanism is employed to selectively emphasize the contribution between different feature maps. Experimental results demonstrate that our method achieves 95.3% accuracy, which outperforms the suboptimal method by 2.5%. Moreover, the speed and model size of our method are 3.76% faster on the GPU and 63.64% smaller than the suboptimal method, respectively.																	1057-7149	1941-0042					2020	29						8467	8475		10.1109/TIP.2020.3016431													
J								Fusion of Magnetic Resonance and Ultrasound Images for Endometriosis Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spatial resolution; Magnetic resonance imaging; Image fusion; Diseases; Magnetic resonance; Image fusion; magnetic resonance imaging; ultrasound imaging; super-resolution; despeckling; proximal alternating linearized minimization	MR; REGISTRATION	This paper introduces a new fusion method for magnetic resonance (MR) and ultrasound (US) images, which aims at combining the advantages of each modality, i.e., good contrast and signal to noise ratio for the MR image and good spatial resolution for the US image. The proposed algorithm is on an inverse problem, performing a super-resolution of the MR image and a denoising of the US image. A polynomial function is introduced to model the relationships between the gray levels of the MR and US images. The resulting inverse problem is solved using a proximal alternating linearized minimization algorithm. The accuracy and the interest of the fusion algorithm are shown quantitatively and qualitatively via evaluations on synthetic and experimental phantom data.																	1057-7149	1941-0042					2020	29						5324	5335		10.1109/TIP.2020.2975977													
J								Deep Saliency Hashing for Fine-Grained Retrieval	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Quantization (signal); Loss measurement; Task analysis; Sun; Birds; Training; Fine-grained retrieval; deep supervised hashing; salient region mining		In recent years, hashing methods have been proved to be effective and efficient for large-scale Web media search. However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have a subtle difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which automatically mines salient regions and learns semantic-preserving hashing codes simultaneously. DSaH is a two-step end-to-end model consisting of an attention network and a hashing network. Our loss function contains three basic components, including the semantic loss, the saliency loss, and the quantization loss. As the core of DSaH, the saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine-grained datasets, including Oxford Flowers, Stanford Dogs, and CUB Birds demonstrate that our DSaH performs the best for the fine-grained retrieval task and beats the strongest competitor (DTQ) by approximately 10% on both Stanford Dogs and CUB Birds. DSaH is also comparable to several state-of-the-art hashing methods on CIFAR-10 and NUS-WIDE.																	1057-7149	1941-0042					2020	29						5336	5351		10.1109/TIP.2020.2971105													
J								Efficient In-Loop Filtering Based on Enhanced Deep Convolutional Neural Networks for HEVC	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Image reconstruction; Convolutional neural networks; Video recording; Quality assessment; Video coding; Training; Convolutional neural networks; high efficiency video coding; in-loop filtering		The raw video data can be compressed much by the latest video coding standard, high efficiency video coding (HEVC). However, the block-based hybrid coding used in HEVC will incur lots of artifacts in compressed videos, the video quality will be severely influenced. To settle this problem, the in-loop filtering is used in HEVC to eliminate artifacts. Inspired by the success of deep learning, we propose an efficient in-loop filtering algorithm based on the enhanced deep convolutional neural networks (EDCNN) for significantly improving the performance of in-loop filtering in HEVC. Firstly, the problems of traditional convolutional neural networks models, including the normalization method, network learning ability, and loss function, are analyzed. Then, based on the statistical analyses, the EDCNN is proposed for efficiently eliminating the artifacts, which adopts three solutions, including a weighted normalization method, a feature information fusion block, and a precise loss function. Finally, the PSNR enhancement, PSNR smoothness, RD performance, subjective test, and computational complexity/GPU memory consumption are employed as the evaluation criteria, and experimental results show that when compared with the filter in HM16.9, the proposed in-loop filtering algorithm achieves an average of 6.45% BDBR reduction and 0.238 dB BDPSNR gains.																	1057-7149	1941-0042					2020	29						5352	5366		10.1109/TIP.2020.2982534													
J								Practically Lossless Affine Image Transformation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Chirp-z; zero-padding; sinc interpolation; MRI	ROTATION	In this contribution we introduce an almost lossless affine 2D image transformation method. To this end we extend the theory of the well-known Chirp-z transform to allow for fully affine transformation of general n-dimensional images. In addition we give a practical spatial and spectral zero-padding approach dramatically reducing losses of our transform, where usual transforms introduce blurring artifacts due to sub-optimal interpolation. The proposed method improves the mean squared error by approx. a factor of 1800 compared to the commonly used linear interpolation, and by a factor of 250 to the best competitor. We derive the transform from basic principles with special attention to implementation details and supplement this paper with python code for 2D images. In demonstration experiments we show the superior image quality compared to usual approaches, when using our method. However runtimes are considerably larger than when using toolbox algorithms.																	1057-7149	1941-0042					2020	29						5367	5373		10.1109/TIP.2020.2982260													
J								Graph-Based Non-Convex Low-Rank Regularization for Image Compression Artifact Reduction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low-rank model; graph Laplacian regularization; manifold structure; non-convex; compression artifact reduction	LAPLACIAN REGULARIZATION; REPRESENTATION; DEBLOCKING; SPARSE; FRAMEWORK; DCT	Block transform coded images usually suffer from annoying artifacts at low bit-rates, because of the independent quantization of DCT coefficients. Image prior models play an important role in compressed image reconstruction. Natural image patches in a small neighborhood of the high-dimensional image space usually exhibit an underlying sub-manifold structure. To model the distribution of signal, we extract sub-manifold structure as prior knowledge. We utilize graph Laplacian regularization to characterize the sub-manifold structure at patch level. And similar patches are exploited as samples to estimate distribution of a particular patch. Instead of using Euclidean distance as similarity metric, we propose to use graph-domain distance to measure the patch similarity. Then we perform low-rank regularization on the similar-patch group, and incorporate a non-convex l(p) penalty to surrogate matrix rank. Finally, an alternatively minimizing strategy is employed to solve the non-convex problem. Experimental results show that our proposed method is capable of achieving more accurate reconstruction than the state-of-the-art methods in both objective and perceptual qualities.																	1057-7149	1941-0042					2020	29						5374	5385		10.1109/TIP.2020.2975931													
J								Generalized Convolutional Sparse Coding With Unknown Noise	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convolutional sparse coding; noise modeling; Gaussian mixture model	FACTORIZATION; ALGORITHM; IMAGES	Convolutional sparse coding (CSC) can learn representative shift-invariant patterns from data. However, existing CSC methods assume the Gaussian noise, which can be restrictive in some challenging applications. In this paper, we propose a generalized CSC model capable of handling complicated unknown noise. The noise is modeled by the Gaussian mixture model, which can approximate any continuous probability density function. The Expectation-Maximization algorithm is used to solve the resultant learning problem. For efficient optimization, the crux is to speed up the convolution in the frequency domain while keeping the other computations involving the weight matrix in the spatial domain. We design an efficient solver for the weighted CSC problem in the M-step. The dictionary and codes are updated simultaneously by an efficient nonconvex accelerated proximal gradient algorithm. The resultant procedure, called generalized convolutional sparse coding (GCSC), obtains the same space complexity and a smaller running time than existing CSC methods (which are limited to the Gaussian noise). Extensive experiments on synthetic and real-world noisy data sets validate that GCSC can model the noise effectively and obtain high-quality filters and representations.																	1057-7149	1941-0042					2020	29						5386	5395		10.1109/TIP.2020.2980980													
J								Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Granular feature representation; transformation invariant; Gaussian Covariance matrix; matrix decomposition & normalisation; remote sensing scene classification	OBJECT DETECTION; FEATURES; COLOR; REPRESENTATION; MODEL; SHAPE	Recognising remote sensing scene images remains challenging due to large visual-semantic discrepancies. These mainly arise due to the lack of detailed annotations that can be employed to align pixel-level representations with high-level semantic labels. As the tagging process is labour-intensive and subjective, we hereby propose a novel Multi-Granularity Canonical Appearance Pooling (MG-CAP) to automatically capture the latent ontological structure of remote sensing datasets. We design a granular framework that allows progressively cropping the input image to learn multi-grained features. For each specific granularity, we discover the canonical appearance from a set of pre-defined transformations and learn the corresponding CNN features through a maxout-based Siamese style architecture. Then, we replace the standard CNN features with Gaussian covariance matrices and adopt the proper matrix normalisations for improving the discriminative power of features. Besides, we provide a stable solution for training the eigenvalue-decomposition function (EIG) in a GPU and demonstrate the corresponding back-propagation using matrix calculus. Extensive experiments have shown that our framework can achieve promising results in public remote sensing scene datasets.																	1057-7149	1941-0042					2020	29						5396	5407		10.1109/TIP.2020.2983560													
J								Sparse Graph Regularized Mesh Color Edit Propagation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Mesh color edit propagation; colorization; color bleeding; sparse graph constraint	VIDEO COLORIZATION; IMAGE COLORIZATION; RESTORATION	Mesh color edit propagation aims to propagate the color from a few color strokes to the whole mesh, which is useful for mesh colorization, color enhancement and color editing, etc. Compared with image edit propagation, luminance information is not available for 3D mesh data, so the color edit propagation is more difficult on 3D meshes than images, with far less research carried out. This paper proposes a novel solution based on sparse graph regularization. Firstly, a few color strokes are interactively drawn by the user, and then the color will be propagated to the whole mesh by minimizing a sparse graph regularized nonlinear energy function. The proposed method effectively measures geometric similarity over shapes by using a set of complementary multiscale feature descriptors, and effectively controls color bleeding via a sparse l(1) optimization rather than quadratic minimization used in existing work. The proposed framework can be applied for the task of interactive mesh colorization, mesh color enhancement and mesh color editing. Extensive qualitative and quantitative experiments show that the proposed method outperforms the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5408	5419		10.1109/TIP.2020.2980962													
J								STC-GAN: Spatio-Temporally Coupled Generative Adversarial Networks for Predictive Scene Parsing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Predictive Scene Parsing; Generative Adversarial Networks; Coupled Architecture; Spatio-Temporal Features		Predictive scene parsing is a task of assigning pixel-level semantic labels to a future frame of a video. It has many applications in vision-based artificial intelligent systems, e.g., autonomous driving and robot navigation. Although previous work has shown its promising performance in semantic segmentation of images and videos, it is still quite challenging to anticipate future scene parsing with limited annotated training data. In this paper, we propose a novel model called STC-GAN, Spatio- Temporally Coupled Generative Adversarial Networks for predictive scene parsing, which employ both convolutional neural networks and convolutional long short-term memory (LSTM) in the encoder-decoder architecture. By virtue of STC-GAN, both spatial layout and semantic context can be captured by the spatial encoder effectively, while motion dynamics are extracted by the temporal encoder accurately. Furthermore, a coupled architecture is presented for establishing joint adversarial training where the weights are shared and features are transformed in an adaptive fashion between the future frame generation model and predictive scene parsing model. Consequently, the proposed STC-GAN is able to learn valuable features from unlabeled video data. We evaluate our proposed STC-GAN on two public datasets, i.e., Cityscapes and CamVid. Experimental results demonstrate that our method outperforms the state-of-the-art.																	1057-7149	1941-0042					2020	29						5420	5430		10.1109/TIP.2020.2983567													
J								Generating Visually Aligned Sound From Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Videos; Gallium nitride; Training; Task analysis; Silicon; Testing; Video sound generation; visually aligned sound; audio forwarding regularizer		We focus on the task of generating sound from natural videos, and the sound should be both temporally and content-wise aligned with visual signals. This task is extremely challenging because some sounds generated outside a camera can not be inferred from video content. The model may be forced to learn an incorrect mapping between visual content and these irrelevant sounds. To address this challenge, we propose a framework named RegNet. In this framework, we first extract appearance and motion features from video frames to better distinguish the object that emits sound from complex background information. We then introduce an innovative audio forwarding regularizer that directly considers the real sound as input and outputs bottlenecked sound features. Using both visual and bottlenecked sound features for sound prediction during training provides stronger supervision for the sound prediction. The audio forwarding regularizer can control the irrelevant sound component and thus prevent the model from learning an incorrect mapping between video frames and sound emitted by the object that is out of the screen. During testing, the audio forwarding regularizer is removed to ensure that RegNet can produce purely aligned sound only from visual features. Extensive evaluations based on Amazon Mechanical Turk demonstrate that our method significantly improves both temporal and content-wise alignment. Remarkably, our generated sound can fool the human with a 68.12% success rate. Code and pre-trained models are publicly available at https://github.com/PeihaoChen/regnet.																	1057-7149	1941-0042					2020	29						8292	8302		10.1109/TIP.2020.3009820													
J								Optical Flow Based Co-Located Reference Frame for Video Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Estimation; Decoding; Optical imaging; Complexity theory; Optical distortion; Bidirectional control; Video coding; Optical flow; video coding; hierarchical structure	MOTION ESTIMATION; SEARCH ALGORITHM; COMPENSATION	This paper proposes a novel bi-directional motion compensation framework that extracts existing motion information associated with the reference frames and interpolates an additional reference frame candidate that is co-located with the current frame. The approach generates a dense motion field by performing optical flow estimation, so as to capture complex motion between the reference frames without recourse to additional side information. The estimated optical flow is then complemented by transmission of offset motion vectors to correct for possible deviation from the linearity assumption in the interpolation. Various optimization schemes specifically tailored to the video coding framework are presented to further improve the performance. To accommodate applications where decoder complexity is a cardinal concern, a block-constrained speed-up algorithm is also proposed. Experimental results show that the main approach and optimization methods yield significant coding gains across a diverse set of video sequences. Further experiments focus on the trade-off between performance and complexity, and demonstrate that the proposed speed-up algorithm offers complexity reduction by a large factor while maintaining most of the performance gains.																	1057-7149	1941-0042					2020	29						8303	8315		10.1109/TIP.2020.3014723													
J								Archived elitism in evolutionary computation: towards improving solution quality and population diversity	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										optimisation; elitist strategies; machine scheduling problems; MSPs; evolutionary computation; population diversity; strong archived elitism; solution quality	ALGORITHM; OPTIMIZATION	Many evolutionary algorithms, developed for solving complex optimisation problems, deploy the elitist strategy. The elitist strategy ensures that a group of the fittest individuals will be transferred to the next generation before performing any algorithmic operations. In general, elitism allows improving the algorithmic performance in terms of solution quality. However, transferring a group of the fittest individuals to the next generation will increase the selection pressure and significantly limit chances of the newly created offspring chromosomes to survive. In order to address the latter drawbacks, this study proposes and evaluates a number of alternative archive-based elitist strategies, where the fittest individuals are stored in the archive and transferred from that archive into the population based on certain rules. The computational experiments are conducted for the unrelated machine scheduling problem, where the total job processing cost is minimised. The results indicate that the proposed 'strong archived elitism' strategy, which samples the best individual discovered from the archive in every generation, outperforms the other elitist strategies in terms of the objective function values by up to 8.29% over the considered problem instances. Moreover, the 'strong archived elitism' strategy improves the population diversity, which further facilitates the explorative capabilities of the algorithm.																	1758-0366	1758-0374					2020	15	3					135	146															
J								Variable-grouping-based exponential crossover for differential evolution algorithm	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										differential evolution; exponential crossover; variable grouping; variable interaction	OPTIMIZATION; CODE	The performance of differential evolution (DE) algorithm largely depends on its crossover operator, whose substantive characteristics are to make the algorithm search in a subspace of the original search space. Different crossover operators use different subspace divisions, and how to choose a suitable crossover operator for a specific optimisation problem is still an open issue. This paper proposes variable-grouping-based exponential crossover (VGExp), where all variables are divided into multiple groups based on interaction information, and the variables that are mutated simultaneously have a high probability of coming from the same group. Moreover, the solutions can improve the accuracy of the variable grouping and provide initial guidance for optimisation. Therefore, the proposed VGExp seamlessly combines variables grouping technique and differential evolution. The experiment results based on 30 CEC2014 test problems show that VGExp can improve the performance of most DE variants, and it is also better than other well-developed crossover operators.																	1758-0366	1758-0374					2020	15	3					147	158															
J								Bat algorithm with Weibull walk for solving global optimisation and classification problems	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										bat algorithm; premature convergence; exploration; exploitation; Weibull walk; inertia weight		Bat algorithm (BA) becomes the most widely employed meta-heuristic algorithm to interpret the diverse kind of optimisation and real-world classification problems. BA suffers from one of the influential challenges called local minima. In this study, we carry out two modifications in the original BA and proposed a modified variant of BA called bat algorithm with Weibull walk (WW-BA) to solve the premature convergence issue. The first modification involves the introduction of Weibull descending inertia weight for updating the velocity of bats. The second modification approach updates the local search strategy of BA by replacing the Random walk with the Weibull Walk. The simulation performed on 19 standard benchmark functions represent the competence and effectiveness of WW-BA compared to the state of the art techniques. The proposed WWBA is also examined for classification problem. The empirical results reveal that the proposed technique outperformed the classical techniques.																	1758-0366	1758-0374					2020	15	3					159	170															
J								Data-driven pollution source location algorithm in water quality monitoring sensor networks	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										sensor networks; pollution source location; simulation optimisation; cooperative optimisation algorithm	PARTICLE SWARM OPTIMIZATION; CONTAMINANT SOURCE CHARACTERIZATION; PHOTOVOLTAIC MODELS; IDENTIFICATION; ENSEMBLE	Water pollution prevention has been a widely concerned issue for the safety of human lives. To this end, water quality monitoring sensors are introduced in the water distribution systems. Due to the limited budget, it is impossible to deploy sensors everywhere but a small number of sensors are deployed. From the sparse sensor data, it is important, but also challenging, to find out the pollution source location. Traditional methods may suffer from local optimum trapping or low localisation accuracy. To address such problems, we propose a cooperative intelligent optimisation algorithm-based pollution source location algorithm, which is a data-driven approach in simulation-optimisation paradigm. Through open-source EPANET simulator-based experiments, we find out our proposed data-driven algorithm can effectively and efficiently localise the pollution location, as well as the pollution injection starting time, duration and mass.																	1758-0366	1758-0374					2020	15	3					171	180															
J								An enhanced breeding swarms algorithm for high dimensional optimisations	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										enhanced breeding swarms; EBS; particle swarm optimisation; PSO; generic algorithm; metaheuristic; improved glowworm swarm optimisation; IGSO; computational intelligence	PARTICLE SWARM; DISTRIBUTION NETWORKS; GENERATION	This paper proposes a metaheuristic optimisation algorithm named enhanced breeding swarms (EBS), which combines the strengths of particle swarm optimisation (PSO) with those of genetic algorithm (GA). In addition, EBS introduces three modifications to the original breeding swarms to improve the performance and the accuracy of the optimisation algorithm. These modifications are applied on the acceptance criteria based on the improved glowworm swarm optimisation, velocity impact factor, and the mutation operator. The EBS algorithm is tested and compared against GA, PSO, and original BS algorithms, using unrotated and rotated six recognised optimisation benchmark functions. Results indicate that the EBS outperforms GA, PSO, and BS in most cases in terms of accuracy and speed of convergence, especially when the dimension of optimisation increases. As an application of the proposed EBS algorithm, a load flow analysis on a 6-bus network is performed, and the comparison results against another heuristic algorithm and the Newton-Raphson are reported.																	1758-0366	1758-0374					2020	15	3					181	193															
J								Genetic optimised serial hierarchical fuzzy classifier for breast cancer diagnosis	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										genetic algorithm; fuzzy logic; classification system; breast cancer diagnosis; variable selection	INTELLIGENT SYSTEM; ALGORITHM; RULES	Accurate early-stage medical diagnosis of breast cancer can improve the survival rates and fuzzy rule-base system (FRBS) has been a promising classification system to detect breast cancer. However, the existing classification systems involves large number of input variables for training and produces a large number of fuzzy rules, which lead to high complexity and barely acceptable accuracy. In this paper, we present a genetic optimised serial hierarchical FRBS, which incorporates lateral tuning of membership functions and optimisation of the rule base. The serial hierarchical structure of FRBS allows selecting and ranking the input variables, which reduces the system complexity and distinguish the importance of attributes in datasets. We conduct an experimental study on Original Wisconsin Breast Cancer Database and Wisconsin Breast Cancer Diagnostic Database from UCI Machine Learning Repository, and show that the proposed system can classify breast cancer accurately and efficiently.																	1758-0366	1758-0374					2020	15	3					194	205															
J								Background Noise Filtering and Distribution Dividing for Crowd Counting	IEEE TRANSACTIONS ON IMAGE PROCESSING										Head; Noise measurement; Estimation; Robustness; Feature extraction; Electronic mail; Internet; Crowd counting; head size estimation; density division; head mask	MULTIPLE	Crowd counting is a challenging problem due to the diverse crowd distribution and background interference. In this paper, we propose a new approach for head size estimation to reduce the impact of different crowd scale and background noise. Different from just using local information of distance between human heads, the global information of the people distribution in the whole image is also under consideration. We obey the order of far- to near-region (small to large) to spread head size, and ensure that the propagation is uninterrupted by inserting dummy head points. The estimated head size is further exploited, such as dividing the crowd into parts of different densities and generating a high-fidelity head mask. On the other hand, we design three different head mask usage mechanisms and the corresponding head masks to analyze where and which mask could lead to better background filtering. Based on the learned masks, two competitive models are proposed which can perform robust crowd estimation against background noise and diverse crowd scale. We evaluate the proposed method on three public crowd counting datasets of ShanghaiTech, UCFQNRF and UCFCC_50. Experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art crowd counting approaches.																	1057-7149	1941-0042					2020	29						8199	8212		10.1109/TIP.2020.3009030													
J								A Transform Coding Strategy for Dynamic Point Clouds	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Image coding; Transforms; Solid modeling; Image color analysis; Sensors; Heuristic algorithms; Dynamic point cloud compression; cellular automata; transform coding; octree; voxel color	COMPRESSION	The development of real-time 3D sensing devices and algorithms (e.g., multiview capturing systems, Time-of-Flight depth cameras, LIDAR sensors), as well as the widespreading of enhanced user applications processing 3D data, have motivated the investigation of innovative and effective coding strategies for 3D point clouds. Several compression algorithms, as well as some standardization efforts, has been proposed in order to achieve high compression ratios and flexibility at a reasonable computational cost. This paper presents a transform-based coding strategy for dynamic point clouds that combines a non-linear transform for geometric data with a linear transform for color data; both operations are region-adaptive in order to fit the characteristics of the input 3D data. Temporal redundancy is exploited both in the adaptation of the designed transform and in predicting the attributes at the current instant from the previous ones. Experimental results showed that the proposed solution obtained a significant bit rate reduction in lossless geometry coding and an improved rate-distortion performance in the lossy coding of color components with respect to state-of-the-art strategies.																	1057-7149	1941-0042					2020	29						8213	8225		10.1109/TIP.2020.3011811													
J								Deep Graph-Convolutional Image Denoising	IEEE TRANSACTIONS ON IMAGE PROCESSING										Graph neural networks; image denoising; graph convolution	CAMERA IDENTIFICATION; SPARSE; REGULARIZATION; TRANSFORM	Non-local self-similarity is well-known to be an effective prior for the image denoising problem. However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information. In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields. The graph convolution operation generalizes the classic convolution to arbitrary graphs. In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns. We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution. Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.																	1057-7149	1941-0042					2020	29						8226	8237		10.1109/TIP.2020.3013166													
J								Gabor Feature-Based LogDemons With Inertial Constraint for Nonrigid Image Registration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Image registration; Force; Optimization; Image texture; Strain; Measurement; Image registration; Gabor feature; LogDemons; inertial constraint	DIFFEOMORPHIC REGISTRATION; MR-IMAGES; MODEL; CLASSIFICATION	Nonrigid image registration plays an important role in the field of computer vision and medical application. The methods based on Demons algorithm for image registration usually use intensity difference as similarity criteria. However, intensity based methods can not preserve image texture details well and are limited by local minima. In order to solve these problems, we propose a Gabor feature based LogDemons registration method in this article, called GFDemons. We extract Gabor features of the registered images to construct feature similarity metric since Gabor filters are suitable to extract image texture information. Furthermore, because of the weak gradients in some image regions, the update fields are too small to transform the moving image to the fixed image correctly. In order to compensate this deficiency, we propose an inertial constraint strategy based on GFDemons, named IGFDemons, using the previous update fields to provide guided information for the current update field. The inertial constraint strategy can further improve the performance of the proposed method in terms of accuracy and convergence. We conduct experiments on three different types of images and the results demonstrate that the proposed methods achieve better performance than some popular methods.																	1057-7149	1941-0042					2020	29						8238	8250		10.1109/TIP.2020.3013169													
J								ORDNet: Capturing Omni-Range Dependencies for Scene Parsing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Convolution; Task analysis; Feature extraction; Correlation; Cats; Visualization; Scene parsing; omni-range dependencies; self-attention		Learning to capture dependencies between spatial positions is essential to many visual tasks, especially the dense labeling problems like scene parsing. Existing methods can effectively capture long-range dependencies with self-attention mechanism while short ones by local convolution. However, there is still much gap between long-range and short-range dependencies, which largely reduces the models' flexibility in application to diverse spatial scales and relationships in complicated natural scene images. To fill such a gap, we develop a Middle-Range (MR) branch to capture middle-range dependencies by restricting self-attention into local patches. Also, we observe that the spatial regions which have large correlations with others can be emphasized to exploit long-range dependencies more accurately, and thus propose a Reweighed Long-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an Omni-Range Dependencies Network (ORDNet) which can effectively capture short-, middle- and long-range dependencies. Our ORDNet is able to extract more comprehensive context information and well adapt to complex spatial variance in scene images. Extensive experiments show that our proposed ORDNet outperforms previous state-of-the-art methods on three scene parsing benchmarks including PASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of capturing omni-range dependencies in deep models for scene parsing task.																	1057-7149	1941-0042					2020	29						8251	8263		10.1109/TIP.2020.3013142													
J								Domain Adaptation by Joint Distribution Invariant Projections	IEEE TRANSACTIONS ON IMAGE PROCESSING										L-2-distance; dimensionality reduction; domain adaptation; joint distribution matching; Riemannian optimization	COVARIATE SHIFT; KERNEL	Domain adaptation addresses the learning problem where the training data are sampled from a source joint distribution (source domain), while the test data are sampled from a different target joint distribution (target domain). Because of this joint distribution mismatch, a discriminative classifier naively trained on the source domain often generalizes poorly to the target domain. In this article, we therefore present a Joint Distribution Invariant Projections (JDIP) approach to solve this problem. The proposed approach exploits linear projections to directly match the source and target joint distributions under the L-2-distance. Since the traditional kernel density estimators for distribution estimation tend to be less reliable as the dimensionality increases, we propose a least square method to estimate the L-2-distance without the need to estimate the two joint distributions, leading to a quadratic problem with analytic solution. Furthermore, we introduce a kernel version of JDIP to account for inherent nonlinearity in the data. We show that the proposed learning problems can be naturally cast as optimization problems defined on the product of Riemannian manifolds. To be comprehensive, we also establish an error bound, theoretically explaining how our method works and contributes to reducing the target domain generalization error. Extensive empirical evidence demonstrates the benefits of our approach over state-of-the-art domain adaptation methods on several visual data sets.																	1057-7149	1941-0042					2020	29						8264	8277		10.1109/TIP.2020.3013167													
J								Boosting Feature Matching Accuracy With Pairwise Affine Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Pipelines; Estimation; Detectors; Shape; Computer vision; Training; Feature matching; image matching; affine estimation; homograph estimation; support region	SCALE; SHAPE	Local image feature matching lies in the heart of many computer vision applications. Achieving high matching accuracy is challenging when significant geometric difference exists between the source and target images. The traditional matching pipeline addresses the geometric difference by introducing the concept of support region. Around each feature point, the support region defines a neighboring area characterized by estimated attributes like scale, orientation, affine shape, etc. To correctly assign support region is not an easy job, especially when each feature is processed individually. In this article, we propose to estimate the relative affine transformation for every pair of to-be-compared features. This "tailored" measurement of geometric difference is more precise and helps improve the matching accuracy. Our pipeline can be incorporated into most existing 2D local image feature detectors and descriptors. We comprehensively evaluate its performance with various experiments on a diversified selection of benchmark datasets. The results show that the majority of tested detectors/descriptors gain additional matching accuracy with proposed pipeline.																	1057-7149	1941-0042					2020	29						8278	8291		10.1109/TIP.2020.3013384													
J								A novel multi-agent scheduling mechanism for adaptation of production plans in case of supply chain disruptions	AI COMMUNICATIONS										Supply chain management; multi-agent system; agent; simulation	SYSTEM DYNAMICS; RISK; MITIGATION; SIMULATION; DESIGN; IMPACT	Manufacturing companies typically use sophisticated production planning systems optimizing production steps, often delivering near-optimal solutions. As a downside for delivering a near-optimal schedule, planning systems have high computational demands resulting in hours of computation. Under normal circumstances this is not issue if there is enough buffer time before implementation of the schedule (e.g. at night for the next day). However, in case of unexpected disruptions such as delayed part deliveries or defectively manufactured goods, the planned schedule may become invalid and swift replanning becomes necessary. Such immediate replanning is unsuited for existing optimal planners due to the computational requirements. This paper proposes a novel solution that can effectively and efficiently perform replanning in case of different types of disruptions using an existing plan. The approach is based on the idea to adhere to the existing schedule as much as possible, adapting it based on limited local changes. For that purpose an agent-based scheduling mechanism has been devised, in which agents represent materials and production sites and use local optimization techniques and negotiations to generate an adapted (sufficient, but non-optimal) schedule. The approach has been evaluated using real production data from Huawei, showing that efficient schedules are produced in short time. The system has been implemented as proof of concept and is currently reimplemented and transferred to a production system based on the Jadex agent platform.																	0921-7126	1875-8452					2020	33	1					1	12		10.3233/AIC-200646													
J								The intelligent system for detection and counteraction of malicious and inappropriate information on the Internet	AI COMMUNICATIONS										Inappropriate information; Internet; machine learning; big data; classifier	TEXT CLASSIFICATION	The Internet is becoming one of the most significant threats to personal and state information security. Therefore, the identification and counteraction of inappropriate information in global network content become the problem of national importance. The paper presents a new approach to building an intelligent system for identification and counteraction of malicious and inappropriate information on the Internet using artificial intelligence methods, in particular, machine learning and big data processing. The system architecture includes a set of intelligent components for data collection, information objects classification, ensuring the timeliness of analysis, eliminating incompleteness and inconsistency of analysis results, selecting the countermeasures for counteraction, and visualization. The paper presents an experimental evaluation of methods implemented for information object classification in single-threaded and multithreaded modes using various classifiers included in the Scikit-learn and Spark MLlib libraries.																	0921-7126	1875-8452					2020	33	1					13	25		10.3233/AIC-200647													
J								Using a Genetic Algorithm to optimize a stacking ensemble in data streaming scenarios	AI COMMUNICATIONS										Genetic algorithms; random forest; stacking ensemble; optimization	RANDOM FORESTS; RECOGNITION	The requirements of Machine Learning applications are changing rapidly. Machine Learning models need to deal with increasing volumes of data, and need to do so quicker as responses are expected more than ever in real-time. Plus, sources of data are becoming more and more dynamic, with patterns that change more frequently. This calls for new approaches and algorithms, that are able to efficiently deal with these challenges. In this paper we propose the use of a Genetic Algorithm to Optimize a Stacking Ensemble specifically developed for streaming scenarios. A pool of solutions is maintained in which each solution represents a distribution of weights in the ensemble. The Genetic Algorithm continuously optimizes these weights to minimize the cost function. Moreover, new models are added at regular intervals, trained on more recent data. These models eventually replace older and less accurate ones, making the ensemble adapt continuously do changes in the distribution of the data.																	0921-7126	1875-8452					2020	33	1					27	40		10.3233/AIC-200648													
J								Block structured scheduling using constraint logic programming	AI COMMUNICATIONS										Planning & scheduling; constraint logic programming; optimization		We propose a Constraint Logic Programming approach for synthesizing block-structured scheduling processes with ordering constraints. Then we extend the model to allow specification of resource constraints. Our goal is to design optimization algorithms. We combine block structured modeling of business processes with results from project scheduling literature. Differently from standard approaches, here we focus on block structured scheduling processes. Our main achievement is the formulation of an abstract mathematical model of block-structured resource-constrained scheduling processes. We tested the correctness and feasibility of our approach using an experimental prototype based on Constraint Logic Programming developed using ECLiPSe-CLP system.																	0921-7126	1875-8452					2020	33	1					41	57		10.3233/AIC-200650													
J								Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH										neural networks; multilevel relative entropy; chaining mutual information; multiscale generalization bound; multiscale Gibbs distribution	DISTRIBUTIONS	We derive generalization and excess risk bounds for neural networks using a family of complexity measures based on a multilevel relative entropy. The bounds are obtained by introducing the notion of generated hierarchical coverings of neural networks and by using the technique of chaining mutual information introduced by Asadi et al. '18. The resulting bounds are algorithm-dependent and multiscale: they exploit the multilevel structure of neural networks. This, in turn, leads to an empirical risk minimization problem with a multilevel entropic regularization. The minimization problem is resolved by introducing a multiscale extension of the celebrated Gibbs posterior distribution, proving that the derived distribution achieves the unique minimum. This leads to a new training procedure for neural networks with performance guarantees, which exploits the chain rule of relative entropy rather than the chain rule of derivatives (as in backpropagation), and which takes into account the interactions between different scales of the hypothesis sets of neural networks corresponding to different depths of the hidden layers. To obtain an efficient implementation of the latter, we further develop a multilevel Metropolis algorithm simulating the multiscale Gibbs distribution, with an experiment for a two-layer neural network on the MNIST data set.																	1532-4435						2020	21																						
J								Orlicz Random Fourier Features	JOURNAL OF MACHINE LEARNING RESEARCH										random Fourier features; kernel derivative; polynomial-growth functions; alpha-exponential Orlicz norm; unbounded empirical processes	NYSTROM METHOD	Kernel techniques are among the most widely-applied and influential tools in machine learning with applications at virtually all areas of the field. To combine this expressive power with computational efficiency numerous randomized schemes have been proposed in the literature, among which probably random Fourier features (RFF) are the simplest and most popular. While RFFs were originally designed for the approximation of kernel values, recently they have been adapted to kernel derivatives, and hence to the solution of large-scale tasks involving function derivatives. Unfortunately, the understanding of the RFF scheme for the approximation of higher-order kernel derivatives is quite limited due to the challenging polynomial growing nature of the underlying function class in the empirical process. To tackle this difficulty, we establish a finite-sample deviation bound for a general class of polynomial-growth functions under alpha-exponential Orlicz condition on the distribution of the sample. Instantiating this result for RFFs, our finite-sample uniform guarantee implies a.s. convergence with tight rate for arbitrary kernel with alpha-exponential Orlicz spectrum and any order of derivative.																	1532-4435						2020	21																						
J								metric-learn: Metric Learning Algorithms in Python	JOURNAL OF MACHINE LEARNING RESEARCH										machine learning; python; metric learning; scikit-learn	DIMENSIONALITY	metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT license.																	1532-4435						2020	21																						
J								Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting	JOURNAL OF MACHINE LEARNING RESEARCH										Contextual bandits; nonparametric learning		We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent "zooming" behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.																	1532-4435						2020	21																						
J								A Data Efficient and Feasible Level Set Method for Stochastic Convex Optimization with Expectation Constraints	JOURNAL OF MACHINE LEARNING RESEARCH										constrained stochastic optimization; level set methods; stochastic gradient methods; min-max optimization; online validation	NEYMAN-PEARSON CLASSIFICATION; LINEAR-PROGRAMMING APPROACH; APPROXIMATION ALGORITHMS; COMPOSITE OPTIMIZATION; GRADIENT-METHOD; RELAXATIONS; REGRESSION; ONLINE; CVAR	Stochastic convex optimization problems with expectation constraints (SOECs) are encountered in statistics and machine learning, business, and engineering. The SOEC objective and constraints contain expectations defined with respect to complex distributions or large data sets, leading to high computational complexity when solved by the algorithms that use exact functions and their gradients. Recent stochastic first order methods exhibit low computational complexity when handling SOECs but guarantee near-feasibility and near-optimality only at convergence. These methods may thus return highly infeasible solutions when heuristically terminated, as is often the case, due to theoretical convergence criteria being highly conservative. This issue limits the use of first order methods in several applications where the SOEC constraints encode implementation requirements. We design a stochastic feasible level set method (SFLS) for SOECs that has low complexity and emphasizes feasibility before convergence. Specifically, our level-set method solves a root-finding problem by calling a novel first order oracle that computes a stochastic upper bound on the level-set function by extending mirror descent and online validation techniques. We establish that SFLS maintains a high-probability feasible solution at each root-finding iteration and exhibits favorable complexity compared to state-of-the-art deterministic feasible level set and stochastic subgradient methods. Numerical experiments on three diverse applications highlight how SFLS finds feasible solutions with small optimality gaps with lower complexity than the former approaches.																	1532-4435						2020	21																						
J								Empirical Priors for Prediction in Sparse High-dimensional Linear Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian inference; data-dependent prior; model averaging; predictive distribution; uncertainty quantification	POSTERIOR CONCENTRATION; VARIABLE SELECTION; HORSESHOE ESTIMATOR; CONVERGENCE-RATES; MODEL SELECTION; LIKELIHOOD; INFERENCE; LASSO	In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein-von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.																	1532-4435						2020	21																						
J								Importance Sampling Techniques for Policy Optimization	JOURNAL OF MACHINE LEARNING RESEARCH										Reinforcement Learning; Policy Optimization; Importance Sampling; Per-Decision Importance Sampling; Multiple Importance Sampling	RENYI DIVERGENCE; GRADIENTS; SIZE	How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods.																	1532-4435						2020	21																						
J								Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer	JOURNAL OF MACHINE LEARNING RESEARCH										transfer learning; natural language processing; multi-task learning; attention-based models; deep learning		Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.(1)																	1532-4435						2020	21																						
J								Nesterov's Acceleration for Approximate Newton	JOURNAL OF MACHINE LEARNING RESEARCH										Nesterov's Acceleration; Approximate Newton; Stochastic Second-order	OPTIMIZATION	Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted considerable attention because of their low computational cost in each iteration. However, these methods might suffer from poor performance when the Hessian is hard to be approximate well in a computation-efficient way. To overcome this dilemma, we resort to Nesterov's acceleration to improve the convergence performance of these second-order methods and propose accelerated approximate Newton. We give the theoretical convergence analysis of accelerated approximate Newton and show that Nesterov's acceleration can improve the convergence rate. Accordingly, we propose an accelerated regularized sub-sampled Newton (ARS SN) which performs much better than the conventional regularized sub-sampled Newton empirically and theoretically. Moreover, we show that ARS SN has better performance than classical first-order methods empirically.																	1532-4435						2020	21																						
J								Influence of sample length on gray fuzzy prediction performance	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Sample size; predictive performance; grey prediction; GM(1,1); real per capita disposable income	GREY; MODEL	Gray fuzzy prediction model is suitable for small-sample-size prediction. The real per capita disposable income of urban residents in Hebei Province used as an example, and samples 3-35 in length selected, the influence of sample length on prediction performance of the GM (1,1) model were investigated. Sample length presents a nonlinear relationship with the predicted relative error of the model. Compared with large samples with lengths more than 15, small samples with lengths below 15 are suitable to establish the gray fuzzy prediction model. Small samples with length of 8-13 are applicable to three-step prediction. Sample lengths suitable for modeling were proposed, and the above conclusions provide a certain theoretical foundation and guidance for the research and application of gray fuzzy prediction in the future.																	1064-1246	1875-8967					2020	38	6			SI		6745	6754		10.3233/JIFS-179752													
J								Rough set-based evaluation of academic entrepreneurial performance of university teachers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										University teachers; academic entrepreneurship; entrepreneurship performance; set-based evaluation	TECHNOLOGY-TRANSFER; TEAM DIVERSITY; PERSPECTIVE	Academic entrepreneurship performance has become the core research content of academic entrepreneurial activities and the growth of entrepreneurial enterprises as a reflection of the academic entrepreneurship process and essence. Among them, university teachers are playing a social function that is equivalent to the enterprise innovation and entrepreneurship, embedding in the spiral mechanism of regional entrepreneurship, industrial, and value chains. However, studies on the academic entrepreneurship performance of university teachers are limited. Therefore, from the academic entrepreneurship data of 10 university teachers with active academic entrepreneurship behavior in China from 2012 to 2016, this study analyzes the approximate dependence of the academic entrepreneurship performance input and output indicators of university teachers. The rough set fuzzy clustering analysis method is used for the first time to measure the importance of academic entrepreneurship input and output indicators of university teachers in the entire entrepreneurial performance indicator system. In addition, this study analyzes the fuzzy evaluation of the academic entrepreneurship performance of university teachers. The objectives are to eliminate the subjectivity of weight setting, reduce the scale of the index, and establish an objective and effective evaluation index system. Results show that among the attributes of the academic entrepreneurial input and output conditions of university teachers, financial salary input and research funding are the reductions of five entrepreneurial output decision attributes. Moreover, financial salary input and research funding are the most important indicators and have a two-factor relationship with entrepreneurial output. The number of spin-off companies should not be selected as an indicator of entrepreneurial performance due to the almost zero discernable relationship with entrepreneurial input.																	1064-1246	1875-8967					2020	38	6			SI		6755	6763		10.3233/JIFS-179753													
J								Influence of the configuration effect of environment and organization factors on the innovation of information technology enterprises-Qualitative comparative analysis based on fuzzy set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Configuration effect; environment factors; organization factors; enterprise innovation; fsQAC		Enterprise innovation (EI) has become an important means to promote the sustainable development of enterprises in the new era. This study aims to explore the multiple concurrent factors and causal complex mechanism of the configuration effect of five conditions in the environmental and organizational levels and their influence on the differences in EI activity. Taking 20 information technology enterprises as case samples, this study uses the qualitative comparative analysis method of fuzzy sets. The main conclusions are as follows: (1) The ability to identify opportunity is the necessary condition affecting the high innovation activity of enterprises; the lack of internal innovation culture, the low innovation activity. (2) The driving mechanism of high EI activity can be divided into three paths, and the different configurations of the five factors can produce many paths to achieve such activity. (3) The driving mechanism of low EI activity can be divided into two paths, and it has an asymmetric causal relationship with the driving mechanism of high EI activity. This study is helpful in expanding the innovation perspective of environment and organization matching. This study also provides enterprises with valuable enlightenment to effectively stimulate innovation vitality.																	1064-1246	1875-8967					2020	38	6			SI		6765	6775		10.3233/JIFS-179754													
J								A model for evaluating the influence factors in trademark infringement based on fuzzy analytical hierarchy process	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										FAHP; intellectual property; trademark infringement; statutory compensation		There has an important modification of the trademark infringement compensation system in China's trademark law (2013). Yet through the big data retrieval and analysis software like Citespace, the study found that such difficult problems as generalization of statutory compensation, being difficult to determine the base of punitive damage, lower average compensation in trademark infringement case still exist. Simultaneously, researching on the evaluation of the compensation system for trademark infringement is not completely. This study has constructed the influence index of calculating the statutory compensation amount for trademark infringement through analyzed the judgment on trademark infringement cases in China in the past decade and previous research. Based on these indexes, researchers used fuzzy analytic hierarchy process (FAHP) to analyze the effective judgment of trademark infringement compensation case of Beijing Intellectual Property Court in 2018. Results point that the Court makes better use of statutory compensation in the trademark infringement case.																	1064-1246	1875-8967					2020	38	6			SI		6777	6784		10.3233/JIFS-179755													
J								Risk evaluation of intellectual property pledge financing based on fuzzy analytical network process	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intellectual property pledge financing; risk evaluation; fuzzy analytical network process (FANP)	QUALITY; MODEL	Intellectual property pledge financing is an innovative application of derivative financial instruments, which provides new financing channels for enterprises with intellectual property advantages. Risk evaluation of intellectual property pledge financing is the key to sustainable financing. Given that the factors affecting the risk of intellectual property pledge financing are complex and difficult to quantify, scant studies have used fuzzy analytical network process (FANP) to evaluate the risks of intellectual property pledge financing comprehensively. In order to offer a feasible and versatile tool in risk evaluation of intellectual property pledge financing, this paper summarizes the influencing factors and establishes a risk evaluation model of intellectual property pledge financing through FANP. Based on questionnaire survey and expert evaluation method, this study calculates the super decision matrix, weighted super matrix and the comprehensive weight of each through Super Decision(SD) software. The results indicate that evaluation risk P-1, realization risk P-2, and moral risk P-3 have the greatest impact on the risk evaluation of intellectual property pledge financing, followed by intellectual property management risk. Through shedding light on the risk factors, this study provides a reference for the practice of intellectual property pledge financing management.																	1064-1246	1875-8967					2020	38	6			SI		6785	6793		10.3233/JIFS-179756													
J								Impact of altruistic motivation perception on corporate hypocrisy: A study based on fuzzy set qualitative comparative analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Altruistic motivation perception; corporate hypocrisy; qualitative comparative analysis of fuzzy sets	SOCIAL-RESPONSIBILITY	This study conducts data analysis on hypocritical and non-hypocritical enterprises to explore the influence of altruistic motivation perception on corporate social responsibility (CSR) implementation, CSR communication, and corporate hypocrisy. To analyze the data of hypocritical and non-hypocritical companies, a fuzzy set qualitative comparative analysis is applied. In hypocrisy enterprises, high neutral altruism and low positive altruism are sufficient conditions for high CSR communication. Low negative altruism is a necessary condition for CSR implementation, and low positive altruism and high neutral altruism are sufficient conditions for CSR implementation. And low positive altruism and low CSR implementation behavior are sufficient conditions for corporate hypocrisy, while high neutral altruism is sufficient conditions for corporate hypocrisy. In non-hypocritical enterprises, high neutral altruism and high negative altruism are sufficient conditions for high CSR communication. And high CSR implementation needs high positive altruism and CSR communication interaction. And High positive altruism and low CSR communication are sufficient conditions for corporate non-hypocrisy.																	1064-1246	1875-8967					2020	38	6			SI		6795	6803		10.3233/JIFS-179757													
J								Research on technology innovation risk evaluation of high-tech enterprises based on fuzzy evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy evaluation; technology innovation; risk evaluation; high-tech enterprises	PRODUCT INNOVATION; PERFORMANCE	While high-tech enterprises have achieved high returns through technological innovation, they also face high risks. This study builds a technology innovation risk evaluation system for high-tech enterprises from eight dimensions: technology risk, capital risk, patent risk, talent risk, management risk, policy risk, industrial risk, and market risk. Based on the subjective and fuzzy characteristics of the evaluation indicators, a risk evaluation model for technological innovation based on fuzzy evaluation was established, and an empirical study was conducted with a technological innovation project of a petrochemical company in Shanghai. The research results show that the high-tech enterprises' technology innovation risk evaluation model constructed in this study has high accuracy for the quantification of technological innovation risk, and the technology innovation risk evaluation model is highly practical, which provides a reasonable basis for risk management decisions in the process of a high-tech company technology innovation.																	1064-1246	1875-8967					2020	38	6			SI		6805	6814		10.3233/JIFS-179758													
J								Establishment and application of fuzzy comprehensive evaluation of green building design based on data mining	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Green building design; data mining; fuzzy mathematics; comprehensive evaluation		Green buildings have many advantages that traditional buildings do not possess, but there are still problems such as insufficient promotion and application. One of the important reasons is the lack of pertinent evaluation of green buildings. The current assessment of the economic benefits of green building design is still at a qualitative stage, and there is a lack of quantitative assessment methods and models. Based on this, this study establishes a green building design evaluation index system including cost and economic benefits. In the cost estimation, a nonlinear evaluation model based on data mining is designed to ensure a better estimation accuracy and credibility. In the evaluation of economic benefits, an incremental benefit evaluation model based on fuzzy mathematics is designed to evaluate its comprehensive economic benefits in a quantitative way. The results of data simulation show that the least squares support vector machine has achieved a good balance between the estimation time and accuracy. The comprehensive evaluation model proposed in this paper has strong operability and good evaluation effect, which is of great significance to promote the development of green building design.																	1064-1246	1875-8967					2020	38	6			SI		6815	6823		10.3233/JIFS-179759													
J								Safety evaluation system of urban traffic network based on topological genetic algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Topology; genetic algorithm; urban traffic; safety evaluation		With the rapid development of rail transit, it puts forward higher requirements for safety. Once a safety accident occurs, it is highly destructive. It is of crucial significance and value to ensuring the safety of urban rail transit network by constructing an effective safety evaluation system. For purpose of this study, an urban transit network safety evaluation system based on topological genetic algorithm was constructed with an example of urban rail transit networking. Four indexes of urban rail transit safety were determined with the literature analysis method in combination with expert scoring method, including network heterogeneity, travel efficiency vulnerability, connection reliability and network capacity vulnerability. Besides, on the basis of some freeway networks and some freeway collecting toll data of Xi'an city, and the result of road network vulnerability, an index weight computing framework based on topological genetic algorithm was constructed. The result of simulation test shows that freeway manager should strengthen protection over nodes of large probability betweenness, and the urban transit network safety evaluation system based on topological genetic algorithm is of high feasibility and rationality.																	1064-1246	1875-8967					2020	38	6			SI		6825	6832		10.3233/JIFS-179760													
J								Establishment of college English teachers' teaching ability evaluation based on Clementine data mining	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Clementine; data mining; English teaching; teaching ability evaluation		Teacher evaluation, an important part of teaching quality management, has an important impact on the cultivation of talents. As scientific and objective teaching evaluation can objectively measure the effectiveness of teachers' teaching means, it is an important criterion for teaching authorities to evaluate teachers' teaching methods as well as effective means and important ways to improve teaching methods. The evaluation method of traditional English teachers' teaching ability is subject to subjective factors, which leads to unreliable evaluation results. This paper describes the use of data mining software Clementine to introduce data mining technology into teaching management, extracts useful information hidden from teaching evaluation data, and explores key factors that influence the quality of teaching. Meanwhile, this paper anonymizes and encodes the data based on data mining technology, discretizes the teaching characteristic variables of English teachers with a large number of eigenvalues, processes teaching data by rough set reduction method, processes English teachers' teaching database to be mined by rough set method, determines the decision table, sets minimum support and minimum confidence, obtains association rules through association rule mining method to realize English teachers' teaching data mining, obtains the evaluation indicators through principal component analysis, calculates the English teacher's teaching ability index, and evaluates English teachers' teaching ability according to the index value at last. The experimental results show that the evaluation results of the proposed method are reliable and can reflect the true ability of teachers, thus providing a decision-making reference for the management of teaching authorities.																	1064-1246	1875-8967					2020	38	6			SI		6833	6841		10.3233/JIFS-179761													
J								Effectiveness evaluation of the ecological responsibility of resource-oriented enterprises based on intuitionistic fuzzy sets-TOPSIS	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Resource-oriented enterprises; ecological responsibility of enterprises; effectiveness evaluation; intuitionistic fuzzy set-TOPSIS		Establishing an ecological responsibility system for resource-oriented enterprises aims to further encourage them to assume ecological responsibility. However, Chinese resource-oriented enterprises exhibit unsatisfactory ecological responsibility performance because they cannot ensure the smooth implementation of the ecological responsibility system. With considerations of the characteristics and detailed contents of ecological responsibility, an evaluation index system was established for resource-oriented enterprises, and their ecological responsibility performances were assessed using the intuitionistic fuzzy set-Technique for Order-Preference by Similarity to Ideal Solution method. Challenges of resource-oriented enterprises in ecological responsibility were recognized from the assessment results. Based on the findings, several policy suggestions were proposed to ensure ecological responsibility performance among resource-oriented enterprises.																	1064-1246	1875-8967					2020	38	6			SI		6843	6852		10.3233/JIFS-179762													
J								The business performance evaluation index method for the high-tech enterprises based on the DEA model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										DEA; new energy vehicles; entropy weight method; high-tech enterprises		With the new round of scientific and technological revolution and industrial change accelerating, "electrification, intelligence, network and sharing" has become a new trend of the current automotive industry reform. Accelerating the cultivation and development of new energy vehicles has not only become an urgent task to effectively alleviate the pressure of energy and environment in China and promote the sustainable development of the automobile industry, but also a strategic measure to accelerate the transformation and upgrading of the automobile industry in China. In view of the low survival rate of high-tech innovating enterprises and high input and high risk of high-tech enterprises, it is of great significance to assess and evaluate the operating efficiency of the high-tech innovating enterprises. This study used the entropy weight method to establish a DEA-based business performance evaluation index model to evaluate and analyze the business performance of the listed new energy vehicles enterprises in China in 2018. The result shows that most of the new energy vehicles enterprises are in the state of increasing returns to scale, conforming to the characteristic of high growth of high-tech enterprises; however, the overall operational efficiency of these enterprises is very low and some of them face the problem of obvious scientific research input redundancy; therefore, the improper input-output framework is the main reason lead to the low scale efficiency for the enterprises and provide corresponding policy proposals for these problems.																	1064-1246	1875-8967					2020	38	6			SI		6853	6861		10.3233/JIFS-179763													
J								A human error mechanism for pilot based on fault tree analysis and Bayesian network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Flight human error; fault tree; Bayesian network; risk management; flight safety	CIVIL-AVIATION; SAFETY	At present, aircraft accidents caused by human factors occur frequently. As the flight crew is the last line of defense for aviation safety, how to identify and quantitatively evaluate human errors in flight and reduce the error rate has become an important issue for the civil aviation, and it is also the most effective way to control flight operation risks. Thus, the human error mechanism for pilot was investigated by the combine of the fault tree and the Bayesian network theories based on historical research data of typical unsafe events caused by flight human errors. Firstly, the fault tree was used to identify and qualitatively analyze the risk of systems, and then transformed into a Bayesian network model to obtain the relative probability of intermediate events and top events. Finally, the system hierarchy of unsafe events caused by flight human error was quantitatively evaluated. The results showed that there were 96 failure modes in the system, and the flight human error was caused by the coupling of multiple risk factors. The probability of non-technical skill loss is the highest, followed by that of the lack of technical skills and violations. The basic events in the organization, environment and equipment factors have a great impact on the flight human error, which is a weak link in the system. The results provide some theoretical basis for developing preventive measures of flight human error and improving the level of flight safety.																	1064-1246	1875-8967					2020	38	6			SI		6863	6871		10.3233/JIFS-179764													
J								Research trends of cultural identity of mongol nationality based on mapping knowledge domain and visualization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bibliometric analysis; mapping knowledge domain and visualization; the mongol nationality; cultural identity; research trends		Bibliometric analysis and information visualization software (Cite Space) were used to analyze and organize related research on whether people with the surname Yu descended from Temujin (Genghis Khan) of the Mongol nationality since 1981. Data were collected from the Chinese National Knowledge Infrastructure database, and analytic dimensionalities included basic knowledge of researcher, research institute, sampling of subjects, release time of research results, keywords and hotspot issues, critical cultural symbols, so on. Colleges, universities, and professors in Guizhou Province, China participated in the research, and few journalists also participated in many related surveys. The kernel keywords and critical symbols embodied in the research were "Mongol Nationality," "Genghis Khan," "Tiegaiyu," "Nine Sons, Ten Jinshi (successful candidates in imperial examinations)," among others. Related studies since 1981 are divided into four development phases and continue to present an expanding trend. Related mainstream viewpoints are divided into three types, namely, the Mongol nationality, Tangut people in the Western Xia Dynasty, and the Han nationality. Research arguments focus on whether people with surname Yu in southwestern China are descendants of Genghis Khan in Mongolia. Future research emphasis and difficulty lies in identifying if people with surname Yu in southwestern China are descendants of Genghis Khan in Mongolia, combining historical truths by processing and analyzing the enormous family trees with the surname Yu. Future research should focus on the emic approach. In the current study, paternal Y chromogene detection and related articles on migrants into Sichuan are predicted to facilitate further research development.																	1064-1246	1875-8967					2020	38	6			SI		6873	6882		10.3233/JIFS-179765													
J								Intelligent simulation of enterprise re-innovation support system based on system dynamics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Re-innovation; support system; system dynamics; intelligent simulation	FAILURE; ENTREPRENEURS	Technology innovation failure exists objectively. How to provide support for failed enterprises and improve their willingness to re-innovate has become a hot issue in the field of innovation management. To build a re-innovation support system for enterprises with failed innovation from the perspective of institutional environment, the system model of the re-innovation support system of the enterprises with failed innovation was established through the system dynamics method, the system boundary was defined, the causal relationship and the system flow graph of the model were analyzed, and the operation effect of the support system model was intelligent simulated by using Vensim software. The results show that institutional support can effectively reduce the number of enterprises with failed innovation, increase the number of re-innovation enterprises, and further improve the enterprise income and government revenue, thus verifying the effectiveness of the re-innovation support system of enterprises with failed innovation.																	1064-1246	1875-8967					2020	38	6			SI		6883	6893		10.3233/JIFS-179766													
J								A method to measure the efficiency of industry finance integration of manufacturing enterprises based on SFA model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										SFA; manufacturing enterprises; integration of production and finance efficiency		This study aims to analyze the efficiency and influencing factors of participating in financial institutions in China's listed manufacturing enterprises, and uses the stochastic frontier model (SFA) to carry out empirical research. Firstly, with the help of SFA, combining with the condition of integration of industry and finance of manufacturing enterprises, the indexes of output, input and influencing factors of the manufacturing enterprises were screened out, and the calculation model of integration of production and financing efficiency was established. Then, the integration of production and financing of manufacturing enterprises in 2006-2016 was used as the research case. Using the Frontier 4.1 and applying the CD production function as the production function of SFA in this study to calculate the efficiency of its panel data. The results show that the efficiency of the integration of production and financing is basically on the rise in 2006-2016. The scale of enterprises, the proportion of shares, the financial risk, the government support and the financial support all significantly affect the efficiency, and the property right has little effect on the efficiency of the integration.																	1064-1246	1875-8967					2020	38	6			SI		6895	6903		10.3233/JIFS-179767													
J								The impact of technological innovation on the development of intelligent industry system: Evidence from Henan, China	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Technological innovation; intelligent industry system; development function		The aim of this study is to analyze how technological innovation promotes the development of intelligent industrial system. First, A factor analysis method was used to measure technological innovation capability of intelligent industry in Henan, China from 2006-2016, the correlation between the improvement of intelligent industrial technology innovation capacity and the growth of gross output value was tested and verified. SPSS software was used to analyze the influence of R&D personnel's total time equivalent, R&D expenditure and total internal expenditure on the total output value of intelligent industrial sectors. Results show that there is a highly positive correlation between the improvement of intelligent industrial technology innovation capacity and the growth of gross output value. R&D personnel full-time equivalent and R&D funds, internal expenditure and intelligent industrial output value is positively related, The number of R&D institutions in large and medium-sized industrial enterprises is negatively related to the total output value of modern industries, the growth of patent application authorization cannot significantly affect the development of intelligent industry system.																	1064-1246	1875-8967					2020	38	6			SI		6905	6909		10.3233/JIFS-179768													
J								An evaluation method of PE classroom teaching quality in colleges and universities based on grey system theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										PE; classroom teaching quality; grey system theory; fuzzy comprehensive evaluation		This study aims to explore the quality evaluation of PE classroom teaching in colleges and universities. The evaluation index system of college PE class-room teaching quality was built by using the application of the analysis hierarchy process (AHP) to determine the weight of each level index, combined with gray system theory and fuzzy comprehensive evaluation method to establish the evaluation method for the study. Taking a college as an example, according to the above methods, the quality of PE teaching was evaluated. The evaluation results show that the quality of PE teaching of the sample is in good and middle level, close to good level. The evaluation index system, index weight and evaluation method have higher applicability.																	1064-1246	1875-8967					2020	38	6			SI		6911	6915		10.3233/JIFS-179769													
J								Robust design for quality characteristics of mechanical processing products based on algorithm decision-making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Quality characteristics; robust design; mechanical processing products; tap lifetime; multivariant optimization process memorise algorithm	PARTICLE SWARM OPTIMIZATION	In the background of high-quality development, production factors, productivity and total factor productivity all propose new high standards to enterprises. Research and development (R&D) design stage is the source and key to assure high quality of enterprise products. The quality-induced loss of products in R&D design stage is significantly higher than that in manufacturing stage. Made in China 2025 brings a new opportunity and a higher challenge to China's machinery manufacturing industry through transforming from low cost competitive advantages to quality-benefit advantages. As the fundamental products of machinery manufacturing industry, mechanical processing products can prevent flaws fundamentally by improving quality characteristics in R&D design stage, thus saving R&D cost effectively and improving product quality and reliability significantly. In this study, a robust design for quality characteristics of tap which is a typical representative of mechanical processing products was proposed. According to theoretical analysis of influencing factors of tap quality characteristics as well as the relevant quality fluctuation caused by influencing factors, its lifespan is the key quality characteristic in R&D design stage. As the key to determine tap lifetime, cryogenic temperature was chosen with soaking time by decision-making and trial evaluation laboratory as two of the most important factors that influence cryogenic treatment in the present study. A functional model between tape lifetime and these two selected factors were constructed through the response surface methodology. Optimized by multivariant optimization process memorise algorithm, the best parameter combination for cryogenic treatment was obtained. Research demonstrates that robust design of product quality characteristics helps enterprises to make product quality better and more quickly, lower operation cost, and improve productivity, thus promoting the high-quality development of enterprises.																	1064-1246	1875-8967					2020	38	6			SI		6917	6926		10.3233/JIFS-179770													
J								Construction of interpretive structure modeling for the influencing factors of emergency industry development	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Emergency industrial; influencing factors; index system; interaction mechanism; ISM		Promotion of the emergency industry plays a major role in pacing up adjustment of industrial structure and enhancing economic vitality. Eleven influencing factors of the emergency industry were explored from economic, social and environmental aspects and the ISM (Interpretive Structure Modeling) model of the emergency industry was constructed by using basic methods of ISM. Results show that investment in emergency industry, input-output efficiency of emergency industry and application of technological innovation in emergency industry are the most direct factors affecting the development of emergency industry at present, while public safety consciousness is the most fundamental factor affecting emergency industry.																	1064-1246	1875-8967					2020	38	6			SI		6927	6935		10.3233/JIFS-179771													
J								Investment compensation mechanism for affordable housing construction project based on decision function	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Affordable housing; construction investment; decision function; net present value of project; compensation mechanism	FINANCE	With continuously accelerated development of China's urbanization, the price of commercial housing is also rising and the housing problem has become a hot issue in China's social development at current stage. The affordable housing, as a means to guarantee the housing rights of medium-and low-income groups, has seriously insufficient attractiveness to the returns on project investment due to its non-profit nature. The social capital is unwilling to enter this field and thus leads to inadequate supply of affordable housing. Therefore, it is very necessary to establish an investment compensation mechanism for affordable housing construction. Based on decision theory and reasonable return principle, this study constructs an investment compensation model for construction of rental-type affordable housing based on satisficing decision. Finally, it uses the data of a public rental housing project in Beijing, China to conduct empirical analysis, quantifies all government subsidies and actually calculates the government compensation amount with the net present value of project as the decision goal. The results show that: (1) determining the reasonable profit rate of affordable housing developers is the key point to promote the investment and development of affordable housing; (2) investment compensation model for affordable housing construction based on satisficing decision can be used to objectively calculate the compensation amount payable to the affordable housing developer; (3) the private sector that invests in affordable housing should not passively wait for subsidies and preferential policies and measures of the government; instead, they should actively innovate in the profit model of affordable housing and promote the construction of affordable housing. The study shows that the investment compensation model for affordable housing based on decision function is good to judge the project feasibility. Compared with other models, it improves the model's interpretation and prediction ability and provides new decision ideas and methods for the government and enterprises to develop and construct affordable housing.																	1064-1246	1875-8967					2020	38	6			SI		6937	6946		10.3233/JIFS-179772													
J								Measurement of corporate social responsibility of automobile enterprises based on AHP-GRA model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										China; automobile enterprises; social responsibility; AHP-GRA model		With the development of economic globalization, great importance has been attached to corporate social responsibility by firms, governments and social organizations. Currently, only a few domestic automobile enterprises can fully disclose their social responsibility information. Most automobile enterprises fail to establish an effective and comprehensive mechanism for social responsibility management and information disclosure, and the concept of social responsibility has not been popularized and widely accepted by automobile enterprises. In order to solve the problem of externalization of internal costs incurred from personal and property safety of consumers and environmental pollution control which should be borne by automobile enterprises, and to promote the establishment and effective implementation of a social responsibility management system in the automobile industry, this study constructs a comprehensive China automobile CSR evaluation index system from four aspects, including CSR to government, CSR to employees, CSR to consumers and CSR to environment and community. In addition, this study makes a comprehensive comparison of the CSR of China's automobile enterprises based on the synthetical evaluation method of Analytic Hierarchy Process and Grey Relation Analysis (AHP-GRA Model). The results show that: (1) under the influence of macro-economic control by the state, automobile enterprises, irrespective of being joint ventures, state-owned or private enterprises, hold the green development of firms in great account, especially corporate environmental governance; (2) the AHP-GRA Model can offer remedies to the single evaluation model, making up for its lack of accuracy and objectivity; (3) through a trend evaluation based on the AHP-GRA comprehensive evaluation model, a simple and reliable evaluation model is provided for the government, market and consumers to understand the current situation and future development of the CSR of China's automobile enterprises in a comprehensive and objective way. The study shows that the AHP-GRA comprehensive evaluation model is feasible in the evaluation of social responsibilities of automobile enterprises, which covers the shortage of the single evaluation method, and provides new decision-making ideas and methods for the evaluation and optimization of social responsibility of automobile enterprises.																	1064-1246	1875-8967					2020	38	6			SI		6947	6956		10.3233/JIFS-179773													
J								Food safety risk intelligence early warning based on support vector machine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Support vector machine; food safety; risk; intelligence early warning	MANAGEMENT; FRAMEWORK; KNOWLEDGE	From the perspective of China's food safety management practices, effective early warning and intervention of food safety risks in the food industry chain will greatly reduce the possibility of food safety accidents, thereby improving the safety and stability of people's social life. This study builds a food safety risk intelligence early warning model based on support vector machines. First, it discusses the process of food safety risk intelligence early warning, classifies warning indicators of warning indicators, and sets the principles of early warning. Secondly, based on the theory of support vector machine technology, an intelligence early warning model for food safety risks is constructed, and the collection and processing of sample data are explained. Finally, based on the analysis results of the early warning model, the results are discussed to verify the effectiveness and accuracy of the early warning model.																	1064-1246	1875-8967					2020	38	6			SI		6957	6969		10.3233/JIFS-179774													
J								Research on the core competitiveness of pharmaceutical listed companies based on fuzzy comprehensive evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Core competitiveness; pharmaceutical listed companies; fuzzy comprehensive evaluation; Hengrui pharmaceutical		The pharmaceutical industry is an important part of our national economy, which is the combination of traditional industry and modern industry. Medicine is an industry with high technology, high risk, high investment and high return, and has always been the focus of competition in developed countries. If Chinese pharmaceutical enterprises want to stand out in such fierce competition, they must find out the core competitiveness that fits their own development path. Based on the construction of the evaluation index system of the core competitiveness of the pharmaceutical industry, this study makes an empirical analysis of Hengrui pharmaceutical based on the fuzzy comprehensive evaluation method, discusses the reasons for the formation of its core competitiveness, and puts forward effective ways for listed companies in the pharmaceutical industry to enhance their core competitiveness.																	1064-1246	1875-8967					2020	38	6			SI		6971	6978		10.3233/JIFS-179775													
J								Intelligent transportation system in China: The optimal evaluation period of transportation's application performance	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intelligent transportation system; optimal evaluation period; hysteresis effect; vector auto-regressive model; application performance		There has been rapid development of intelligent transportation systems around the world, helping to relieve urban traffic congestion problems. In particular, many mega-cities in China have devoted significant money and resources to the development of intelligent transportation systems. However, appropriate evaluation period of the application effectiveness of intelligent transportation systems for urban setting remains poorly defined partly due to the hysteretic nature of both the construction of infrastructure transportation and the application effect displayed. In this study, Beijing City was investigated to determine the appropriate evaluation period to evaluate the application effect of the intelligent transportation system. Then the vector auto-regressive model is introduced to calculate the evaluation period of intelligent transportation system's application effect. The results show a three-year lag length of the application effect in Beijing's intelligent transportation system, which provides the rationale and reference methods to determine the appropriate evaluation period for similar projects. Finally, the importance and key criteria of determining the optimal evaluation period for intelligent transportation system are described.																	1064-1246	1875-8967					2020	38	6			SI		6979	6990		10.3233/JIFS-179776													
J								A fuzzy-TOPSIS approach to enhance emergency logistics supply chain resilience	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Emergency logistics SCs; supply chain risks; supply chain resilience; fuzzy mathematics; TOPSIS	SERVICE; DESIGN	Unconventional emergencies are difficult to be predicted and controlled. Emergency logistics supply chains (SCs) are exposed to great natural risks in operation and prone to encounter chain breakage. Based on a large number of literature analysis and practical research, this paper proposes 8 strategies and develops a Fuzzy-topsis (Technique for Order Preference by Similarity to an Ideal Solution) approach to enhance emergency logistics SC resilience. An empirical analysis is carried out among several Emergency Logistics Experts. The results show that the 3 strategies of "increasing the number of transport links", "improving information monitoring and warning capabilities", "improving the accuracy of the plans" and "establishing a green passage" are most effective to enhance the resilience of the emergency logistics SC. Sensitivity analysis shows that the 8 strategies proposed play different roles in the contribution of SC resilience enhancing and can be applied in different situations.																	1064-1246	1875-8967					2020	38	6			SI		6991	6999		10.3233/JIFS-179777													
J								Research on financing risk of mining enterprises based on fuzzy comprehensive evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Financing risk; mining enterprises; Zijin mining; fuzzy comprehensive evaluation		Mining is the important basic industry about economy, people's livelihood and social development in our country. Its development degree will directly affect the level of industrial development in China. There are financing difficult problems in most mining companies which restrict its further development because of the imperfect capital market in China as well as the mining industry's development level and high risk characteristic of environmental impact. Through the use of the combination of theoretical and empirical research methods, this study takes Zijin Mining as an example and analyzes its financing risk systematically, which based on the fuzzy comprehensive evaluation method combined with the current financing situation of China's mining industry. The purpose is to evaluate the financing risk effectively so as to help the management make more effective financing decisions.																	1064-1246	1875-8967					2020	38	6			SI		7001	7007		10.3233/JIFS-179778													
J								Interaction mechanism between sustainable innovation capability and capital stock: Based on PVAR model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										China's manufacturing industry; sustainable innovation capability; capital stock; PVAR model; interaction mechanism	RESEARCH-AND-DEVELOPMENT; PRODUCTIVITY; PERFORMANCE	The promotion of sustainable innovation capability of the manufacturing industry is a strategic move to the mid-to-high end of the global value chain. We construct the evaluation index system of sustainable innovation capability based on the panel data of China's manufacturing industry from 2000 to 2015. We analyze the interaction mechanism between sustainable innovation capability and capital stock by using the PVAR model. The results show that there is a long-term co-integration relationship between them, and both of them have the characteristic of self-inertia. The sustainable innovation capability has a positive impact on capital stock, while the capital stock has a negative effect on sustainable innovation capability. The capital stock has a positive impact on sustainable innovation capacity in the low-tech and medium-tech industries, while it has a negative impact in the high-tech industry. Additionally, the capital stock has the least impact on sustainable innovation capacity in the low-tech industry, and sustainable innovation capacity has the greatest impact on capital stock in the medium-tech industry.																	1064-1246	1875-8967					2020	38	6			SI		7009	7025		10.3233/JIFS-179779													
J								Evaluation of international port city based on fuzzy comprehensive evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										International port city; fuzzy comprehensive evaluation; the evaluation index system	INLAND PORTS; INTERFACE	Port city is the result of interactive development of city and port, and international port city is the advanced stage of port city development. Firstly, the evaluation index system of international port city is constructed, which includes openness and internationalization of the city, economic development and technological innovation, port radiation and influence, balanced development and ecological environment, residents' life and social development. Taking Ningbo as an example, the fuzzy comprehensive evaluation method is used to evaluate the international port city. The results show that Ningbo port has strong radiation and influence, the urban development is not balanced, and the level of urban openness and internationalization needs to be improved.																	1064-1246	1875-8967					2020	38	6			SI		7027	7032		10.3233/JIFS-179780													
J								Evaluation of capital input and output efficiency of China's ai enterprises based on DEA model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AI enterprises; capital input and output efficiency; DEA		As an important part of China's strategic emerging industries, the development of AI industry is rapid. The state and local governments have issued policies to encourage its development. The number of AI enterprises is increasing, its quality is also improving, and the application scope and market share of products are gradually expanding. With the large amount of capital input, the efficiency of capital input-output in AI enterprises has been paid more and more attention. In this paper, 48 listed companies in AI stock market are selected, with undistributed profit, net profit and surplus reserve as output indicators, monetary capital, accounts receivable, inventory and fixed assets as input indicators, and DEA model is used to evaluate their capital input-output efficiency. The results show that the capital input-output efficiency of 16 companies is DEA effective, and for non DEA effective sample companies, relevant improvement strategies are proposed.																	1064-1246	1875-8967					2020	38	6			SI		7033	7040		10.3233/JIFS-179781													
J								The influence mechanism of organizational slack on CSR from the perspective of property heterogeneity: Evidence from China's intelligent manufacturing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Organizational slack; corporate social responsibility; property heterogeneity; listed manufacturing companies	CORPORATE; RESOURCES	It has become an important strategy for Chinese manufacturing companies to actively fulfill social responsibility. However, it is subject to the profitability of economic organizations and resource specificity, slack resources are the main source of fulfilling corporate social responsibility; And due to differences in management systems of enterprises in different natures, there are significant differences in the level and path of using slack resources to fulfill corporate social responsibility. In order to deeply explore the impact level and internal mechanism of organizational slack on corporate social responsibility in China, this paper conducts empirical research on the panel data of 373 listed intelligent manufacturing companies in Chinese A-share from 2013 to 2017, from the perspective of property heterogeneity. The research results show that organizational slack is widespread and presents the characteristics of property heterogeneity. Non-state-owned companies tend to reserve more slack resources due to more strict resource constraints. Organizational slack has a positive effect on improving corporate social responsibility performance, and its transmission is to improve the level of corporate responsibility performance and increase the quality of social responsibility information disclosure, but will not promote enterprises to take social responsibility into strategic considerations. There is a heterogeneous difference in the level of influence of organizational slack on the fulfillment of corporate social responsibility. The effect is more significant on the State-owned companies, nevertheless, its mechanism of action is not affected by the heterogeneous conditions of property. The research conclusions of this paper could make contribution for government agencies to improving the legal system of corporate social responsibility, accelerating the reform of factor markets, and formulating different incentive policies of property rights for fulfilling corporate social responsibility.																	1064-1246	1875-8967					2020	38	6			SI		7041	7052		10.3233/JIFS-179782													
J								Evaluation of interorganizational collaboration effectiveness in distributed innovation networks based on fuzzy-AHP	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Distributed innovation networks; interorganizational collaboration effectiveness; evaluation; FAHP		Although the value of interorganizational collaboration in DINs is widely acknowledged, a critical issue is that how we should evaluate its actual effect in practice. This issue is of practical significance in that network managers can get feedback from the results of evaluation and then improve their management. In this study, we argue that interorganizational collaboration effectiveness is a construct that comprise three dimensions: innovation efficiency, interorganizational learning and social capital. Furthermore, we establish an evaluation index system and develop an evaluation model based on fuzzy analytic hierarchy process (FAHP). At last, the evaluation model is applied to analyze the interorganizational collaboration effectiveness in the joint venture between Fiberhome and SMC.																	1064-1246	1875-8967					2020	38	6			SI		7053	7059		10.3233/JIFS-179783													
J								The effect of trade creation in China-Asean free trade area based on the gravity model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Trade gravity model; China-Asean free trade area; trade creation effect		Based on the gravity model of trade, this paper studies the trade creation effect of China-Asean free trade area. 23 trading countries in North America, South America, Asia, Europe and Oceania are selected as samples. Construct the trade gravity model of China-Asean free trade area and make the empirical analysis. The increase of GDP in trading countries will promote the growth of trade flow. Secondly, CAFTA will increase trade volume, but the effect is not significant. In addition, geographical distance has a negative impact on the increase of trade flows. In order to promote the trade development of China-Asean free trade area, differentiated cooperation is needed. It needs to carry out sub-regional cooperation with CAFTA and promote trade cooperation with One Belt and One Road.																	1064-1246	1875-8967					2020	38	6			SI		7061	7065		10.3233/JIFS-179784													
J								The evaluation to the social responsibility consciousness of the college students based on AHP-fuzzy model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										College students; social responsibility consciousness; AHP-fuzzy comprehensive evaluation		In recent years, with the development of diversified social values, new requirements put forward to the university students' social responsibility consciousness education. And the evaluation of to the social responsibility consciousness of the college students is putting on the agenda. In this study, AHP-fuzzy comprehensive evaluation method was used to establish the evaluation model of the social responsibility consciousness of the college students with student-based, which is used to assessment the social responsibility consciousness of the college students with student-based, to reflect advantages and disadvantages of the system of the social responsibility consciousness of the college students with student-based. Thus decision-making basis is provided to the education and training of the social responsibility consciousness of the college students with student-based. The building of the model has great theoretical and practical value on the integrated assessment of the social responsibility consciousness of the college students with student-(b)ased.																	1064-1246	1875-8967					2020	38	6			SI		7067	7072		10.3233/JIFS-179785													
J								Research on intelligent extraction of literature knowledge for the risk factors of chronic diseases	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Chronic disease; risk factors; intelligent extraction; knowledge discovery		Medical literature research results are more accurate and representative than patient medical record data. The medical literature intelligently extracts research objects, research areas, and research result entities, identifies their indexing features in the abstract, and finds risk factors for chronic illness and its association with regions and populations. This study proposes a literature knowledge extraction model for chronic disease risk factors. Based on dictionaries and rules, manual annotation extraction methods and gCLUTO dual cluster analysis, the Chinese biomedical literature database was published to correlate with chronic disease risk factors. The literature is a corpus, which intelligently identifies extracts and clusters the literature abstracts. The discovery of chronic disease risk factors from the perspective of human health was explored by taking the literature of hypertension risk factor research as an example; the literature knowledge extraction model for chronic disease risk factors was verified to construct a chronic disease risk factor set. It also reveals the relationship between chronic disease risk factors and regions/populations, and provides reference and reference for the research of chronic disease risk factors.																	1064-1246	1875-8967					2020	38	6			SI		7073	7081		10.3233/JIFS-179786													
J								Recommendation system design for college network education based on deep learning and fuzzy uncertainty	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Improved neural network; path sorting; network education; educational resources; personalized recommendation	PERSONALIZED RECOMMENDATION	If the online education platform simply presents offline course content and teaching resources, it will result in poor interaction between teachers and students and poor student learning efficiency because of the time and space barriers in distance education. At the same time, it has a one-way teaching for all students without any difference, so its personalization is poor. In order to improve the efficiency of network education knowledge recommendation, based on the traditional neural network, this study combines the actual needs of network education to construct an improved neural network-path sorting algorithm. Moreover, this study builds a network education knowledge recommendation system based on the algorithm and explores the functions of each module of the system to realize all system functions. In addition, this study performs system function tests on a complete system. The results show that the proposed algorithm is higher than the traditional recommendation algorithm in recommendation effect and recommendation efficiency, and has certain practical effects, and can provide theoretical reference for subsequent related research.																	1064-1246	1875-8967					2020	38	6			SI		7083	7094		10.3233/JIFS-179787													
J								Intelligent English translation mobile platform and recognition system based on support vector machine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Support vector machine; speech recognition; spoken English; system model; feature fusion		In the process of learning English, the status of spoken language is particularly important, and it is also the most concerned aspect of most English learners. However, the current situation is that due to the limited resources of traditional teachers and the lack of oral practice environment, it is difficult for many learners to effectively improve their English level. Based on this, this study builds a smart English recognition system based on support vector machine. Moreover, this paper introduces a support vector machine to characterize speech signals. In addition, this paper uses feature fusion to map complex nonlinear relationships between features based on support vector machines and establishes a smart English recognition system based on support vector machine. The model can accurately identify the syllables and pronunciations in the words. Moreover, the use of a large-scale corpus based on non-specific people in this article can represent the generality of spoken learner.																	1064-1246	1875-8967					2020	38	6			SI		7095	7106		10.3233/JIFS-179788													
J								Design of intelligent community security system based on visual tracking and large data natural language processing technology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Video surveillance system; big data; community security; early warning system	EXPERT-SYSTEM; SMART; REVIEWS; SET	With the improvement of public security awareness, it is of great value to establish an intelligent crowd screening video early warning system for community security. The method is to use artificial intelligence and big data technology to analyze the facial features of past people through video data in video surveillance. In this paper, the authors analyze the design of intelligent community security system based on visual tracking and large data natural language processing technology. Through JMF technology and video acquisition card, the signal can be transmitted from the matrix machine to the server of the system. In the process of transmission, we also need to use multi-threading technology and double buffer technology to compress video, which can improve the compression efficiency and reduce the CPU usage time. The information module is mainly responsible for receiving and playing video. This module needs JAVA technology which can receive video. Once the client requests monitoring, the client can immediately send relevant operation instructions to the server, and the server will return the corresponding video stream in time. The simulation results show that the system can identify risks efficiently and accurately.																	1064-1246	1875-8967					2020	38	6			SI		7107	7117		10.3233/JIFS-179789													
J								Enterprise ERP system optimization based on deep learning and dynamic fuzzy model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; neural network; enterprise ERP system; data quality optimization		The goal of enterprise ERP is to select useful information and finally realize the goal of transforming information into enterprise competitiveness. At present, the research on the data quality optimization management of ERP system is not systematic enough, which can not achieve the effect of optimization management. In this paper, the author analyzes the enterprise ERP system optimization based on deep learning and dynamic fuzzy model. After dividing all kinds of typical activities of information process, this paper builds a model of information process of each ERP module with the method of information product diagram, which makes the invisible information production process visible, so as to realize the continuous improvement of information quality, and provides a complete set of optimized management methods and measures for enterprise information quality. The simulation results show that it is feasible to use neural network to solve the optimization design of the system. The main reason is that the variables among neural networks are distributed in parallel, and when the energy function tends to be stable, it can reach its minimum value, so as to achieve the purpose of optimization.																	1064-1246	1875-8967					2020	38	6			SI		7119	7131		10.3233/JIFS-179790													
J								Analysis of news transmission mode based on fuzzy data classification and neural network simulation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data prediction; neural network; news information; intelligent search	HUMAN ACTION RECOGNITION; DECISION TREE; SELECTION	Online news media websites and mobile news applications can report hot events in real time. With the change of news duration, we urgently need a tool that can automatically extract hot events from massive data and show how events change dynamically with time. In this paper, the authors analyze the news transmission mode based on fuzzy data classification and neural network simulation. Due to the limitation of BP neural network algorithm, there are some problems in the prediction of information transmission. Therefore, we use fuzzy algorithm to optimize BP neural network, which is easy to fall into local minimum and slow convergence speed, so that BP neural network has higher prediction accuracy. The simulation results show that after the introduction of the neural network model, the features of neural network with richer semantic information can be used, and the new event line can be processed at the same time. The training speed of news content processing is much faster than that of probability graph model. It can be seen that under the influence of new media, news communication shows new characteristics, which further affects people's news reading habits.																	1064-1246	1875-8967					2020	38	6			SI		7133	7143		10.3233/JIFS-179791													
J								Intelligent English teaching prediction system based on SVM and heterogeneous multimodal target recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Support vector machine; intelligent English; recognition; feature analysis	PLATFORM	With the development of speech recognition technology, human services in many fields have gradually been replaced by intelligent voice services. Natural language processing technology and speech recognition technology are gradually becoming the key technologies in human-computer interaction systems. However, there are still some problems in intelligent speech recognition. In view of this, this study uses English as the basic language to carry out research and combines support vector machine to construct intelligent English recognition and predic-tion system to obtain EEG signals related to Chinese speech. Moreover, this paper adopts wavelet packet decomposition and co-space mode to extract the features of the acquired EEG signals and uses the support vector machine model to classify and compare the signal features. The research results show that the recognition rate of the traditional algorithm is significantly lower than the speech recognition rate of the algorithm model, and the algorithm of this study basically meets the requirements of the actual use of the system.																	1064-1246	1875-8967					2020	38	6			SI		7145	7154		10.3233/JIFS-179792													
J								Modeling of performance evaluation of educational information based on big data deep learning and cloud platform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; cloud computing; big data; teaching quality; machine		The traditional teaching quality evaluation model can only evaluate the data of the corresponding period, does not have the deep learning function, and cannot realize the prediction based on the existing data. In order to improve the operation effect of the teaching evaluation model, this study built a teaching evaluation model by combining data mining with deep learning and constructed a data processing module based on cloud computing. Moreover, this study used the improved incomplete multi-classification algorithm to construct the multi-divider and applied the constructed classifier to the teaching evaluation system to automatically classify the teachers. In addition, after learning the relevant theoretical knowledge of the university evaluation system, this paper also designed a prototype of the teaching quality evaluation framework based on the improved algorithm. Through the analysis of the model effect, it can be seen that the results of this study have certain evaluation effects, which can be applied to practice, and can provide theoretical reference for subsequent related research.																	1064-1246	1875-8967					2020	38	6			SI		7155	7165		10.3233/JIFS-179793													
J								Application of face expression recognition technology in skilled unsupervised course based on ultra-wide regression network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ultra-wide regression network; unsupervised learning; classroom education; expression recognition; feature extraction	REPRESENTATION	In traditional teaching, people can quickly guess the emotion type of the other person based on facial expressions but understanding the human facial expression by computer is a very complicated problem. There may be some differences in the expression of each person. Therefore, how to make the computer 'read and understand' the emotional state of the person according to the facial expression of the person is the research focus of this paper. In this paper, the expression recognition based on dynamic sequence is developed, and the mapping relationship between basic expression and emotion is studied to construct the emotional model, and the emotional state recognition of the learner is realized by the research on facial expression. In the intelligent teaching environment, the teacher adjusts the teaching strategy according to the emotional state recognition test results, improves the teaching efficiency, and realizes the wisdom teaching. Moreover, combined with the actual situation, an expression recognition algorithm based on the ultra-wide regression network model for unsupervised learning classroom education is constructed. Through experimental analysis, we can know that this research algorithm has certain advantages in facial expression recognition.																	1064-1246	1875-8967					2020	38	6			SI		7167	7177		10.3233/JIFS-179794													
J								Application of deep learning and BP neural network sorting algorithm in financial news network communication	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neural network algorithm improvement; news dissemination; prediction model; network input		The reasonable ranking of web pages is an important step in the realization of search engine technology, and plays a key role in improving the information quality of high retrieval and presentation web pages. In this paper, the authors analyze the application of deep learning and BP neural network sorting algorithm in financial news network communication. According to the historical data of users in the process of searching and browsing, we can extract the potential connection between the information of the page itself and the user's behavior habits, so as to mine the user's potential preference page. Finally, we use a variety of technologies to mix recommendation and sorting. By observing the effect of multiple training, it is found that the convergence speed of the network model is fast, the training time is short and the training effect is good. We need to fully understand the characteristics of financial news communication in the new media era, and then fully grasp people's financial news reading habits on this basis, and then put forward the innovative mode of financial news communication in the new media era on the basis of these two points.																	1064-1246	1875-8967					2020	38	6			SI		7179	7190		10.3233/JIFS-179795													
J								Statistical optimization of supply chain financial credit based on deep learning and fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning algorithm; supply chain finance; credit rating; fuzzy algorithm	RECOGNITION; NETWORK	By controlling the transaction background and data of supply chain enterprises, supply chain finance can reduce the degree of information asymmetry in the process of enterprise financing and provide more financing mode options for enterprises. In this paper, the author analyzes the statistical optimization of supply chain financial credit based on deep learning and fuzzy algorithm. We use particle swarm optimization to train BP neural network and improve the previous algorithm. By changing the speed of the particle search in the weight space, that is, updating the weight of the net-work, the mean square error of the network output is gradually reduced. Simulation results show that the model is helpful to analyze the correlation between supply chain finance and economy, compared with the traditional BP neural network, the original data of BP neural network based on particle swarm optimization is better fitted, so it can be used to predict supply chain financial credit level.																	1064-1246	1875-8967					2020	38	6			SI		7191	7202		10.3233/JIFS-179796													
J								Neural network model analysis of consumption expenditure prediction of urban and rural residents based on Lasso regression analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lasso regression analysis; BP neural network; urban and rural residents; consumption; combination model	GENETIC ALGORITHM	How to analyze and predict the consumption behavior of Chinese residents is of great significance based on the existing consumption theory research and the actual situation in China. This study studies the factors influencing the consumption of urban and rural residents in China through factor analysis and builds a model of urban and rural residents' consumption expenditure based on BP neural network. At the same time, through the data collection, the consumption data of urban and rural residents in China from 1986 to 2017 were collected. The BP neural network prediction model was used to predict the consumption expenditure of urban and rural residents, and the absolute error and relative error were obtained. In addition, this study establishes a combined forecasting model based on Lasso regression analysis and BP neural network to predict the consumption expenditure of urban and rural residents and compares its prediction results with BP neural network prediction model. The research shows that the combined prediction model based on Lasso regression analysis and BP neural network comprehensively surpasses BP neural network prediction model in prediction accuracy, and it has certain effectiveness and puts forward practical suggestions, which provides theoretical reference for subsequent related research.																	1064-1246	1875-8967					2020	38	6			SI		7203	7214		10.3233/JIFS-179797													
J								Intelligent financial management of company based on neural network and fuzzy volatility evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neural network; volatility assessment; corporate finance; intelligent management system	PARTICLE SWARM OPTIMIZATION; ECONOMIC-GROWTH; PREDICTION	By integrating business processes, financial intelligent management system can provide effective data information with high quality and low cost for strategic decision-making. In this paper, the authors analyze the intelligent financial management of company based on neural network and fuzzy volatility evaluation. With the help of the development and support of information technology, the construction of financial intelligent management center realizes the efficient and standardized business process by building the information system center platform. It is the basis for the smooth operation of process reengineering and can guarantee the successful and safe operation of financial intelligent management center. The correlation dimension information reflecting the financial data abnormal feature is extracted to construct discrimination statistic and test criterion. The abnormal feature of financial data is mined according to the significant difference of discrimination statistic to realize anomaly analysis of financial data. The results show that the accuracy of financial data anomaly mining with this method is better. It has good application value in the financial audit and economic investigation and other fields.																	1064-1246	1875-8967					2020	38	6			SI		7215	7228		10.3233/JIFS-179798													
J								Information network security construction based on depth learning and modulus algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; fuzzy algorithm; intrusion detection; network security	BIG DATA	The information network itself has the characteristics of vulnerability, and the information network is facing more and more threats. The existence of these two factors is the main reason for the frequent discovery of information network security problems. Network security depends on the development of information technology. Only the independent and controllable technology can obtain a safe and stable development space. However, network security is the premise of the development of information technology, so we can't go ahead blindly without considering the cost and cost for the development. In the face of safety and risk, we should master a balance point, and take multiple measures such as management, technology and system to control the safety risk in a controllable range. This paper analzye the information network security construction based on depth learning and modulus algorithm. The improved algorithm can be stably controlled within 0.007s in training time. Moreover, as the training difficulty increases, the training time will gradually increase, but the change is not obvious, and the overall tends to be stable. By training neural network with the selected instance and make its maximum close to real system working model or network attack model, thus the input to any of its monitoring data to be able to make quite the right judgment.																	1064-1246	1875-8967					2020	38	6			SI		7229	7240		10.3233/JIFS-179799													
J								Application of deep learning and artificial intelligence algorithm in multimedia music teaching	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; artificial intelligence algorithm; multimedia music teaching; system design	TECHNOLOGY	Modern information technology conforms to the development of the times. Now we live in an increasingly information, digital and network environment. This paper presents a design scheme of music assisted instruction system, which makes use of the features of intelligent chip, such as powerful function, high cost performance, deep learning and artificial intelligence algorithm with the advantages of global optimization, self-adaptive and fast, so that the system can objectively and quantificationally evaluate the result of each performance of the evaluated person. In order to use the advantages of simulated annealing algorithm, which can find the global optimal solution and fast convergence speed, the standard track is divided into several key groups according to the music score, and the evaluation function is introduced for each key group. The improved algorithm MPRA has significantly improved the prediction accuracy in the online music learning knowledge base network. This shows that the improved algorithm is better than the original path sorting algorithm. However, in a network with a large average number of paths, the accuracy of link prediction is relatively lower, which indicates that the applicability of the weight value of the path has certain limitations.																	1064-1246	1875-8967					2020	38	6			SI		7241	7251		10.3233/JIFS-179800													
J								Educational resource online evaluation system based on neural network dynamic feedback algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Neural network; feedback algorithm; educational resources; recommendation; online evaluation		At present, a large number of teaching resources on the web have not been systematically classified and organized, and the storage of similar resources is not concentrated, and various types of resources are often mixed together, and educational resources are not distinguished between good and bad. If we want to fully access and utilize the teaching resources on the web, we need to take some measures to search from many websites and numerous web pages and obtain high-scoring resources. Based on online evaluation system of educational resources of neural network dynamic feedback algorithm. This study uses the feedback system to classify and evaluate teaching resources and select appropriate training methods and architectures. Moreover, through data analysis and analysis, this study conducts performance analysis on the algorithm model proposed in this study. The results obtained by the system simulation show that the algorithm model proposed in this study has certain practical effects, and is suitable for the online evaluation system of educational resources, and can provide theoretical reference for subsequent related research.																	1064-1246	1875-8967					2020	38	6			SI		7253	7265		10.3233/JIFS-179801													
J								Research on neural network model for new energy industry economy based on particle swarm optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Genetic neural network; fuzzy uncertainty; new energy; industrial economy	FACE RECOGNITION; REPRESENTATION	Climate warming has triggered a series of external uneconomic problems that have had a tremendous impact on human production and life. At the same time, the current intelligent models for low-carbon economic research have some drawbacks. Therefore, in order to explore a more scientific low-carbon development model, based on the analysis of low-carbon development at home and abroad, this study combines particle swarm optimization and neural network to analyze China's carbon productivity through multidimensional analysis, determine multiple factors and select more important data as factor analysis objects.In this paper, the authors analyze the neural network model for new energy industry economy based on particle swarm optimization. The result shows that the particle swarm algorithm has a very fast convergence rate and can avoid falling into local minimum values in the later stage and finally achieve higher solution precision. The development of new energy industry is an important strategy and means for implementing sustainable development and adjusting the industrial structure. This article, from the perspective of industrial economics, uses genetic neural networks and fuzzy uncertainty to explore how to effectively combine the capital market with the development of new energy industries. Bring into play the role of the capital market in supporting the development of new energy industries.																	1064-1246	1875-8967					2020	38	6			SI		7267	7277		10.3233/JIFS-179802													
J								Intelligent driving system of robot based on computer vision and neural network algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Computer vision; neural network; robot; intelligent driving system	KNOWLEDGE; PLATFORM	The target detection and recognition module of traditional automatic driving system is vulnerable to the influence of the surrounding environment, which makes the system lack of accuracy and real-time. In this paper, based on the analysis of the traditional algorithm, the SVM method of intelligent control algorithm is used to optimize the existing vehicle automatic driving system. The optimization algorithm obtains the corresponding classifier model through the extraction and training of the input image features, and then uses the model to detect and recognize the target. The experimental results show that the optimized algorithm is much better than the traditional feature-based automatic driving system target detection algorithm in detecting speed and accuracy. At the same time, the false alarm rate of detection also decreased, which fully verified the feasibility of the optimization method. Intelligent car driving technology greatly reduces the driver's learning cost, and also reduces the workload of human brain. The continuous maturity of perception, communication and embedded technology will strongly support the development of intelligent vehicles. Combined with the active safety technology of auxiliary driving, such as adaptive cruise, vehicle approach notification, etc., it can make the intelligent vehicle driving more safe and reliable.																	1064-1246	1875-8967					2020	38	6			SI		7279	7290		10.3233/JIFS-179803													
J								Research on English grammar recognition system based on combination of genetic algorithm and KNN algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Genetic algorithm; KNN algorithm; English; grammar; feature recognition		It is a very challenging task to develop a grammatical error correction model with many types of error correction and high accuracy of error correction for various English grammar mistakes. Based on Chinese English learners, this paper analyzes the grammatical mistakes that they often make, and then studies and implements an English grammar correction model with high precision and many types of error correction. Moreover, this study combines genetic algorithm and KNN algorithm to construct a smart English grammar recognition system. At the same time, on the basis of summarizing the work of predecessors, this paper, based on advanced theoretical and technical knowledge, designs a grammatical error correction model based on rules and statistics. In addition, for the corresponding related word errors, this study proposes an error correction model of related word grammar. Finally, this study selects the features of artificially extracted English related words for error correction, and finally selects the optimal feature subset to improve the accuracy of model error correction.																	1064-1246	1875-8967					2020	38	6			SI		7291	7302		10.3233/JIFS-179804													
J								Action recognition model of athletes at the scene of the game based on SVM and multitarget tracking algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine learning; game scene; athletes; motion recognition		At present, most athletes' motion recognition models still have certain problems, and relevant development is still in infancy. Moreover, the key issues mainly include the detection, tracking and identification of moving targets, and most of the technologies are not mature enough. In view of this, this study introduces the classification algorithm of support vector machine in detail on the basis of theory, which is in line with the statistical principle. At the same time, this paper uses the optimization theory to solve the optimal classification decision function, and finally gives its algorithm steps. The experimental results on the data set prove that the proposed algorithm can achieve better classification results. Subsequently, this paper analyzes the basic theory of Kalman filter, and introduces the multi-target tracking algorithm combined with motion and apparent information. Finally, this paper combines experiments to analyze the performance of the algorithm. The research results show that the method proposed in this study has certain practical effects.																	1064-1246	1875-8967					2020	38	6			SI		7303	7314		10.3233/JIFS-179805													
J								Application of deep learning and artificial intelligence in the psychological mechanism of language activity	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; artificial intelligence; language activities; algorithm simulation	MODEL	The deep learning architecture consists of multiple layers of non-linear computing units, and the output of each lower layer is used as the input of the higher layer, which can learn effective feature representations from a large amount of input data. In this paper, the author analyze the application of deep learning and artificial intelligence in the psychological mechanism of language activity. Language activity is a complex cognitive process that drives the comprehensive response, coordination, and operation of body-related functions. In this activity, there are three information processing links: language input, intermediate processing and language output. Intermediate processing is the core part. Information processing drives systemic language responses, including those of the oral muscles and nervous system. According to the experimental results, we analyze the psychological factors in the processing of language activity information, study the language processing and information processing modes of deep learning and artificial intelligence in the brain, and the psychological mechanism of English-Chinese alternate language activities. the result predicted by SVM regression is 5.35 m/s, and the prediction error is 4.5%. In addition to the speed prediction, we used the same sample data to predict the direction of the next second. The simulation results this system can predict the language activity.																	1064-1246	1875-8967					2020	38	6			SI		7315	7327		10.3233/JIFS-179806													
J								Delay control system of intelligent traffic scheduling based on deep learning and fuzzy control	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Dispatching system; intelligent transportation; delay control system; deep learning	LESSONS	The intelligent traffic dispatch delay control system is an application system that combines system integration and command dispatch, and system integration serves command dispatch. This system has relatively independent system functions and various research results obtained from multi-data source analysis. Therefore, it is advisable to adopt a technical route of functional modularization and multi-layered architecture. Each functional module relatively completes the required functions independently, and provides a hierarchical service relationship between the modules. Maximizing the efficiency of road use is an important part of urban road traffic control. Urban road traffic control is mainly the control of traffic signals. Simulation results show that this system can effectively improve the efficiency of traffic dispatching, and at the same time has a real-time feedback function. The shortest path can be calculated through vehicle monitoring and combined with the GIS module. The vehicle position information and the shortest path between each vehicle and the accident point are displayed in the map panel, and the operator selects a vehicle to be dispatched according to a vehicle selected on the map and associated with a row in the vehicle information panel table.																	1064-1246	1875-8967					2020	38	6			SI		7329	7339		10.3233/JIFS-179807													
J								Research on English spoken semantic recognition machine learning model based on neural network and statistics fusion	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pronunciation rules; spoken English; semantic recognition; feature analysis; simulation model	ONLINE; PLATFORM	The traditional spoken English semantic recognition model requires a hand-made pronunciation dictionary and a predefined alignment between audio and phonemes, so the recognition is not good. In order to improve the efficiency of spoken English semantic recognition, this study studies the colloquial semantic feature recognition method of English colloquial database under the framework of machine learning, and then studies the semantic recognition of spoken English based on the results of Chinese name recognition. Moreover, this study has carried out a detailed analysis of the traditional speech recognition system, introduced the preprocessing and feature extraction of speech, traditional acoustic models, language models, decoding search and dictionary, etc., which covers the components necessary for the traditional speech recognition system. In addition, this study further analyzes the basic theory of acoustic models and improves the model. Finally, this study designed a controlled trial to perform a performance analysis of the model. The analysis of the experimental results shows that the English spoken language semantic recognition model proposed in this study has a significant improvement over the traditional speech recognition system.																	1064-1246	1875-8967					2020	38	6			SI		7341	7350		10.3233/JIFS-179808													
J								Smart community security monitoring based on artificial intelligence and improved machine learning algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial intelligence; community security identification; smart community; image detection	FEATURE RECOGNITION; TRANSLATION	Smart communities are the main content of smart city construction and a microcosm of urban smart management. Its construction has developed with the advancement of technology. The smart community contains various refined smart scenarios, involving various smart hardware and smart system ecological chain construction. This paper analyze the smart community security monitoring based on artificial intelligence and improved machine learning algorithm. In the deployment of urban surveillance, a smart camera embedded with face recognition technology can transmit facial feature data to a computer center database through a computer network while taking a suspicious portrait, and the system can automatically integrate the facial database of fugitives compared quickly and accurately. Simulation results show that the artificial intelligence mode can effectively improve the accuracy of facial recognition, and at the same time, it is effective for dynamic image monitoring of moving objects. The size of the detected interest point will be reduced by more than 30%, which reduces the calculation amount to some extent and improves the decision efficiency.																	1064-1246	1875-8967					2020	38	6			SI		7351	7363		10.3233/JIFS-179809													
J								Regional enterprise economic development dimensions based on k-means cluster analysis and nearest neighbor discriminant	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										K-means clustering; nearest neighbor discrimination; economic development; regional economy; dimensional analysis		In the analysis of the dimension of economic development, the clustering method has a strong dependence on the selection of the central point. However, the random selection of the central point by the traditional K-means clustering method involves the most sensitive central point selection problem of the clustering method. In order to improve the economic development dimension analysis, this paper improves the initial center selection method based on the traditional K-means clustering method, so that the traditional K-means clustering method is no longer a random selection of initial center, and the problem of local optimal solution is also solved. At the same time, the system operation reduces the number of clustering and iterations and improves the efficiency of the algorithm. In addition, this article uses an example to perform algorithm performance analysis. The results show that the proposed algorithm has certain effects and can provide theoretical reference for subsequent related research.																	1064-1246	1875-8967					2020	38	6			SI		7365	7375		10.3233/JIFS-179810													
J								Research on spoken English analysis model based on transfer learning and machine learning algorithms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Transfer learning; spoken English; speech recognition; system model; feature analysis	PLATFORM	From the current situation, it can be seen that there are certain deficiencies in the current models of spoken English analysis. In order to improve the English spoken analysis effect, this study builds an English spoken analysis model based on transfer learning and analyzes the performance of spoken English recognition. In order to make full use of the characteristics of speech feature modes to compensate for the shortcomings of single mode in speech recognition, this paper proposes a multimodal shared speech feature learning method, that is, multimodal shared speech feature learning method based on locality, sparsity, and identifiable typical correlation analysis. The method introduces locality, sparsity and discriminability, and the method effectively improves the English spoken recognition effect to a certain extent. In addition, this paper designs a controlled experiment to analyze the performance of the system model. The research results show that the algorithm has certain effects and can be applied to practice.																	1064-1246	1875-8967					2020	38	6			SI		7377	7387		10.3233/JIFS-179811													
J								Intelligence computing approach for solving second order system of Emden-Fowler model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Emden-Fowler system; artificial neural network; genetic algorithm; sequential quadratic programming; statistical analysis	GENETIC-ALGORITHM; EQUATIONS; DISPATCH; DESIGN	In this research study, an advance computational intelligence paradigm is used for solving second order Emden -Fowler system (EFS) based on artificial neural network, genetic algorithm (GA) which is a famous global search method, sequential quadratic programming (SQP) known as rapid local refinement and the hybrid of GA-SQP. The proficiency of the designed scheme is inspected by solving the three examples of EFS to check the efficiency, consistency, precision and exactness of the technique. The numerical outcomes of the purposed scheme are compared with the exact solution that shows the significance of the scheme based on accuracy, correctness and convergence. Moreover, statistical explorations have been executed to verify the precision and accuracy of the outcomes based on performance measures of mean absolute deviation, root mean squared error and variance account for.																	1064-1246	1875-8967					2020	38	6			SI		7391	7406		10.3233/JIFS-179813													
J								Definition method for carbon footprint of iron and steel energy supply chain based on relational dispersed degree	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Carbon footprint; supply chain; data preprocessing; accounting		The research on carbon footprint of steel energy supply chain plays an important role in energy conservation and emission reduction. Therefore, this article puts forward a definition method of carbon footprint of iron and steel energy supply chain based on correlation dispersion degree. Firstly, the definition of carbon footprint was given, and the carbon emission of iron and steel industry was calculated. On this basis, the influencing factors were analyzed quantitatively. Meanwhile, the carbon footprint data was processed based on carbon footprint accounting standards, including the carbon footprint data preprocessing, transformation and reduction. According to the preprocessing results, the computation of carbon footprint was checked. Finally, the correlation dispersion degree of carbon footprints in the steel energy supply chain was analyzed, so as to achieve the definition of carbon footprints in the steel energy supply chain. In order to test the validity and feasibility of the proposed method, a simulation was carried out. Simulation results show that the result of the proposed method is close to the actual result, so that the proposed method can accurately get the influence of various factors on CO2 emissions of the steel industry, and thus to present some targeted measures to reduce the carbon emission of steel industry.																	1064-1246	1875-8967					2020	38	6			SI		7407	7416		10.3233/JIFS-179814													
J								Stability detection of building concrete structure based on discrete element method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Discrete element method; buildings; concrete structure; failure criterion; collapse; stability detection	SIMULATION; DEM	In order to improve the safety of building structure, the discrete element method is introduced to test the stability of building concrete structure. Taking reinforced concrete as the research object and using Mohr-Coulomb criterion as the concrete failure criterion, determine the stiffness coefficient, damping coefficient and time step of reinforced concrete. Simulation of failure modes of reinforced concrete beams under different shear-span ratios. Determining the collapse process of reinforced concrete structure under the action of strong earthquake El-Centro north-south wave, and testing the stability of reinforced concrete structure. The results show that at the moment of 3.25 s, the bottom spring of the bottom column of the frame structure is damaged, and the upper end of the right column of the middle two layers is deformed and damaged. At this moment, the frame structure collapses. As the shear-span ratio increases, the failure mode of reinforced concrete beams evolves in the order of diagonal pressure, shear pressure and cable tension. Its shear strength is gradually weakened, and the proposed method can effectively simulate the stability of concrete structures under the action of earthquakes and collapse.																	1064-1246	1875-8967					2020	38	6			SI		7417	7426		10.3233/JIFS-179815													
J								The balance of ecological and economic benefits of sea-buckthorn	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										System theory; liaoning province; index system; weight; entropy weight method	HIPPOPHAE-RHAMNOIDES L.; WASTE; MODEL; CHAOS	Sea-buckthorn, with high economic value, is one of the important tree species for windbreak, sand fixation and conservation of water and soil. Based on this, an equilibrium analysis method for ecological and economic benefits of sea-buckthorn was proposed. After analyzing the ecological function and economic value of sea-buckthorn, based on the system theory, the utilization benefit equilibrium model of sea-buckthorn was constructed from economy and ecology two dimensions. The equilibrium of sea-buckthorn utilization benefit in an evolutionary cycle was divided into low-level symbiosis, coordinated development, limit development and regeneration development four stages. In order to comprehensively reflect the overall situation of sea-buckthorn utilization benefits, an index system including two major systems of social economy and ecological environment was designed, and 12 indicators in the ecological function and economic value analysis results of sea-buckthorn were selected to judge the interaction balance between economic benefits and ecological benefits. Taking Liaoning Province as the research object, the comprehensive economic and ecological benefits of sea-buckthorn utilization between 2004 and 2017 shows that during this period, the growth rate of ecological benefits was slower than that of economic benefits and had fluctuations. From the evolution curve of the equilibrium of utilization benefits, the sea-buckthorn in Liaoning Province is in the coordinated development stage, but the coupling degree has been increasing in recent years, indicating that the utilization benefits of sea-buckthorn in Liaoning Province may enter the limit development stage. According to the research results, this method can effectively analyze the balance of ecological and economic benefits of sea-buckthorn, which is conducive to the maximum value of sea-buckthorn.																	1064-1246	1875-8967					2020	38	6			SI		7427	7436		10.3233/JIFS-179816													
J								Agricultural information resource scheduling algorithm based on firefly algorithm in cloud computing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; firefly algorithm; agricultural information; resource scheduling	GAS	For the problem of slow scheduling speed and poor load balancing of agricultural information resource scheduling in the current cloud computing, an agricultural information resource scheduling algorithm based on firefly algorithm is proposed in this paper. In cloud computing, according to the idea of earliest completing resource scheduling and using the least cost, the agricultural information resource scheduling model with time constraint is constructed. The minimum completion time and scheduling cost of resource scheduling are determined and the objective function of resource scheduling is built. The chaos algorithm is used to optimize the firefly algorithm to solve the objective function and improve the convergence speed of resource scheduling. Agricultural information resource scheduling is achieved according to the convergence results and by using Lagrangian relaxation function. Experimental results show that the proposed algorithm is faster in scheduling agricultural information resources and higher in load balancing.																	1064-1246	1875-8967					2020	38	6			SI		7437	7448		10.3233/JIFS-179817													
J								Intelligent control system of agricultural unmanned tractor tillage trajectory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine vision; unmanned; agricultural tractor; intelligent control algorithm		As the large error and low efficiency of manual tillage, tractor tillage has become the focus of the current research. However, tractor driving requires a lot of manpower and time. Therefore, the overall efficiency of agricultural tillage is reduced. To address this problem, an intelligent control system of agricultural unmanned tractor tillage trajectory based on machine vision is proposed in this paper. First, the hardware of the automatic control system of agricultural unmanned tractor is designed. Then the intelligent control algorithm is proposed to realize the software design, and the image of tractor tillage environment is processed. Finally, the adaptive path tracking algorithm is applied to realize the adaptive tracking of the agricultural unmanned tractor tillage trajectory, so as to realize the trajectory control. Experimental results show that the designed system can accurately track and control the agricultural unmanned tractor tillage trajectory and improve tractor operation efficiency.																	1064-1246	1875-8967					2020	38	6			SI		7449	7459		10.3233/JIFS-179818													
J								Real-time emergency management mode of cold chain logistics for agricultural products under the background of "Internet plus "	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet; agricultural products; cold chain logistics; management mode; supply chain	SYSTEM; WSN	To solve the problems of the traditional cold chain logistics management mode of agricultural products, such as loose circulation, inaccurate information transmission, untimely processing, serious loss of fresh agricultural products, a real-time and accurate management of cold chain logistics is established to study the real-time emergency management mode of cold chain logistics for agricultural products under the background of "Internet+". Method: Under the background of "Internet+", the e-commerce platform and the cold chain logistics network are combined, and the physical real-time emergency management mode of the cold chain of agricultural products is divided into the sub-mode of "production warehouse+cold chain", the sub-mode of "e-commerce+cold chain express+smart cabinet" and the sub-mode of "cold chain storage+chain new retail"; through the circulation efficiency improvement algorithm of cold logistics supply chain, the circulation loss between nodes is reduced, to improve the circulation efficiency of the supply chain; The evaluation method of real-time emergency management mode for cold chain logistics, the entropy weight method and the analytic hierarchy process are selected to jointly determine the weight, and the fuzzy comprehensive evaluation method is used to comprehensively evaluate the emergency management effect of cold chain logistics for agricultural product. Results: The final score of the performance evaluation in enterprise SX's fresh agricultural product supply chain using the emergency management mode is 69.52 points, and the efficiency value of cold product logistics management in the enterprise C is 0.536. Conclusion: The real-time emergency management mode of cold chain logistics of agricultural products studied has certain feasibility, which can accurately analyze the advantages and disadvantages of cold chain logistics management of different enterprises.																	1064-1246	1875-8967					2020	38	6			SI		7461	7473		10.3233/JIFS-179819													
J								Control of automatic seeding robot based on basketball movement capture	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Basketball motion capture; automatic seeding robot; multimedia database; feature extraction; path planning	BIOLOGICAL CELLS	In order to solve the problem that the seeder operated by hand controller lacks high-precision visual function, cannot self-identify the seed target, and has low sowing precision, this paper proposes a new automatic sowing robot control research based on basketball motion capture. Through motion capture, obtain the target characteristics. Based on the target characteristics acquired by the vision module of the robot, the path planning of the robot is realized by rotating a certain parking space Angle on the basis of real-time positioning. The performance test shows that the accurate recognition rate is higher than 95.2%, and the maximum is 98.1%. The accurate seed localization rate was 95.5% and the maximum was 98.2%. By comparing the trajectory precision of the joint space of the auto-seeding robot, the precision of the trajectory control method is better than that of the traditional method. The residual removal rate of this method is 39.87%-46.25%. The effect of different placement methods on seed placement depth showed that the average depth value obtained by this method was 0.001.49, and the depth variation coefficient was 0.70%-2.00%. This method can improve the target recognition rate and positioning accuracy, meet the needs of high precision sowing operations, and increase the crop yield.																	1064-1246	1875-8967					2020	38	6			SI		7475	7485		10.3233/JIFS-179820													
J								Plaintext recovery attack on 3DES algorithm with different byte keys	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Different bytes; key; 3DES algorithm; plaintext; recovery; chaotic key	SECRET-KEY; AGREEMENT	Aiming at the problem of plaintext recovery attack on 3DES algorithm with different byte keys, a plaintext recovery method for 3DES algorithm with different byte keys is proposed. In order to strengthen the anti-attack ability of DES algorithm, 3DES algorithm is used, which includes double-key 3DES of and triple-double-key 3DES. The 3DES algorithm with different seed key length is used to recover the plaintext. The attack algorithm for the first 256-byte plaintext encrypted by 3DES algorithm are given through the single byte deviation rule and double byte deviation rule. The results show that with the increase of the corresponding quantization order, the processing effect of the method is better than that of the chaos mapping method (4.2 dB), and with the increase of the quantization order, the advantage of the method is constantly improved; the attack result of the method on 256 plaintext is shown as the 114th byte to 127th byte from 3072 bytes, so that in practical application, the plaintext encrypted by 3DES algorithm can be recovered better.																	1064-1246	1875-8967					2020	38	6			SI		7487	7495		10.3233/JIFS-179821													
J								Discrete element simulation of coupled vibration of high speed railway track under load	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Load; high speed railway; coupling vibration; discrete element simulation	ASPHALT MIXTURE; SYSTEM	In order to solve the problem of poor simulation accuracy of the traditional method for the coupled vibration of high-speed railway track, a discrete element simulation method for the coupled vibration of high-speed railway track under load is proposed. Based on the coupled vibration equation of high-speed railway track, the coupled vibration equation of high-speed railway track is built on the basis of determining the vector and parameters. The discrete element model is used to analyze the coupled vibration of high-speed railway track under the action of load, and the results of the discrete element simulation experiment of the coupled vibration of high-speed railway track are obtained. In order to verify the simulation accuracy of the proposed discrete element method, a comparative experiment is designed. According to the experimental results, the proposed discrete element simulation method for coupled vibration of high-speed railway track under load greatly improves the simulation accuracy. In the 100th experiment, the simulation accuracy of this method is as high as 93%. The coupling vibration discrete element simulation method has better simulation effect.																	1064-1246	1875-8967					2020	38	6			SI		7497	7508		10.3233/JIFS-179822													
J								False paratactic constructions and symbolic discreteness in the activation diffusion model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Activation diffusion model; language; pseudo paratactic construction; symbol; vocabulary; discreteness		In order to further improve the students' learning ability, discrete analysis of linguistic pseudo-parallel constructions and symbols under the activation diffusion model is performed. Taking English as an example, this paper analyzes the usage of false paratactic construction from the perspectives of violating the restriction rules of paratactic structure and the irreplaceable nature of "and"; briefly introduces the discreteness of English vocabulary and analyzes the metaphorical function of the cognitive mode of lexical discreteness. Based on the improved activation diffusion model, the experiment class uses the activation diffusion model to implement the false parallel construction and vocabulary discrete teaching, while the control class uses the traditional method to implement the false parallel construction and vocabulary discrete teaching. The final results show that the average English score of the experiment class is improved by 14.5%, while the control class is only improved by 3.2%. The results show that the activation diffusion model is helpful to grasp the pseudo paratactic construction and analyze the discreteness of English vocabulary.																	1064-1246	1875-8967					2020	38	6			SI		7509	7520		10.3233/JIFS-179823													
J								IoT perception layer scheduling deadlock relieving optimization method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of things; perception layer; information scheduling; deadlock relieving	FLEXIBLE MANUFACTURING SYSTEMS	When resources are limited and the perception tasks are time-constrained, if multiple perception tasks request the response at the same time, it is easy to cause scheduling deadlocks in the perception layer and affect its access speed and perception efficiency. This paper studies the mathematical feature expression method of task and resource allocation when deadlock occurs in the perception layer. According to the conditions of deadlock mutual exclusion, non preemption, request and hold, cycle waiting, a deadlock detection model of hierarchical scheduling system based on time constraints is adopted to establish the time constraint relationship of the deadlock detector and the genetic algorithm is applied to solve the problem of multi-tasks deadlock relieving strategy optimization in the perceptive layer. The research results and simulation experiments show that the deadlock relieving optimization method based on genetic algorithm can quickly relieve the deadlock, ensure the minimum cost of relieving, and significantly improve the efficiency of deadlock relieving.																	1064-1246	1875-8967					2020	38	6			SI		7521	7529		10.3233/JIFS-179824													
J								Fuzzy comprehensive evaluation on high-quality development of China's rural economy based on entropy weight	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										High-quality development of rural economy; entropy weight; fuzzy comprehensive evaluation; measurement system		It is an inevitable requirement of the realization of Chinese rural revitalizing strategies to achieve high-quality development of rural economy. In this essay, the evaluation system for high-quality development of rural economy in the new era is constructed from six dimensions, including innovative development, infrastructure, ecological environment, economic achievements, economic stability and market mechanism and the method of entropy weight-fuzzy comprehensive evaluation is adopted to measure the levels of high-quality comprehensive development of rural economy of 30 provinces and cities of China in 2017. The research finds that the dimensional weight of ecological environment is the biggest, which is 31.76%, followed by infrastructure, economic achievements, economic stability, innovative development and market mechanism from large to small. The level of high-quality development of rural economy in China is generally low and a majority of the provinces are backward. Therefore, to grasp the development status of Chinese rural economy and clarify balanced development between regions could provide basis for promoting comprehensively high-quality development of Chinese rural economy.																	1064-1246	1875-8967					2020	38	6			SI		7531	7539		10.3233/JIFS-179825													
J								Mechanical property test of OLED bending area based on discrete element method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Relaxation method; OLED; Newton's second law; stress analysis		Gradually, OLED all screen has become one of mainstream mobile display devices. In this technology, the metal wiring in the bending area is prone to breakage, thus forming dead pixels. The yield rate may be affected by it. Therefore, the stress -strain analysis for the bending area in the bending process has become one of the main problems in OLED research. Firstly, the structural characteristics of OLED were analyzed, and the bending technology was introduced and analyzed. Secondly, the test process of discrete element method was analyzed. Finally, the strain analysis model was established through the multi -layer stacking characteristics of bending area. Moreover, the simulation of bending process was completed by quasi -static analysis. Then, the validity of analysis was verified by the test comparison. The results show that the proposed method can not only analyze the mechanical properties of OLED bending area effectively under different cover plate stiffness, and the maximum error rate of the method is only 4.87, which is far lower than the traditional method. Therefore, the content and conclusion provide the further research of mechanical properties of OLED bending area with an important technical foundation.																	1064-1246	1875-8967					2020	38	6			SI		7541	7551		10.3233/JIFS-179826													
J								Numerical analysis of hydrodynamic performance of biomimetic flapping foils based on the RANS method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										RANS method; biomimetic flapping foils; rolling motion; rolling and pitching composite motion; hydrodynamic performance		This paper has investigated hydrodynamic performances of biomimetic flapping foils based on RANS method, and numerical results of aero foil profile of NACA are verified. Besides, propulsion performances of six types of biomimetic flapping foils with pure rolling motion are compared. The influence of phase difference and moving amplitude of rolling and pitching composite motion on propulsion performance is researched and the change of wake stream field is analyzed. The results showthat the hydrodynamic performance of bionic humpback whale flapping foil is better, whose average thrust in one cycle and the maximum propulsive efficiency increase with the increasing flapping frequency; the propulsion performance of rolling and pitching composite motion is superior to that of pure rolling motion, and its better values of phase difference and moving amplitude are respectively 90 degrees and 6 degrees; the leading edge vortex appears two times in one cycle, consistent with the instantaneous thrust numerical results. This method provides a reference value for the design and application of AUV.																	1064-1246	1875-8967					2020	38	6			SI		7553	7562		10.3233/JIFS-179827													
J								A detection method for DC power disturbance data of charging pile based on linear algebra	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mathematical morphology; GK clustering algorithm; signal recognition; SNR	QUALITY DISTURBANCES; CLASSIFICATION	Due to poor noise elimination effect and low detection accuracy in traditional methods, a method to detect DC power disturbance data for charging piles based on linear algebra was presented. Firstly, a filter was constructed based on the mathematical morphology theory of linear algebra to preprocess the disturbed waveform, so as to filter out the random noise and impulse noise in signal. Secondly, the filtered waveform was analyzed according to the change rule of grid. And then, a simple and fast singularity detection criterion was proposed to detect the disturbance accurately and quickly and thus to locate the time. According to the location result, the clustering algorithm based on GK was used to identify various types of disturbance data, so as to make countermeasures. Finally, voltage sag, voltage swell, harmonic wave and the combined disturbances were used to verify the proposed method. The experimental results show that the maximum denoising result of this method is 37.3, and the maximum detection accuracy of disturbance data is 0.952, which is much higher than the traditional method. The results show that the signal-to-noise ratio is larger and the detection effect is better based on the proposed method, so that the Intersection-over-Union (IoU) is improved and the detection accuracy is better. In conclusion, the overall detection performance is better.																	1064-1246	1875-8967					2020	38	6			SI		7563	7573		10.3233/JIFS-179828													
J								Control system of trajectory tracking of discretely-actuated manipulator based on computed torque method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Monocular stereo sensor; drive controller; servo driver; calculation moment method	VEHICLE	In the past, in the process of tracking and controlling the discretely-actuated manipulator, the tracking result has a large error. To this end, a control system of trajectory tracking of discretely-actuated manipulator based on computed torque method was designed. The overall framework of the system was analyzed and designed, and the hardware of the system was designed in an integrated DSP information processing environment. The design of the monocular stereo sensor, drive controller, servo drive, etc. was described in detail. Then the control algorithm, the computed torque method, was analyzed. Finally, with the help of the control platform of discretely-actuated manipulator, the control system of trajectory tracking of discretely-actuated manipulator based on computed torque method was verified. After the different control algorithms were added, the trajectory tracking accuracy of the discretely-actuated manipulator was compared. The experimental results show that the maximum errors of each joint in the system are 0.0568 rad, 0.0347 rad and 0.0044 rad, which are lower than the traditional system, it is shows that the reliability of the system is verified.																	1064-1246	1875-8967					2020	38	6			SI		7575	7584		10.3233/JIFS-179829													
J								Method of suppressing torsional vibration noise of automobile drive-train system based on discrete wavelet	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Torsional vibration mechanics model; noise sampling; vibration signal; differential equation		In order to solve the problem of low efficiency and low efficiency of traditional methods, a method based on discrete wavelet is proposed to suppress the torsional vibration noise of automobile transmission system. Based on the operation mechanism of automobile transmission system, the transmission system model is built. In this model, the features of torsional vibration of transmission system were analyzed and the vibration signals were measured. The hardware equipments were used to collect the noise caused by torsional vibration, and then the discrete wavelet processing technology was used to quantify the torsional vibration noise. Based on the quantized noise data, the torsional vibration noise of automobile driveline was suppressed by active suppression and passive suppression. The experimental results show that the suppression time of this method for different noise size is less than 0.5 s, far less than the traditional method. Experimental results prove that the designed noise suppression method can greatly reduce the torsional vibration noise, so it has good application effect.																	1064-1246	1875-8967					2020	38	6			SI		7585	7594		10.3233/JIFS-179830													
J								Empirical analysis of organizational quality defect management enabling factors identification based on SMT, interval-valued hesitant fuzzy set ELECTRE and QRA methods	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued hesitant fuzzy set ELECTRE for multi-attribute decision-making; organizational quality defect management; quantile regression method; sense-making theory		Quality is of great significance in enterprises, strengthening and identifying enabling factors of quality defect management are one of core goals of organizational operation management. In this study, sense-making theory is adopted to identify organizational quality defect management enabling factors, and interval-valued hesitant fuzzy set ELECTRE for multi-attribute decision-making method and quantile regression analysis method are used to empirically identify and verify organizational quality defect management enabling factors, revealing influence size and scale, influence degree and influence significance level of enabling factors on organizational quality defect management. Empirical analysis results indicate that combination methods of sense-making theory, interval-valued hesitant fuzzy set ELECTRE for multi-attribute decision-making method and quantile regression analysis method can effectively identify organizational quality defect management enabling factors, which will have great promise in the field of organizational quality management and operation management based on the rationality, feasibility and applicability.																	1064-1246	1875-8967					2020	38	6			SI		7595	7608		10.3233/JIFS-179831													
J								Intuitively fuzzy multi-attribute group decision making of organizational quality specificity immune evolution ability based on evidential reasoning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitively fuzzy multi-attribute group decision making; evidential reasoning; organizational quality specificity immune evolution ability	MANAGEMENT-PRACTICES; PERFORMANCE; ENTERPRISES	Combined with the relevant theories and frameworks, this study constructs evaluation indicator and index system of organization quality specificity immune evolution ability. On the basis, this study uses the intuitively fuzzy multi-attribute decision making method based on evidential reasoning to carry out empirical analysis of organizational quality specificity immune evolution ability, empirical analysis data derive from the research objects of relevant experts, typical and representative large-sized manufacturing enterprises. The empirical analysis results indicate that the intuitively fuzzy multi-attribute decision making method based on evidential reasoning considers subjective and objective factors, which integrates the intuitively fuzzy information and gives out the corresponding decision according to the evaluation value. It is also useful, effective, feasible and operable to take the intuitively fuzzy multi-attribute decision making method based on evidential reasoning to evaluate, select and make decision choice on the best and optimal partner and investee object in multi-attribute group decision making according to organization quality specificity immune evolution ability, and give out orders of all partners and investees. The research methods and results can further improve the organization quality specificity immune efficiency, identify organization quality problems and defects, improve organization quality performance, and maintain organization health, which also provide new decision-making method for the multi-attribute groups of organization quality specificity immune evolution ability.																	1064-1246	1875-8967					2020	38	6			SI		7609	7622		10.3233/JIFS-179832													
J								A network intrusion detection system based on convolutional neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional Neural Network; intrusion detection; NSL-KDD; data flow analysis	FEATURE-SELECTION	Intrusion detection systems (IDSs) play an important point in resisting hacker intrusion. With the rapid development of the network technology, network security has received more and more attention from researchers of different fields, and the traditional network security system based on the regular intrusion detection rules cannot meet the growing demand of changeable and timely intrusion prevention. Therefore, the development of efficient IDSs always is an open challenge. Firstly, a novel intrusion detection method based on the Convolutional Neural Network (CNN) is proposed in this paper. Secondly, based on the proposed method, an efficient, real-time and automated intrusion detection system named IDS-CNN is well designed. The system is built by several open source tools, such as the packet capture interface Tcpdump, the traffic analysis Bro and the machine learning interface Tensorflow. The system is composed of data preprocessing, neural network training, network testing and intrusion response based on Linux platform. Finally, through the simulation experiment with NSL-KDD data set and the actual network flow test, the experimental results indicate that the proposed IDS-CNN system can not only complete the intrusion detection for network data streams efficiently, but also its detection precision is better than the state-of-the-art method.																	1064-1246	1875-8967					2020	38	6			SI		7623	7637		10.3233/JIFS-179833													
J								Monitoring study on vertical bearing capacity of pile foundation in soft rock of lhasa human settlements	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lhasa; human settlements; soft rock; geological pile foundation; vertical; bearing capacity	AGE	In view of the weak vertical bearing capacity and low safety performance of the traditional soft rock pile foundation for human settlements, this paper proposes to monitor the vertical bearing capacity of the soft rock pile foundation for human settlements in Lhasa. The vertical mechanism of soft rock is analyzed, including the uniaxial compressive strength of rock around the pile, the relative displacement of pile rock and the interface condition of pile rock. The vertical bearing capacity of the pile foundation in soft rock is calculated by load transfer method. The side friction model and the pile end of the pile are established to determine the pile foundation. The calculation formula of the load settlement curve is obtained by using the side friction model and the pile end resistance model. The vertical bearing capacity of the pile foundation in soft rock is analyzed when the pile side rock is in the elastic stage and the surrounding rock is in the plastic stage. The test results show that friction pile plays an important role in the test pile under the working load. This method can effectively monitor the vertical bearing capacity of pile in the soft rock geological environment of human settlements, and can be applied to practical projects.																	1064-1246	1875-8967					2020	38	6			SI		7639	7650		10.3233/JIFS-179834													
J								Application of discrete fruit fly algorithm in enhancement of wireless sensor node coverage	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Discrete fruit fly algorithm; wireless sensor; node coverage; enhancement; adaptive compensation; classification olfactory	DROSOPHILA	When studying the application of discrete fruit fly algorithm in enhancement of wireless sensor node coverage, the wireless sensor node coverage model is designed according to the node measurement model, and the discrete fruit fly algorithm is used to evaluate each initial population in the wireless sensor node coverage to determine the elite individuals. The number of retentions of the elite individual is calculated in the essence library, and the final result is output when the number of retentions of the elite individual is greater than the minimum number of retentions of the elite individual. Conversely, inter-population immigrations are implemented to form new fruit fly populations. The classification olfactory random search method based on adaptive compensation is adopted in the population. The fruit fly individuals are evaluated by the taste judgment function, and they are visually located to re-determine the optimal individuals of each population. The above steps are repeated until the number of retentions of elite individuals is greater than the minimum number of retentions of elite individuals. Then the optimal solution of the wireless sensor node coverage model is output, and the optimization scheme of the wireless sensor node coverage is designed according to the optimal solution. The experimental analysis shows that when the number of iterations is 50, the maximum node coverage and node fitness are 99.81% and 0.99 respectively. When the number of nodes is 20, the time of the enhanced node is the shortest, 9.8 ms. That is, the method can significantly enhance the node coverage effect.																	1064-1246	1875-8967					2020	38	6			SI		7651	7660		10.3233/JIFS-179835													
J								Singular point region enhancement of fingerprint image based on symmetric phase consistency	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Symmetric phase; consistency; fingerprint; image; singular point; enhancement		Aiming at the problem that Separable Gabor filter can destroy the ridge structure of fingerprint adjacent singular points, and the effect of directional Fourier filter in repairing fingerprint ridges is not obvious, a method for singular point region enhancement of fingerprint image based on symmetric phase consistency is proposed. Fingerprint image is preprocessed to extract effective fingerprint region; the singularity region of the effective fingerprint region is obtained by using the singularity extraction method; the method of singular region feature extraction is used to obtain the singular region feature of fingerprint image; the fuzzy clustering algorithm is used to enhance the singular region feature of fingerprint image, which can enhance the singular region of fingerprint image. The results show that the quality of fingerprint image enhanced by the proposed method is better, the performance of denoising is remarkable, and the efficiency of extracting singular feature of fingerprint image is high.																	1064-1246	1875-8967					2020	38	6			SI		7661	7669		10.3233/JIFS-179836													
J								Based on Copula-CoVaR model of risk spillover effect of oil markets and other commodity markets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Oil markets; commodity markets; risk spillover effect	GOLD	Based on the Copula-CoVaR framework, the Beta-skew-t-EGARCH model is used to capture the leverage effect of financial assets, the thick tail distribution and the conditional skewness. Then, based on the standard deviation, this paper introduces the extreme value theory, combined with the Copula function and the CoVaR method which are used to measure the effect of China's crude oil market on the risk spillover effects of domestic and international commodity markets, and reveals the impact of China's crude oil market. The results show that, regardless of the domestic Crude oil market and international commodity market, domestic crude oil market and domestic commodity market, there is a two-way positive risk spillover effect, which shows asymmetric characteristics. The results provide some theoretical supports for the regulatory authorities to enhance the marketization level of China's crude oil market, some suggestions for the investment institutions to assess the risk level of China's crude oil market, and some reference value for investors to invest in China's crude oil market. Finally, based on the conclusions of the study, this paper provides some targeted recommendations to the development of China's crude oil market.																	1064-1246	1875-8967					2020	38	6			SI		7671	7682		10.3233/JIFS-179837													
J								Simulation on static detection of malicious code based on behavior information gain	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Behavior information gain; malicious code; static detection; feature selection method; dispersion degree	INTERNET	When the malicious code is detected by the current method, the features of the malicious code cannot be classified in detail. A static detection method of malicious code based on behavior information gain is proposed. The method uses the feature selection method of behavior information gain to get the average mutual information between different code types, and gives different types of feature libraries. On this basis, the character sets are fused and the sample characteristics of the malicious code are extracted. The dispersion degree of the sequence distribution of malicious code samples is measured, the distance vector of the plurality of eigenvectors is obtained, and the weighted matching of the features is performed. The malicious code is detected statically based on the matching result. The results demonstrated that the proposed method tends to be more accurate and provides a scientific basis for ensuring the security and stability of the Android operating system.																	1064-1246	1875-8967					2020	38	6			SI		7683	7692		10.3233/JIFS-179838													
J								Dynamic tracking method for multi-frame moving target based on symmetry algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Symmetry algorithm; multi-frame; moving target; tracking	MULTITARGET TRACKING	Aiming at the problems of poor tracking effect and low precision of traditional methods, a symmetric algorithm is introduced to dynamically track moving objects in multiple frames. Symmetric encryption algorithm is used to hide the reversible information of the image, and the correlation between adjacent pixels of the encrypted image is increased through high-bit flip prediction. Generate sparse pixel classification matrix to improve compression efficiency and reduce the occupation of effective bit-plane space. Embed secret information in the most significant bit of the encrypted pixel based on the hidden key to generate an encrypted-portable image. The receiving end can generate two kinds of decrypted images, one is consistent with the original image, and the other contains the secret information. Has hidden key to extract secret information from ciphertext image and plaintext image separately. Under the hiding of target image information, a window adaptive adjustment algorithm is introduced to track the target, and a comprehensive predictor is designed to predict and track the target when the target is temporarily blocked. The experimental results show that the target tracking accuracy of this method can reach 99% and the prediction accuracy can reach 100%. The proposed method has strong security performance and high tracking prediction accuracy, and is a feasible target dynamic tracking method.																	1064-1246	1875-8967					2020	38	6			SI		7693	7703		10.3233/JIFS-179839													
J								Methods of detection by single camera for target pose in visual sorting	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Edge distance vector; declination detection; attitude detection; relative center; contour recognition; fixed point positioning		In the process of visual sorting, in order to solve the problem that the irregular contour target position is difficult to locate quickly and accurately, and it is difficult to realize the self-defined grab point, a method of target pose detection based on the contour center of gravity edge distance vector and local polar coordinates is proposed. Firstly, the target contour is determined by adaptive edge detection and the comparison of contour moments, and the larger distance direction from the contour center of gravity to the smallest outer rectangle is defined as the direction from the contour center of gravity to the edge distance vector. The deflection angle of the target is determined by the vector direction and the position of the outer rectangle to detect the deflection angle of the irregular workpiece. On this basis, judge whether the designated point is the relative center of gravity in the local polar coordinate system, and realize the intelligent positioning of the set point on the target by detecting the coordinates of the designated point in the center of gravity positioning. The experimental results show that the method is suitable for complex contour targets, and the positioning results are accurate and the positioning time is short, so it has high engineering application value.																	1064-1246	1875-8967					2020	38	6			SI		7705	7714		10.3233/JIFS-179840													
J								Application of symmetry algorithm in energy optimal allocation of wireless sensor networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Symmetric algorithm; wireless sensor network; energy optimization; configuration		. Aiming at the problem that the current algorithm cannot effectively complete the optimal energy allocation in wireless sensor networks, a method of optimal energy allocation in wireless sensor networks based on a symmetric algorithm is proposed. The structural characteristics of wireless sensor networks are analyzed, and the constraints of sensor nodes are completed according to the structural characteristics of wireless sensor network nodes. The application scenarios of wireless sensor networks are described, the energy distribution of the network is analyzed, and an optimal energy allocation model is established. The energy consumption per unit time of wireless sensor networks is calculated by the energy consumption of data collection and reception. Calculate the optimal routing hops and minimum transmit power in the model. Combined with the number of network keys, the optimal energy distribution of the wireless sensor network can be completed. Experimental results show that the algorithm is effective by testing the time and energy allocation efficiency of network optimization, and it also guarantees the security of wireless sensor network communication.																	1064-1246	1875-8967					2020	38	6			SI		7715	7724		10.3233/JIFS-179841													
J								The diffusion of intelligent manufacturing applications based SIR model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intelligent manufacturing; industrial policies; technology diffusion		At present, manufacturing industry is obviously experiencing the rapid development of intelligent manufacturing on a global scale. To meet the needs in the new era, China has continuously been introducing new measures to develop smart manufacturing, and remarkable results have been achieved for the development of smart manufacturing. This paper first summarizes the diffusion model for applying intelligent manufacturing in manufacturing enterprises in Hangzhou. Secondly, the improved SIR model and Matlab software are used to analyze the impact of industrial policy accuracy on the proliferation of intelligent manufacturing applications. Finally based on the simulation results, it is found that the policies play an important role in the diffusion of intelligent manufacturing applications. The good top-down design of the government is the prerequisite for the sound promotion of intelligent manufacturing technology application, but the accuracy of policies needs to be further strengthened to better promote the proliferation of smart manufacturing applications.																	1064-1246	1875-8967					2020	38	6			SI		7725	7732		10.3233/JIFS-179842													
J								Automatic detection and precise location of texture image defect based on ambiguity resolution algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ambiguity resolution algorithm; texture image; defect; automatic detection; precise location		Most of the detail structure characteristic information of the texture image is lost due to the space variation during the automatic detection, which reduces the precision of auto detection and precise location. In order to solve the problem, a method based on the ambiguity resolution algorithm is proposed in this article. The background texture information was eliminated via the Gaussian filter and threshold value method to acquire initial rough edge of the defect area. In the stage of precision location, the modified Chan-Vcsc active contour model based on the fuzzy energy was introduced and the precision location for the defect area was achieved via level set method using the rough edge as the initial curve of the model evolution. Experimental results show that the proposed method can achieve accurate and automatic localization of different types of defect areas, and has high computational efficiency. When the number of samples is 600, the positioning time is only 19 s. When the peak signal-to-noise ratio reaches 100 dB, the absolute error of this method is only 0.01%, the relative error is 0.04%, and the positioning accuracy is as high as 99.58%.																	1064-1246	1875-8967					2020	38	6			SI		7733	7741		10.3233/JIFS-179843													
J								Text information classification method based on secondly fuzzy clustering algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Preprocessing; text classification; feature selection; classifier		Traditional artificial information processing and classification methods can not deal with large-scale information well, and there is a problem of low classification purity. In view of the above situation, a new text information classification method based on quadratic fuzzy clustering algorithm is proposed. Firstly, in the method, text preprocessing is carried out, including Chinese word segmentation and de stop words, then text feature selection is made, and vector space model is used to represent the text. Finally, based on the secondly fuzzy clustering algorithm, a classifier is constructed to achieve text information classification. The results show that: compared with the traditional artificial information processing and classification method, when using the method based on the secondly fuzzy clustering algorithm for text information classification, the F value is more than 90%, and the average purity is higher, which shows that the classification performance of this method is better.																	1064-1246	1875-8967					2020	38	6			SI		7743	7754		10.3233/JIFS-179844													
J								Design of data transmission system for 3D laser scanning of liquefied gas railway tanker based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy algorithm; 3D laser scanning; data transmission system; design	FREQUENCY; SCHEME; FIBER; TIME	Aiming at the problem of high packet loss rate in the traditional 3D laser scanning data transmission system, a 3D laser scanning data transmission system based on fuzzy algorithm is designed. The operational amplifier is used as the analog front-end of ADC, and the high-performance controller is selected to meet the needs of system development. By programming the FPGA chip, the state control of the transmission module and the configuration of each chip are realized. The data is preprocessed by three-dimensional point cloud, the data information of control system is obtained according to fuzzy set and fuzzy rule reasoning, Gigabit Ethernet is initialized, the data is transmitted through UDP user data protocol package, and the research on three-dimensional laser scanning data transmission system of liquefied gas railway tank car based on fuzzy algorithm is completed. The experimental results show that the designed system can effectively reduce the packet loss rate, the highest packet loss rate is only 22%, which makes up for the shortcomings of the traditional system, and is more suitable for the three-dimensional laser scanning data transmission of liquefied gas railway tank car.																	1064-1246	1875-8967					2020	38	6			SI		7755	7766		10.3233/JIFS-179845													
J								Data evaluation method for ceramic 3D printing samples based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy algorithm; ceramic material; 3D printing; sample data; data evaluation		In order to improve the evaluation score and accuracy of ceramic 3D printing sample data, a fuzzy algorithm based evaluation method of ceramic 3D printing sample data is proposed. The data of ceramic 3D printing samples were collected and preprocessed by fuzzy algorithm. The specific content of the evaluation index of 3D printing sample data is determined, and the evaluation system of 3D printing sample data is established. Fuzzy algorithm is used to determine the weight of data quality evaluation index of ceramic 3D printing samples. According to the steps of setting up the weight matrix of the evaluation index of 3D printed ceramic sample data, the weight matrix of the evaluation index is established. Because there are many data sources of ceramic 3D printing samples, there may be data conflicting with the description of the same object. Under the background of fuzzy algorithm, the effective dimension of ceramic 3D printing sample data is established. According to the membership function of each evaluation index, the fuzzy set of 3D printing sample data of ceramics is established. Finally, the data evaluation process is designed to realize the data evaluation of ceramic 3D printing samples based on fuzzy algorithm. The experimental results show that the evaluation accuracy of the method is higher than 96%, and the scores of users and experts on the interior design scheme are above 80 points.																	1064-1246	1875-8967					2020	38	6			SI		7767	7778		10.3233/JIFS-179846													
J								A method for interval-valued intuitionistic fuzzy multiple attribute decision making based on fuzzy entropy	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued intuitionistic fuzzy sets; multi-attribute decision making; TOPSIS method; improved fuzzy entropy		With respect to multi-attribute decision making problem, in which attribute values are expressed in interval-valued intuitionistic fuzzy numbers, a decision making method based on fuzzy entropy and TOPSIS is presented. In this paper, the score value function and precision function of interval-valued intuitionistic fuzzy numbers are defined, and the attribute weight determination method based on improved fuzzy entropy is given. Then, by using the ranking method of interval-valued intuitionistic fuzzy numbers, the positive and negative ideal solutions of interval-valued intuitionistic fuzzy multiple attribute decision making problems are obtained. On this basis, an interval-valued intuitionistic fuzzy multi-attribute decision making method based on TOPSIS is proposed. Finally, the method is applied to the evaluation of local government public finance expenditure performance and its effectiveness is illustrated by a numerical example.																	1064-1246	1875-8967					2020	38	6			SI		7779	7785		10.3233/JIFS-179847													
J								Optimization of operation safety risk indicator based on grey relational and sensitivity analysis of the south-to-north water diversion project	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Operational safety; correlation degree; sensitivity; maximum deviation; indicator optimization	SELECTION; QUALITY; INDEX; MODEL	To ensure that the safety risk indicator system of the south-to-north water diversion project is both scientific and independent, an indicator optimization method based on correlation and sensitivity analysis is proposed. The 3D grey correlation degree among the indicators is obtained by maximum deviation. At the same time, the principal component analysis method is adopted to calculate risk sensitivity based on the preliminary analysis on the risk indicators. The correlation degree reflects whether it is possible for the connotation overlapping between the risk indicators, and the sensitivity reflects the indicators' influence on the system. The purpose of the research is to optimize the risk system through removing the less sensitive one of the two relatively correlated indicators, and the calculation results prove the effectiveness and feasibility of the method.																	1064-1246	1875-8967					2020	38	6			SI		7787	7793		10.3233/JIFS-179848													
J								A general threshold GARCH process with volatility asymmetry	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Volatility asymmetry; GARCH model; threshold value; maximum likelihood method; Monte Carlo simulation	STATIONARITY; VARIANCE	Through considering the volatility asymmetry in financial markets, we extend the threshold autoregressive condi- tional heteroskedasticity model to the general threshold autoregressive conditional heteroskedasticity model with asymmetry. The sufficient conditions for the existence of the stationary solution and the finiteness of the moments of the model are derived. The parameters and the threshold value of the model are estimated using the quasi-maximum likelihood method. The results of Monte Carlo simulation method show that the method performs well. By applying this method to the Shanghai daily return and volatility series with the significant leverage effect, we find that the asymmetric model with general threshold better reflects the asymmetric fluctuation characteristics than the original model.																	1064-1246	1875-8967					2020	38	6			SI		7795	7801		10.3233/JIFS-179849													
J								Structural equation modeling of the intelligent manufacturing entrepreneurship's network characteristics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Network characteristics; corporate entrepreneurship; entrepreneurial competence; SMEs; structural equation modeling	SOCIAL-STRUCTURE; PROMISE; FIELD	The entrepreneurship of intelligent manufacturing enterprises in manufacturing clusters have significant influences on China's economic growth in the process of industry 4.0. This paper reveals the entrepreneurship of intelligent manufacturing enterprises by the analysis of network theory. By the questionnaire of 470 effective sample enterprises in the 6 manufacturing industrial clusters selected, Structural Equation Modeling and the AMOS software are used to verify the theoretical hypotheses. The findings are as follows: first, entrepreneurial competence has a positive effect on corporate entrepreneurship; second, network relation, betweenness centrality and network scale have positive effect on entrepreneurial competence; third, network relationship strength and network density have different effects on entrepreneurial competence. Based on the results of empirical analysis, this paper puts forward some policy suggestions to promote enterprise entrepreneurship in industrial clusters.																	1064-1246	1875-8967					2020	38	6			SI		7803	7811		10.3233/JIFS-179850													
J								Research on the weight calculation of social benefit evaluation of Chinese film and TV enterprises based on fuzzy comprehensive evaluation method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy comprehensive evaluation; film and TV enterprises; benefit evaluation; weight calculation		With the development of science and technology, as well as the improvement of living standards, the calculation of social benefit evaluation has been considered as one of the key points for Chinese film and TV enterprises to achieve a sustainable development in the future. The exploration of seeking improvement paths through actively responding to changes in consumer demands and improving social benefits, as well as applying calculation and analysis of evaluation weights, has attracted widespread attention from the academic community. Based on the fuzzy comprehensive evaluation method, this paper applied the concept of fuzzy computing, and conducted a weighted analysis of the social benefits of Chinese film and TV enterprises through questionnaire and analysis of large sample survey data. Accordingly, it proposed the countermeasures for Chinese film and TV enterprises to achieve innovative development in the future.																	1064-1246	1875-8967					2020	38	6			SI		7813	7824		10.3233/JIFS-179851													
J								Vehicle recognition model for complex scenarios based on human memory mechanism	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Vehicle recognition; human memory; complex environment; cognitive	ATTENTION	The prevailing vehicle recognition technology is adversely affected by the environment such as complex traffic scenarios and weather conditions. This paper proposes a robust vehicle recognition model based on human memory mechanism named Memory-based Vehicle Recognition Model (MVRM). Motivated by the success of memory and attention mechanism, we explore some features of human visual attention model. Fusing short term and long-term memory modules together yield deeper architectures recognizing increasing complex environmental scenarios. Firstly, a rare motion feature has been introduced to measure the visual salience, which improves the accuracy of the visual attention mechanism. Second, a model of vehicle salient region recognition has been established. The results of experiments show that the dynamic vehicle recognition rate of MVRM is 77.10%, while its false recognition rate has only a nominal value of similar to 4.5%. Furthermore, the model offers good recognition of vehicle targets under complex environment conditions related to weather and road traffic.																	1064-1246	1875-8967					2020	38	6			SI		7825	7835		10.3233/JIFS-179852													
J								Supply chain finance credit risk evolving intelligent analysis system based on system dynamic model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Supply chain finance; credit risk; systematic dynamic; evolving mechanism; intelligent analysis system	TRADE CREDIT; LOT	Supply chain finance (SCF) plays a prominent role in solving the small -middle enterprises financing difficulties. But its credit risk is easy to conduct and magnify by the supply chain relation to develop into system risk. In the paper the main structure and causal relationship of SCF credit risk system are analyzed. And then the intelligent analysis system model of SCF credit risk evolving mechanism is constructed based on systematic dynamic, which simulates how the key factors act to each other and how they function to SCF credit risk. The intelligent analysis results show that: (1) The sub -system of bank, core enterprise, small -mid enterprise and risk intermediary interact and constitute the SCF credit risk system. (2) The credit risk management strategy execution and the management infrastructure input of the bank sub -system can promote improving the management base of core enterprise and small -mid enterprises. (3) To increase the management efficiency of core enterprise and small -mid enterprises can improve enterprises operating level and solvency, and then decrease the SCF credit risk. And also, the core enterprise influences the SMEs? solvency by actively function to the operation ability. (4) The increasing of risk intermediary can accelerate to the SCF loan quantity, and at the same time, reduce the credit risk.																	1064-1246	1875-8967					2020	38	6			SI		7837	7847		10.3233/JIFS-179853													
J								Intelligent logistics service combination algorithm based on Internet of Things	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of Things; intelligent logistics; service composition; web service; logistics service		This paper proposes the concept of an intelligent logistics system (ILS) in the context of the Internet of Things (IoT), expounds the characteristics of the ILS, and constructs a logical framework of the ILS based on the IoT. In the dynamic IoT environment, in order to obtain a highly satisfied logistics service, a locally optimal choice of logistics service selection scheme is proposed. This scheme uses the traditional web service combination idea to solve the intelligent logistics service problem. First, model and calculate the Quality of Service (QoS) attributes in logistics, and then select an atomic solution with the best utility value from each logistics sub-process, perform service composition and execute. The feasibility and effectiveness of the model and algorithm under this scheme are verified by experiments.																	1064-1246	1875-8967					2020	38	6			SI		7849	7856		10.3233/JIFS-179854													
J								Fuzzy comprehensive evaluation of innovation capability of Chinese national high-tech zone based on entropy weight-taking the northern coastal comprehensive economic zone as an example	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										National high-tech zone; northern coastal comprehensive economic zone; fuzzy comprehensive evaluation; innovation ability		The National High-tech Zone is an important driving force for the transformation and upgrading of the industrial structure and it is also an important guarantee for the realization of an innovation-driven China. Thirteen indexes are selected from three aspects of innovation investment, innovation performance and innovation support to construct a comprehensive evaluation index system of innovation capability of national high-tech zones in the northern coastal comprehensive economic zone. On this basis, the entropy weight-fuzzy comprehensive evaluation model is used to evaluate the innovation and develop-ment capability of 19 national high-tech zones from 2015 to 2017 comprehensively. The results show that the comprehensive index of innovation capability of national high-tech zones shows an upward trend, but the overall innovation capability is relatively low; The contribution of each subsystem to innovation capability is different, the innovation environment is the largest, the innovation performance is the second, and the innovation investment is the smallest; According to the level of innovation ability, 19 high-tech zones are divided into star type, moderate type and backward type, with moderate type high-tech zones accounting for the largest proportion.																	1064-1246	1875-8967					2020	38	6			SI		7857	7864		10.3233/JIFS-179855													
J								Control method of wheel slip rate based on fuzzy algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										ESC system; least square optimization algorithm; controller; optimal slip rate	OPTIMIZATION	The existing control method of wheel slip rate has a long braking time, the gap between the slip rate and the optimal slip rate is obvious, and there is a defect of poor control effect. In order to solve the above problems, the fuzzy algorithm is introduced to design the wheel slip control method. According to the characteristics of ESC system, the control model of wheel slip rate was built. On this basis, the least square optimization algorithm was used to estimate the best slip rate. Based on the estimated optimal slip rate, the multi -agent system was used to design the slip rate controller, and thus to get the threshold value of slip rate controller. Moreover, the fuzzy algorithm was used to optimize the parameters of slip rate controller, so as to keep the optimal slip ratio. Finally, the control for wheel slip ratio based on fuzzy algorithm was achieved. The simulation results show that the slip ratio of this method is kept between 0.18-0.45, which is less different from the optimal slip ratio. Compared with the existing control method of wheel slip ratio, the proposed control method of wheel slip ratio greatly improves the control effect, which fully shows that the proposed control method of wheel slip ratio has better performance.																	1064-1246	1875-8967					2020	38	6			SI		7865	7874		10.3233/JIFS-179856													
J								Compound control method for overcoming transmission delay impact on networked control inverter for AC microgrid	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Network-based control; transmission delay; compound control method; AC microgrid	TIME-DELAY; STABILITY ANALYSIS; FREQUENCY CONTROL; COMPENSATION; ALGORITHM; SYSTEM	Recent studies show that power electronic system performance can be improved by combining with communica-tion technologies, however, time-delay impact that is introduced by network could cause performance degradation. As for networked control inverters, such delay results in an increase in the THD of the output. To solve the time-delay impact, in this paper, we propose a wireless network control inverter architecture. It transmits the information of AC microgrid by Zigbee and a compound control method (internal model control and quasi-PR control) of networked control inverter is proposed. The architecture enables remote control of the inverter and wireless parallel connection, the method can either reduce the THD increase and the circumfluence of wireless parallel inverters. The proposed method is concretely applied in networked control system of quasi-PR controlled AC microgrid in grid mode. The simulation and experimental results are presented to prove the proposed control on several operating condition, which provide a competive performance.																	1064-1246	1875-8967					2020	38	6			SI		7875	7892		10.3233/JIFS-179857													
J								Analysis and improvement of image segmentation algorithm based on fuzzy edge compensation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Compensation term; edge segmentation; image enlargement; interpolation	INTERPOLATION	In order to solve the problems of mosaics and fuzzy details in traditional image interpolation and magnification algorithms, an image magnification algorithm based on compensation item edge segmentation is proposed. According to the boundary degree and pixel distance, the boundary line is extracted from the image, and then the plane area, the adja-cent boundary line and the boundary line are respectively enlarged by using their own adaptive interpolation amplification algorithm. The linear interpolation algorithm based on the one-dimensional compensation term is adopted to obtain the new interpolation edge points between each point, and retain the boundary information and texture details information, so as to enhance the image magnification It can improve the resolution and definition of the image. Experiments show that the algorithm can effectively improve the magnification effect of the image, retain the details of the image, and is conducive to image processing.																	1064-1246	1875-8967					2020	38	6			SI		7893	7902		10.3233/JIFS-179858													
J								Big data attribute selection method in distributed network fault diagnosis database	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Distributed network; fault diagnosis database; big data attribute selection; support vector machine; fault diagnosis database	ALGORITHM	The selection of big data attributes plays a positive role in the development of the network. At present, the attribute selection for big data is completed by detecting the attribute of data, which can not guarantee the accuracy of the selection. In this paper, a big data attribute selection method based on support vector machine (SVM) is proposed for distributed network fault diagnosis database. The method is used to mine big data in the distributed network fault diagnosis database, and calculate its attribute weights according to which complete attribute classification, so as to complete the selection if big data attributes. Experiments show that the proposed method improves the efficiency of big data attribute selection, and has certain practical value.																	1064-1246	1875-8967					2020	38	6			SI		7903	7914		10.3233/JIFS-179859													
J								Artificial intelligence control algorithm for steering motion of wheeled soccer robot	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wheeled soccer robot; steering motion; D'Alembert's principle; double eccentric mass		In order to improve the function of driving of wheeled soccer robot and solve the time delay of the pivot steering motion of traditional robot, an artificial intelligent control algorithm for steering motion of wheeled soccer robotwas proposed. Combined with the calculation of double eccentric steering motion control, the driving characteristic of wheeled soccer robot steering eccentric mass block was analyzed. Then, D'Alembert's principle was used to analyze the stress of robot and design the kinematics model of spherical shell steering of spherical mobile robot. Moreover, the stick-slip principle was used to control the steering of wheeled soccer robot. Thus, the steering motion control of robot was achieved. Experiment results prove that the steering speed of wheeled soccer robot is 30% higher than that of traditional robot, which effectively solves the problem about the time delay of traditional robot steering motion.																	1064-1246	1875-8967					2020	38	6			SI		7915	7923		10.3233/JIFS-179860													
J								3D martial arts image classification algorithm based on symmetry theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Denoising; sharpening; SDBN network; mean filtering; gray uniformity	SYSTEM	Aiming at the problems of large error and poor robustness of traditional image classification methods, a three-dimensional martial arts image classification algorithm based on symmetry theory is proposed. According to the preprocessed 3D martial arts image, the classification algorithm based on symmetric neural network is used to realize the classification of 3D martial arts image. The experimental results show that the minimum error rate of this algorithm is 6.1%, which is far lower than the traditional algorithm, it is shows that the improved algorithm has higher definition, better robustness, and the test results of different topological nodes on the test solution show that the average error rate of the algorithm is lower. Compared with the same type of algorithm, the application value of the proposed algorithm is significant.																	1064-1246	1875-8967					2020	38	6			SI		7925	7934		10.3233/JIFS-179861													
J								Data stable aggregation algorithm based on fuzzy algorithm in cloud computing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; fuzzy algorithm; data; stable convergence; clustering		In order to improve the ability of cloud computing data scheduling, this paper proposes a new method for multi-task, multi-level cloud computing data aggregation based on fuzzy association feature extraction. Heterogeneous directed graph analysis method is used to design cloud computing data. The semantic correlation fusion method is used to implement cloud computing data feature extraction and adaptive scheduling. The fuzzy clustering is used to process the characteristic amount of cloud computing data, and the optimal aggregation of cloud computing data is realized. Simulation results show that the method has a higher recall rate for multi-task and multi-level cloud data aggregation, and the highest recall rate can reach 1, which improves the accuracy of resource aggregation.																	1064-1246	1875-8967					2020	38	6			SI		7935	7944		10.3233/JIFS-179862													
J								Summarization for online reviews based on hierarchical attention network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Document summarization; attention network; review analysis		With the rapid growth of online reviews, the rapid and effective access to summary information is a hot topic of research. In this paper, an abstract sentence extraction model based on hierarchical attention network is proposed. The model has an encoder-decoder structure, which introduces a two-layer attention mechanism. The sentence encoder uses attention mechanism to obtain the vectorized representation of the sentence by inputting the aspect words. The review document encoder uses the attention mechanism to realize the context association of the sentences before and after. When decoding, a sentence output component composed of the GRU network makes the choice whether the sentence should be selected as the candidate sentence. Greedy algorithm is used to eliminate the redundantly result, and then obtains the final summary based on the sentence sorting method. The experiment results show that the proposed model is better than the benchmark model.																	1064-1246	1875-8967					2020	38	6			SI		7945	7952		10.3233/JIFS-179863													
J								Mining Spatial-Temporal Similarity for Visual Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hydrogen; Fuels; Marine vehicles; Ammonia; Liquids; Propulsion; Cathodes; Spatial-temporal similarity; correlation filter; visual object tracking	OBJECT TRACKING; LONG-TERM	Correlation filter (CF) is a critical technique to improve accuracy and speed in the field of visual object tracking. Despite being studied extensively, most existing CF methods suffer from failing to make the most of the inherent spatial-temporal prior of videos. To address this limitation, as consecutive frames are eminently resemble in most videos, we investigate a novel scheme to predict targets & x2019; future state by exploiting previous observations. Specifically, in this paper, we propose a prediction based CF tracking framework by learning the spatial-temporal similarity of consecutive frames for sample managing, template regularization, and training response pre-weighting. We model the learning problem theoretically as a novel objective and provide effective optimization algorithms to solve the learning task. In addition, we implement two CF trackers with different features. Extensive experiments are conducted on three popular benchmarks to validate our scheme. The encouraging results demonstrate that the proposed scheme can significantly boost the accuracy of CF tracking, and the two trackers achieve competitive performances against state-of-the-art trackers. We finally present a comprehensive analysis on the efficacy of our proposed method and the efficiency of our trackers to facilitate real-world visual tracking applications.																	1057-7149	1941-0042					2020	29						8107	8119		10.1109/TIP.2020.2981813													
J								Reliable Multi-Kernel Subtask Graph Correlation Tracker	IEEE TRANSACTIONS ON IMAGE PROCESSING										Target tracking; Correlation; Kernel; Feature extraction; Robustness; Laplace equations; Layered multi-subtask learning; correction filter tracking; temporal-spatial consistency; object tracking	OBJECT TRACKING	Many astonishing correlation filter trackers pay limited concentration on the tracking reliability and locating accuracy. To solve the issues, we propose a reliable and accurate cross correlation particle filter tracker via graph regularized multi-kernel multi-subtask learning. Specifically, multiple non-linear kernels are assigned to multi-channel features with reliable feature selection. Each kernel space corresponds to one type of reliable and discriminative features. Then, we define the trace of each target subregion with one feature as a single view, and their multi-view cooperations and interdependencies are exploited to jointly learn multi-kernel subtask cross correlation particle filters, and make them complement and boost each other. The learned filters consist of two complementary parts: weighted combination of base kernels and reliable integration of base filters. The former is associated to feature reliability with importance map, and the weighted information reflects different tracking contribution to accurate location. The second part is to find the reliable target subtasks via the response map, to exclude the distractive subtasks or backgrounds. Besides, the proposed tracker constructs the Laplacian graph regularization via cross similarity of different subtasks, which not only exploits the intrinsic structure among subtasks, and preserves their spatial layout structure, but also maintains the temporal-spatial consistency of subtasks. Comprehensive experiments on five datasets demonstrate its remarkable and competitive performance against state-of-the-art methods.																	1057-7149	1941-0042					2020	29						8120	8133		10.1109/TIP.2020.3009883													
J								Speeding Up VP9 Intra Encoder With Hierarchical Deep Learning-Based Partition Prediction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Machine learning; Image coding; Databases; Encoding; Video coding; Task analysis; Video codecs; VP9; video encoding; block partitioning; intra prediction; convolutional neural networks; machine learning	QUALITY ASSESSMENT	In VP9 video codec, the sizes of blocks are decided during encoding by recursively partitioning 64 x 64 superblocks using rate-distortion optimization (RDO). This process is computationally intensive because of the combinatorial search space of possible partitions of a superblock. Here, we propose a deep learning based alternative framework to predict the intra-mode superblock partitions in the form of a four-level partition tree, using a hierarchical fully convolutional network (H-FCN). We created a large database of VP9 superblocks and the corresponding partitions to train an H-FCN model, which was subsequently integrated with the VP9 encoder to reduce the intra-mode encoding time. The experimental results establish that our approach speeds up intra-mode encoding by 69.7% on average, at the expense of a 1.71% increase in the BjontegaardDelta bitrate (BD-rate). While VP9 provides several built-in speed levels which are designed to provide faster encoding at the expense of decreased rate-distortion performance, we find that our model is able to outperform the fastest recommended speed level of the reference VP9 encoder for the good quality intra encoding configuration, in terms of both speedup and BD-rate.																	1057-7149	1941-0042					2020	29						8134	8148		10.1109/TIP.2020.3011270													
J								Deep-Like Hashing-in-Hash for Visual Retrieval: An Embarrassingly Simple Method	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Neural networks; Machine learning; Optimization; Computer architecture; Task analysis; Computational modeling; Cascaded hashing; hashing-in-hash architecture; multi-level learning; image retrieval	ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH; CODES	Existing hashing methods have yielded significant performance in image and multimedia retrieval, which can be categorized into two groups: shallow hashing and deep hashing. However, there still exist some intrinsic limitations among them. The former generally adopts a one-step strategy to learn the hashing codes for discovering the discriminative binary feature, but the latent discriminative information in the learned hashing codes is not well exploited. The latter, as deep neural network based hashing models, can learn highly discriminative and compact features, but relies on large-scale data and computation resources for numerous network parameters tuning with back-propagation optimization. Straightforward training of deep hashing models from scratch on small-scale data is almost impossible. Therefore, in order to develop efficient but effective learning to hash algorithm that depends only on small-scale data, we propose a novel non-neural network based deep-like learning framework, i.e. multi-level cascaded hashing (MCH) approach with hierarchical learning strategy, for image retrieval. The contributions are threefold. First, a hashing-in-hash architecture is designed in MCH, which inherits the excellent traits of traditional neural networks based deep learning, such that discriminative binary features that are beneficial to image retrieval can be effectively captured. Second, in each level the binary features of all preceding levels and the visual appearance feature are simultaneously cascaded as inputs of all subsequent levels to retrain, which fully exploits the implicated discriminative information. Third, a basic learning to hash (BLH) model with label constraint is proposed for hierarchical learning. Without loss of generality, the existing hashing models can be easily integrated into our MCH framework. We show experimentally on small- and large-scale visual retrieval tasks that our method outperforms several state-of-the-arts.																	1057-7149	1941-0042					2020	29						8149	8162		10.1109/TIP.2020.3011796													
J								Semantics-Preserving Graph Propagation for Zero-Shot Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Object detection; Task analysis; Visualization; Motorcycles; Bicycles; Correlation; Zero-shot object detection; semantic embedding; semantic relation; graph propagation		Most existing object detection models are restricted to detecting objects from previously seen categories, an approach that tends to become infeasible for rare or novel concepts. Accordingly, in this paper, we explore object detection in the context of zero-shot learning, i.e., Zero-Shot Object Detection (ZSD), to concurrently recognize and localize objects from novel concepts. Existing ZSD algorithms are typically based on a strict mapping-transfer strategy that suffers from a significant visual-semantic gap. To bridge the gap, we propose a novel Semantics-Preserving Graph Propagation model for ZSD based on Graph Convolutional Networks (GCN). More specifically, we develop a graph construction module to flexibly build category graphs by leveraging diverse correlations between category nodes; this is followed by two semantics-preserving graph propagation modules that enhance both category and region representations. Benefiting from the multi-step graph propagation process, both the semantic description and structural knowledge exhibited in prior category graphs can be effectively leveraged to boost the generalization capability of the learned projection function. Experiments on existing seen/unseen splits of three popular object detection datasets demonstrate that the proposed approach performs favorably against state-of-the-art ZSD methods.																	1057-7149	1941-0042					2020	29						8163	8176		10.1109/TIP.2020.3011807													
J								Semantics in the Edge: Sensors and actuators in the Web of Linked Data and Things	SEMANTIC WEB										Internet of Things (IoT); Web of Things (WoT); Cyber-Physical Systems; Ontology; Knowledge Representation; semantics; linked data	ONTOLOGY; INDUSTRY																		1570-0844	2210-4968					2020	11	4					571	580		10.3233/SW-200379													
J								Weather data publication on the LOD using SOSA/SSN ontology	SEMANTIC WEB										Semantic Sensor Ontology; Sensor; Observation; Sample; and Actuator; Climate Linked Data; meteorological observation	WEB	This paper presents an RDF dataset of meteorological measurements. The measurements come from one weather station at the Irstea experimental farm located in Montoldre. The measurements have been made from August 2018 until now. They have been transformed and published as Linked Open Data (LOD). The data schema is based on the new version of the Semantic Sensor Network ontology. This ontology version integrates the Sensor, Observation, Sample, and Actuator pattern. We first present the network of ontologies used to organize the data. Then, the transformation process for publishing the dataset is detailed. To conclude we present some use cases of queries related to Irstea research projects.																	1570-0844	2210-4968					2020	11	4					581	591		10.3233/SW-200375													
J								Ontologies for observations and actuations in buildings: A survey	SEMANTIC WEB										Observations; actuations; sensor; actuator; ontology; BIM	IOT; INTERNET; INDUSTRY; THINGS; MODEL; REPRESENTATION; ENVIRONMENT; NETWORK; IMPACT; SCHEMA	Spaces and elements in the built environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on a principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?																	1570-0844	2210-4968					2020	11	4					593	621		10.3233/SW-200378													
J								EDR: A generic approach for the distribution of rule-based reasoning in a Cloud-Fog continuum	SEMANTIC WEB										Distributed reasoning; SWoT; Semantic Fog computing; SHACL rules	SEMANTIC WEB; INTERNET; ONTOLOGY; EDGE	The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes accord-ing to the transformation of the network topology. For this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.																	1570-0844	2210-4968					2020	11	4					623	654		10.3233/SW-200377													
J								A systematic survey of temporal requirements of bio-health ontologies	SEMANTIC WEB										Ontology; bio-health; temporal; OWL; description logics; requirement		The Description Logic SROIQ(D), as the logical core of the W3C standard Web Ontology Language (OWL 2), is a widely used formalism for ontologies in the life sciences. Bio-health applications including health-care and life science domains commonly have a need to represent temporal information such as medication frequency or stage-based development. Different classes of temporal phenomena may generate different sorts of requirements on SROIQ(D) or extensions of SROIQ(D). In this paper, we deliver the first precise investigation into identifying exactly what kinds of temporal requirements are most important for bio-health ontologies. We conduct an empirical investigation of the OBO Foundry using a bespoke methodological approach by searching each of its ontologies for specific temporal features and go on to calculate the importance of these features using a sophisticated set of measures. By doing so, we derive a formal set of Temporal Requirements that act as a set of guidelines which a language or logical extension to OWL 2 would need to satisfy in order to meet the temporal requirements of bio-health ontologies.																	1570-0844	2210-4968					2020	11	4					657	688		10.3233/SW-190357													
J								Survey on complex ontology matching	SEMANTIC WEB										Ontology matching; complex alignment; survey; schema matching	SEMANTIC MAPPINGS; EVOLUTION; INTEGRATION; WEB; GENERATION; FRAMEWORK; ALIGNMENT; RDF	Simple ontology alignments, largely studied in the literature, link a single entity of a source ontology to a single entity of a target ontology. A limitation of these alignments is their lack of expressiveness which can be overcome by complex align-ments. While diverse state-of-the-art surveys mainly review the matching approaches in general, to the best of our knowledge, there is no study of the specificities of the complex matching problem. In this paper, a review of the different complex matching approaches is provided. It proposes a classification of the complex matching approaches based on their specificities (i.e., type of correspondences, guiding structure). The evaluation aspects and the limitations of these approaches are also discussed. Insights for future work in the field are provided.																	1570-0844	2210-4968					2020	11	4					689	727		10.3233/SW-190366													
J								Phish webpage classification using hybrid algorithm of machine learning and statistical induction ratios	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										phish webpage; machine learning; optimised feature occurrence; OFC; phishing induction ratio; PIR; hybrid feature-based classifier; HFBC	ARCHITECTURE; WEBSITES; FEATURES; MODEL	Although the conventional machine learning-based anti-phishing techniques outperform their competitors in phishing detection, they are still targeted by zero-hour phish webpages due to their constraints of phishing induction. Therefore, phishing induction must be boosted up with the extraction of new features, the selection of robust subsets of decisive features, the active learning of classifiers on a big webpage stream. In this paper, we propose a hybrid feature-based classification algorithm (HFBC) for decisive phish webpage classification. HFBC hybridises two statistical criteria optimised feature occurrence (OFC) and phishing induction ratio (PIR) with the induction settings of the most salient machine learning algorithms, Naive bays and decision tree. Additionally, we propose two constituent algorithms of features extraction and features selection for holistic phish webpage characterisation. The superiority of our proposed approach is justified and proven throughout chronological, real-time, and comparative analyses against existing machines learning-based anti-phishing techniques.																	1759-1163	1759-1171					2020	12	3					255	276		10.1504/IJDMMM.2020.108727													
J								Proposal and study of statistical features for string similarity computation and classification	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										word comparison; string similarity; classification; statistical features; text mining; optical character recognition; OCR; text plagiarism; text entailment; supervised learning		Adaptations of features commonly applied in the field of visual computing, co-occurrence matrix (COM) and run-length matrix (RLM), are proposed for the similarity computation of strings in general (words, phrases, codes and texts). The proposed features are not sensitive to language related information. These are purely statistical and can be used in any context with any language or grammatical structure. Other statistical measures that are commonly employed in the field such as longest common subsequence, maximal consecutive longest common subsequence, mutual information and edit distances are evaluated and compared. In the first synthetic set of experiments, the COM and RLM features outperform the remaining state-of-the-art statistical features. In 3 out of 4 cases, the RLM and COM features were statistically more significant than the second best group based on distances (P-value < 0.001). When it comes to a real text plagiarism dataset, the RLM features obtained the best results.																	1759-1163	1759-1171					2020	12	3					277	307		10.1504/IJDMMM.2020.108731													
J								Weighted LSTM for intrusion detection and data mining to prevent attacks	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										cloud security breaches; intrusion-detection; weight of evidence; WoE; deep learning; long short-term memory; LSTM	LEARNING ALGORITHMS	The usage of cloud opportunities brings not only resources and storage availability, but puts also customer's privacy at stake. These services are carried out through web that generate log files. These files contain valuable information in tracking malicious behaviours. However, they are variant, voluminous and have high velocity. This paper structures input log files using data preparation treatment (DPT), anticipates missing features, and performs a weighted conversion to ease the discrimination of malicious activities. Regarding the robustness of deep learning in analysing high dimension databases, selecting dynamically features and detecting intrusions, our architecture avails its strength and proposes a weighted long short-term memory (WLSTM) deep learning algorithm. WLSTM mine network traffic predictors considering past events, and minimizes the vanishing gradient. Results prove its effectiveness; it achieves 98% of accuracy and reduces false alarm rates to 1.47%. For contextual malicious behaviours, the accuracy attained 97% and the loss was 22%.																	1759-1163	1759-1171					2020	12	3					308	329		10.1504/IJDMMM.2020.108728													
J								Extracting useful reply-posts for text forum threads summarisation using quality features and classification methods	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										information retrieval; initial-post replies pairs; text data; text forum threads; TFThs; text forum threads summarisation; text summarisation; thread retrieval		Text forums threads have a large amount of information furnished by users who discuss on a specific topic. At times, certain thread reply-posts are entirely off-topic, thereby deviating from the main discussion. It negatively affects the user's preference to continue replying to the discussion. Thus, there is a possibility that the user prefers to read certain selected reply-posts that provide a short summary of the topic of the discussion. The objective of the paper is to choose quality reply-posts regarding a topic considered in the initial-post, which also serve a brief summary. We offer an exhaustive examination of the conversational patterns of the threads on the basis of 12 quality features for analysis. These features can ensure selection of relevant reply-posts for the thread summary. Experimental outcomes obtained using two datasets show that the presented techniques considerably enhanced the performance in selecting initial-post replies pairs for text forum threads summarisation.																	1759-1163	1759-1171					2020	12	3					330	349		10.1504/IJDMMM.2020.108725													
J								Performance of authorship attribution classifiers with short texts: application of religious Arabic fatwas	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										authorship attribution; AA; stylomatric features; SF; attribution classifiers; JGAAP tool; Arabic language	IDENTIFICATION; MESSAGES	Although authorship attribution is a well-known problem in authorship analysis domain, researches on Arabic contexts are still limited. In addition, examining the performance of the attribution methods on training set with short textual documents is also not considered well in other languages, such as English, Chinese, Spanish and Dutch. Therefore, this current work aims at examining the performance of attribution classifiers in the context of short Arabic textual documents. The experimental part of this work is conducted with well-known classifiers namely: decision tree C4.5 method, naive Bayes model, K-NN method, Markov model, SMO and Burrows Delta method. We experiment with various features combination. The results show that combining the word-based lexical features with the structural features yields the best accuracy. At this end, we use this combination as a baseline for further investigation. We also examine the effect of combining the n-gram features. The results indicate that some classifiers show an improvement while the others do not. In addition, the results show that the naive Bayes method gives the highest accuracy among all the attribution classifiers.																	1759-1163	1759-1171					2020	12	3					350	364		10.1504/IJDMMM.2020.108719													
J								Context-Aware Graph Label Propagation Network for Saliency Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										saliency detection; superpixel pooling; graph neural network	OBJECT DETECTION; MODEL	Recently, a large number of existing methods for saliency detection have mainly focused on designing complex network architectures to aggregate powerful features from backbone networks. However, contextual information is not well utilized, which often causes false background regions and blurred object boundaries. Motivated by these issues, we propose an easy-to-implement module that utilizes the edge-preserving ability of superpixels and the graph neural network to interact the context of superpixel nodes. In more detail, we first extract the features from the backbone network and obtain the superpixel information of images. This step is followed by superpixel pooling in which we transfer the irregular superpixel information to a structured feature representation. To propagate the information among the foreground and background regions, we use a graph neural network and self-attention layer to better evaluate the degree of saliency degree. Additionally, an affinity loss is proposed to regularize the affinity matrix to constrain the propagation path. Moreover, we extend our module to a multiscale structure with different numbers of superpixels. Experiments on five challenging datasets show that our approach can improve the performance of three baseline methods in terms of some popular evaluation metrics.																	1057-7149	1941-0042					2020	29						8177	8186		10.1109/TIP.2020.3002083													
J								MCMT-GAN: Multi-Task Coherent Modality Transferable GAN for 3D Brain Image Synthesis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Gallium nitride; Magnetic resonance imaging; Image generation; Image segmentation; Task analysis; Biomedical imaging; Synthesis; GANs; anatomical structure; multi-modality; brain MRI	SUPERRESOLUTION	The ability to synthesize multi-modality data is highly desirable for many computer-aided medical applications, e.g. clinical diagnosis and neuroscience research, since rich imaging cohorts offer diverse and complementary information unraveling human tissues. However, collecting acquisitions can be limited by adversary factors such as patient discomfort, expensive cost and scanner unavailability. In this paper, we propose a multi-task coherent modality transferable GAN (MCMT-GAN) to address this issue for brain MRI synthesis in an unsupervised manner. Through combining the bidirectional adversarial loss, cycle-consistency loss, domain adapted loss and manifold regularization in a volumetric space, MCMT-GAN is robust for multi-modality brain image synthesis with visually high fidelity. In addition, we complement discriminators collaboratively working with segmentors which ensure the usefulness of our results to segmentation task. Experiments evaluated on various cross-modality synthesis show that our method produces visually impressive results with substitutability for clinical post-processing and also exceeds the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						8187	8198		10.1109/TIP.2020.3011557													
J								Bayesian Closed Surface Fitting Through Tensor Products	JOURNAL OF MACHINE LEARNING RESEARCH										3-d shapes; Bayesian nonparametrics; Imaging; Manifold learning; Splines; Tensors	CONVERGENCE-RATES; DESIGN; BEZIER	Closed surfaces provide a useful model for 3-d shapes, with the data typically consisting of a cloud of points in R-3. The existing literature on closed surface modeling focuses on frequentist point estimation methods that join surface patches along the edges, with surface patches created via Bezier surfaces or tensor products of B-splines. However, the resulting surfaces are not smooth along the edges and the geometric constraints required to join the surface patches lead to computational drawbacks. In this article, we develop a Bayesian model for closed surfaces based on tensor products of a cyclic basis resulting in infinitely smooth surface realizations. We impose sparsity on the control points through a double-shrinkage prior. Theoretical properties of the support of our proposed prior are studied and it is shown that the posterior achieves the optimal rate of convergence under reasonable assumptions on the prior. The proposed approach is illustrated with some examples.																	1532-4435						2020	21						1	26															
J								Agnostic Estimation for Misspecified Phase Retrieval Models	JOURNAL OF MACHINE LEARNING RESEARCH											REGRESSION	The goal of noisy high-dimensional phase retrieval is to estimate an 8-sparse parameter beta* is an element of R-d from n realizations of the model Y = (X-inverted perpendicular beta*)(2) + epsilon. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which Y = f (X-inverted perpendicular beta*, epsilon) with unknown f and Cov(Y, (X-inverted perpendicular beta*)(2)) > 0. For example, MPR encompasses Y = h(vertical bar X-inverted perpendicular beta*vertical bar) + epsilon with increasing h as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of beta*. Furthermore, we prove that our procedure is minimax optimal over the class of MPR models. Interestingly, our minimax analysis characterizes the statistical price of misspecifying the link function in phase retrieval models. Our theory is backed up by thorough numerical results.																	1532-4435						2020	21						1	39															
J								Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering	JOURNAL OF MACHINE LEARNING RESEARCH										BATSE; DEMP; DEMP; DBSCAN*; density peaks algorithm; GRB; GSL-NN; k-clips; k-means; k(m)-means; kernel density estimation; KNOB-SynC; MixModCombi; MGHD; MSAL; overlap; PGMM; SDSS; spectral clustering; TiK-means	PROBABILITY DENSITY-FUNCTION; GAMMA-RAY BURSTS; MIXTURE COMPONENTS; SIMULATING DATA; FUNCTIONAL MRI; PERFORMANCE; ACTIVATION; PREDICTION; DISCOVERY; SOFTWARE	Commonly-used clustering algorithms usually find ellipsoidal, spherical or other regular-structured clusters, but are more challenged when the underlying groups lack formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial clustering algorithm that can be used with k-means and other algorithms. Our approach estimates the cumulative distribution function of the normed residuals from an appropriately fit k-groups model and calculates the estimated nonparametric overlap between each pair of clusters. Groups with high pairwise overlap are merged as long as the estimated generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in several datasets and can be applied to datasets with scatter or incomplete records. The approach is also used to identify the distinct kinds of gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and the distinct kinds of activation in a functional Magnetic Resonance Imaging study.																	1532-4435						2020	21																						
J								Apache Mahout: Machine Learning on Distributed Dataflow Systems	JOURNAL OF MACHINE LEARNING RESEARCH												APACHE MAHOUT is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as APACHE SPARK and APACHE FLINK. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org.																	1532-4435						2020	21																						
J								AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models	JOURNAL OF MACHINE LEARNING RESEARCH										explainability; interpretability; transparency; taxonomy; open source		As artificial intelligence algorithms make further inroads in high-stakes societal applications, there are increasing calls from multiple stakeholders for these algorithms to explain their outputs. To make matters more challenging, different personas of consumers of explanations have different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360, an open-source Python toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics (http://aix360.mybluemix.net). Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of interpretation and explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. The toolkit is not only the software, but also guidance material, tutorials, and an interactive web demo to introduce AI explainability to different audiences. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.																	1532-4435						2020	21								1														
J								Probabilistic Learning on Graphs via Contextual Architectures	JOURNAL OF MACHINE LEARNING RESEARCH										Structured domains; deep graph networks; graph neural networks; deep learning; maximum likelihood; graph classification; node classification	NEURAL-NETWORK; MARKOV-MODELS; CLASSIFICATION	We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature.																	1532-4435						2020	21																						
J								Convergence of Sparse Variational Inference in Gaussian Processes Regression	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian processes; approximate inference; variational methods; Bayesian non-parameterics; kernel methods	EIGENVALUES; MATRIX; ERROR	Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, N, due to the cubic (in N) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on M << N inducing variables to form an approximation at a cost of O(NM2). While the computational cost appears linear in N, the true complexity depends on how M must scale with N to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how M needs to grow with N to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with M << N. Specifically, for the popular squared exponential kernel and D-dimensional Gaussian distributed covariates, M = O ((log N)(D)) suffice and a method with an overall computational cost of O N (log N)(2D)(log log N)(2)) can be used to perform inference.																	1532-4435						2020	21								1														
J								A General System of Differential Equations to Model First-Order Adaptive Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										Adaptive algorithms; convex and non-convex optimization; first-order methods; differential equation; forward Euler discretization	LONG-TIME BEHAVIOR; OPTIMIZATION	First-order optimization algorithms play a major role in large scale machine learning. A new class of methods, called adaptive algorithms, were recently introduced to adjust iteratively the learning rate for each coordinate. Despite great practical success in deep learning, their behavior and performance on more general loss functions are not well understood. In this paper, we derive a non-autonomous system of differential equations, which is the continuous time limit of adaptive optimization methods. We study the convergence of its trajectories and give conditions under which the differential system, underlying all adaptive algorithms, is suitable for optimization. We discuss convergence to a critical point in the non-convex case and give conditions for the dynamics to avoid saddle points and local maxima. For convex loss function, we introduce a suitable Lyapunov functional which allows us to study its rate of convergence. Several other properties of both the continuous and discrete systems are briefly discussed. The differential system studied in the paper is general enough to encompass many other classical algorithms (such as Heavy Ball and Nesterov's accelerated method) and allow us to recover several known results for these algorithms.																	1532-4435						2020	21								1														
J								Convergence Rates for the Stochastic Gradient Descent Method for Non-Convex Objective Functions	JOURNAL OF MACHINE LEARNING RESEARCH										stochastic gradient descent; mini-batch algorithm; machine learning; non-convex optimization		We prove the convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily locally convex nor contracting objective functions. In particular, the analysis relies on a quantitative use of mini-batches to control the loss of iterates to non-attracted regions. The applicability of the results to simple objective functions arising in machine learning is shown.																	1532-4435						2020	21								1														
J								Distributed Minimum Error Entropy Algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										Information theoretic learning; minimum error entropy; distributed method; semi-supervised data; reproducing kernel Hilbert space	REGULARIZATION; OPERATORS; RATES	Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators.																	1532-4435						2020	21																						
J								Tensor Regression Networks	JOURNAL OF MACHINE LEARNING RESEARCH										Machine Learning; Tensor Methods; Tensor Regression Networks; Low-Rank Regression; Tensor Regression Layers; Deep Learning; Tensor Contraction	BRAIN STRUCTURE; DECOMPOSITIONS; CONNECTOME; PATH; AGE	Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.																	1532-4435						2020	21																						
J								Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers	JOURNAL OF MACHINE LEARNING RESEARCH										distributed optimization; stochastic gradient descent	APPROXIMATION; ERROR	We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.																	1532-4435						2020	21																						
J								A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks	JOURNAL OF MACHINE LEARNING RESEARCH										real-time recurrent learning; backpropagation through time; approximation; biologically plausible learning; local; online	BACKPROPAGATION	We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing these algorithms on two parametric task families shows that performances cluster according to our criteria. Although a similar clustering is also observed for pairwise gradient alignment, alignment with exact methods does not explain ultimate performance. This suggests the need for better comparison metrics.																	1532-4435						2020	21																						
J								Monte Carlo Gradient Estimation in Machine Learning	JOURNAL OF MACHINE LEARNING RESEARCH										gradient estimation; Monte Carlo; sensitivity analysis; score-function estimator; pathwise estimator; measure-valued estimator; variance reduction	PERTURBATION ANALYSIS; SENSITIVITY-ANALYSIS; AUTOMATIC DIFFERENTIATION; APPROXIMATION; OPTIMIZATION; INFERENCE	This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies-the pathwise, score function, and measure valued gradient estimators-exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.																	1532-4435						2020	21																						
J								A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning	JOURNAL OF MACHINE LEARNING RESEARCH										Stochastic optimization; large-scale learning; asynchronous methods; parallel algorithms	COORDINATE DESCENT METHOD; GLOBAL CONVERGENCE; SUPERLINEAR CONVERGENCE; BFGS METHOD; CONVEX	We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. RAPSA is doubly stochastic since each processor utilizes a random set of functions to compute the stochastic gradient associated with a randomly chosen sets of variable coordinates. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is strongly convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset.																	1532-4435						2020	21																						
J								Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization	JOURNAL OF MACHINE LEARNING RESEARCH										Continuous submodularity; non-monotone submodular maximization; approximation algorithms		In this paper we study the fundamental problems of maximizing a continuous non-monotone submodular function over the hypercube, both with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1/2-approximation algorithm for continuous submodular function maximization; this approximation factor of 1/2 is the best possible for algorithms that only query the objective function at polynomially many points. For the special case of DR-submodular maximization, i.e. when the submodular function is also coordinate-wise concave along all coordinates, we provide a different 1/2-approximation algorithm that runs in quasi-linear time. Both these results improve upon prior work (Bian et al., 2017a,b; Soma and Yoshida, 2017). Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.																	1532-4435						2020	21																						
J								A Regularization-Based Adaptive Test for High-Dimensional Generalized Linear Models	JOURNAL OF MACHINE LEARNING RESEARCH										Adaptive Test; Truncated Lasso Penalty; Gene-Environmental Interaction	ENVIRONMENT INTERACTIONS; SIMULTANEOUS INFERENCE; CONFIDENCE-INTERVALS; VARIABLE SELECTION; ASSOCIATION TEST; 2-SAMPLE TEST; POWERFUL; LIKELIHOOD; SET	In spite of its urgent importance in the era of big data, testing high-dimensional parameters in generalized linear models (GLMs) in the presence of high-dimensional nuisance parameters has been largely under-studied, especially with regard to constructing powerful tests for general (and unknown) alternatives. Most existing tests are powerful only against certain alternatives and may yield incorrect Type I error rates under high-dimensional nuisance parameter situations. In this paper, we propose the adaptive interaction sum of powered score (aiSPU) test in the framework of penalized regression with a non-convex penalty, called truncated Lasso penalty (TLP), which can maintain correct Type I error rates while yielding high statistical power across a wide range of alternatives. To calculate its p-values analytically, we derive its asymptotic null distribution. Via simulations, its superior finite-sample performance is demonstrated over several representative existing methods. In addition, we apply it and other representative tests to an Alzheimer's Disease Neuroimaging Initiative (ADNI) data set, detecting possible gene-gender interactions for Alzheimer's disease. We also put R package "aispu" implementing the proposed test on GitHub.																	1532-4435						2020	21																						
J								Fast Bayesian Inference of Sparse Networks with Automatic Sparsity Determination	JOURNAL OF MACHINE LEARNING RESEARCH										Gaussian Graphical Models; Structure Learning; Tuning Free; Time Complexity; Variational Bayes; Variance Reduction; Decaying Recursive Stochastic Gradient (DRSG)	INVERSE COVARIANCE ESTIMATION; FUNCTIONAL CONNECTIVITY; GRAPHICAL MODELS; SYSTEMIC RISK; SELECTION; HORSESHOE; CONSTRUCTION; ESTIMATOR	Structure learning of Gaussian graphical models typically involves careful tuning of penalty parameters, which balance the tradeoff between data fidelity and graph sparsity. Unfortunately, this tuning is often a "black art" requiring expert experience or brute-force search. It is therefore tempting to develop tuning-free algorithms that can determine the sparsity of the graph adaptively from the observed data in an automatic fashion. In this paper, we propose a novel approach, named BISN (Bayesian inference of Sparse Networks), for automatic Gaussian graphical model selection. Specifically, we regard the off-diagonal entries in the precision matrix as random variables and impose sparse-promoting horseshoe priors on them, resulting in automatic sparsity determination. With the help of stochastic gradients, an efficient variational Bayes algorithm is derived to learn the model. We further propose a decaying recursive stochastic gradient (DRSG) method to reduce the variance of the stochastic gradients and to accelerate the convergence. Our theoretical analysis shows that the time complexity of BISN scales only quadratically with the dimension, whereas the theoretical time complexity of the state-of-the-art methods for automatic graphical model selection is typically a third-order function of the dimension. Furthermore, numerical results show that BISN can achieve comparable or better performance than the state-of-the-art methods in terms of structure recovery, and yet its computational time is several orders of magnitude shorter, especially for large dimensions.																	1532-4435						2020	21								1														
J								Detection of IoT-botnet attacks using fuzzy rule interpolation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of things; fuzzy rule interpolation; botnet attack; intrusion detection system	INTERNET	Recently, the Internet of Things (IoT) has been used in technology for different aspects to increase the efficiency and comfort of human life. Protecting the IoT infrastructure is not a straightforward task. There is an urgent need to handle different attack scenarios within the IoT smart environment. Attackers continuously targeted the modern aspects of technology, and trying abusing these technologies using complex attack scenarios such as Botnet attacks. Botnet attacks considered a serious challenge faces of the IoT smart environment. In this paper, we introduce a novel idea that capable of supporting the detecting of IoT-Botnet attack and in meanwhile to avoid the issues associated with the deficiencies of the knowledge-based representation and the binary decision. This paper aims to introduce a detection approach for the IoT-BotNet attack by using the Fuzzy Rule Interpolation (FRI). The FRI reasoning methods added a benefit to enhance the robustness of fuzzy systems and effectively reduce the system's complexity. These benefits help the Intrusion Detection System (IDS) to generate more realistic and comprehensive alerts. The proposed approach was applied to an open-source BoT-IoT dataset from the Cyber Range Lab of the center of UNSW Canberra Cyber. The proposed approach was tested, evaluated and obtained a 95.4% detection rate. Moreover, it effectively smooth the boundary between normal and IoT-BotNet traffics because of its fuzzynature, as well as, it had the ability to generate the required IDS alert in case of the deficiencies of the knowledge-based representation.																	1064-1246	1875-8967					2020	39	1					421	431		10.3233/JIFS-191432													
J								Cluster ensemble of valid small clusters	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Graph representation; cluster ensemble; kmeans clustering; small cluster	COMBINING MULTIPLE CLUSTERINGS; ITERATIVE FUSION; SELECTION; FRAMEWORK; ALGORITHM; CONSENSUS; QUALITY; DIVERSITY; STABILITY; IMPROVE	During the last decade, ensemble clustering has been the subject of many researches in data mining. In ensemble clustering, several basic partitions are first generated and then a function is used for the clustering aggregation in order to create a final partition that is similar to all of the basic partitions as much as possible. Ensemble clustering has been proposed to enhance efficiency, strength, reliability, and stability of the clustering. A common slogan concerning the ensemble clustering techniques is that "the model combining several poorer models is better than a stronger model". Here at this paper, an ensemble clustering method is proposed using the basic k-means clustering method as its base clustering algorithm. Also, this study could raise the diversity of consensus by adopting some measures. Although our clustering ensemble approach has the strengths of kmeans, such as its efficacy and low complexity, it lacks the drawbacks which the kmeans suffers from; such as its problem in detection of clusters that are not uniformly distributed or in the circular shape. In the empirical studies, we test the proposed ensemble clustering algorithm as well as the other up-to-date cluster ensembles on different data-sets. Based on the experimental results, our cluster ensemble method is stronger than the recent competitor cluster ensemble algorithms and is the most up-to-date clustering method available.																	1064-1246	1875-8967					2020	39	1					525	542		10.3233/JIFS-191530													
J								Cluster ensemble of valid small clusters	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Graph representation; cluster ensemble; kmeans clustering; small cluster	COMBINING MULTIPLE CLUSTERINGS; ITERATIVE FUSION; SELECTION; FRAMEWORK; ALGORITHM; CONSENSUS; QUALITY; DIVERSITY; STABILITY; IMPROVE	During the last decade, ensemble clustering has been the subject of many researches in data mining. In ensemble clustering, several basic partitions are first generated and then a function is used for the clustering aggregation in order to create a final partition that is similar to all of the basic partitions as much as possible. Ensemble clustering has been proposed to enhance efficiency, strength, reliability, and stability of the clustering. A common slogan concerning the ensemble clustering techniques is that "the model combining several poorer models is better than a stronger model". Here at this paper, an ensemble clustering method is proposed using the basic k-means clustering method as its base clustering algorithm. Also, this study could raise the diversity of consensus by adopting some measures. Although our clustering ensemble approach has the strengths of kmeans, such as its efficacy and low complexity, it lacks the drawbacks which the kmeans suffers from; such as its problem in detection of clusters that are not uniformly distributed or in the circular shape. In the empirical studies, we test the proposed ensemble clustering algorithm as well as the other up-to-date cluster ensembles on different data-sets. Based on the experimental results, our cluster ensemble method is stronger than the recent competitor cluster ensemble algorithms and is the most up-to-date clustering method available.																	1064-1246	1875-8967					2020	39	1					525	542		10.3233/JIFS-191530													
J								Advanced reliability analysis method for mechanisms based on uncertain measure	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertainty quantification; mechanism reliability; reliability index; uncertainty theory; belief reliability	OPTIMIZATION METHOD; TIME	In traditional mechanism reliability analysis, probability theory or statistical approaches are employed. However, these methods cannot be used under lack of data and great epistemic uncertainty. In this paper, an advanced mechanism reliability analysis method is put forward based on uncertain measure. To satisfy the subadditivity of epistemic uncertainties, a novel uncertainty quantification method based on uncertainty theory is proposed for mechanism reliability analysis. Then, a point kinematic reliability analysis method combined with uncertain measure is presented to calculate the kinematic uncertainty reliability of motion mechanism at each time instant. Three models are developed for estimating kinematic uncertainty reliability. Furthermore, first-order Taylor series expansion is used to solve nonlinear limit state functions. A new kinematic uncertainty reliability index (KURI) is presented based on normal uncertainty distribution. Finally, by applying the proposed method to a numerical experiment, the trend of uncertainty reliability was found to be consistent with the traditional method. The two practical engineering applications show that the presented method are more reasonable compared with the classical approaches when the information of design parameters is insufficient.																	1064-1246	1875-8967					2020	39	1					1045	1059		10.3233/JIFS-191970													
J								Controllable Image Processing via Adaptive FilterBank Pyramid	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tuning; Task analysis; Image processing; Filter banks; Smoothing methods; Adaptation models; Adaptive systems; Controllable image processing; image restoration; adaptive filterbank		Traditional image processing operators often provide some control parameters to tweak the final results. Recently, different convolutional neural networks have been used to approximate or improve these operators. However, in those methods, one single model can only handle one operator of a specific parameter value and does not support parameter tuning. In this paper, we propose a new plugin module, "Adaptive Filterbank Pyramid", which can be inserted into a backbone network to support multiple operators and continuous parameter tuning. Our module explicitly represents one operator with one filterbank pyramid. To generate the results of a specific operator, the corresponding filterbank pyramid is convolved with the intermediate feature pyramid produced by the backbone network. The weights of the filterbank pyramid are directly regressed by another sub-network, which is jointly trained with the backbone network and adapted to the input parameter, thus enabling continuous parameter tuning. We applied the proposed module for a large variety of image processing tasks, including image smoothing, image denoising, image deblocking, image enhancement and neural style transfer. Experiments show that our method is generalized to different types of image processing tasks and different backbone network structures. Compared to the single-operator-single-parameter baseline, our method can produce comparable results but is significantly more efficient in both training and testing.																	1057-7149	1941-0042					2020	29						8043	8054		10.1109/TIP.2020.3009844													
J								Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Training; Feature extraction; Semantics; Machine learning; Clustering algorithms; Unsupervised learning; Convolutional neural networks; unsupervised learning; feature clustering		The usage of convolutional neural networks (CNNs) for unsupervised image segmentation was investigated in this study. Similar to supervised image segmentation, the proposed CNN assigns labels to pixels that denote the cluster to which the pixel belongs. In unsupervised image segmentation, however, no training images or ground truth labels of pixels are specified beforehand. Therefore, once a target image is input, the pixel labels and feature representations are jointly optimized, and their parameters are updated by the gradient descent. In the proposed approach, label prediction and network parameter learning are alternately iterated to meet the following criteria: (a) pixels of similar features should be assigned the same label, (b) spatially continuous pixels should be assigned the same label, and (c) the number of unique labels should be large. Although these criteria are incompatible, the proposed approach minimizes the combination of similarity loss and spatial continuity loss to find a plausible solution of label assignment that balances the aforementioned criteria well. The contributions of this study are four-fold. First, we propose a novel end-to-end network of unsupervised image segmentation that consists of normalization and an argmax function for differentiable clustering. Second, we introduce a spatial continuity loss function that mitigates the limitations of fixed segment boundaries possessed by previous work. Third, we present an extension of the proposed method for segmentation with scribbles as user input, which showed better accuracy than existing methods while maintaining efficiency. Finally, we introduce another extension of the proposed method: unseen image segmentation by using networks pre-trained with a few reference images without re-training the networks. The effectiveness of the proposed approach was examined on several benchmark datasets of image segmentation.																	1057-7149	1941-0042					2020	29						8055	8068		10.1109/TIP.2020.3011269													
J								Fractal Dimension of Color Fractal Images With Correlated Color Components	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Fractals; Correlation; Complexity theory; Color; Mathematical model; Histograms; Color fractal dimension; color fractal images with correlated components; midpoint displacement; color fractal image generation	ALGORITHM; TEXTURE	We mathematically prove that color fractal images with two and three correlated color components generated with the midpoint displacement approach obey the property of self-similarity, thus enabling the estimation of their color fractal dimension. We generate various sets of color fractal images with two and three correlated color components, controlled both by the Hurst parameter and the variance-covariance matrix, with and without a global normalization, and use them for the calibration of the embraced color fractal dimension estimator. We improve the existing fractal dimension estimator based on probabilistic box-counting by reducing the variance of the regression line estimators through the iterative elimination of most error-ed measurement points. We independently estimate the variance-covariance matrix and Hurst parameter for the sets of generated color fractal images with correlated color components. We show the experimental results and discuss both the improvements and the limitations of the proposed approach.																	1057-7149	1941-0042					2020	29						8069	8082		10.1109/TIP.2020.3011283													
J								Latent Complete Row Space Recovery for Multi-View Subspace Clustering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Clustering algorithms; Clustering methods; Video surveillance; Sparse matrices; Tensile stress; Unsupervised learning; Approximation algorithms; Multi-view clustering; subspace clustering; latent representation; row space recovery	ALGORITHM; SHAPE	Multi-view subspace clustering has been applied to applications such as image processing and video surveillance, and has attracted increasing attention. Most existing methods learn view-specific self-representation matrices, and construct a combined affinity matrix from multiple views. The affinity construction process is time-consuming, and the combined affinity matrix is not guaranteed to reflect the whole true subspace structure. To overcome these issues, the Latent Complete Row Space Recovery (LCRSR) method is proposed. Concretely, LCRSR is based on the assumption that the multi-view observations are generated from an underlying latent representation, which is further assumed to collect the authentic samples drawn exactly from multiple subspaces. LCRSR is able to recover the row space of the latent representation, which not only carries complete information from multiple views but also determines the subspace membership under certain conditions. LCRSR does not involve the graph construction procedure and is solved with an efficient and convergent algorithm, thereby being more scalable to large-scale datasets. The effectiveness and efficiency of LCRSR are validated by clustering various kinds of multi-view data and illustrated in the background subtraction task.																	1057-7149	1941-0042					2020	29						8083	8096		10.1109/TIP.2020.3010631													
J								Unsupervised Feature Selection via Data Reconstruction and Side Information	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature selection; side information; data reconstruction error; the graph embedding		Data reconstruction, which aims at preserving statistical properties of the data during the reconstruction has become a new criterion for feature selection. Although feature selection could benefit from the perspective of data reconstruction, it is unable to exploit other crucial information, namely, graph structure and pairwise constraints. To address previously mentioned deficiency, we propose a novel feature selection approach in this paper, known as unsupervised feature selection via data reconstruction and side information. More specifically, the proposed method takes advantage of the prior knowledge regarding pairwise constraints (side information), the minimization of data reconstruction error, and the graph embedding simultaneously, such that pivotal features are selected with preserving data manifold structure. To obtain the robust solution, a robust loss function is applied to the feature selection problem, which interpolates between l(1)-norm and l(2)-norm. Eventually, extensive experiments are conducted to demonstrate the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29						8097	8106		10.1109/TIP.2020.3011253													
J								Detection of IoT-botnet attacks using fuzzy rule interpolation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of things; fuzzy rule interpolation; botnet attack; intrusion detection system	INTERNET	Recently, the Internet of Things (IoT) has been used in technology for different aspects to increase the efficiency and comfort of human life. Protecting the IoT infrastructure is not a straightforward task. There is an urgent need to handle different attack scenarios within the IoT smart environment. Attackers continuously targeted the modern aspects of technology, and trying abusing these technologies using complex attack scenarios such as Botnet attacks. Botnet attacks considered a serious challenge faces of the IoT smart environment. In this paper, we introduce a novel idea that capable of supporting the detecting of IoT-Botnet attack and in meanwhile to avoid the issues associated with the deficiencies of the knowledge-based representation and the binary decision. This paper aims to introduce a detection approach for the IoT-BotNet attack by using the Fuzzy Rule Interpolation (FRI). The FRI reasoning methods added a benefit to enhance the robustness of fuzzy systems and effectively reduce the system's complexity. These benefits help the Intrusion Detection System (IDS) to generate more realistic and comprehensive alerts. The proposed approach was applied to an open-source BoT-IoT dataset from the Cyber Range Lab of the center of UNSW Canberra Cyber. The proposed approach was tested, evaluated and obtained a 95.4% detection rate. Moreover, it effectively smooth the boundary between normal and IoT-BotNet traffics because of its fuzzy - nature, as well as, it had the ability to generate the required IDS alert in case of the deficiencies of the knowledge-based representation.																	1064-1246	1875-8967					2020	39	1					421	431		10.3233/JIFS-191432													
J								Generating Target Image-Label Pairs for Unsupervised Domain Adaptation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Adaptation models; Task analysis; Image segmentation; Gallium nitride; Data models; Feature extraction; Domain adaptation; generative adversarial network; image classification; semantic segmentation; image generation		Deep learning demonstrates its impressive success across various machine learning problems. However, its performance often suffers in the case where the training and test data sets follow different distributions, due to the domain shift. Most current domain adaptation methods minimize the discrepancy between the source and target domains by enforcing the alignment of their marginal distributions without considering the class-level matching. Consequently, data from different classes may become close together after mapping. To address this issue, we propose an unsupervised domain adaptation method by generating image-label pairs in the target domain, in which the model is augmented with the generated target pairs and achieve class-level transfer. Specifically, we integrate generative adversarial networks (GAN) into the model predictor, where the generator fed with labels aims to produce corresponding target domain images with a well-designed semantic loss. Meanwhile, compared to previous methods which focus on discrepancy reduction across domains, i.e., image to image translation, our model focuses on semantic preservation during image generation. Our model is straightforward yet effective for unsupervised domain adaptation problems. Without any labels in the target domain in all the experiments, we demonstrate the validity of our approach by presenting the plausible generated target image-label pairs. In addition, our proposed method achieves the best or comparable performance on multiple unsupervised domain adaptation datasets which include image classification and semantic segmentation.																	1057-7149	1941-0042					2020	29						7997	8011		10.1109/TIP.2020.3009853													
J								Learning Deeply Aggregated Alternating Minimization for General Inverse Problems	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image restoration; Minimization; Noise reduction; Image resolution; Optimization; Training; Task analysis; Regularization-based image restoration; joint restoration; convolutional neural network; alternating minimization; half-quadratic minimization; proximal mapping	IMAGE; ALGORITHM	Regularization-based image restoration is one of the most powerful tools in image processing and computer vision thanks to its flexibility for handling various inverse problems. However, designing an optimal regularization function still remains unsolved since natural images and related scene types have a complex structure. In this paper, we present a general and principled framework, called deeply aggregated alternating minimization (DeepAM). We design a convolutional neural network (CNN) to implicitly parameterize the regularizer of the alternating minimization (AM) algorithm. Contrary to the conventional AM algorithm based on a point-wise proximal mapping, the DeepAM projects intermediate estimate into a set of natural images via deep aggregation. Since the CNN is fully integrated into the AM procedure, all parameters can be jointly optimized through end-to-end training. These properties enable the DeepAM to converge with a small number of iterations, while maintaining an algorithmic simplicity. We show that the DeepAM outperforms state-of-the-art methods, including nonlocal-based methods, Plug-and-Play regularization, and recent data-driven approaches. The effectiveness of our framework is demonstrated in a variety of image restoration tasks: Guassian denoising, deraining, deblurring, super-resolution, color-guided depth upsampling, and RGB/NIR restoration.																	1057-7149	1941-0042					2020	29						8012	8027		10.1109/TIP.2020.3010082													
J								A Truncated Matrix Decomposition for Hyperspectral Image Super-Resolution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spatial resolution; Matrix decomposition; Hyperspectral imaging; Image segmentation; Electronic mail; Machine learning; Super-resolution; hyperspectral image; matrix decomposition; low rank; superpixel	FACE RECOGNITION; FUSION; FACTORIZATION; ALGORITHM; FORMULATION	Hyperspectral image super-resolution addresses the problem of fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to produce a high-resolution hyperspectral image (HR-HSI). In this paper, we propose a novel fusion approach for hyperspectral image super-resolution by exploiting the specific properties of matrix decomposition, which consists of four main steps. First, an endmember extraction algorithm is used to extract an initial spectral matrix from LR-HSI. Then, with the initial spectral matrix, we estimate the spatial matrix, i.e., the spatial-contextual information, from the degraded observations of HR-HSI. Third, the spatial matrix is further utilized to estimate the spectral matrix from LR-HSI by solving a least squares (LS)-based problem. Finally, the target HR-HSI is constructed by combing the estimated spectral and spatial matrixes. In particular, two models are proposed to estimate the spatial matrix. One is a simple case that involves a LS-based problem, and the other is an elaborate case that consists of two fidelity terms and a spatial regularizer, where the spatial regularizer aiming to restrain the range of solutions is achieved by exploiting the superpixel-level low-rank characteristics of HR-HSI. Experiment results conducted on both synthetic and real data sets demonstrate the effectiveness of the proposed approach as compared to other hyperspectral image super-resolution methods.																	1057-7149	1941-0042					2020	29						8028	8042		10.1109/TIP.2020.3009830													
J								Unified Intra Mode Coding Based on Short and Long Range Correlations	IEEE TRANSACTIONS ON IMAGE PROCESSING										Correlation; Image coding; Video coding; Encoding; Indexes; Interpolation; Standards; Intra coding; versatile video coding; intra mode	VIDEO; PREDICTION	There has been a consensus regarding the intra prediction technique in video coding that the spatial redundancy can be efficiently removed by the locally accessible reference samples with certain projections and interpolations. In this paper, we revisit the short and long range correlations of the image content in the context of video coding, and it is interesting to find that the natural scene videos exhibit substantially different characteristics from screen content videos. This motivates us to redesign the intra mode coding method based on both short and long range correlations, as the existing approaches based on local content correlations cannot always effectively capture the most probable mode. One key feature of the proposed method is that it achieves unified content adaptive coding and is applicable across different video content. Experimental results on the versatile video coding (VVC) platform VTM-3.0 show the effectiveness of the proposed approach, leading to 3.73% bit rate savings for screen content videos and 0.10% bit rate savings for natural scene videos under all intra configuration.																	1057-7149	1941-0042					2020	29						7245	7260		10.1109/TIP.2020.3000351													
J								MVSNet plus plus : Learning Depth-Based Attention Pyramid Features for Multi-View Stereo	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Three-dimensional displays; Solid modeling; Computational modeling; Image reconstruction; Estimation; Training; Multi-view stereo; deep learning; 3D model reconstruction; feature aggregation; plane sweep algorithm		The goal of Multi-View Stereo (MVS) is to reconstruct 3D point-cloud model from multiple views. On the basis of the considerable progress of deep learning, an increasing amount of research has moved from traditional MVS methods to learning-based ones. However, two issues remain unsolved in the existing state-of-the-art methods: (1) only high-level information is considered for depth estimation. This may reduce the localization accuracy of 3D points as the learned model lacks spatial information; and (2) most of the methods require additional post-processing or network refinement to generate a smooth 3D model. This significantly increases the number of model parameters or the computational complexity. To this end, we propose MVSNet++, an end-to-end trainable network for dense depth estimation. Such an estimated depth map can further be applied to 3D model reconstruction. Different from previous methods, in the proposed method, we first adopt feature pyramid structures for both feature extraction and cost volume regularization. This can lead to accurate 3D point localization by fusing multi-level information. To generate smooth depth map, we then carefully integrate instance normalization into MVSNet++ without increasing model parameters and computational burden. Furthermore, we additionally design three loss functions and integrate Curriculum Learning framework into the training process, which can lead to an accurate reconstruction of 3D model. MVSNet++ is evaluated on DTU and Tanks & Temples benchmarks with comprehensive ablation studies. Experimental results demonstrate that our proposed method performs favorably against previous state-of-the-art methods, showing the accuracy and effectiveness of the proposed MVSNet++.																	1057-7149	1941-0042					2020	29						7261	7273		10.1109/TIP.2020.3000611													
J								Single Image Deraining Using Time-Lapse Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rain; Training data; Task analysis; Convolutional neural networks; Rendering (computer graphics); Training; Feature extraction; Single image deraining; convolutional neural networks (CNNs); time-lapse dataset; dynamic fusion module	REMOVAL	Leveraging on recent advances in deep convolutional neural networks (CNNs), single image deraining has been studied as a learning task, achieving an outstanding performance over traditional hand-designed approaches. Current CNNs based deraining approaches adopt the supervised learning framework that uses a massive training data generated with synthetic rain streaks, having a limited generalization ability on real rainy images. To address this problem, we propose a novel learning framework for single image deraining that leverages time-lapse sequences instead of the synthetic image pairs. The deraining networks are trained using the time-lapse sequences in which both camera and scenes are static except for time-varying rain streaks. Specifically, we formulate a background consistency loss such that the deraining networks consistently generate the same derained images from the time-lapse sequences. We additionally introduce two loss functions, the structure similarity loss that encourages the derained image to be similar with an input rainy image and the directional gradient loss using the assumption that the estimated rain streaks are likely to be sparse and have dominant directions. To consider various rain conditions, we leverage a dynamic fusion module that effectively fuses multi-scale features. We also build a novel large-scale time-lapse dataset providing real world rainy images containing various rain conditions. Experiments demonstrate that the proposed method outperforms state-of-the-art techniques on synthetic and real rainy images both qualitatively and quantitatively. On the high-level vision tasks under severe rainy conditions, it has been shown that the proposed method can be utilized as a pre-preprocessing step for subsequent tasks.																	1057-7149	1941-0042					2020	29						7274	7289		10.1109/TIP.2020.3000612													
J								Scanning Imaging Restoration of Moving or Dynamically Deforming Objects	IEEE TRANSACTIONS ON IMAGE PROCESSING										Scanning electron microscopy; Optical imaging; Strain; Optical distortion; Image restoration; Numerical analysis; Image restoration; Scanning imaging model; Dynamic deformation; Scanning electron microscope; Non-uniform blur	ELECTRON-MICROSCOPY; HIGH-RESOLUTION; DISTORTION; FILTER	The raster scanning imaging mode is widely used in scanning electron microscopes (SEMs), transmission electron microscopes (TEM), and atomic force microscopes (AFM), and can achieve subatomic resolution. However, only a point on the shallow surface of an object can be imaged at one time using the raster scanning imaging mode, whereas the entire surface of the object can be imaged in the image plane once and instantaneously using the optical imaging mode, which is a parallel imaging mode. Therefore, the image distortion and blur for the scanning imaging mode are different from the optical imaging. In this paper, we propose a theory to describe the mechanism of the scanning imaging process and restore the degraded image (distorted and blurred image) obtained using an SEM. The theory consists of a scanning equation, motion or deformation equations, and an assumption called the intensity-invariant hypothesis. Numerical simulations of the scanning imaging process and restoration of the degraded images are performed using the scanning imaging formulas, spatial non-uniform point spread function, and inverse restoration algorithms, including algebraic, interpolation, and their hybrid methods to verify the feasibility of our theory. In situ experiments on uniform linear motion, uniaxial tensile, and fatigue were also conducted to demonstrate the validity and efficiency of the proposed scanning imaging theory and restoration methods. We anticipate that this imaging and restoration theory will enable the scanning imaging mode to be used in in situ dynamic imaging and for mechanical property measurement of materials.																	1057-7149	1941-0042					2020	29						7290	7305		10.1109/TIP.2020.3000663													
J								Deep Pyramidal Pooling With Attention for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Computer architecture; Semantics; Deep learning; Feature extraction; Visualization; Task analysis; Measurement; Person re-identification; pyramid representation; neural networks; deep learning	NETWORK	Learning discriminative, view-invariant and multi-scale representations of object appearance with different semantic levels is of paramount importance for person Re-Identification (ReID). Recently, the community has focused on learning deep Re-ID models to capture a single holistic representation. To improve the achieved results, additional visual attributes and object part-driven models have been considered, inevitably introducing additional human annotation labor or computational efforts. In this paper, we argue that pyramid-inspired methods capturing multi-scale information may overcome such requirements. Precisely, multi-scale pooled regions representing visual information of an object are integrated within a novel deep architecture factorizing them into discriminative features at multiple semantic levels. These are exploited through an attention mechanism later considered in an identification-similarity multi-task loss, trained by means of a curriculum learning strategy. Extensive results on three person ReID benchmarks demonstrate that better performance than existing methods are achieved. Code is available at https://github.com/iN1k1.																	1057-7149	1941-0042					2020	29						7306	7316		10.1109/TIP.2020.3000904													
J								Effective Super-Resolution Methods for Paired Electron Microscopic Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Optical imaging; Training; Electron optics; Deep learning; Electron microscopic image; deep learning; global and local registration; library-based non-local mean; paired-image super-resolution		This paper is concerned with investigating super-resolution algorithms and solutions for handling electron microscopic images. We note two main aspects differentiating the problem discussed here from those considered in the literature. The first difference is that in the electron imaging setting. We have a pair of physical high-resolution and low-resolution images, rather than a physical image with its downsampled counterpart. The high-resolution image covers about 25% of the view field of the low-resolution image, and the objective is to enhance the area of the low-resolution image where there is no high-resolution counterpart. The second difference is that the physics behind electron imaging is different from that of optical (visible light) photos. The implication is that super-resolution models trained by optical photos are not effective when applied to electron images. Focusing on the unique properties, we devise a global and local registration method to match the high- and low-resolution image patches and explore training strategies for applying deep learning super-resolution methods to the paired electron images. We also present a simple, non-local-mean approach as an alternative. This alternative performs as a close runner-up to the deep learning approaches, but it takes less time to train and entertains a simpler model structure.																	1057-7149	1941-0042					2020	29						7317	7330		10.1109/TIP.2020.3000964													
J								Modeling Generalized Rate-Distortion Functions	IEEE TRANSACTIONS ON IMAGE PROCESSING										Quality-of-experience (QoE); rate-distortion theory; content distribution; Clough-Toucher interpolation; quadratic programming; statistical sampling	QUALITY ASSESSMENT	Many multimedia applications require precise understanding of the rate-distortion characteristics measured by the function relating visual quality to media attributes, for which we term it the generalized rate-distortion (GRD) function. In this study, we explore the GRD behavior of compressed digital videos in a two-dimensional space of bitrate and resolution. Our analysis on a large-scale video dataset reveals that empirical parametric models are systematically biased while exhaustive search methods require excessive computation time to depict the GRD surfaces. By exploiting the properties that all GRD functions share, we develop an Robust Axial-Monotonic Clough-Tocher (RAMCT) interpolation method to model the GRD function. This model allows us to accurately reconstruct the complete GRD function of a source video content from a moderate number of measurements. To further reduce the computational cost, we present a novel sampling scheme based on a probabilistic model and an information measure. The proposed sampling method constructs a sequence of quality queries by minimizing the overall informativeness in the remaining samples. Experimental results show that the proposed algorithm significantly outperforms state-of-the-art approaches in accuracy and efficiency. Finally, we demonstrate the usage of the proposed model in three applications: rate-distortion curve prediction, per-title encoding profile generation, and video encoder comparison.																	1057-7149	1941-0042					2020	29						7331	7344		10.1109/TIP.2020.3001405													
J								Jointly Learning Commonality and Specificity Dictionaries for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dictionaries; Feature extraction; Cameras; Image coding; Deep learning; Visualization; Person re-identification; dictionary learning; low-rank and sparse; commonality dictionary; specificity dictionary	THRESHOLDING ALGORITHM; K-SVD; NETWORK	Despite advances in person re-identification (re-ID), it is still far from meeting the needs of real-world applications due to tremendous visual ambiguity in person appearance across cameras. To overcome this problem, we propose a person re-ID method by decomposing a pedestrian's appearance feature into different components. This method assumes that each pedestrian image is composed of person-shared components that reflect the similarities of different pedestrians and person-specific components that reflect unique identity information. Based on this assumption, we propose to reduce the ambiguity in visual features by removing person-shared components from pedestrian visual features. To this end, we develop a framework for learning a pair of commonality and specificity dictionaries, while introducing a distance constraint to force the particularities of the same person over the specificity dictionary to have the same coding coefficients and the coding coefficients of different pedestrians to have weak correlation. Furthermore, considering the similarity of the commonality dictionary and the sparsity of the specificity dictionary, low-rank and sparse regularization terms are introduced into the dictionary learning framework to improve their representation ability and discriminative ability. Extensive experimental results show that the proposed algorithm outperforms or is competitive with the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						7345	7358		10.1109/TIP.2020.3001424													
J								Analysis of Affine Motion-Compensated Prediction in Video Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video coding; Bit rate; Estimation error; Encoding; Predictive models; Rate-distortion; Image coding; Video coding; (simplified) affine motion-compensated prediction (MCP); rate-distortion theory; Versatile Video Coding (VVC)		Motion-compensated prediction is used in video coding standards like High Efficiency Video Coding (HEVC) as one key element of data compression. Commonly, a purely translational motion model is employed. In order to also cover non-translational motion types like rotation or scaling (zoom), e. g. contained in aerial video sequences such as captured from unmanned aerial vehicles (UAV), an affine motion model can be applied. In this work, a model for affine motion-compensated prediction in video coding is derived. Using the rate-distortion theory and the displacement estimation error caused by inaccurate affine motion parameter estimation, the minimum required bit rate for encoding the prediction error is determined. In this model, the affine transformation parameters are assumed to be affected by statistically independent estimation errors, which all follow a zero-mean Gaussian distributed probability density function (pdf). The joint pdf of the estimation errors is derived and transformed into the pdfof the location-dependent displacement estimation error in the image. The latter is related to the minimum required bit rate for encoding the prediction error. Similar to the derivations of the fully affine motion model, a four-parameter simplified affine model is investigated. Both models are of particular interest since they are considered for the upcoming video coding standard Versatile Video Coding (VVC) succeeding HEVC. Both models provide valuable information about the minimum bit rate for encoding the prediction error as a function of affine estimation accuracies.																	1057-7149	1941-0042					2020	29						7359	7374		10.1109/TIP.2020.3001734													
J								Watershed-Based Superpixels With Global and Local Boundary Marching	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Image segmentation; Optimization; Real-time systems; Education; Clustering algorithms; Merging; Image segmentation; superpixel; watershed-based; real-time; boundary marching		Superpixels are widely used in computer vision applications, as they conserve the running costs of subsequent processing while preserving the original performance. In most of the existing algorithms, the boundary adherence and the compactness of superpixels are necessarily inter-inhibitive because the color/gradient information is balanced against the position constraints, and the set criteria define all pixels indiscriminately. In this paper, we present a two-phase superpixel segmentation method based on the watershed transformation. After designing a new approach for calculating the flooding priority, we propose a new strategy with two distinct criteria for global and local refinement of the boundary pixels. These criteria reduce the compromise between the boundary adherence and compactness. Unlike the indiscriminate standards, our method applies different treatments to pixels in different environments, preserving the color homogeneity in content-rich areas while improving the regularity of the superpixels in content-plain regions. The superior accuracy and computing time of our proposed method are verified in comparison experiments with several state-of-the-art methods.																	1057-7149	1941-0042					2020	29						7375	7388		10.1109/TIP.2020.3002078													
J								FoveaBox: Beyound Anchor-Based Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; anchor free; foveabox		We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox.																	1057-7149	1941-0042					2020	29						7389	7398		10.1109/TIP.2020.3002345													
J								Compressed Image Restoration via Artifacts-Free PCA Basis Learning and Adaptive Sparse Modeling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Compressed image restoration; sparse modeling; paired PCA learning; adaptive distribution modeling	JPEG DECOMPRESSION; REDUCTION; ALGORITHM; RECONSTRUCTION; DEBLOCKING; DCT	Visually unpleasant compression artifacts frequently appear in block-based transform coding, especially at low bit rates. This paper presents a new artifact reduction scheme based on Bayesian sparse modeling and artifacts-free PCA basis learning. To avoid the effect of blocking artifacts, we propose to learn artifacts-free PCA basis from clean images. We concatenate the clean patches and their compressed counterparts to learn paired distribution prior via the Gaussian Mixture Model (GMM). By this way, the GMM characterizes the mapping between the clean image and its compressed version. To restore a compressed patch, the best matched GMM component is assigned using the patch in the compressed image subspace. The artifacts-free PCA basis is obtained according to the mapping learned by the paired GMM. In practice, the statistical distributions of different sparse coefficients in different patches may dramatically vary with image contents. Instead of using a global zero-mean distribution for all coefficients, we propose to adaptively model the prior of each band in a Bayesian framework. The expectation and variance of each band are adaptively learned from the similar patches within the image. Thus, different transform bands are regularized unequally according to the learned priors. Experimental results show that the proposed scheme outperforms most of the compared schemes in terms of both objective quality and perceptual quality.																	1057-7149	1941-0042					2020	29						7399	7413		10.1109/TIP.2020.3002452													
J								End-to-End Blind Image Quality Prediction With Cascaded Deep Neural Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Blind image quality assessment (BIQA); hierarchical degradation concatenation; end-to-end; deep convolutional neural network		The deep convolutional neural network (CNN) has achieved great success in image recognition. Many image quality assessment (IQA) methods directly use recognition-oriented CNN for quality prediction. However, the properties of IQA task is different from image recognition task. Image recognition should be sensitive to visual content and robust to distortion, while IQA should be sensitive to both distortion and visual content. In this paper, an IQA-oriented CNN method is developed for blind IQA (BIQA), which can efficiently represent the quality degradation. CNN is large-data driven, while the sizes of existing IQA databases are too small for CNN optimization. Thus, a large IQA dataset is firstly established, which includes more than one million distorted images (each image is assigned with a quality score as its substitute of Mean Opinion Score (MOS), abbreviated as pseudo-MOS). Next, inspired by the hierarchical perception mechanism (from local structure to global semantics) in human visual system, a novel IQA-orientated CNN method is designed, in which the hierarchical degradation is considered. Finally, by jointly optimizing the multilevel feature extraction, hierarchical degradation concatenation (HDC) and quality prediction in an end-to-end framework, the Cascaded CNN with HDC (named as CaHDC) is introduced. Experiments on the benchmark IQA databases demonstrate the superiority of CaHDC compared with existing BIQA methods. Meanwhile, the CaHDC (with about 0.73M parameters) is lightweight comparing to other CNN-based BIQA models, which can be easily realized in the microprocessing system. The dataset and source code of the proposed method are available at https://web.xidian.edu.cn/wjj/paper.html.																	1057-7149	1941-0042					2020	29						7414	7426		10.1109/TIP.2020.3002478													
J								PMBANet: Progressive Multi-Branch Aggregation Network for Scene Depth Super-Resolution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Depth map; super-resolution; aggregation; progressive; multi-branch	CO-SALIENCY DETECTION; IMAGE; RESOLUTION	Depth map super-resolution is an ill-posed inverse problem with many challenges. First, depth boundaries are generally hard to reconstruct particularly at large magnification factors. Second, depth regions on fine structures and tiny objects in the scene are destroyed seriously by downsampling degradation. To tackle these difficulties, we propose a progressive multi-branch aggregation network (PMBANet), which consists of stacked MBA blocks to fully address the above problems and progressively recover the degraded depth map. Specifically, each MBA block has multiple parallel branches: 1) The reconstruction branch is proposed based on the designed attention-based error feed-forward/-back modules, which iteratively exploits and compensates the downsampling errors to refine the depth map by imposing the attention mechanism on the module to gradually highlight the informative features at depth boundaries. 2) We formulate a separate guidance branch as prior knowledge to help to recover the depth details, in which the multi-scale branch is to learn a multi-scale representation that pays close attention at objects of different scales, while the color branch regularizes the depth map by using auxiliary color information. Then, a fusion block is introduced to adaptively fuse and select the discriminative features from all the branches. The design methodology of our whole network is well-founded, and extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in comparison with the state-of-the-art methods. Our code and models are available at https://github.com/Sunbaoli/PMBANet_DSR/.																	1057-7149	1941-0042					2020	29						7427	7442		10.1109/TIP.2020.3002664													
J								No Reference Quality Assessment for 3D Synthesized Views by Local Structure Variation and Global Naturalness Change	IEEE TRANSACTIONS ON IMAGE PROCESSING										Depth image based rendering (DIBR); View synthesis; no reference (NR); image quality assessment	DATABASE; IMAGES; SIMILARITY; DIBR	Depth image based rendering (DIBR) has been widely used to generate different virtual viewpoints of the same scene from the new perspective. However, DIBR tends to introduce annoying artifacts including blurring, discontinuity, blocking, and stretching, etc.. Thus, to improve DIBR performance, it is important to accurately measure the visual quality of synthesized views. In this paper, we propose a novel and effective no reference (NR) quality assessment method for 3D synthesized views by local variation and global change (LVGC). More specifically, we firstly compute the Gaussian derivatives for the input image to extract structure and chromatic features. Then, we use the local binary pattern (LBP) operator to encode the structure and chromatic feature maps, which are used to calculate quality-aware features to measure the local structural and chromatic distortion. Besides, we extract luminance features by global change to evaluate the naturalness of 3D synthesized views. With these extracted features, we utilize random forest regression (RFR) to train the quality prediction model from visual features to human ratings. Experimental results on three public benchmark databases demonstrate the effectiveness of our method on estimating visual quality of 3D synthesized views.																	1057-7149	1941-0042					2020	29						7443	7453		10.1109/TIP.2020.3003218													
J								Scripted Video Generation With a Bottom-Up Generative Adversarial Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Generative adversarial networks; video generation; semantic alignment; temporal coherence		Generating videos given a text description (such as a script) is non-trivial due to the intrinsic complexity of image frames and the structure of videos. Although Generative Adversarial Networks (GANs) have been successfully applied to generate images conditioned on a natural language description, it is still very challenging to generate realistic videos in which the frames are required to follow both spatial and temporal coherence. In this paper, we propose a novel Bottom-up GAN (BoGAN) method for generating videos given a text description. To ensure the coherence of the generated frames and also make the whole video match the language descriptions semantically, we design a bottom-up optimisation mechanism to train BoGAN. Specifically, we devise a region-level loss via attention mechanism to preserve the local semantic alignment and draw details in different sub-regions of video conditioned on words which are most relevant to them. Moreover, to guarantee the matching between text and frame, we introduce a frame-level discriminator, which can also maintain the fidelity of each frame and the coherence across frames. Last, to ensure the global semantic alignment between whole video and given text, we apply a video-level discriminator. We evaluate the effectiveness of the proposed BoGAN on two synthetic datasets (i.e., SBMG and TBMG) and two real-world datasets (i.e., MSVD and KTH).																	1057-7149	1941-0042					2020	29						7454	7467		10.1109/TIP.2020.3003227													
J								Improve Person Re-Identification With Part Awareness Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Person re-identification; part awareness; part segmentation; multi-task learning	NETWORK	Person re-identification (ReID) aims to predict whether two images from different cameras belong to the same person. Due to low image quality and variance in view point and body pose, it remains a difficult task. To solve the task, a model is supposed to appropriately capture features that describe body regions for identification. With the simple intuition that explicitly incorporating ReID model with part awareness could be beneficial for learning a more discriminative feature space, we propose part segmentation as an assistant body perception task during the training of a ReID model. Specifically, we add a lightweight segmentation head to the backbone of ReID model during training, which is supervised with part labels. Note that our segmentation head is only introduced during training and that it does not change network input or the way of extracting ReID feature. Experiments show that part segmentation considerably improves the performance of ReID. Through quantitative and qualitative analyses, we further reveal that body part perception helps ReID model to capture a set of more diverse features from the body, with decreased similarity between part features and increased focus on different body regions. We experiment with various representative ReID models and achieve consistent improvement on several large-scale datasets including Market1501, CUHK03, DukeMTMC-reID and MSMT17. E.g. on MSMT17, our method increases Rank-1 Accuracy of GlobalPool-ResNet-50, PCB and MGN by 2.3%, 2.9% and 3.9%, respectively. Incorporated with MGN, our model achieves state-of-the-art performance, with Rank-1 Accuracy 95.8%, 78.8%, 90.0% and 84.0% on four datasets, respectively.																	1057-7149	1941-0042					2020	29						7468	7481		10.1109/TIP.2020.3003442													
J								Accelerate CTU Partition to Real Time for HEVC Encoding With Complexity Control	IEEE TRANSACTIONS ON IMAGE PROCESSING										High efficiency video coding; coding tree unit partition; complexity control	EFFICIENCY; DECISION; ALLOCATION; SELECTION	Recently, extensive approaches have been proposed for reducing the encoding complexity of high efficiency video coding, by predicting the coding tree unit partition using deep neural networks. However, these approaches cannot work in real time due to the complexity of the network architectures. In this paper, we propose a network pruning approach to accelerate a state-of-the-art deep neural network model, for real-time coding tree unit partition. Specifically, we first investigate the computational complexity throughout the network, and find that most calculations can be simplified by pruning the weight parameters. Considering that the number of weight parameters drastically differs by network layer and partition level, we design an adaptive pruning scheme by applying a well-suitable retention ratio of weight parameters to each layer at a level. The retention ratio indicates the ratio of weight parameters after and before pruning. By varying the retention ratios, we can obtain several accelerated network models with different levels of complexity. We further propose a complexity control algorithm by applying different accelerated models to different coding tree units, to ensure that the actual encoding complexity is close to a given target. To guarantee the rate-distortion performance, we model the complexity control algorithm as a convex optimization problem, and we can obtain a closed-form solution. Experimental results show that our approach can accelerate the original deep neural network model by 17-20 times, with little expense on the Bjontegaard delta bit-rate. For complexity control, we achieve high control accuracy with a control error of less than 2% for most video sequences.																	1057-7149	1941-0042					2020	29						7482	7496		10.1109/TIP.2020.3003730													
J								Task Decomposition and Synchronization for Semantic Biomedical Image Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantic segmentation; fully convolutional network; task decomposition; sync-regularization; deep learning		Semantic segmentation is essentially important to biomedical image analysis. Many recent works mainly focus on integrating the Fully Convolutional Network (FCN) architecture with sophisticated convolution implementation and deep supervision. Such complex networks need large training datasets, a requirement which is challenging for medical image analysis. In this paper, we propose to decompose the single segmentation task into three subsequent sub-tasks, including (1) pixel-wise image semantic segmentation, (2) prediction of the instance class labels of the objects within the image, and (3) classification of the scene the image belonging to. While these three sub-tasks are trained to optimize their individual loss functions at different perceptual levels, we propose to allow their interaction within the task-task context ensemble. Moreover, we propose a novel sync-regularization to penalize the deviation between the outputs of the pixel-wise semantic segmentation and the instance class prediction tasks. These effective regularizations help FCN utilize context information comprehensively and attain accurate segmentation, even though the number of images for training may be limited in many biomedical applications. We have successfully applied our framework to three diverse 2D/3D medical image datasets, including Robotic Scene Segmentation Challenge 18 (ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus Glaucoma Challenge (REFUGE18). We have achieved outperformed or comparable performance in all the three challenges. Our code, typical data and trained models are available at https://github.com/xuhuaren/TDSNet.																	1057-7149	1941-0042					2020	29						7497	7510		10.1109/TIP.2020.3003735													
J								Deep Joint Deinterlacing and Denoising for Single Shot Dual-ISO HDR Reconstruction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dual-ISO; HDR imaging; noise; deep learning		HDR images have traditionally been obtained by merging multiple exposures each captured with a different exposure time. However, this approach entails longer capture times and necessitates deghosting if the captured scene contains moving objects. With the advent of modern camera sensors that can perform per-pixel exposure modulation, it is now possible to capture all of the required exposures within a single shot. The new challenge then becomes how to best combine different pixels with different exposure values into a single full-resolution and low-noise HDR image. We propose a joint multi-exposure frame deinterlacing and denoising algorithm powered by deep convolutional neural networks (DCNN). In our algorithm, we first train two DCNNs, with one tuned for reconstructing low exposures and the other for high exposures. Each DCNN takes the same mosaicked dual-ISO input image and outputs either the low exposure or high exposure depending on the type of the network. The resulting exposures can be demosaicked and converted to the desired target color space prior to HDR assembly. Our evaluations indicate that the quality of our results significantly surpasses the state-of-the-art in single-image HDR reconstruction algorithms.																	1057-7149	1941-0042					2020	29						7511	7524		10.1109/TIP.2020.3004014													
J								Contrast Enhancement Using Novel White Balancing Parameter Optimization for Perceptually Invisible Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Contrast enhancement; gray world algorithm; white balancing; gamma correction	HISTOGRAM EQUALIZATION; QUALITY; MODEL	A novel white balancing algorithm is proposed in this paper to automatically enhance the global contrast degraded imperceptible images. The technique is applied on four publicly available image dataset, CSIQ, KADID, TID and SIPI. Colour images consist of three channels viz. Red, Blue and Green. A contrast degraded colour image visually appears similar to an image with one or more distorted channel. 12 images are obtained by enhancing one channel of the contrast degraded image at the cost of other channel using White Balancing algorithm. Four images with best quantitative performance metrics, visual similarity index (VSI), gradient magnitude similarity index (GMSD), patch-based contrast quality index (PCQI) and peak signal-to-noise ratio (PSNR) determines the pair of weak and prominent channels. An optimization algorithm then enhances these channels and the image with the best quantitative performance metrics is chosen as the enhanced image. Quantitative and qualitative results demonstrate that the proposed method produces an enhanced image with superior perceptual quality, and gives the best average results for all the parameters across every dataset as compared to the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						7525	7536		10.1109/TIP.2020.3004036													
J								Semi-Supervised Texture Filtering With Shallow to Deep Understanding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Texture filtering; image translation; Generative Adversarial Networks (GANs); semi-supervised		This work proposed a semi-supervised method for automatic texture filtering. Our method leveraged a limited amount of labeled data and a large amount of unlabeled data to train Generative Adversarial Networks (GANs). Separate loss functions were designed for both labeled and unlabeled datasets. Our main contribution is the introduction of knowledge extracted from shallow and deep layers in neural networks. Loss defined within shallow layers preserves the edge, while loss defined within the deep layers identifies the semantic content and conversely removes the small-scale texture variations. This contribution directly addresses the major challenge for texture filtering, distinguishing the structural content from non-structural textures at the pixel level. The extracted information, in our study, improved the content and color consistency before and after the process of filtering, for unlabeled samples in particular. The proposed method offers twofold benefits: first, significant reductions in the amounts of time and effort expended in reconstructing the labeled dataset, especially given the delicate operations required at the pixel level; second, a reduction in over-fitting, in supervised learning with a small amount of labeled data, by utilizing a large amount of unlabeled data. The results confirm that our method can perform comparably with non-learning-based methods, alleviating the demand for the determination of optimal parameter values.																	1057-7149	1941-0042					2020	29						7537	7548		10.1109/TIP.2020.3004043													
J								MetaSearch: Incremental Product Search via Deep Meta-Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Product search; Few-shot learning; Incremental search; Meta-learning; Multipooling		With the advancement of image processing and computer vision technology, content-based product search is applied in a wide variety of common tasks, such as online shopping, automatic checkout systems, and intelligent logistics. Given a product image as a query, existing product search systems mainly perform the retrieval process using predefined databases with fixed product categories. However, real-world applications often require inserting new categories or updating existing products in the product database. When using existing product search methods, the image feature extraction models must be retrained and database indexes must be rebuilt to accommodate the updated data, and these operations incur high costs for data annotation and training time. To this end, we propose a few-shot incremental product search framework with meta-learning, which requires very few annotated images and has a reasonable training time. In particular, our framework contains a multipooling-based product feature extractor that learns a discriminative representation for each product, and we also design a meta-learning-based feature adapter to guarantee the robustness of the few-shot features. Furthermore, when expanding new categories in batches during a product search, we reconstruct the few-shot features by using an incremental weight combiner to accommodate the incremental search task. Through extensive experiments, we demonstrate that the proposed framework achieves excellent performance for new products while still guaranteeing the high search accuracy of the base categories after gradually expanding new product categories without forgetting.																	1057-7149	1941-0042					2020	29						7549	7564		10.1109/TIP.2020.3004249													
J								FusionNet: An Unsupervised Convolutional Variational Network for Hyperspectral and Multispectral Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hyperspectral images; multispectral images; image fusion; probabilistic generative model; convolutional neural network; meta-learning		Due to hardware limitations of the imaging sensors, it is challenging to acquire images of high resolution in both spatial and spectral domains. Fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to obtain an HR-HSI in an unsupervised manner has drawn considerable attention. Though effective, most existing fusion methods are limited due to the use of linear parametric modeling for the spectral mixture process, and even the deep learning-based methods only focus on deterministic fully-connected networks without exploiting the spatial correlation and local spectral structures of the images. In this paper, we propose a novel variational probabilistic autoencoder framework implemented by convolutional neural networks, in order to fuse the spatial and spectral information contained in the LR-HSI and HR-MSI, called FusionNet. The FusionNet consists of a spectral generative network, a spatial-dependent prior network, and a spatial-spectral variational inference network, which are jointly optimized in an unsupervised manner, leading to an end-to-end fusion system. Further, for fast adaptation to different observation scenes, we give a meta-learning explanation to the fusion problem, and combine the FusionNet with meta-learning in a synergistic manner. Effectiveness and efficiency of the proposed method are evaluated based on several publicly available datasets, demonstrating that the proposed FusionNet outperforms the state-of-the-art fusion methods.																	1057-7149	1941-0042					2020	29						7565	7577		10.1109/TIP.2020.3004261													
J								Fine-Grained Spatial Alignment Model for Person Re-Identification With Focal Triplet Loss	IEEE TRANSACTIONS ON IMAGE PROCESSING										Person re-identification; spatial alignment; focal triplet loss	TRACKING	Recent advances of person re-identification have well advocated the usage of human body cues to boost performance. However, most existing methods still retain on exploiting a relatively coarse-grained local information. Such information may include redundant backgrounds that are sensitive to the apparently similar persons when facing challenging scenarios like complex poses, inaccurate detection, occlusion and misalignment. In this paper we propose a novel Fine-Grained Spatial Alignment Model (FGSAM) to mine fine-grained local information to handle the aforementioned challenge effectively. In particular, we first design a pose resolve net with channel parse blocks (CPB) to extract pose information in pixel-level. This network allows the proposed model to be robust to complex pose variations while suppressing the redundant backgrounds caused by inaccurate detection and occlusion. Given the extracted pose information, a locally reinforced alignment mode is further proposed to address the misalignment problem between different local parts by considering different local parts along with attribute information in a fine-grained way. Finally, a focal triplet loss is designed to effectively train the entire model, which imposes a constraint on the intra-class and an adaptively weight adjustment mechanism to handle the hard sample problem. Extensive evaluations and analysis on Market1501, DukeMTMC-reid and PETA datasets demonstrate the effectiveness of FGSAM in coping with the problems of misalignment, occlusion and complex poses.																	1057-7149	1941-0042					2020	29						7578	7589		10.1109/TIP.2020.3004267													
J								Simultaneous Reconstruction and Moving Object Detection From Compressive Sampled Surveillance Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low rank approximation; 3D anisotropic total variation; moving object detection; compressed sensing	LOW-RANK APPROXIMATION; BACKGROUND SUBTRACTION; SPARSE; SEGMENTATION; RPCA; COMPLETION; ALGORITHM; FRAMEWORK; NORM	The spatially distributed digital cameras in Wireless Multimedia Sensor Networks (WMSN) are provided with miniature batteries resulting in power constraints. These cameras acquire compressive measurements of video at a rate significantly below the Nyquist rate and transmit them wirelessly in the network. Thus, the encoding side (transmitter) is made less complex at the expense of increased complexity at the more resourceful decoder (receiver). Well grounded on this relevant practical scenario, a unified system is proposed that integrates detection of moving objects into the Compressive Sensing (CS) recovery framework and thereby realizing simultaneous data recovery and object detection in a single optimization problem which guarantees fast response. In this work, a new tensor RPCA approach is proposed to accomplish this requirement. For background separation, low rank approximation is done on the highly correlated background components. A Laplace function based surrogate for tensor tubal rank is formulated to provide adaptive thresholding for the singular value tubes of the background tensor. Moreover, the spatio-temporal continuity of the foreground is explored using 3D-Piecewise Smoothness Constraints combinations based Anisotropic Total Variation (3D-PSCATV) regularization. Additionally, l(1) regularization has been adopted to describe the sparsity of moving objects. The proposed model is solved using Alternative Direction Method of Multipliers (ADMM) scheme. The quantitative and qualitative results validate the superior performance of the proposed method against the compared approaches.																	1057-7149	1941-0042					2020	29						7590	7602		10.1109/TIP.2020.3004696													
J								Just Noticeable Distortion-Based Perceptual Rate Control in HEVC	IEEE TRANSACTIONS ON IMAGE PROCESSING										Human visual perception; JND; HEVC; perceptual rate control	RATE CONTROL SCHEME; LEVEL RATE CONTROL; BIT ALLOCATION; FRAME-LEVEL; VIDEO; OPTIMIZATION; MOTION; CODER	In this paper, we propose a just noticeable distortion (JND)-based perceptual rate control method for high efficiency video coding (HEVC). First, the JND factor of a coding unit has been mathematically shown to be an approximation of the average pixel-level JND weight, which means that it can also be used as a weight for bitrate allocation. Second, rate-distortion (R-D) modelling is conducted based on the JND factor. Finally, the proposed R-D model is integrated into an existing rate control framework to improve the coding efficiency, and the proposed algorithm is implemented in the newest video coding standard. As the experimental results reveal, compared with HEVC reference software, our algorithm achieves significantly improved coding performance, subjective coding quality and bitrate accuracy.																	1057-7149	1941-0042					2020	29						7603	7614		10.1109/TIP.2020.3004714													
J								Spatio-Temporal Memory Attention for Image Captioning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image captioning; spatio-temporal relationship; attention transmission; memory attention; LSTM	VISUAL-ATTENTION; MECHANISMS; MODEL	Visual attention has been successfully applied in image captioning to selectively incorporate the most relevant areas to the language generation procedure. However, the attention in current image captioning methods is only guided by the hidden state of language model, e.g. LSTM (Long-Short Term Memory), indirectly and implicitly, and thus the attended areas are weakly relevant at different time steps. Besides the spatial relationship of attention areas, the temporal relationship in attention is crucial for image captioning according to the attention transmission mechanism of human vision. In this paper, we propose a new spatio-temporal memory attention (STMA) model to learn the spatio-temporal relationship in attention for image captioning. The STMA introduces the memory mechanism to the attention model through a tailored LSTM, where the new cell is used to memorize and propagate the attention information, and the output gate is used to generate attention weights. The attention in STMA transmits with memory adaptively and dependently, which builds strong temporal connections of attentions and learns the spatio-temporal relationship of attended areas simultaneously. Besides, the proposed STMA is flexible to combine with attention-based image captioning frameworks. Experiments on MS COCO dataset demonstrate the superiority of the proposed STMA model in exploring the spatio-temporal relationship in attention and improving the current attention-based image captioning.																	1057-7149	1941-0042					2020	29						7615	7628		10.1109/TIP.2020.3004729													
J								Investigating Task-Driven Latent Feasibility for Nonconvex Image Modeling	IEEE TRANSACTIONS ON IMAGE PROCESSING										Low-level vision; nonconvex image modeling; task-driven feasibility; maximum a posterior	REGULARIZATION; MINIMIZATION; DESIGN; ALGORITHM	Properly modeling latent image distributions plays an important role in a variety of image-related vision problems. Most exiting approaches aim to formulate this problem as optimization models (e.g., Maximum A Posterior, MAP) with handcrafted priors. In recent years, different CNN modules are also considered as deep priors to regularize the image modeling process. However, these explicit regularization techniques require deep understandings on the problem and elaborately mathematical skills. In this work, we provide a new perspective, named Task-driven Latent Feasibility (TLF), to incorporate specific task information to narrow down the solution space for the optimization-based image modeling problem. Thanks to the flexibility of TLF, both designed and trained constraints can be embedded into the optimization process. By introducing control mechanisms based on the monotonicity and boundedness conditions, we can also strictly prove the convergence of our proposed inference process. We demonstrate that different types of image modeling problems, such as image deblurring and rain streaks removals, can all be appropriately addressed within our TLF framework. Extensive experiments also verify the theoretical results and show the advantages of our method against existing state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						7629	7640		10.1109/TIP.2020.3004733													
J								Learning Symmetry Consistent Deep CNNs for Face Completion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face completion; reflectional symmetry; convolutional neural networks		Deep convolutional networks (CNNs) have achieved great success in face completion to generate plausible facial structures. These methods, however, are limited in maintaining global consistency among face components and recovering fine facial details. On the other hand, reflectional symmetry is a prominent property of face images and benefits face analysis and consistency modeling, yet remaining uninvestigated in deep face completion. In this work, we leverage two kinds of symmetry-enforcing modules to form a symmetry-consistent CNN model (i.e., SymmFCNet) for effective face completion. For missing pixels on only one of the half-faces, an illumination-reweighted warping subnet is developed to guide the warping and illumination reweighting of the other half-face. As for missing pixels on both of half-faces, we present a generative reconstruction subnet together with a perceptual symmetry loss to enforce symmetry consistency of recovered structures. The SymmFCNet is constructed by stacking generative reconstruction subnet upon illumination-reweighted warping subnet, and can be learned in an end-to-end manner. Experiments show that SymmFCNet can generate globally consistent results on images with synthetic and real occlusions, and performs favorably against state-of-the-arts.																	1057-7149	1941-0042					2020	29						7641	7655		10.1109/TIP.2020.3005241													
J								Biased Mixtures of Experts: Enabling Computer Vision Inference Under Data Transfer Limitations	IEEE TRANSACTIONS ON IMAGE PROCESSING										Mixtures of experts; constrained data transfer; single shot object detection; single image super resolution; realtime action classification	DIMENSIONALITY	We propose a novel mixture-of-experts class to optimize computer vision models in accordance with data transfer limitations at test time. Our approach postulates that the minimum acceptable amount of data allowing for highly-accurate results can vary for different input space partitions. Therefore, we consider mixtures where experts require different amounts of data, and train a sparse gating function to divide the input space for each expert. By appropriate hyperparameter selection, our approach is able to bias mixtures of experts towards selecting specific experts over others. In this way, we show that the data transfer optimization between visual sensing and processing can be solved as a convex optimization problem. To demonstrate the relation between data availability and performance, we evaluate biased mixtures on a range of mainstream computer vision problems, namely: (i) single shot detection, (ii) image super resolution, and (iii) realtime video action classification. For all cases, and when experts constitute modified baselines to meet different limits on allowed data utility, biased mixtures significantly outperform previous work optimized to meet the same constraints on available data.																	1057-7149	1941-0042					2020	29						7656	7667		10.1109/TIP.2020.3005508													
J								Rain O'er Me: Synthesizing Real Rain to Derain With Data Distillation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image deraining; deep learning; unpaired data; image filter	REMOVAL	We present a weakly-supervised technique for learning to remove rain from images without using synthetic rain software. The method is based on a two-stage data distillation approach, which requires only some unpaired rainy and clean images to generate supervision. First, a rainy image is paired with a coarsely derained version using on a simple filtering technique ("rain-to-clean"). Then a clean image is randomly matched with the rainy soft-labeled pair. Through a shared deep neural network, the rain that is removed from the first image is then added to the clean image to generate a second pair ("clean-to-rain"). The neural network simultaneously learns to map both images such that high resolution structure in the clean images can inform the deraining of the rainy images. Demonstrations show that this approach can address those visual characteristics of rain not easily synthesized by software in the usual way.																	1057-7149	1941-0042					2020	29						7668	7680		10.1109/TIP.2020.3005517													
J								Multimodal Target Detection by Sparse Coding: Application to Paint Loss Detection in Paintings	IEEE TRANSACTIONS ON IMAGE PROCESSING										Sparse representation; target detection; paint loss; kernel; multiple imaging modalities	FACE RECOGNITION; IMAGE; CLASSIFICATION; ART; REMOVAL; CANVAS	Sparse representation based methods have demonstrated their superior performance in target detection tasks compared to more traditional approaches such as matched subspace detectors and adaptive subspace detectors. However, the existing sparsity-based target detection methods were mostly formulated for and validated on a single imaging modality (sometimes with multiple spectral bands). In many application domains, including art investigation, multimodal data, acquired by different sensors are readily available, and yet, efficient processing techniques for such data are still scarce. In this paper, we propose a sparsity-based multimodal target detection method that processes jointly the information from multiple imaging modalities in a kernel feature space, and making use of the spatial context. We develop our target detector such to be robust to errors in labelled data, which is especially important in applications like digital painting analysis, where pixel-wise manual annotations are unreliable. We apply the proposed method to a challenging application of paint loss detection in master paintings and we demonstrate its effectiveness on a case study with multimodal acquisitions of the Ghent Altarpiece.																	1057-7149	1941-0042					2020	29						7681	7696		10.1109/TIP.2020.3005520													
J								CONTENT-BASED FEATURE FUSION REPRESENTATION FOR MARINE INVERTEBRATES	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Feature Fusion; Colour; Shape; Texture; Content-based Image Retrieval (CBIR)	IMAGE RETRIEVAL; CLASSIFICATION; COLOR; TEXTURE	Marine species representation and retrieval is crucial for its studies and conservation. The images of these animals are usually captured underwater with complex background, at different angle, position, and size, which makes it very hard to provide a good representation with the current methods. Most of the current methods only support content-based representation for marine life images with clear background (taken in laboratory or in environments which have been set up), containing just one animal in an image, or the animal is positioned nicely at the centre of the image. Responding to these important needs, a multi-feature method for Content-based Image Retrieval (CBIR) that employs colour, shape, and texture information of marine life images is proposed. The colour feature vectors are obtained by extracting first and second order of Colour Moments. Shape information is constructed through the implementation of Discrete Wavelet transform up to four sub-bands and the extraction of Canny edge feature. Texture features are obtained with the Zernike Moments (ZM) of order four and the extraction of few Grey Level Co-occurrence Matrix properties. We conducted two experiments to determine the best order of ZM as well as to measure the retrieval performance of the proposed descriptor. Retrieval results based on marine invertebrate and Fish4Knowledge datasets clearly shown that the proposed method has effectively obtained the best precision value at 11 standard recall levels (72.42%) and MAP value (67.7%). The proposed method is further measured based on the statistical two-tailed paired t-test and has revealed a significant improvement in retrieval effectiveness.																	0127-9084						2020	33	3					170	187		10.22452/mjcs.vol33no3.1													
J								FSI DRIVENERGY: MITIGATING SMARTPHONE ENERGY CONSUMPTION USING FUZZY INFERENCE	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Energy Efficiency; Context-Awareness; Smartphone; Fuzzy Inference	CONTEXT AWARENESS; MANAGEMENT	Smartphones today help people accomplish daily activities, from simple tasks such as taking a reminder, to more complicated processes such as predicting road traffic while driving. However, performing complicated processes on smartphones cost the device's energy to drain much faster. As an effect, the phone's main purpose which is to enable communication whenever it is needed is degraded in terms of performance. In relation to this problem, researchers have tried to mitigate smartphone's energy usage. An area that has yet to be further explored, is on examining smartphone's context while the user is travelling. This paper presents a method to mitigate smartphone's energy usage by gathering its context and to analyze it using Fuzzy Inference technique whilst the user is driving and using navigational application at the same time. Our finding shows that using this method can improve smartphone's battery lifespan at an average rate, ranging between 1.6% to 3.1% for every hour with up to 19% of energy saving. These results proved that analyzing smartphone using context-awareness and Fuzzy Inference can reduce smartphone's energy usage whilst the user is travelling, which in turn improves the overall energy efficiency in smartphone.																	0127-9084						2020	33	3					188	201		10.22452/mjcs.vol33no3.2													
J								A SYSTEM FOR RECOGNITION OF DESTINATION ADDRESS IN POSTAL DOCUMENTS OF INDIA	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Destination Address; Indian script recognition; Indian Pin code recognition; Document analysis; Lexicon information	CITY-NAME RECOGNITION; WRITTEN; BANGLA	Recognition of destination address is compulsory for automation of the postal system in India. Our observation found that such recognition becomes a very challenging task due to inter-mixing of three languages (Hindi, English and the official language of the particular state in which the postal document is supposed to reach).In this paper, our attempt towards development of a dynamic programming based system for city-name and pin code recognition of destination address in postal documents of India not only managed to address the difficulties related to identification of the scripts but also managed to get rid of those problems which is generated due to character touching in postal documents. For city-name recognition, lexicon information is used. However, no lexicon information is used for pin code recognition since an Indian pin code contains only 6 digits. We obtained 99.55% reliability from tri-lingual city-name recognition system where error rates are 0.20% and rejection rates are 28.11%. From our experiment on recognition of handwritten pin codes, 99.01% reliability rate, 0.83% error rate and15.27% rejection rate are obtained. Furthermore, to enhance the city-name results by distributing the lexicon size district-wise, we conducted an experiment and presented the results.																	0127-9084						2020	33	3					202	216		10.22452/mjcs.vol33no3.3													
J								AN ARTICLE REORGANIZATION MODEL BASED ON EMOTION IMPLIED IN FORUM ARTICLES	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Virtual Forum; Sentences Similarity; Emotion Determination; Article Statement Reorganization	KNOWLEDGE; TECHNOLOGY; EXTRACTION	At present, the virtual forums are free and convenient speaking platforms, according to the speaking specifications and through the examination of forum administrators, the forum users can publish various articles easily. However, as the number of users increases, it is difficult for the forum administrators to check them one by one, and return all the articles to be revised. Also, the publishers (article providers) may write violative words unconsciously, so the article writers infringe the forum specifications with the publication, and shall revise the violative articles. Therefore, this paper develops an Article Reorganization Model based on Emotion Implied in Forum Articles including Article Expressed Emotion Determination Module and Publisher's Article Statement Reorganization Module. The first module deduces the emotion type of articles by analyzing representative events of articles, creating emotion word membership coefficient, analyzing similar statements and analyzing emotion probability and stable value. The second module uses integrated semantic similarity analysis, review score analysis and multiple combined sentences establishment, the article's statement structure can be reorganized. This paper builds a Web-based system and a real-world case is applied to confirm the feasibility of this methodology. For forum administrators, violative articles can be extracted rapidly from the articles with specific emotions and the violative statements will be reorganized. For article writers, the statements of violative articles can be revised automatically to save the time for revising violative articles.																	0127-9084						2020	33	3					217	239		10.22452/mjcs.vol33no3.4													
J								SENTI2VEC: AN EFFECTIVE FEATURE EXTRACTION TECHNIQUE FOR SENTIMENT ANALYSIS BASED ON WORD2VEC	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Sentiment analysis; SentiWordNet; Word2Vec; Word embeddings	REPRESENTATIONS; CLASSIFICATION; NETWORKS	The discovery of an active feature extraction technique has been the focus of many researchers to improve the performance of classification methods, such as for sentiment analysis. Many of them have shown interest in using word embeddings especially Word2Vec as the features for text classification tasks. Its ability to model high-quality distributional semantics among words has contributed to its success in many of the functions. Despite the success, Word2Vec features are high dimensional that lead to an increase in the complexity of the classifier. In this paper, an effective method for feature extraction based on Word2Vec is proposed for sentiment analysis. The process discovers polarity clusters of the terms in the vocabulary through Word2Vec and opinion lexical dictionary. The features vector for each text is constructed from the polarity clusters, which lead to a lower-dimensional vector to represent the text. This paper also investigates the effect of two opinion lexical dictionaries on the performance of sentiment analysis, and one of the dictionaries are created based on SentiWordNet. The effectiveness of the proposed method is evaluated on the IMDB with two classifiers, namely the Logistic Regression and the Support Vector Machine. The result is promising, showing that the proposed method can be more effective than the baseline approaches.																	0127-9084						2020	33	3					240	251		10.22452/mjcs.vol33no3.5													
J								FACEST: feedback-assisted estimation of end-to-end capacity in IP-based communication networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Capacity estimation; ensemble estimation; testbed; network measurement	BANDWIDTH ESTIMATION	The end-to-end capacity, defined as the maximal transmission rate of the weakest link on the entire path between two end hosts, plays an important role in efficient network design and management. Although various capacity estimation tools have been proposed in the literature, there is still uncertainty in their accuracy and reliability when they are used in today's IP-based communication networks. The main reason for this is that all current capacity estimation tools only yield a potential candidate for an acceptable estimate, without being aware of its reliability level. In this study, we propose a new feedback-assisted end-to-end capacity estimation (FACEST) procedure that not only produces a candidate for a potentially acceptable estimate but also improves and categorizes its reliability level. Particularly, FACEST follows an ensemble estimation approach which meaningfully utilizes the correlation among the estimates produced by 3 independent capacity estimation tools; namely pathrate, DietTOPP and PBProbe. Through the correlation of 3 individual estimates, additional information about their reliability level is gained and, if necessary, the experiment is iteratively repeated with different sets of measurement parameter values until the required level of estimation accuracy is achieved, or in the worst case a kernel density estimator is applied on the collected experiment results. The proposed ensemble estimation approach has been implemented in a tool called FACEST, the performance of which has experimentally been evaluated on a three-hop testbed using a variety of tests with several scenarios and degrees of cross-traffic. For comparison purposes, individual experiments with pathrate, DietTOPP and PBProbe as well as with other alternative hybrid estimation tool from literature have also been conducted. The results reveal that FACEST outperforms individual and other hybrid capacity estimation tools and yields up to 18.29% lower estimation errors along with additional consistent information about the reliability level of the produced estimates.																	1300-0632	1303-6203					2020	28	4					1803	1823		10.3906/elk-1906-1													
J								Deep reinforcement learning for acceptance strategy in bilateral negotiations	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Deep reinforcement learning; automated bilateral negotiation; acceptance strategy	AGENT; OPPONENT	This paper introduces an acceptance strategy based on reinforcement learning for automated bilateral negotiation, where negotiating agents bargain on multiple issues in a variety of negotiation scenarios. Several acceptance strategies based on predefined rules have been introduced in the automated negotiation literature. Those rules mostly rely on some heuristics, which take time and/or utility into account. For some negotiation settings, an acceptance strategy solely based on a negotiation deadline might perform well; however, it might fail in another setting. Instead of following predefined acceptance rules, this paper presents an acceptance strategy that aims to learn whether to accept its opponent's offer or make a counter offer by reinforcement signals received after performing an action. In an experimental setup, it is shown that the performance of the proposed approach improves over time.																	1300-0632	1303-6203					2020	28	4					1824	1840		10.3906/elk-1907-215													
J								Assessment of environmental factors affecting software reliability: a survey study	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Software reliability; survey; software engineering; environmental factors		Currently, many systems depend on software, and software reliability as such has become one of the key challenges. Several studies have been carried out that focus on the impact of external environmental factors that impact software reliability. These studies, however, were all carried out in the same geographical context. Given the rapid developments in software engineering, this study aims to identify and reinvestigate the environmental factors that impact software reliability by also considering a different context. The environmental factors that have an impact on software reliability as reported in earlier studies have been analyzed and synthesized. Subsequently, a survey study is conducted to analyze the impact of 32 environmental factors from the perspective of multiple stakeholders. Several statistical analysis methods were applied for the analysis. Data were collected from 24 organizations and 70 software professionals. Most factors shown in top 10 lists of previous studies remain in the top 10 in our study, but their order is different. Testing coverage is now the most significant factor and testing effort is considered as the second most significant factor. The environmental factors defined previously retain their impact. The ordering of the importance of the environmental factors has changed though.																	1300-0632	1303-6203					2020	28	4					1841	1858		10.3906/elk-1907-49													
J								ZEKI: unsupervised zero-day exploit kit intelligence	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Exploit kit; web malware; drive-by download; URL analysis; unsupervised machine learning; cybercrime	BY DOWNLOAD ATTACKS	Over the last few years, exploit kits (EKs) have become the de facto medium for large-scale spread of malware. Drive-by download is the leading method that is widely used by EK flavors to exploit web-based client-side vulnerabilities. Their principal goal is to infect the victim's system with a malware. In addition, EK families evolve quickly, where they port zero-day exploits for brand new vulnerabilities that were never seen before and for which no patch exists. In this paper, we propose a novel approach for categorizing malware infection incidents conducted through EKs by leveraging the inherent "overall URL patterns" in the HTTP traffic chain. The proposed approach is based on the key finding that EKs infect victim systems using a specially designed chain, where EKs lead the web browser to download a malicious payload by issuing several HTTP requests to more than one malicious domain addresses. This practice in use enables the development of a system that is capable of clustering the responsible EK instances. The method has been evaluated with a popular and publicly available dataset that contains 240 different real-world infection cases involving over 2250 URLs, the incidents being linked with the 4 major EK flavors that occurred throughout the year 2016. The system achieves up to 93.7% clustering accuracy with the estimators experimented.																	1300-0632	1303-6203					2020	28	4					1859	1870		10.3906/elk-1908-150													
J								Multiplicative-additive despeckling in SAR images	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Synthetic aperture radar; despeckling; variational methods; general speckle model; multiplicative gamma noise; additive Gaussian noise	SPECKLE REDUCTION; NOISE REMOVAL; FILTER	Visual and automatic analyses using synthetic aperture radar (SAR) images are challenging because of inherently formed speckle noise. Thus, reducing speckle noise in SAR images is an important research area for SAR image analysis. During speckle noise reduction, homogeneous regions should be smoothed while details such as edges and point scatterers need to be preserved. General speckle noise model contains gamma distributed multiplicative part which is dominant and Gaussian distributed additive part which is in low amount and mostly neglected in literature. In this study, a novel sparsity-driven speckle reduction method is proposed that takes both multiplicative noise model and additive noise model into consideration. The proposed speckle reduction method uses a cost function with multiplicative and additive data terms besides the total variation smoothness term. Also, an efficient and stable numerical minimization scheme is proposed for the proposed cost function that deals with multiplicative and additive noise. Speckle reduction performance of the proposed method is shown on synthetically generated SAR images and real-world SAR images.																	1300-0632	1303-6203					2020	28	4					1871	1885		10.3906/elk-1908-163													
J								ReMAC: a novel hybrid and reservation-based MAC protocol for VANETs	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Vehicular ad hoc networks; reservation; DSRC; multiple channel access; MAC protocol	SECURITY CHALLENGES	The period in which a roadside unit (RSU) of a vehicle remains in the coverage area is too short in vehicular ad hoc networks (VANETs) in which the vehicles move at high speeds. In this short period, it is necessary for the vehicles to join the network and establish continuous communication with the RSU. To accomplish this, the RSU and vehicles need an efficient and robust media access control (MAC) protocol. In this study, a new reservation-based MAC (ReMAC) protocol in which multiple channels are used is suggested to ensure rapid connection to RSUs, smooth transition between RSUs, and efficient channel usage. The protocol uses the dedicated short-range communication (DSRC) standard's data/control channels by dividing them into multiple subchannels to support efficient channel usage, rapid service provision, and high vehicle density scenarios. The simulation of the protocol was conducted with OPNET Modeler and the performance comparisons were carried out with the metrics such as channel access delay, channel collision rate, success rate, and throughput. These are the metrics that are used in VeMAC, CFR-MAC, TMMAC, and VATMAC protocols that exist in this area. All the simulation results show in all scenarios that ReMAC significantly surpassed the current protocols.																	1300-0632	1303-6203					2020	28	4					1886	1904		10.3906/elk-1908-94													
J								Bases of polymatroids and problems on graphs	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Submodular function; bases of polymatroid; hamiltonian path and circuit; traveling salesman problem		In the paper, we present new theorems to show that a Hamiltonian path and circuit on an undirected graph can be formulated in terms of bases of polymatroids or extended polymatroids associated with submodular functions defined on subsets of the node-set of a given graph. In this way, we give a new formulation of the well-known traveling salesman problem including constraints in these terms. The main result in the paper states that using a special base of the polymatroid, a Hamiltonian path on an undirected graph can be solved effectively. Since the determination of a Hamiltonian circuit can be reduced to finding a Hamiltonian path between some node and its adjacent nodes, an efficient Hamiltonian path algorithm will lead to solving the Hamiltonian circuit problem. Finding some special base is the main problem in solving these NP-hard problems.																	1300-0632	1303-6203					2020	28	4					1905	1915		10.3906/elk-1909-102													
J								Harmonic reduction of SVC with system integrated APF	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Static var compensators (SVC); active power filters (APF); feedback control; feedforward control; DC-link control	POWER; STRATEGY; DESIGN	Static vAR compensators (svcs) are widely used to compensate for reactive power in a system. The hybrid active power filter (HAPF) in combination with svc has extensively been studied in the literature to reduce harmonics generated by the svc. This study proposes a new topology of svc for three-phase systems based on a three-phase thyristor-control reactor (TcR). The harmonics generated by the TcR are minimized by an active power filter (APF), which can be realized by a reduced resonant capacitor size. A control strategy comprising feedback and feedforward control is employed to achieve good harmonic reduction and fast transient response. Simulated and experimental results are presented to validate the theoretical analysis and performance of the proposed topology and control scheme. The results show that the proposed topology can mitigate the harmonics generated by the TcR.																	1300-0632	1303-6203					2020	28	4					1916	1931		10.3906/elk-1909-115													
J								A novel genome analysis method with the entropy-based numerical technique using pretrained convolutional neural networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										DNA; genome analysis; convolutional neural network; classification; entropy-based mapping technique	REPRESENTATION; SEQUENCES; ENSEMBLE	The identification of DNA sequences as exon and intron is a common problem in genome analysis. The methods used for feature extraction and mapping techniques for the digitization of sequences affect directly the solution of this problem. The existing mapping techniques are not enough to detect coding and noncoding regions in some genomes because the digital representation of each base in a DNA sequence with an integer does not fully reflect the structure of an original DNA sequence. In the entropy-based mapping technique, we could overcome this problem because the technique deepens distinction rates of exon regions, and better reflects the complexity of DNA sequences. Moreover, in the literature, features are extracted by using various statistical techniques. The statistical features to be extracted are chosen by a system designer's experience. The other proposed approach in this study is to carry out the feature extraction using the transfer learning method. Transfer learning and feature extraction are performed automatically by convolutional neural network models as independent of the data set. In this study, we propose a new method to classify DNA sequences as exon and intron using two approaches. In the first approach, the entropy-based numerical technique was used for the numerical representation of DNA sequences. In the second approach, transfer learning was used to extract features. Then, the obtained features were classified by support vector machine and k-nearest neighbors algorithm. As a result of the classification, accurate performance with 97.8% was achieved. The performance of the current method was compared with the other numerical mapping techniques and feature extraction methods. The results showed that the developed method was much more successful than other methods.																	1300-0632	1303-6203					2020	28	4					1932	1948		10.3906/elk-1909-119													
J								Quasi-physical modeling of robot IRB 120 using Simscape Multibody for dynamic and control simulation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Dynamic model; quasi-physical modeling; robot manipulator; Simscape Multibody	DESIGN	The mathematical model of robot that is used to design control algorithms is mostly reused in numerical simulations as a virtual plant. The use of same model for both control design and simulation tasks makes the outcome idealized. Consequently, the effectiveness and feasibility of designed control methodologies when applied in practice seem to be questionable. The paper presents the quasi-physical modeling of a 6-DOF robot using MATLAB/Simscape Multibody for dynamic and control simulation. The bodies of the robot are assembled into a physical network with connections that represent physical domains. The dynamical manners of the quasi-physical model are close to that of real robot manipulators. This model can be exploited to verify the accuracy of mathematical models. After designing process, the control laws are validated with this model instead of an ideal mathematical model or an actual expensive prototype. The efficiency of the proposed modeling approach is demonstrated through the dynamic and control simulation of robot IRB 120.																	1300-0632	1303-6203					2020	28	4					1949	1964		10.3906/elk-1909-131													
J								Performance analysis of a fuzzy disparity selector for stereo matching of image segments under radiometric variations	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Stereo matching; fuzzy logic; Middlebury stereo dataset; radiometric variations; multiple data costs	COST	Stereo matching algorithms generate disparity maps, which contain the depth information of the environment, from two or more images of a scene taken from different viewpoints. The process of obtaining dense disparity maps is a problem which is still being actively researched. The presence of radiometric differences in the images only further complicates the stereo matching problem. In the present research work, the images are initially split into small patches of pixels, such that pixels in each patch have similar intensities. The authors attempt to study the effect of the parameters, namely, tuning parameter 'alpha' and the number of segments, while the images are subjected to variations in exposure and illumination. The value 'alpha' performs the function of a weight signifying the contribution of each data cost, when the two data costs are combined in a linear fashion. Lastly, the results of this methodology are compared with other methods that try to tackle the problem of stereo matching under radiometric variations.																	1300-0632	1303-6203					2020	28	4					1965	1983		10.3906/elk-1909-17													
J								Improvement of the distribution network state estimation with increase of accurate information and using a two-step method	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Distribution network; meters placement process; network reduction process; state estimation; two-step state estimation; weighted least squares error	FALSE DATA INJECTION; UNCERTAINTY; ATTACKS; SYSTEMS	Distribution networks (DNs) are gradually changing and this makes their control and utilization complicated. State estimation (SE) plays a significant role in active distribution networks. The performance of the energy management center in modern distribution networks is highly dependent on the results obtained from the SE. In the present study, considering the shortage of measurements in the DN, a two-step state estimation method with a new network reduction process (NRP) is proposed. In the proposed method, a new NRP is used in a two-step state estimation method to improve the performance of SE in a DN. Obtaining accurate initial information on the network condition improves the performance of SE. The initial SE is performed using a new NRP process to obtain accurate initial data. This information is used as the measurement to improve the performance of secondary SE. This method resolves the shortage of accurate measurements and redundancy measurements, and it improves network SE accuracy without adding any real-time measurement. Moreover, the proposed method is economically affordable. Simulations are performed on the 18-bus UK radial feeder and the IEEE 69-bus distribution network in MATLAB software to guarantee valid operation of the proposed style.																	1300-0632	1303-6203					2020	28	4					1984	2003		10.3906/elk-1909-7													
J								Finite-time dynamic surface approach to nonlinear systems with mismatched uncertainties	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										High-order siding mode observer; dynamic surface control; extended state observer; finite-time convergence; sliding mode control	TRACKING CONTROL; SLIDING MODES	This paper develops a finite-time dynamic surface control (DSC) scheme for nonlinear systems with mis-matched uncertainties via a high-order sliding mode (HOSM) observer. By designing a second-order terminal sliding surface based on the estimated signals, an observer-based sliding mode control (SMC) is designed to counteract the mismatched uncertainties in each step of backstepping. The proposed DSC scheme exhibits the following two attractive features. One is the application of HOSM observer to deal with mismatched system uncertainty functions. This is very different from the traditional approximator-based adaptive methods in dealing with high-order uncertain nonlinear sys-tems. The other is the finite-time convergence of the provided algorithm, which guarantees the transient performance of tracking signals. Especially, the finite convergence time is explicitly given in the controller design and stability analysis. Simulation results of numerical example illustrates that the proposed approach shows better control performance than traditional approximators-based adaptive methods.																	1300-0632	1303-6203					2020	28	4					2004	2019		10.3906/elk-1909-75													
J								Detection of BGA solder defects from X-ray images using deep neural network	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										BGA; X-Ray; DNN; short-circuit; bonding defect; void defect		In the literature it is observed that complex image processing operations are used in the classification of Ball Grid Array (BGA) X-ray images, however high classification results were not achieved. In recent years, it has been shown that deep learning methods are very successful especially in classification problems. In this study, a new deep neural network (DNN) model is proposed to classify the BGA X-ray images. The proposed DNN model contains feature extractor layers and a minimum distance classifier. Since the proposed network consists of less number of layers (4 convolution layers and 1 fully connected layer), determination of the hyper-parameters of the network and training of the network are accomplished in a short time. BGA X-ray images are categorized into 4 classes according to the conditions of the solder joints: normal, short-circuit, bonding defect and void defect. The dataset used in this study is comprised of 67, 76, 53 and 76 images for these classes, respectively. 80% of all data is allocated for the training set and the remaining 20% is allocated for the test set. Compared with the existing methods in the literature, a very high success rate of 97% is achieved for the classification of BGA X-ray images with the proposed method.																	1300-0632	1303-6203					2020	28	4					2020	2029		10.3906/elk-1910-135													
J								Sketic: a machine learning-based digital circuit recognition platform	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Digital logic circuits; logic gates; machine learning; deep neural networks; detection; recognition; classification		In digital system design, digital logic circuit diagrams are built using interconnects and symbolic representations of the basic logic gates. Constructing such diagrams using free sketches is the first step in the design process. After that the circuit schematic or code has to be generated before being able to simulate the design. While most of the mentioned steps are automated using design automation tools, drafting the schematic circuit and then converting it into a valid format that can be simulated are still done manually due to the lack of robust tools that can recognize the free sketches and incorporate them into end user simulators. Hence, the goal of this paper is to construct and deploy computer simulation tools capable of understanding free sketches and incorporate them into useful simulation tools. Such a tool will be useful at both the educational and the industrial levels. Moreover, while this tool is designed to deal with sketched logic circuits, it can be generalized and applied to many other fields to convert the sketched design into a digital format. To implement this tool, we relied on the emerging machine learning and image processing concepts to make sure that the designed system is robust and accurate. Our results show that our system is able to recognize all the gates in the digital circuit with more than 95% accuracy.																	1300-0632	1303-6203					2020	28	4					2030	2045		10.3906/elk-1910-16													
J								An efficient reliability simulation tool for lifetime-aware analog circuit synthesis	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Reliability; aging; simulation; CAD; EDA; analog; synthesis		Time-dependent degradation (aging) has become more severe in modern CMOS technologies. Therefore, it is highly critical to capture the variation effects and design reliable circuits against aging. Simulation of time-dependent variations is quite complicated since the degradation is a function of time, where the step count of simulation directly affects the accuracy and the efficiency of the analysis. Commercial simulator tools use a constant step count during reliability simulations, in which choosing a large step count degrades the efficiency whereas keeping it small may result in accuracy problems. To overcome this bottleneck, a couple of different approaches have been proposed in the literature. Nevertheless, they suffer from the initial workload during step count determination and some other accuracy problems. In this study, a two-level step count determination approach is presented, in which the step count induced estimation error can be promptly determined via an effective simulation strategy at the first level. At the second level, the error is fitted into a saturated power law model; thus, the efficient step count can be determined without any simulation effort. To demonstrate the developed tool, two case study circuits have been designed using 130 nm technology parameters. According to the simulation results, the proposed approach decreases the initial workload by up to 67%. The proposed approach provides a remarkable save in computation time and can be used for all analog circuits without loss of generality. Moreover, a reliability-aware analog circuit synthesis tool is implemented to demonstrate the efficiency of the proposed approach. The developed tool provides a 37% improvement in the computation time compared to the only tool in the literature.																	1300-0632	1303-6203					2020	28	4					2046	2059		10.3906/elk-1910-22													
J								Dynamic optimal management of a hybrid microgrid based on weather forecasts	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Economic scheduling; hybrid microgrid management; linear optimization; rolling horizon; near future renewable power prediction; weather forecast		Hybrid microgrids containing both renewable and conventional power sources are becoming increasingly attractive for a variety of reasons. However, intermittency of renewable power production and uncertainty in future load prediction increase risks of electric grid instability and, by consequence, restrict the portion of renewable power production in microgrids. In order, to prefigure the upcoming renewable power production, particularly, wind power and photovoltaic power, we suggest using weather forecasts. In addition to illustrating short term renewable power prediction based on ensemble weather forecasts, this paper focuses on optimizing the management of distributed power generation, power storage, and power exchange with the commercial electric grid. Establishing an optimal operating plan for the grid makes the integration of the predicted renewable production more efficient. The optimization problem is formulated as an integer linear program. The operating plan is initially optimized over the upcoming day and then regularly updated and incremented to cover a longer time horizon. After analyzing the obtained plans, we test and evaluate them relative to the observed weather conditions. Finally, we investigate some practical problems that had arisen and we apply a time cascade optimization technique to mitigate the effects of initial conditions and end-of-horizon effects, as well as to take advantage of updated forecasts.																	1300-0632	1303-6203					2020	28	4					2060	2076		10.3906/elk-1910-6													
J								Transmit antenna selection schemes for STBC-SM	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Antenna selection; spatial modulation; diversity; space time block coded spatial modulation	SPATIAL MODULATION; PERFORMANCE; CODES	The coupling of space-time block codes (STBCs) with the spatial modulation (SM) scheme can introduce a spatial diversity to the SM systems. This paper presents antenna selection techniques to provide an additional coding gain with the space-time block coded spatial modulation (STBC-SM). Specifically, the selection of optimal transmit antenna elements at the transmitter to send a spatially modulated space-time block code is studied. This paper studies the effects of applying the capacity optimized antenna selection (COAS) algorithm with the STBC-SM. The COAS algorithm has a suboptimal performance with a trade-off between complexity and bit-error-rate (BER) enhancement. Simulation results for STBC-SM with a spectral efficiency of 3 bits/s/Hz have been introduced with and without antenna selection.																	1300-0632	1303-6203					2020	28	4					2077	2087		10.3906/elk-1910-92													
J								The quantum version of the shifted power method and its application in quadratic binary optimization	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Quantum algorithms; discrete optimization; power iteration		In this paper, we present a direct quantum adaptation of the classical shifted power method. The method is very similar to the iterative phase estimation algorithm; however, it does not require any initial estimate of an eigenvector, and as in the classical case its convergence and the required number of iterations are directly related to the eigengap. If the amount of the gap is in the order of 1/poly(n), then the algorithm can converge to the dominant eigenvalue in O(poly(n)) time. The method can be potentially used for solving any eigenvalue related problem and finding minimum/maximum of a data set in lieu of Grover's search algorithm. In addition, if the solution space of an optimization problem with n parameters is encoded as the eigenspace of an 2n dimensional unitary operator in O(poly(n)) time and the eigengap is not too small, then the solution for such a problem can be found in O(poly(n)). As an example, using the quantum gates, we show how to generate the solution space of the quadratic unconstrained binary optimization as the eigenvectors of a diagonal unitary matrix and find the solution for the problem.																	1300-0632	1303-6203					2020	28	4					2088	2095		10.3906/elk-1910-99													
J								Image denoising using deep convolutional autoencoder with feature pyramids	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Image denoising; convolutional autoencoder; feature pyramid; image processing	QUALITY	Image denoising is 1 of the fundamental problems in the image processing field since it is the preliminary step for many computer vision applications. Various approaches have been used for image denoising throughout the years from spatial filtering to model-based approaches. Having outperformed all traditional methods, neural-network-based discriminative methods have gained popularity in recent years. However, most of these methods still struggle to achieve flexibility against various noise levels and types. In this paper, a deep convolutional autoencoder combined with a variant of feature pyramid network is proposed for image denoising. Simulated data generated by Blender software along with corrupted natural images are used during training to improve robustness against various noise levels. Experimental results show that the proposed method can achieve competitive performance in blind Gaussian denoising with significantly less training time required compared to state of the art methods. Extensive experiments showed the proposed method gives promising performance in a wide range of noise levels with a single network.																	1300-0632	1303-6203					2020	28	4					2096	2109		10.3906/elk-1911-138													
J								Passenger scoring for free-pass promotion in public transportation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Information systems; scoring; public transportation; RFM model; mobile application	CUSTOMER RELATIONSHIP MANAGEMENT; NEW-MODEL; INDUSTRY; SYSTEMS; SEGMENTATION; FRAMEWORK; TRANSIT; MARKET	The focus of promotions targeted to increase the use of public transportation concentrates on increasing the attractiveness of it, particularly by decreasing transportation fares. To serve that purpose, this paper proposes a novel passenger scoring model, namely RFLT (recency, frequency, loyalty, and time), for offering a free-pass promotion in public transportation. It presents the comparison results of RFLT and wRFLT (weighted version) using a real-world dataset obtained by a near field communication (NFC) mobile payment application. The experimental results show that the w-RFLT model provides a more balanced score distribution than the RFLT model, and the frequency parameter (F), among four metrics (R, F, L, and T), influences the scoring results the most. The results of this study can be used to establish an efficient policy for increasing public transportation ridership.																	1300-0632	1303-6203					2020	28	4					2110	2127		10.3906/elk-1911-15													
J								A content-based recommender system for choosing universities	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Recommender system; content-based filtering; decision-making system; information retrieval; private university admission; data normalization		Recommender system (RS) is a knowledge discovery and decision-making system that has been extensively used in a myriad of applications to assist people in making distinct choices from vast sources. This paper proposes a recommendation system that will help the prospective students of Bangladesh in choosing the most suitable private universities for getting admission. Since selecting the best private university does not depend merely on a few criteria or choices and making a decision considering all those criteria is not an easy task, a recommendation system can be of great assistance in this scenario for the prospective students. In this proposed recommendation system a list of top -K private universities is recommended to the students who are willing to get admitted to the private universities using content-based filtering technique. To attain this goal we considered six parameters, namely grade point average (GPA) of secondary school certificate (SSC) examination, GPA of higher secondary certificate (HSC) examination, total GPA, tuition fees, university ratings, and university rankings. Finally, we evaluated the system with a total of 947 real feedback from prospective students and obtained the accuracies of 89.05%, 95.85%, 48%, 92.32%, and 71.93% using 5 different performance metrics: precision, recall, specificity, F1 score, and balanced accuracy, respectively.																	1300-0632	1303-6203					2020	28	4					2128	2142		10.3906/elk-1911-37													
J								Improving performance of indoor localization using compressive sensing and normal hedge algorithm	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Wireless sensor networks; indoor localization; direct position determination; compressive sensing; normal hedge	DIRECT POSITION DETERMINATION; NETWORKS	Accurate indoor localization technologies are currently in high demand in wireless sensor networks, which strongly drive the development of various wireless applications including healthcare monitoring, patient tracking and endoscopic capsule localization. The precise position determination requires exact estimation of the time varying characteristics of wireless channels. In this paper, we address this issue and propose a three-phased scheme, which employs an optimal single stage TDOA/FDOA/AOA indoor localization based on spatial sparsity. The first contribution is to formulate the received unknown signals from the emitter as a compressive sensing problem. Then, we solve an l1 minimization problem to localize the emitter's position. To combat the nonstationary behavior of wireless channels between sensor nodes, the results of our proposed localization algorithm are finally fused using a novel fusion method based on the adaptive normal hedge algorithm. To improve the accuracy of the estimated location, an optimal set of weighed coefficients are derived through introducing a new loss function. Monte Carlo simulation results show that the accuracy of the proposed localization framework is superior compared to the existing indoor localization schemes in low SNR regimes.																	1300-0632	1303-6203					2020	28	4					2143	2157		10.3906/elk-1911-45													
J								Replica bias circuit for common-source amplifier	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Common-source amplifier; low-noise amplifier; design for manufacturing		A replica bias circuit to set the current of common-source amplifier to reduce the gain variations across process, voltage, and temperature (PVT) changes is proposed. The gain of a common-source amplifier is set by the load resistor and transistor transconductance which is set proportional to a resistor using a constant-gm bias circuit. The success of constant-gm biasing depends on the accuracy of copying of generated current to the transistor. The leakage current at the transistor gate due to the electrostatic discharge protection diodes prevents the matching of the common source transistor current to the constant-gm circuit current. A low-power replica of the common-source amplifier is used to determine the current error which is minimized using a feedback circuit. Corner simulations indicate that gain variation across PVT reduced to +/- 1.9 dB using the proposed biasing method as opposed to +/- 9.9 dB for traditional biasing method for 1 mu A leakage current. Monte Carlo simulations with process and mismatch indicate that the standard deviation of the gain is reduced to 0.34 dB from 3.57 dB.																	1300-0632	1303-6203					2020	28	4					2158	2164		10.3906/elk-1912-113													
J								A random subspace based conic functions ensemble classifier	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Random subspace; classification; linear programming; ensemble learning; conic functions	LARGE-SCALE CLASSIFICATION; FEATURE-SELECTION; ALGORITHM; MACHINE; DATASET	Classifiers overfit when the data dimensionality ratio to the number of samples is high in a dataset. This problem makes a classification model unreliable. When the overfitting problem occurs, one can achieve high accuracy in the training; however, test accuracy occurs significantly less than training accuracy. The random subspace method is a practical approach to overcome the overfitting problem. In random subspace methods, the classification algorithm selects a random subset of the features and trains a classifier function trained with the selected features. The classification algorithm repeats the process multiple times, and eventually obtains an ensemble of classifier functions. Conic functions based classifiers achieve high performance in the literature; however, these classifiers cannot overcome the overfitting problem when it is the case data dimensionality ratio to the number of samples is high. The proposed method fills the gap in the conic functions classifiers related literature. In this study, we combine the random subspace method and a novel conic function based classifier algorithm. We present the computational results by comparing the new approach with a wide range of models in the literature. The proposed method achieves better results than the previous implementations of conic function based classifiers and can compete with the other well-known methods.																	1300-0632	1303-6203					2020	28	4					2165	2182		10.3906/elk-1911-89													
J								A 88 mu W digital phase-domain GFSK demodulator compatible with low-IF and zero-IF receiver with preamble detection for BLE	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Bluetooth; BLE; GFSK; demodulator	ADC; OFFSET	Unlike conventional analog-to-digital converter (ADC), phase-domain ADC (Ph-ADC) is more power efficient for the implementation of fully digital Gaussian frequency shift keying (GFSK) demodulator in bluetooth low energy (BLE). Besides, Ph-ADC based demodulator is flexible to pair with low-IF and zero-IF receiver, opposed to limiter based demodulator that work with low-IF receiver only. Yet, currently reported Ph-ADC based demodulator lack of preamble detection for BLE which will be used as symbol clock synchronization. In this work, a Ph-ADC based demodulator is proposed with the feature of preamble detection on BLE's packet. The detected preamble is used for symbol clock recovery and compensation of carrier frequency offset in a BLE packet. Besides, the proposed demodulator is flexible to demodulate IF or baseband signal by simply configuring a parameter value. Using MATLAB, minimum signal-to-noise Ratio (SNR) needed to demodulate BLE packet is estimated using Monte Carlo simulation with 99% confidence level. For hardware implementation, the proposed demodulator is implemented at RTL in Synopsys and its layout is generated using 0.18 mu m CMOS technology. To understand the trade-off between power consumption, layout size and minimum SNR needed, the proposed Ph-ADC based demodulator is scaled to a different combination of 4-bit to 6-bit resolution and 2 MHz to 16 MHz sampling rate. Configuration with the best trade-off for the proposed Ph-ADC demodulator can achieve bit error rate (BER) of 0.1% at SNR of 12.5 dB and able to tolerate carrier frequency offset of +/- 200 kHz while using only half the power needed by state of the art limiter based demodulator.																	1300-0632	1303-6203					2020	28	4					2183	2199		10.3906/elk-1912-155													
J								BIBSQLQC: Brown infomax boosted SQL query clustering algorithm to detect anti-patterns in the query log	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										SQL log analysis; patterns and antipatterns; Brown infomax boosting; query; clustering	INJECTION	Discovery of antipatterns from arbitrary SQL query log depends on the static code analysis used to enhance the quality and performance of software applications. The existence of antipatterns reduces the quality and leads to redundant SQL statements. SQL log includes a large load on the database and it is difficult for an analyst to extract large patterns in a minimal time. Existing techniques which discover antipatterns in SQL query face a lot of innumerable challenges to discover the normal sequences of queries within the log. In order to discover the antipatterns in the log, an efficient technique called Brown infomax boosted SQL query clustering (BIBSQLQC) technique is introduced. Initially, the number of patterns (i.e. queries) are extracted from the SQL query log. After extracting the patterns, the ensemble clustering process is carried out to find out the antipatterns from the given query log. The Brown infomax boost clustering is an ensemble learning method for grouping the patterns by constructing several weak learners. The Brown clustering is used as a weak learner for partitioning the patterns into 'k' number of clusters based on the Euclidean distance measure. Then the weak learner merges the two clusters with maximum information gained to minimize the time complexity. The clustering results of weak learners are combined into strong results with minimal error rate (ER). By this way, the antipattern in the SQL query log is detected with a higher accuracy. Experimental evaluation is conducted with different parameters namely detection accuracy (DA), false positive rate (FPR) and time complexity (TC) using the two SQL query log data-sets (DS). The experimental result shows that, the BIBSQLQC technique achieves higher DA with lower TC and FPR than the conventional methods.																	1300-0632	1303-6203					2020	28	4					2200	2212		10.3906/elk-1912-108													
J								Disturbance observer based control of twin rotor aerodynamic system	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Disturbance observer; twin rotor aerodynamic system; H-infinity approach; optimal control; decoupling	IMPLEMENTATION; DESIGN; INPUT	Twin rotor aerodynamic system (TRAS) approximates the dynamics of helicopters and other vertical take off rotor crafts. The nonlinear nature with significant cross-coupling between the inputs and outputs of the main and tail rotors make the control of such system for either stabilization or reference tracking a challenging task. In this paper, the problem of disturbance rejection for TRAS is addressed by designing disturbance observers through H infinity based approach. The system is decoupled into main and tail rotors subsystems. For each subsystem, an inner loop disturbance observer is synthesized that provides disturbance rejection, whereas to ensure stability and performance an outer loop baseline feedback controller is designed. Two different cases are considered. In first case 2 proportional-integral-derivative controllers are designed to use as outer loop baseline feedback controllers with disturbance observers whereas in the second case linear quadratic Gaussian (LQG) controllers are designed. For both cases simulations are performed with nonlinear Matlab Simulink model of TRAS and results are compared to determine which approach delivers better performance. Simulation results show that the 2 conflicting requirements of reference tracking and disturbance rejection can be met simultaneously with the proposed approach increasing the disturbance rejection capability of the closed loop system.																	1300-0632	1303-6203					2020	28	4					2213	2227		10.3906/elk-1912-34													
J								Connectivity considerations for mission planning of a search and rescue drone team	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Drones; unmanned aerial vehicles; path planning; target detection; monitoring; swarms; networking	UNMANNED AERIAL VEHICLES; DEPLOYMENT	In this paper, we analyze the mission success performance and mission times of centralized, distributed, and hybrid path-planning methods for a drone team whose mission is to find a target and inform the ground control. We propose two methods that integrate connectivity into the search mission path decisions. We observe that even though the coverage path-planning leads to lower search times, when target connectivity is also required, schemes that incorporate end-end connectivity into path planning result in at least 50% better mission times for small communication ranges and lower number of drones. Our results also indicate that methods to efficiently allocate resources to search and communication tasks in mission-oriented drone networks need to be designed.																	1300-0632	1303-6203					2020	28	4					2228	2243		10.3906/elk-1912-46													
J								NET-LDA: a novel topic modeling method based on semantic document similarity	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Aspect extraction; cooccurence relation; latent Dirichlet allocation (LDA); semantic similarity; topic modeling	LATENT DIRICHLET ALLOCATION	Topic models, such as latent Dirichlet allocation (LDA), allow us to categorize each document based on the topics. It builds a document as a mixture of topics and a topic is modeled as a probability distribution over words. However, the key drawback of the traditional topic model is that it cannot handle the semantic knowledge hidden in the documents. Therefore, semantically related, coherent and meaningful topics cannot be obtained. However, semantic inference plays a significant role in topic modeling as well as in other text mining tasks. In this paper, in order to tackle this problem, a novel NET-LDA model is proposed. In NET-LDA, semantically similar documents are merged to bring all semantically related words together and the obtained semantic similarity knowledge is incorporated into the model with a new adaptive semantic parameter. The motivation of the study is to reveal the impact of semantic knowledge in the topic model researches. Therefore, in a given corpus, different documents may contain different words but may speak about the same topic. For such documents to be correctly identified, the feature space of the documents must be elaborated with more powerful features. In order to accomplish this goal, the semantic space of documents is constructed with concepts and named entities. Two datasets in the English and Turkish languages and 12 different domains have been evaluated to show the independence of the model from both language and domain. The proposed NET-LDA, compared to the baselines, outperforms in terms of topic coherence, F-measure, and qualitative evaluation.																	1300-0632	1303-6203					2020	28	4					2244	2260		10.3906/elk-1912-62													
J								PERI-Net: a parameter efficient residual inception network for medical image segmentation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Inception modules; U-net; axon; residual connections; vessels	BLOOD-VESSEL SEGMENTATION; RETINAL IMAGES; MODEL	Recent developments in deep networks allow us to train networks with more parameters by yielding better performance given sufficient amount of data. However, we are still restricted with the availability of labelled data in medical image segmentation, where the problem is exacerbated with high intra- and intervariability of anatomical structures. In order to bypass this problem without compromising network performance, this study introduces a PERI-Net, which promises to achieve higher performance while being with smaller parameter count such as on the order of 0.8 million than its counterparts. The network benefits from rich features generated by our versions of inception modules, better communication between encoding and decoding paths and an effective way of segmentation mask generation. We evaluate the performance of our architecture on the segmentation of retinal vasculature in fundus image datasets of DRIVE, CHASE_DB 1 and IOSTAR and the segmentation of axons in a 2-photon microscopy image dataset. According to the results of our experiments, PERI-Net achieves state of the art performance on sensitivity and G-mean metrics with a significant margin for the 3 datasets, by outperforming our training of a U-net sharing the same properties and training strategies as PERI-Net.																	1300-0632	1303-6203					2020	28	4					2261	2277		10.3906/elk-1912-97													
J								Controlling a launch vehicle at exoatmospheric flight conditions via adaptive control allocation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Adaptive control allocation; reaction control jets; reusable launch vehicles	DESIGN	The focus of this paper is the control of a reusable launch vehicle at exoatmospheric flight conditions, in the presence of actuator effectiveness uncertainty. Since during exoatmospheric flight, dynamic pressure is nonexistent, aerodynamic control surfaces cannot be used. Under these conditions, reaction control jet actuators can provide the necessary thrust to control the vehicle. Reaction control jets have only 2 states, namely, on and off, and continuous control inputs can be implemented with the help of pulse width modulation, which is also employed in this paper. A continuous controller is designed in the outer loop and a control allocator is used to distribute the total control input among redundant actuators, whose effectiveness are assumed to be unknown. The unknown actuator effectiveness is addressed with the help of an adaptive control allocator. A representative model of a reusable launch vehicle equipped with reaction control jets is used to demonstrate the effectiveness of the overall control scheme.																	1300-0632	1303-6203					2020	28	4					2278	2287		10.3906/elk-2001-123													
J								Simplified model predictive current control of non-sinusoidal low power brushless DC machines	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										BLDC motor drive; predictive control; current control; synchronous motor; nonsinusoidal back-emf; speed control	TORQUE-RIPPLE MINIMIZATION; SENSORLESS CONTROL; CONTROL STRATEGY; MOTOR; DRIVES; SPEED; IMPLEMENTATION; REDUCTION; SCHEME; PMSM	Several strategies have been proposed to control nonsinusoidal brushless DC machines (BLDCMs). However, high electromagnetic torque ripple and current overshoots occur in commutation times, which are significant problems of those strategies such as for hysteresis current controllers. This paper proposes a model predictive strategy to solve the above issues. It is simple and straightforward. Moreover, it reduces the motor torque ripple significantly and improves the response rate of the control system to the load torque variation in comparison with the conventional technique. The torque varies smoothly, and the performance of the system at commutation time is improved by eliminating the adverse effects of commutation times on the machine's current and torque. This method also operates better than the conventional controller in medium switching frequencies. The novelty of this strategy is that it employs a model predictive strategy to realize the above claims. The real implementation possibility and performance of the controller are investigated by simulations for a 60-V/180-W/300-RPM BLDCM. This paper also compares the proposed current controller with the conventional controller. The results show that the torque ripple reduces 50%, and the brake and response times are improved.																	1300-0632	1303-6203					2020	28	4					2288	2302		10.3906/elk-2001-141													
J								A secure and energy-efficient opportunistic routing protocol with void avoidance for underwater acoustic sensor networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Energy efficiency; communication voids; routing protocols; secure data transmission; QoS; underwater acoustic sensor networks	INTERNET	Recently, underwater acoustic sensor networks (UASNs) have gained wide attention due to their numerous applications in underwater surveillance, oil leakage detection, assisted navigation, and disaster prevention. With unique characteristics like increased propagation delay, constant mobility of sensor nodes, high error rate, and limitations in energy and interference, efficient routing of data packets from the source node to the destination is a major challenge in UASNs. Most of the protocols proposed for traditional sensor networks do not work well in UASNs. Although many protocols have been specifically proposed for underwater environments, the aim of most of them is to improve only the quality of service (QoS) in the network. The security of the transmitted data, energy efficiency of the participating nodes, and handling of communication voids are three significant challenges that need to be adequately addressed in UASNs. In this research work, a secure and energy-efficient opportunistic routing protocol with void avoidance (SEEORVA) is proposed. This protocol uses the latest opportunistic routing strategy for reliable data delivery in the network and also provides priority to the nodes having energy above a specific threshold in the forwarding process, thereby increasing the lifetime and energy efficiency in the network. The transmitted messages are encrypted using a secure lightweight encryption technique. The protocol is also integrated with a strategy to handle the communication voids in the network. Simulation results with Aqua-Sim validate the better performance of the proposed system compared to the existing ones.																	1300-0632	1303-6203					2020	28	4					2303	2315		10.3906/elk-2001-51													
J								On a yearly basis prediction of soil water content utilizing sar data: a machine learning and feature selection approach	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Synthetic aperture radar; support vector regression; generalized regression neural network; adaptive neurofuzzy inference system; feature selection; soil water content	REMOTE-SENSING DATA; MOISTURE RETRIEVAL; NEURAL-NETWORK; ALGORITHMS; SCATTERING; MODEL; SCHEME; RADAR; BARE	Soil water content (SWC) performs an important role in many areas including agriculture, drought cases, usage of water resources, hydrology, crop diseases and aerology. However, the measurement of the SWC over large terrains with standard computational techniques is very hard. In order to overcome this situation, remote sensing tools are preferred, which can produce much more successful results in less time than standard calculation techniques. Among all remote sensing tools, synthetic aperture radar (SAR) has a significant impact on determining SWC over large terrains. The main objective of this study is to predict SWC on a yearly basis over the vegetation-covered terrains with the aid of different machine learning techniques and SAR based Radarsat-2 data, which obtained in 2015 and 2016 years.The proposed system consists of several stages, respectively. In the feature extraction stage, the backscatter coefficients of different polarizations and the parameters obtained from different models of decomposition (Freeman-Durden and H/A/alpha) were combined and nine polarimetric features were formed for each sample point. In the next stage, support vector regression (SVR), generalized regression neural network (GRNN) and adaptive neuro-fuzzy inference system (ANFIS) were employed for the prediction of SWC. In the last stage, a machine learning based feature selection was implemented to the obtained feature vectors for determining optimal feature sets. Finally, a feature set with 6 parameters was determined as most optimal feature set over the SWC prediction and a slightly better performance was observed thanks to this feature set compared to the other results.																	1300-0632	1303-6203					2020	28	4					2316	2330		10.3906/elk-2002-99													
J								Partial discharge detection and localization on the medium voltage XLPE cables with multiclass support vector machines	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Partial discharges; pattern classification; fault diagnosis; cross-linked polyethylene insulation; discrete Fourier transforms	FEATURE-EXTRACTION; CLASSIFICATION; IMAGE; MV	In medium voltage cables, partial discharges (PD's) are the major problems that trigger electrical insulation failures. Therefore, classification of PD source type and failure localization in medium voltage cables are significant issues of medium voltage engineering. Therefore, in this study, both detection and localization of PD are studied. As a first step, 4 different kind of defects are artificially generated at the same length of the same kind of medium voltage cross-linked polyethylene (XLPE) cables. Consequently, an experimental setup is built. During the experiments, different medium voltage levels are applied to the cables, then the PD signals are measured and recorded. To classify the signals of different defects, different statistics of frequency spectrum of the signals are considered as features. As a final task of this step, multiclass support vector machine is employed and the PD signals are classified. In the second step, one kind of defect is generated at different locations of same kind of longer XLPE cable. Consequently, the cable exposed to different medium voltage levels and PD signals are measured and recorded. The statistics of the data are employed as features. Finally, PD signals measured from different lengths are classified by the help of multiclass support vector machine.																	1300-0632	1303-6203					2020	28	4					2331	2344		10.3906/elk-2003-16													
J								Asymmetric slope compensation for digital hybrid current mode control of a three-level flying capacitor buck converter	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										DC-DC converters; current mode control; three-level flying capacitor buck converter		The three-level flying capacitor (3LFC) buck converter has the potential to provide better efficiency and higher power density as compared to the traditional buck converter. However, due to the flying capacitor's instability issues, control is challenging. In this paper, the digital hybrid current mode (DHCM) control method, which combines the average and peak current mode control techniques, is modified and implemented to a 3LFC buck converter. For flying capacitor (FC) voltage balancing, a novel asymmetric slope compensation (ASC) technique is presented. The proposed ASC technique achieves FC voltage balancing by adjusting the slope compensation of the two switching pairs asymmetrically, and therefore is applicable to any type of digital peak current mode control (PCM) method. The ASC technique for DHCM control of the 3LFC buck converter is verified in simulations and experimentally with an ARM -based mixed-signal microcontroller on a 250-kHz, 24-V input, and 35-W/4-A output prototype. The overall control method enables the implementation of the 3LFC buck converter for accurate and fast current regulation, and therefore is especially advantageous for LED and laser diode driver applications.																	1300-0632	1303-6203					2020	28	4					2345	2359		10.3906/elk-2002-149													
J								Optimal SVC allocation in power systems using lightning attachment procedure optimization	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Power system optimization; voltage deviation index; power losses; FACTS; SVC; LAPO	STABILITY	Flexible AC transmission systems (FACTS) technology is widely adopted and utilized to maintain the performance of power systems. However, the improvements of power system performance achieved by FACTS devices depend on the right sizing and allocation of such devices. For technical and economic considerations, a FACTS device's location and size should be selected very carefully in order to maximize its benefits to the power system. In this paper, the sizing and location of a static VAR compensator (SVC) are optimally determined using a new optimization technique called lightning attachment procedure optimization (LAPO). The optimal allocation of the SVC is determined regarding the improvement of voltage deviation index and the reduction of total active power losses. The system is optimized in two cases: once without SVC installation in order to find out the optimal settings of the system that achieve the objective functions, and another time with SVC installation to determine its optimum sizes and locations by which the required objective functions are achieved. Then the system performance is analyzed after optimization with and without SVC devices to show the impact of the optimum sizing and location of the SVC on the system. The study is validated using the standard IEEE 30-bus system, while the developed LAPO is performed by MATLAB M-Files and the system performance analysis in different cases is performed by NEPLAN software.																	1300-0632	1303-6203					2020	28	4					2360	2374		10.3906/elk-1904-152													
J								Inspiration-wise swarm intelligence meta-heuristics for continuous optimisation: a survey-part I	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										swarm intelligence; swarm-based meta-heuristics; bio-inspired computation	ANT COLONY OPTIMIZATION; SEARCH ALGORITHM; STRATEGY; NETWORK; ALLOCATION; SEQUENCE; BEHAVIOR; DESIGN	In many science fields, such as engineering, administration, transportation, economics and biology, among many others, optimisation problem is a handy required tool. Global optimisation techniques aim to find the best solution in a set of feasible solutions to a problem. Currently, there are numerous optimisation techniques. In general, the problems to be optimised are complex, nonlinear and in some cases may be intractable. Meta-heuristics are general algorithmic frameworks adaptable to various optimisation problems and are generally applied to highly complex problems. Swarm intelligence represents intelligent models inspired by real-world social systems, based on interaction and organisation between simple agents to perform simple tasks. Nowadays, there are many swarm-based meta-heuristics. In this survey, we take advantage of the inspiration behind the strategies to contribute an inspiration-oriented novel taxonomy for the current state-of-the-art of swarm-oriented optimisation methods. The overall survey, which will be divided into three separate parts, provides a review of swarm-based meta-heuristic, which is commonly employed to solve complex continuous optimisations, aiming at building a inspiration-based taxonomy for such search strategies. In this part of the survey, we review meta-heuristics that are guided by some interesting human relation's characteristics and those that are inspired by physical system's properties.																	1758-0366	1758-0374					2020	15	4					207	223															
J								A swarm intelligence labour division approach to solving complex area coverage problems of swarm robots	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										area coverage; swarm robot; swarm intelligence; labour division; response threshold model; activation-inhibition model	OF-LABOR; THRESHOLD REINFORCEMENT; TASK ALLOCATION; POLYETHISM; SIMULATION; COLONY	The complex area coverage problem is classical and widespread in the research field of swarm robots. In order to solve the complex area coverage problem with complex nonlinear boundary and special task area (forbidden area or threat area), firstly, the task area is adjusted and grid discretisation. Then, inspired by the labour division phenomenon of typical biological groups such as bee colony and ant colony, the paper analyses the performance characteristics of typical ant colony labour division model (response threshold model) and bee colony labour division model (activation-inhibition model) from the perspectives of individual and environment, individual and individual, and a new swarm intelligence labour division approach (activation-inhibition response threshold algorithm) to solve the complex area coverage problem of swarm robot. Three experiments are carried out to illustrate that the algorithm are endowed with great ability of area coverage and dynamic environment. It can respond to the sudden threat in time and make an efficient response, which has a good practical application prospects.																	1758-0366	1758-0374					2020	15	4					224	238															
J								A bee colony optimisation algorithm with a sequential-pattern-mining-based pruning strategy for the travelling salesman problem	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										meta-heuristic; data mining; sequential pattern mining; FBPS; frequency-based pruning strategy; frequent-close-pattern-based pruning strategy; combinatorial optimisation	BAT ALGORITHM; LOCAL SEARCH; DESIGN; EXPLOITATION; ACCEPTANCE; MECHANISM; GROWTH	The unique foraging behaviour of bees via waggle dance has been computationally realised as an algorithm named bee colony optimisation (BCO) to solve different types of combinatorial optimisation problems such as travelling salesman problem (TSP). In order to enhance the performance of BCO, local optimisation can be integrated. However, local optimisation incurs high processing overhead especially when all solutions are allowed to undergo the local optimisation. This paper proposes a pruning strategy based on the top-k sequential patterns (TKS) mining algorithm. Specifically, TKS is employed to identify the frequent building blocks along the optimisation process. A total of 19 TSP benchmark problem instances ranging from 318 cities to 1,291 cities were used as the test bed. The proposed pruning strategy shows a significant reduction in terms of the computational time to yield TSP solutions with similar tour length as compared with two state-of-the-art approaches.																	1758-0366	1758-0374					2020	15	4					239	253															
J								PSO-MoSR: a PSO-based multi-objective software remodularisation	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										remodularisation; restructuring; optimisation; module clustering	PARTICLE SWARM OPTIMIZATION; MULTIPLE OBJECTIVES	The quality of modular structure of a software system highly affects the success of a software project. Software remodularisation which is used to improve the software structure is a complex task and involves the optimisation of multiple conflicting aspects. To address the optimisation of multiple objectives, many metaheuristic optimisation algorithms have been designed. The customisation of these algorithms according to the suitability of real-world multi-objective software remodularisation problem is a challenging task. In this article, particle swarm optimisation (PSO) a widely used metaheuristic heuristic technique is customised and proposed a PSO-based multi-objective software remodularisation (PSO-MoSR) to address the optimisation of multiple objective issues of software remodularisation. The effectiveness of the proposed PSO-MoSR is evaluated by conducting several experiments by modularising 17 real-world software systems.																	1758-0366	1758-0374					2020	15	4					254	263															
J								A routing algorithm based on simulated annealing algorithm for maximising wireless sensor networks lifetime with a sink node	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										routing algorithm; sink node; wireless sensor network; WSNs; simulated annealing; SA; optimal path	SWARM OPTIMIZATION ALGORITHM; MOBILE SINK; PROTOCOL	Energy saving becomes a central issue in the design of wireless sensor network routing algorithms. In the wireless sensor networks (WSNs), when intra-network communication is ensured, the lifetime of node can be extended by reducing data transmission or data volume as much as possible. However, the problem is that energy of the nodes around the sink node becomes exhausted quickly due to excessive communication overhead. To handle this problem, in this study, we propose a routing algorithm based on the sink node path optimisation. The study uses the energy consumption model as a constraint, transforms the time optimisation problem into an optimisation model, optimises the sink node path with the aid of simulated annealing (SA) algorithm, and uses data fusion to reduce the intra-network redundant data in the time domain. The proposed algorithm innovatively self-adjusts the path of sink node that is optimised by SA using new fitness function. Comprehensive simulation results show that the proposed algorithm can reduce the node energy consumption of waiting of sink node at the address of sink node, balance the network load and improve survival time of WSNs by 30% in comparison with results produced with the state-of-the art algorithms REAC-IN and DALMDT.																	1758-0366	1758-0374					2020	15	4					264	275															
J								Geometric retrieval algorithm-based ear biometry with occluded images	INTERNATIONAL JOURNAL OF BIOMETRICS										biometry; ear recognition; occlusion; interpolation; particle swarm optimisation; PSO; Euclidian distance	FEATURE-EXTRACTION; RECOGNITION	Ear is a potential biometric parameter which has drawn the attention due to its structural uniqueness and stability over the age, obesity, disease, expression, etc., unlike other common biometric traits. In this work a geometric retrieval algorithm has been proposed for ear-based biometric analysis with occluded image. First the occlusion problem is countered by an empirical data driven technique and then PSO-based optimal features are extracted for comparison that reveals the authenticity of the subject with respect to a stored database. A search of minima from Euclidian distance-based analysis is used for final decision. The proposed system is tested on 50 subjects collected in multiple sessions in laboratory with a good recognition rate superior to similar reported works as indicated in the result section.																	1755-8301	1755-831X					2020	12	3					247	268															
J								Eigen-based binary feature amalgamation in multimodal biometrics	INTERNATIONAL JOURNAL OF BIOMETRICS										multimodal biometrics; feature-level fusion; feature combination; feature fusion; feature amalgamation; eigen analysis; face; iris; palm-print	FUSION; FACE; REPRESENTATION; RECOGNITION; INFORMATION; SCALE	In this paper, a quantised eigen analysis (QEA) for the extracted features is proposed and an associated eigen-based binary feature amalgamation (EBFA) based on QEA is developed for feature fusion in multimodal biometrics. As opposed to feature combination, EBFA projects heterogeneous features onto the projection kernel and uses only the sign parts to encode the features as bit strings to maximise its expression rather than directly combine them. Thus, the feature codes can be simply concatenated or compared by XOR bit-wise operation into a serial or parallel amalgamated feature vector. To evaluate the performance of EBFA, a series of experiments are performed on multiple biometric modalities, including face, palm-print and iris. The experimental results show that the proposed binary feature amalgamation scheme at feature-level is superior to some other feature fusion methods and score-level methods in terms of multimodal recognition accuracy performance.																	1755-8301	1755-831X					2020	12	3					269	282															
J								Laughter signature: a novel biometric trait for person identification	INTERNATIONAL JOURNAL OF BIOMETRICS										person identification; laughter signature; biometrics trait; support vector machine; SVM; Gaussian mixture model; GMM	SPEAKER IDENTIFICATION; SPEECH	Laughter is a naturally occurring feature in speech and social interactions. Human intelligence can identify people by their laughter, but this has not been explored as a potential biometric in person identification systems. This study proposes a novel behavioural biometric based on individual laughter signatures. Mel frequency cepstral coefficients (MFCC) features were extracted and Kruskal-Wallis test was performed on each coefficient. A dynamic-average Mel frequency cepstral coefficients (DA-MFCC) was developed from the typical MFCC features for system training using Gaussian mixture model (GMM) and support vector machine (SVM). Test results showed an accuracy of 90%-person identification for SVM while the GMM was 65%. The use of GA-MFCC improved the accuracy of the system by 5.06% and 2.99% for GMM and SVM respectively. Laughter has thus been shown to be a viable biometric feature for person identification which can be embedded into artificial intelligence systems in diverse applications.																	1755-8301	1755-831X					2020	12	3					283	300															
J								Efficient fusion of face and palmprint in Gabor filtered Wigner domain	INTERNATIONAL JOURNAL OF BIOMETRICS										multimodal system; feature level fusion; score level fusion; Gabor filtered Wigner transform; particle swarm optimisation; PSO	FEATURE-LEVEL FUSION; FINGER-VEIN; KNUCKLE-PRINT; RECOGNITION; INFORMATION; EAR	In this paper, a new transform Gabor filtered Wigner transform (GFWT) has been proposed. In GFWT, Gabor filtering is performed on the Wigner transformed image. Wigner transform gives a simultaneous representation of an image in time and frequency domain which is further processed using Gabor filters. The proposed transform is then used to extract the features from the biometrics to develop different multimodal biometric systems. A detailed study has been carried out in which, different unimodal and multimodal systems such as feature level and score level fusion are analysed. In order to improve the performance of the system, an optimisation technique, particle swarm optimisation (PSO) is used to find the optimal parameters of the Gabor filter and to select the significant GFWT feature vector. The PSO technique not only improves the performance of the system but also able to reduce the dimension of the feature vectors. Numerical experiments are carried out on face and palmprint database to show the effectiveness of the proposed transform for different unimodal and multimodal systems.																	1755-8301	1755-831X					2020	12	3					301	316															
J								Fingerprint pores extraction by using automatic scale selection	INTERNATIONAL JOURNAL OF BIOMETRICS										automatic scale selection; biometrics; fingerprint; local maxima; pores	FAKE FINGERS	Extraction of fingerprint sweat pores is a critical step in those applications which are based on highly secured features. Pores are varying in scale (size) and evenly distributed along the ridges. It is the main challenge to design a technique which determines the pores of different sizes in the fingerprint image. In this paper, pore extraction algorithm is proposed for high-resolution fingerprint images which utilised multiscale gamma-normalised Laplacian of Gaussian (LoG) filter. A block-wise approach is implemented in which each region is filtered at multiple scale values. Scale space theory is applied and candidate pixels of high negative response are identified through local maxima approach. The efficacy of the proposed algorithm is tested by measuring average true detection rate (TDR) and average false detection rate (FDR). Results of the proposed algorithm achieve average TDR and average FDR values as 82.89% and 21.2% respectively which are better in comparison to the state-of-art techniques.																	1755-8301	1755-831X					2020	12	3					317	336															
J								Supervised and unsupervised machine learning for gender identification through hand's anthropometric data	INTERNATIONAL JOURNAL OF BIOMETRICS										hand anthropometric data; K-means; discriminant analysis; k-nearest neighbours; gender identification; features selection	CLASSIFICATION	The goal of this study is to determine the best gender identifiers from the hand anthropometric measurements. Five algorithms are used and their performances quantified. The first algorithm is based on computing distances of test subjects to pre-computed masculine/feminine mean characteristics. Then, the k-nearest neighbours, the K-means algorithms, the linear and the quadratic discriminant techniques are applied to segregate males and females. To select the relevant attributes, the recursive feature elimination and the stepwise regression methods are used. All these methods are leading to high accuracy rates of genders recognition. However, the linear and quadratic discriminant methods are the most accurate. Breadth and circumference features are better than the length features in identifying the gender. The palm and the thumb are the parts of the hand with the highest rate of gender recognition. Breadths of the index and the thumb and the palm circumference are the best individual identifiers.																	1755-8301	1755-831X					2020	12	3					337	355															
J								Dynamic Sampling Networks for Efficient Action Recognition in Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Videos; Learning (artificial intelligence); Training; Testing; Machine learning; Image recognition; Three-dimensional displays; Dynamic sampling networks; reinforcement learning; efficient action recognition		The existing action recognition methods are mainly based on clip-level classifiers such as two-stream CNNs or 3D CNNs, which are trained from the randomly selected clips and applied to densely sampled clips during testing. However, this standard setting might be suboptimal for training classifiers and also requires huge computational overhead when deployed in practice. To address these issues, we propose a new framework for action recognition in videos, called Dynamic Sampling Networks (DSN), by designing a dynamic sampling module to improve the discriminative power of learned clip-level classifiers and as well increase the inference efficiency during testing. Specifically, DSN is composed of a sampling module and a classification module, whose objective is to learn a sampling policy to on-the-fly select which clips to keep and train a clip-level classifier to perform action recognition based on these selected clips, respectively. In particular, given an input video, we train an observation network in an associative reinforcement learning setting to maximize the rewards of the selected clips with a correct prediction. We perform extensive experiments to study different aspects of the DSN framework on four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.3. The experimental results demonstrate that DSN is able to greatly improve the inference efficiency by only using less than half of the clips, which can still obtain a slightly better or comparable recognition accuracy to the state-of-the-art approaches.																	1057-7149	1941-0042					2020	29						7970	7983		10.1109/TIP.2020.3007826													
J								Lightening Network for Low-Light Image Enhancement	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Task analysis; Lighting; Image resolution; Reflectivity; Image enhancement; Data mining; Low-light image enhancement; image processing; deep learning	HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT	Low-light image enhancement is a challenging task that has attracted considerable attention. Pictures taken in low-light conditions often have bad visual quality. To address the problem, we regard the low-light enhancement as a residual learning problem that is to estimate the residual between low- and normal-light images. In this paper, we propose a novel Deep Lightening Network (DLN) that benefits from the recent development of Convolutional Neural Networks (CNNs). The proposed DLN consists of several Lightening Back-Projection (LBP) blocks. The LBPs perform lightening and darkening processes iteratively to learn the residual for normal-light estimations. To effectively utilize the local and global features, we also propose a Feature Aggregation (FA) block that adaptively fuses the results of different LBPs. We evaluate the proposed method on different datasets. Numerical results show that our proposed DLN approach outperforms other methods under both objective and subjective metrics.																	1057-7149	1941-0042					2020	29						7984	7996		10.1109/TIP.2020.3008396													
J								FOG AND CLOUD COMPUTING ASSISTED IOT MODEL BASED PERSONAL EMERGENCY MONITORING AND DISEASES PREDICTION SERVICES	COMPUTING AND INFORMATICS										EMDPS; DML; PHR; EHR; IoT; fog computing; cloud computing; APC model	ARTIFICIAL NEURAL-NETWORKS; SYSTEM	Along with the rapid development of modern high-tech and the change of people's awareness of healthy life, the demand for personal healthcare services is gradually increasing. The rapid progress of information and communication technology and medical and bio technology not only improves personal healthcare services, but also brings the fact that the human being has entered the era of longevity. At present, there are many researches focused on various wearable sensing devices and implant devices and Internet of Things in order to capture personal daily life health information more conveniently and effectively, and significant results have been obtained, such as fog computing. To provide personal healthcare services, the fog and cloud computing is an effective solution for sharing health information. The health big data analysis model can provide personal health situation reports on a daily basis, and the gene sequencing can provide hereditary disease prediction. However, the injury mortality and emergency diseases since long ago caused death and great pain for the family. And there are no effective rescue methods to save precious lives and no methods to predict the disease morbidity likelihood. The purpose of this research is to capture personal daily health information based on sensors and monitoring emergency situations with the help of fog computing and mobile applications, and disease prediction based on cloud computing and big data analysis. Through the comparison of test results it was proved that the proposed emergency monitoring based on fog and cloud computing and the diseases prediction model based on big data analysis not only gain more of the rescue time than the traditional emergency treatment method, but they also accumulate lots of different personal healthcare related experience. The Taian 960 hospital of PLA and the Yanbian Hospital as IM testbed were joined to provide emergency monitoring tests, and to ensure the CVD and CVA morbidity likelihood medical big data analysis, the people around Taian city participated in personal health tests. Through the project, the five network layers architecture and integrated MAPE-K Model based EMDPS platform not only made the cooperation between hospitals feasible to deal with emergency situations, but also the Internet medicine for the disease prediction was built.																	1335-9150						2020	39	1-2					5	27		10.31577/cai_2020_1-2_5													
J								STOCHASTIC MODELING AND PERFORMANCE ANALYSIS OF ENERGY-AWARE CLOUD DATA CENTER BASED ON DYNAMIC SCALABLE STOCHASTIC PETRI NET	COMPUTING AND INFORMATICS										Stochastic Petri net; QoS; energy efficiency; performance evaluation; cloud computing	POWER; MANAGEMENT	The characteristics of cloud computing, such as large-scale, dynamics, heterogeneity and diversity, present a range of challenges for the study on modeling and performance evaluation on cloud data centers. Performance evaluation not only finds out an appropriate trade-off between cost-benefit and quality of service (QoS) based on service level agreement (SLA), but also investigates the influence of virtualization technology. In this paper, we propose an Energy-Aware Optimization (EAO) algorithm with considering energy consumption, resource diversity and virtual machine migration. In addition, we construct a stochastic model for Energy-Aware Migration-Enabled Cloud (EAMEC) data centers by introducing Dynamic Scalable Stochastic Petri Net (DSSPN). Several performance parameters are defined to evaluate task backlogs, throughput, reject rate, utilization, and energy consumption under different runtime and machines. Finally, we use a tool called SPNP to simulate analytical solutions of these parameters. The analysis results show that DSSPN is applicable to model and evaluate complex cloud systems, and can help to optimize the performance of EAMEC data centers.																	1335-9150						2020	39	1-2					28	50		10.31577/cai_2020_1-2_28													
J								OPTIMIZING DATA PLACEMENT FOR COST EFFECTIVE AND HIGH AVAILABLE MULTI-CLOUD STORAGE	COMPUTING AND INFORMATICS										Data hosting; cloud storage; multi-cloud; multi-objective optimization; genetic algorithm	REPLICATION; ALGORITHM; SECURITY; SYSTEMS; ISSUES	With the advent of big data age, data volume has been changed from trillionbyte to petabyte with incredible speed. Owing to the fact that cloud storage offers the vision of a virtually infinite pool of storage resources, data can be stored and accessed with high scalability and availability. But a single cloud-based data storage has risks like vendor lock-in, privacy leakage, and unavailability. Multi-cloud storage can mitigate these risks with geographically located cloud storage providers. In this storage scheme, one important challenge is how to place a user's data cost-effectively with high availability. In this paper, an architecture for multi-cloud storage is presented. Next, a multi-objective optimization problem is defined to minimize total cost and maximize data availability simultaneously, which can be solved by an approach based on the non-dominated sorting genetic algorithm II (NSGA-II) and obtain a set of non-dominated solutions called the Pareto-optimal set. Then, a method is proposed which is based on the entropy method to determine the most suitable solution for users who cannot choose one from the Pareto-optimal set directly. Finally, the performance of the proposed algorithm is validated by extensive experiments based on real-world multiple cloud storage scenarios.																	1335-9150						2020	39	1-2					51	82		10.31577/cai_2020_1-2_51													
J								VIRTUAL MACHINE DEPLOYMENT STRATEGY BASED ON IMPROVED PSO IN CLOUD COMPUTING	COMPUTING AND INFORMATICS										Cloud computing; Pareto optimal solution; particle swarm optimization algorithm; resource reservation; virtual machine deployment	PARTICLE SWARM OPTIMIZATION; PLACEMENT; ALGORITHM	Energy consumption is an important cost driven by growth of computing power, thereby energy conservation has become one of the major problems faced by cloud system. How to maximize the utilization of physical machines, reduce the number of virtual machine migrations, and maintain load balance under the constraints of physical machine resource thresholds that is the effective way to implement energy saving in data center. In the paper, we propose a multi-objective physical model for virtual machine deployment. Then the improved multi-objective particle swarm optimization (TPSO) is applied to virtual machine deployment. Compared to other algorithms, the algorithm has better ergodicity into the initial stage, improves the optimization precision and optimization efficiency of the particle swarm. The experimental results based on CloudSim simulation platform show that the algorithm is effective at improving physical machine resource utilization, reducing resource waste, and improving system load balance.																	1335-9150						2020	39	1-2					83	104		10.31577/cai_2020_1-2_83													
J								MULTI-DIMENSIONAL RECOMMENDATION SCHEME FOR SOCIAL NETWORKS CONSIDERING A USER RELATIONSHIP STRENGTH PERSPECTIVE	COMPUTING AND INFORMATICS										Recommendation system; social network; user relationship strength; user interest; entity similarity	TRUST; SYSTEM; ALGORITHM; MODEL; GRAPH	Developing a computational method based on user relationship strength for multi-dimensional recommendation is a significant challenge. The traditional recommendation methods have relatively low accuracy because they lack considering information from the perspective of user relationship strength into the recommendation algorithm. User relationship strength reflects the degree of closeness between two users, which can make the recommendation system more efficient between users in pairs. This paper proposes a multi-dimensional comprehensive recommendation method based on user relationship strength. We take three main factors into consideration, including the strength of user relationship, the similarity of entities, and the degree of user interest. First, we introduce a novel method to generate a user candidate set and an entity candidate set by calculating the relationship strength between two users and the similarity between two entities. Then, the algorithm will calculate the user interest degree of each user in the user candidate set to each entity in the entity candidate set, if the user interest degree is larger than or equal to a threshold, this particular entity will be recommended to this user. The performance of the proposed method was verified based on the real-world social network dataset and the e-commerce website dataset, and the experimental result suggests that this method can improve the recommendation accuracy.																	1335-9150						2020	39	1-2					105	140		10.31577/cai_2020_1-2_105													
J								TIME-SENSITIVE COLLABORATIVE FILTERING ALGORITHM WITH FEATURE STABILITY	COMPUTING AND INFORMATICS										Collaborative filtering; recommendation algorithm; long tail; time-sensitive	SERVICE COMPOSITION	In the recommendation system, the collaborative filtering algorithm is widely used. However, there are lots of problems which need to be solved in recommendation field, such as low precision, the long tail of items. In this paper, we design an algorithm called FSTS for solving the low precision and the long tail. We adopt stability variables and time-sensitive factors to solve the problem of user's interest drift, and improve the accuracy of prediction. Experiments show that, compared with Item-CF, the precision, the recall, the coverage and the popularity have been significantly improved by FSTS algorithm. At the same time, it can mine long tail items and alleviate the phenomenon of the long tail.																	1335-9150						2020	39	1-2					141	155		10.31577/cai_2020_1-2_141													
J								REVERSE INTERVENTION FOR DEALING WITH MALICIOUS INFORMATION IN ONLINE SOCIAL NETWORKS	COMPUTING AND INFORMATICS										Malicious information; social network; reverse intervention	DETECTING COMMUNITY STRUCTURE; RUMOR PROPAGATION; INTERNET	Malicious information is often hidden in the massive data flow of online social networks. In "We Media" era, if the system is closed without intervention, malicious information may spread to the entire network quickly, which would cause severe economic and political losses. This paper adopts a reverse intervention strategy from the perspective of topology control, so that the spread of malicious information could be suppressed at a minimum cost. Noting that as the information spreads, social networks often present a community structure and multiple malicious information promoters may appear. Therefore, this paper adopts a divide and conquer strategy and proposes an intervention algorithm based on subgraph partitioning, in which we search for some influential nodes to block or release clarification. The main algorithm consists of two main phases. Firstly, a subgraph partitioning method based on community structure is given to quickly extract the community structure of the information dissemination network. Secondly, a node blocking and clarification publishing algorithm based on the Jordan Center is proposed in the obtained subgraphs. Experiments show that the proposed algorithm can effectively suppress the spread of malicious information with a low time complexity compared with the benchmark algorithms.																	1335-9150						2020	39	1-2					156	173		10.31577/cai_2020_1-2_156													
J								A METHOD FOR LEARNING A PETRI NET MODEL BASED ON REGION THEORY	COMPUTING AND INFORMATICS										Petri net; robot model; robot learning; region theory; Petri net synthesis		The deployment of robots in real life applications is growing. For better control and analysis of robots, modeling and learning are the hot topics in the field. This paper proposes a method for learning a Petri net model from the limited attempts of robots. The method can supplement the information getting from robot system and then derive an accurate Petri net based on region theory accordingly. We take the building block world as an example to illustrate the presented method and prove the rationality of the method by two theorems. Moreover, the method described in this paper has been implemented by a program and tested on a set of examples. The results of experiments show that our algorithm is feasible and effective.																	1335-9150						2020	39	1-2					174	192		10.31577/cai_2020_1-2_174													
J								CHECKING DATA-FLOW ERRORS BASED ON THE GUARD-DRIVEN REACHABILITY GRAPH OF WFD-NET	COMPUTING AND INFORMATICS										Petri net; workflow system; data-flow errors; reachability graph	WORKFLOW NETS; PETRI NETS; VERIFICATION; SOUNDNESS	In order to guarantee the correctness of workflow systems, it is necessary to check their data-flow errors, e.g., missing data, inconsistent data, lost data and redundant data. The traditional Petri-net-based methods are usually based on the reachability graph. However, these methods have two flaws, i.e., the state space explosion and pseudo states. In order to solve these problems, we use WFD-nets to model workflow systems, and propose an algorithm for checking data-flow errors based on the guard-driven reachability graph (GRG) of WFD-net. Furthermore, a case study and some experiments are given to show the effectiveness and advantage of our method.																	1335-9150						2020	39	1-2					193	212		10.31577/cai_2020_1-2_193													
J								ANALYSIS AND APPLICATION OF MIN-COST TRANSITION SYSTEMS TO BUSINESS PROCESS MANAGEMENT	COMPUTING AND INFORMATICS										Petri nets; event logs; process models; transition systems; business process management	PROCESS MODELS; CONFORMANCE CHECKING; PETRI NETS; FRAMEWORK	To improve the efficiency of conformance checking in process mining, new alignment approaches are presented between event logs and process models based on the min-cost transition systems of Petri nets. An algorithm is presented to obtain the transition system with the minimum cost based on the product of the event net and process net. The min-cost transition system is a directed acyclic graph, where the paths from the initial node to the final node include all optimal alignments between the trace and the process model based on the given cost function. Two algorithms are proposed to calculate an optimal alignment and all optimal alignments, respectively. All algorithms are implemented in ProM platform. After a series of the simulation experiments, the feasibility and effectiveness of the proposed approaches are illustrated.																	1335-9150						2020	39	1-2					213	245		10.31577/cai_2020_1-2_213													
J								ADAPTIVE FAULT DIAGNOSIS OF MOTORS USING COMPREHENSIVE LEARNING PARTICLE SWARM OPTIMIZER WITH FUZZY PETRI NET	COMPUTING AND INFORMATICS										Fuzzy Petri net; CLPSO; fault diagnosis; motor; adaptive	WIRELESS SENSOR; NETWORKS	This study proposes and applies a comprehensive learning particle swarm optimization (CLPSO) fuzzy Petri net (FPN) algorithm, which is based on the CLPSO algorithm and FPN, to the fault diagnosis of a complex motor. First, the transition confidence is replaced by a Gaussian function to deal with the uncertainty of fault propagation. Then, according to the Petri net principle, a competition operator is introduced to improve the matrix reasoning. Finally, a CLPSO-FPN model for motor fault diagnosis is established based on the motor failure mechanism and fault characteristics. The CLPSO algorithm is used to generate the system parameters for fault diagnosis and to improve the adaptability and accuracy of fault diagnosis. This study considers the example of a three-phase asynchronous motor. The results show that the proposed algorithm can diagnose faults in this motor with satisfactory adaptability and accuracy compared with the traditional FPN algorithm. By establishing the system model, the fault propagation process of motors can be accurately and intuitively expressed, thus improving the fault treatment and equipment maintenance of motors.																	1335-9150						2020	39	1-2					246	263		10.31577/cai_2020_1-2_246													
J								A LOGIC PETRI NET-BASED REPAIR METHOD OF PROCESS MODELS WITH INCOMPLETE CHOICE AND CONCURRENT STRUCTURES	COMPUTING AND INFORMATICS										Process model; model repair; process tree; alignment; logic Petri net		Current model repair methods cannot repair incomplete choice and concurrent structures precisely and simply. This paper presents a repair method of process models with incomplete choice and concurrent structures via logic Petri nets. The relation sets are constructed based on process trees, including branch sets, choice activity sets and concurrent activity sets. The deviations are determined by analyzing the relation between relation sets and activities in the optimal alignment. The model repair method is proposed for models with incomplete choice and concurrent structures via logic Petri nets according to different deviation positions. Finally, the correctness and effectiveness of the logic Petri net-based repair method are illustrated by simulation experiments.																	1335-9150						2020	39	1-2					264	297		10.31577/cai_2020_1-2_264													
J								TRAVEL MODE RECOGNITION FROM GPS DATA BASED ON LSTM	COMPUTING AND INFORMATICS										GPS; LSTM; QGA; deep learning; travel mode	TRANSPORTATION MODES	A large amount of GPS data contains valuable hidden information. With GPS trajectory data, a Long Short-Term Memory model (LSTM) is used to identify passengers' travel modes, i.e., walking, riding buses, or driving cars. Moreover, the Quantum Genetic Algorithm (QGA) is used to optimize the LSTM model parameters, and the optimized model is used to identify the travel mode. Compared with the state-of-the-art studies, the contributions are: 1. We designed a method of data processing. We process the GPS data by pixelating, get grayscale images, and import them into the LSTM model. Finally, we use the QGA to optimize four parameters of the model, including the number of neurons and the number of hidden layers, the learning rate, and the number of iterations. LSTM is used as the classification method where QGA is adopted to optimize the parameters of the model. 2. Experimental results show that the proposed approach has higher accuracy than BP Neural Network, Random Forest and Convolutional Neural Networks (CNN), and the QGA parameter optimization method can further improve the recognition accuracy.																	1335-9150						2020	39	1-2					298	317		10.31577/cai_2020_1-2_298													
J								DEEP CONVOLUTION AND CORRELATED MANIFOLD EMBEDDED DISTRIBUTION ALIGNMENT FOR FOREST FIRE SMOKE PREDICTION	COMPUTING AND INFORMATICS										Transfer learning; domain adaptation; deep convolution; small dataset; forest fire smoke		This paper proposes the deep convolution and correlated manifold embedded distribution alignment (DC-CMEDA) model, which is able to realize the transfer learning classification between and among various small datasets, and greatly shorten the training time. First, pre-trained Resnet50 network is used for feature transfer to extract smoke features because of the difficulty in training small dataset of forest fire smoke; second, a correlated manifold embedded distribution alignment (CMEDA) is proposed to register the smoke features in order to align the input feature distributions of the source and target domains; and finally, a trainable network model is constructed. This model is evaluated in the paper based on satellite remote sensing image and video image datasets. Compared with the deep convolutional integrated long short-term memory (DC-ILSTM) network, DC-CMEDA has increased the accuracy of video images by 1.50 %, and the accuracy of satellite remote sensing images by 4.00 %. Compared the CMEDA algorithm with the ILSTM algorithm, the number of iterations of the former has decreased to 10 times or less, and the algorithm complexity of CMEDA is lower than that of ILSTM. DC-CMEDA has a great advantage in terms of convergence speed. The experimental results show that DC-CMEDA can solve the problem of small sample smoke dataset detection and recognition.																	1335-9150						2020	39	1-2					318	339		10.31577/cai_2020_1-2_318													
J								AN IMPROVED PDR LOCALIZATION ALGORITHM BASED ON PARTICLE FILTER	COMPUTING AND INFORMATICS										Indoor positioning; PDR; particle filtering; particle swarm optimization; data fusion		Pedestrian Dead Reckoning (PDR) helps to realize step frequency detection, step estimation and direction estimation through data collected by inertial sensors such as accelerometer, gyroscope, magnetometer, etc. The initial positioning information is used to calculate the position of pedestrians at any time, which can be applied to indoor positioning technology researching. In order to improve the position accuracy of pedestrian track estimation, this paper improves the step frequency detection, step size estimation and direction detection in PDR, and proposes a particle swarm optimization particle filter (PSO-IPF) PDR location algorithm. Using the built-in accelerometer information of the smartphone to carry out the step frequency detection, the step frequency parameter construction model is introduced to carry out the step estimation, the direction estimation is performed by the Kalman filter fusion gyroscope and the magnetometer information, and the positioning data is merged by using the particle filter. The fitness function in the particle swarm optimization process is changed in the localization algorithm to improve particle diversity and position estimation. The experimental results show that the error rate of the improved step frequency detection method is reduced by about 2.1% compared with the traditional method. The angle accuracy of the direction estimation is about 4.12 degrees higher than the traditional method. The overall positioning accuracy is improved.																	1335-9150						2020	39	1-2					340	360		10.31577/cai_2020_1-2_340													
J								Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions	JOURNAL OF MACHINE LEARNING RESEARCH										Conditional independence; Gaussian models; Graphical models; Kullback-Leibler divergence; Sensitivity analysis	BAYESIAN NETWORKS; DIVERGENCE	The accuracy of probability distributions inferred using machine-learning algorithms heavily depends on data availability and quality. In practical applications it is therefore fundamental to investigate the robustness of a statistical model to misspecification of some of its underlying probabilities. In the context of graphical models, investigations of robustness fall under the notion of sensitivity analyses. These analyses consist in varying some of the model's probabilities or parameters and then assessing how far apart the original and the varied distributions are. However, for Gaussian graphical models, such variations usually make the original graph an incoherent representation of the model's conditional independence structure. Here we develop an approach to sensitivity analysis which guarantees the original graph remains valid after any probability variation and we quantify the effect of such variations using different measures. To achieve this we take advantage of algebraic techniques to both concisely represent conditional independence and to provide a straightforward way of checking the validity of such relationships. Our methods are demonstrated to be robust and comparable to standard ones, which can break the conditional independence structure of the model, using an artificial example and a medical real-world application.																	1532-4435						2020	21																						
J								Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Medical diagnostic imaging; Noise measurement; Collaboration; Feature extraction; Task analysis; Training; Unsupervised domain adaptation; deep learning; label noise; medical image diagnosis		Deep learning based medical image diagnosis has shown great potential in clinical medicine. However, it often suffers two major difficulties in real-world applications: 1) only limited labels are available for model training, due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noise (e.g., mislabeling labels) due to diagnostic difficulties of diseases. To address these, we seek to exploit rich labeled data from relevant domains to help the learning in the target task via Unsupervised Domain Adaptation (UDA). Unlike most UDA methods that rely on clean labeled data or assume samples are equally transferable, we innovatively propose a Collaborative Unsupervised Domain Adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. We theoretically analyze the generalization performance of the proposed method, and also empirically evaluate it on both medical and general images. Promising experimental results demonstrate the superiority and generalization of the proposed method.																	1057-7149	1941-0042					2020	29						7834	7844		10.1109/TIP.2020.3006377													
J								A Comprehensive Benchmark for Single Image Compression Artifact Reduction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Compression artifacts removal; benchmark; side information; loop filter; deep learning	QUALITY ASSESSMENT; JPEG DECOMPRESSION; DEBLOCKING; DCT	We present a comprehensive study and evaluation of existing single image compression artifact removal algorithms using a new 4K resolution benchmark. This benchmark is called the Large-Scale Ideal Ultra high-definition 4K (LIU4K), and it includes including diversified foreground objects and background scenes with rich structures. Compression artifact removal, as a common post-processing technique, aims at alleviating undesirable artifacts, such as blockiness, ringing, and banding caused by quantization and approximation in the compression process. In this work, a systematic listing of the reviewed methods is presented based on their basic models (handcrafted models and deep networks). The main contributions and novelties of these methods are highlighted, and the main development directions are summarized, including architectures, multi-domain sources, signal structures, and new targeted units. Furthermore, based on a unified deep learning configuration (i.e. same training data, loss function, optimization algorithm, etc.), we evaluate recent deep learning-based methods based on diversified evaluation measures. The experimental results show state-of-the-art performance comparisons of existing methods based on both full-reference, non-reference, and task-driven metrics. Our survey gives a comprehensive reference source for future research on single image compression artifact removal and inspires new directions in related fields.																	1057-7149	1941-0042					2020	29						7845	7860		10.1109/TIP.2020.3007828													
J								Truncated Low-Rank and Total p Variation Constrained Color Image Completion and its Moreau Approximation Algorithm	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tensile stress; TV; Color; Image color analysis; Approximation algorithms; Correlation; Machine learning; Matrix; tensor completion; multichannel image inpainting; low-rank; total variation; Moreau approximation	MATRIX COMPLETION; OPTIMIZATION	Recently, low-rank (LR) and total variation (TV) constrained tensor completion algorithms have been broadly studied for image restoration. These algorithms, however, ignore the difference of the intrinsic properties along spatial structure, spectral correlation, and unfolded mode. In this paper, we go further by providing a detailed comparison of the LR and TV properties in matrix and tensor cases, and figure out the LRTV constraints for pixel matrices are more evident and accordant than for others. This inspires us to develop a simple yet effective multichannel LRTV model that is capable of genuinely discovering the intrinsic properties with reduced computational cost. Moreover, due to the suboptimality of nuclear norm and l(1) norm in approximating the essential low rank and low gradient properties, we employ two enhanced constraints, i.e., truncated nuclear norm (TNN) and total p variation TpV, for a better performance. This results in a challenging problem since that both TNN and TpV are nonsmooth and nonconvex. Observing that the Moreau approximation of TpV constraint is a continuous difference-of-convex function, we then develop a first-order method by repeatedly computing two simple proximal operators. Under mild assumption, we further prove that the sequence generated by our method clusters at a stationary point. Extensive experimental results on color image completion show the efficacy and efficiency of our method over state-of-the-art competitors.																	1057-7149	1941-0042					2020	29						7861	7874		10.1109/TIP.2020.3008367													
J								Multi-Cue Semi-Supervised Color Constancy With Limited Training Samples	IEEE TRANSACTIONS ON IMAGE PROCESSING										Color constancy; illumination estimation; white balancing; multi-cue; semi-supervised	ILLUMINATION CHROMATICITY; ALGORITHMS	Color constancy is one of the fundamental tasks in computer vision. Many supervised methods, including recently proposed Convolutional Neural Networks (CNN)-based methods, have been proved to work well on this problem, but they often require a sufficient number of labeled data. However, it is expensive and time-consuming to collect a large number of labeled training images with accurately measured illumination. In order to reduce the dependence on labeled images and leverage unlabeled ones without measured illumination, we propose a novel semi-supervised framework with limited training samples for illumination estimation. Our key insight is that the images with similar features from different cues will share similar lighting conditions. Consequently, three graphs based on three visual cues, low-level RGB color distribution, mid-level initial illuminant estimates and high-level scene content, are constructed to represent the relationship among different images. Then a multi-cue semi-supervised color constancy method (MSCC) is proposed after integrating these three graphs into a unified model. Extensive experiments on benchmark datasets demonstrate that our proposed MSCC method outperforms nearly all the existing supervised methods with limited labeled samples. Even with no unlabeled samples, MSCC still obtains better performance and stableness than most supervised methods.																	1057-7149	1941-0042					2020	29						7875	7888		10.1109/TIP.2020.3007823													
J								Enhanced 3DTV Regularization and Its Applications on HSI Denoising and Compressed Sensing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Noise reduction; Task analysis; Compressed sensing; Correlation; Tensile stress; TV; Sensors; Hyperspectral image; denoising; compressed sensing; sparsity; correlation; total variation	NOISE REMOVAL; TENSOR; SPARSE; IMAGES	The total variation (TV) is a powerful regularization term encoding the local smoothness prior structure underlying images. By combining the TV regularization term with low rank prior, the 3D total variation (3DTV) regularizer has achieved advanced performance in general hyperspectral image (HSI) processing tasks. Intrinsically, 3DTV assumes i.i.d. sparsity structures on all bands of the gradient maps calculated along the spectrum and space of an HSI. This, however, largely deviates from the real-world cases, where the gradient maps generally have different while correlated gradient map structures across all bands. To alleviate this issue, we propose an enhanced 3DTV (E-3DTV) regularization term beyond the conventional. Instead of imposing sparsity on gradient maps themselves, the new term calculates sparsity on the subspace bases on gradient maps along all bands of an HSI, which naturally encodes the correlation and difference among all these bands, and thus more faithfully reflects the insightful configurations of an HSI. The E-3DTV term can easily replace the conventional 3DTV term and be embedded into an HSI processing model to ameliorate its performance. We made such attempts on two typical related tasks: HSI denoising and compressed sensing. The superiority of our proposed method is substantiated by extensive experiments on synthetic and real HSI data, visually and quantitatively on both tasks, as compared with current state-of-the-arts. The code of our algorithm is released at https://github.com/andrew-pengjj/Enhanced-3DTV.git.																	1057-7149	1941-0042					2020	29						7889	7903		10.1109/TIP.2020.3007840													
J								HAM: Hidden Anchor Mechanism for Scene Text Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Shape; Image segmentation; Computational efficiency; Computer architecture; Iterative methods; Task analysis; Electronic mail; Scene text detection; multi-oriented text; multitask; fully convolutional network	UNIFIED FRAMEWORK; TRACKING; RECOGNITION	Direct regression and anchor are the two mainly effective and prevailing mechanisms in the paradigm of scene text detection. However, the use of direct regression-based methods may be challenging during optimization without the help of anchors as references. Unfortunately, the anchor-based methods always suffer from the careful design of the anchors, degrading the robustness to complex scenes. To address the above-mentioned problems, we propose a novel hidden anchor mechanism (HAM) especially for scene text detection. The predictions of anchors are innovatively regarded as hidden layers, and the weighted sum of the predictions is integrated into a direct regression-based network. Hence, the architecture of our HAM still has the characteristic of simplicity as with direct regression-based methods. Moreover, it is easier to optimize anchors as references with this type of method than with direct regression-based methods. In this way, our network can take advantage of both direct regression and anchor mechanisms. In addition, we decouple three kinds of one-dimensional anchors from three-dimensional anchors, greatly reducing the number of anchors in text bounding box matching without performance degradation. We also propose a post-processing technique for long text detection, named iterative regression box (IRB), which takes a few additional computational costs and can be easily generalized to other methods. Experiments on several public datasets demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/hjbplayer/HAM.																	1057-7149	1941-0042					2020	29						7904	7916		10.1109/TIP.2020.3008863													
J								An Improved Multi-View Convolutional Neural Network for 3D Object Retrieval	IEEE TRANSACTIONS ON IMAGE PROCESSING										Three-dimensional displays; Shape; Feature extraction; Measurement; Training; Convolutional neural networks; Task analysis; 3D object retrieval; metric learning; similarity learning; loss functions; multi-view CNN	PERSON REIDENTIFICATION; ENSEMBLE	Learning robust and discriminative representations is essential for 3D object retrieval. In this paper, we present an improved Multi-view Convolutional Neural Network (MVCNN) for view-based 3D object representation learning. Our technical contributions are divided into two aspects. First, we propose to employ Group-view Similarity Learning (GSL) over the multi-view representations before the aggregation operation (i.e., max-pooling in MVCNN). We assume that the similarity information among the view groups of different 3D objects can provide an important cue but has been neglected more or less by previous methods. To enhance it, we add a branch to the original MVCNN architecture and learn to maintain such group-view similarity relationships. Second, we utilize an end-to-end metric learning loss function to improve the representation learning process. In particular, we propose an improved Triplet-Center Loss (TCL) named Adaptive Margin based Triplet-Center Loss (AMTCL). The original TCL assumes a fixed and common margin to control the relative distance relationship between a sample to its corresponding class center and to the nearest negative center. Though TCL has demonstrated its great capacity on the 3D object retrieval task, however, when considering the distinguishability between samples of one class and samples of another class, we assume that it would be more appropriate that the margin takes different values based on the distinguishability of samples of different classes. Therefore we propose to adaptively and dynamically adjust the margin hyperparameter based on the normalized confusion matrix which is obtained on the training set during the training process. Extensive experiments on several public 3D shape benchmarks show that our method, GSL + AMTCL, can learn more suitable representations for 3D object retrieval, obtaining superior performance against state-of-the-art methods.																	1057-7149	1941-0042					2020	29						7917	7930		10.1109/TIP.2020.3008970													
J								Boosting Compressed Sensing Using Local Measurements and Sliding Window Reconstruction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image Reconstruction; local measurements; compressed sensing; non-regular sampling	SIGNAL RECOVERY; FREQUENCY; IMAGES	In the framework of compressed sensing, image data is measured using less measurements than the total number of pixels. Each measurement consists of a (random) linear combination of all pixels. Since image data is approximately sparse in an appropriate transform domain, a reasonable reconstruction is possible for many measurement matrices, especially for i.i.d. Gaussian measurement matrices. In a seemingly different field, non-regular sampling techniques such as three-quarter sampling have shown promising results to enhance the resolution of an imaging sensor by effectively sub-sampling a higher resolution image. Here, the measurements can be described as linear combinations of only three pixels, which can also be seen as a (spectral) compressed sensing measurement. Since each measurement is spatially localized, the reconstruction can be performed in overlapping sliding windows. In this work, we show that compressed sensing reconstruction algorithms can greatly benefit from such an overlapping sliding window reconstruction. Compared to conventional block-wise compressed sensing with i.i.d. Gaussian measurement matrices, the reconstruction quality in terms of the PSNR increases up to +5dB using small, local i.d.d. Gaussian measurement blocks. Additionally, we propose a local joint sparse deconvolution and extrapolation (L-JSDE) to reconstruct images from arbitrary local measurements. For several applications with local measurements we show that L-JSDE increases the PSNR by +2.2dB relative to conventional block-wise i.i.d. Gaussian measurements reconstructed with the state-of-the-art reconstruction algorithm D-AMP using the same overall sampling density.																	1057-7149	1941-0042					2020	29						7931	7944		10.1109/TIP.2020.3007822													
J								Light Field Image Quality Assessment via the Light Field Coherence	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Image quality; Coherence; Image color analysis; Distortion; Distortion measurement; Light field image; light field coherence; sub-aperture image; epi-polar plane image; image quality assessment	ASSESSMENT MODEL; SIMILARITY	In this paper, a novel full-reference image quality assessment (IQA) method for evaluating the quality of the distorted light field (LF) image against its reference LF image is proposed, called the log-Gabor feature-based light field coherence (LGF-LFC). Based on the fact that to compare two LF images, it essentially boils down to measure how coherent of these two LF images, we attempt to measure the degree of their LF coherence (LFC). To pursue this goal, the salient features from the reference and distorted LF images under comparison need to be extracted. By considering that the Gabor feature has the ability to well characterize the human visual system (HVS) perception, and the special characteristics of the LF images, the multi-scale and single-scale Gabor feature extraction schemes are developed to extract the multi-scale log-Gabor features from the sub-aperture images (SAIs) and the single-scale log-Gabor feature from the epi-polar images (EPIs), respectively. Note that the former can reflect the image details (via the SAIs), while the latter indicates the viewing consistency (via the EPI's depth information). The similarity measurements are subsequently conducted on the comparison of their SAIs and that of their EPIs separately, followed by combining them together for arriving at the final score. Extensive simulation results have clearly demonstrated that the proposed LGF-LFC is more consistent with the perception of the HVS on the quality evaluation of the LF images than multiple classical and state-of-the-art IQA methods.																	1057-7149	1941-0042					2020	29						7945	7956		10.1109/TIP.2020.3008856													
J								Long-Term Action Dependence-Based Hierarchical Deep Association for Multi-Athlete Tracking in Sports Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-object tracking; sports video analysis; siamese network	MULTITARGET; PLAYERS	Tracking multiple athletes in sports videos is a very challenging Multi-Object Tracking (MOT) task, as athletes generally share high similarity in appearance with large deformations. In this paper, unlike the existing hand-crafted solutions, we propose a novel and effective approach to this issue, which hierarchically associates detections of the same identity through discriminative and robust deep features. First, in detection association, we make use of athlete appearances and poses instead of traditional position cues to generate short tracklets for better initialization. Second, in tracklet association, a new deep architecture, namely Siamese Tracklet Affinity Networks (STAN), is presented, which is able to bi-directionally simulate the unseen dynamics of actions, comprehensively models the long-term action dependences, and sequentially estimates their affinity. Such hierarchical association is finally solved as a minimum-cost network flow problem. We extensively evaluate the proposed approach on the APIDIS, NCAA Basketball and VolleyTrack (newly collected) databases, and the experimental results show its advantages.																	1057-7149	1941-0042					2020	29						7957	7969		10.1109/TIP.2020.3009034													
J								An outlier ensemble for unsupervised anomaly detection in honeypots data	INTELLIGENT DATA ANALYSIS										Outlier ensembles; network security; anomaly detection; honeypots	FRAMEWORK	Nowadays, computers, as well as smart devices, are connected through communication networks making them more vulnerable to attacks. Honeypots are proposed as deception tools but usually used as part of a proactive defense strategy. Hence, this article demonstrates how honeypots data can be analyzed in an active defense strategy. Furthermore, anomaly detection based on unsupervised machine learning techniques allows to build autonomous systems and to detect unknown anomalies without the need for prior knowledge. However, the unsupervised techniques applied for honeypots data analysis do not value the advantages of these tools' data, particularly the high probability that they include a large number of previously unseen anomalies with unexpected and diverse patterns. Therefore, in the present work, the aim is to improve the unsupervised anomaly detection in honeypots data by varying the data feature subset and the parameterization of the anomaly detection algorithm. To this purpose, an outlier ensemble with LOF (Local Outlier Factor) as a base algorithm is proposed. The ensemble outperforms existing solutions as depicted in the experiments where a detection rate higher than 92% is achieved.																	1088-467X	1571-4128					2020	24	4					743	758		10.3233/IDA-194656													
J								An advanced profile hidden Markov model for malware detection	INTELLIGENT DATA ANALYSIS										Malware detection; metamorphic; static analysis; profile hidden Markov models	HYBRID ANALYSIS; ALIGNMENT	The rapid growth of malicious software (malware) production in recent decades and the increasing number of threats posed by malware to network environments, such as the Internet and intelligent environments, emphasize the need for more research on the security of computer networks in information security and digital forensics. The method presented in this study identifies "species" of malware families, which are more sophisticated, obfuscated, and structurally diverse. We propose a hybrid technique combining aspects of signature detection with machine learning based methods to classify malware families. The method is carried out by utilizing Profile Hidden Markov Models (PHMMs) on the behavioral characteristics of malware species. This paper explains the process of modeling and training an advanced PHMM using sequences obtained from the extraction of each malware family's paramount features, and the canonical sequences created in the process of Multiple Sequence Alignment (MSA) production. Due to the fact that not all parts of a file are malicious, the goal is to distinguish the malicious portions from the benign ones and place more emphasis on them in order to increase the likelihood of malware detection by having the least impact from the benign portions. Based on "consensus sequences", the experimental results show that our proposed approach outperforms other HMM-based techniques even when limited training data is available. All supplementary materials including the code, datasets, and a complete list of results are available for public access on the Web.'																	1088-467X	1571-4128					2020	24	4					759	778		10.3233/IDA-194639													
J								Two deterministic selection methods for the initial centers in fuzzy c-means based algorithms	INTELLIGENT DATA ANALYSIS										Data clustering; fuzzy c-means; initial centers; cluster validation indexes		Fuzzy C-Means (FCM) is the most commonly used and discussed fuzzy clustering algorithm in the literature. Nevertheless, it is well known that the performance of FCM is strongly affected by the selection of the initial cluster centers. In other words, the selection of a good set of initial cluster centers plays an important role in the performance of this algorithm. The most common selection method is the trial-and-test random method, in which each execution is performed with different initial centers, randomly generated, resulting in different dataset partitions. This paper proposes two methods to obtain the initial cluster centers which are applied in FCM and its variants. The proposed methods are deterministic, since, for each data set and number of clusters, they will always provide the same cluster centers set. The main advantage of these methods is to provide high quality partitions faster than the original methods as well as other FCM and ckMeans-based algorithms with deterministic selection of cluster centers.																	1088-467X	1571-4128					2020	24	4					779	798		10.3233/IDA-194588													
J								RobustRepStream: Robust stream clustering using self-controlled connectivity graph	INTELLIGENT DATA ANALYSIS										Clustering; data streams; data clustering; data mining; stream clustering	ALGORITHMS	A major challenge in stream clustering is the evolution in the statistical properties of the underlying data. As clustering is inherently unsupervised, selecting suitable parameter values is often difficult. Clustering algorithms with sensitive parameters are often not robust to such changes, leading to poor clustering outputs. Algorithms using K-NN graphs face this problem, as they have a sensitive K-connectivity parameter which prohibits them from adapting to stream concept evolution. We address this by controlling the excess of the skewness of edge length distributions in the underlying K-NN graph by introducing novel skewness excess concept. We demonstrate the asymptotic linear dependency of skewness excess against the graph connectivity and propose the novel RobustRepStream algorithm, which extends the RepStream algorithm, and provides improved robustness against stream evolution. By automatically controlling the skewness excess, the user no longer needs to specify the K-connectivity parameter, and RobustRepStream can adjust the graph connectivity locally in order to achieve performance close to when the optimal K value is known. We demonstrate that RobustRepStream's skewness threshold parameter is insensitive and universal across all data sets. We comprehensively evaluate RobustRepStream on real-world benchmark data sets against previous stream clustering algorithms, and demonstrate that it provides better clustering performance.																	1088-467X	1571-4128					2020	24	4					799	830		10.3233/IDA-194715													
J								An efficient algorithm for hiding sensitive-high utility itemsets	INTELLIGENT DATA ANALYSIS										High utility mining; high utility itemset; sensitive-high utility itemset hiding; privacy-preserving utility mining	ASSOCIATION RULES	Privacy-preserving utility itemset mining is the process of hiding sensitive-high utility itemsets (SHUIs) appearing in original database such that they will not be discovered in the sanitized database. The purpose of SHUI hiding algorithm is to conceal the set of SHUIs while minimizing the side effects which caused by data distortion process. In this paper, a novel algorithm, named EHSHUI (An Efficient Algorithm for Hiding Sensitive-high utility Itemsets), is proposed to minimize the side effects of the sanitization process. The proposed algorithm includes three heuristic steps: (1) The transaction on which the SHUI achieves maximal utility among transactions containing it is specified as victim transaction; (2) The item that causes minimal impacts on non-SHUIs is selected as victim item; and (3) An exactly number of utility is calculated for reducing internal utility of victim item from victim transaction. This strategy exactly identifies item and transaction for data modification such that it minimizes the impacts on non-SHUIs, data distortions, and the time to access database. The experiment results illustrate that the proposed algorithm achieves higher performance and lower side effects than the state-of-the-art.																	1088-467X	1571-4128					2020	24	4					831	845		10.3233/IDA-194697													
J								A hybrid node classification mechanism for influential node prediction in Social Networks	INTELLIGENT DATA ANALYSIS										Structural hole; influential node; influential range; regression classifier; similarity metrics; centrality; Euclidean distance; hamming distance; AUC	STRUCTURAL HOLES; KEY NODES	Social Networks is an essential phenomenon in all aspects through various perspectives. These networks contain a large number of users better termed as nodes and the connections between the users termed as edges. For efficient information processing and retrieving, accessing the influential node is essential for improving the diffusion process. To identify the influential node inside a heterogeneous community, incorporating probability metrics with regression classifier is put forth stated by proposed method Support Vector Bayesian Machine (SVBM). Node metrics such as degree centrality, closeness centrality is measured for eliminating the nodes primarily. A standardized index based on the centrality values computed for enhancing into SVBM. After the standardized index, similarity dissimilarity index values evaluated by combining the Euclidean, Hamming, Pearson coefficient for valued relations and Jaccard for binary relations which results in a single index value considered as the power degree value(p). The value p determines the node's boundedness, which indicates the range of influence within the community. The outlier nodes in the bounded region get eliminated, and the nodes remaining taken for the final phase of SVBM, probability regression line predicts the node inhibiting the most influential nature. Experimental evaluation of the proposed system with the existing Support Vector Machine (SVM) technique resulted in 0.95 and 0.41 respectively for Area Under Curve (AUC) denoting that the true positive influential node classification process from the other existing nodes was higher than SVM. In comparison with the existing SVM, the proposed methodology SVBM attained a node detection, which influenced a higher diffusion rate within the networks.																	1088-467X	1571-4128					2020	24	4					847	871		10.3233/IDA-194724													
J								Feature learning for representing sparse networks based on random walks	INTELLIGENT DATA ANALYSIS										Feature learning; embedding graph; sparse network; deep learning		Identifying features to represent graphs such as social networks, protein graphs is increasingly common in both research and business communities, thanks to the fact that data has increased not only in quantity but also in complexity. This results in the graphs to be sparser because not all nodes are fully connected. In addition, if this whole graph is used as input data for learning algorithms e.g. neural network, a lot of training time will be required. Substantial efforts have been made to convert the graphs to better yet compact representations, among of which is graph embedding. The traditional methods used to map the original graph to its embedding representation had not yielded significant results until deep learning was invented. Many good approaches in this direction, as examples, are DeepWalk, node2vec. However, their general weakness is many important connections in the original graph could be lost. In this paper, we propose another approach to retain more edge information while ensuring the embedding graph is still sufficiently small, compared to the original one. Our experiment results show that the method also increases the accuracy of latter learning models.																	1088-467X	1571-4128					2020	24	4					873	886		10.3233/IDA-194676													
J								A comparative analysis of Bayesian network structure learning algorithms applied to crime data	INTELLIGENT DATA ANALYSIS										Graphical models; Bayesian inference; crime prediction; computational statistics; criminology; learning algorithms	CAUSAL	The theories about crime and correction have their inception in the eighteenth century, highly influenced by the anthropological thoughts emerging during the age of Enlightenment. Throughout the decades, the criminological studies observed their sociological essence encompassing practices from other scientific fields to explain the more contemporary questions, becoming Criminology an inherently interdisciplinary science as a result. The adoption of concepts from Exact Sciences is a recent moving, originating it a novel research area, called Computational Criminology, which employs procedures from Applied Mathematics, Statistics and Computer Science to provide original or enhanced solutions to such questions. One of the most prominent tasks brought by this rising field is crime prediction, which attempts to uncover potential targets for future police intervention and also help solving already committed offenses. The present comparative analysis thus investigates the employment of statistical inference by means of Bayesian network for predictive policing, using the openly accessible registers from Chicago Police Department. Numerous algorithms are available to learn the structure for a Bayesian network purely from data and a comparative examination about them is hence described, with the purpose to establish the most precise and efficient one, according to the attributes of the said criminal dataset, for the implementation of the intended inference.																	1088-467X	1571-4128					2020	24	4					887	907		10.3233/IDA-194534													
J								A novel adaptive k-NN classifier for handling imbalance: Application to brain MRI	INTELLIGENT DATA ANALYSIS										k-NN classifier; adaptive distance; skewed data; brain tumors; medical imagery	NEAREST-NEIGHBOR RULE; SELECTION; DATASETS; SMOTE; SETS	The problem of efficiently classifying imbalanced data has become one of the most challenging tasks in machine learning. Some real world examples include medical image analysis, fraud detection, fault diagnosis, and anomaly detection. Although several data-level algorithms have been developed to address imbalance, they are typically subject to some restrictions. We propose a novel variant of the k-NN family of classifiers, and name this as Density-based Adaptive-distance kNN (DAkNN). It can effectively handle data with skewed distributions and varying class-densities using the concept of adaptive distance. Comparative superiority is experimentally established over related data-level algorithms (SMOTE, ADASYN), using ten sets of two-class data, in terms of geometric mean (of the true positive and negative rates) and accuracy. Additionally, five sets of multi-class data are considered and compared with different variants of k-NN, which are currently very popular. Finally, DAkNN is successfully applied on the highly imbalanced Lower Grade Glioma (LGG) MR images, with an Average-Dice score of 0.9082 for delineating the tumor regions. The results demonstrate clear superiority over state-of-the-art algorithms.																	1088-467X	1571-4128					2020	24	4					909	924		10.3233/IDA-194647													
J								Real-time detection and trend tracing of burst topics based on Negative Binomial Distribution on spark	INTELLIGENT DATA ANALYSIS										Negative Binomial Distribution; stream data; topic modelling; burst events tracking; spark		Social networks have evolved into a popular information and communication platform, and the vast amount of data it generates are rapidly changing and spreading. Thus, it is essential to detect and trace large events and burst topics in mass social network data based on real-time Big Data parallel computing. In this paper, we propose a model that uses the Negative Binomial Distribution to fit the distribution of Weibo topic words. Then, we introduce the concepts of the 'hot degree' and the 'dispersion degree' of a topic with their corresponding computing methods. And we validate the efficiency of the model using real data. Furthermore, we design a topic detection and trend-tracing algorithm based on stream data, and implement the algorithm on Spark Streaming which is a streaming processing framework that uses memory computing. Finally, the experiments on real data demonstrate that our proposal is effective and efficient in tracking bursting events.																	1088-467X	1571-4128					2020	24	4					925	940		10.3233/IDA-194663													
J								Multi-fuzzy-constrained graph pattern matching with big graph data	INTELLIGENT DATA ANALYSIS										Graph pattern matching; big graph data; multi-fuzzy-constrained	SUBGRAPHS; FRAMEWORK; ALGORITHM	Graph pattern matching has been widespread used for protein structure analysis, expert finding and social group selection, ect. Recently, the study of graph pattern matching using the abundant attribute information of vertices and edges as constraint conditions has attracted the attention of scholars, and multi-constrained simulation has been proposed to address the problem in contextual social networks. Actually, multi-constrained graph pattern matching is an NP-complete problem and the fuzziness of constraint variables may exist in many applications. In this paper, we introduce a multi-fuzzy-constrained graph pattern matching problem in big graph data, and propose an efficient first-k algorithm Fuzzy-ETOF-K for solving it. Specifically, exploration-based method based on edge topology is adopted to improve the efficiency of edge connection, and breadth-first bounded search is used for edge matching instead of shortest path query between two nodes to improve the efficiency of edge matching. The results of our experiments conducted on three datasets of real social networks illustrate that our proposed algorithm Fuzzy-ETOF-K significantly outperforms existing approaches in efficiency and the introduction of fuzzy constraints makes our proposed algorithm more efficient and effective.																	1088-467X	1571-4128					2020	24	4					941	958		10.3233/IDA-194653													
J								A community-based algorithm for influence maximization on dynamic social networks	INTELLIGENT DATA ANALYSIS										Influence spread; community structure; dynamic social network; three phases	NODES	The purpose of the influence maximization is to find the top k influential seeds which can maximize the influence spread. Recently, some researchers address this problem through community structure. However, most of these community-based studies only consider the static social network, which ignores that social networks change frequently. In order to deal with the above problem, we present a community-based algorithm on the dynamic social network, which is divided into three phases: (i) community detection, (ii) candidate seed set on the dynamic network, and (iii) final seed set. In the first phase, we use the Louvain algorithm to obtain the community structure. In each community, we analyze the node location to judge the importance of the node. In the second phase, considering the dynamic social network, when a node is added to the network or removed from the network, we update the structure of the social network. Then, the candidate nodes are those nodes with a large influence in each community. And in the third phase, we select k influential seeds from the candidate seeds by CELF algorithm. Extensive experimental results show that our algorithm obtains a better influence spread than many baseline algorithms as well as an acceptable running time while considering the dynamic social networks.																	1088-467X	1571-4128					2020	24	4					959	971		10.3233/IDA-194675													
J								A spatial pyramidal decomposition method for finger vein recognition using local descriptors	INTERNATIONAL JOURNAL OF BIOMETRICS										finger vein recognition; FVR; spatial pyramid decomposition; SPD; local binary pattern; LBP; binarised statistical image feature; BSIF; local phase quantisation; LPQ	IMAGES; SYSTEM	Finger vein patterns have been proved as one of the most promising biometric modality for its convenience and security. Most of the current available finger vein recognition methods utilise features from a segmented blood vessel network. This manner of processing however may not provide optimal recognition accuracies as reported in many studies. Therefore, this paper proposes in the feature extraction stage, the use of the spatial pyramid decomposition (SPD) method aiming at partitioning the finger vein images into increasingly fine subregions from which local texture descriptors are obtained. The descriptors adopted in this paper are local binary pattern (LPB), binarised statistical image feature (BSIF) and local phase quantisation (LPQ). The performance of the proposed approach evaluated on two publicly databases PolyU and SDUMLA achieves a recognition accuracy higher than that of some existing systems reported in the literature for both the SDUMLA and the PolyU databases.																	1755-8301	1755-831X					2020	12	2					131	146															
J								Multi-pose facial expression recognition using rectangular HOG feature extractor and label-consistent KSVD classifier	INTERNATIONAL JOURNAL OF BIOMETRICS										facial expression recognition; emotional classification; sparse coding; dictionary learning; histogram oriented gradient; HOG; label-consistent KSVD; LC-KSVD	DISCRIMINATIVE DICTIONARY; K-SVD; NETWORKS	In this paper, a new approach to the classification of facial expressions from multiple pose images is proposed. In this approach, a rectangular histogram of oriented gradient (R-HOG) algorithm is first designed to extract features of face images. The parameters of the R-HOG algorithm, which is a modification of the original HOG algorithm include cell shape, cell size, block size, and the number of orientation bins. The R-HOG is capable of capturing more discriminative texture features of different facial expressions. In addition, a supervised dictionary learning classifier, the label-consistent K-SVD (LC-KSVD) algorithm, is adopted to recognise the facial expression of the subject. To investigate its effectiveness, the proposed technique was applied to classify emotional states of the face images in the two public available facial expression datasets: KDFE and RafD. The experiment study showed that the new method outperformed in many aspects those methods reported in the literature tested with the same datasets. First, the new method handles pose variations better. Second, it is more robust in cases where the size of a training dataset is small. Finally, it's accuracy performance is more consistent measured by standard deviations.																	1755-8301	1755-831X					2020	12	2					147	162															
J								An accurate and fast method for eyelid detection	INTERNATIONAL JOURNAL OF BIOMETRICS										biometics; Canny edge detector; eyelid detection; iris localisation; iris recognition system; Prewitt operator; Sobel operator	IRIS RECOGNITION; LOCALIZATION	A novel method called refine-connect-extend-smooth (R-C-E-S) for detecting eyelids is presented. It consists of four algorithms, Canny edge detector with Prewitt operator, modified refine edge map (MREM), connect edges-extend (CEE) and smooth curve (SC). The method is not based on pre-assumptions that consider eyelids as parabola or lines and it does not use curve fitting algorithm, therefore sever deviation of the detected eyelid curve from the actual eyelid path is avoided. The method is applied to three types of database, CASIA-V1.0, CASIA-V4.0-Lamp and SDUMLA-HMT. The accuracies for detecting the lower eyelid, upper eyelid and free iris are (93.2%, 99.1%, 96.7%) for CASIA-V1.0, while for CASIA-V4.0-Lamp are (97.6%, 98.3%, 97.8%) and for SDUMLA-HMT are (95.1%, 95.3%, 96.92%). The processing times for detecting single eyelid, both eyelids and free iris are (42 ms, 49 ms, 35 ms) for CASIA-V1.0, while for CASIA-V4.0-Lamp are (23 ms, 26 ms, 21 ms) and for SDUMLA-HMT are (35 ms, 40 ms, 31 ms).																	1755-8301	1755-831X					2020	12	2					163	178															
J								New method for identification of persons using geometry foot outline	INTERNATIONAL JOURNAL OF BIOMETRICS										biometrics; foot outline measurements; personal identification	STATURE; DIMENSIONS; FOOTPRINT; HAND	In this study we try to improve existing recognition rates using plantar surface of foot to extract 19 local geometrical features based on anatomical points and axes, in order to extract informations and relationships required to perform a biometric system for identifying individuals. Before the primary data collection, a pilot study was conducted to assess the precision of stature acquisition and foot measurements. A computer database has been constructed taking measurements from feet pictures of 102 volunteers using the measuring tool in Photofilter. Using the measurements with +/- 0.38 like error margin and searching through it shows that each foot is unique. This study also showed that each measurement parameter's has an identification percentage and the highest percentages were noted for the parallel and diagonal lengths measurements. To validate our database and our results we tried to study the relation between the stature and some foot measurements using a linear equation of regression.																	1755-8301	1755-831X					2020	12	2					179	192															
J								Biometric face classification with the hybridised rough neural network	INTERNATIONAL JOURNAL OF BIOMETRICS										ant colony optimisation; ACO; biometric face; genetic algorithm; GA; gender; particle swarm optimisation; PSO; rough neural network; RNN	GENETIC ALGORITHM; FEATURE-SELECTION; SET-THEORY; FINGERPRINT CLASSIFICATION; OPTIMIZATION; RECOGNITION	Biometric face classification is an important indexing scheme to reduce face matching time for large volumes of a database. In this paper, a hybridised approach based on rough set theory (RST) and back propagation neural network (BPN) to classify human face is proposed. Local binary pattern (LBP) method is exploited to extract the features from pre-processed face images. The evolutionary optimisation algorithms such as genetic algorithm (GA), particle swarm optimisation (PSO), ant colony optimisation (ACO), hybridisation of ACO and GA (ACO-GA) and hybridisation of PSO and GA (PSO-GA) are investigated for feature selection. Finally, the hybridised rough neural network (RNN) is employed for classification. The experimental results of the proposed RNN is compared in terms of precision, recall, f-measure, accuracy and error rate with Naive Bayes, support vector machine (SVM), radial basis function network (RBFN), conventional BPN, and convolutional neural network (CNN) to conclude the efficacy of the proposed approach.																	1755-8301	1755-831X					2020	12	2					193	217															
J								An improved Weber-face-based method for face recognition under uncontrolled illumination conditions	INTERNATIONAL JOURNAL OF BIOMETRICS										face recognition; illumination normalisation; local texture patterns; contrast enhancement; pattern classification	COMPLEX WAVELET TRANSFORM; TEXTURE CLASSIFICATION; VARYING ILLUMINATIONS; FEATURE-EXTRACTION; QUOTIENT IMAGE; BINARY PATTERN; NORMALIZATION; INVARIANT; DECOMPOSITION; RETINEX	The paper presents a new face recognition system robust to illumination variations and moderate occlusion. Two main contributions are discussed. First, we introduce an approach based on contrast equalisation (CE) to improve the traditional Weber-face (WF) technique and make it more robust. Second, we use the local binary patterns (LBP) and local phase quantisation (LPQ) descriptors to make the Weber-face method more resilient to variations in illumination by exploiting both spatial and frequency domains information. By combining the two descriptors, enhanced facial features are obtained showing more discriminating power for variable lighting conditions as well as occlusion. The concept of complementing the WF model with spatial-frequency descriptors is novel and shown to result in a robust system resilient to changing lighting conditions, variations in pose, and occlusion. The method was compared to a number of existing techniques over three public databases. The proposed algorithm outperformed existing techniques under challenging environments.																	1755-8301	1755-831X					2020	12	2					218	246															
J								The quantitative evaluation on the advertisement design effects with fuzzy number intuitionistic fuzzy information	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Evaluation; fuzzy number intuitionistic fuzzy sets; Hamacher aggregation operators; induced fuzzy number intuitionistic fuzzy Hamacher correlated average (IFNIFHCA) operator; advertisement design effects	HAMACHER AGGREGATION OPERATORS; MULTIATTRIBUTE DECISION-MAKING; SIMILARITY MEASURES; MEAN OPERATORS; MODELS; EFFICIENCY; PRODUCTS; SETS	With the rapid growth of the market economy, ranked according to the proportion of consumption, China has become the world's second, a lot of foreign companies increasingly value the Chinese market, and constantly improve their design philosophy and business philosophy. Throughout today's advertising industry, it can be seen in the many excellent advertising designs, non-mainstream appearance has a high rate of non-mainstream cultural elements is a common design approach in the design of modern advertising, and more and more people sought after. In this paper, we investigate the multiple attribute decision making problems with fuzzy number intuitionistic fuzzy information. Firstly, we analyze several operations on the fuzzy number intuitionistic fuzzy sets. Then, we use the induced fuzzy number intuitionistic fuzzy Hamacher correlated average (IFNIFHCA) operator to solve multiple attribute decision making with the fuzzy number intuitionistic fuzzy information. Finally, an illustrative example for evaluating the advertisement design effects is given to verify the developed approach.																	1327-2314	1875-8827					2020	24	2					73	82		10.3233/KES-200031													
J								RELAS: A reliable authentication system for patient monitoring using IoT	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										RELAS; IoT; NodeMCU; PMS; real-time	HEALTH; SENSOR; CARE	Owing to ignorance of self health status and lack of regular check-ups, a large number of persons suddenly die in our society. Immediate physical presence of the injured or affected person at a health center for emergency service is also difficult at times. Through advancement in communication and information technologies, Internet of Things (IoT) uses a collection of applications to bring the passive mode healthcare system to the next level for providing real-time qualitative patient care. It incorporates several body sensors to trigger the service to a patient based on his/her current health status. The performances of these tiny sensors are bottlenecked with their limited energy and less computing capability. These constraints present a challenging situation for continuous monitoring of a person. We propose a RELiable Authentication System (RELAS) in which the entire process is divided into four different modes and each mode can have different subtasks and they are capable of executing their tasks either locally or in a server. The process supports both sequential as well as parallel way of execution, which includes periodic collection of physical parameters, environment details of the patient and stores the data in a server for forecasting. RELAS considers all the constraints during the execution period to reduce the battery consumption so as to maintain uninterrupted monitoring of a person. It is observed that the proposed model ensures less consumption of battery power, reduced time complexity and can handle large data size.																	1327-2314	1875-8827					2020	24	2					83	93		10.3233/KES-200032													
J								Query expansion via learning change sequences	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										query expansion; change sequence; DBN		Proksch has proved the changed terms of source code negatively affect code search quality. However, current query expansion (QE) methods always ignore it. In this paper we propose a novel QE method based on the semantics of change sequences (QESC). It not only captures which changes occurred by extracting change sequences from Github commits, but also understands why changes occurred by learning sequence semantics with Deep Belief Network (DBN). Thus it could extract relevant terms to expand or irrelevant terms to exclude from the changes semantically similar to a query. Our experimental results show QESC outperforms the existing QE methods by 15-23% in terms of precision on inspecting the first query result.																	1327-2314	1875-8827					2020	24	2					95	105		10.3233/KES-200033													
J								Dichotomy and well conditioning of two-point boundary value problems on time scale	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Boundary value problems; time-scale dynamical systems		In this paper, we establish close relationships between the stability constants on one hand and the global behaviour of fundamental matrices on the other hand to the two-point boundary value problems on time-scale dynamical systems. We introduce the concept of conditioning number k and show that conditioning number is the right criteria in estimating the global error due to small perturbations of two point boundary value problems on time scale dynamical systems. Further, the moderate stability constants imply a dichotomy with moderate k-bound will be developed. Further, the exponential behaviour of solutions of the Green's matrix will be investigated. We also investigate the conditions under which strong dichotomy exists for two-point boundary value problems when the boundary conditions are separable.																	1327-2314	1875-8827					2020	24	2					107	115		10.3233/KES-200034													
J								An experimental study on inverse adaptive neural fuzzy control for nonlinear systems	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Water bath temperature system; inverse control; type-2 fuzzy neural; uncertainty	MODEL	In this paper, a type-2 fuzzy neural network is used to adaptive inverse control of a class of nonlinear systems. The proposed method has proper performance for real-time control of nonlinear and time-invariant systems and has a quick response to following changes. In this method, the high approximation ability of type 2 fuzzy systems is used to reverse system modeling and this inverse model is used immediately to control the system. In order to measure the efficiency of the proposed method, the control of dynamic, highly nonlinear and time varying water both temperature system with certain and uncertain water pressure is experimentally considered. The results show the accuracy of the proposed method.																	1327-2314	1875-8827					2020	24	2					135	143		10.3233/KES-200036													
J								An improved dynamic polynomial integrity based QCP-ABE framework on large cloud data security	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Chaotic CP-ABE; cloud environment; data security	STORAGE	With the exponential growth of cloud data and network services, the computational resources and cloud data security has become one of the interesting research area of real-time cloud environment. Different types of cloud services are integrated in various domain applications such as defense, e-health, clinical databases etc, for data storage and resource computing. Attribute based encryption is a public key cryptographic algorithm that allows the cloud users to provide more security to the cloud data in the cloud storage services. Most of the traditional attribute based encryption techniques are applied on small datasets to generate constant size cipher text using limited computing resources. In the existing attribute based techniques, most of the attributes are considered as textual information and static values for key generation, data encryption and decryption process. To overcome these issues, a novel dynamic chaotic map based hashing is implemented to improve the security of the quantum based CP-ABE model. In the proposed model, user's attribute are secured using the dynamic chaotic map function for key initialization, data encoding and decoding process. In this model, both structured and unstructured large medical data are taken as input for integrity verification and encryption process. Practical simulation results show that the presented model has better accuracy in terms of cloud data encryption and decryption time and computed memory compared to the existing attribute based encryption and decryption techniques.																	1327-2314	1875-8827					2020	24	2					145	156		10.3233/KES-200037													
J								A novel Q/P droop control of standalone micro grid having multiple sources with adaptive controller	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Adaptive neuro-fuzzy interface system (ANFIS); battery energy storage system (BESS); state of charge (SOC); frequency control; Q/P droop control; standalone micro grid; voltage damping effect; voltage control	FREQUENCY CONTROL; WIND; GENERATORS; SYSTEM; ENERGY	In this paper a novel Q/P droop control strategy for regulating the voltage and frequency in Standalone micro grid with multiple renewable sources like solar and wind is presented. In contemporary to frequency control by synchronous machine a battery storage system is used for frequency control of Micro Grid. During the case of high discharge of the battery system a low rated synchronous generator is used to maintain the state of charge of the battery during frequency control. Since the output of the wind and solar varies continuously, a novel reactive (Q/P) droop control instead of conventional (P/F) and (Q/V) for voltage control. Adaptive Neuro Fuzzy logic Interface system (ANFIS) controller is used for frequency and voltage control for Renewable generation system. The induced voltage fluctuations are reduced to get nominal output power. The proposed model is tested on different cases and results show that the proposed method is capable of compensating voltage and frequency variations occurring in the micro grid with minimal rated synchronous generator.																	1327-2314	1875-8827					2020	24	2					157	170		10.3233/KES-200038													
J								Indexing of real time geospatial data by IoT enabled devices: Opportunities, challenges and design considerations	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Spatial indexing; concurrency control; GIS; Internet of things; spatiotemporal data	DISTRIBUTED SPATIAL INDEX; NEAREST-NEIGHBOR QUERY; BIG DATA ANALYTICS; MOVING-OBJECTS; KEYWORD QUERY; CONCURRENCY-CONTROL; QUADTREE INDEX; EFFICIENT; TREE; SEARCH	We are moving towards 'smart' world in which industries, such as healthcare, smart cities, transportation, and agri-culture have started using IoT (Internet of Things). These applications involve huge number of sensors and devices that generate high volume of real time data. To perform useful analytics on this data, location and spatial awareness characteristics of devices need to be considered. Wide range of location-based services and sensors in GIS have to manage moving objects that change their position with respect to time. These applications generate voluminous amount of real time geospatial data that demands an effective query processing mechanism to minimize the response time of a query. Indexing is one of the traditional ways to minimize the response time of a query by pruning the search space. In this paper, we performed a detailed survey of the liter-ature regarding the indexing of real time geospatial data generated by IoT enabled devices. Some major challenges relevant to indexing of moving objects are highlighted. Various important index design considerations are also discussed. The goal is to help researchers in understanding the principles, methods, and challenges in the indexing of real time geospatial data. This will also aid in identifying the future research opportunities.																	1876-1364	1876-1372					2020	12	4					281	312		10.3233/AIS-200565													
J								Evolving models for incrementally learning emerging activities	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Activity recognition; continual learning; smart home	NEURAL-NETWORKS	Ambient Assisted Living (AAL) systems are increasingly being deployed in real-world environments and for long periods of time. This significantly challenges current approaches that require substantial setup investment and cannot account for frequent, unpredictable changes in human behaviours, health conditions, and sensor deployments. The state-of-the-art method-ology in studying human activity recognition is cultivated from short-term lab or testbed experimentation, i.e., relying on well -annotated sensor data and assuming no change in activity models. This paper propose a technique, EMILEA, to evolve an ac-tivity model over time with new types of activities. This technique novelly integrates two recent advances in continual learning: Net2Net - expanding the architecture of a model while transferring the knowledge from the previous model to the new model and Gradient Episodic Memory - controlling the update on the model parameters to maintain the performance on recognising previously learnt activities. This technique has been evaluated on two real-world, third-party, datasets and demonstrated promis-ing results on enhancing the learning capacity to accommodate new activities that are incrementally introduced to the model while not compromising the accuracy on old activities.																	1876-1364	1876-1372					2020	12	4					313	325		10.3233/AIS-200566													
J								Personal productivity monitoring through smartphones	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Productivity; smartphone; logistic regression; SVM; KNN	PHYSICAL-ACTIVITY; BEHAVIOR; SENSORS; SYSTEM	Smartphones, with built-in array of sensors, provide an opportunity to ubiquitously collect user's behavioral data. This leads to variety of founding applications that identifies interesting patterns in the smartphone data to learn human behavior. In this paper, we propose an approach that enhances the productivity of individual's by unobtrusively learning their routine through smartphones. We design and develop a non-intrusive smartphone app - Prodmapp that periodically collects sensing data from user's smartphone. We extract several potentially useful behavioral features from the data and perform correlation analysis among the features and user's productivity score (ground truth). We collect 15 days sensing data from 10 users through Prodmapp. Ground truth is collected from the users in the form of questionnaires to quantify their productivity. The results showed that there exists a significant correlation among several behavioral features and user's productivity score. Finally, we train and evaluate a prediction model using significantly correlated features that can predict the change in productivity of users by analyzing the variation in feature values. We train three classifiers i.e., logistic regression, SVM and KNN to compare their performance on the two benchmark datasets, one collected through Prodmapp and other from CASAS smart home project. Results shows that our proposed approach performs well and all three classifiers achieve good prediction accuracy on both datasets.																	1876-1364	1876-1372					2020	12	4					327	341		10.3233/AIS-200567													
J								Harmonizing divergent user preferences for cultural enrichment of small group visit	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Collaborative tools; human computer interaction; path planning; recommender systems; cultural enrichment	RECOMMENDER SYSTEMS; DESIGN; PATHS; TOUR	People spend their leisure time on cultural and historical stories by visiting exhibitions and museums with family members and friends. This small gathering establishes a voluntary platform to engage in activities and share experiential knowl-edge. Since members of a small group tend to have diverse preferences, both individual and group preferences should be consid-ered for a cohesive and harmonious guide. This paper proposes a mixed-initiative tour path planning system for multiple visitors that builds a group path supported by automatic path generation and users' participation. In the proposed system, individual user profiles are merged into a group profile by favoring multiple users' higher preferences with low deviation. Then the system constructs a tour path based on mixed-initiative interaction that consists of the system's automatic decision and participation of members. When group members have similar preferences, a tour path is automatically decided by the system. If group mem-bers have different and divergent preferences, the system asks group members to intervene and confirm appropriate places to collaboratively construct a tour path. To show feasibility of our proposed system, we evaluate our approach in twofold consists of a synthesized group simulation and a user study. For the simulation, we collected user preferences of 12 real users to simu-late groups of two, three, and four members and explored characteristics of similar and different preference scenarios. For the user study, we conducted a comparative study with 11 participants under 4 different exhibition scenarios using the implemented mixed-initiative path planning system. Evaluation results support that our system constructs a cohesive, satisfying and usable path for a small group tour.																	1876-1364	1876-1372					2020	12	4					343	357		10.3233/AIS-200568													
J								A smart data logger system based on sensor and Internet of Things technology as part of the smart faculty	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Smart data logger system; Internet of Things; sensor technology; application; PIC microcontroller; measuring		This manuscript presents a new approach to the use of Internet of Things technologies in a practical and significant way in the system of smart data loggers. The subject of research is the observation and control of the microclimate using Internet of Things technology and smart sensor nodes. It is known that working environments take into account the ambient parameters that influence the concentration at work so that efficiency is at the highest level. For this reason, the primary goal is to create an inexpensive smart system for storing data obtained by measuring different ambient parameters and controlling them without any human involvement. The proposed smart data logger system is based on the following steps: directly monitoring the environment, measuring and storing data, and then allowing the user to adjust the parameters and provide a more comfortable working environment. This research will present the design and implementation of the practical smart data logger system, which can be further expanded where such a realized system can form part of a smart faculty. The system is based on: a group of built-in sensors, a microcontroller with a peripheral interface (PIC) as a core and a server system and a wireless Internet using the Global System of Mobile Telecommunications (GSM) module with General Packet Radio Service (GPRS) as a communication protocol. There is also a smartphone application that allows the user to monitor and control the ambient parameters. It is possible to implement a smart faculty service, in which the realized smart data logger system could be implemented, which enables automatic control of ambient parameters at the faculty.																	1876-1364	1876-1372					2020	12	4					359	373		10.3233/AIS-200569													
J								A novel population initialization strategy for accelerating Levy flights based multi-verse optimizer	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Antlion optimizer; Economic load dispatch; Design engineering problems; Firefly algorithm; Improved Multi-verse optimizer with Levy flights; Lambda iteration; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; ENGINEERING OPTIMIZATION; PARAMETER-ESTIMATION; ECONOMIC-DISPATCH; SEARCH ALGORITHM; DESIGN; EVOLUTION; SOLVE; MODEL	In this paper, we have designed a new optimization technique, which is named as the Improved Multi-verse Algorithm with Levy Flights (ILFMVO) algorithm. The quality of the population is an important factor that can directly or indirectly affect the strength of an algorithm in searching for the given search space for an optimal solution. Also, having an initialization of the initial population with randomly generated candidate solutions is not an effective idea in every case, especially when the search space is large. Hence, we have updated the Levy flights based Multi-verse Optimizer (LFMVO) by dividing initialization into two parts. To investigate the ability of ILFMVO, we have solved a constrained economic dispatch problem with a non-smooth, non-convex cost functions of three, six, and twenty thermal generator systems and two design engineering problems with nonlinear objectives and complex nonlinear constraints. We have compared our results with other standard algorithms. We have presented the sensitivity analysis to check the robustness and stability of our approach. The outcome demonstrated that ILFMVO has better accuracy, stability, and convergence.																	1064-1246	1875-8967					2020	39	1					1	17		10.3233/JIFS-190112													
J								Computational aspects of line simplicial complexes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Euler characteristic; Betti number; facet ideal; connected simplicial complex; shellability		LetGbe a finite simple graph. The line graph L(G) represents adjacencies between edges ofG. We define first line simplicial complex Delta(L) (G) of G containing Gallai and anti-Gallai simplicial complexes Delta(Gamma)(G) and Delta(Gamma')(G) (respectively) as spanning subcomplexes. We establish the relation between Euler characteristics of line and Gallai simplicial complexes. We prove that the shellability of a line simplicial complex does not hold in general. We give formula for Euler characteristic of line simplicial complex associated to Jahangir graph J(m,n) by presenting an algorithm.																	1064-1246	1875-8967					2020	39	1					35	42		10.3233/JIFS-190369													
J								Labor-management negotiation conflict analysis based on soft preference relation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Conflict analysis; soft set; preference relation; soft preference relation; soft dominance relation	GROUP DECISION-MAKING; SET-THEORY	Conflict analysis plays a prominent role in negotiation during contract-management process in government and industry. The main problem to be solved is how to model conflict situation when there is uncertainty about agreement, disagreement and neutrality among agents in a conflict situation. This paper aims to introduce the novel concepts of the hybridized structures called soft preference relation and soft dominance relation. Further we initiate the approach to handle the labor-management negotiation conflict situation using soft preference and soft dominance relations. Another novelty of the proposed techniques is to classify exactly the agreement, disagreement and neutrality among all the agents in a conflict situation. In addition the proposed techniques can be applied to find the character of all the agents in the conflict situation when compared with other existing techniques.																	1064-1246	1875-8967					2020	39	1					43	52		10.3233/JIFS-190425													
J								A hybrid classification model for churn prediction based on customer clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Customer churn; data mining; hybrid classification; customer clustering	TELECOMMUNICATION SECTOR; NETWORK; OPTIMIZATION; ATTRITION; ALGORITHM	Customer churn prediction is an active research topic for the data mining community and business managers in this rapidly growing society. The ability to detect churn customers precisely is something that every company would wish to achieve. From different experiments on customer churn, it can be seen that customers always could be divided into different types and the customers in the same segment generally have similar personas, behavioral preferences, and focus points. Therefore, a hybrid classification model named ClusGBDT for customer churn prediction is proposed. This model has three steps: a feature transformation stage, a customer clustering stage, and a prediction stage. At first, the multi-layer perceptron is used to training a prediction model and replace the original attributes with low-dimensional vectors. Then, customer segments are divided using K-means. Lastly, the unique prediction model based on GBDT is constructed for every customer segment. Several measures are used to evaluate the prediction performance. From the experiments, it is observed that our design could improve original classification algorithms include GBDT, random forest and logistic regression. Additionally, the proposed framework helps us to comprehend customer data.																	1064-1246	1875-8967					2020	39	1					69	80		10.3233/JIFS-190677													
J								Soft ordered based multi-granulation rough sets and incomplete information system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Rough set; soft set; preference relation; multi-granulation rough sets	FUZZY-SETS; PREFERENCE-RELATION; DECISION-MAKING; DOMINANCE; MODEL; CLASSIFICATION; APPROXIMATION	This study proposes the multi attribute group decision making in the presence of incomplete multi attribute and incomplete multi decision while making a decision with preferences in an incomplete information system. We then consider resolving the problem in an incomplete information system by using two different approximation strategies, that is seeking the common reserving difference and common rejecting difference, four kinds of soft dominance based multi-granulation rough sets namely soft dominance based optimestic multi-granulation rough sets and soft dominance based pessimistic multi-granulation rough sets are presented. Another worth mentioning contibution of this paper is to disclose the ideas of two kinds of approximate precision, rough degree, approximate quality, maximal and minimal rough member ships and their mutual relationships. Finally the validity of these concepts are proved by constructing two algorithms and applying them in solving incomplete multi-agent conflict analysis problem.																	1064-1246	1875-8967					2020	39	1					81	105		10.3233/JIFS-190684													
J								Choquet integrals with respect to the credibility measure	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Choquet integral; credibility measure; symmetric choquet integral; translatable choquet integral	FUZZY; SET	The Choquet integral is proven quite reasonable as an integral form with respect to monotone measures, where the credibility measure is a specific case with self-duality. The main objective of this paper is to propose the Choquet integral of measurable functions on the credibility space, which bridges the gap between the Choquet integral and credibility theory. First, the Choquet integrals for nonnegative functions with respect to the credibility measure are introduced, and their properties are investigated such as the monotonicity and translatability. Then, the symmetric Choquet integrals and translatable Choquet integrals of any measurable functions are developed through the use of the Choquet integrals of nonnegative functions. Finally, Choquet integrals on finite sets based on the credibility measure are presented to simplify the calculation procedures.																	1064-1246	1875-8967					2020	39	1					107	118		10.3233/JIFS-190765													
J								HIBoost: A hubness-aware ensemble learning algorithm for high-dimensional imbalanced data classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hubness; class imbalance; high dimension; SMOTE; Ada Boost	DATA-SETS	Learning from high-dimensional imbalanced data is prevalent in many vital real-world applications, which poses a severe challenge to traditional data mining and machine learning algorithms. The existing works generally use dimension reduction methods to deal with the curse of dimensionality, then apply traditional imbalance learning techniques to combat the problem of class imbalance. However, dimensionality reduction may cause the loss of useful information, especially for the minority classes. This paper introduces an ensemble-based method, HIBoost, to directly handle the imbalanced learning problem in high dimensional space. HIBoost takes into account the inherent high-dimensional hubness phenomenon, i.e., high-dimensional data tends to contain the singular points (hubs and anti-hubs) which frequently or rarely occur in k-nearest neighbors of other points. For the singular hubs and anti-hubs induced by high dimension, HIBoost introduces a discount factor to restrict the weight growth of them in the process of updating weight, so that the risk of over fitting can be reduced when training component classifiers. For class imbalance problem, HIBoost uses SMOTE to balance the training data in each iteration so as to alleviate the prediction bias of component classifiers. Experimental results based on sixteen high-dimensional imbalanced data sets demonstrate the effectiveness of HIBoost.																	1064-1246	1875-8967					2020	39	1					133	144		10.3233/JIFS-190821													
J								Partial similarity measure of uncertain random variables and its application to portfolio selection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Chance theory; uncertain random variable; partial similarity measure; portfolio selection; pattern recognition	VAGUE SETS; DISTANCE; ENTROPY	A similarity measure determines the similarity between two objects. As important roles of similarity measure in chance theory, this paper introduces the concept of partial similarity measure for two uncertain random variables. Based on maximum similarity principle, partial similarity measure are used to recognize pattern problems. As an application in finance, partial similarity measure is applied to optimize portfolio selection of uncertain random returns via Monte-Carlo simulation and craw search algorithm.																	1064-1246	1875-8967					2020	39	1					155	166		10.3233/JIFS-190942													
J								Bipolar fuzzy abundant semigroups with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bipolar fuzzy set; bipolar fuzzy abundant semigroup; good homomorphism; regularity condition; bipolar fuzzy regular semigroup	SETS	The adequate analysis of bipolar information of a semigroup using a fuzzy set requires incorporation of a bipolar fuzzy set and an appropriate semigroup structure. Motivated by studying partial order and lattice of bipolar fuzzy sets, and algebraic framework of bipolar fuzzy sets, in this paper, we introduce the notion of a bipolar fuzzy abundant semigroup by developing a new technique for constructing fuzzy semigroups. After obtaining some properties of bipolar fuzzy abundant semigroups, we give necessary and sufficient conditions of a bipolar fuzzy subset of an abundant semigroup to be bipolar fuzzy abundant. As an application, we extend our results to the case of regular semigroup. In particular, bipolar fuzzy regular semigroups are investigated.																	1064-1246	1875-8967					2020	39	1					167	176		10.3233/JIFS-190951													
J								EEG-based intelligent system for cognitive behavior classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										EEG; intelligent system; IQ; learning style; neural network	ARTIFICIAL NEURAL-NETWORKS; POWER; PERFORMANCE; PREDICTION; ATTENTION; ALPHA; RATIO	Intelligence and learning styles are among most widely studied traits in cognitive psychology. Currently, both aspects of cognition can only be assessed using paper-based psychometric tests. The methods however, are exposed to inconsistency issues due to the variation of examination format and language barriers. Hence, this study proposes an intelligent system for assessing intelligence quotient (IQ) level and learning style from the resting brainwaves using artificial neural network (ANN). Eighty-five individuals from varying educational backgrounds have participated in this study. Resting electroencephalogram (EEG) is recorded from the left prefrontal cortex using NeuroSky. Control groups are established using Kolb's Learning Style Inventory (LSI) and a model developed based on Raven's Progressive Matrices (RPM). Subsequently, theta, alpha and beta power ratio is extracted from the pre-processed EEG. Distribution and pattern of features show a correlation with the Neural Efficiency Hypothesis of intelligence and Alpha Suppression Theory. The power ratio features are then used to train, validate and test the ANN model. The system has demonstrated satisfactory performance for IQ classification with accuracies of 98.3% for training and 94.7% for testing. The proposed model is also able to classify learning style with accuracies of 96.9% for training and 80.0% for testing.																	1064-1246	1875-8967					2020	39	1					177	194		10.3233/JIFS-190955													
J								Intelligent frequency control in microgrid: Fractional order fuzzy PID controller	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy PID controller; fractional order fuzzy PID controller; microgrid; frequency deviation	HYBRID POWER-SYSTEM	Due to integration of different distributed power sources in microgrid, power quality is adversely affected and has caused many control problems. Hence power system requires much more proficiency and adaptability in control and optimization to overcome these problems. The power quality issues in microgrid system are mainly from frequency fluctuations. In real scenario, frequency fluctuations happen because of impulsive variations in load/generation or both. This research study presents a Fractional Order Fuzzy PID (FOFPID) controller for frequency control in microgrid. To test effectiveness of proposed controller, its performance is evaluated and compared with standard PID and Fuzzy PID (FPID) controller. To find optimal parameters of the FOFPID, Gravitational Search Algorithm (GSA) is employed. To illustrate the effectiveness of GSA, its outcome is compared with existing algorithms such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) algorithms. Further performance of each controller and optimizing method is assessed by looking at the fitness function value, statistical data, frequency deviation, amplitude and oscillations of control signal. Finally, the most optimized algorithm-based controller is tested for robustness against parameter variations and nonlinearities like Generation Rate Constraint (GRC).																	1064-1246	1875-8967					2020	39	1					195	212		10.3233/JIFS-190963													
J								A new approach to local H-infinity control for continuous-time T-S fuzzy models	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Takagi-Sugeno fuzzy model; linear matrix inequalities (LMIs); non-quadratic fuzzy Lyapunov function; H-infinity control	LMI-BASED STABILITY; NONLINEAR-SYSTEMS; OBSERVER DESIGN; STABILIZATION	This paper addresses local H-infinity control method to reject the disturbance for continuous-time T-S fuzzy models. Firstly, in order to overcome a few drawbacks of the previous results such as the special structure of free variable, redundant restrictive conditions and parameters, the time derivatives of the membership functions are analyzed and new linear matrix inequalities are obtained. Secondly, the H-infinity control theorem is obtained based on the new conditions. Finally, two examples are given to illustrate the effectiveness of the results.																	1064-1246	1875-8967					2020	39	1					213	220		10.3233/JIFS-190974													
J								A CNN channel pruning low-bit framework using weight quantization with sparse group lasso regularization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network (CNN); weight quantization; sparse group lasso (SGL); alternating direction method of multipliers (ADMM); channel pruning		The deployment of large-scale Convolutional Neural Networks (CNNs) in limited-power devices is hindered by their high computation cost and storage. In this paper, we propose a novel framework for CNNs to simultaneously achieve channel pruning and low-bit quantization by combining weight quantization with Sparse Group Lasso (SGL) regularization. We model this framework as a discretely constrained problem and solve it by Alternating Direction Method of Multipliers (ADMM). Different from previous approaches, the proposed method reduces not only model size but also computational operations. In experimental section, we evaluate the proposed framework on CIFAR datasets with several popular models such as VGG-7/16/19 and ResNet-18/34/50, which demonstrate that the proposed method can obtain low-bit networks and dramatically reduce redundant channels of the network with slight inference accuracy loss. Furthermore, we also visualize and analyze weight tensors, which showing the compact group-sparsity structure of them.																	1064-1246	1875-8967					2020	39	1					221	232		10.3233/JIFS-191014													
J								Extreme learning adaptive neuro-fuzzy inference system model for classifying the epilepsy using Q-Tuned wavelet transform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Epilepsy electroencephalogram (EEG); Q-Tuned wavelet transform (QTWT); approximate entropy (ApEn); extreme learning adaptive neuro-fuzzy inference system model (EXL-ANFIS)	CAROTID-ARTERY WALL; AUTOMATED DIAGNOSIS; CLASSIFICATION; SEIZURES; ENTROPY; DOMAIN	Epilepsy is a nervous disorder that causes arbitrary recurrent seizures within the cerebral cortex region of the encephalon. The early diagnosis of a seizure is important in clinical therapy. An automatic epileptic seizure detection method for electroencephalogram (EEG) signals can significantly enhance the patient's life in clinical aspect. The proposed paper is principally based on a completely unique approach of epileptic seizure detection using Q-TunedWavelet Transform (QTWT) and Approximate entropy (ApEn). This work focuses by utilizing and testing the common sense of Extreme Learning Adaptive Neuro-Fuzzy Inference System Model (EXL-ANFIS) which foresees the elements of the mind states as a trajectory that results in the seizure event. QTWT is used for decomposing EEG signals into sub-band frequency signals. Approximate entropy is carried out to those sub-band signals as a discriminatory function because of its indefinite disordered feature. The solutions obtained by directing towards EXL-ANFIS shows an incredible advancement in the perpetual performance outlay for the classification of an epileptic seizure. The proposed classification method is implemented on publicly available Bonn dataset. The outcome confirms that by combining extreme learning and ANFIS model improves the classification accuracy and decrease the feature dimension with reduced computational complexity. This method achieves 99.72% of classification accuracy over existing models.																	1064-1246	1875-8967					2020	39	1					233	248		10.3233/JIFS-191015													
J								Novel JavaScript malware detection based on fuzzy Petri nets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy reasoning; JavaScript malware detection; high-level fuzzy Petri net; cyber security	ATTACKS	Currently, JavaScript is a popular scripting language for building web pages. It allows website creators to run any program code they want when users are visiting their websites. Meanwhile, malicious JavaScript becomes one of the biggest threats in the cyber world. Researchers are now searching for a convenient and effective way to detect JavaScript malware. Consequently, this paper aims to propose a novel method of detecting the JavaScript malware by using a high-level fuzzy Petri net (HLFPN). First, the web pages are crawled to get JavaScript files. Second, those main features are extracted from JavaScript files. In total, six main features of the JavaScript, including longest word size, entropy, specific character, commenting style, function calls, and abstract syntax tree (AST) features are collected. Finally, an HLFPN model is used to determine whether the malicious code is available or not. The experimental results have fully demonstrated the effectiveness of our proposed approach.																	1064-1246	1875-8967					2020	39	1					249	261		10.3233/JIFS-191038													
J								On total uniform fuzzy soft graphs	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy soft graph; regular fuzzy soft graph; totally regular fuzzy soft graph; degree of a vertex; total degree of a vertex; complement fuzzy soft graph	OPERATIONS	In this paper, we introduce the concepts of total uniform vertex fuzzy soft graphs and total uniform edge fuzzy soft graphs. In the view of this concept, we study the degree of a vertex, the total degree of a vertex and the complement fuzzy soft graphs. Also, we prove our main results about regular and totally regular fuzzy soft graphs, and the conditions under which the complement of regular fuzzy soft graph becomes regular as well as totally regular fuzzy soft graphs. We also describe applications of fuzzy soft graphs in telecommunication network.																	1064-1246	1875-8967					2020	39	1					263	275		10.3233/JIFS-191058													
J								Fuzzy neutrosophic soft sigma-algebra and fuzzy neutrosophic soft measure with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										fns sigma-algebra; fns-measure; fns outer measure; MCGDM	ROUGH SET MODELS; DECISION-MAKING; MATRIX-THEORY; TOPOLOGY; TOPSIS; (I	The inspiration behind this article is to introduce the notions of fuzzy neutrosophic soft sigma-algebra (fns sigma-algebra), fuzzy neutrosophic soft measure (fns measure) and fns outer measure using the concepts of fuzzy sets, soft sets, neutrosophic sets and soft sigma-algebra. Anumber of related results along with elaborative examples are also included. We render an algorithm based upon fns-mapping to deal with imprecise data utilizing mean proportional operator and employ it on multi-criteria group decision making (MCGDM) problem to exhibit its efficacy.																	1064-1246	1875-8967					2020	39	1					277	287		10.3233/JIFS-191062													
J								Energy-efficient bi-objective manufacturing scheduling with intermediate buffers using a three-stage genetic algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Flexible multi-task scheduling; Energy consumption; Intermediate buffer; Bi-objective optimization; Pareto; Genetic algorithm	SHOP; CONSUMPTION; SEARCH	The flexible multi-task scheduling problem has been extensively investigated in manufacturing systems, and its objectives are often related to the quality of manufacturing services. However, energy-related objectives along with workload balance have rarely been considered. Thus, a novel bi-objective optimization model is proposed to achieve green manufacturing. The Pareto-based fitness evaluation is employed to make a trade-off between total energy consumption and workload balance. Intermediate buffers are also considered, making the model more practical and more complicated. To solve the proposed model, a new three-stage genetic algorithm (3S-GA) is presented. A Pareto-based adaptive population size method is proposed to maintain the diversity of the population and ensure the convergence rate. To cope with the subtask sequencing complexity, a real-time sequence scheduling heuristic is explored to effectively initialize the subtask sequence to save the energy in manufacturing systems, which is designed by minimizing the standby time according to the laxity of subtasks. After a series of experimental designs based on the Taguchi method, a suitable parameter combination of the 3S-GA is utilized. Further, computational experiments based on five instances demonstrate that the 3S-GA outperforms other four baseline algorithms taken from the literature in solving the proposed bi-objective optimization model.																	1064-1246	1875-8967					2020	39	1					289	304		10.3233/JIFS-191072													
J								Adaptive multilevel thresholding based on multiobjective artificial bee colony optimization for noisy image segmentation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image segmentation; multi-objective optimization; artificial bee colony; multilevel thresholding; interval-valued fuzzy information	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHM; ENTROPY; CHAOS	Multilevel thresholding is one of the effective image segmentation methods. However, it faces three big challenges: (1) howto adaptively determine the number of multiple thresholds; (2) howto overcome the sensitivity to image noise; (3) how to perform multilevel thresholding under several segmentation requirements. In order to solve these problems, an adaptive multilevel thresholding algorithm based on multiobjective artificial bee colony optimization (AMT-MABCO) segmentation is presented for noisy image in this paper. To improve the robustness of AMT-MABCO to image noise, a line intercept histogram which considers both the intensity and coordinate information in the neighborhood of the pixels is firstly utilized to define a novel between-class variance function as one fitness function. Then, an interval-valued fuzzy entropy function is constructed as another fitness function to deal with the blurred characteristic in images. AMT-MABCO tries to obtain a compromising multilevel thresholding result under these two segmentation requirements. To adaptively determine the number of thresholds, a grouping population initialization and evaluation strategies are proposed in AMT-MABCO. Furthermore, two novel search equations are constructed in AMT-MABCO to generate candidate solutions in the employed bees and onlookers phases, respectively. Experimental results show that AMT-MABCO outperforms state-of-the-art thresholding methods in noise robustness and segmentation performance.																	1064-1246	1875-8967					2020	39	1					305	323		10.3233/JIFS-191083													
J								Some preference involved aggregation models for basic uncertain information using uncertainty transformation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Aggregation function; BUI aggregation; decision making; evaluation; OWA operators; uncertain decision making	WEIGHTED AVERAGING AGGREGATION; INTEGRALS; OPERATORS	In decision making, very often the data collected are with different extents of uncertainty. The recently intro-duced concept, Basic Uncertain Information (BUI), serves as one ideal information representation to well model involved uncertainties with different extents. This study discusses some methods of BUI aggregation by proposing some uncertainty transformations for them. Based on some previously obtained results, we at first define IOWA operator with poset valued input vector and inducing vector. The work then defines the concept of uncertain system, on which we can further introduce the multi-layer uncertainty transformation for BUI. Subsequently, we formally introduce MUT IOWA aggregation procedure, which has good potential to more and wider application areas. A numerical example is also offered along with some simple usage of it in decision making.																	1064-1246	1875-8967					2020	39	1					325	332		10.3233/JIFS-191106													
J								Attention-based LSTM, GRU and CNN for short text classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Long short term memory; gated recurrent unit; convolutional neural network; attention mechanism; text classification		Text classification is a fundamental task in Nature Language Processing(NLP). However, with the challenge of complex semantic information, how to extract useful features becomes a critical issue. Different from other traditional meth-ods, we propose a new model based on two parallel RNNs architecture, which captures context information through LSTM and GRU respectively and simultaneously. Motivated by the siamese network, our proposed architecture generates attention matrix through calculating similarity between the parallel captured context information, which ensures the effectiveness of extracted features and further improves classification results. We evaluate our proposed model on six text classification tasks. The result of experiments shows that the ABLGCNN model proposed in this paper has the faster convergence speed and the higher precision than other models.																	1064-1246	1875-8967					2020	39	1					333	340		10.3233/JIFS-191171													
J								An efficient author information retrieval tool for bibliographic record analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Author; consistency; contribution factor; cooperativeness; DBLP; graph; publication stability; solidity		Digital Bibliography and Library Project dataset is a collection of bibliographic records of computer science publications of various authors and co-authors. It contains approximately 1.5 million bibliographic records. An algorithm for an author's information retrieval is developed to retrieve details of specific author publications and correlation among authors. Further performance of an author is measured with parameters like consistency, contribution factor, stability, cooperativeness, and solidity. The work presented is tested on the DBLP dataset. Experimental results clearly support the claim that it works efficiently for retrieving specific author-publication records and its analysis with respect to suggested parameters.																	1064-1246	1875-8967					2020	39	1					341	353		10.3233/JIFS-191289													
J								Progressive image painting outside the box with edge domain	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Outpainting; edge detector; generative adversarial network; focal loss		Motivated by a widely studied computer vision task: image inpainting, we became interested in a less concerned problem image outpainting. By which, contents beyond the image boundaries may be extrapolated. In recent years, deep learning methods have achieved remarkable improvements in image inpainting, these techniques can be considered to be applied to image outpainting as solutions. However, many of these inpainting methods generate image blocks generally resulting in blur or smooth. Recently, hallucinating edges for the missing holes before completion has been proved to be a state-of-the-art image inpainting method. Refer to the aforementioned method, we propose a three-phase outpainting model that consists of an edge generation phase, an image expansion phase and a refinement phase. In order to depict the edge lines more accurately, we adopt a comparatively effective focal loss for edge prediction. An optimization stage with a refinement network is also added since large portions outside the image need to be inferred, and discriminator in this stage works on a decreased patch size with a coarse-to-fine fashion. In addition, with recursive outpainting, an image could be expanded arbitrarily. Experiments show that an image can be effectively expanded by our method, and our outpainting method of predicting edges and then coloring is generally superior to other methods both quantitatively and qualitatively.																	1064-1246	1875-8967					2020	39	1					371	381		10.3233/JIFS-191310													
J								Optimization of open channels using particle swarm optimization algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Particle swarm optimization; open channels; dimension	OPTIMAL-DESIGN	Due to recent increasing water demand, planning and projecting of water resources has resulted in increased costs. Therefore, it is important to obtain optimum results in project planning. In this study, dimensioning of open channels used especially for irrigation purposes has been studied using a particle swarm optimization algorithm to investigate optimum base width, channel height, and slope angles. The results are summarized in graphs and tables. In the study, it was found that the optimum slope angle varied between 0.2(0) (boolean OR) 0.45(0) . Furthermore, it was found that increasing the slope angle significantly increased costs. Finally, the increase in flow increases costs but the rate of increase diminishes.																	1064-1246	1875-8967					2020	39	1					399	405		10.3233/JIFS-191355													
J								Detection of probe flow anomalies using information entropy and random forest method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Power system; flow detection; network probe; random forest algorithms	PMU DATA; MODELS	Aiming at the problems of excessive dependence on manual work, low detection accuracy and poor real-time performance of current probe flow anomaly detection in power system network security detection, a detection method for calculating information entropy of probe flow and random forest classification is proposed. Firstly, the network probe stream data are captured and aggregated in real-time to extract network stream metadata. Secondly, by calculating Pearson correlation coefficient and maximum mutual information coefficient, feature selection of network metadata is carried out. Finally, the information entropy and stochastic forest algorithm are combined to detect the anomaly of probe traffic based on the selected key feature groups, and the probe traffic is accurately classified by multiple incremental learning. The results show that the proposed method can quickly locate the abnormal position of probe traffic and analyze the abnormal points, which greatly reduces the workload of application platform for power system security monitoring, and has high detection accuracy. It effectively improves the reliability and early warning ability of power system network security.																	1064-1246	1875-8967					2020	39	1					433	447		10.3233/JIFS-191448													
J								Filter theory of pseudo equality algebras	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pseudo equality algebra; prelinear pseudo equality algebra; divisible pseudo equality algebra; (positive) implicative filter; fantastic filter; strong normal filter		In this paper, we study the generating formula of filters in pseudo equality algebras, also we introduce prelinear pseudo equality algebras and divisible pseudo equality algebras, and then we investigate some characterizations of them. We focus on algebraic structures of the set F(X) of all filters in pseudo equality algebras and obtain that F(X) can form a Heyting algebra. Moreover, we give the notions of some types of filters ((positive) implicative filters, fantastic filters) in pseudo equality algebras and investigate their properties. Finally, we discuss the relations among these filters.																	1064-1246	1875-8967					2020	39	1					475	487		10.3233/JIFS-191512													
J								Edge and region segmentation in high-resolution aerial images using improved kernel density estimation: A hybrid approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Kernel density estimation; anisotropic diffusion; fractal dimension; aerial image; image smoothing; segmentation; edge detection	ANISOTROPIC DIFFUSION	Edge detection and segmentation are the two main approaches being used since last three decades for successful image analysis in remote sensing domain. Although many intensive studies were undertaken, they all were limited to high-resolution aerial images and none addressed this problem exhaustively. The purpose of this study was to investigate both edge detection and segmentation by employing a novel hybrid method combining probability density function and partial differential equation to obtain accurate estimations. The newly proposed method is implemented in two phases: the first phase deals with smoothening that include improved kernel density estimation (KDE) with anisotropic diffusion coefficient function kernel with both adaptive bandwidth and constant threshold selection using Shannon entropy, in addition to a weighting parameter of 3 x 3 window for lower probability of the whole image in diffusion function; whereas in the second phase, edge detection and segmentation are dealt with by incorporating two prominent techniques, namely diffusion coefficient equation and six-sigma control limit. We carried out a cross-sectional analysis using different datasets such as SIPI database and ground truth images for smoothing, edging and segmentation. Afterward, the results were compared with the other state-of-the-art techniques. Finally, the performance measures of the implemented technique were evaluated by means of entropy, fractal dimension, and an equivalent number of looks for smoothened images, by the Pratt metric for edge detection, and in the case of segmentation, misclassification error was considered. The experimental results demonstrated that the proposed scheme outperforms its counterparts in all aspects. Hence, the proposed hybrid scheme is better and robust, and results in accurate estimation for the given datasets.																	1064-1246	1875-8967					2020	39	1					543	560		10.3233/JIFS-191547													
J								A novel approach to multi-attribute group decision-making based on q-rung orthopair fuzzy power dual Muirhead mean operators and novel score function	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										q-rung orthopair fuzzy set; power geometric operator; dual muirhead mean; q-rung orthopair fuzzy power dual; muirhead mean; novel score function; multi-attribute group decision-making	AGGREGATION OPERATORS; SIMILARITY MEASURE; SETS	The recently proposed q-rung orthopair fuzzy sets (q-ROFSs) have been proved to be an effective tool to describe decision makers' evaluation information and this paper attempts to propose a new multi-attribute group decision-making (MAGDM) method with q-rung orthopair fuzzy information. First of all, we propose a new score function of q-rung orthopair fuzzy numbers (q-ROFNs) by taking the hesitancy degree into account. When considering to fuse q-ROFNs, this paper tries to propose some novel aggregation operators. The power geometric (PG) operator has the ability of reducing or eliminating the bad influence of decision makers' unreasonable assessments on final decision results. Hence, we extend PG to q-ROFSs and propose the q-ROF power geometric operator and its weighted form. The most prominent advantage of dual Muirhead mean (DMM) is that it can capture the interrelationships among any numbers of input arguments. To take full advantages of PG and DMM, we further combine PG with DMM within q-rung orthopair fuzzy environment and propose the q-rung orthopair fuzzy power dual Muirhead mean, and q-rung orthopair fuzzy weighted power dual Muirhead mean operators. The proposed operators can reduce the negative effects of unreasonable evaluations on the decision results, and simultaneously take the interrelationship among any numbers of input arguments into account. In addition, we propose a new MAGDM method based on the proposed aggregation operators. Finally, we provide numerical examples to demonstrate the validity and merits of the proposed method.																	1064-1246	1875-8967					2020	39	1					561	580		10.3233/JIFS-191552													
J								Two lambda-correlation coefficients of q-rung orthopair fuzzy sets and their application to clustering analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										q-rung orthopair fuzzy set; correlation coefficient; clustering analysis	BONFERRONI OPERATORS; SIMILARITY MEASURES	The q-rung orthopair fuzzy set is a significant part of the existing orthopair fuzzy sets, whose advantage is to more comprehensively describe uncertain information. For q-rung orthopair fuzzy sets, the correlation between them is generally measured by the correlation coefficient. In order to express the positive and negative correlations of q-rung orthopair fuzzy sets simultaneously from a statistical perspective, and to reflect the attitude of decision makers, in this paper, two new correlation coefficients of q-rung orthopair fuzzy sets are proposed and investigated. Firstly, a lambda-variance-based correlation coefficient of q-rung orthopair fuzzy sets is proposed from the statistical viewpoint. Secondly, a lambda-matching-function-based correlation coefficient of q-rung orthopair fuzzy sets is defined from the perspective of vector calculation. In the end, an example of clustering analysis is presented to verify the feasibility and superiority of the proposed correlation coefficients by comparing with other existing correlation coefficient of q-rung orthopair fuzzy sets. It can be seen from the clustering results that the two new lambda-correlation coefficients not only consider the positive or negative correlation at the same time, but also can be dynamically adjusted according to the needs of decision makers. Furthermore, clustering results using lambda-variance-based and lambda-matching-function-based correlation coefficients converge faster than clustering results using the existing correlation coefficient in the q-rung orthopair fuzzy environment.																	1064-1246	1875-8967					2020	39	1					581	591		10.3233/JIFS-191553													
J								Weighted minimax programming subject to the two-sides fuzzy relation inequalities with max-product composition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Two-sides fuzzy relation inequalities; max-product composition; weighted minimax programming; solution matrix approach; nonlinear optimization	LINEAR OBJECTIVE FUNCTION; DUBOIS-PRADE FAMILY; RELATION EQUATIONS; OPTIMIZATION; MIN; ALGORITHM; SYSTEM; CONSTRAINTS; RESOLUTION	Considering the application in wireless communication basic-station (terminal) system, we investigate the weighted minimax programming subject to two-sides fuzzy relation inequalities with max-product composition in this paper. By establishing the maximum solution and the discrimination matrix of the inequalities system, we give the sufficient and necessary condition that the inequalities system is consistent and further obtain the structure of the solution set. We develop a solution matrix approach method for solving the proposed problem and further develop a step-by-step algorithm for carrying out the method. The theory analysis and numerical example indicate that the algorithm is feasible and efficient.																	1064-1246	1875-8967					2020	39	1					593	605		10.3233/JIFS-191565													
J								Bench calibration method for automotive electric motors based on deep reinforcement learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interior permanent magnet synchronous motor; deep reinforcement learning; bench calibration; optimal control; optimal efficiency	MAGNET SYNCHRONOUS MOTOR; ADAPTIVE OPTIMAL-CONTROL; TORQUE-PER-AMPERE; MAXIMUM-TORQUE; MTPA CONTROL; IPMSM; CONTROLLER; DRIVES; MODEL; IMPLEMENTATION	The efficiency and control accuracy of Interior Permanent Magnet Synchronous Motor (IPMSM) are the main factors affecting performance. Manual calibration has the disadvantage of high work intensity, long calibration period and high technical requirement, which leads to low calibration accuracy and motor efficiency. Thus, a novel calibration method based on Deep Deterministic Policy Gradient (DDPG) and Long Short-Term Memory (LSTM) is proposed. By constructing a deep reinforcement learning network, the self-optimization of the optimal working point under any working condition is realized, and the MAP for IPMSM in full speed-torque range is obtained. The method can be used to quickly realize the optimal matching of d-q axis current with arbitrary stator current. It focuses on solving the problem of motor overheating caused by long adjustment time of manually calibrated MAP when the motor is overloaded, to realize fast calibration in overload area. Moreover, the method reduces the dependence on the motor parameters and increases the adaptability of the calibration MAP data to the operating conditions. The simulation and bench test indicate that the method can meet the response requirements of motor torque, and results reveal that the motor efficiency is greatly improved.																	1064-1246	1875-8967					2020	39	1					607	626		10.3233/JIFS-191567													
J								Ordering based on uni-nullnorms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uninorm; bounded lattice; partial order; uni-nullnorm	SEMI-T-OPERATORS; UNINORMS; EQUIVALENCE; DISTRIBUTIVITY; NORMS	In this paper, orders based on uni-nullnorms on bounded lattices are introduced and discussed. By this way, the existing orders in the literature induced by t-norms, t-conorms, uninorms and nullnorms are extended to much more general form. The relationships between the orders induced by uni-nullnorms and the orders induced by their underlying t-norms, t-conorms, uninorms and nullnorms are presented. A necessary and sufficient condition making a bounded lattice again a lattice with respect to the orders based on uni-nullnorms is given. Also, the relationships between the partially ordered sets based on the orders induced by t-norms and induced by their N-dual t-conorms and conjugate t-norms, which are special uni-nullnorms, are investigated.																	1064-1246	1875-8967					2020	39	1					645	663		10.3233/JIFS-191583													
J								Gaussian kernel fuzzy rough based attribute reduction: An acceleration approach	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Acceleration strategy; attribute reduction; fuzzy dependency; fuzzy rough set; Gaussian kernel	FEATURE-SELECTION; SET-THEORY; MODEL; APPROXIMATIONS; UNCERTAINTY	Presently, the Gaussian kernel approach has been widely accepted for measuring the similarities among samples and then constructing various fuzzy rough sets. Notably, the considered parameter plays a crucial role in deriving Gaussian kernel based similarities. This is mainly because different parameters will generate different scales of the similarities. From this point of view, different parameters may result in different fuzzy rough approximations and the corresponding reducts. Generally speaking, to search a parameterized reduct with better generalization performance, a naive approach can be designed by repeating the process of computing reduct through using different parameters. Obviously, it is very time-consuming. To fill such a gap, an acceleration approach is proposed which aims to reduce the elapsed time of searching reducts based on different parameters. The main mechanism of our proposed approach is to take the variation of the used parameters into account, and then the process of finding reduct under current parameter can be realized based on the previous parameter related reduct. The experimental results over 16 UCI data sets, which are obtained by testing different Gaussian kernel based fuzzy rough sets, demonstrate that our proposed acceleration strategy not only can significantly reduce the time consumption of finding reducts in terms of different parameters, but also will not lead to poorer classification performance and significant variation of length of the obtained reducts by comparing with the results obtained by the naive process. This study suggests technical support for quickly finding reducts of parameterized fuzzy rough sets.																	1064-1246	1875-8967					2020	39	1					679	695		10.3233/JIFS-191633													
J								Humor identification using affect based content in target text	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Humor identification; affective computing; natural language processing; machine learning	CONTENT REPRESENTATION	Human beings often use figurative language during communication to express their thoughts. Uncovering the meaning out of figurative language is not as simple as literal language. Humor identification is considered to be an important linguistic device for sentiment analysis of figurative text because it can often change the sentiments of the text. Moreover, during verbal communication people use facial expressions, gestures and other modalities to convey their feeling and to automatically understand the meaning out of figurative sentences using these modalities is part of computer vision and digital image processing. It is difficult for written sentences where facial expressions, gestures, other modalities, and emotions are absent and so it is an interesting question of research. Humor is a figurative device and a creative linguistic phenomenon. To understand the meaning of humor, we need to correctly understand the mood and emotions conveyed in the text, which is beyond the semantics of literal language communication. In this work, we have addressed these issues of understanding the emotions using affect-based information from text with various well established machine learning classifiers. We have exploited various affective content that inhibits the emotions and feeling of a writer such as emoticons, writing styles like punctuation, capitalization, sentiment words and so on. The proposed affect-based humor identification model is evaluated on the SemEval 2017 HashTagWars dataset and yelp review dataset with different types of the experimental configuration. This evaluates the effectiveness of the proposed humor identification model with different types of features.																	1064-1246	1875-8967					2020	39	1					697	708		10.3233/JIFS-191648													
J								Interpolative fuzzy reasoning method based on the incircle of a generalized triangular fuzzy number	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy interpolative reasoning; sparse fuzzy rule-based systems; incircle triangular fuzzy numbers; incircle FRI method	RULE INTERPOLATION; SYSTEMS	Fuzzy Rule Interpolation (FRI) is an important technique for implementing inference with sparse fuzzy rulebases. Even if a given observation has no overlap with the antecedent of any rule from the rule-base, FRI may still conclude a conclusion. This paper introduces a new method called "Incircle FRI" for fuzzy interpolation which is based on the incircle of a triangular fuzzy number. The suggested method is defined for triangular CNF fuzzy sets, for a single antecedent universe and two surrounding rules from the rule-base. The paper also extends the suggested "Incircle FRI" to trapezoidal, and hexagonal shaped fuzzy sets by decomposing their shapes to multiple triangulars. The generated conclusion is also a CNF fuzzy set. The performance of the suggested method is evaluated based on numerical examples and a comprehensive comparison to other current FRI methods.																	1064-1246	1875-8967					2020	39	1					709	729		10.3233/JIFS-191660													
J								Renyi's-Tsallis fuzzy divergence measure and its applications to pattern recognition and fault detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Renyi's-Tsallis divergence measure; convex function; fuzzy set; fuzzy divergence measure; pattern recognition; fault detection		In this communication, we have characterized the sum of two general measures associated with two distributions with discrete random variables as well as fuzzy sets. One of these measures is logarithmic, while other contains the power of variables, named as joint representation of Renyi's-Tsallis divergence measure which implies that the proposed measure is equal to the constant time the sum of Renyi's and Tsallis divergence measure. Besides the validation of the proposed measures, some of its major properties are also discussed for probability distributions and fuzzy sets. The performance of the proposed measure is contrasted with other existing measures in the literature. Some illustrative examples are solved in the context of pattern recognition and fault detection problem which demonstrate the practicality and adequacy of measure between fuzzy sets.																	1064-1246	1875-8967					2020	39	1					731	742		10.3233/JIFS-191689													
J								Topic modeling in short-text using non-negative matrix factorization based on deep reinforcement learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Topic modeling; knowledge discovery; short text; non-negative matrix factorization; machine learning		Topic modeling for short texts is a challenging and interesting problem in the machine learning and knowledge discovery domains. Nowadays, millions of documents published on the internet from various sources. Internet websites are full of various topics and information, but there is a lot of similarity between topics, contents, and total quality of sources, which causes data repetition and gives the user the same information. Another issue is data sparsity and ambiguity because the length of the short text is limited, which causes unsatisfactory results and give irrelevant results to end-users. All these mentioned issues in short texts made an interesting topic for researchers to use machine learning and knowledge discovery techniques to discover underlying topics from a massive amount of data. In this paper, we propose a combination of deep reinforcement learning (RL) and semantics-assisted non-negative matrix factorization model to extract meaningful and underlying topics from short document contents. The main objective of this work is to reduce the problem of repetitive information and data sparsity in short texts to help the users to get meaningful and relevant contents. Furthermore, our propose model reviews an issue of the Seq2Seq approach based on the reinforcement learning perspective and provides a combination of reinforcement learning and SeaNMF formulation using the block coordinate descent algorithm. Moreover, we compare different real-world datasets by using numerical calculation and present a couple of state-of-art models to get better performance on short text document topic modeling. Based on experimental results and comparative analysis, our propose model outperforms the state of art techniques in terms of short document topic modeling.																	1064-1246	1875-8967					2020	39	1					743	760		10.3233/JIFS-191690													
J								A note on a method for constructing uniform topologies on algebras associated with logical systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Algebraic structure; Bl-algebra; uniform structure; partition space; partition uniformity	FUZZY FILTERS; BL	In the recent years, many authors have used a single method for equipping algebraic structures with uniformities which are induced by families of algebraic objects. This paper is devoted to a description of this well-known method in general, and provides insight into those results which are obtained using the method. In fact, we prove that the uniform topology induced by this method coincides with a partition topology generated by an equivalence relation, and illustrate the logic behind the continuity of algebraic operations in these kinds of uniform topologies. Furthermore, the main topological properties of the partition topology induced by a congruence relation are presented. As an application, we explain why many results obtained from this method are trivial. These results have been collected from the works of several mathematicians on more than twenty different algebraic systems over the course of two decades.																	1064-1246	1875-8967					2020	39	1					789	793		10.3233/JIFS-191709													
J								A content recommendation system for effective e-learning using embedded feature selection and fuzzy DT based CNN	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Classification; deep learning; feature selection (FS); fuzzy logic; weighted genetic algorithm (WGA)	TEXT CLASSIFICATION; NETWORK; MODEL	This paper proposes a new content recommendation system which combines the newly proposed embedded feature selection method and the new Fuzzy Temporal Logic based Decision Tree incorporated Convolutional Neural Network classifier. The newly proposed embedded feature selection called Fuzzy Decision Tree and Weighted Gini-Index based Feature Selection Algorithm (FDTWGI-FSA) that contains the existing incorporated the Fuzzy Decision Tree (FDT) and the Weighted Gini-index based Feature Selection Algorithm (WGIFSA) for getting optimized feature subset. Moreover, an enhanced CNN and Fuzzy Temporal Decision Tree for performing the deep learning process which is able to identify the exact e-content from the huge volume of data with the help of the recommended features by the proposed embedded feature selection method. The exact e-content can be identified after performing the five-layer network structure for extracting the relevant features and it also can be classified by applying the Fuzzy Temporal Decision Tree for the e-learners. Finally, the proposed content recommendation system provides exact content to the e-learners according to their level of understanding and it also satisfies them by providing the exact high level contents. The experiments have been conducted for evaluating the proposed content recommendation system and compared with the existing classifier including the standard CNN.																	1064-1246	1875-8967					2020	39	1					795	808		10.3233/JIFS-191721													
J								Multiple criteria decision making method based on the new similarity measures of Pythagorean fuzzy set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy set; Pythagorean fuzzy set; similarity measure; multiple criteria decision making	SOFT SETS; INFORMATION; EXTENSION; TOPSIS	As an extension of the intuitionistic fuzzy set, the Pythagorean fuzzy set can depict uncertain information more effectively, so it has been well applied in multiple criteria decision making problems. At present, the multiple criteria decision making methods using the Pythagorean fuzzy set are generally ranked based on the aggregation operator or the distance measure, ignoring the important tool of the similarity measure. Therefore, this paper proposes several new similarity measures of the Pythagorean fuzzy set and applies them to multiple criteria decision making problems. Firstly, several new similarity measures of the Pythagorean fuzzy set are proposed, and their properties are discussed. Then, based on the weighted similarity measures, the multiple criteria decision making method is proposed. Finally, the accuracy and reliability of the new similarity measures and the proposed multiple criteria decision making method are verified by the simulation cases.																	1064-1246	1875-8967					2020	39	1					809	820		10.3233/JIFS-191723													
J								Solar power station site selection: A model based on data analysis and MCGDM considering expert consensus	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-criteria group decision-making; site selection; probabilistic linguistic term sets; K-means; bonferroni mean operator; sensitivity analysis	LINGUISTIC TERM SETS; MULTICRITERIA DECISION-MAKING; ENERGY; PLANTS	Solar energy, as a major and least-cost renewable resource, has attracted extensive attention of experts and scholars. However, the establishment of the power station is time-consuming and costly. And once selected, it is difficult to change. So it is crucial to choose the appropriate site of power station. This paper combines data analysis with multi-criteria group decision-making to solve this problem. First of all, K-means clustering method is selected to process the data according to the characteristics of the data. Secondly, the results obtained by K-means method are represented by probabilistic linguistic term sets. Thirdly, Bonferroni Mean operator is used to adjust the weight of the criterion, which considers the consensus among experts. Fourthly, Technique for Order Preference by Similarity to Ideal Solution method is employed to rank the alternatives and select the best one. Finally, sensitivity analysis, comparison analysis and simulation are carried out to further confirm the robustness and advantage of the model. This model can help decision makers to better understand the basic situation of power station sites, make the right decisions, and improve some candidate sites according to the results.																	1064-1246	1875-8967					2020	39	1					821	840		10.3233/JIFS-191739													
J								Moment estimations for parameters in uncertain delay differential equations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain differential equation; parameter estimation; moments method; uncertainty theory	FUZZY NUMBERS; STABILITY	As a type of differential equations driven by Liu process, uncertain delay differential equations (UDDEs) model dynamic systems with after-effects or memories in uncertain environment by incorporating time delay terms. Because it is natural for UDDEs to incorporate some unknown parameters, how to estimate them is a crucial problem in practice. This paper undertakes this issue by applying the method of moments based on discrete observations of solutions. With the Euler difference form of UDDEs, a function with respect to unknown parameters is proved to follow a standard normal uncertainty distribution. The moment estimations for unknown parameters are obtained by solving a system of equations which uses sample moments to approximate population moments. Analytic solutions for some types of UDDEs are derived. Numerical examples show that estimations give small biases and standard deviations as long as time steps are not too large. Applications to population growth models further illustrate the practicability of our method.																	1064-1246	1875-8967					2020	39	1					841	849		10.3233/JIFS-191751													
J								Fermatean fuzzy TOPSIS method with Dombi aggregation operators and its application in multi-criteria decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fermatean fuzzy sets; multi-criteria decision making; TOPSIS; Dombi operations; Fermatean fuzzy aggregation operators	SETS	Algebraic operations are used effectively in decision-making problems. Especially, Dombi, Hamacher and Einstein algebraic operators are used frequently in the decision-making field. On the other hand, it is known that aggregation operators affect the decision-making process in decision-making problems. In this paper, we used Dombi operations to develop some Fermatean fuzzy aggregation operators. Arithmetic and geometric analysis of each aggregation method were performed. We defined the following operators: Fermatean fuzzy Dombi weighted average operator, Fermatean fuzzy Dombi weighted geometric operator, Fermatean fuzzy Dombi ordered weighted average operator, Fermatean fuzzy Dombi ordered weighted geometric operator, Fermatean fuzzy Dombi hybrid weighted average operator, Fermatean fuzzy Dombi hybrid weighted geometric operator. Also, an analysis was performed for the beta value of the Dombi parameter. Properties of proposed operators were presented, and operators were defined on Fermatean fuzzy sets. Finally, proposed operators were compared with the existing aggregation operators. To understand the impact of the proposed operators on the decision-making process, Fermatean fuzzy TOPSIS was established.																	1064-1246	1875-8967					2020	39	1					851	869		10.3233/JIFS-191763													
J								A hybrid two-stage feature selection method based on differential evolution	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Feature selection; cluster validity index; wrapper approach; differential evolution; trial vector generation strategy	PARTICLE SWARM OPTIMIZATION; MUTUAL INFORMATION; ALGORITHM; COLONY; CLASSIFICATION; RELEVANCE; SCHEME	Feature selection is a crucial data pre-processing step in classification problems. The wrapper approach is widely used due to their good classification performance. However, it is very computational expensive due to the cross validation scheme in the evaluation phase. In order to solve this problem, this paper proposes a novel hybrid two-stage feature selection method based on differential evolution (HTSDE). In the first stage, a cluster validity index named DB index is employed to evaluate the feature subset and the wrapper approach in used in the second stage to improve the classification accuracy of the feature subsets. In order to find global optimal feature subsets, different trail vector generation strategies of DE are used in the two stages where the first stage focuses on global exploration and the second stage emphasizes fast convergence. The hybrid method is able to combine the advantages of both DB index and wrapper approach and improve the computational efficiency of the wrapper approach while maintaining the classification performance. HTSDE is compared with several state-of-the-art feature selection methods on 12 datasets. Experimental results show the proposed HTSDE achieves higher classification accuracy than both wrapper and filter approaches. Moreover, its computational cost is much less than those wrapper approaches.																	1064-1246	1875-8967					2020	39	1					871	884		10.3233/JIFS-191765													
J								Development of multilayer fuzzy inference system for diagnosis of renal cancer	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial intelligence; fuzzy inference system; renal cancer	KIDNEY; LOGIC	In this research work, a new multilayer fuzzy inference system is proposed for diagnosis of renal cancer. This proposed automated diagnosis of renal cancer using multilayer Mamdani fuzzy inference system can help to classify the different stages of renal cancer such as no cancer, stage 1, stage 2, stage 3 or stage 4 cancer. This expert system has four input variables at layer 1 and similarly seven input variables at layer 2. At layer 1, the input variables are smoking, dialysis, occupational exposure and genetic or hereditary that recognize the output conditions of renal or kidney to be normal or to have renal cancer. The further input variables for layer 2 are haematuria (blood in urine), red blood cell count, flank pain, tumor size, Von Hippel-Lindau gene, high blood pressure and trichloroethylene exposure that reveal the output condition of kidney such as stage 1 cancer, stage 2 cancer, stage 3 cancer or stage 4 cancer. The novelty in this research work is development of multilayer fuzzy inference system that deals with fuzzy values, uncertain and ambiguous data to detect the stage of renal cancer by using two layers. This paper presents an analysis of results accurately using the proposed expert system to model the renal cancer process with medical expert advice. The confidence indicator for this proposed expert system is 95%.																	1064-1246	1875-8967					2020	39	1					885	898		10.3233/JIFS-191785													
J								A nonparallel support vector machine with pinball loss for pattern classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pattern classification; nonparallel support vector machine; pinball loss; anti-noise	NONNEGATIVE MATRIX FACTORIZATION; CLASSIFIERS	In this paper, we propose a nonparallel support vector machine with pinball loss (Pin-NPSVM) that deals with the noise sensitivity and resampling instability of NPSVM. More specifically, we redefine a pinball loss funtion and build a pair of quantile hyper-planes. Each quantile hyper-plane is constructed by using the new pinball loss instead of epsilon-insensitive loss, which makes the new classification model be insensitive to noise samples, especially for feature noise samples around the decision boundary. Moreover, instead of hinge loss, Pin-NPSVM also builds a pair of decision boundaries based on traditional pinball loss, which further improves the anti-nosie ability of the classification model. In a word, Pin-NPSVM not only inherits the characteristics of the nonparallel optimal hyper-planes, but also has a consistent model with Pin-SVM, which can process noise data well. Finally, numerical experimental results show that the Pin-NPSVM has more obvious advantages than other models in classification performance, especially for noise datasets.																	1064-1246	1875-8967					2020	39	1					911	923		10.3233/JIFS-191845													
J								Automatic segmentation of liver/kidney area with double-layered fuzzy C-means and the utility of hepatorenal index for fatty liver severity classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fatty liver severity classification; Fuzzy c-means; Self-organizing map; Hepatorenal index; Decision tree	DIAGNOSTIC-ACCURACY; ULTRASOUND; DISEASE; EXTRACTION; ULTRASONOGRAPHY; ASSOCIATION; ATTENUATION; PROGRESSION; IMAGES; RISK	BACKGROUND: Hepatorenal index (HRI) has been an efficient and simple quantified measure in distinction between normal and abnormalities of diagnosing fatty liver. However, considering the clinical significance, the diagnosis of severity stage is more important and single HRI cutoff may not be enough. Also, the segmentation of Liver/Kidney area should be automatic to get rid of operator subjectivity from ultrasonography analysis. METHOD: Double-layered Fuzzy C-Means (DFCM) pixel clustering method is proposed to extract the target area of analysis automatically. HRI and other shape related variables of Liver intensity distribution such as the skewness, the kurtosis, and the coefficient of variance (CV) are automatically computed for the fatty liver severity stage classification. RESULT: From fifty ultrasound images obtained from regular health checkup with 24 normal, 12 mild, 11 moderate, 3 severe stage determined by three different radiologists, the proposed DFCM automatically extracts the region of interests(ROI) and generates a set of statistically significant variables including HRI, the skewness, the kurtosis, the coefficient of variance of liver intensity distribution as well as liver echogenicity. In severity stage classification, the echogenicity of the liver and distribution shape variables such as the skewness and the kurtosis are better predictors than HRI based on our simple decision tree learning analysis. CONCLUSION: For better diagnosis of fatty liver severity stages, we need better set of features than the single HRI cutoff. Better machine learning structures are necessary in this severity stage classification problem with automatic segmentation method proposed in this paper.																	1064-1246	1875-8967					2020	39	1					925	936		10.3233/JIFS-191850													
J								Pythagorean fuzzy investment multiple attribute decision making method based on combined aggregation method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean fuzzy set; HM operator; PA operator; multiple attribute decision making; investment selection	HERONIAN MEAN OPERATORS; INFORMATION	The power average (PA) operator can reduce the influence of unreasonable information given by biased decision makers effectively, while Heronian mean (HM) operator can take into account the correlation information between attribute variables in multiple attribute decision making (MADM). Pythagorean fuzzy set (PFS) is a useful tool to handle uncertain information, which has been widely applied in kinds of areas. In order to better infuse the Pythagorean fuzzy evaluation, in this paper we unify the advantages of the PA operator and HM operator, and present the Pythagorean fuzzy power Heronian mean (PFPHM) operator and the Pythagorean fuzzy weighted power Heronian mean (PFWPHM) operator. Some merits of the developed operators are further explored. Furthermore, on the basis of the PFWPHM operator, an approach for MADM under PFS situation is presented. Finally, a numerical case concerning investment company selection is illustrated to demonstrate the availability and feasibility of the developed approach.y																	1064-1246	1875-8967					2020	39	1					949	959		10.3233/JIFS-191905													
J								A real-time traffic environmental perception algorithm fusing stereo vision and deep network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Environmental perception; stereo vision; deep network; multiple object tracking	KALMAN FILTER; TRACKING; INTERPRETABILITY	Object detection and tracking are critical and challenging problems in vehicle environment perception systems, and have received broad attention in recent years. A novel detection and tracking algorithm taking both accuracy and real-time performance into account is proposed in this paper. First, we employ a fusion algorithm based on stereo vision and deep learning in object detection, which achieves high accuracy using two complementary algorithms. Then, a prediction-association algorithm which uses a Kalman filter and Hungarian assignment for multiple object tracking is employed for object tracking. In addition, a detection and tracking framework based on stereo vision improves the robustness of environmental perception system. Experimental results demonstrate that the proposed algorithm has high accuracy and can meet the real-time performance requirement.																	1064-1246	1875-8967					2020	39	1					975	986		10.3233/JIFS-191917													
J								Recognition of human emotions based on user context and brain signals applied to electrical power systems operators evaluation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Emotion recognition; electroencephalography; signal processing; context-awareness	COMPUTER INTERFACES; FEATURE-EXTRACTION; CLASSIFICATION; SELECTION; FEATURES	In this article, we propose a method to recognize human emotions based on user context and brain signals. We evaluated the method through an experiment during which individuals performed tasks using a simulator for electrical power systems operator training. We collected user context through log data retrieval and brain signals using an Electroencephalography (EEG) portable monitor. The experimental results demonstrated that the method could be successfully applied to recognize the emotional states based on EEG signals and user context.																	1064-1246	1875-8967					2020	39	1					987	1003		10.3233/JIFS-191923													
J								Apply new entropy based similarity measures of single valued neutrosophic sets to select supplier material	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Entropy of SVNS; similarity measure of SVNS; MCDM	INTUITIONISTIC FUZZY-SETS; DIVERGENCE MEASURE; MEDICAL DIAGNOSIS; DISTANCE MEASURE; CROSS-ENTROPY; ALGORITHM	The single-valued neutrosophic set (SVNS) is an extension of the fuzzy set and intuitionistic fuzzy set. This is a useful tool to deal with uncertain and inconsistent information. In the information theory, the distance measure, entropy measure and similarity measures have an important role. Several entropy measures of SVNSs have been proposed and applied in many real problems. But they have some restriction in practice and in the academic study. The similarity measures induced from entropy were studied and gave interesting results. In this paper, we introduce a new entropy measure concept based on the SVNS, which overcomes the restriction of existing entropy measures. At the same time, we also investigate some similarity measures which are induced from new entropy measures and apply them to propose the multi-criteria decision making (MCDM) model in selecting the supplier.																	1064-1246	1875-8967					2020	39	1					1005	1019		10.3233/JIFS-191929													
J								Paraphrase identification using collaborative adversarial networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Paraphrase identification; text classification; adversarial networks; LSTM; NLP		The paper presents a Collaborative Adversarial Network (CAN) model for paraphrase identification, which is a collaborative network holding generator that is pitted against an adversarial network called discriminator. There has been tremendous research work and countless examinations done on sentence similarity demonstration. Learning and identifying the constant highlights, specifically in various areas and domains is the main focus of paraphrase identification. It Involves the capture of regular highlights between two sentences and the community-oriented learning upon traditional ill-disposed and adversarial learning for common feature extraction. The model outperforms the MaLSTM model, which is the baseline model, and also proves to be comparable to many of the state-of-the-art techniques.																	1064-1246	1875-8967					2020	39	1					1021	1032		10.3233/JIFS-191933													
J								A neural decoding strategy based on convolutional neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network (CNN); gestures recognition; neural decoding; surface electromyogram (sEMG)	SEMG; HAND; RECOGNITION	Neural decoding is a technology to analyze intentions produced by neural activities, which has important applications in military, medical, entertainment and so on. As a typical application, decoding electromyogram (EMG) signals into corresponding gestures is an important content. In order to improve the accuracy of EMG signals recognition, researchers often extract effective features from EMG signals and classify gestures by constructing a reasonable classifier. However, because of the stochasticity of the signals, this method is not robust enough. This paper proposes a convolutional neural network (CNN) based on feature fusion, which can automatically learn and classify features from time-domain(TD) and frequency-domain(FD). To make full use of information, two fusion methods are used and compared. Experiments show that the proposed fusion methods are superior to the traditional algorithm for both normal people and amputees, and have better performance compared with CNN method using only one kind of information.																	1064-1246	1875-8967					2020	39	1					1033	1044		10.3233/JIFS-191964													
J								Advanced reliability analysis method for mechanisms based on uncertain measure	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertainty quantification; mechanism reliability; reliability index; uncertainty theory; belief reliability	OPTIMIZATION METHOD; TIME	In traditional mechanism reliability analysis, probability theory or statistical approaches are employed. However, these methods cannot be used under lack of data and great epistemic uncertainty. In this paper, an advanced mechanism reliability analysis method is put forward based on uncertain measure. To satisfy the subadditivity of epistemic uncertainties, a novel uncertainty quantification method based on uncertainty theory is proposed for mechanism reliability analysis. Then, a point kinematic reliability analysis method combined with uncertain measure is presented to calculate the kinematic uncertainty reliability of motion mechanism at each time instant. Three models are developed for estimating kinematic uncertainty reliability. Furthermore, first-order Taylor series expansion is used to solve nonlinear limit state functions. A new kinematic uncertainty reliability index (KURI) is presented based on normal uncertainty distribution. Finally, by applying the proposed method to a numerical experiment, the trend of uncertainty reliability was found to be consistent with the traditional method. The two practical engineering applications show that the presented method are more reasonable compared with the classical approaches when the information of design parameters is insufficient.																	1064-1246	1875-8967					2020	39	1					1045	1059		10.3233/JIFS-191970													
J								Swarm intelligence and fuzzy sets for bed exit detection of elderly	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bed exit alarms; elderly care; intelligent swarm; fuzzy sets	FALL DETECTION; MODEL; SYSTEM	Falls in the elderly are a public health problem because this population tends to have a longer recovery time and consequently longer hospital beds. Studies show that 84% of falls in hospital rooms occur near the bed, that led to strategies to prevent falls in the elderly population have been studied. In this context, this paper presents a schema for the detection and emission of bed exit alerts in the elderly. This schema uses signals derived from RFID sensors processed by a model based on Intelligent Swarm and Fuzzy Sets. The main contribution of this study is the use of a Membership Windows that reduces the effects of missclassification of other strategies. The proposed work evaluated a data set containing 14 elderly aged between 66 and 86 years divided into two rooms. The results show that the presented approach improves the precision and recall in environments with greater uncertainty of classification.																	1064-1246	1875-8967					2020	39	1					1061	1072		10.3233/JIFS-191971													
J								On construction of fuzzy chromatic number of cartesian product of path and other fuzzy graphs	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy chromatic number; cartesian product; path; fuzzy graph; algorithm		We use the notion of fuzzy chromatic number (FCN) of fuzzy graphs based on fuzzy independent vertex sets introduced in 2015. Let (G) over tilde (1) be a path fuzzy graph and (G) over tilde (2) be any fuzzy graphs where their vertex sets are disjoint. Let (G) over tilde = (G) over tilde (1) square (G) over tilde (2) be a cartesian product of (G) over tilde (1) and (G) over tilde (2). In this paper, we construct formula for FCN of (G) over tilde (1) square (G) over tilde (2) and verify connection between maximum of FCN of both fuzzy graphs and FCN of their cartesian product. Also, we create an algorithm to determine FCN of the cartesian product according to the properties obtained. The last two statements show novelties of the present work. Evaluation of the algorithm is presented in the experimental results.																	1064-1246	1875-8967					2020	39	1					1073	1080		10.3233/JIFS-191982													
J								Computing the topological descriptors of line graph of the complete m-ary trees	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Topological indices; line graph; complete m-ary trees	ATOM-BOND CONNECTIVITY; WIENER INDEX; ENERGY	The structures of many molecules such as dendrimers, alkanes and acyclic molecules are like trees. Rooted trees have wide applications in chemical graph theory such as enumeration and encoding of chemical structures. Structures of chemical compounds can be systematized in form of chemical and empirical formulae through mathematical means. Chemists have a long tradition of using atomic valences (vertex degrees) to find molecular structures graphically. In structural chemistry number of graph applications exist. This paper reflects the work on the following indices: first general Zagreb index M-alpha, general Randic connectivity index R-alpha, general sum-connectivity index chi(alpha), atom-bond connectivity index ABC, geometric-arithmetic index GA, fourth atom-bond connectivity index ABC(4), fifth geometric-arithmetic index GA(5), hyper-Zagreb index HM(G), first multiple Zagreb index PM1(G), second multiple Zagreb index PM2(G) and Zagreb polynomials M-1(G, x), M-1(G, x) for line graph of complete m-ary tree.																	1064-1246	1875-8967					2020	39	1					1081	1088		10.3233/JIFS-191992													
J								Logical foundation of symmetric implicational methods for fuzzy reasoning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy reasoning; symmetric implicational method; L Pi logic	L-PI; SYSTEMS	The symmetric implicational methods for fuzzy reasoning characterizes the solution B-* (A*) of the formula (A -> (1) B ) -> (2) (A * -> (1) B * ) for the fuzzy modus ponens (fuzzy modus tollens), where ->(1) and ->(2) are two different implications. In this study, we provide a predicate formal representation of the solution for the symmetric implicational methods based on the L Pi formal logic system , including detailed logic proofs. We bring the symmetric implicational methods within a logical framework and provide a sound logic foundation for the symmetric implicational methods of fuzzy reasoning.																	1064-1246	1875-8967					2020	39	1					1089	1095		10.3233/JIFS-191998													
J								Redescending intuitionistic fuzzy clustering to brain magnetic resonance image segmentation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Brain MRI image segmentation; intuitionistic fuzzy C-means; German-McClure redescending M-estimator	C-MEANS ALGORITHM	In brain medical imaging, magnetic resonance is an important and effective means to support the computer aided diagnosis. Notwithstanding, inherent conditions such as atypical information, artifacts and vaguely delimited boundaries between existing tissues can hinder the segmentation task. A popular method to carry out this process is through Fuzzy C-Means algorithm, as well as its variants. These include the Intuitionistic Fuzzy C-Means algorithm, which is found suitable for brain magnetic resonance image segmentation, since it incorporates the advantage of intuitionistic fuzzy sets theory to handle the uncertainty. Most clustering algorithms depend of customized hand-crafted features as well as an appropriate initialization process; this last aspect is a mandatory pre-requisite for convergence of the algorithm. In order to develop the brain image segmentation, in this paper we enhance the Intuitionistic Fuzzy C-Means performance by means of Robust Statistics. Explicitly, a non-parametric German-McClure Redescending M-Estimator is used at the initialization and clustering stages, it behaves such as a robust location estimator when the centroid vector is computed, and as a weighting when the membership matrix is updated. The fusion of both paradigms allows us to propose a clustering algorithm that develops efficiently the segmentation of magnetic resonance images, with the important merit of reduce the iteration required to converge. The robustness and effectiveness of this proposal is verified by experiments on simulated and real brain images.																	1064-1246	1875-8967					2020	39	1					1097	1108		10.3233/JIFS-192005													
J								Bayesian statistical models with uncertainty variables	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bayes' theorem; uncertain variables; uncertain theory; uncertainty Bayesian statistical inference	CONVERGENCE	Bayesian statistical inference is an important method of mathematical statistics in which both sample information and prior information are employed. Traditionally, it is often assumed that the sample observations from the population are observed precisely and characterized by crisp values. However, in many cases, the sample observations are collected in an imprecise way and characterized by uncertain values. In this paper, based on uncertain theory, we propose three kinds of uncertain Bayesian statistical inference including Bayesian point estimation, Bayesian interval estimation and Bayesian hypothesis test. Some numerical examples of uncertain Bayesian inference are presented to illustrate the proposed methods.																	1064-1246	1875-8967					2020	39	1					1109	1117		10.3233/JIFS-192014													
J								The optimized evidence k-Nearest Neighbor based on FOA under the hesitant fuzzy environment and its application in classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										k-Nearest neighbor; dempster-shafer evidence theory; hesitant fuzzy set; fruit fly optimization algorithm	PATTERN-RECOGNITION; ALGORITHM; NETWORK	The k-Nearest Neighbor (k-NN) is one of the simplest intelligent algorithms in the field of pattern recognition and classification. The increasing complexity of practical applications brings more uncertainty and fuzziness. In this paper, we take advantage of the Dempster-Shafer evidence theory (D-S evidence theory) and the hesitant fuzzy set (HFS) in depicting uncertain preference and information, and develop the evidence k-Nearest Neighbor (Ek-NN) under the hesitant fuzzy environment. The fruit fly optimization algorithm (FOA) is adopted to determine the most appropriate value of k in Ek-NN, and a specific implementation process of the optimized Ek-NN based on FOA is also provided. Moreover, two numerical examples about classification problems are presented to evaluate the performance of the proposed method. Comparative analysis and sensitivity analysis are further conducted to illustrate the advantages of the optimized Ek-NN based on FOA under the hesitant fuzzy environment.																	1064-1246	1875-8967					2020	39	1					1119	1129		10.3233/JIFS-192026													
