PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								IT2-based multidimensional evaluation approach to the signaling: investors' priorities for the emerging industries	SOFT COMPUTING										Emerging industries; Fuzzy logic; Interval type 2 fuzzy sets; Hybrid decision-making approach; DANP; MOORA	MODEL COMBINING DANP; DECISION-MAKING; PERFORMANCE; FOUNDERS; FIRMS; SELECTION; IMPACT; INVESTMENT; REPUTATION; PROVIDER	Grounded in the signaling theory, the study aims to develop a model for evaluation of emerging industries. The integrated method is proposed to evaluate the emerging industries with DANP (DEMATEL-based Analytic Network Process) and MOORA (Multi-Objective Optimization on the basis of Ratio Analysis) methodologies based on interval type 2 fuzzy sets, respectively. The application is illustrated by considering cleantech, new generation of information technology, biology/biotechnology and high-end equipment manufacturing industries, three dimensions and 12 criteria. The novelties of the study lie in a set of criteria and alternatives for the signaling in emerging industries supported by the literature and suggest a hybrid decision-making model based on the type 2 fuzzy sets. The signaling determinants for emerging industries could be evaluated by the proposed interval type 2 hybrid decision-making approach more accurately. The results demonstrate that the firms, operating in emerging industries at the early stage of their development, have to put emphasis on the third-party endorsements and human capital criteria, aiming to attract external investors. The suggested method lets the external investors narrow their options in selecting emerging industries and select the most attractive industries or spread investments among the most attractive industries.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13517	13534		10.1007/s00500-019-04288-6													
J								An ATLD-ALS method for the trilinear decomposition of large third-order tensors	SOFT COMPUTING										Computational efficiency; Convergence speed; CP model; Degeneracy; Polyadic form	CANONICAL POLYADIC DECOMPOSITION; 2ND-ORDER CALIBRATION; MULTIWAY ALGORITHMS; PRINCIPAL COMPONENT; PART II; UNIQUENESS; EFFICIENT; NUMBER; SPEED	CP decomposition of large third-order tensors can be computationally challenging. Parameters are typically estimated by means of the ALS procedure because it yields least-squares solutions and provides consistent outcomes. Nevertheless, ALS presents two major flaws which are particularly problematic for large-scale problems: slow convergence and sensitiveness to degeneracy conditions such as over-factoring, collinearity, bad initialization and local minima. More efficient algorithms have been proposed in the literature. They are, however, much less dependable than ALS in delivering stable results because the increased speed often comes at the expense of accuracy. In particular, the ATLD procedure is one of the fastest alternatives, but it is hardly employed because of the unreliable nature of its convergence. As a solution, multi-optimization is proposed. ATLD and ALS steps are concatenated in an integrated procedure with the purpose of increasing efficiency without a significant loss in precision. This methodology has been implemented and tested under realistic conditions on simulated data sets.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13535	13546		10.1007/s00500-019-04320-9													
J								Seasonality in crude oil returns	SOFT COMPUTING										Crude oil returns; Month-of-the-year effect; Day-of-the-week effect; Efficient market hypothesis	PRICE SHOCKS; STOCK-MARKET; ANOMALIES; US; PERSISTENCE; VOLATILITY; REAL; TIME	This paper explores the efficiency issue of the oil market in order to test the seasonal behavior of oil price returns, specifically day-of-the-week effect and month-of-the-year effect for the period December 1987 to January 2016. We use a dummy variable regression estimation technique to test seasonal anomalies for the Brent and WTI crude oil returns. Our empirical results find support for the negative Monday effect. The evidence of negative Monday returns is consistent with the relevant empirical literature. Moreover, the returns on Thursday are highest in a week followed by returns on Friday for both oil markets. This study also found evidence on month-of-the-year effect as the negative returns in November and December for Brent and WTI oil markets. Finally, this study is important for energy researchers, market participants, and policy-makers because anomalous oil markets' behavior implies return predictability and the implementation of profitable investment strategies by market players and may also impact the macroeconomic variables and stock market returns.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13547	13556		10.1007/s00500-019-04329-0													
J								Robust distance measure to detect outliers for categorical data	SOFT COMPUTING										Categorical data; Distance measure; Agglomerative linkage; Outliers	2-WAY CONTINGENCY-TABLES; OUTLYING CELLS; IDENTIFICATION	Distance-based techniques in detecting outliers appears to be an effective tool in both univariate and multivariate data. However, the effectiveness of the same is yet to be firmly established in categorical data as it poses challenges due to polarization of cell frequencies. The purpose of this paper is to evolve a new distance-based measure to detect outliers in two-dimensional contingency tables. The new distance measure based on pivotal element is evaluated through a comparison with other suitable distance measures from the literature for its performance. The consistency of the four distance measures is examined through a simulation study followed by the application to real datasets.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13557	13564		10.1007/s00500-019-04340-5													
J								Quantification of qualitative assessments using computing with words: in framework of fuzzy set theory	SOFT COMPUTING											LINGUISTIC REPRESENTATION MODEL	When qualitative data are collected, which is a widespread situation encountered in several disciplines ranging from social sciences to decision making to engineering as a result of dealing with ill-defined concepts or with complex tasks to assess, there is a certain need to crunch them and infer from them. Hence, quantification, namely computing with words (CW), emerges as a research area. Even though social science disciplines deal with these types of data numerically using various Likert scales, fuzzy set theory first proposed by Zadeh dealing words or sentences with mathematically oriented manner in order to create machines that mimic the reasoning of human being brings new perspectives. Since then, its applicability has expanded into various disciplines for different purposes to transform subjectivity into objectivity. In this manuscript, the effort of transformation from subjectivity into objectivity will be reviewed based on the available proposed methods in the literature. Two illustrative examples will be employed. While the first illustrative example, which consists of balanced and unbalanced linguistic term sets, is used for each reviewed method in the literature to show its computations step by step, the second of which, balanced linguistic term set, is an example used in the literature that is employed. Therefore, a comprehensive review of the proposed methods with illustrative examples is presented.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13565	13577		10.1007/s00500-019-04354-z													
J								Seismic risk of critical facilities in the Dominican Republic: case study of school buildings	SOFT COMPUTING										Seismic risk analysis; Seismic vulnerability; Existing structures; School; Dominican Republic		The island of Hispaniola, shared by the Dominican Republic and Haiti, is located in a subduction zone between the North America plate and the Caribbean plate. In addition, there are 13 geological faults in the interior of the island, some of which have shown the potential to generate earthquakes of magnitude 7.5 and higher. Thus, the whole island is considered to be a high seismic risk region. In the past 100 years, several earthquakes have affected both parts of the island. In the case of the Dominican Republic, two earthquakes stand out: a magnitude 8.1 earthquake on August 4, 1946, north of the Samana Province, which caused a tsunami, soil liquefaction, and the loss of about 100 lives, and a magnitude 6.5 earthquake on September 22, 2003, in the city of Puerto Plata, which caused significant damage for infrastructures. Among the observed effects, the partial and total collapse of several school buildings had a remarkable impact on local communities. In addition to the high seismic risk, a large part of the national infrastructure may exhibit high vulnerability to earthquakes because the seismic regulations had been the same for 32 years, namely from 1979 to 2011. During these three decades, thousands of structures were built nationwide, including essential facilities such as hospitals and schools. Considering that the current student population in public schools in the Dominican Republic is over 2 million, with the majority attending buildings that were designed with the 1979 seismic code and which proved to be highly vulnerable during the Puerto Plata earthquake, it is vital to take measures that reduce the risk and minimize potential earthquake damage to school buildings. In this context, the Technological Institute of Santo Domingo (INTEC) has undertaken recently a project with the main objective to assess the seismic vulnerability of 22 schools located in the San Cristobal Province, in the south of the Dominican Republic. The latter schools were all built prior to the adoption of the current updated seismic code. This paper presents the results of the assessment of the Fernando Cabral Ortega School. Although only the results of a single RC building are presented, the response of such structure can be considered representative of a portfolio of existing schools in Dominican Republic.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13579	13595		10.1007/s00500-019-04361-0													
J								Sentiment analysis of expectation and perception of MILANO EXPO2015 in twitter data: a generalized cross entropy approach	SOFT COMPUTING										Sentiment analysis; General cross entropy; Milano EXPO2015; Latent semantic analysis; Gap analysis; Twitter	MAXIMUM-ENTROPY; GME ESTIMATION; QUALITY; MODEL	In this paper, data concerning MILANO EXPO2015 is collected from the official twitter page of the event before and after its opening. In order to extract a semi-supervised ontology and to evaluate the global sentiment around the event, a variety of language processing techniques has been applied on the collected "tweets": Latent Semantic Analysis, sentiment polarity tracking, along with gap analysis has allowed the semantic evaluation of users' opinions. Moreover, the generalized cross entropy approach has been applied for the first time on web data, adding prior information on the effect of semantic classes on the global sentiment, improving accuracy and adding detail to the analysis.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13597	13607		10.1007/s00500-019-04368-7													
J								Systemic shock propagation in a complex system	SOFT COMPUTING										Complexity; Complex system; Systemic risk; Shock; Simulation; Mathematica; Recovery; Contagion; Contagion index		We study the effects of delivering a shock to a complex system comprising components ('agents') that interact in a pairwise fashion, independent of other parts of the system and with no central control. There are three aspects to the contribution of this paper. First, shock propagation in a network is developed purely from fundamental principles of complex systems. Second, systemic risk is shown to arise naturally in such a complex system. If a shock is delivered either to one agent or to many agents simultaneously, that shock may be transmitted further, thereby resulting in systemic risk. Third, the monetary loss to the entire system as a result of systemic shock is quantified. Simulations are used to study two particular characteristics of the interactions. The first is the resistance or susceptibility of individual agents to a shock. The second is the time it takes for the shock to affect the entire system. The results show that if a shock is applied to all agents in a network, the systemic effect of that shock is transmitted very quickly. Applying a shock to very few agents results only in an idiosyncratic effect. If an agent can transmit the shock further, a systemic effect will result. The recovery period for agents affected by a systemic shock can be orders of magnitude greater than the time taken for the shock to take effect. The overall effect of the shock on the system is quantified by formulating a 'contagion index', which measures the ratio of the total capital lost due to the systemic effect to the total capital before the shock was delivered. The result (approximately 7%) is consistent with other studies, but is more widely applicable because it is not based on one empirical data set.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13667	13685		10.1007/s00500-019-04466-6													
J								Consensus dynamics, network interaction, and Shapley indices in the Choquet framework	SOFT COMPUTING										Consensus reaching; Linear dynamical models; Network interaction; Choquet capacities; Mobius transforms; Shapley power and interaction indices	DISCRETE FUZZY MEASURES; GROUP DECISION-MAKING; OPINION DYNAMICS; SOCIAL-INFLUENCE; AGGREGATION; INTEGRATION; CAPACITIES; OPERATORS; RESPECT; MODEL	We consider a set N = {1,..., n} of interacting agents whose individual opinions are denoted by xi, i. N in some domain D. R. The interaction among the agents is expressed by a symmetric interaction matrix with null diagonal and off-diagonal coefficients in the open unit interval. The interacting network structure is thus that of a complete graph with edge values in (0, 1). In the Choquet framework, the interacting network structure is the basis for the construction of a consensus capacity mu, where the capacity value mu( S) of a coalition of agents S. N is defined to be proportional to the sum of the edge interaction values contained in the subgraph associated with S. The capacity mu is obtained in terms of its 2-additive Mobius transform m mu, and the corresponding Shapley power and interaction indices are identified. We then discuss two types of consensus dynamics, both of which refer significantly to the notion of context opinion. The second type converges simply the plain mean, whereas the first type produces the Shapley mean as the asymptotic consensual opinion. In this way, it provides a dynamical realization of Shapley aggregation.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13757	13768		10.1007/s00500-019-04512-3													
J								Fundamental bubbles in equity markets	SOFT COMPUTING										Bubble; Affine model; Principal component analysis; Data-rich; Stationarity	TERM STRUCTURE MODELS; MONETARY-POLICY; AFFINE MODELS; YIELD CURVE; ARBITRAGE; PREMIA; TESTS; PRICE; IDENTIFICATION; GROWTH	Using an affine model to compute the price of equities based on a dataset of macroeconomic factors, we propose a measure of equity bubbles. We use a dynamic affine term structure framework to price equity and bonds jointly, and investigate how prices are related to a set of macrofactors extracted from a large dataset of economic time series. We analyze the discrepancies between market and model implied equity prices and use them as a measure for bubbles. A bubble is diagnosed over a given period whenever the discrepancies are not stationary and impact the underlying economy consistently with the literature's findings, increasing over the shorter term economic activity before leading to a net loss in it. We perform the analysis over 3 major US and 3 major European equity indices over the 1990-2017 period and find bubbles only for two of the US equity indices, the S&P500 and the Dow Jones.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13769	13796		10.1007/s00500-019-04514-1													
J								Non-parametric news impact curve: a variational approach	SOFT COMPUTING										Volatility modelling; News impact curve; Calculus of variations; Wavelet theory; ARCH	EFFICIENT ESTIMATION; VOLATILITY; MODELS; HETEROSKEDASTICITY; VARIANCE	In this paper, we propose an innovative algorithm for modelling the news impact curve. The news impact curve provides a nonlinear relation between past returns and current volatility and thus enables to forecast volatility. Our news impact curve is the solution of a dynamic optimization problem based on variational calculus. Consequently, it is a non-parametric and smooth curve. The technique we propose is directly inspired from noise removal techniques in signal theory. To our knowledge, this is the first time that such a method is used for volatility modelling. Applications on simulated heteroskedastic processes as well as on financial data show a better accuracy in estimation and forecast for this approach than for standard parametric (symmetric or asymmetric ARCH) or non-parametric (Kernel-ARCH) econometric techniques.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13797	13812		10.1007/s00500-019-04607-x													
J								Predicting one type of technological motion? A nonlinear map to study the 'sailing-ship' effect	SOFT COMPUTING										Nonlinear dynamics; Sailing-ship effect; Econophysics	INNOVATION; DIFFUSION; DYNAMICS	In this work, we use a proven model to study a dynamic duopolistic competition between an old technology and a new technology which through an improved technical performance-e.g. data transmission capacity-fight in order to conquer market share. The process whereby an old technology fights a new one off through own improvements has been named 'sailing-ship effect'. In the simulations proposed, intentional improvements of both the old technology and the new technology are affected by the values of three key parameters: scientific-technological, purely technological and purely economic. The interaction between these components gives rise to different outcomes in terms of prevalence of one technology over the other.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13813	13822		10.1007/s00500-019-04622-y													
J								Using Markov-switching models with Markov chain Monte Carlo inference methods in agricultural commodities trading	SOFT COMPUTING										Markov-switching GARCH; Markovian chain processes; Markov chain Monte Carlo; Commodities; Alpha creation; Financial crisis; Computational finance; Financial market crisis prediction; Commodities market trading	EXCHANGE-RATE; TIME-SERIES; NONLINEAR IMPACT; ECONOMIC-GROWTH; PRICES FOLLOW; RANDOM-WALK; REGIME; OIL; RISK; RETURNS	In this work, the use of Markov-switching GARCH (MS-GARCH) models is tested in an active trading algorithm for corn and soybean future markets. By assuming that a given investor lives in a two-regime world (with low- and high-volatility time periods), a trading algorithm was simulated (from January 2000 to March 2019), which helped the investor to forecast the probability of being in the high-volatility regime att + 1. Once this probability was known, the investor could decide to invest either in commodities, during low-volatility periods or in the 3-month US Treasury bills, during high-volatility periods. Our results suggest that the Gaussian MS-GARCH model is the most appropriate to generate alpha or extra returns (from a passive investment strategy) in the corn market and thet-Student MS-GARCH is the best one for soybean trading.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13823	13836		10.1007/s00500-019-04629-5													
J								New social media for health promotion management: a statistical analysis	SOFT COMPUTING										Facebook; Multiple correspondence analysis; Cluster analysis; Health promotion	INFORMATION; NETWORKING; INTERVENTIONS; TECHNOLOGIES	The literature from several areas of study reports social media Web sites plays an important role for society in the third millennium. Using the Internet as a tool for health is increasingly common: In particular, Facebook has played increasing role in health promotion management. The objective of the paper was to demonstrate, by a sample survey, how and how much Facebook promotes health by encouraging healthy lifestyles. The online questionnaire has been loaded on Facebook pages and groups that promote healthy lifestyle and well-being. Statistical data analysis, used in an integrated and complementary approach, has been multiple correspondence analysis, cluster analysis and cross-tabulations. The final sample consisted of 3.640 Facebook users. Social media Web sites are widely used by younger generation, but also by adult and senior population to find health information. Starting from the research results, it can be argued that Facebook users would like to have more "certified information" on health through social media Web sites. Starting from this information, governments could promote healthy lifestyles through Facebook, paying attention to certify the contents.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13853	13862		10.1007/s00500-019-04664-2													
J								Sustainable consumption behaviours in P2P accommodation platforms: an exploratory study	SOFT COMPUTING										Customer sustainable behaviours; P2P platform; Services portfolio complexity; Network membership; Reputation; Innovative practices	PORTFOLIO COMPLEXITY; ABSORPTIVE-CAPACITY; SHARING ECONOMY; PERFORMANCE; KNOWLEDGE; CONSUMERS; PRODUCT; PATHWAY; TRUST; RISK	This paper examines how sustainable consumption behaviours are assembled in peer-to-peer (P2P) platforms, based on four factors-services portfolio complexity, network membership, reputation and innovative practices-and its impact on P2P platform performance. Using data from one P2P accommodation platform in Romania and based on 2556 observations, we tested the research hypothesis using ordinary least squares regression. Specifically, services portfolio complexity positively influences sustainable consumption behaviours, while network membership has a negative influence. Services portfolio complexity has a positive influence on sustainable consumption behaviours when innovative practices are high. Finally, sustainable consumption behaviours positively influence P2P platform performance.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13863	13870		10.1007/s00500-020-04681-6													
J								Information reconciliation through an agent-controlled graph model	SOFT COMPUTING										Damage assessment; Recovery; Transaction dependency; Malicious attacks	RECOVERY	With the advancement of Internet technology and the rise of big data, securing information from malicious attacks has become more important; yet, more challenging. Even though prevention techniques exist, they are not enough to fully secure the data from malicious activities. This dictated the need for a detection and recovery model to assess the damage and bring the database back to its consistent state in case of an attack. This recovery should be done as quickly and efficiently as possible in order to avoid damage propagation and inaccurate access to data. Multiple models have been proposed, and different techniques and data structures were used to recover the database to its reliable state. In this work, we present a superior damage assessment and recovery algorithm that is centered on agents. Our hybrid lightweight approach is based on clustering database transactions based on a given set of criteria using graphs to keep track of transactional dependencies. This way, our model allows for: (1) parallel information processing-which makes recovery more effectual, (2) separation of concerns-which makes it easier to maintain a given data structure, (3) attack/problem isolation where a malicious transaction will be isolated from the remaining unharmed parts of database (the undamaged parts of the database can remain 'live' and there is no need to take them offline, and (4) easier scaling as bottlenecks are diminished. The presented approach is compared to other existing ones and exhibited superior performance.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14019	14037		10.1007/s00500-020-04779-x													
J								One more look on visualization of operation of a root-finding algorithm	SOFT COMPUTING										Self-adaptation; Root finding; Dynamics; Iterations; Visualization	PARTICLE SWARM OPTIMIZATION; FIXED-POINTS; PARAMETER SELECTION; POLYNOMIOGRAPHY	Many algorithms that iteratively find solution of an equation require tuning. Due to the complex dependence of many algorithm's elements, it is difficult to know their impact on the work of the algorithm. The article presents a simple root-finding algorithm with self-adaptation that requires tuning, similarly to evolutionary algorithms. Moreover, the use of various iteration processes instead of the standard Picard iteration is presented. In the algorithm's analysis, visualizations of the dynamics were used. The conducted experiments and the discussion regarding their results allow to understand the influence of tuning on the proposed algorithm. The understanding of the tuning mechanisms can be helpful in using other evolutionary algorithms. Moreover, the presented visualizations show intriguing patterns of potential artistic applications.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14135	14155		10.1007/s00500-020-04784-0													
J								Multiple-kernel combination fuzzy clustering for community detection	SOFT COMPUTING										Multiple-kernel clustering; Community detection; Random walk		Community detection on social network is a challenging task, and the multiple-kernel learning method is gaining popularity. In this paper, we propose a new multiple-kernel combination algorithm for community partitioning. We study several base kernel matrices from the adjacency matrix of a network. By adjusting the weights of different base kernel matrices, a new kernel matrix is constructed using linear combination of those matrices. To partition networks whose number of communities are known in advance, we derive a new kernel matrix which forms a basis for community partitioning. We further propose a novel robust multiple-kernel combination-based fuzzy clustering algorithm. Extensive experiments are conducted on many real-world networks that contain ground truth on community structures. The experimental results indicate that the proposed algorithm is more efficient than other existing community detection methods and related kernel clustering algorithms. This study demonstrates the feasibility and efficiency of the multiple-kernel learning method for community detection.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14157	14165		10.1007/s00500-020-04785-z													
J								Computer vision algorithms for dominant contact lens feature extraction using fuzzy-logic-based classifications	SOFT COMPUTING										Iris recognition; Feature extraction; Classifiers	IRIS	Iris image recognition is an emerging approach for human identification but offers low reliability. An algorithm for dominant contact lens feature extraction based on an improved neighboring binary pattern (NBP) approach is proposed herein. Features are compared with neighboring features in various directions, assigning a value of 1 to dominant features and 0 otherwise. The features in two-dimensional binary tables are then trained using an adaptive neuro fuzzy inference system (ANFIS) and classified using various classifiers. The performance of various feature descriptors based on the classification algorithms is measured and compared using parameters such as the accuracy, training time, positive acceptance rate (PAR), and negative acceptance rate (NAR), and the PAR and NAR are compared based upon a confusion matrix of classifiers. The proposed dominant feature extraction method achieves an accuracy rate of 95.7%.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14235	14249		10.1007/s00500-020-04791-1													
J								Domain bias in distinguishing Flemish and Dutch subtitles	NATURAL LANGUAGE ENGINEERING										Text classification; Dutch; Methodology; Dialect recognition; Topic bias		This paper describes experiments in which I tried to distinguish between Flemish and Netherlandic Dutch subtitles, as originally proposed in the VarDial 2018 Dutch-Flemish Subtitle task. However, rather than using all data as a monolithic block, I divided them into two non-overlapping domains and then investigated how the relation between training and test domains influences the recognition quality. I show that the best estimate of the level of recognizability of the language varieties is derived when training on one domain and testing on another. Apart from the quantitative results, I also present a qualitative analysis, by investigating in detail the most distinguishing features in the various scenarios. Here too, it is with the out-of-domain recognition that some genuine differences between Flemish and Netherlandic Dutch can be found.																	1351-3249	1469-8110				SEP	2020	26	5					493	510	PII S1351324919000445	10.1017/S1351324919000445													
J								Uncovering the language of wine experts	NATURAL LANGUAGE ENGINEERING										Corpus linguistics; Information extraction; Semantics; Statistical methods; Wine expertise	STANDARDIZED SYSTEM; OLD WINE; ODOR; FLAVOR; PERCEPTION; AROMA; COLOR; SMELL; DISCRIMINATION; IDENTIFICATION	Talking about odors and flavors is difficult for most people, yet experts appear to be able to convey critical information about wines in their reviews. This seems to be a contradiction, and wine expert descriptions are frequently received with criticism. Here, we propose a method for probing the language of wine reviews, and thus offer a means to enhance current vocabularies, and as a by-product question the general assumption that wine reviews are gibberish. By means of two different quantitative analyses-support vector machines for classification and Termhood analysis-on a corpus of online wine reviews, we tested whether wine reviews are written in a consistent manner, and thus may be considered informative; and whether reviews feature domain-specific language. First, a classification paradigm was trained on wine reviews from one set of authors for which the color, grape variety, and origin of a wine were known, and subsequently tested on data from a new author. This analysis revealed that, regardless of individual differences in vocabulary preferences, color and grape variety were predicted with high accuracy. Second, using Termhood as a measure of how words are used in wine reviews in a domain-specific manner compared to other genres in English, a list of 146 wine-specific terms was uncovered. These words were compared to existing lists of wine vocabulary that are currently used to train experts. Some overlap was observed, but there were also gaps revealed in the extant lists, suggesting these lists could be improved by our automatic analysis.																	1351-3249	1469-8110				SEP	2020	26	5					511	530	PII S1351324919000500	10.1017/S1351324919000500													
J								It all starts with entities: A Salient entity topic model	NATURAL LANGUAGE ENGINEERING										Entity Salience; Entity Topic Model	SHORT TEXT	Entities play an essential role in understanding textual documents, regardless of whether the documents are short, such as tweets, or long, such as news articles. In short textual documents, all entities mentioned are usually considered equally important because of the limited amount of information. In long textual documents, however, not all entities are equally important: some are salient and others are not. Traditional entity topic models (ETMs) focus on ways to incorporate entity information into topic models to better explain the generative process of documents. However, entities are usually treated equally, without considering whether they are salient or not. In this work, we propose a novel ETM, Salient Entity Topic Model, to take salient entities into consideration in the document generation process. In particular, we model salient entities as a source of topics used to generate words in documents, in addition to the topic distribution of documents used in traditional topic models. Qualitative and quantitative analysis is performed on the proposed model. Application to entity salience detection demonstrates the effectiveness of our model compared to the state-of-the-art topic model baselines.																	1351-3249	1469-8110				SEP	2020	26	5					531	549	PII S1351324919000585	10.1017/S1351324919000585													
J								Unsupervised modeling anomaly detection in discussion forums posts using global vectors for text representation	NATURAL LANGUAGE ENGINEERING										Text classification; Text clustering; Anomaly detection; Word embeddings	CLUSTER; VALIDATION; SUPPORT; ONLINE	Anomaly detection can be seen as an unsupervised learning task in which a predictive model created on historical data is used to detect outlying instances in new data. This work addresses possibly promising but relatively uncommon application of anomaly detection to text data. Two English-language and one Polish-language Internet discussion forums devoted to psychoactive substances received from home-grown plants, such as hashish or marijuana, serve as text sources that are both realistic and possibly interesting on their own, due to potential associations with drug-related crime. The utility of two different vector text representations is examined: the simple bag of words representation and a more refined Global Vectors (GloVe) representation, which is an example of the increasingly popular word embedding approach. They are both combined with two unsupervised anomaly detection methods, based on one-class support vector machines (SVM) and based on dissimilarity tok-medoids clusters. The GloVe representation is found definitely more useful for anomaly detection, permitting better detection quality and ameliorating the curse of dimensionality issues with text clustering. The cluster dissimilarity approach combined with this representation outperforms one-class SVM with respect to detection quality and appears a more promising approach to anomaly detection in text data.																	1351-3249	1469-8110				SEP	2020	26	5					551	578	PII S1351324920000066	10.1017/S1351324920000066													
J								Benchmarks and goals	NATURAL LANGUAGE ENGINEERING										Benchmarks; Knowledge graph completion; Bilingual lexicon induction; MUSE; WordNet		Benchmarks can be a useful step toward the goals of the field (when the benchmark is on the critical path), as demonstrated by the GLUE benchmark, and deep nets such as BERT and ERNIE. The case for other benchmarks such as MUSE and WN18RR is less well established. Hopefully, these benchmarks are on a critical path toward progress on bilingual lexicon induction (BLI) and knowledge graph completion (KGC). Many KGC algorithms have been proposed such as Trans[DEHRM], but it remains to be seen how this work improves WordNet coverage. Given how much work is based on these benchmarks, the literature should have more to say than it does about the connection between benchmarks and goals. Is optimizing P@10 on WN18RR likely to produce more complete knowledge graphs? Is MUSE likely to improve Machine Translation?																	1351-3249	1469-8110				SEP	2020	26	5					579	592	PII S1351324920000418	10.1017/S1351324920000418													
J								A Constrained Randomization Approach to Interactive Visual Data Exploration with Subjective Feedback	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Data visualization; Data models; Computational modeling; Data mining; Reactive power; Visualization; Tools; Exploratory data mining; dimensionality reduction; data randomization; subjective interestingness	NONLINEAR DIMENSIONALITY REDUCTION; LINES; FIT	Data visualization and iterative/interactive data mining are growing rapidly in attention, both in research as well as in industry. However, while there are a plethora of advanced data mining methods and lots of works in the field of visualization, integrated methods that combine advanced visualization and/or interaction with data mining techniques in a principled way are rare. We present a framework based on constrained randomization which lets users explore high-dimensional data via 'subjectively informative' two-dimensional data visualizations. The user is presented with 'interesting' projections, allowing users to express their observations using visual interactions that update a background model representing the user's belief state. This background model is then considered by a projection-finding algorithm employing data randomization to compute a new 'interesting' projection. By providing users with information that contrasts with the background model, we maximize the chance that the user encounters striking new information present in the data. This process can be iterated until the user runs out of time or until the difference between the randomized and the real data is insignificant. We present two case studies, one controlled study on synthetic data and another on census data, using the proof-of-concept tool SIDE that demonstrates the presented framework.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1666	1679		10.1109/TKDE.2019.2907082													
J								Adaptive Self-Paced Deep Clustering with Data Augmentation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Clustering algorithms; Training; Task analysis; Gallium nitride; Gaussian mixture model; Feature extraction; Deep clustering; self-paced learning; data augmentation; unsupervised feature learning		Deep clustering gains superior performance than conventional clustering by jointly performing feature learning and cluster assignment. Although numerous deep clustering algorithms have emerged in various applications, most of them fail to learn robust cluster-oriented features which in turn hurts the final clustering performance. To solve this problem, we propose a two-stage deep clustering algorithm by incorporating data augmentation and self-paced learning. Specifically, in the first stage, we learn robust features by training an autoencoder with examples that are augmented by random shifting and rotating the given clean examples. Then, in the second stage, we encourage the learned features to be cluster-oriented by alternatively finetuning the encoder with the augmented examples and updating the cluster assignments of the clean examples. During finetuning the encoder, the target of each augmented example in the loss function is the center of the cluster to which the clean example is assigned. The targets may be computed incorrectly, and the examples with incorrect targets could mislead the encoder network. To stabilize the network training, we select most confident examples in each iteration by utilizing the adaptive self-paced learning. Extensive experiments validate that our algorithm outperforms the state of the arts on four image datasets.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1680	1693		10.1109/TKDE.2019.2911833													
J								An Effective Clustering Method over CF+ Tree Using Multiple Range Queries	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Clustering; BIRCH; CF tree; CF+ tree; range query; very large data sets	ALGORITHMS	Many existing clustering methods usually compute clusters from the reduced data sets obtained by summarizing the original very large data sets. BIRCH is a popular summary-based clustering method that first builds a CF tree, and then performs a global clustering using the leaf entries of the tree. However, to the best of our knowledge, no prior studies have proposed a global clustering method that uses the structure of a CF tree. Therefore, we propose a novel global clustering method ERC (effective multiple range queries-based clustering), which takes advantage of the structure of a CF tree. We further propose a CF+ tree, which optimizes the node split scheme used in the CF tree. As a result, the CF+-ERC (CF+ tree-based ERC) method effectively computes clusters over large data sets. Furthermore, it does not require a predefined number of clusters to compute the clusters. We present in-depth theoretical and experimental analyses of our method. Experimental results on very large synthetic data sets demonstrate that the proposed approach is effective in terms of cluster quality and robustness and is significantly faster than existing clustering methods. In addition, we apply our clustering method to real data sets and achieve promising results.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1694	1706		10.1109/TKDE.2019.2911520													
J								Equilibrium of Redundancy in Relational Model for Optimized Data Retrieval	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Optimization; Data models; Redundancy; Unified modeling language; Distributed databases; Indexes; Relational databases; optimization; denormalization; data redundancy; binary linear programming	STRATEGIES	Conceptual and relational data models of online transaction processing (OLTP) applications are usually created and maintained following the principle of normalization, which implies avoidance of redundancy. Data retrieval from a disk-based normalized relational database often requires complex and inefficient queries that may cause noticeable performance issues when executed on larger volumes of data. Computer professionals sometimes intentionally trade off the strict normal form to optimize data retrieval queries through error-prone manual tuning and denormalization. We propose a fully automatic optimization approach, based on data redundancy, that relies on a formal cost-benefit model. We prove that finding the optimal level of data redundancy, for given workload statistics, is an NP-Complete optimization problem. A detailed reduction of the problem to binary linear programming is presented in the paper. The proposed optimization approach was evaluated using the TPCE benchmark for OLTP systems. The evaluation has shown that the proposed optimization approach is highly scalable, and that it can be efficiently applied to real-life relational data models.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1707	1721		10.1109/TKDE.2019.2911580													
J								Evaluating Overfit and Underfit in Models of Network Community Structure	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Clustering algorithms; Probabilistic logic; Bayes methods; Detection algorithms; Prediction algorithms; Partitioning algorithms; Community detection; model selection; overfitting; underfitting; link prediction; link description	STOCHASTIC BLOCKMODELS; COMPLEX NETWORKS; PREDICTION; RESOLUTION; SELECTION	A common graph mining task is community detection, which seeks an unsupervised decomposition of a network into groups based on statistical regularities in network connectivity. Although many such algorithms exist, community detection's No Free Lunch theorem implies that no algorithm can be optimal across all inputs. However, little is known in practice about how different algorithms over or underfit to real networks, or how to reliably assess such behavior across algorithms. Here, we present a broad investigation of over and underfitting across 16 state-of-the-art community detection algorithms applied to a novel benchmark corpus of 572 structurally diverse real-world networks. We find that (i) algorithms vary widely in the number and composition of communities they find, given the same input; (ii) algorithms can be clustered into distinct high-level groups based on similarities of their outputs on real-world networks; (iii) algorithmic differences induce wide variation in accuracy on link-based learning tasks; and, (iv) no algorithm is always the best at such tasks across all inputs. Finally, we quantify each algorithm's overall tendency to over or underfit to network data using a theoretically principled diagnostic, and discuss the implications for future advances in community detection.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1722	1735		10.1109/TKDE.2019.2911585													
J								Evaluation of Community Detection Methods	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Indexes; Mutual information; Entropy; Clustering methods; Measurement; Complex networks; Normalized mutual information; kappa index; F-score; community structures detection	MODULARITY; NETWORKS	Community structures are critical towards understanding not only the network topology but also how the network functions. However, how to evaluate the quality of detected community structures is still challenging and remains unsolved. The most widely used metric, normalized mutual information (NMI), was proven to have finite size effect, and its improved form relative normalized mutual information (rNMI) has reverse finite size effect. Corrected normalized mutual information (cNMI) was thus proposed and has neither finite size effect nor reverse finite size effect. However, in this paper, we show that cNMI violates the so-called proportionality assumption. In addition, NMI-type metrics have the problem of ignoring importance of small communities. Finally, they cannot be used to evaluate a single community of interest. In this paper, we map the computed community labels to the ground-truth ones through integer linear programming, and then use kappa index and F-score to evaluate the detected community structures. Experimental results demonstrate the advantages of our method.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1736	1746		10.1109/TKDE.2019.2911943													
J								Feature Selective Projection with Low-Rank Embedding and Dual Laplacian Regularization	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Dimensionality reduction; feature extraction; feature selection; subspace learning; low rank representation; graph Laplacian regularization	UNSUPERVISED FEATURE-SELECTION; HIGH-DIMENSIONAL DATA; PRESERVING PROJECTIONS; DISCRIMINANT-ANALYSIS; FEATURE-EXTRACTION; REDUCTION; RECOGNITION; ALGORITHM; FRAMEWORK; GRAPH	Feature extraction and feature selection have been regarded as two independent dimensionality reduction methods in most of the existing literature. In this paper, we propose to integrate both approaches into a unified framework and design an unsupervised linear feature selective projection (FSP) for feature extraction with low-rank embedding and dual Laplacian regularization, with the aim to exploit the intrinsic relationship among data and suppress the impact of noise. Specifically, a projection matrix with an l(2,1)-norm regularization is introduced to project original high dimensional data points into a new subspace with lower dimension, where the l(2,1)-norm regularization can endow the projection with good interpretability. We deploy a coefficient matrix with low rank constraint to reconstruct the data points and the l(2,1)-norm is imposed to regularize the data reconstruction errors in the low-dimensional subspace and make FSP robust to noise. Furthermore, a dual graph Laplacian regularization term is imposed on the low dimensional data and data reconstruction matrix for preserving the local manifold geometrical structure of data. Finally, an alternatively iterative algorithm is carefully designed for solving the proposed optimization model. Theoretical convergence and computational complexity analysis of the algorithm are also provided. Comprehensive experiments on various benchmark datasets have been carried out to evaluate the performance of the proposed FSP. As indicated, our algorithm significantly outperforms other state-of-the-art methods for feature extraction.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1747	1760		10.1109/TKDE.2019.2911946													
J								LERI: Local Exploration for Rare-Category Identification	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Rare category; classification; local exploration; arbitrary shape; k-nearest neighbors	EVENTS	To identify the data examples of rare categories that form small compact clusters in large data sets, existing approaches mostly require enough labeled data examples as a training set to learn a classifier, assuming that the rare-category clusters are spherical or nearly spherical. Nonetheless, a large enough training set is usually difficult to obtain in practice, and rare categories in many real-world applications often form small compact clusters with arbitrary shapes. In this paper, we investigate how to identify all data examples of a rare category with an arbitrary shape based on only one seed (i.e., a labeled rare-category data example). Instead of finding a compact and spherical local region around the seed, we locally explore the data set from the seed by continuously searching and visiting the k-nearest neighbors of each newly visited data example. The local exploration connects the data examples in the objective rare category by the relationship of k-nearest neighbors, and meanwhile, suspected external data examples are filtered out if they are not close enough to any visited data example. Experimental results on both synthetic and real-world data sets are conducted, and the results verify the effectiveness and efficiency of our approach.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1761	1772		10.1109/TKDE.2019.2911941													
J								Member Behavior in Dynamic Online Communities: Role Affiliation Frequency Model	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Analytical models; Social networking (online); Time-frequency analysis; Heuristic algorithms; Data models; Cultural differences; Logic gates; Dynamic online community; influential members; online behavior; role frequency; social roles	SOCIAL ROLES; NETWORKS; NUMBER	People's social life has become more embedded in dynamic online communities. Each online community can be viewed as a temporal online social network (OSN). The interaction level among OSN members leads to the emergence of dynamic social roles, which change and evolve over time, creating a sequence of temporal roles. These role sequences show diversity in the role-affiliation frequency of members. That diversity enables modeling the dynamic behaviors of individuals. This paper proposes a temporal role-affiliation frequency model (RAFM) which detects the time evolving roles of each member and analyzes her/his role-affiliation frequency to infer her/his latent behavior. Applying the RAFM to real interaction data, collected in four online communities, revealed the identity of influential members. In addition, members with similar temporal behavioral patterns were found to have similar latent behavior patterns. These patterns are manifested via similar role transitions in different OSNs whose temporal interaction rhythms were compatible. These two research findings contribute to OSN research and knowledge via improved understanding of member behavior online based on role-affiliation frequency and role transitions. Thus, member latent behavior can be inferred, and influential members can be identified.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1773	1784		10.1109/TKDE.2019.2911067													
J								Memory Augmented Deep Generative Models for Forecasting the Next Shot Location in Tennis	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Sports; Generative adversarial networks; Semantics; Trajectory; Memory modules; Adaptation models; Predictive models; Neural memory networks; generative adversarial networks; tennis shot prediction; player behavior analysis	INTEGRATION; SKILLS	This paper presents a novel framework for predicting shot location and type in tennis. Inspired by recent neuroscience discoveries, we incorporate neural memory modules to model the episodic and semantic memory components of a tennis player. We propose a Semi-Supervised Generative Adversarial Network architecture that couples these memory models with the automatic feature learning power of deep neural networks, and demonstrate methodologies for learning player level behavioral patterns with the proposed framework. We evaluate the effectiveness of the proposed model on tennis tracking data from the 2012 Australian Tennis Open and exhibit applications of the proposed method in discovering how players adapt their style depending on the match context.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1785	1797		10.1109/TKDE.2019.2911507													
J								Mining Novel Multivariate Relationships in Time Series Data Using Correlation Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Time series analysis; Gain; Correlation; Meteorology; Eigenvalues and eigenfunctions; Neuroscience; Time measurement; Multivariate linear patterns; correlation mining; spatio-temporal; climate teleconnections; fMRI	TELECONNECTIONS; SELECTION; PARIETAL; CLIMATE	In many domains, there is significant interest in capturing novel relationships between time series that represent activities recorded at different nodes of a highly complex system. In this paper, we introduce multipoles, a novel class of linear relationships between more than two time series. A multipole is a set of time series that have strong linear dependence among themselves, with the requirement that each time series makes a significant contribution to the linear dependence. We demonstrate that most interesting multipoles can be identified as cliques of negative correlations in a correlation network. Such cliques are typically rare in a real-world correlation network, which allows us to find almost all multipoles efficiently using a clique-enumeration approach. Using our proposed framework, we demonstrate the utility of multipoles in discovering new physical phenomena in two scientific domains: climate science and neuroscience. In particular, we discovered several multipole relationships that are reproducible in multiple other independent datasets and lead to novel domain insights.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1798	1811		10.1109/TKDE.2019.2911681													
J								On Optimally Partitioning Variable-Byte Codes	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Inverted Index Compression; Variable-Byte Encoding; Performance Evaluation	COMPRESSION	The ubiquitous Variable-Byte encoding is one of the fastest compressed representation for integer sequences. However, its compression ratio is usually not competitive with other more sophisticated encoders, especially when the integers to be compressed are small which is the typical case for inverted indexes. This paper shows that the compression ratio of Variable-Byte can be improved by 2x by adopting a partitioned representation of the inverted lists. This makes Variable-Byte surprisingly competitive in space with the best bit-aligned encoders, hence disproving the folklore belief that Variable-Byte is space-inefficient for inverted index compression. Despite the significant space savings, we show that our optimization almost comes for free, given that: we introduce an optimal partitioning algorithm that does not affect indexing time because of its linear-time complexity; we show that the query processing speed of Variable-Byte is preserved, with an extensive experimental analysis and comparison with several other state-of-the-art encoders.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1812	1823		10.1109/TKDE.2019.2911288													
J								Structural Representation Learning for User Alignment Across Social Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Social networking (online); Task analysis; Computational modeling; Learning systems; Context modeling; Optimization; Manifolds; User alignment; network embedding; representation learning; social networks	IDENTIFICATION	Aligning users across different social networks has become increasingly studied as an important task to social network analysis. In this paper, we propose a novel representation learning method that mainly exploits social structures for the network alignment. In particular, the proposed network embedding framework models the follower-ship and followee-ship of each user explicitly as input and output context vectors, while preserving the proximity of users with "similar" followers and followees in the embedded space. We incorporate both known and predicted user anchors across the networks as constraints to facilitate the transfer of context information to achieve accurate user alignment. Both network embedding and user alignment are inferred under a unified optimization framework with negative sampling adopted to ensure scalability. Also, variants of the proposed framework, including the incorporation of higher-order structural features, are also explored for further boosting the alignment accuracy. Extensive experiments on large-scale social and academia network datasets demonstrate the efficacy of our proposed model compared with state-of-the-art methods.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1824	1837		10.1109/TKDE.2019.2911516													
J								Variable Weighting in Fuzzy k-Means Clustering to Determine the Number of Clusters	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Fuzzy k-means; clustering; number of clusters; data mining; variable weighting	DATA SETS; ALGORITHM; SELECTION; MODEL	One of the most significant problems in cluster analysis is to determine the number of clusters in unlabeled data, which is the input for most clustering algorithms. Some methods have been developed to address this problem. However, little attention has been paid on algorithms that are insensitive to the initialization of cluster centers and utilize variable weights to recover the number of clusters. To fill this gap, we extend the standard fuzzy k-means clustering algorithm. It can automatically determine the number of clusters by iteratively calculating the weights of all variables and the membership value of each object in all clusters. Two new steps are added to the fuzzy k-means clustering process. One of them is to introduce a penalty term to make the clustering process insensitive to the initial cluster centers. The other one is to utilize a formula for iterative updating of variable weights in each cluster based on the current partition of data. Experimental results on real-world and synthetic datasets have shown that the proposed algorithm effectively determined the correct number of clusters while initializing the different number of cluster centroids. We also tested the proposed algorithm on gene data to determine a subset of important genes.																	1041-4347	1558-2191				SEPT 1	2020	32	9					1838	1853		10.1109/TKDE.2019.2911582													
J								A Comprehensive Analysis of Deep Regression	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Computer architecture; Task analysis; Pose estimation; Computer vision; Systematics; Deep learning; Benchmark testing; Deep learning; regression; computer vision; convolutional neural networks; statistical significance; empirical and systematic evaluation; head-pose estimation; full-body pose estimation; facial landmark detection	CONVOLUTIONAL NEURAL-NETWORKS; REJECTIVE MULTIPLE TEST; FORESTS; MIXTURE; TESTS	Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e., convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g., VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2065	2081		10.1109/TPAMI.2019.2910523													
J								A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Hidden Markov models; Bayes methods; Muscles; Face recognition; Analytical models; Facial muscles; Interval temporal restricted Boltzmann machine; global spatial and temporal patterns; posed and spontaneous expressions distinction; expressions categories recognition	RECOGNITION; DELIBERATE	Facial expression analysis could be greatly improved by incorporating spatial and temporal patterns present in facial behavior, but the patterns have not yet been utilized to their full advantage. We remedy this via a novel dynamic model-an interval temporal restricted Boltzmann machine (IT-RBM) - that is able to capture both universal spatial patterns and complicated temporal patterns in facial behavior for facial expression analysis. We regard a facial expression as a multifarious activity composed of sequential or overlapping primitive facial events. Allen's interval algebra is implemented to portray these complicated temporal patterns via a two-layer Bayesian network. The nodes in the upper-most layer are representative of the primitive facial events, and the nodes in the lower layer depict the temporal relationships between those events. Our model also captures inherent universal spatial patterns via a multi-value restricted Boltzmann machine in which the visible nodes are facial events, and the connections between hidden and visible nodes model intrinsic spatial patterns. Efficient learning and inference algorithms are proposed. Experiments on posed and spontaneous expression distinction and expression recognition demonstrate that our proposed IT-RBM achieves superior performance compared to state-of-the art research due to its ability to incorporate these facial behavior patterns.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2082	2095		10.1109/TPAMI.2019.2911937													
J								Absolute Cluster Validity	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Clustering algorithms; Indexes; Benchmark testing; Task analysis; Proposals; Autonomous systems; Clustering; cluster validity	DENSITY-FUNCTION; INDEX; CONSISTENCY; VALIDATION; ALGORITHMS; SYSTEM	The application of clustering involves the interpretation of objects placed in multi-dimensional spaces. The task of clustering itself is inherently submitted to subjectivity, the optimal solution can be extremely costly to discover and sometimes even unreachable or nonexistent. This fact introduces a trade-off between accuracy and computational effort, moreover given that engineering applications usually work well with suboptimal solutions. In such applied scenarios, cluster validation is mandatory to refine algorithms and ensure that solutions are meaningful. Validity indices are commonly intended to benchmark diverse clustering setups, therefore they are coefficients with a relative nature, i.e., useful when compared to one another. In this paper, we propose a validation methodology that enables absolute evaluations of clustering results. Our method performs geometric measurements of the solution space and provides a coherent interpretation of the data structure by using indices based on inter- and intra-cluster distances, density, and multimodality within clusters. Conducted tests and comparisons with well-known indices show that our validation methodology improves the robustness of the clustering application for knowledge discovery. While clustering is often performed as a black box technique, our index is construable and therefore allows for the implementation of systems enriched with self-checking capabilities.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2096	2112		10.1109/TPAMI.2019.2912970													
J								Age from Faces in the Deep Learning Revolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Estimation; Deep learning; Face recognition; Training; Face detection; Age estimation; deep learning; face analysis; survey; review	NEURAL-NETWORKS; FEATURES; GENDER; CLASSIFICATION; DATABASE; CASCADE; IMAGES	Face analysis includes a variety of specific problems as face detection, person identification, gender and ethnicity recognition, just to name the most common ones; in the last two decades, significant research efforts have been devoted to the challenging task of age estimation from faces, as witnessed by the high number of published papers. The explosion of the deep learning paradigm, that is determining a spectacular increasing of the performance, is in the public eye; consequently, the number of approaches based on deep learning is impressively growing and this also happened for age estimation. The exciting results obtained have been recently surveyed on almost all the specific face analysis problems; the only exception stands for age estimation, whose last survey dates back to 2010 and does not include any deep learning based approach to the problem. This paper provides an analysis of the deep methods proposed in the last six years; these are analysed from different points of view: the network architecture together with the learning procedure, the used datasets, data preprocessing and augmentation, and the exploitation of additional data coming from gender, race and face expression. The review is completed by discussing the results obtained on public datasets, so as the impact of different aspects on system performance, together with still open issues.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2113	2132		10.1109/TPAMI.2019.2910522													
J								Aggregated Wasserstein Distance and State Registration for Hidden Markov Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Hidden Markov models; Monte Carlo methods; Gaussian distribution; Measurement; Computational modeling; Approximation methods; Markov processes; Hidden Markov model; Gaussian mixture model; Wasserstein distance; optimal transport	TRANSPORT; HMM	We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time position follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as HMM. The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of states. The distance is defined meaningfully even for two HMMs that are estimated from data of different dimensionality, a situation that can arise due to missing variables. This distance quantifies the dissimilarity of HMMs by measuring both the difference between the two marginal GMMs and that between the two transition matrices. Our new distance is tested on tasks of retrieval, classification, and t-SNE visualization of time series. Experiments on both synthetic and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2133	2147		10.1109/TPAMI.2019.2908635													
J								Can We See More? Joint Frontalization and Hallucination of Unaligned Tiny Faces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Spatial resolution; Neural networks; Training; Solid modeling; Feature extraction; Face; super-resolution; hallucination; face frontalization	SUPERRESOLUTION; MODEL	In popular TV programs (such as CSI), a very low-resolution face image of a person, who is not even looking at the camera in many cases, is digitally super-resolved to a degree that suddenly the person's identity is made visible and recognizable. Of course, we suspect that this is merely a cinematographic special effect and such a magical transformation of a single image is not technically possible. Or, is it? In this paper, we push the boundaries of super-resolving (hallucinating to be more accurate) a tiny, non-frontal face image to understand how much of this is possible by leveraging the availability of large datasets and deep networks. To this end, we introduce a novel Transformative Adversarial Neural Network (TANN) to jointly frontalize very-low resolution (i.e., 16 x 16 pixels) out-of-plane rotated face images (including profile views) and aggressively super-resolve them (8x), regardless of their original poses and without using any 3D information. TANN is composed of two components: a transformative upsampling network which embodies encoding, spatial transformation and deconvolutional layers, and a discriminative network that enforces the generated high-resolution frontal faces to lie on the same manifold as real frontal face images. We evaluate our method on a large set of synthesized non-frontal face images to assess its reconstruction performance. Extensive experiments demonstrate that TANN generates both qualitatively and quantitatively superior results achieving over 4 dB improvement over the state-of-the-art.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2148	2164		10.1109/TPAMI.2019.2914039													
J								Defining Image Memorability Using the Visual Memory Schema	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Visualization; Observers; Semantics; Psychology; Organizations; Image recognition; Computer vision; Image memorability; visual memory schema; memory experiments; deep features		Memorability of an image is a characteristic determined by the human observers' ability to remember images they have seen. Yet recent work on image memorability defines it as an intrinsic property that can be obtained independent of the observer. The current study aims to enhance our understanding and prediction of image memorability, improving upon existing approaches by incorporating the properties of cumulative human annotations. We propose a new concept called the Visual Memory Schema (VMS) referring to an organization of image components human observers share when encoding and recognizing images. The concept of VMS is operationalised by asking human observers to define memorable regions of images they were asked to remember during an episodic memory test. We then statistically assess the consistency of VMSs across observers for either correctly or incorrectly recognised images. The associations of the VMSs with eye fixations and saliency are analysed separately as well. Lastly, we adapt various deep learning architectures for the reconstruction and prediction of memorable regions in images and analyse the results when using transfer learning at the outputs of different convolutional network layers.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2165	2178		10.1109/TPAMI.2019.2914392													
J								Hierarchical Gaussian Descriptors with Application to Person Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Image color analysis; Gaussian distribution; Manifolds; Covariance matrices; Histograms; Colored noise; Measurement; Person re-identification; image feature descriptor; Gaussian distribution; Riemannian geometry; symmetric positive definite matrices; log-Euclidean Riemannian metric	FISHER VECTOR; COVARIANCE; CLASSIFICATION; MANIFOLDS	Describing the color and textural information of a person image is one of the most crucial aspects of person re-identification (re-id). Although a covariance descriptor has been successfully applied to person re-id, it loses the local structure of a region and mean information of pixel features, both of which tend to be the major discriminative information for person re-id. In this paper, we present novel meta-descriptors based on a hierarchical Gaussian distribution of pixel features, in which both mean and covariance information are included in patch and region level descriptions. More specifically, the region is modeled as a set of multiple Gaussian distributions, each of which represents the appearance of a local patch. The characteristics of the set of Gaussian distributions are again described by another Gaussian distribution. Because the space of Gaussian distribution is not a linear space, we embed the parameters of the distribution into a point of Symmetric Positive Definite (SPD) matrix manifold in both steps. We show, for the first time, that normalizing the scale of the SPD matrix enhances the hierarchical feature representation on this manifold. Additionally, we develop feature norm normalization methods with the ability to alleviate the biased trends that exist on the SPD matrix descriptors. The experimental results conducted on five public datasets indicate the effectiveness of the proposed descriptors and the two types of normalizations.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2179	2194		10.1109/TPAMI.2019.2914686													
J								Learning Complexity-Aware Cascades for Pedestrian Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Complexity theory; Detectors; Boosting; Feature extraction; Proposals; Deep learning; Energy consumption; Real-time pedestrian detection; detector cascades; boosting; complexity constrained learning	BOOSTING ALGORITHMS; OBJECT	The problem of pedestrian detection is considered. The design of complexity-aware cascaded pedestrian detectors, combining features of very different complexities, is investigated. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of pedestrian detectors with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables accurate detection at fairly fast speeds.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2195	2211		10.1109/TPAMI.2019.2910514													
J								Learning More Universal Representations for Transfer-Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Task analysis; Visualization; Measurement; Semantics; Additives; Veins; Training; Universal representations; universality evaluation; transfer-learning; visual recognition	LEVEL	A representation is supposed universal if it encodes any element of the visual world (e.g., objects, scenes) in any configuration (e.g., scale, context). While not expecting pure universal representations, the goal in the literature is to improve the universality level, starting from a representation with a certain level. To improve that universality level, one can diversify the source-task, but it requires many additive annotated data that is costly in terms of manual work and possible expertise. We formalize such a diversification process then propose two methods to improve the universality of CNN representations that limit the need for additive annotated data. The first relies on human categorization knowledge and the second on re-training using fine-tuning. We propose a new aggregating metric to evaluate the universality in a transfer-learning scheme, that addresses more aspects than previous works. Based on it, we show the interest of our methods on 10 target-problems, relating to classification on a variety of visual domains.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2212	2224		10.1109/TPAMI.2019.2913857													
J								Multi-Source Causal Feature Selection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Feature extraction; Diseases; Training; Search problems; Reliability; Predictive models; Markov processes; Causal feature selection; Markov blanket; multiple datasets; Bayesian network; causal invariance	GENE-EXPRESSION; ADENOCARCINOMA; CLASSIFICATION; INFORMATION; RELEVANCE	Causal feature selection has attracted much attention in recent years, as the causal features selected imply the causal mechanism related to the class attribute, leading to more reliable prediction models built using them. Currently there is a need of developing multi-source feature selection methods, since in many applications data for studying the same problem has been collected from various sources, such as multiple gene expression datasets obtained from different experiments for studying the causes of the same disease. However, the state-of-the-art causal feature selection methods generally tackle a single dataset, and a direct application of the methods to multiple datasets will result in unreliable results as the datasets may have different distributions. To address the challenges, by utilizing the concept of causal invariance in causal inference, we first formulate the problem of causal feature selection with multiple datasets as a search problem for an invariant set across the datasets, then give the upper and lower bounds of the invariant set, and finally we propose a new Multi-source Causal Feature Selection algorithm, MCFS. Using synthetic and real world datasets and 16 feature selection methods, the extensive experiments have validated the effectiveness of MCFS.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2240	2256		10.1109/TPAMI.2019.2908373													
J								On Perfect Clustering of High Dimension, Low Sample Size Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Clustering algorithms; Indexes; Euclidean distance; Sociology; Statistics; Single photon emission computed tomography; Estimation; Dunn index; hierarchical clustering; high dimensional consistency; k-means clustering; pairwise distances; Rand index	LARGE NUMBERS; DATA SET; LAWS; PCA	Popular clustering algorithms based on usual distance functions (e.g., the Euclidean distance) often suffer in high dimension, low sample size (HDLSS) situations, where concentration of pairwise distances and violation of neighborhood structure have adverse effects on their performance. In this article, we use a new data-driven dissimilarity measure, called MADD, which takes care of these problems. MADD uses the distance concentration phenomenon to its advantage, and as a result, clustering algorithms based on MADD usually perform well for high dimensional data. We establish it using theoretical as well as numerical studies. We also address the problem of estimating the number of clusters. This is a challenging problem in cluster analysis, and several algorithms are available for it. We show that many of these existing algorithms have superior performance in high dimensions when they are constructed using MADD. We also construct a new estimator based on a penalized version of the Dunn index and prove its consistency in the HDLSS asymptotic regime. Several simulated and real data sets are analyzed to demonstrate the usefulness of MADD for cluster analysis of high dimensional data.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2257	2272		10.1109/TPAMI.2019.2912599													
J								Properties of Mean Shift	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Kernel; Probability density function; Convergence; Clustering algorithms; Estimation; Bandwidth; Trajectory; Mode estimation; mode clustering; mean shift algorithm; conditional mean shift algorithm; subspace constrained mean shift algorithm	DENSITY RIDGES; CONVERGENCE; ALGORITHM; MODE	We study properties of the mean shift (MS)-type algorithms for estimating modes of probability density functions (PDFs), via regarding these algorithms as gradient ascent on estimated PDFs with adaptive step sizes. We rigorously prove convergence of mode estimate sequences generated by the MS-type algorithms, under the assumption that an analytic kernel function is used. Moreover, our analysis on the MS function finds several new properties of mode estimate sequences and corresponding density estimate sequences, including the result that in the MS-type algorithm using a Gaussian kernel the density estimate monotonically increases between two consecutive mode estimates. This implies that, in the one-dimensional case, the mode estimate sequence monotonically converges to the stationary point nearest to an initial point without jumping over any stationary point.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2273	2286		10.1109/TPAMI.2019.2913640													
J								The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Semantics; Feature extraction; Mathematical model; Cognition; Natural languages; Deep learning; Math word problem; semantic parser; reasoning; survey; natural language processing; machine learning	EFFICIENT; COMPUTER; GAME; GO	Solving mathematical word problems (MWPs) automatically is challenging, primarily due to the semantic gap between human-readable words and machine-understandable logics. Despite the long history dated back to the 1960s, MWPs have regained intensive attention in the past few years with the advancement of Artificial Intelligence (AI). Solving MWPs successfully is considered as a milestone towards general AI. Many systems have claimed promising results in self-crafted and small-scale datasets. However, when applied on large and diverse datasets, none of the proposed methods in the literature achieves high precision, revealing that current MWP solvers still have much room for improvement. This motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2287	2305		10.1109/TPAMI.2019.2914054													
J								Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Language Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Hidden Markov models; Assistive technology; Gesture recognition; Synchronization; Shape; Supervised learning; Speech recognition; Weakly supervised learning; hybrid CNN-LSTM-HMMs; continuous sign language recognition; lip reading; hand shape recognition	HIDDEN MARKOV-MODELS; RECOGNITION; CLASSIFICATION	In this work we present a new approach to the field of weakly supervised learning in the video domain. Our method is relevant to sequence learning problems which can be split up into sub-problems that occur in parallel. Here, we experiment with sign language data. The approach exploits sequence constraints within each independent stream and combines them by explicitly imposing synchronisation points to make use of parallelism that all sub-problems share. We do this with multi-stream HMMs while adding intermediate synchronisation constraints among the streams. We embed powerful CNN-LSTM models in each HMM stream following the hybrid approach. This allows the discovery of attributes which on their own lack sufficient discriminative power to be identified. We apply the approach to the domain of sign language recognition exploiting the sequential parallelism to learn sign language, mouth shape and hand shape classifiers. We evaluate the classifiers on three publicly available benchmark data sets featuring challenging real-life sign language with over 1,000 classes, full sentence based lip-reading and articulated hand shape recognition on a fine-grained hand shape taxonomy featuring over 60 different hand shapes. We clearly outperform the state-of-the-art on all data sets and observe significantly faster convergence using the parallel alignment approach.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2306	2320		10.1109/TPAMI.2019.2911077													
J								Bound and Conquer: Improving Triangulation by Enforcing Consistency	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Cameras; Uncertainty; Biological system modeling; Matrix decomposition; Geometry; Computational modeling; Multi-camera imaging; multiple-view geometry; triangulation	PRACTICAL GLOBAL OPTIMIZATION; OVERCOMPLETE EXPANSIONS; QUANTIZATION; POLYTOPES	We study the accuracy of triangulation in multi-camera systems with respect to the number of cameras. We show that, under certain conditions, the optimal achievable reconstruction error decays quadratically as more cameras are added to the system. Furthermore, we analyze the error decay-rate of major state-of-the-art algorithms with respect to the number of cameras. To this end, we introduce the notion of consistency for triangulation, and show that consistent reconstruction algorithms achieve the optimal quadratic decay, which is asymptotically faster than some other methods. Finally, we present simulations results supporting our findings. Our simulations have been implemented in MATLAB and the resulting code is available in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2939530.																	0162-8828	1939-3539				SEPT 1	2020	42	9					2321	2326		10.1109/TPAMI.2019.2939530													
J								Human grasp position estimation for human-robot cooperative object manipulation	ROBOTICS AND AUTONOMOUS SYSTEMS										Physical human-robot interaction; Collaborative robots; Object manipulation; Estimation		This paper addresses the problem of human grasp position estimation in a physical human-robot object handling scenario. The problem is formulated as a linear regression by considering the human grasp position and their exerted torque as unknown parameters. We propose a modified least-squares algorithm to estimate the parameters by evaluating the quality of the estimates based on the assumption that the parameters should remain constant for a period of time. The solution is model-agnostic in terms of the human force/torque model - requiring only force/torque measurements on the robot side and proprioception - and is model-based in terms of the object model. The proposed grasp position estimation method is compared statistically with a conventional contact point estimation method using the collected experimental data. Moreover, the performance of the developed method is evaluated through various scenarios of physical human-robot interaction. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103600	10.1016/j.robot.2020.103600													
J								Unsupervised semantic clustering and localization for mobile robotics tasks	ROBOTICS AND AUTONOMOUS SYSTEMS										Topological mapping; Illumination invariance; Community detection; Robot localization	ROBUST PLACE RECOGNITION; ALGORITHM; VISION; MAPS	Due to its vast applicability, the semantic interpretation of regions or entities increasingly attracts the attention of scholars within the robotics community. The paper at hand introduces a novel unsupervised technique to semantically identify the position of an autonomous agent in unknown environments. When the robot explores a certain path for the first time, community detection is achieved through graph-based segmentation. This allows the agent to semantically define its surroundings in future traverses even if the environment's lighting conditions are changed. The proposed semantic clustering technique exploits the Louvain community detection algorithm, which constitutes a novel and efficient method for identifying groups of measurements with consistent similarity. The produced communities are combined with metric information, as provided by the robot's odometry through a hierarchical agglomerative clustering method. The suggested algorithm is evaluated in indoors and outdoors datasets creating topological maps capable of assisting semantic localization. We demonstrate that the system categorizes the places correctly when the robot revisits an environment despite the possible lighting variation. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103567	10.1016/j.robot.2020.103567													
J								Advanced mapping robot and high-resolution dataset	ROBOTICS AND AUTONOMOUS SYSTEMS										Mobile robot; Sensor synchronization; Sensor calibration; Robotic datasets; Simultaneous Localization and Mapping (SLAM)	DATA SET; LOCALIZATION; VISION; LIDAR; SLAM; CALIBRATION; PERCEPTION; NAVIGATION; ODOMETRY; ROBUST	This paper presents a fully hardware synchronized mapping robot with support for a hardware synchronized external tracking system, for super-precise timing and localization. Nine high-resolution cameras and two 32-beam 3D Lidars were used along with a professional, static 3D scanner for ground truth map collection. With all the sensors calibrated on the mapping robot, three datasets are collected to evaluate the performance of mapping algorithms within a room and between rooms. Based on these datasets we generate maps and trajectory data, which is then fed into evaluation algorithms. We provide the datasets for download and the mapping and evaluation procedures are made in a very easily reproducible manner for maximum comparability. We have also conducted a survey on available robotics-related datasets and compiled a big table with those datasets and a number of properties of them. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103559	10.1016/j.robot.2020.103559													
J								A lobster-inspired articulated shaft for minimally invasive surgery	ROBOTICS AND AUTONOMOUS SYSTEMS										Soft pneumatic actuation; Hybrid robots; Minimally invasive surgery; Soft robotics	ROBOTIC JOINT; SOFT; STIFFNESS; DRIVEN; SENSOR; SAFE; HAND	Novel applications of soft pneumatic actuation in minimally invasive surgery (MIS) are proposed due to its relatively safe robot-environment interactions. Although the inherent compliance of soft robots makes them suitable for surgery, their low force output and complicated system response and behavior may limit their potential as practical MIS instruments. In this paper, three lobster-inspired antagonistic modules are proposed to realize bidirectional translational, bending and rotational motions and variable stiffness in centimeter scale. Their modular design enables flexible combinations of articulated shafts to satisfy end-effector workspace requirements in MIS. Theoretical models are proposed to relate the input pressure, deformation, output force/torque and stiffness, which provide quantitative solutions for independent adjustment on the deformation and stiffness of each module. A series of experimental results show that the proposed modules can deliver sufficient force and torque output for MIS applications, and they can be conveniently assembled into articulated shafts featuring safe actuation, high dexterity, stiffness tuning and reconfigurability. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103599	10.1016/j.robot.2020.103599													
J								Vision-based posture-consistent teleoperation of robotic arm using multi-stage deep neural network	ROBOTICS AND AUTONOMOUS SYSTEMS										Visual teleoperation; Deep neural networks; Human-robot posture-consistent mapping; Data generator		This paper proposes a visual teleoperation with human-robot posture-consistent based on deep neural network. A multi-stage structure of visual teleoperation network, in which the angles of robotic joints are obtained from human, is deduced. Furthermore, a novel human-robot posture-consistent mapping method is developed to generate dataset of the visual teleoperation network by solving constrained nonlinear matrix functions. Based on the designed framework, the data generator and a well trained multi-stage visual teleoperation network are presented. Finally teleoperation experiments are implemented to demonstrate that the proposed method is effectiveness and reliable. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103592	10.1016/j.robot.2020.103592													
J								Coordination of thrusters, reaction wheels, and arm in orbital robots	ROBOTICS AND AUTONOMOUS SYSTEMS										Space robotics; Thrusters; Reaction wheels; Stability analysis; Nonlinear control; Hardware-in-the-loop	SPACE MANIPULATORS; DYNAMICS; MOTION; KINEMATICS; DESIGN	A fuel-efficient control strategy for a manipulator-equipped spacecraft is presented. The strategy uses the thrusters, the reaction wheels, and the arm drives in a coordinated way to limit the use of the thrusters and achieve ideally zero fuel consumption in contact-free maneuvering. The thrusters are activated automatically only after contact, to stabilize the inertial motion of the system. The controller regulates the translation of the center-of-mass (CoM) of the whole space robot, the rotation of the spacecraft, and the pose of the end-effector (EE) in a decoupled way, utilizing the thrusters to control the CoM translation only and the remaining actuators to control the rotation and end-effector coordinately. The method is validated experimentally using a hardware-in-the-loop simulator composed of a seven degrees-of-freedom (DOF) arm mounted on a 6DOF simulated spacecraft. Numerical simulations with discrete thrusters assess the fuel efficiency of the proposed strategy. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103564	10.1016/j.robot.2020.103564													
J								Monocular vision-based gripping of objects	ROBOTICS AND AUTONOMOUS SYSTEMS										Underwater robotics; Object detection; Autonomy; Dynamic positioning; Manipulator	UNDERWATER; MAINTENANCE; INSPECTION	Optics-based systems may provide high spatial and temporal resolution for close range object detection in underwater environments. By using a monocular camera on a low cost underwater vehicle manipulator system, objects can be tracked by the vehicle and handled by the manipulator. In this paper, a monocular camera is used to detect an object of interest through object detection. Spatial features of the object are extracted, and a dynamic positioning system is designed for the underwater vehicle in order for it to maintain a desired position relative to the object. A manipulator mounted under the vehicle is used to retrieve the object through a developed kinematic control system. Experimental tests verify the proposed methodology. A stability analysis proves asymptotic stability properties for the chosen sliding mode controller and exponential stability for the task error. (C) 2020 The Author(s). Published by Elsevier B.V.																	0921-8890	1872-793X				SEP	2020	131								103589	10.1016/j.robot.2020.103589													
J								Vehicle tracking with Kalman filter using online situation assessment	ROBOTICS AND AUTONOMOUS SYSTEMS										Vehicle tracking; Kalman filter; Situation assessment	NETWORK	Vehicle tracking is an attractive problem in the field of public transportation with several research projects conducted using Kalman filter (KF) to tackle this. While a driver may act on his own decision, there exist parameters affecting his behavior so called situation assessment such as neighboring drivers, possible obstacles, or alternative routes changing over time. In this paper, utilizing online situation assessment (SA) inside Kalman filter is studied. Motion History Graph is used as online modeling of the history of the vehicle motions and is used to augment the estimation. Experimental results on video sequences from different datasets show an average 25 percent performance improvement when using online SA inside KF. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103596	10.1016/j.robot.2020.103596													
J								A smart mobile robot commands predictor using recursive neural network	ROBOTICS AND AUTONOMOUS SYSTEMS										Virtual simulator; Smart navigation; Mobile robot; RNNC; Prediction controller; TurtleBot		Autonomous navigation of mobile robot via classic neural network (NN) models are no more valid in terms of efficiency and accuracy due to the development of new advanced techniques. However, the necessity of finding an implementable Recursive Neural Network (RNN) model to predict the motor control of the robot with both speed and accuracy constraints still remains stagnant because of the nonlinearity and complexity of the trajectories. To provide new solutions for smart navigation problems, this paper proposes a new implementable recursive neural network controller (RNNC) predictor that calculates the Pulse Width Modulation (PMW) signals of the motors. Such proposed Multi-input Multi-output (MIMO) Controller succeeded to solve the problem of speed and accuracy of autonomous navigation. The Smart RNNC model design is illustrated with its architecture in details. Due to the complexity and the non-efficiency of the training process in real-world, a 3D Simulator was developed to create all possible scenarios. The machine learning and navigation predictions processes for designing the new RNNC model are presented together in details. In addition, the motor commands generation speed and accuracy as well as their efficiency are theoretically and practically proven. Moreover, numerical studies, 3D scenarios of trajectory tracking and obstacle avoidance prove the effectiveness and robustness of the proposed technique. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103593	10.1016/j.robot.2020.103593													
J								Planning the sequence of tasks for harvesting robots	ROBOTICS AND AUTONOMOUS SYSTEMS										Harvesting robot; Task sequencing; Traveling salesman problem; Sweet pepper; Agriculture robotics	AGRICULTURAL ROBOTS; CONTROL FRAMEWORK	A methodology for planning the sequence of tasks for a harvesting robot is presented. The fruit targets are situated at unknown locations and must be detected by the robot through a sequence of sensing tasks. Once the targets are detected, the robot must execute a harvest action at each target location. The traveling salesman paradigm (TSP) is used to plan the sequence of sensing and harvesting tasks taking into account the costs of the sensing and harvesting actions and the traveling times. Sensing is planned online. The methodology is validated and evaluated in both laboratory and greenhouse conditions for a case study of a sweet pepper harvesting robot. The results indicate that planning the sequence of tasks for a sweet pepper harvesting robot results in 12% cost reduction. Incorporating the sensing operation in the planning sequence for fruit harvesting is a new approach in fruit harvesting robots and is important for cycle time reduction. Furthermore, the sequence is re-planned as sensory information becomes available and the costs of these new sensing operations are also considered in the planning. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103591	10.1016/j.robot.2020.103591													
J								Robust image completion and masking with application to robotic bin picking	ROBOTICS AND AUTONOMOUS SYSTEMS										Generative adversarial network; Latent space association network; Image completion; Image masking; Robotic bin picking		Automated image completion and masking have been emerged as a subject of keen interest due to their impact on image modification and interpretation. The current state-of-the-art approaches require a fixed format of missing parts and are ineffective for handling corrupted images. Besides, they focus exclusively on the image completion without taking into consideration the image masking as an inverse process of completion. This paper proposes a deep learning approach to an integrated framework of image completion and masking based on the cross-mapping generative adversarial network or CM-GAN, in short. CM-GAN offers the robustness in image completion under corruptions as well as the capability of synthesizing various masked images with arbitrary mask locations and shapes. In particular, the capability of CM-GAN in image masking is shown to be extended into the removal of unwanted backgrounds in images. We verify the superior performance of CM-GAN for image completion and masking based on extensive experiments. Furthermore, we implement a deep learning based robotic bin picking to demonstrate that the background removal capability of CM-GAN plays a key role for estimating the 3D pose of randomly filed multiple industrial parts in a bin. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103563	10.1016/j.robot.2020.103563													
J								Real-time topological localization using structured-view ConvNet with expectation rules and training renewal	ROBOTICS AND AUTONOMOUS SYSTEMS												Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103578	10.1016/j.robot.2020.103578													
J								Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving	ROBOTICS AND AUTONOMOUS SYSTEMS										Autonomous driving; Decision making; Interpretability; Reinforcement learning; Parameter-based exploration		The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103568	10.1016/j.robot.2020.103568													
J								Obstacle avoidance in dynamic environments based on velocity space optimization	ROBOTICS AND AUTONOMOUS SYSTEMS										Collision avoidance; Dynamic environment; Robot motion control; Reactive control	NAVIGATION	Robotic obstacle avoidance is an important issue in robotic navigation for unknown or partially known, dynamic environments. A good number of techniques have already been proposed to navigate obstacles in this kind of environment. They include a series of velocity space methods that have been successful implemented in several applications. They formulate the problem as one of constrained optimization in the velocity space of the robot. The constraints include the obstacles in the environment assuming they are static. In this paper, we present an efficient, real-time method (BCM-DO) to include the restrictions imposed by dynamic objects. The optimization function has also been adapted to include these new restrictions. The new function is evaluated in two sets of points. A first set is obtained from a coarse sampling in the reachable window of velocities and a second set is selected in the limits of each curvature interval to avoid missing small openings between static objects. The whole system has first been extensively tested in several simulated robots and finally applied to a hotel assistant robot (BellBot) resulting in an efficient, real-time obstacle avoidance method that produces smooth and reliable routes. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103569	10.1016/j.robot.2020.103569													
J								Safeguarding against prefix interception attacks via online learning	ROBOTICS AND AUTONOMOUS SYSTEMS										Online learning; Prefix interception; Routing attacks; Secure route	OPTIMIZATION; HIJACKING; ROBOTICS; PROTOCOL; INTERNET	In human-robot cooperation, the information interaction plays a key role. Most of the information interaction rely on Border Gateway Protocol (BGP), which is a vital route protocol on networks. However, the BGP is susceptible to the prefix interception attacks because the rightful origin of each prefix cannot be verified in BGP. For this reason, we propose a novel and effective route selection method against prefix interception attacks, which combines the resilience of routers and the historical performance of routers to choose a secure route. Moreover, we estimate the performance of BGP by introducing the definition of resilience and the historical performance of routers via online learning against the prefix interception attack. Furthermore, we analyze the bound of regret and obtain O(root T) regret, where T denotes the time horizon. In addition, the proposed method is verified both on synthetic data and network simulations. The results show that the proposed method has more resilience against prefix interception attacks than Counter-Raptor. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103556	10.1016/j.robot.2020.103556													
J								Visual-inertial teach and repeat	ROBOTICS AND AUTONOMOUS SYSTEMS										UAV; Embedded; Navigation; Visual-inertial; Stereo; Teach-and-replay; Relative	SLAM	Teach and Repeat (T&R) refers to the technology that allows a robot to autonomously follow a previously traversed route, in a natural scene and using only its onboard sensors. In this paper we present a Visual-Inertial Teach and Repeat (VI-T&R) algorithm that uses stereo and inertial data and targets Unmanned Aerial Vehicles with limited on-board computational resources. We propose a tightly-coupled relative formulation of the visual-inertial constraints that is tailored to the T&R application. In order to achieve real-time operation on limited hardware, we reduce the problem to motion-only visual-inertial Bundle Adjustment. In the repeat stage, we detail how to generate a trajectory and smoothly follow it with a constantly changing relative frame. The proposed method is validated in simulated environments, using real sensor data from the public EuRoC dataset, and using our own robotic setup and closed-loop control. Our experimental results demonstrate high accuracy and real-time performance both on a standard desktop system and on a low-cost Odroid X-U4 embedded computer. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103577	10.1016/j.robot.2020.103577													
J								TDOA based localization and its application to the initialization of LiDAR based autonomous robots	ROBOTICS AND AUTONOMOUS SYSTEMS										Squared range difference-based robot localization; TDOA; Least squares; LiDAR; Scan matching; Initialization		This work considers the problem of locating a single robot given a set of squared noisy range difference measurements to a set of points (anchors) whose positions are known. In the sequel, localization problem is solved in the Least-Squares (LS) sense by writing the robot position in polar/spherical coordinates. This representation transforms the original nonconvex/multimodal cost function into the quotient of two quadratic forms, whose constrained maximization is more tractable than the original problem. Simulation results indicate that the proposed method has similar accuracy to state-of-the-art optimization-based localization algorithms in its class, and the simple algorithmic structure and computational efficiency makes it appealing for applications with strong computational constraints. Additionally, location information is used to find the initial orientation of the robot with respect to the previously obtained map in scan matching. Thus, the crucial problem of the autonomous initialization and localization in robotics is solved. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103590	10.1016/j.robot.2020.103590													
J								A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach	ROBOTICS AND AUTONOMOUS SYSTEMS										Extrinsic calibration; ROS; Optimization; Bundle adjustment; Intelligent vehicles; OpenCV	SELF-CALIBRATION; CAMERAS	This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103558	10.1016/j.robot.2020.103558													
J								Appearance-invariant place recognition by adversarially learning disentangled representation	ROBOTICS AND AUTONOMOUS SYSTEMS										Visual place recognition; Changing environment; Adversarial learning; Representation disentanglement		Place recognition is an essential component to address the problem of visual navigation and SLAM. The long-term place recognition is challenging as the environment exhibits significant variations across different times of the days, months, and seasons. In this paper, we view appearance changes as multiple domains and propose a Feature Disentanglement Network (FDNet) based on a convolutional auto-encoder and adversarial learning to extract two independent deep features - content and appearance. In our network, the content feature is learned which only retains the content information of images through the competition with the discriminators and content encoder. Besides, we utilize the triplets loss to make the appearance feature encode the appearance information. The generated content features are directly used to measure the similarity of images without dimensionality reduction operations. We use datasets that contain extreme appearance changes to carry out experiments, which show how meaningful recall at 100% precision can be achieved by our proposed method where existing state-of-art approaches often get worse performance. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103561	10.1016/j.robot.2020.103561													
J								Human-friendly control system design for two-wheeled service robot with optimal control approach	ROBOTICS AND AUTONOMOUS SYSTEMS										Autonomous mobile service robot; Nonlinear model predictive control; Human-friendly control system	DYNAMIC WINDOW APPROACH; MOBILE ROBOT; NAVIGATION; MODEL; TRACKING; WALKING; PEOPLE; MOTION	This paper proposes a novel control system design for a two-wheeled service robot that follows a person as an assistant without knowing the person's destination. For this kind of service robot, the key skill is to realize human-friendly movement. However, appropriate motion always changed depending on the situation. For instance, when the robot is close and person turns toward it, it is important to suppress the robot's acceleration. Likewise, if the person turns away from the robot, the robot should maintain its position within an appropriate area. Therefore, to deal with various required movements, our control system is able to change its properties automatically and suitably depending on the situation by using weights of the cost function in nonlinear model predictive control (NMPC) as a function of the relative distance between the person and the robot. Unlike previous methods, our design includes only one controller. Consequently, we are able to take into account system stability. Moreover, owing to proposing in NMPC framework, it is easy to extend our method by adopting other recognition or goal-setting methods. We conducted simulations using actual human walking data taken by the robot's laser range sensors. The experiments demonstrate that the robot can follow a person who performs U-turn, confirming that our method can produce human-friendly robot movement in a practical scene. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103562	10.1016/j.robot.2020.103562													
J								Planning the trajectory of an autonomous wheel loader and tracking its trajectory via adaptive model predictive control	ROBOTICS AND AUTONOMOUS SYSTEMS										Automatic drive; Wheel loader; Trajectory planning; Trajectory tracking; Model predictive control	PERFORMANCE; NAVIGATION; SIMULATION	In a typical operation mode, a wheel loader frequently accelerates and decelerates, and the curvature of the driving path is inconsistent. In the past, autonomous vehicle trajectory planning has not considered the related changes in the velocity of the vehicle. Therefore, the trajectory tracking control process has seldom considered the impact of curving paths on the trajectory tracking performance. To address these problems, this study evaluated an autonomous wheel loader based on the trajectory of its non-uniform driving motion and constructed an adaptive model predictive control (AMPC) trajectory tracking system that considers disturbances in the path curvature. The trajectory of the autonomous wheel loader was then tracked using the proposed AMPC system with a planned non-uniform motion trajectory as the target. Its performance was then compared with that of a conventional model predictive control (MPC) trajectory tracking system that does not consider any path curvature disturbances. The maximum displacement error and heading error obtained by the proposed AMPC system were found to be 65.7% and 60%, respectively, smaller than those obtained by the MPC system. The desired trajectory can also be tracked well under different curvature amplitudes using the AMPC trajectory tracking system, ensuring active safety performance of an autonomous wheel loader in the process of trajectory tracking. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103570	10.1016/j.robot.2020.103570													
J								Trajectory coordination for a cooperative multi-manipulator system and dynamic simulation error analysis	ROBOTICS AND AUTONOMOUS SYSTEMS										Multi-manipulator; Track coordination; Dynamic simulation	MODELS; ROBOT	To achieve temporal and spatial correspondence between multiple robotic manipulators, the system must correctly analyze the coordinated path required for a specific task. Based on manipulator kinematics analysis, we first studied the kinematic constraints between the end-effectors of cooperative manipulators, and deduced the multi-manipulator cooperative kinematics constraint equations in the Cartesian coordinate system space under different motion modes. Then, we created two MD-6 manipulator models to simulate the trajectory simulation of the synchronous and relative motion of the manipulator. This allowed us to verify the correctness of the proposed trajectory coordination method, and analyze the influence of external loads on the position and posture of the end-effector of the manipulator, to effectively predict the cooperative motion error of the manipulator system. Finally, in order to verify the effectiveness of the proposed trajectory coordination method, we established a robotic experimental platform and conducted experimental research. The results show that the multi-manipulator trajectory coordination method studied in this paper can make multi-manipulators effectively achieve the target requirements of tasks such as time and space cooperative handling and circular drawing operations. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103588	10.1016/j.robot.2020.103588													
J								A spiking network classifies human sEMG signals and triggers finger reflexes on a robotic hand	ROBOTICS AND AUTONOMOUS SYSTEMS										Neurorobotics; Human-robot-interaction; Neural control system; Humanoid robot; Motion representation; sEMG classification; Spiking neural networks; Anthropomorphic robot hand		The interaction between robots and humans is of great relevance for the field of neurorobotics as it can provide insights on how humans perform motor control and sensor processing and on how it can be applied to robotics. We propose a spiking neural network (SNN) to trigger finger motion reflexes on a robotic hand based on human surface Electromyography (sEMG) data. The first part of the network takes sEMG signals to measure muscle activity, then classify the data to detect which finger is being flexed in the human hand. The second part triggers single finger reflexes on the robot using the classification output. The finger reflexes are modeled with motion primitives activated with an oscillator and mapped to the robot kinematic. We evaluated the SNN by having users wear a noninvasive sEMG sensor, record a training dataset, and then flex different fingers, one at a time. The muscle activity was recorded using a Myo sensor with eight different channels. The sEMG signals were successfully encoded into spikes as input for the SNN. The classification could detect the active finger and trigger the motion generation of finger reflexes. The SNN was able to control a real Schunk SVH 5-finger robotic hand online. Being able to map myo-electric activity to functions of motor control for a task, can provide an interesting interface for robotic applications, and a platform to study brain functioning. SNN provide a challenging but interesting framework to interact with human data. In future work the approach will be extended to control also a robot arm at the same time. (C) 2020 The Author(s). Published by Elsevier B.V.																	0921-8890	1872-793X				SEP	2020	131								103566	10.1016/j.robot.2020.103566													
J								Coupled task scheduling for heterogeneous multi-robot system of two robot types performing complex-schedule order fulfillment tasks	ROBOTICS AND AUTONOMOUS SYSTEMS										Task scheduling; Heterogeneous multi-robot system; Complex-schedule constraints; Block sequence graph	OPEN-SHOP PROBLEM; SERVICE AGENT; ALGORITHMS; ALLOCATION; TAXONOMY; TIME	This paper addresses multi-robot task scheduling for two robot types arising from heterogeneous robotic order fulfillment systems. The heterogeneous multi-robot system comprises two types of robots with specialized and complementary capabilities to achieve long-cycle and multi-station order fulfillment tasks on a logistic network. This problem is extremely challenging because of innate complex-schedule constraints of tasks and coupled temporal-spatial relations between all robots. After set-theoretic and mixed integer linear programming problem formulations, we use coupled approach, instead of decoupled approaches to explore the synergy between heterogeneous robots, which is different from most existing similar works. To model the structural (complex-schedule) and quantitative (temporal-spatial) coupledness of robots' time-extended task schedules, an edge-weighted and vertex-weighted block sequence graph is introduced. Based on this model, time-extended task scheduling is achieved using rank-minimal heuristic and genetic algorithm metaheuristic. Theoretically, this model is complete and non-redundant. Empirically, compared with decoupled approach, optimality and efficiency of the proposed methods are evaluated on designed instances. The results demonstrate that coupled methods can achieve near-optimal solutions with higher performance ratio than decoupled methods in moderate time. At the same time, coupled methods can leverage spatial and temporal properties of miscellaneous tasks, and balance instantaneous and time-extended decisions to achieve tight collective synergy in the long run. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103560	10.1016/j.robot.2020.103560													
J								Fixed-Wing UAVs flocking in continuous spaces: A deep reinforcement learning approach	ROBOTICS AND AUTONOMOUS SYSTEMS										Fixed-wing UAV; Flocking; Reinforcement learning; Actor-critic	OBSTACLE AVOIDANCE; AGENTS; CONSENSUS; NETWORKS; SYSTEMS	Fixed-Wing UAVs (Unmanned Aerial Vehicles) flocking is still a challenging problem due to the kinematics complexity and environmental dynamics. In this paper, we solve the leader-followers flocking problem using a novel deep reinforcement learning algorithm that can generate roll angle and velocity commands by training an end-to-end controller in continuous state and action spaces. Specifically, we choose CACLA (Continuous Actor-Critic Learning Automation) as the base algorithm and we use the multi-layer perceptron to represent both the actor and the critic. Besides, we further improve the learning efficiency by using the experience replay technique that stores the training data in the experience memory and samples from the memory as needed. We have compared the performance of the proposed CACER (Continuous Actor-Critic with Experience Replay) algorithm with benchmark algorithms such as DDPG and double DQN in numerical simulation, and we have demonstrated the performance of the learned optimal policy in semi-physical simulation without any parameter tuning. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103594	10.1016/j.robot.2020.103594													
J								An efficient RRT cache method in dynamic environments for path planning	ROBOTICS AND AUTONOMOUS SYSTEMS										Dynamic obstacle avoidance; Path planning; Multiple-query; Cache information; ROS	OBSTACLE AVOIDANCE; ROBOT; MOTION	This paper is concentrated on path planning for robots working in a dynamic environment to satisfy real-time needs. An efficient bias-goal factor RRT (EBG-RRT), which is multiple-query sampling-based replanning algorithm, is proposed with rapid response and high success rate. Specifically, a relay node method is proposed to get a position where the robot and dynamic obstacles will be no-collision and help robots to move without suspended. Based on the relay node method, Connection strategy performs minimal modifications to maintain the interrupted path. In order to overcome the short of Waypoint Cache method, an efficient and optimal Waypoint Cache (EOWC) method is proposed to make use of potential cache information and find an optimal path to repair. The EOWC method is combined with the BG-RRT algorithm according to the iterative characteristics. Finally, the EBG-RRT algorithm is verified on ROS with Aubo-i5 manipulator. Simulation results provide the EBG-RRT algorithm is outperformed both in static and dynamic environments. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103595	10.1016/j.robot.2020.103595													
J								Rapidly-exploring Random Trees multi-robot map exploration under optimization framework	ROBOTICS AND AUTONOMOUS SYSTEMS										Rapidly-exploring Randomized Trees; Multiple robots; Map exploration; Optimization framework	MODELING RESOURCE CONTENTION; ALGORITHMS	Rapidly-exploring Randomized Trees (RRT) is a kind of probabilistically complete exploration algorithm based on the tree structure. It has been widely used in the robotic navigation since it guarantees the complete discovery and the exploration of environment maps through robots. In the present study, the RRT algorithm is extended to propose an optimization-based map exploration strategy for multiple robots to actively explore and build environment maps. The present study adopts a market-based task allocation strategy, which to maximize the profit, for the coordination between robots. In the extension of the RRT, the cost function consists the unknown region and the passed unknown region. The unknown region is explored for a given frontier point, while the passed unknown region is the area, where the robot moves towards the target frontier point. When the robot moves from the start position to the target frontier point, the trajectory length is defined as a constraint for the optimization. The main contributions of the present study can be summarized in optimizing the frontier points, defining a new task allocation strategy and applying different evaluation rules, including the running time and the trajectory length. These rules are applied to explore the multi-robot map in simulated and practical environments. Then the Robot Operating System (ROS) is utilized to evaluate the application of the proposed exploration strategy on Turtlebots in a 270 m(2) room. Obtained results from the simulation and the experiment demonstrate that the proposed method outperforms the Umari's approach from both the running time and the trajectory length aspects. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				SEP	2020	131								103565	10.1016/j.robot.2020.103565													
J								The distortion of distributed voting	ARTIFICIAL INTELLIGENCE										Distributed voting; District-based elections; Distortion	SOCIAL CHOICE FUNCTIONS	Voting can abstractly model any decision-making scenario and as such it has been extensively studied over the decades. Recently, the related literature has focused on quantifying the impact of utilizing only limited information in the voting process on the societal welfare for the outcome, by bounding the distortion of voting rules. Even though there has been significant progress towards this goal, almost all previous works have so far neglected the fact that in many scenarios (like presidential elections) voting is actually a distributed procedure. In this paper, we consider a setting in which the voters are partitioned into disjoint districts and vote locally therein to elect local winning alternatives using a voting rule; the final outcome is then chosen from the set of these alternatives. We prove tight bounds on the distortion of well-known voting rules for such distributed elections both from a worst-case perspective as well as from a best-case one. Our results indicate that the partition of voters into districts leads to considerably higher distortion, a phenomenon which we also experimentally showcase using real-world data. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103343	10.1016/j.artint.2020.103343													
J								Boolean algebras of conditionals, probability and logic	ARTIFICIAL INTELLIGENCE										Conditional probability; Conditional events; Boolean algebras; Preferential consequence relations	INFERENCE	This paper presents an investigation on the structure of conditional events and on the probability measures which arise naturally in that context. In particular we introduce a construction which defines a (finite) Boolean algebra of conditionals from any (finite) Boolean algebra of events. By doing so we distinguish the properties of conditional events which depend on probability and those which are intrinsic to the logico-algebraic structure of conditionals. Our main result provides a way to regard standard two-place conditional probabilities as one-place probability functions on conditional events. We also consider a logical counterpart of our Boolean algebras of conditionals with links to preferential consequence relations for non-monotonic reasoning. The overall framework of this paper provides a novel perspective on the rich interplay between logic and probability in the representation of conditional knowledge. (C) 2020 The Authors. Published by Elsevier B.V.																	0004-3702	1872-7921				SEP	2020	286								103347	10.1016/j.artint.2020.103347													
J								On the limits of forgetting in Answer Set Programming	ARTIFICIAL INTELLIGENCE										Forgetting; Answer Set Programming; Strong equivalence; Relativized equivalence; Computational complexity	KNOWLEDGE REPRESENTATION; LOGIC PROGRAMS; PERSISTENCE; MODULARITY; INFERENCE; SEMANTICS	Selectively forgetting information while preserving what matters the most is becoming an increasingly important issue in many areas, including in knowledge representation and reasoning. Depending on the application at hand, forgetting operators are defined to obey different sets of desirable properties. It turns out that, of the myriad of desirable properties discussed in the context of forgetting in Answer Set Programming, strong persistence, which imposes certain conditions on the correspondence between the answer sets of the program pre- and post-forgetting, and a certain independence from non-forgotten atoms, seems to best capture its essence, and be desirable in general. However, it has remained an open problem whether it is always possible to forget a set of atoms from a program while obeying strong persistence. In this paper, we investigate the limits of forgetting in Answer Set Programming. After showing that it is not always possible to forget a set of atoms from a program while obeying this property, we move forward and precisely characterize what can and cannot be forgotten from a program, by presenting a necessary and sufficient criterion. This characterization allows us to draw some important conclusions regarding the existence of forgetting operators for specific classes of logic programs, to characterize the class of forgetting operators that achieve the correct result whenever forgetting is possible, and investigate the related question of determining what we can forget from some specific logic program. Subsequently, we address the issue of what to do when we must forget a set of atoms, but cannot without violating this property. To this end, we investigate three natural alternatives to forget when forgetting without violating strong persistence is not possible, which turn out to correspond to the different natural possible relaxations of the characterization of strong persistence. Additionally, before concluding, we address computational complexity issues - namely of checking whether the novel criterion holds and whether a certain program is a result according to the different classes of forgetting operators we introduce - and discuss the related literature. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103307	10.1016/j.artint.2020.103307													
J								Dynamic term-modal logics for first-order epistemic planning	ARTIFICIAL INTELLIGENCE										Epistemic planning; Planning formalisms; Multi-agent systems; Term-modal logic; Dynamic epistemic logic	FRAGMENT	Many classical planning frameworks are built on first-order languages. The first-order expressive power is desirable for compactly representing actions via schemas, and for specifying quantified conditions such as -there exists xblocks_door(x). In contrast, several recent epistemic planning frameworks are built on propositional epistemic logic. The epistemic language is useful to describe planning problems involving higher-order reasoning or epistemic goals such as K-a-problem. This paper develops a first-order version of Dynamic Epistemic Logic (DEL). In this framework, for example, there exists xKx there exists yblocks_door(y) is a formula. The formalism combines the strengths of DEL (higher-order reasoning) with those of first-order logic (lifted representation) to model multi-agent epistemic planning. The paper introduces an epistemic language with a possible-worlds semantics, followed by novel dynamics given by first-order action models and their execution via product updates. Taking advantage of the first-order machinery, epistemic action schemas are defined to provide compact, problem-independent domain descriptions, in the spirit of PDDL. Concerning metatheory, the paper defines axiomatic normal term-modal logics, shows a Canonical Model Theorem-like result which allows establishing completeness through frame characterization formulas, shows decidability for the finite agent case, and shows a general completeness result for the dynamic extension by reduction axioms. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103305	10.1016/j.artint.2020.103305													
J								Effective footstep planning using homotopy-class guidance	ARTIFICIAL INTELLIGENCE										Humanoids; Footstep planning; Search-based planning; Topology; Homotopy classes; Heuristic generation		Planning the motion for humanoid robots is a computationally-complex task due to the high dimensionality of the system. Thus, a common approach is to first plan in the low-dimensional space induced by the robot's feet-a task referred to as footstep planning. This low-dimensional plan is then used to guide the full motion of the robot. One approach that has proven successful in footstep planning is using search-based planners such as A* and its many variants. To do so, these search-based planners have to be endowed with effective heuristics to efficiently guide them through the search space. However, designing effective heuristics is a time-consuming task that requires the user to have good domain knowledge. Thus, our goal is to be able to effectively plan the footstep motions taken by a humanoid robot while obviating the burden on the user to carefully design local-minima free heuristics. To this end, we propose to use user-defined homotopy classes in the workspace that are intuitive to define. These homotopy classes are used to automatically generate heuristic functions that efficiently guide the footstep planner. Additionally, we present an extension to homotopy classes such that they are applicable to complex multi-level environments. We compare our approach for footstep planning with a standard approach that uses a heuristic common to footstep planning. In simple scenarios, the performance of both algorithms is comparable. However, in more complex scenarios our approach allows for a speedup in planning of several orders of magnitude when compared to the standard approach. Published by Elsevier B.V.																	0004-3702	1872-7921				SEP	2020	286								103346	10.1016/j.artint.2020.103346													
J								Interpretable time series kernel analytics by pre-image estimation	ARTIFICIAL INTELLIGENCE										Pre-image problem; Time series; Kernel machinery; Time series averaging; Kernel PCA; Dictionary learning; Representation learning	ALGORITHM	Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space. To interpret and analyse the obtained results, it is often required to restore in the input space the results obtained in the feature space, by using pre-image estimation methods. This work proposes a new closed-form pre-image estimation method for time series kernel analytics that consists of two steps. In the first step, a time warp function, driven by distance constraints in the feature space, is defined to embed time series in a metric space where analytics can be performed conveniently. In the second step, the time series pre-image estimation is cast as learning a linear (or a nonlinear) transformation that ensures a local isometry between the time series embedding space and the feature space. The proposed method is compared to the state of the art through three major tasks that require pre-image estimation: 1) time series averaging, 2) time series reconstruction and denoising and 3) time series representation learning. The extensive experiments conducted on 33 publicly-available datasets show the benefits of the pre-image estimation for time series kernel analytics. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103342	10.1016/j.artint.2020.103342													
J								Handling and measuring inconsistency in non-monotonic logics	ARTIFICIAL INTELLIGENCE										Non-monotonic reasoning; Inconsistency handling; Inconsistency measurement	COMPLEXITY; INFORMATION	We address the issue of quantitatively assessing the severity of inconsistencies in non-monotonic frameworks. While measuring inconsistency in classical logics has been investigated for some time now, taking the non-monotonicity into account poses new challenges. In order to tackle them, we focus on the structure of minimal strongly K-inconsistent subsets of a knowledge base K-a sound generalization of minimal inconsistent subsets to arbitrary, possibly non-monotonic, frameworks which induces a generalization of Reiter's famous hitting set duality between minimal inconsistent and maximal consistent subsets of a knowledge base. We propose measures based on this notion and investigate their behavior in a non-monotonic setting by revisiting existing rationality postulates, analyzing the compliance of the proposed measures with these postulates, and by investigating their computational complexity. Motivated by the observation that a knowledge base of a non-monotonic logic can also be repaired by adding formulas - whereas Reiter's duality is only concerned about removing -, we also investigate situations where we are given potential additional assumptions to repair a knowledge base. For this, we characterize the minimal modifications to a knowledge base in terms of a hitting set duality (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103344	10.1016/j.artint.2020.103344													
J								The logic of gossiping	ARTIFICIAL INTELLIGENCE										Communication; Gossip; Knowledge; Protocols	KNOWLEDGE; COMMUNICATION	The so-called gossip problem is a formal model of peer-to-peer communication. In order to perform such communication efficiently, it is important to keep track of what agents know about who holds what information at a given point in time. The knowledge that the agents possess depends strongly on the particular type of communication that is used. Here, we formally define a large number of different variants of the gossip problem, that differ in the extent to which communication is private (observable, synchronous or asynchronous), the direction of the flow of information (caller to callee, callee to caller or both) and whether the agents become aware of the exact set of information possessed by their communication partner. We consider a number of formulas that represent interesting properties that a gossip situation may or may not enjoy, and show for which variants they are valid. Additionally, we show that the model checking and validity checking problems for each variant are decidable, and we introduce sound and complete proof systems for them. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103306	10.1016/j.artint.2020.103306													
J								On the equivalence of optimal recommendation sets and myopically optimal query sets	ARTIFICIAL INTELLIGENCE										Preference elicitation; Utility elicitation; Preferences; Minimax regret; Bayesian inference; Utility theory; Decision-making; Recommender systems; Value of information; Submodularity	DECISION-MAKING; ELICITATION; OPTIMIZATION; REGRET; CHOICE	Preference elicitation is an important component in many AI applications, including decision support and recommender systems. Such systems must assess user preferences, based on interactions with their users, and make recommendations using (possibly incomplete and imprecise) beliefs about those preferences. Mechanisms for explicit preference elicitation-asking users to answer direct queries about their preferences-can be of great value; but due to the cognitive and time cost imposed on users, it is important to minimize the number of queries by asking those that have high (expected) value of information. An alternative approach is to simply make recommendations and have users provide feedback (e.g., accept a recommendation or critique it in some way) and use this more indirect feedback to gradually improve the quality of the recommendations. Due to inherent uncertainty about a user's true preferences, often a set of recommendations is presented to the user at each stage. Conceptually, a set of recommendations can also be viewed as choice query, in which the user indicates which option is most preferred from that set. Because of the potential tension between making a good set recommendation and asking an informative choice query, we explore the connection between the two. We consider two different models of preference uncertainty and optimization: (a) a Bayesian framework in which a posterior over user utility functions is maintained, optimal recommendations are assessed using expected utility, and queries are assessed using expected value of information; and (b) a minimax-regret framework in which user utility uncertainty is strict (represented by a polytope), recommendations are made using the minimax-regret robustness criterion, and queries are assessed using worst-case regret reduction. We show that, somewhat surprisingly, in both cases, there is no tradeoff to be made between good recommendations and good queries: we prove that the optimal recommendation set of size k is also an optimal choice query of size k. We also examine the case where user responses to choice queries are error prone (using both constant and mixed multinomial logit noise models) showing the results are robust to this form of noise. In both frameworks, our theoretical results have practical consequences for the design of interactive recommenders. Our results also allow us to design efficient algorithms to compute optimal query/recommendation sets. We develop several such algorithms (both exact and approximate) for both settings and provide empirical validation of their performance. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103328	10.1016/j.artint.2020.103328													
J								PopMNet: Generating structured pop music melodies using neural networks	ARTIFICIAL INTELLIGENCE										Melody generation; Melody structure; Artificial neural network; Generative adversarial network; ISTM		Recently, many deep learning models have been proposed to generate symbolic melodies. However, generating pop music melodies with well organized structures remains to be challenging. In this paper, we present a melody structure-based model called PopMNet to generate structured pop music melodies. The melody structure is defined by pairwise relations, specifically, repetition and sequence, between all bars in a melody. PopMNet consists of a Convolutional Neural Network (CNN)-based Structure Generation Net (SGN) and a Recurrent Neural Network (RNN)-based Melody Generation Net (MGN). The former generates melody structures and the latter generates melodies conditioned on the structures and chord progressions. The proposed model is compared with four existing models AttentionRNN, LookbackRNN, MidiNet and Music Transformer. The results indicate that the melodies generated by our model contain much clearer structures compared to those generated by other models, as confirmed by human behavior experiments. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				SEP	2020	286								103303	10.1016/j.artint.2020.103303													
J								The ambiguous proposal evaluation problem	DECISION SUPPORT SYSTEMS										Proposal selection problem; Ambiguous specifications; MCDM methods; Fuzzy set theory	PROJECT PORTFOLIO SELECTION; DATA ENVELOPMENT ANALYSIS; METHODOLOGY	A complex decision-making challenge senior managers commonly face is the selection of the winning bid from multiple project proposals. Project selection decisions become more complex when providers deliberately choose to introduce ambiguity to their project proposals rather than address the clients predetermined set of desired specifications. In particular, providers suggest in their proposals a range of values for some product specifications. Providers introduce ambiguity for various reasons, such as future technological advances and strategic misrepresentation. Further, such disruptive behaviour is tolerated by clients for reasons such as ambition, lack of knowledge, uncertain needs, complexity and a lack of competition. This paper defines the ambiguous proposal evaluation problem and develops a solution which enables such proposals to be compared and ranked. The solution is developed through the utilisation of fuzzy logic in combination with a multi-criteria decision-making method, and is illustrated on a procurement project. The contribution of this paper is firstly to define a practical problem in the literature and secondly to develop a solution which enables ranking ambiguous options.																	0167-9236	1873-5797				SEP	2020	136								113359	10.1016/j.dss.2020.113359													
J								Geo-semantic-parsing: AI-powered geoparsing by traversing semantic knowledge graphs	DECISION SUPPORT SYSTEMS										Geoparsing; Geotagging; Artificial intelligence; Knowledge graphs; Twitter	DECISION-SUPPORT-SYSTEMS; SOCIAL MEDIA; REAL-TIME; LOCATION; TWITTER; GEOLOCATION	Online social networks convey rich information about geospatial facets of reality. However in most cases, geographic information is not explicit and structured, thus preventing its exploitation in real-time applications. We address this limitation by introducing a novel geoparsing and geotagging technique called Geo-Semantic-Parsing (Gs P). GSP identifies location references in free text and extracts the corresponding geographic coordinates. To reach this goal, we employ a semantic annotator to identify relevant portions of the input text and to link them to the corresponding entity in a knowledge graph. Then, we devise and experiment with several efficient strategies for traversing the knowledge graph, thus expanding the available set of information for the geoparsing task. Finally, we exploit all available information for learning a regression model that selects the best entity with which to geotag the input text. We evaluate GSP on a well-known reference dataset including almost 10 k event-related tweets, achieving F1 = 0.66. We extensively compare our results with those of 2 baselines and 3 state-of-the-art geoparsing techniques, achieving the best performance. On the same dataset, competitors obtain F1 <= 0.55. We conclude by providing in-depth analyses of our results, showing that the overall superior performance of GSP is mainly due to a large improvement in recall, with respect to existing techniques.																	0167-9236	1873-5797				SEP	2020	136								113346	10.1016/j.dss.2020.113346													
J								Missing care: A framework to address the issue of frequent missing values;The case of a clinical decision support system for Parkinson's disease	DECISION SUPPORT SYSTEMS										Electronic health records; Data missing values; Clinical decision support systems; Predictive healthcare analytics; Imbalanced data learning; Parkinson's disease	ELECTRONIC HEALTH RECORDS; INFORMATION-SYSTEMS; BUSINESS INTELLIGENCE; DIABETIC-RETINOPATHY; DATA QUALITY; TECHNOLOGY; MACHINE; ONLINE; RISK; MANAGEMENT	In recent decades, the implementation of electronic health record (EHR) systems has been evolving worldwide, leading to the creation of immense data volume in healthcare. Moreover, there has been a call for research studies to enhance personalized medicine and develop clinical decision support systems (CDSS) by analyzing the available EHR data. In EHR data, usually, there are millions of patients records with hundreds of features collected over a long period of time. This enormity of EHR data poses significant challenges, one of which is dealing with many variables with very high degrees of missing values. In this study, the data quality issue of incompleteness in EHR data is discussed, and a framework called 'Missing Care' is introduced to address this issue. Using Missing Care, researchers will be able to select the most important variables at an acceptable missing values degree to develop predictive models with high predictive power. Moreover, Missing Care is applied to analyze a unique, large EHR data to develop a CDSS for detecting Parkinson's disease. Parkinson is a complex disease, and even a specialist's diagnosis is not without error. Besides, there is a lack of access to specialists in more remote areas, and as a result, about half of the patients with Parkinson's disease in the US remain undiagnosed. The developed CDSS can be integrated into EHR systems or utilized as an independent tool by healthcare practitioners who are not necessarily specialists; therefore, making up for the limited access to specialized care in remote areas.																	0167-9236	1873-5797				SEP	2020	136								113339	10.1016/j.dss.2020.113339													
J								From predictive to prescriptive analytics: A data-driven multi-item newsvendor model	DECISION SUPPORT SYSTEMS										Multi-item newsvendor model; Machine learning; Quantile regression; Resource allocation; Hierarchical forecast	NEURAL-NETWORKS; NEWSBOY PROBLEM; SUPPLY CHAIN; TOP-DOWN; INVENTORY; AGGREGATE	This paper considers a multi-item newsvendor problem with a capacity constraint (Z). The problem has already been addressed in the literature using the classical newsvendor problem. However, provided solutions made assumptions for demand distributions, which are often incorrect and led to errors in the inventory optimization. This research proposes a distribution-free and completely data-driven solution approach to Z. The proposed approach uses sample demand data as input, and machine (and deep) learning methods with empirical risk minimization principle to find order quantities. A heuristic is developed using hierarchies of the retail products to perform multi-item inventory optimization when a capacity constraint is active. The proposed approach is tested on a real-world dataset of retail products. The results from the proposed method are compared with datadriven max-min and empirical inventory optimization methods, and it outperformed all of them. The machine (and deep) learning-based demand forecasting methods (part of the proposed approach) providing better results than neural networks, multiple regression, arima, arimax, etc. Finally, a comparison of total inventory cost from the proposed, max-min, and empirical inventory optimization methods are carried out, and it is observed that the proposed data-driven approach leads to a significant reduction in inventory cost.																	0167-9236	1873-5797				SEP	2020	136								113340	10.1016/j.dss.2020.113340													
J								Partial order resolution of event logs for process conformance checking	DECISION SUPPORT SYSTEMS										Process mining; Conformance checking; Partial order resolution; Data uncertainty		While supporting the execution of business processes, information systems record event logs. Conformance checking relies on these logs to analyze whether the recorded behavior of a process conforms to the behavior of a normative specification. A key assumption of existing conformance checking techniques, however, is that all events are associated with timestamps that allow to infer a total order of events per process instance. Unfortunately, this assumption is often violated in practice. Due to synchronization issues, manual event recordings, or data corruption, events are only partially ordered. In this paper, we put forward the problem of partial order resolution of event logs to close this gap. It refers to the construction of a probability distribution over all possible total orders of events of an instance. To cope with the order uncertainty in real-world data, we present several estimators for this task, incorporating different notions of behavioral abstraction. Moreover, to reduce the runtime of conformance checking based on partial order resolution, we introduce an approximation method that comes with a bounded error in terms of accuracy. Our experiments with real-world and synthetic data reveal that our approach improves accuracy over the state-of-the-art considerably.																	0167-9236	1873-5797				SEP	2020	136								113347	10.1016/j.dss.2020.113347													
J								Emphasizing the entrepreneur or the idea? The impact of text content emphasis on investment decisions in crowdfunding	DECISION SUPPORT SYSTEMS										Crowdfunding; Investment willingness; Text content emphasis; Text analytics; deep learning	CREATIVITY; SUCCESS; PERFORMANCE; ENGAGEMENT; MANAGEMENT; INNOVATION; ANALYTICS; BEHAVIOR; MODEL	The criteria for evaluating a crowdfunding project vary from person to person. Some investors are concerned with the entrepreneurs' profiles, while others are more interested in the creativity behind the project or the idea. Most investors evaluate a project by referring to founder-generated content. Entrepreneurs then face the choice of emphasizing their own profiles or the project creativity within the limited narrative. Our study is thus motivated to examine how the emphasis of the text narratives affect the fundraising outcomes. We propose an improved deep learning model for text content emphasis (TCE) detection from various textual sources, including titles, blurbs, and detailed descriptions, and then estimate the impact of TCE on the success of fundraising campaigns. With data collected from public sources of Kickstarter platform, our empirical analyses demonstrate that TCE matters differently for various textual sources. It is more effective for the title and blurb to emphasize the entrepreneur profile, while the detailed description should highlight the idea creativity. Furthermore, for the detailed description, the entrepreneur profile has a more positive effect when it is placed at the beginning of the narrative. And the effects of entrepreneur-oriented narratives are more pronounced in these projects with more social connections. This study contributes to the understanding of TCE, entrepreneur profile disclosure, and social connection in economic exchanges in online crowdfunding.																	0167-9236	1873-5797				SEP	2020	136								113341	10.1016/j.dss.2020.113341													
J								Evaluating the credit risk of SMEs using legal judgments	DECISION SUPPORT SYSTEMS										Credit risk; Information asymmetry; Legal judgment; SME; Text mining	BANKRUPTCY PREDICTION; SOFT INFORMATION; DEFAULT; ACCESS; FIRMS	Loan application assessments of small and medium-sized enterprises (SMEs) are difficult because of information asymmetry. To mitigate the information asymmetry, this paper focuses on information found in legal judgments involving the company and its principles and combines this information with financial and firm-specific information to help evaluate the credit risk of SMEs. We propose a framework to identify legal judgments that are effective in predicting credit risk and extract relevant features that are contained within the effective legal judgments. Empirical evaluation shows that features extracted from effective legal judgments significantly improve the discrimination performance and granting performance of our model compared with the baseline model, which uses financial and firm-specific features only.																	0167-9236	1873-5797				SEP	2020	136								113364	10.1016/j.dss.2020.113364													
J								Error bounds for deep ReLU networks using the Kolmogorov-Arnold superposition theorem	NEURAL NETWORKS										Deep ReLU networks; Curse of dimensionality; Approximation theory; Kolmogorov-Arnold superposition theorem	NUMERICAL IMPLEMENTATION; REPRESENTATION; VARIABLES	We prove a theorem concerning the approximation of multivariate functions by deep ReLU networks, for which the curse of the dimensionality is lessened. Our theorem is based on a constructive proof of the Kolmogorov-Arnold superposition theorem, and on a subset of multivariate continuous functions whose outer superposition functions can be efficiently approximated by deep ReLU networks. (C) 2019 Published by Elsevier Ltd.																	0893-6080	1879-2782				SEP	2020	129						1	6		10.1016/j.neunet.2019.12.013													
J								DLPNet: A deep manifold network for feature extraction of hyperspectral imagery	NEURAL NETWORKS										Hyperspectral imagery; Deep learning; Feature extraction; Deep manifold network; Graph embedding	FEATURE FUSION; CLASSIFICATION; FRAMEWORK; REPRESENTATION	Deep learning has received increasing attention in recent years and it has been successfully applied for feature extraction (FE) of hyperspectral images. However, most deep learning methods fail to explore the manifold structure in hyperspectral image (HSI). To tackle this issue, a novel graph-based deep learning model, termed deep locality preserving neural network (DLPNet), was proposed in this paper. Traditional deep learning methods use random initialization to initialize network parameters. Different from that, DLPNet initializes each layer of the network by exploring the manifold structure in hyperspectral data. In the stage of network optimization, it designed a deep-manifold learning joint loss function to exploit graph embedding process while measuring the difference between the predictive value and the actual value, then the proposed model can take into account the extraction of deep features and explore the manifold structure of data simultaneously. Experimental results on real-world HSI datasets indicate that the proposed DLPNet performs significantly better than some state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						7	18		10.1016/j.neunet.2020.05.022													
J								Multi-view clustering on data with partial instances and clusters	NEURAL NETWORKS										Multi-view; Non-negative matrix factorization; Partial data		Most multi-view clustering algorithms apply to data with complete instances and clusters in the views. Recently, multi-view clustering on data with partial instances has been studied. In this paper, we study the more general version of the problem, i.e., multi-view clustering on data with partial instances and clusters in the views. We propose a non-negative matrix factorization (NMF) based algorithm. For the special case with partial instances, it introduces an instance-view-indicator matrix to indicate whether an instance exists in a view. Then, it maps the instances representing the same object to the same vector, and maps the instances representing different objects to different vectors. For the general case with partial instances and clusters, it further introduces a cluster-view-indicator matrix to indicate whether a cluster exists in a view. In each view, it also maps the instances representing the same object to the same vector, but it further makes the elements of the vector 0 if the elements correspond to missing clusters. Then it minimizes the disagreements between the approximated indicator vectors of instances representing the same object. Experimental results show that the proposed algorithm performs well on data with partial instances and clusters, and outperforms existing algorithms on data with partial instances. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						19	30		10.1016/j.neunet.2020.05.021													
J								Quasi-bipartite synchronization of signed delayed neural networks under impulsive effects	NEURAL NETWORKS										Quasi-bipartite synchronization; Delayed neural networks; Impulsive control; Disturbance	EXPONENTIAL STABILITY; MULTIAGENT SYSTEMS; CONSENSUS; INEQUALITY; TRACKING; FINITE	This paper mainly studies quasi-bipartite synchronization (QBPS) of signed delayed neural networks (SDNNs) under impulsive effects, in which the nodes have cooperative as well as antagonistic interactions. It is assumed that disturbance occurs in the communication channels between some neighboring agents at impulsive occurring instants. Under the balanced network topology, some sufficient criteria to achieve QBPS of SDNNs are proposed by utilizing algebraic graph theory and extended Halanay differential inequality. Moreover, for the QBPS error of SDNNs, the upper bound of the final error state is also provided explicitly. Two numerical examples are presented to demonstrate the correctness of the theoretical results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						31	42		10.1016/j.neunet.2020.05.012													
J								An end-to-end exemplar association for unsupervised person Re-identification	NEURAL NETWORKS										End-to-end exemplar-based training; Exemplar association; Dynamic selection threshold		Tracklet association methods learn the cross camera retrieval ability though associating underlying cross camera positive samples, which have proven to be successful in unsupervised person re-identification task. However, most of them use poor-efficiency association strategies which costs long training hours but gains the low performance. To solve this, we propose an effective end-to-end exemplar associations (EEA) framework in this work. EEA mainly adapts three strategies to improve efficiency: (1) end-to-end exemplar-based training, (2) exemplar association and (3) dynamic selection threshold. The first one is to accelerate the training process, while the others aim to improve the tracklet association precision. Compared with existing tracklet associating methods, EEA obviously reduces the training cost and achieves the higher performance. Extensive experiments and ablation studies on seven RE-ID datasets demonstrate the superiority of the proposed EEA over most state-of-the-art unsupervised and domain adaptation RE-ID methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						43	54		10.1016/j.neunet.2020.05.015													
J								Interpretable and lightweight convolutional neural network for EEG decoding: Application to movement execution and imagination	NEURAL NETWORKS										Electroencephalography; Convolutional neural network; Sinc-convolutional layer; Feature learning; Interpretability	COMMON SPATIAL-PATTERN; DESYNCHRONIZATION; SYNCHRONIZATION; RHYTHM; TIME	Convolutional neural networks (CNNs) are emerging as powerful tools for EEG decoding: these techniques, by automatically learning relevant features for class discrimination, improve EEG decoding performances without relying on handcrafted features. Nevertheless, the learned features are difficult to interpret and most of the existing CNNs introduce many trainable parameters. Here, we propose a lightweight and interpretable shallow CNN (Sinc-ShallowNet), by stacking a temporal sinc-convolutional layer (designed to learn band-pass filters, each having only the two cut-off frequencies as trainable parameters), a spatial depthwise convolutional layer (reducing channel connectivity and learning spatial filters tied to each band-pass filter), and a fully-connected layer finalizing the classification. This convolutional module limits the number of trainable parameters and allows direct interpretation of the learned spectral-spatial features via simple kernel visualizations. Furthermore, we designed a post-hoc gradient-based technique to enhance interpretation by identifying the more relevant and more class-specific features. Sinc-ShallowNet was evaluated on benchmark motor-execution and motor-imagery datasets and against different design choices and training strategies. Results show that (i) Sinc-ShallowNet outperformed a traditional machine learning algorithm and other CNNs for EEG decoding; (ii) The learned spectral-spatial features matched well-known EEG motor-related activity; (iii) The proposed architecture performed better with a larger number of temporal kernels still maintaining a good compromise between accuracy and parsimony, and with a trialwise rather than a cropped training strategy. In perspective, the proposed approach, with its interpretative capacity, can be exploited to investigate cognitive motor aspects whose EEG correlates are yet scarcely known, potentially characterizing their relevant features. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						55	74		10.1016/j.neunet.2020.05.032													
J								Group visualization of class-discriminative features	NEURAL NETWORKS										Convolutional neural networks; Shapley values; Matrix decomposition; Feature visualization		Research explaining the behavior of convolutional neural networks (CNNs) has gained a lot of attention over the past few years. Although many visualization methods have been proposed to explain network predictions, most fail to provide clear correlations between the target output and the features extracted by convolutional layers. In this work, we define a concept, i.e., class-discriminative feature groups, to specify features that are extracted by groups of convolutional kernels correlated with a particular image class. We propose a detection method to detect class-discriminative feature groups and a visualization method to highlight image regions correlated with particular output and to interpret class-discriminative feature groups intuitively. The experiments showed that the proposed method can disentangle features based on image classes and shed light on what feature groups are extracted from which regions of the image. We also applied this method to visualize "lost" features in adversarial samples and features in an image containing a non-class object to demonstrate its ability to debug why the network failed or succeeded. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						75	90		10.1016/j.neunet.2020.05.026													
J								Deep learning for symbols detection and classification in engineering drawings	NEURAL NETWORKS										Deep learning; YOLO; P&ID; Engineering drawings; Symbols recognition; GANs	RECOGNITION	Engineering drawings are commonly used in different industries such as Oil and Gas, construction, and other types of engineering. Digitising these drawings is becoming increasingly important. This is mainly due to the need to improve business practices such as inventory, assets management, risk analysis, and other types of applications. However, processing and analysing these drawings is a challenging task. A typical diagram often contains a large number of different types of symbols belonging to various classes and with very little variation among them. Another key challenge is the class-imbalance problem, where some types of symbols largely dominate the data while others are hardly represented in the dataset. In this paper, we propose methods to handle these two challenges. First, we propose an advanced bounding-box detection method for localising and recognising symbols in engineering diagrams. Our method is end-to-end with no user interaction. Thorough experiments on a large collection of diagrams from an industrial partner proved that our methods accurately recognise more than 94% of the symbols. Secondly, we present a method based on Deep Generative Adversarial Neural Network for handling class-imbalance. The proposed GAN model proved to be capable of learning from a small number of training examples. Experiment results showed that the proposed method greatly improved the classification of symbols in engineering drawings. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						91	102		10.1016/j.neunet.2020.05.025													
J								Initializing photonic feed-forward neural networks using auxiliary tasks	NEURAL NETWORKS										Photonic deep learning; Neural network initialization; Photonic activation functions	LEARNING ALGORITHMS; MACHINE	Photonics is among the most promising emerging technologies for providing fast and energy-efficient Deep Learning (DL) implementations. Despite their advantages, these photonic DL accelerators also come with certain important limitations. For example, the majority of existing photonic accelerators do not currently support many of the activation functions that are commonly used in DL, such as the ReLU activation function. Instead, sinusoidal and sigmoidal nonlinearities are usually employed, rendering the training process unstable and difficult to tune, mainly due to vanishing gradient phenomena. Thus, photonic DL models usually require carefully fine-tuning all their training hyper-parameters in order to ensure that the training process will proceed smoothly. Despite the recent advances in initialization schemes, as well as in optimization algorithms, training photonic DL models is still especially challenging. To overcome these limitations, we propose a novel adaptive initialization method that employs auxiliary tasks to estimate the optimal initialization variance for each layer of a network. The effectiveness of the proposed approach is demonstrated using two different datasets, as well as two recently proposed photonic activation functions and three different initialization methods. Apart from significantly increasing the stability of the training process, the proposed method can be directly used with any photonic activation function, without further requiring any other kind of fine-tuning, as also demonstrated through the conducted experiments. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						103	108		10.1016/j.neunet.2020.05.024													
J								Phase portraits as movement primitives for fast humanoid robot control	NEURAL NETWORKS										Imitation learning; Reinforcement learning; Movement primitives; Phase estimation; Coupled oscillators	LOCOMOTION; MODELS	Currently, usual approaches for fast robot control are largely reliant on solving online optimal control problems. Such methods are known to be computationally intensive and sensitive to model accuracy. On the other hand, animals plan complex motor actions not only fast but seemingly with little effort even on unseen tasks. This natural sense to infer temporal dynamics and coordination motivates us to approach robot control from a motor skill learning perspective to design fast and computationally light controllers that can be learned autonomously by the robot under mild modeling assumptions. This article introduces Phase Portrait Movement Primitives (PPMP), a primitive that predicts dynamics on a low dimensional phase space which in turn is used to govern the high dimensional kinematics of the task. The stark difference with other primitive formulations is a built-in mechanism for phase prediction in the form of coupled oscillators that replaces model-based state estimators such as Kalman filters. The policy is trained by optimizing the parameters of the oscillators whose output is connected to a kinematic distribution in the form of a phase portrait. The drastic reduction in dimensionality allows us to efficiently train and execute PPMPs on a real human-sized, dual-arm humanoid upper body on a task involving 20 degrees-of-freedom. We demonstrate PPMPs in interactions requiring fast reactions times while generating anticipative pose adaptation in both discrete and cyclic tasks. (C) 2020 Published by Elsevier Ltd.																	0893-6080	1879-2782				SEP	2020	129						109	122		10.1016/j.neunet.2020.04.007													
J								Image style transfer with collection representation space and semantic-guided reconstruction	NEURAL NETWORKS										Style transfer; Collection representation space; Semantic guidance		Image style transfer renders the content of an image into different styles. Current methods made decent progress with transferring the style of single image, however, visual statistics from one image cannot reflect the full scope of an artist. Also, previous work did not put content preservation in the important position, which would result in poor structure integrity, thus deteriorating the comprehensibility of generated image. These two problems would limit the visual quality improvement of style transfer results. Targeting at style resemblance and content preservation problems, we propose a style transfer system composed of collection representation space and semantic-guided reconstruction. We train an encoder-decoder network with art collections to construct a representation space that can reflect the style of the artist. Then, we use semantic information as guidance to reconstruct the target representation of the input image for better content preservation. We conduct both quantitative analysis and qualitative evaluation to assess the proposed method. Experiment results demonstrate that our approach well balanced the trade-off between capturing artistic characteristics and preserving content information in style transfer tasks. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						123	137		10.1016/j.neunet.2020.05.028													
J								Structure learning with similarity preserving	NEURAL NETWORKS										Similarity preserving; Clustering; Semisupervised classification; Similarity measure; Deep auto-encoder	LOW-RANK; DIMENSIONALITY REDUCTION; REPRESENTATION; GRAPH; RECOGNITION; REGRESSION; FRAMEWORK	Leveraging on the underlying low-dimensional structure of data, low-rank and sparse modeling approaches have achieved great success in a wide range of applications. However, in many applications the data can display structures beyond simply being low-rank or sparse. Fully extracting and exploiting hidden structure information in the data is always desirable and favorable. To reveal more underlying effective manifold structure, in this paper, we explicitly model the data relation. Specifically, we propose a structure learning framework that retains the pairwise similarities between the data points. Rather than just trying to reconstruct the original data based on self-expression, we also manage to reconstruct the kernel matrix, which functions as similarity preserving. Consequently, this technique is particularly suitable for the class of learning problems that are sensitive to sample similarity, e.g., clustering and semisupervised classification. To take advantage of representation power of deep neural network, a deep auto-encoder architecture is further designed to implement our model. Extensive experiments on benchmark data sets demonstrate that our proposed framework can consistently and significantly improve performance on both evaluation tasks. We conclude that the quality of structure learning can be enhanced if similarity information is incorporated. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						138	148		10.1016/j.neunet.2020.05.030													
J								Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks	NEURAL NETWORKS										Recurrent neural network; Reinforcement learning; Partially observable Markov decision process; Multiple timescale; Compositionality	TIME SCALES; TIMESCALES; MEMORY; GAME; GO	Recurrent neural networks (RNNs) for reinforcement learning (RL) have shown distinct advantages, e.g., solving memory-dependent tasks and meta-learning. However, little effort has been spent on improving RNN architectures and on understanding the underlying neural mechanisms for performance gain. In this paper, we propose a novel, multiple-timescale, stochastic RNN for RL. Empirical results show that the network can autonomously learn to abstract sub-goals and can self-develop an action hierarchy using internal dynamics in a challenging continuous control task. Furthermore, we show that the self-developed compositionality of the network enhances faster re-learning when adapting to a new task that is a re-composition of previously learned sub-goals, than when starting from scratch. We also found that improved performance can be achieved when neural activities are subject to stochastic rather than deterministic dynamics. (C) 2020 The Authors. Published by Elsevier Ltd.																	0893-6080	1879-2782				SEP	2020	129						149	162		10.1016/j.neunet.2020.06.002													
J								Encoding primitives generation policy learning for robotic arm to overcome catastrophic forgetting in sequential multi-tasks learning	NEURAL NETWORKS										Sequential multi-tasks learning; Continual learning; Catastrophic forgetting; Robotics		Continual learning, a widespread ability in people and animals, aims to learn and acquire new knowledge and skills continuously. Catastrophic forgetting usually occurs in continual learning when an agent attempts to learn different tasks sequentially without storing or accessing previous task information. Unfortunately, current learning systems, e.g., neural networks, are prone to deviate the weights learned in previous tasks after training new tasks, leading to catastrophic forgetting, especially in a sequential multi-tasks scenario. To address this problem, in this paper, we propose to overcome catastrophic forgetting with the focus on learning a series of robotic tasks sequentially. Particularly, a novel hierarchical neural network's framework called Encoding Primitives Generation Policy Learning (E-PGPL) is developed to enable continual learning with two components. By employing a variational autoencoder to project the original state space into a meaningful low-dimensional feature space, representative state primitives could be sampled to help learn corresponding policies for different tasks. In learning a new task, the feature space is required to be close to the previous ones so that previously learned tasks can be protected. Extensive experiments on several simulated robotic tasks demonstrate our method's efficacy to learn control policies for handling sequentially arriving multi-tasks, delivering improvement substantially over some other continual learning methods, especially for the tasks with more diversity. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						163	173		10.1016/j.neunet.2020.06.003													
J								Graph embedded rules for explainable predictions in data streams	NEURAL NETWORKS										Rule-based classifiers; Data stream; Interpretable machine learning; Attribute-based Decision Graphs	CLASSIFIERS; DRIFT	Understanding the reason why a prediction has been made by a machine is crucial to grant trust to a human decision-maker. However, data mining based decision support systems are, in general, not designed to promote interpretability; instead, they are developed to improve accuracy. Interpretability becomes a more challenging issue in the context of data stream mining. Where the prediction model has to deal with enormous volumes of data gathered continuously at a fast rate and whose underlying distribution may change over time. On the one hand, the majority of the methods that address classification in a data stream are black-box models or white-box models into ensembles. Either do not provide a clear view of why a particular decision has been made. On the other hand, white-box models, such as rule-based models, do not provide acceptable accuracy to be considered in many applications. This paper proposes modeling the data as a special graph, which is built over the attribute space, and from which interpretable rules can be extracted. To overcome concept drift and enhance model accuracy, different variants of such graphs are considered within an ensemble that is updated over time. The proposed approach has shown the best overall classification results when compared to six rule-based algorithms in twelve streaming domains. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						174	192		10.1016/j.neunet.2020.05.035													
J								Novel results on finite-time stabilization of state-based switched chaotic inertial neural networks with distributed delays	NEURAL NETWORKS										Finite-time stabilization; Distributed time-varying delays; Neural networks; Non-smooth analysis; State-based switched	VARYING DELAYS; EXPONENTIAL STABILITY; ROBUST STABILITY; SYNCHRONIZATION; CRITERION; SYSTEMS	The p-norm finite-time stabilization (FTS) issue of a class of state-based switched inertial chaotic neural networks (SBSCINNs) with distributed time-varying delays is investigated. By using a suitable variable transformation, such second-order SBSCINNs are turned into the first-order differential equations. Then some novel criteria are obtained to stabilize SBSCINNs in a finite time based on the theory of finite-time control and non-smooth analysis together with designing two proper delay-dependent feedback controllers. Besides, the settling time of FTS is also estimated and discussed. Finally, the validity and practicability of the deduced theoretical results are verified by examples and applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						193	202		10.1016/j.neunet.2020.06.004													
J								A neurodynamic model of the interaction between color perception and color memory	NEURAL NETWORKS										Adaptive resonance theory; Cognitive impenetrability of vision; Color vision; Memory color effect; Predictive coding	PRIMARY VISUAL-CORTEX; ART CLUSTERING NETWORKS; NEURAL MECHANISMS; COGNITIVE IMPENETRABILITY; GRANDMOTHER CELLS; VIEW-INVARIANT; ATTENTION; BRAIN; TOP; REPRESENTATION	The memory color effect and Spanish castle illusion have been taken as evidence of the cognitive penetrability of vision. In the same manner, the successful decoding of color-related brain signals in functional neuroimaging studies suggests the retrieval of memory colors associated with a perceived gray object. Here, we offer an alternative account of these findings based on the design principles of adaptive resonance theory (ART). In ART, conscious perception is a consequence of a resonant state. Resonance emerges in a recurrent cortical circuit when a bottom-up spatial pattern agrees with the top-down expectation. When they do not agree, a special control mechanism is activated that resets the network and clears off erroneous expectation, thus allowing the bottom-up activity to always dominate in perception. We developed a color ART circuit and evaluated its behavior in computer simulations. The model helps to explain how traces of erroneous expectations about incoming color are eventually removed from the color perception, although their transient effect may be visible in behavioral responses or in brain imaging. Our results suggest that the color ART circuit, as a predictive computational system, is almost never penetrable, because it is equipped with computational mechanisms designed to constrain the impact of the top-down predictions on ongoing perceptual processing. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						222	248		10.1016/j.neunet.2020.06.008													
J								Missing data imputation with adversarially-trained graph convolutional networks	NEURAL NETWORKS										Imputation; Graph neural network; Graph data; Convolutional network	NEURAL-NETWORK; CHAINED EQUATIONS	Missing data imputation (MDI) is the task of replacing missing values in a dataset with alternative, predicted ones. Because of the widespread presence of missing data, it is a fundamental problem in many scientific disciplines. Popular methods for MDI use global statistics computed from the entire dataset (e.g., the feature-wise medians), or build predictive models operating independently on every instance. In this paper we propose a more general framework for MDI, leveraging recent work in the field of graph neural networks (GNNs). We formulate the MDI task in terms of a graph denoising autoencoder, where each edge of the graph encodes the similarity between two patterns. A GNN encoder learns to build intermediate representations for each example by interleaving classical projection layers and locally combining information between neighbors, while another decoding GNN learns to reconstruct the full imputed dataset from this intermediate embedding. In order to speed-up training and improve the performance, we use a combination of multiple losses, including an adversarial loss implemented with the Wasserstein metric and a gradient penalty. We also explore a few extensions to the basic architecture involving the use of residual connections between layers, and of global statistics computed from the dataset to improve the accuracy. On a large experimental evaluation with varying levels of artificial noise, we show that our method is on par or better than several alternative imputation methods. On three datasets with pre-existing missing values, we show that our method is robust to the choice of a downstream classifier, obtaining similar or slightly higher results compared to other choices. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						249	260		10.1016/j.neunet.2020.06.005													
J								Contextual encoder-decoder network for visual saliency prediction	NEURAL NETWORKS										Saliency prediction; Human fixations; Convolutional neural networks; Computer vision; Deep learning	ATTENTION; INTEGRATION; INFORMATION	Predicting salient regions in natural images requires the detection of objects that are present in a scene. To develop robust representations for this challenging task, high-level visual features at multiple spatial scales must be extracted and augmented with contextual information. However, existing models aimed at explaining human fixation maps do not incorporate such a mechanism explicitly. Here we propose an approach based on a convolutional neural network pre-trained on a large-scale image classification task. The architecture forms an encoder-decoder structure and includes a module with multiple convolutional layers at different dilation rates to capture multi-scale features in parallel. Moreover, we combine the resulting representations with global scene information for accurately predicting visual saliency. Our model achieves competitive and consistent results across multiple evaluation metrics on two public saliency benchmarks and we demonstrate the effectiveness of the suggested approach on five datasets and selected examples. Compared to state of the art approaches, the network is based on a lightweight image classification backbone and hence presents a suitable choice for applications with limited computational resources, such as (virtual) robotic systems, to estimate human fixations across complex natural scenes. Our TensorFlow implementation is openly available at https://github.com/alexanderkroner/saliency. (C) 2020 The Author( s). Published by Elsevier Ltd.																	0893-6080	1879-2782				SEP	2020	129						261	270		10.1016/j.neunet.2020.05.004													
J								A spiking neural network-based long-term prediction system for biogas production	NEURAL NETWORKS										Spiking neural networks; Training algorithms; Neural models; NeuCube; Biogas; Anaerobic process models	MICROBIAL COMMUNITIES; DIGESTION; NEURONS; MODEL	Efficient energy production from biomass is a central issue in the context of clean alternative energy resource. In this work we propose a novel model based on spiking neural networks cubes in order to model the chemical processes that goes on in a digestor for the production of usable biogas. For the implementation of the predictive structure, we have used the NeuCube computational framework. The goals of the proposed model were: develop a tool for real applications (low-cost and efficient), generalize the data when the system presents high sensitivity to small differences on the initial conditions, take in account the "multi-scale" temporal dynamics of the chemical processes occurring in the digestor, since the variations present in the early stages of the processes are very quick, whereas in the later stages are slower. By using the first ten days of observation the implemented system has been proven able to predict the evolution of the chemical process up to the 100th day obtaining a high degree of accuracy with respect to the experimental data measured in laboratory. This is due to the fact that the spiking neural networks have shown to be able to modeling complex information processes and then it has been shown that spiking neurons are able to handle patterns of activity that spans different time scales. Thanks to such properties, our system is able to capture the multi-scale trend of the time series associated to the early-stage evolutions, as well as their interaction, which are crucial in the point of view of the information content to obtain a good long-term prediction. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						271	279		10.1016/j.neunet.2020.06.001													
J								A neurodynamic optimization approach for complex-variables programming problem	NEURAL NETWORKS										Complex variables; Nonsmooth optimization; Convex programming; Neural networks; CR calculus	RECURRENT NEURAL-NETWORK; PSEUDOCONVEX OPTIMIZATION; CONSTRAINED OPTIMIZATION; CONVERGENCE; GRADIENT	A neural network model upon differential inclusion is designed for solving the complex-variables convex programming, and the chain rule for real-valued function with the complex-variables is established in this paper. The model does not need to choose penalty parameters when applied to practical problems, which makes it easier to design. The result is obtained that its state reaches the feasible region in finite time. Furthermore, the convergence for its state to an optimal solution is proved. Some typical examples are shown for the effectiveness of the designed model. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						280	287		10.1016/j.neunet.2020.06.012													
J								A new Lyapunov functional for stability analysis of neutral-type Hopfield neural networks with multiple delays	NEURAL NETWORKS										Neutral-type Hopfield systems; Multiple delays; Lyapunov theorems; Stability analysis	DEPENDENT EXPONENTIAL STABILITY; GLOBAL LAGRANGE STABILITY; SYNCHRONIZATION CRITERIA; ROBUST STABILITY; FEEDBACK CONTROL; TIME; DISCRETE; OPTIMIZATION; SYSTEMS; STORAGE	This research paper conducts an investigation into the stability issue for a more general class of neutral-type Hopfield neural networks that involves multiple time delays in the states of neurons and multiple neutral delays in the time derivatives of the states of neurons. By constructing a new proper Lyapunov functional, an alternative easily verifiable algebraic criterion for global asymptotic stability of this type of Hopfield neural systems is derived. This new stability condition is entirely independent of time and neutral delays. Two instructive examples are employed to indicate that the result obtained in this paper reveals a new set of sufficient stability criteria when it is compared with the previously reported stability results. Therefore, the proposed stability result enlarges the application domain of Hopfield neural systems of neutral types. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						288	297		10.1016/j.neunet.2020.06.013													
J								Batch process fault detection for multi-stage broad learning system	NEURAL NETWORKS										Affinity propagation algorithm; Broad learning system; Penicillin fermentation process; Fault detection	INDEPENDENT COMPONENT ANALYSIS; ONLINE MONITORING STRATEGY; PHASE PARTITION; NETWORK; STABILITY; ALGORITHM; DIAGNOSIS; MODEL	In the real industrial production process, some minor faults are difficult to be detected by multivariate statistical analysis methods with mean and variance as detection indicators due to the aging equipment and catalyst deactivation. With structural characteristics, deep neural networks can better extract data features to detect such faults. However, most deep learning models contain a large number of connection parameters between layers, which causes the training time-consuming and thus makes it difficult to achieve a fast-online response. The Broad Learning System (BLS) network structure is expanded without a retraining process and thus saves a lot of training time. Considering that different stages of the batch production process have different production characteristics, we use the Affinity Propagation (AP) algorithm to separate the different stages of the production process. This paper conducts research on a multi-stage process monitoring framework that integrates AP and the BLS. Compared with other monitoring models, the monitoring results in the penicillin fermentation process have verified the superiority of the AP-BLS model. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						298	312		10.1016/j.neunet.2020.05.031													
J								Partial transfer learning in machinery cross-domain fault diagnostics using class-weighted adversarial networks	NEURAL NETWORKS										Fault diagnosis; Partial transfer learning; Deep learning; Rotating machinery; Domain adversarial network	NEURAL-NETWORK; BEARINGS	Recently, transfer learning has been receiving growing interests in machinery fault diagnosis due to its strong generalization across different industrial scenarios. The existing methods generally assume identical label spaces, and propose minimizing marginal distribution discrepancy between source and target domains. However, this assumption usually does not hold in real industries, where testing data mostly contain a subspace of the source label space. Therefore, transferring diagnosis knowledge from a comprehensive source domain to a target domain with limited machine conditions is motivated. This challenging partial transfer learning problem is addressed in this study using deep learning-based domain adaptation method. A class weighted adversarial neural network is proposed to encourage positive transfer of the shared classes and ignore the source outliers. Experimental results on two rotating machinery datasets suggest the proposed method is promising for partial transfer learning. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						313	322		10.1016/j.neunet.2020.06.014													
J								Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning	NEURAL NETWORKS										Snake-like robot; Reinforcement learning; Inverse reinforcement learning; Motion planning; Damage recovery	LOCOMOTION	Similar to real snakes in nature, the flexible trunks of snake-like robots enhance their movement capabilities and adaptabilities in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently and adaptively to unforeseeable joint damage. In this work, we present an approach for designing an energy-efficient and damage-recovery slithering gait for a snake-like robot using the reinforcement learning (RL) algorithm and the inverse reinforcement learning (IRL) algorithm. Specifically, we first present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Then, by taking the RL-based controller as an expert and collecting trajectories from it, we train an IRL-based controller using the adversarial inverse reinforcement learning (AIRL) algorithm. For the purpose of comparison, a traditional parameterized gait controller is presented as the baseline and the parameter sets are optimized using the grid search and Bayesian optimization algorithm. Based on the analysis of the simulation results, we first demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. We then demonstrate that the IRL-based controller cannot only exhibit similar performances as the RL-based controller, but can also recover from the unpredictable damage body joints and still outperform the model-based controller, which has an undamaged body, in terms of energy efficiency. Videos can be viewed at https://videoviewsite.wixsite.com/rlsnake. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						323	333		10.1016/j.neunet.2020.05.029													
J								Appearance variation adaptation tracker using adversarial network	NEURAL NETWORKS										Visual tracking; Convolutional neural network; Adversarial learning	OBJECT TRACKING	Visual trackers using deep neural networks have demonstrated favorable performance in object tracking. However, training a deep classification network using overlapped initial target regions may lead an overfitted model. To increase the model generalization, we propose an appearance variation adaptation (AVA) tracker that aligns the feature distributions of target regions over time by learning an adaptation mask in an adversarial network. The proposed adversarial network consists of a generator and a discriminator network that compete with each other over optimizing a discriminator loss in a mini-max optimization problem. Specifically, the discriminator network aims to distinguish recent target regions from earlier ones by minimizing the discriminator loss, while the generator network aims to produce an adaptation mask to maximize the discriminator loss. We incorporate a gradient reverse layer in the adversarial network to solve the aforementioned mini-max optimization in an end-to-end manner. We compare the performance of the proposed AVA tracker with the most recent state-of-the-art trackers by doing extensive experiments on OTB50, OTB100, and VOT2016 tracking benchmarks. Among the compared methods, AVA yields the highest area under curve (AUC) score of 0.712 and the highest average precision score of 0.951 on the OTB50 tracking benchmark. It achieves the second best AUC score of 0.688 and the best precision score of 0.924 on the OTB100 tracking benchmark. AVA also achieves the second best expected average overlap (EAO) score of 0.366, the best failure rate of 0.68, and the second best accuracy of 0.53 on the VOT2016 tracking benchmark. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						334	343		10.1016/j.neunet.2020.06.011													
J								Fast generalization error bound of deep learning without scale invariance of activation functions	NEURAL NETWORKS										Deep learning; Fast learning rate; Empirical risk minimizer; Sigmoid activation function; Exponential linear unit	NEURAL-NETWORK; APPROXIMATION; COMPLEXITY	In the theoretical analysis of deep learning, discovering which features of deep learning lead to good performance is an important task. Using the framework for analyzing the generalization error developed by Suzuki (2018), we derive a fast learning rate for deep neural networks with general activation functions. According to Suzuki (2018), scale invariance of the activation functions is essential to derive tight error bounds. While the rectified linear unit (ReLU; Nair and Hinton, 2010) satisfies scale invariance, the other famous activation functions, such as the sigmoid, the hyperbolic tangent functions, and the exponential linear unit (ELU; Clevert et al., 2016), do not satisfy this condition. The existing analysis indicates the possibility that deep learning with non scale invariant activations may have a slower convergence rate of O(1/root n) whereas with scale invariant activation functions it can reach a faster rate. In this paper, without scale invariance of activation functions, we derive the tight generalization error bound which is essentially the same as that of Suzuki (2018). From this result, at least in the framework of Suzuki (2018), we show that scale invariance of the activation functions is not essential to obtain a fast rate of convergence. We also conclude that the theoretical framework proposed by Suzuki (2018) can be widely applied to the analysis of deep learning with general activation functions. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						344	358		10.1016/j.neunet.2020.05.033													
J								Noise can speed backpropagation learning and deep bidirectional pretraining	NEURAL NETWORKS										Backpropagation; Noise benefit; Stochastic resonance; Expectation-Maximization algorithm; Bidirectional associative memory; Contrastive divergence	NEURAL-NETWORKS; STOCHASTIC RESONANCE; FUZZY-SYSTEMS; EM; ALGORITHM; MODELS; REPRESENTATIONS; REGULARIZATION	We show that the backpropagation algorithm is a special case of the generalized Expectation-Maximization (EM) algorithm for iterative maximum likelihood estimation. We then apply the recent result that carefully chosen noise can speed the average convergence of the EM algorithm as it climbs a hill of probability or log-likelihood. Then injecting such noise can speed the average convergence of the backpropagation algorithm for both the training and pretraining of multilayer neural networks. The beneficial noise adds to the hidden and visible neurons and related parameters. The noise also applies to regularized regression networks. This beneficial noise is just that noise that makes the current signal more probable. We show that such noise also tends to improve classification accuracy. The geometry of the noise-benefit region depends on the probability structure of the neurons in a given layer. The noise-benefit region in noise space lies above the noisy-EM (NEM) hyperplane for classification and involves a hypersphere for regression. Simulations demonstrate these noise benefits using MNIST digit classification. The NEM noise benefits substantially exceed those of simply adding blind noise to the neural network. We further prove that the noise speed-up applies to the deep bidirectional pretraining of neural-network bidirectional associative memories (BAMs) or their functionally equivalent restricted Boltzmann machines. We then show that learning with basic contrastive divergence also reduces to generalized EM for an energy-based network probability. The optimal noise adds to the input visible neurons of a BAM in stacked layers of trained BAMs. Global stability of generalized BAMs guarantees rapid convergence in pretraining where neural signals feed back between contiguous layers. Bipolar coding of inputs further improves pretraining performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						359	384		10.1016/j.neunet.2020.04.004													
J								Prediction of N6-methyladenosine sites using convolution neural network model based on distributed feature representations	NEURAL NETWORKS										CNN; Natural language processing; word2vec; 10-fold cross-validation	SEQUENCE-BASED PREDICTOR; MESSENGER-RNA; N-6-METHYLADENOSINE SITES; GENERAL-FORM; NUCLEAR-RNA; WEB SERVER; IDENTIFICATION; METHYLATION; PROTEINS; SPACE	N-6-methyladenosine (m(6)A) is a well-studied and most common interior messenger RNA (mRNA) modification that plays an important function in cell development. N(6)A is found in all kingdoms of life and many other cellular processes such as RNA splicing, immune tolerance, regulatory functions, RNA processing, and cancer. Despite the crucial role of m(6)A in cells, it was targeted computationally, but unfortunately, the obtained results were unsatisfactory. It is imperative to develop an efficient computational model that can truly represent m(6)A sites. In this regard, an intelligent and highly discriminative computational model namely: m6A-word2vec is introduced for the discrimination of m(6)A sites. Here, a concept of natural language processing in the form of word2vec is used to represent the motif of the target class automatically. These motifs (numerical descriptors) are automatically targeted from the human genome without any clear definition. Further, the extracted feature space is then forwarded to the convolution neural network model as input for prediction. The developed computational model obtained 83.17%, 92.69%, and 90.50% accuracy for benchmark datasets S-1, S-2, and S-3, respectively, using a 10-fold cross-validation test. The predictive outcomes validate that the developed intelligent computational model showed better performance compared to existing computational models. It is thus greatly estimated that the introduced computational model "m6A-word2vec" may be a supportive and practical tool for elementary and pharmaceutical research such as in drug design along with academia. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						385	391		10.1016/j.neunet.2020.05.027													
J								Pinning bipartite synchronization for inertial coupled delayed neural networks with signed digraph via non-reduced order method	NEURAL NETWORKS										Bipartite synchronization; Inertial coupled delayed neural network; Strongly connected network; Directed spanning tree; Pinning control	TIME-VARYING DELAYS; ANTAGONISTIC INTERACTIONS; EXPONENTIAL STABILITY; MULTIAGENT SYSTEMS; NONLINEAR-SYSTEMS; CONSENSUS; CRITERIA	The study investigates bipartite synchronization of inertial coupled delayed neural networks (ICDNNs) with signed digraph by non-reduced order method and pinning control. The second-order CDNNs will not be converted into a first-order differential system by introducing variable substitution. Instead, a novel Lyapunov-Krasovskii functional is proposed which depends on the topology of the ICDNNs. Some sufficient conditions for linear matrix inequalities (LMI) are derived to realize bipartite synchronization, which is based on matrix decomposition theory and Barbalat Lemma in strongly connected signed networks. And then, M-matrix theory is utilized to generalize the results to networks containing directed spanning trees. Finally, two examples are used to verify the validity of the derived theoretical results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				SEP	2020	129						392	402		10.1016/j.neunet.2020.06.017													
J								Distributed learning for supervised multiview feature selection	APPLIED INTELLIGENCE										Feature selection; Multiview learning; Distributed strategy; ADMM	REGRESSION; CLASSIFICATION; SCALE; IMAGE	Multiview feature selection technique is specifically designed to reduce the dimensionality of multiview data and has received much attention. Most proposed multiview supervised feature selection methods suffer from the problem of efficiently handling the large-scale and high-dimensional data. To address this, this paper designs an efficient supervised multiview feature selection method for multiclass problems by combining the distributed optimization method in the Alternating Direction Method of Multipliers (ADMM). Specifically, the distributed strategy is reflected in two aspects. On the one hand, a sample-partition based distributed strategy is adopted, which calculates the loss term of each category individually. On the other hand, a view-partition based distributed strategy is used to explore the consistent and characteristic information of views. We adopt the individual regularization on each view and the common loss term which is obtained by fusing different views to jointly share the label matrix. Benefited from the distributed framework, the model can realize a distributed solution for the transformation matrix and reduce the complexity for multiview feature selection. Extensive experiments have demonstrated that the proposed method achieves a great improvement on training time, and the comparable or better performance compared to several state-of-the-art supervised feature selection algorithms.																	0924-669X	1573-7497				SEP	2020	50	9					2749	2769		10.1007/s10489-020-01683-7													
J								Mutation probability-based lion algorithm for design and optimization of microstrip patch antenna	EVOLUTIONARY INTELLIGENCE										Microstrip patch antenna; Parameter optimization; Gain; Directivity; Lion algorithm; Mutation probability	PHOTONIC CRYSTAL; FREQUENCY; POLARIZATION; ARRAY	The advantages and performance of microstrip patch antennas (MPA) namely, reduced weight, reduced profile, and reduced cost formulate them the ideal choice for communication networks. However, the difficulties in structure size and design remain a major concern. Hence, this paper aims to propose a new model that derives a nonlinear objective model for helping in the design of solution space of antenna parameters. To attain this, it is planned to incorporate the optimization concept, thereby a new mutation probability based lion algorithm (MP-LA), which is proposed for the tuning MPA constraints. The main objective model of the antenna design is to maximize the gain by optimizing the patch length, width, thickness of substrate, and value of dielectric substrate. After executing the simulation model, this paper compares the performance of proposed MP-LA-based antenna design with numerous conventional approaches namely, antenna design without optimization, artificial bee colony-based AD, genetic-based AD, firefly-based AD, part icle swarm optimization-based AD and grey wolf optimization-based AD, proposed GWO-based AD and lion optimization algorithm-based AD. Moreover, the analysis is done with respect to radiation pattern, E-plane, and H-plane of proposed and conventional antenna designs. Other performance analysis on characteristics impedance, directivity, efficiency and gain of proposed and conventional models is done.																	1864-5909	1864-5917				SEP	2020	13	3					331	344		10.1007/s12065-019-00292-9													
J								A space transformational crow search algorithm for optimization problems	EVOLUTIONARY INTELLIGENCE										Crow search algorithm; Optimization; Space transform search	MODEL; EVOLUTION; STRATEGY	New and efficient meta-heuristic algorithms are always in demand to solve real world optimization problems due to its exploiting capability in the search domain to generate the global optimal solution. Crow search algorithm (CSA) is one of the latest meta-heuristic algorithms introduced in the literature to solve optimization tasks. The clever behaviour of crows attracted the researchers to think how to achieve a better optimization by using crow as a base element. Like other optimization algorithms, the CSA suffers with local optima and stagnation problem. In addition, for complex real world problems, CSA has not sufficient exploration capability. Therefore, in the current work, an attempt is made to enhance the explorative behaviour of the CSA by combining the space transform search (STS) method. The proposed algorithm is named as STS-CSA. The proposed STS-CSAintegrates space transformation search technique and computes the solution in current search space and transformed search space simultaneously to generate solutions that is closer to global optimum solution. To assess the performance in solving optimization problems, STS-CSA has been evaluated by applying standard IEEE CEC 2017 benchmark functions. Three real-world engineering problems are also verified to assess the effectiveness of the proposed algorithm in solving the practical problems. The performed analysis such as statistical measure, convergence analysis and complexity measure reveal that the proposed method is reliable and efficient in solving practical optimization problems.																	1864-5909	1864-5917				SEP	2020	13	3					345	364		10.1007/s12065-019-00294-7													
J								Improving many objective optimisation algorithms using objective dimensionality reduction	EVOLUTIONARY INTELLIGENCE										Evolutionary multi-objective optimisation; Many-objective optimisation; Objective dimensionality reduction; Principal component analysis	MULTIOBJECTIVE EVOLUTIONARY ALGORITHMS; DESIGN	Many-objective optimisation problems (MaOPs) have recently received a considerable attention from researchers. Due to the large number of objectives, MaOPs bring serious difficulties to existing multi-objective evolutionary algorithms (MOEAs). The major difficulties includes the poor scalability, the high computational cost and the difficulty in visualisation. A number of many-objective evolutionary algorithms (MaOEAs) has been proposed to tackle MaOPs, but existing MaOEAs have still faced with the difficulties when the number of objectives increases. Real-world MaOPs often have redundant objectives that are not only inessential to describe the Pareto-optimal front, but also deteriorate MaOEAs. A common approach to the problem is to use objective dimensionality reduction algorithms to eliminate redundant objectives. By removing redundant objectives, objective reduction algorithms can improve the search efficiency, reduce computational cost, and support for decision making. The performance of an objective dimensionality reduction strongly depends on nondominated solutions generated by MOEAs/MaOEAs. The impact of objective reduction algorithms on MOEAs and vice versa have been widely investigated. However, the impact of objective reduction algorithms on MaOEAs and vice versa have been rarely investigated. This paper studies the interdependence of objective reduction algorithms on MaOEAs. Experimental results show that combining an objective reduction algorithm with an MOEA can only successfully remove redundant objectives when the total number of objectives is small. In contrast, combining the objective reduction algorithm with an MaOEA can successfully remove redundant objectives even when the total number of objectives is large. Experimental results also show that objective reduction algorithms can significantly improve the performance of MaOEAs.																	1864-5909	1864-5917				SEP	2020	13	3					365	380		10.1007/s12065-019-00297-4													
J								Imaging the search space: a nature-inspired metaheuristic extension	EVOLUTIONARY INTELLIGENCE										Search space imaging; Metaheuristics; Optimization; Particle swarm optimization; Population initialization	OPTIMIZATION; POPULATION	Humans have a long history of exploration throughout which they have devised many imaging technologies such as telescopes, radars and satellites to increase the level of effectiveness and success of their expeditions. This paper proposes the use of imaging concepts to support the search effort of metaheuristics that deploy expedition teams simulating among other things ants, birds and chromosomes to explore the search space of optimization problems. The research involves proposing and developing a set of experimental imaging techniques. Another purpose of the paper is to measure the effectiveness of those proposed imaging techniques on improving the performance of metaheuristic searches that start from initial populations. As a case study an extend to Particle Swarm Optimization metaheuristic algorithm has been performed by implementing and incorporating the proposed imaging techniques and benchmarking them on a platform for comparing continuous optimizers in a black box setting called COCO. The performance of the developed techniques has been evaluated against each other, and against the particle swarm optimization algorithm alone based on the criterion of how many function evaluations were required to reach the set of target values defined by COCO platform. The results show that the use of imaging could produce better results.																	1864-5909	1864-5917				SEP	2020	13	3					463	474		10.1007/s12065-019-00325-3													
J								Incremental document clustering using fuzzy-based optimization strategy	EVOLUTIONARY INTELLIGENCE										Incremental document; Stop word removal; Boundary degree; Stemming; Clustering		The technical advances in the information systems contribute towards the massive availability of the documents stored in the electronic database, such as e-mails, internet and web pages. Thus, it becomes a complex task for arranging and browsing the required document. This paper proposes an incremental document clustering method for performing effective document clustering. The proposed model undergoes three steps for document clustering, namely pre-processing, feature extraction and Incremental document categorization. The pre-processing step is carried out for removing the artifacts and redundant data from the documents by undergoing stop word removal process and stemming process. Then, the next step is the feature extraction based on Term Frequency-Inverse Document Frequency (TF-IDF) and Wordnet features. Here, the feature is selected using support measure named ModSupport, and then, the incremental document clustering is performed based on the hybrid fuzzy bounding degree and Rider-Moth Flame optimization algorithm (RMFO) using the boundary degree. Here, the RMFO aims at the selection of the optimal weights for the boundary degree model and is designed by integrating Rider Optimization Algorithm (ROA) with Moth Flame optimization (MFO). The performance of the proposed RMFO outperformed the existing techniques using accuracy, F-measure, precision, and recall with maximal values 93.98%, 94.876%, 93.958% and 93.964% respectively.																	1864-5909	1864-5917				SEP	2020	13	3					497	510		10.1007/s12065-019-00335-1													
J								A parallel classification framework for protein fold recognition	EVOLUTIONARY INTELLIGENCE										Protein fold recognition; Feature selection; Map_Reduce; Parallel computing; Distributed computing; Vortex search algorithm	ENSEMBLE CLASSIFIER; BIG DATA; ALGORITHMS; PREDICTION; SEQUENCE; NETWORK	Proteins' tertiary structure, which is determined by its amino acid sequence via the protein folding process, have essential role in the function of protein. Protein fold recognition is one of the interesting studies in bioinformatics. In this paper, to address this issue, we propose a Feature Selection (FS) method based on Map_Reduce framework and Vortex Search Algorithm (VSA). FS is one of the most important steps of pre-processing data, which aims to select a variable subset of relevant features. In unparalleled mode and typical data, over hundreds of feature selection and dimension reduction algorithms have been provided such as Principle Component Analysis, Linear Discriminant Analysis, and so on. Nevertheless, these algorithms are not implemented for real-world applications when data instances increasing in three-dimensional: volume, velocity and variety that called Big Data, actually if we want to use previous feature selection methods on Big Data, volume of large and complex computing will be required. VSA was inspired from the vortex pattern created by the vortical flow of the stirred fluids. In Map_Reduce framework, Map and Reduce functions executed in parallel mode. In the proposed method, in each step of Map function, a VSA is employed to find an optimized subset of features and decrease feature search space. In the light of the above consideration, we evaluate the proposed method in classification of a benchmark dataset for protein fold recognition. The experimental results indicate that the proposed method improves prediction accuracy considerably.																	1864-5909	1864-5917				SEP	2020	13	3					525	535		10.1007/s12065-020-00350-7													
J								Using Ontology as a Strategy for Modeling the Interface Between the Cognitive and Robotic Systems	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Social robotics; Cognitive robotics; Simulation and animation	SOCIAL ROBOTICS	This work contributes to the social robotics area by defining an architecture, calledCognitive Model Development Environment(CMDE) that models the interaction between cognitive and robotic systems. The communication between these systems is formalized with the definition of an ontology, calledOntPercept, that models the perception of the environment using the information captured by the sensors present in the robotic system. The formalization offered by theOntPerceptontology simplifies the development, reproduction and comparison of experiments. The validation of the results required the development of two additional components. The first, calledRobot House Simulator(RHS), provides an environment where robot and human can interact socially with increasing levels of cognitive processing. The second component is represented by the cognitive system that models the behavior of the robot with the support of artificial intelligence based systems.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		431	449		10.1007/s10846-019-01076-0													
J								Pure Perception Motion Control based on Stochastic Nonlinear Model Predictive Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Mobile robotics; Perception driven control; Stochastic NMPC	TRACKING	Noise coming from sensors or caused by external world phenomena results in measurement errors that cause uncertainties in some robotic tasks, e.g. tracking a robot displacement and tracking an observed target. Control approaches such as model predictive control (MPC) usually guarantee constraints satisfaction by way of using detailed models of prediction. Although the deterministic MPC allows certain robustness to be controlled in the system, it usually does not adequately deal with uncertainties. Therefore, we introduce in this manuscript a pure perception motion control, which consists of an approach that deals with the uncertainty problem through a stochastic nonlinear model predictive control (SNMPC) by minimizing only the covariances matrices of target observation and robot state estimation. As introduced previously, it can be used to track targets that are observed during some tasks. The SNMPC penalizes the undesired behavior, allowing the robot to converge to the optimal pose to observe the target optimally. A modification provided both in the prediction model and the cost function allows this minimization to be achieved. The proposed stochastic nonlinear controller is validated, providing a satisfactory control of the target tracking, by way of results obtained from simulation, which are presented and discussed in the paper to verify our proposal.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		451	466		10.1007/s10846-019-01141-8													
J								A General Framework for Optimal Tuning of PID-like Controllers for Minimum Jerk Robotic Trajectories	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot control; Minimum jerk principle; Fractional PID controller; Performance indices; Particle swarm optimization	FRACTIONAL ORDER CONTROLLERS; TRACKING; DESIGN; MANIPULATORS; VELOCITY	The minimum jerk principle is commonly used for trajectory planning of robotic manipulators. However, since this principle is stated in terms of the robot's kinematics, there is no guarantee that the joint controllers will actually track the planned acceleration and jerk profiles because the tuning of the controllers' gains is decoupled from the trajectory planning. Bearing this in mind, in this paper we introduce a comprehensive framework for optimal estimation of the gains of PID-like controllers for tracking minimum-jerk (MJ) robot trajectories. The proposed methodology relies mainly on a novel variant of error-based performance indices (ISE, ITSE, IAE and ITAE) which are adapted to the tracking of MJ trajectories. Furthermore, the particle swarm optimization (PSO) algorithm is used to search for optimal values for the gains of the controllers of all joints simultaneously. The resulting approach is much simpler than recent developments based on more complex performance indices, in which joint controllers were individually optimized. The proposed approach is general enough to easily encompass the tuning of fractional PID controllers and a comprehensive set of experiments are reported comparing the performances of standard and fractional PID controllers for the task of interest.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		467	486		10.1007/s10846-019-01121-y													
J								Sliding Mode Control with Gaussian Process Regression for Underwater Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Sliding mode control; Gaussian process regression; Underwater robotic vehicle; Dynamic positioning system	PREDICTIVE CONTROL; DEPTH CONTROL; DESIGN	Sliding mode control is a very effective strategy in dealing not only with parametric uncertainties, but also with unmodeled dynamics, and therefore has been widely applied to robotic agents. However, the adoption of a thin boundary layer neighboring the switching surface to smooth out the control law and to eliminate the undesired chattering effect usually impairs the controller's performance and leads to a residual tracking error. As a matter of fact, underwater robots are very sensitive to this issue due to their highly uncertain plants and unstructured operating environments. In this work, Gaussian process regression is combined with sliding mode control for the dynamic positioning of underwater robotic vehicles. The Gaussian process regressor is embedded within the boundary layer in order to enhance the tracking performance, by predicting unknown hydrodynamic effects and compensating for them. The boundedness and convergence properties of the tracking error are analytically proven. Numerical results confirm the improved performance of the proposed control scheme when compared with the conventional sliding mode approach.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		487	498		10.1007/s10846-019-01128-5													
J								Strategies for Patrolling Missions with Multiple UAVs	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Patrolling problem; Unmanned aerial vehicles; Watershed; Evaporation; Communication	INSECT PHEROMONE STRATEGY; COVERAGE; SURVEILLANCE; COORDINATION; ENERGY	This paper proposes a set of strategies for the patrolling problem using multiple UAVs and as a result, improving our original NC-Drone algorithm. We present four strategies: Watershed Strategy, Time-based Strategies, Evaporation Strategy, and Communication-Frequency Strategy. The novel strategies consider important aspects of the patrolling movement, such as time, uncertainty, and communication. Results point out that these strategies improve the centralized version of the NC-Drone considering the uniform distribution of visits and drastically reduce in 76% the standard deviation, making the algorithm more stable. Based on the results, we found that there is a trade-off between the evaluated metrics, making it necessary to perform a large number of turns to obtain a more spatially distributed patrolling. We also present a series of strategy combinations, achieving slight improvements as more combinations are adopted. The resulting algorithm from the combination of all strategies reduces the communication frequency in 50 times and outperforms the original version of the NC-Drone in 4.5%.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		499	515		10.1007/s10846-019-01090-2													
J								Localization Using Ultra Wideband and IEEE 802.15.4 Radios with Nonlinear Bayesian Filters: a Comparative Study	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot; Localization; Non-linear Kalman filters; Wireless sensor network; Received signal strength (RSS); Ultra Wideband (UWB)	WIRELESS SENSOR NETWORK; PERFORMANCE; ALGORITHMS	In this work, we study the localization problem considering two Wireless Sensor Network (WSN) metrics commonly used for distance estimation. In this sense, we use Bayesian filters to combine odometry and distance estimations provided by the WSN devices. Our strategies aim at a general application and can be used for both indoor and outdoor environments depending only on the type of metric and radio technology employed. In this work, we investigate two metrics: Ultra Wideband (UWB) and the Received Signal Strength (RSS). The first one is a recent technology and presents better overall accuracy than other metrics, and the second is one of the most well-explored metrics in WSN-based localization approaches. We evaluated the performance of our two approaches and compare them with the Decawave (R) built-in application. The experiments were performed in simulated and real environments with different scenarios (indoors and outdoors) and sensor configurations. The results show the proposed strategies feasibility by improving the localization accuracy for both types of environments. In indoor environments, the proposed system has a mean position error bellow 0.09 meters and mean orientation error of 0.08 rads. Furthermore, the proposed system is in average 0.03 meters more accurate than the built-in application.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		571	587		10.1007/s10846-019-01126-7													
J								CNN Based Image Restoration Adjusting Ill-Exposed sRGB Images in Post-Processing	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Image enhancement; Image restoration; Deep neural networks	REFERENCE QUALITY ASSESSMENT; NORMALITY	This work proposes an artificial neural network model to restore images damaged by inadequate sensor exposure, saturation, and underexposure, at the time of acquisition. The problem has significant relevance in computational and robotics vision applications, especially when obtaining images of scenes with non-Lambertian surfaces, as well as natural images where the sensor limitation or optical arrangement prevents the scene details from being adequately represented in the captured image. We chose to model an alternative based on deep neural networks, which is adequate, considering the variability in equipment and photography techniques, along with several uncontrolled variables affecting the process. Given a set of synthetic and real image pairs, the representation structure is able to converge into a robust image enhancement model. The proposal incorporates recent advances made by convolutional networks on issues such as semantic segmentation and classification in images. The development and evaluation of the research results are primarily quantitative, using qualitative analysis when appropriate. Results measured by different indicators of image quality indicate that the proposed neural network model can improve images damaged by an amount of 3%on the best scenario on the PSNR metric.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		609	627		10.1007/s10846-019-01124-9													
J								A Robot Architecture for Outdoor Competitions	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										System architecture; Autonomous robot; Robotics competition; Kalman filters; MRAC; Tracking	MOBILE; NAVIGATION; FUSION; SYSTEM	Autonomous navigation in unstructured environments is a common topic of research, being motivated by robotic competitions and involving several sets of skills. We present a modular architecture to integrate different components for path planning and navigation of an autonomous mobile robot. This architecture was developed in order to participate in the RoboMagellan competition hosted by RoboGames. It is divided in the organizational, functional and executive levels in order to secure that the developed system has programmability, autonomy, adaptability and extensibility. Global and local localization strategies use unscented and extended Kalman filters (UKF and EKF) to fuse data from a Global Positioning System (GPS) receiver, inertial measurement unit (IMU), odometry and camera. Movement is controlled by a model reference adaptive controller (MRAC) and a proportional controller. To avoid obstacles a deformable virtual zone (DVZ) approach is used. The architecture was tested in simulated environments and with a real robot, providing a very flexible approach to testing different configurations.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		629	646		10.1007/s10846-019-01140-9													
J								Implementation of a Perceptual Controller for an Inverted Pendulum Robot	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Perceptual control theory; Inverted pendulum; Robot; LQR control	SYSTEMS	Perceptual Control Theory (PCT) theorizes that a creature's behaviour is varied so that their perception can reach and maintain certain fixed limits, despite external disturbances. The distinguishing characteristic of PCT is that the controlled variables are the inputs (perceptions, as opposed to the system outputs). This paper presents the first direct comparison of a PCT controller for a mobile robot (a two-wheeled 'inverted pendulum' balancing robot) with a classical control method, LQR. Simulations and experimental validation results show that the performance of the PCT controller is comparable to the LQR controller and better at disturbance rejection.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		683	692		10.1007/s10846-020-01158-4													
J								Wireless HROV Control with Compressed Visual Feedback Using Acoustic and RF Links	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Underwater communications; Robotics; Network simulation; Teleoperation	UNDERWATER; INTEGRATION	Underwater cooperative robotics offers the possibility to perform challenging intervention applications, such as recovering archeological objects as within the context of the MERBOTS research project, or grasping, transporting and assembly of big objects, using more than one mobile manipulator, as faced by the TWINBOT project. In order to enhance safety during the intervention, it is reasonable to avoid the umbilical, also giving more mobility to the robots, and enabling a broader set of cooperative movements. Several solutions, based on acoustic, radiofrequency (RF) or Visual Light Communication (VLC) have been proposed for underwater communications in the literature. This paper presents the architecture of an underwater wireless communication framework for the control of multiple semi-autonomous robots in cooperative interventions. The proposed framework is composed of several modules as the virtual reality interface using UWSim, the Underwater Multi-robot Cooperative Intervention Remote Control Protocol (UMCI-RCP) and a Generic Link Layer (GLL). UMCI-RCP allows the control of an underwater robot over limited communication links. UMCI-RCP integrates a progressive compression algorithm that provides visual feedback at a constant rate and ensures image reception even in channels with loses. The Time Division Multiple Access (TDMA) medium access strategy minimizes the jitter of transmitted packets. The GLL has been designed in order to provide support for multimodal transmission (i.e. acoustic, RF and VLC) and also to interface with the UWSim-NET simulator so that facilitates the experimentation either with a real or with a simulated modem. The possibility of exchange real and simulated devices in the proposed framework are demonstrated by means of a teleoperation experiment with a BlueROV equipped with the S100 RF modems. Hardware-In-the-Loop (HIL) capabilities are demonstrated repeating the experiment with the real modems and modeling the BlueROV, and also modeling both the modems and the BlueROV.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		713	728		10.1007/s10846-020-01157-5													
J								Multivariate bounded support Laplace mixture model	SOFT COMPUTING										Bounded Laplace mixture model (BLMM); Expectation maximization (EM); Feature extraction; Medical data clustering; Wavelet transforms; Image clustering; Content-based image retrieval (CBIR)	FEATURE-SELECTION; KNOWLEDGE DISCOVERY; WAVELET-DOMAIN; IMAGE; CATEGORIZATION; COMPRESSION	In this paper, bounded Laplace mixture model (BLMM) is proposed. The parameters of proposed model are estimated by maximum likelihood approach via expectation maximization and Newton-Raphson algorithm. The model is proposed for data modeling to perform clustering using synthetic data for univariate and multivariate examples and real datasets of different medical experiments. BLMM is validated through correctness of estimated parameters for synthetic data and clustering accuracy of medical datasets. A new modeling scheme is also introduced for wavelet coefficients which is based on BLMM. It is applied to image clustering and content-based image retrieval (CBIR) for feature extraction in wavelet domain. For feature extraction in this application, each image is decomposed into a set of wavelet subspaces and BLMM with two components is adopted to model the statistical characteristics of the wavelet coefficients for each wavelet subspace. The model parameters adapted from BLMM represent the image features in wavelet domain for each subspace and selected to formulate the feature space which is further used in clustering and CBIR. In the framework for clustering and image retrieval, features extracted in wavelet domain are further modeled through BLMM to categorize images into different groups and trained model is adopted for CBIR. In order to perform image retrieval with trained model via BLMM, city block distance, posterior probability and Kullback-Leibler divergence are introduced. We also propose a novel solution to compute Kullback-Leibler divergence which is very effective for image retrieval due to its low computational complexity and high retrieval rate. The effectiveness and viability of BLMM in texture image clustering and CBIR are demonstrated through UIUC, KTH-TIPS, DTD, STex and Kylberg databases. Different experiments are performed in the chosen applications, and from the results, BLMM has demonstrated its effectiveness in modeling synthetic data, real datasets from medical experiments, feature extraction in wavelet domain, image clustering and CBIR.																	1432-7643	1433-7479				SEP	2020	24	17					13239	13268		10.1007/s00500-020-04737-7													
J								Enhanced decision support system to predict and prevent hypertension using computational intelligence techniques	SOFT COMPUTING										Medical decision support system; Medical diagnosis; Support vector machine; Boosting mechanism; Fuzzy-based association rule mining; Pattern recognition; Hypertension		Medical decision support systems have been a core of intense research for years. The ongoing study shows that artificial intelligence has been accustomed to probe risk factors for hypertension. Factors, like health-damaging personal behaviors and changes in lifestyle and environment, are major contributors to chronic diseases. The goal of this research was to forecast the risk of developing hypertension by revealing hidden patterns in medical datasets. Quality of the data is the key to enhance the performance of learning model. But most healthcare data suffer from class imbalance problem, which induce the need for an intelligent model which can learn from such grimy data. This paper incorporates a novel approach by combing learning model and rule-based mining to offer decision support. Typically, the proposed work comprises two main implications. First suggests an intelligent learning model using boosting-based support vector machine to diagnose and expose multi-class categories in the imbalanced datasets. Finally, the enhanced predictive model is built upon the classification solution which will portray the innate data similarities. An intelligent fuzzy-based approach was employed to recognize frequent behavioral patterns. Based on these rules, valid decisions could be made to prevent hypertension. The suggested enhanced model is evaluated using a real-time hypertension dataset obtained through primary health centers. With the combination of ensemble strategies, the proposed intelligent learning model attains high classification accuracy for the imbalanced dataset above the traditional model. Thus, the efficient integration of personalized behavior with health data could provide a better understanding regarding patient health. In future this can serve as an eye toward personalized medicine.																	1432-7643	1433-7479				SEP	2020	24	17					13293	13304		10.1007/s00500-020-04743-9													
J								Uncertain random portfolio selection based on risk curve	SOFT COMPUTING										Uncertain random variable; Risk curve; Uncertain random portfolio selection; Mean-risk model; Optimization; Sensitivity analysis	OPTIMIZATION; MODEL; SUBJECT; INDEX; CVAR	This paper discusses the uncertain random portfolio selection problem when there are some existing risky securities which have enough historical data and some newly listed ones with insufficient data in the portfolio. So far, in the field of uncertain random portfolio selection, variance, skewness, and value-at-risk have been proposed as the risk criterion. This paper gives a new risk criterion for uncertain random portfolio selection and proposes a new type of mean-risk model based on this criterion to optimization. And in the end, a numerical example is presented for the sake of illustration.																	1432-7643	1433-7479				SEP	2020	24	17					13331	13345		10.1007/s00500-020-04751-9													
J								The feedback artificial tree (FAT) algorithm	SOFT COMPUTING										Artificial tree algorithm; Heuristic algorithms; Feedback mechanism; Self-propagating operator; Dispersive propagation operator	DIFFERENTIAL EVOLUTION; DESIGN OPTIMIZATION; GENETIC ALGORITHMS; LOCAL SEARCH; PERFORMANCE	Inspired by the transport of organic matters and the update theories of branches, the artificial tree (AT) algorithm was proposed recently. This work presents an improved version of AT algorithm that is called the feedback artificial tree (FAT) algorithm. In FAT, besides the transfer of organic matters, the feedback mechanism of moistures is introduced. Meanwhile, the self-propagating operator and dispersive propagation operator are also put forward. Some typical benchmark problems are applied to test the performance of FAT. The experimental results have clearly demonstrated the higher performance of FAT compared with AT over the tested set of problems. In addition, some well-known heuristic algorithms and their improved algorithms are also applied to validate the performance of FAT, and the computational results of FAT listed in this study are the best among these algorithms. In addition, sensitive analyses on the specific parameters of FAT algorithm are carried out, and the performance of FAT is validated.																	1432-7643	1433-7479				SEP	2020	24	17					13413	13440		10.1007/s00500-020-04758-2													
J								Effective video hyperlinking by means of enriched feature sets and monomodal query combinations	INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL										Video retrieval; TRECVID; Multimedia indexing; Feature selection	SEARCH	Video content has been increasing at an unprecedented rate in recent years, bringing the need for improved tools providing efficient access to specific contents of interest. Within the management of video content, hyperlinking aims at determining related video segments from a collection with respect to an input video anchor. This paper describes the system we designed to address feature selection for the video hyperlinking challenge, as defined by TRECVID, one of the top worldwide venues for multimedia benchmarking. The proposed solution is based on different combinations of textual and visual features, enriched to capture the various facets of the videos: automatically generated transcripts, visual concepts, video metadata, named-entity recognition, and concept-mapping techniques. The different combinations of monomodal queries are experimentally evaluated, and the impact of both parameters and single features are discussed to identify their contributions. The best performing approach at the TRECVID 2017 video hyperlinking challenge was the ensemble feature selection, which includes three different monomodal queries based on enriched feature sets.																	2192-6611	2192-662X				SEP	2020	9	3					215	227		10.1007/s13735-019-00173-y													
J								Legal public opinion news abstractive summarization by incorporating topic information	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Legal public opinion news; Abstractive summarization; Topic information; Domain knowledge		Automatically generate accurate summaries from legal public opinion news can help readers to grasp the main ideas of news quickly. Although many improved sequence-to-sequence models have been proposed for the abstractive text summarization task, these approaches confront two challenges when addressing domain-specific summarization task: (1) the appropriate selection of domain knowledge; (2) the effective manner of integrating domain knowledge into summarization model. In order to tackle the above challenges, this paper selects the pre-training topic information as the legal domain knowledge, which is then integrated into the sequence-to-sequence model to improve the performance of public opinion news summarization. Concretely, two kinds of topic information are utilized: first, the topic words which denote the main aspects of the source document are encoded to guide the decoding process. Furthermore, the predicted output is forced to have a similar topic probability distribution with the source document. We evaluate our model on a large dataset of legal public opinion news collected from micro-blog, and the experimental results show that the proposed model outperforms existing baseline systems under the rouge metrics. To the best of our knowledge, this work represents the first attempt in the legal public opinion domain for text summarization task.																	1868-8071	1868-808X				SEP	2020	11	9					2039	2050		10.1007/s13042-020-01093-8													
J								Hesitant fuzzy psychological distance measure	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Hesitant fuzzy set; Subjective preference; Psychological distance; Clustering analysis	DECISION-MAKING; PREFERENCE RELATIONS; AGGREGATION; SIMILARITY	Distance is an indispensable measure in many fields such as clustering analysis, decision making and pattern recognition, etc. When calculating the distance of hesitant fuzzy information, the existing methods normally only take the values of the attributes into consideration while ignore the preferential relationship between the options, which may not meet some actual situations. Thus, it is necessary to propose a new distance measure for hesitant fuzzy information considering both the two aspects. In order to realize this in our paper, firstly, a multi-attribute space is built, in which each attribute is given a unique weight from the experts to show the subjective importance; secondly, the distance vector between the hesitant fuzzy sets (HFSs) is constructed and a balancing coefficient is proposed; thirdly, a novel distance measure for HFS, called the hesitant fuzzy psychological distance measure is developed. In view of the experts' preferences for the options, the proposed hesitant fuzzy psychological distance between the alternatives can be enlarged relative to the traditional hesitant fuzzy distance measures, which shows a good reasonability in reflecting the experts' subjective preferences for different alternatives. Furthermore, two numerical examples are used to illustrate the effectiveness and feasibility of the hesitant fuzzy psychological distance measure.																	1868-8071	1868-808X				SEP	2020	11	9					2089	2100		10.1007/s13042-020-01102-w													
J								A novel learning-based approach for efficient dismantling of networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Network dismantling; Graph embedding; Reinforcement learning		Dismantling of complex networks is a problem to find a minimal set of nodes in which the removal leaves the network broken into connected components of sub-extensive size. It has a wide spectrum of important applications, including network immunization and network destruction. Due to its NP-hard computational complexity, this problem cannot be solved exactly with polynomial time. Traditional solutions, including manually-designed and considerably sub-optimal heuristic algorithms, and accurate message-passing ones, all suffer from low efficiency in large-scale problems. In this paper, we introduce a simple learning-based approach, CoreGQN, which seeks to train an agent that is able to smartly choose nodes that would accumulate the maximum rewards. CoreGQN is trained by hundreds of thousands self-plays on small synthetic graphs, and can then be able to generalize well on real-world networks across different types with different scales. Extensive experiments demonstrate that CoreGQN performs on par with the state-of-art algorithms at greatly reduced computational costs, suggesting that CoreGQN should be the better choice for practical network dismantling purposes.																	1868-8071	1868-808X				SEP	2020	11	9					2101	2111		10.1007/s13042-020-01104-8													
J								Multiple-instance learning via multiple-point concept based instance selection	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multiple-instance learning; Multiple-point concept; Iterative instance selection; Embedding based learning framework	CLASSIFICATION	Multiple-instance learning (MIL) is a kind of weakly supervised learning where a single label is assigned to a bag of instances. To solve MIL problems, researchers have presented an effective embedding based framework that projects bags into a new feature space, which is constructed from some selected instances that can represent target concepts to some extent. Most previous studies use single-point concepts for the instance selection, where every possible concept is represented by only a single point (i.e., instance). However, multiple points may be more powerful for the same concept than a single. In this paper, we propose the notion of multiple-point concept, jointly represented by a group of similar points, and then build an iterative instance-selection method for MIL upon Multiple-Point Concepts. The proposed algorithm is thus named MILMPC, and its main difference from other MIL algorithms is selecting instances via multiple-point concept rather than single-point concept. The experimental results on five data sets have validated the convergence of the iterative instance-selection method, and the generality of the resulting MIL model in that it performs consistently well under three different kinds of relevance evaluation criteria (used to measure the relevance of a candidate concept to the target). Furthermore, compared to other MIL algorithms, the proposed model has been demonstrated not only suitable for common MIL problems, but more suitable for hybrid problems.																	1868-8071	1868-808X				SEP	2020	11	9					2113	2126		10.1007/s13042-020-01105-7													
J								Selection of relevant texture descriptors for recognition of HEp-2 cell staining patterns	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										HEp-2 cell staining pattern recognition; Local binary pattern; Descriptor selection; Support vector machine; Rough sets	LEARNING ALGORITHMS; CLASSIFICATION; FEATURES; AUTOANTIBODIES; MACHINE	One of the important applications of computer-aided diagnosis is the detection of connective tissue disorders through automatic classification of antinuclear autoantibodies (ANAs). The recognition of ANAs is primarily done by analyzing indirect immunofluorescence (IIF) images of human epithelial type 2 (HEp-2) cells. In this regard, the paper introduces a novel approach for automatic classification of ANAs by staining pattern recognition of HEp-2 cell IIF images. Considering a set of HEp-2 cell images, the proposed method selects a set of relevant local texture descriptors for a pair of staining pattern classes, as well as identifies a set of important features corresponding to each relevant descriptor. The set of features for multiple classes is obtained from each of the important feature sets selected under various relevant local texture descriptors for all possible class-pairs. The relevance of a descriptor is evaluated based on the theory of rough hypercuboid approach, while the important feature set of a local descriptor is formed by reducing the impact of both noisy pixels present in an HEp-2 cell image and noisy HEp-2 cell images in a staining pattern class. Finally, the support vector machine is used to recognize one of the known staining patterns present in IIF images. The effectiveness of the proposed staining pattern recognition method, along with a comparison with related approaches, is illustrated on two benchmark databases of HEp-2 cell images using different classifiers and experimental set-up. The results show that the proposed approach performs significantly better than existing methods, with respect to both classification accuracy and F1 score, irrespective of the databases and classifiers used.																	1868-8071	1868-808X				SEP	2020	11	9					2127	2147		10.1007/s13042-020-01106-6													
J								An MADM approach to covering-based variable precision fuzzy rough sets: an application to medical diagnosis	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Covering-based variable precision fuzzy rough set; Fuzzy neighborhood operator; Fuzzy logical operator; Multi-attribute decision-making; Medicine selection problem	CRITERIA DECISION-MAKING; TOPSIS METHOD; 3-WAY DECISIONS; OPERATORS; MODELS; VIKOR; (I	In medical diagnosis, how to select an optimal medicine from some medicines with similar efficacy values to treat diseases has become common problems between doctors and patients. To solve this problem, we describe it as a multi-attribute decision-making (MADM) in a finite fuzzy covering approximation space. This paper aims to propose two pairs of covering-based variable precision fuzzy rough sets. By combining the proposed rough set model with the VIKOR method, we construct a novel method to medicine selection MADM problems in the context of medical diagnosis. A real-life case study of selecting a proper medicine to treat Alzheimer's disease is given to demonstrate the practicality of our proposed method. Through a comparative analysis and an experimental analysis, we further explore the effectiveness and stability of the established method.																	1868-8071	1868-808X				SEP	2020	11	9					2181	2207		10.1007/s13042-020-01109-3													
J								Learning feature spaces for regression with genetic programming	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Representation learning; Feature construction; Variation; Regression	SYMBOLIC REGRESSION; NEUROEVOLUTION; CLASSIFIERS; NETWORKS	Genetic programming has found recent success as a tool for learning sets of features for regression and classification. Multidimensional genetic programming is a useful variant of genetic programming for this task because it represents candidate solutions as sets of programs. These sets of programs expose additional information that can be exploited for building block identification. In this work, we discuss this architecture and others in terms of their propensity for allowing heuristic search to utilize information during the evolutionary process. We investigate methods for biasing the components of programs that are promoted in order to guide search towards useful and complementary feature spaces. We study two main approaches: (1) the introduction of new objectives and (2) the use of specialized semantic variation operators. We find that a semantic crossover operator based on stagewise regression leads to significant improvements on a set of regression problems. The inclusion of semantic crossover produces state-of-the-art results in a large benchmark study of open-source regression problems in comparison to several state-of-the-art machine learning approaches and other genetic programming frameworks. Finally, we look at the collinearity and complexity of the data representations produced by different methods, in order to assess whether relevant, concise, and independent factors of variation can be produced in application.																	1389-2576	1573-7632				SEP	2020	21	3			SI		433	467		10.1007/s10710-020-09383-4													
J								Enhancing unsupervised domain adaptation by discriminative relevance regularization	KNOWLEDGE AND INFORMATION SYSTEMS										Domain adaptation; Nuclear norm; Image classification; Transfer learning; Semantic segmentation	KERNEL	Unsupervised domain adaptation (UDA) serves to transfer specific knowledge from massive labeled source domain data to unlabeled target domain data via mitigating domain shift. In this paper, we propose a discriminative relevance regularization term (DRR) to enhance the performance of UDA by reducing the domain shift from the aspect of semantic relevance across domains. In particular, DRR is formulated as the min-max rank problem which seeks a projection matrix to minimize the rank of intra-class projected features and maximize the rank of the means of inter-class projected features simultaneously. To test the potential effectiveness of DRR, we design a relevance regularized distribution adaptation method (RRDA) and relevance regularized adaptation networks (RRAN) for image classification, and a relevance regularized self-supervised learning method (RRSL) for semantic segmentation by incorporation of DRR. The corresponding optimization algorithms are proposed to solve them. Experiments of cross-domain image classification show that both RRDA and RRAN outperform several state-of-the-art compared methods. Moreover, experiments of domain-adaptation semantic segmentation on two synthetic-to-real segmentation datasets demonstrate the capacity of RRSL. Such results imply the efficacy of DRR on both image classification and semantic segmentation tasks.																	0219-1377	0219-3116				SEP	2020	62	9					3641	3664		10.1007/s10115-020-01466-z													
J								Person identification using EEG channel selection with hybrid flower pollination algorithm	PATTERN RECOGNITION										EEG; Biometric; Channel selection; Flower pollination algorithm; beta-hill climbing	PARTICLE SWARM OPTIMIZATION; CLASSIFICATION; PATTERNS	Recently, electroencephalogram (EEG) signal presents a great potential for a new biometric system to deal with a cognitive task. Several studies defined the EEG with uniqueness features, universality, and natural robustness that can be used as a new track to prevent spoofing attacks. The EEG signals are the graphical recording of the brain electrical activities which can be measured by placing electrodes (channels) in various positions of the scalp. With a large number of channels, some channels have very important information for biometric system while others not. The channel selection problem has been recently formulated as an optimisation problem and solved by optimisation techniques. This paper proposes hybrid optimisation techniques based on binary flower pollination algorithm (FPA) and beta-hill climbing (called FPA beta-hc) for selecting the most relative EEG channels (i.e., features) that come up with efficient accuracy rate of personal identification. Each EEG signals with three different groups of EEG channels have been utilized (i.e., time domain, frequency domain, and time-frequency domain). The FPA beta-hc is measured using a standard EEG signal dataset, namely, EEG motor movement/imagery dataset with a real world data taken from 109 persons each with 14 different cognitive tasks using 64 channels. To evaluate the performance of the FPA beta-hc, five measurement criteria are considered:accuracy (Acc), (ii) sensitivity (Sen), (iii) F-score (F_s), (v) specificity (Spe), and (iv) number of channels selected (No. Ch). The proposed method is able to identify the personals with high Acc, Sen., F_s, Spe, and less number of channels selected. Interestingly, the experimental results suggest that FPA beta-hc is able to reduce the number of channels with accuracy rate up to 96% using time-frequency domain features. For comparative evaluation, the proposed method is able to achieve results better than those produced by binary-FPA-OPF method using the same EEG motor movement/imagery datasets. In a nutshell, the proposed method can be very beneficial for effective use of EEG signals in biometric applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107393	10.1016/j.patcog.2020.107393													
J								Robust one-stage object detection with location-aware classifiers	PATTERN RECOGNITION										Object detetion; Classification; Localization; Feature visualization; Receptive field		Recent progress on one-stage detectors focuses on improving the quality of bounding boxes, while they pay less attention to the classification head. In this work, we focus on investigating the influence of the classification head. To understand the behavior of the classifier in one-stage detectors, we resort to the methods of the Explainable deep learning area. We visualize its learned representations via activation maps and analyze its robustness to image scene context. Based on the analysis, we observe that the classifier limits the performance of the detector due to its limited receptive field and the lack of object locations. Then, we design a simple but efficient location-aware multi-dilation module (LAMD) to enhance the weak classifier. We conduct extensive experiments on the COCO benchmark to validate the effectiveness of LAMD. The results suggest that our LAMD can achieve consistent improvements and leads to robust detection across various one-stage detectors with different backbones. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107334	10.1016/j.patcog.2020.107334													
J								Supervised deep hashing with a joint deep network	PATTERN RECOGNITION										Image retrieval; Supervised hashing; CNN; RNN; Deep learning	IMAGE RETRIEVAL; QUANTIZATION; CODES	Hashing has gained great attention in large-scale image retrieval due to efficient storage and fast search. Recently, many deep hashing approaches have achieved good results since deep neural network owns powerful learning capability. However, these deep hashing approaches can perform deep features learning and binary-like codes learning synchronously, the information loss between binary-like codes and binary codes will increase due to the binarization operation. A further deficiency is that binary-like codes learning based on deep feature representations is a shallow learning procedure, which cannot fully exploit deep feature representations to generate hash codes. To solve the above problems, we propose a Deep Learning Supervised Hashing (DLSH) method which adopts deep structure to learn binary codes based on deep feature representations for large-scale image retrieval. Specifically, we integrate deep features learning module, deep mapping module and binary codes learning module in one unified architecture. The network is trained in an end-to-end way. In addition, a new objective function is designed to preserve the balancing property and semantic similarity of binary codes by incorporating the semantic similarity term and the balanceable property term. Experimental results on four benchmarks demonstrate that the proposed approach outperforms several state-of-the-art hashing methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107368	10.1016/j.patcog.2020.107368													
J								Self-attention driven adversarial similarity learning network	PATTERN RECOGNITION										Self-attention mechanism; Adversarial loss; Similarity learning network; Explainable deep learning	ONLINE	Similarity learning is a kind of machine learning algorithm that aims to measure the relevance between given objects. However, conventional similarity learning algorithms usually measure the distance between the entire given objects in the latent feature space. Consequently, the obtained similarity scores only represent how close are the entire given objects, but are incapable of demonstrating which part of them are similar to each other and how semantically similar are they. To address the above problems, in this paper, we propose a self-attention driven adversarial similarity learning network. Discriminative self-attention weights are firstly assigned to different regions of the given objects. The similarity learning step measures the relevance between these self-attention weighted feature maps of given objects under various topic vectors. The topic vectors are conditioned to capture and preserve hidden semantic information within data distribution by a generator-discriminator model with adversarial loss. This model aims to generate objects from topic vectors and propagates the difference between the generated and the real objects back to the similarity learning step, which forces the topic vectors to not only assign discriminative similarity scores to different object pairs but also further mine the hidden semantic information within data distribution. The final similarity scores represent how tight the given objects are connected to the topics. In addition, the regions with higher self-attention weights make more contribution to the discriminative similarity scores. The effectiveness of the proposed method is demonstrated through evaluations based on image retrieval task and document retrieval task and compared against various state-of-the-art algorithms in the field. The visualization results of topic vectors and self-attention weighted feature maps are demonstrated to make our proposed method explainable. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107331	10.1016/j.patcog.2020.107331													
J								Projection based weight normalization: Efficient method for optimization on oblique manifold in DNNs	PATTERN RECOGNITION										Deep learning; Weight normalization; Oblique manifold; Image classification	GEOMETRY	Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling based weight space symmetry (SBWSS) in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over the Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art neural networks. The experimental results show that our method is able to improve the performance of different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method achievers the state-of-the-art methods: we obtain test errors as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples, respectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107317	10.1016/j.patcog.2020.107317													
J								Reinterpreting CTC training as iterative fitting	PATTERN RECOGNITION										Connectionist temporal classification (CTC)		The connectionist temporal classification (CTC) enables end-to-end sequence learning by maximizing the probability of correctly recognizing sequences during training. The outputs of a CTC-trained model tend to form a series of spikes separated by strongly predicted blanks, know as the spiky problem. To figure out the reason for it, we reinterpret the CTC training process as an iterative fitting task that is based on frame-wise cross-entropy loss. It offers us an intuitive way to compare target probabilities with model outputs for each iteration, and explain how the model outputs gradually turns spiky. Inspired by it, we put forward two ways to modify the CTC training. The experiments demonstrate that our method can well solve the spiky problem and moreover, lead to faster convergence over various training settings. Beside this, the reinterpretation of CTC, as a brand new perspective, may be potentially useful in other situations. The code is publicly available at https://github.com/hzli-ucas/caffe/tree/ctc. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107392	10.1016/j.patcog.2020.107392													
J								Simplified unsupervised image translation for semantic segmentation adaptation	PATTERN RECOGNITION										Domain adaptation; Image segmentation; Image translation		Image to image translation achieves superior performance with the advent of generative adversarial networks. In this paper, we propose a Simplified Unsupervised Image Translation (SUIT) model for domain adaptation on semantic segmentation. We adopt adversarial training for superior image generation, and design a novel semantic-content loss to enhance visual appearance preservation. Thus, the high-fidelity generated images with target-style can help the model generalize to the target domain. Besides, the semantic-content loss contains two components, which focus on label- and content-consistency, respectively. Both of them can be derived from existing modules of SUIT, which makes it simple yet suitable for domain adaptation on semantic segmentation tasks. Meanwhile, since the transformation network (generator) is decoupled from the segmentation network, the former can be easily transplanted to other semantic segmentation models. Extensive experimental results demonstrate that these translated images within SUIT can significantly improve performance of the model on the target domain, and our model with FCN8s-VGG16 architecture achieves around 13 percentage points improvement in terms of mIoU on multiple semantic segmentation adaptation benchmarks. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107343	10.1016/j.patcog.2020.107343													
J								Learning residual refinement network with semantic context representation for real-time saliency object detection	PATTERN RECOGNITION										Salient object detection; Convolutional neural networks; Deep learning; Residual learning	OPTIMIZATION; IMAGE	Salient object detection (SOD) aims to precisely segment out the most attractive areas in a single image. With the rapid development of deep learning, much effort has been paid to learn an effective representation for SOD from bottom-up or top-down pathways. However, they fail to precisely separate out the whole salient object with fine boundaries due to the repeated subsampling operations such as pooling and striding leading to the loss of fine structures and spatial details. To address these issues, in this paper, we propose a residual refinement network with semantic context features for SOD. First, we design an encoder-decoder structure with side-connections to capture the sharper object boundaries, which can not only gradually recover the spatial details in each feature map from top to down, but also enhance the features at all scales with high-level semantic context information. The semantic context enhanced features are further strengthen by using a set of atrous convolutional filters with multiple atrous rates to encode multi-scale context information. Finally, using the side-output features as input, we develop a recurrent residual module to gradually learn to recover the missing boundary details in the previous coarsely predicted saliency map in a coarse-to-fine manner. Extensive evaluations on six popular SOD benchmark datasets demonstrate leading performance of the proposed approach compared with state-of-the-art methods. Especially, our approach runs in real-time at a speed of 29 fps. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107372	10.1016/j.patcog.2020.107372													
J								Gait recognition invariant to carried objects using alpha blending generative adversarial networks	PATTERN RECOGNITION										Alpha blending; Generative adversarial network; Gait recognition; Carried objects	ROBUST	Gait recognition invariant to carried objects (COs) is very difficult in a real-life scene because the COs can have various shapes and sizes, in addition to unpredictable carrying locations (e.g., front, back, and side, or multiple locations). Therefore, in this paper, we propose a robust method for gait recognition against various COs by reconstructing a gait template without COs. A straightforward approach is to directly generate a gait template without COs given a gait template with COs as the input using a conventional generative adversarial network. There is, however, a potential risk of unnecessarily altering parts that were originally unaffected by COs (e.g., leg parts for a person carrying a backpack). Because we do not want to touch such unaffected parts in the original template, we first estimate a gait template without COs, and then blend it with the original template by an estimated alpha matte that indicates the blending parameters. We then create an alpha-blended template from the original template and the generated template without COs based on the estimated alpha matte. We use two independent generators to estimate the alpha matte and the generated template without COs. Finally, we feed the alpha-blended gait template into a state-of-the-art discrimination network for gait recognition. The experimental results on three publicly available gait databases with real-life COs demonstrate the state-of-the-art performance of the proposed method. (C) 2020 The Authors. Published by Elsevier Ltd.																	0031-3203	1873-5142				SEP	2020	105								107376	10.1016/j.patcog.2020.107376													
J								Revisiting spectral clustering for near-convex decomposition of 2D shape	PATTERN RECOGNITION										Convex decomposition; Visibility; Shape signature; Spectral graph cut	PARTS; SEGMENTATION	We present a novel 2D shape decomposition algorithm via a recursive partitioning process. Starting with the contour points of a shape, we repeatedly separate the points into two parts by spectral clustering, until the stopping condition is met. Motivated by the fact that the points in a convex part are mutually visible, we regard the visibility matrix of points as the affinity matrix of spectral clustering to obtain a near-convex decomposition. Additionally, we present an efficient stopping rule to avoid over-segmentation on the shape branches. The stopping criterion is based on a novel shape signature called visible protrusion strength which can be used to measure the segmentability of a sub-shape. Finally, we demonstrate the efficiency of our algorithm on a variety of publicly available shapes, and provide qualitative and quantitative comparisons with state-of-art approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107371	10.1016/j.patcog.2020.107371													
J								Towards interpretable and robust hand detection via pixel-wise prediction	PATTERN RECOGNITION										Interpretability; Hand detection; Pixel level; Explainable representation; Rotation map		The lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model, which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 h in our experiments. Experimental results on the VIVA and Oxford hand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed. Models and code are available: https://isrc.iscas.ac.cn/gitlab/research/pr2020-phdn. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107202	10.1016/j.patcog.2020.107202													
J								Supervised dimensionality reduction of proportional data using mixture estimation	PATTERN RECOGNITION										Dimensionality reduction; Feature extraction	COMPONENT ANALYSIS; DIRICHLET	In this paper, an effective novel approach for dimensionality reduction of labeled proportional data is proposed. By avoiding formulating an eigenvalue problem and constructing a neighborhood graph, the introduced method mitigates some of the major problems from which the well-known algorithms in this category suffer. These disadvantages include problem handling multi-modal or sparse data as well as curse of dimensionality. The devised method transfers the data from high-dimensional space into low-dimensional space using a linear transform which is optimized using an information theoretic measure. To find this projection, a novel approach has been adopted in which projected data are transfered into the low-dimensional space first, and a mixture of distributions is estimated using the projected data for each class separately. In the next step, the distance between the estimated distributions is used as a measure of separation for data classes, and a heuristic search is carried on to find the optimal projection. The effectiveness of the proposed algorithm is demonstrated using different datasets in different scenarios in comparison with other well-known algorithms in the literature. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107379	10.1016/j.patcog.2020.107379													
J								Context from within: Hierarchical context modeling for semantic segmentation	PATTERN RECOGNITION										Semantic segmentation; Context modeling; Network evolution; Conditional random field; Probabilistic graphical models		Conditional Random Fields (CRFs) have been widely adopted in conjunction with Fully Convolutional Networks (FCNs) to model and integrate contextual information in the semantic segmentation procedure. In contrast to existing approaches applying CRFs in parallel or in cascade with FCNs, we propose a new paradigm to incorporate CRFs deeper inside the architecture of FCNs to model the context exhibited within the middle layers of an FCN. We approximate the mean-field inference process of a dense CRF as a multi-dimensional Gated Recurrent Unit (GRU) layer, termed CRF-GRU layer, effectively extracting intermediate context within an FCN. More importantly, multiple CRF-GRU layers can be injected into an FCN to model hierarchical contexts presented in multiple middle layers, showing competitive results on the PASCAL VOC 2012 and PASCAL-Context datasets. Secondly, we contribute a new approach to automatically learn, from the training data, the optimal segmentation architecture of the FCN with multiple CRF-GRU layers injected. The proposed approach relies on Genetic Evolution Strategies to allow the existing architecture to iteratively evolve towards higher accuracy instances. The discovered network not only outperforms state-of-the-art segmentation techniques, but also provides exciting new insights into the design of the segmentation networks. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107358	10.1016/j.patog.2020.107358													
J								Scoring disease-microRNA associations by integrating disease hierarchy into graph convolutional networks	PATTERN RECOGNITION										microRNAs; Protein coding genes; Interaction network; Graph convolutional network; Disease hierarchy	ONTOLOGY	In this study, we present an updated predictor DimiG 2.0, which uses a semi-supervised multi-label graph convolutional network (GCN) to infer disease-associated microRNAs (miRNAs) on an interaction network between protein coding genes (PCGs) and miRNAs using disease-PCG associations. DimiG 2.0 benefits from integrating the hierarchy of diseases into the GCN. DimiG 2.0 has the following updates: 1) It incorporates the hierarchy of diseases to regularize the GCN, encouraging diseases in the hierarchy to share similar miRNAs. 2) It integrates the PCGs with interacting partners but without associated diseases into model training, these unlabeled PCGs increase the size of the constructed interaction network. 3) It is able to predict associated miRNAs for 1017 diseases (updated from 248). 4) It updates expression data across tissues from the latest GTEx v7, and the expression values are quantified in Transcripts Per Million (TPM). Our results show that DimiG 2.0 outperforms state-of-the-art semi-supervised and supervised methods on the constructed benchmarked sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107385	10.1016/j.patcog.2020.107385													
J								Efficient adaptive inference for deep convolutional neural networks using hierarchical early exits	PATTERN RECOGNITION										Adaptive inference; Early exits; Bag-of-Features; Deep convolutional neural networks; Hierarchical representations	BAG; CLASSIFICATION	Early exits are capable of providing deep learning models with adaptive computational graphs that can readily adapt on-the -fly to the available resources. Despite their advantages, existing early exit methods suffer from many limitations which limit their performance, e.g., they ignore the information extracted from previous exit layers, they are unable to efficiently handle feature maps with large sizes, etc. To overcome these limitations we propose a Bag-of-Features (BoF)-based method that is capable of constructing efficient hierarchical early exit layers with minimal computational overhead, while also providing an adaptive inference method that allows for early stopping the inference process when the network is confident enough for its output, leading to significant performance benefits. To this end, the BoF model is extended and adapted to the needs of early exits by constructing additive shared histogram spaces that gradually refine the information extracted from the various layers of a network, in a hierarchical manner, while also employing a classification layer reuse strategy to further reduce the number of parameters needed per exit layer. Note that the proposed method is generic and can be readily combined with any neural network architecture. The effectiveness of the proposed method is demonstrated using five different image datasets, proving that early exits can be readily transformed into a practical tool, which can be effectively used in various real-world embedded applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107346	10.1016/j.patcog.2020.107346													
J								Binary neural networks: A survey	PATTERN RECOGNITION										Binary neural network; Deep learning; Model compression; Network quantization; Model acceleration		The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107281	10.1016/j.patcog.2020.107281													
J								Learning Direct Optimization for scene understanding	PATTERN RECOGNITION										Computer vision; Scene understanding; 3D Reconstruction; Inverse graphics; Object recognition; Scene graph; Analysis-by-synthesis; Graphics	RECOGNITION	We develop a Learning Direct Optimization (LiDO) method for the refinement of a latent variable model that describes input image x. Our goal is to explain a single image x with an interpretable 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g(z), which can be compared to the image x. The standard way to proceed is then to measure the error E(x, g(z)) between the two, and use an optimizer to minimize the error. However, it is unknown which error measure E would be most effective for simultaneously addressing issues such as misaligned objects, occlusions, textures, etc. In contrast, the LiDO approach trains a Prediction Network to predict an update directly to correct z, rather than minimizing the error with respect to z. Experiments show that LiDO converges rapidly as it does not need to perform a search on the error landscape, produces better solutions than error-based competitors, and is able to handle the mismatch between the data and the fitted scene model. We apply LiDO to a realistic synthetic dataset, and show that the method also transfers to work well with real images. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107369	10.1016/j.patcog.2020.107369													
J								Adaptive iterative attack towards explainable adversarial robustness	PATTERN RECOGNITION										Adversarial example; Adversarial attack; Image classification		Image classifiers based on deep neural networks show severe vulnerability when facing adversarial examples crafted on purpose. Designing more effective and efficient adversarial attacks is attracting considerable interest due to its potential contribution to interpretability of deep learning and validation of neural networks' robustness. However, current iterative attacks use a fixed step size for each noise-adding step, making further investigation into the effect of variable step size on model robustness ripe for exploration. We prove that if the upper bound of noise added to the original image is fixed, the attack effect can be improved if the step size is positively correlated with the gradient obtained at each step by querying the target model. In this paper, we propose Ada-FGSM (Adaptive FGSM), a new iterative attack that adaptively allocates step size of noises according to gradient information at each step. Improvement of success rate and accuracy decrease measured on ImageNet with multiple models emphasizes the validity of our method. We analyze the process of iterative attack by visualizing their trajectory and gradient contour, and further explain the vulnerability of deep neural networks to variable step size adversarial examples. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107309	10.1016/j.patcog.2020.107309													
J								Robust twin support vector regression based on rescaled Hinge loss	PATTERN RECOGNITION										Twin support vector regression; Correntropy; Gaussian noise; Outliers; Linear kernel; Non-linear kernels; Res-TSVR	PINBALL LOSS; MACHINE; CLASSIFIERS	In this work, with the help of the rescaled Hinge loss, we propose a twin support vector regression (TSVR) model that is robust to noise. The corresponding optimization problem turns out to be non-convex with smooth l(2) regularizer. To solve the problem efficiently, we convert it to its dual form, thereby transforming it into a convex optimization problem. An algorithm, named Res-TSVR, is provided to solve the formulated dual problem. The proof of the convergence of the algorithm is given. It is shown that the maximum number of iterations to achieve an epsilon-precision solution to the dual problem is O(log(1/epsilon)). We conduct a set of numerical experiments to compare the proposed method with the recently proposed robust approaches of TSVR and the standard SVR. Experimental results reveal that the proposed approach outperforms other robust methods of TSVR in terms of generalization performance and robustness to noise with comparable training time. This claim is based on the experiments performed using seven real-world data sets and three synthetic data sets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107395	10.1016/j.patcog.2020.107395													
J								Optical flow-based structure-from-motion for the reconstruction of epithelial surfaces	PATTERN RECOGNITION										3D Image mosaicing; Structure-from-Motion (SfM); Dense optical flow; Endoscopy; Dermatology		This paper details a novel optical flow-based structure from motion (SfM) approach for the reconstruction of surfaces with few textures using video sequences acquired under strong illumination changes. An original image search and grouping strategy allows to reconstruct each 3D scene point using a large set of 2D homologous points extracted from a reference image and its superimposed images acquired from different viewpoints. A variational optical flow scheme with a descriptor-based data term leads to a robust, accurate and dense homologous point determination between the image pairs. Thus, contrary to classical SfM usable for textured scenes, the proposed dense point cloud reconstruction algorithm requires neither a feature point tracking method nor any multi-view stereo technique. The performance of the proposed SfM approach is assessed on phantoms with known ground truth and on very complex patient data of various medical examinations and image modalities. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107391	10.1016/j.patcog.2020.107391													
J								Deep quantization generative networks	PATTERN RECOGNITION										Compression; Acceleration; Generative models; Network quantization		Equipped with powerful convolutional neural networks (CNNs), generative models have achieved tremendous success in various vision applications. However, deep generative networks suffer from high computational and memory costs in both model training and deployment. While many efforts have been devoted to accelerate discriminative models by quantization, effectively reducing the costs for deep generative models is more challenging and remains unexplored. In this work, we investigate applying quantization technology to deep generative models. We find that keeping as much information as possible for quantized activations is key to obtain high-quality generative models. With this in mind, we propose Deep Quantization Generative Networks (DQGNs) to effectively accelerate and compress deep generative networks. By expanding the dimensions of the quantization basis space, DQGNs can achieve lower quantization error and are highly adaptive to complex data distributions. Various experiments on two powerful frameworks (Le., variational auto-encoders, and generative adversarial networks) and two practical applications (i.e., style transfer, and super-resolution) demonstrate our findings and the effectiveness of our proposed approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107338	10.1016/j.patcog.2020.107338													
J								Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching	PATTERN RECOGNITION										Vision and language; Cross-modal retrieval; Visual-Semantic embedding		In recent years, visual-textual matching has been widely studied in the intersection of computer vision and natural language processing communities. A feasible scheme for learning discriminative representations is leveraging hierarchical features to align both modalities at multiple semantic levels. However, most existing approaches rely on pre-trained object detectors or semantic parsers to generate multi-level representations, whose performance is overly dependent on the extra supervision and thereby leads to its vulnerability. In this paper, we introduce a Stacked Squeeze-and-Excitation Recurrent Residual Network (SER2-Net) for visual-textual matching. Firstly, an efficient multi-level representation module is presented to produce a series of semantically discriminative features without the aid of extra supervision, which is built by stacking the squeeze-and-excitation recurrent residual (SER2) learning components. Specifically, SER2 incorporates the residual learning and inverse recurrent connection into the squeeze-and-excitation learning block, which allows for utilizing complementary current information and residual information to improve the modality-specific representation ability. Besides, to capture the implicit correlations contained among multi-level features, we propose a novel objective namely Cross-modal Semantic Discrepancy (CMSD) loss, which is characterized by exploiting the interdependency among different semantic levels to narrow the cross-modal distribution discrepancy. Extensive experiments on two benchmark datasets validate the superiority of our model, which compares favorably with the state-of-the-art approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107359	10.1016/j.patcog.2020.107359													
J								Deep multi-person kinship matching and recognition for family photos	PATTERN RECOGNITION										Kinship matching and recognition; Deep learning; R-CRF Algorithm	FACE VERIFICATION	In this paper, we propose a novel Deep Kinship Matching and Recognition (DKMR) framework for multi-person kinship matching and recognition, which is a complicated and challenging task with little previous literature. Compared with most existing kinship understanding methods that mainly work on matching kinship in pairwise face images, we target at recognizing the exact kinship in nuclear family photos consisting of multiple persons. The proposed DKMR framework contains three modules. Firstly, we design a deep kinship matching model (termed DKM-TRL) to predict kin-or-not scores by integrating the triple ranking loss into a Siamese CNN model. Secondly, we develop a deep kinship recognition model (named DKR-GA) to predict the exact kinship categories, in which gender and relative age attributes are utilized to learn more discriminative representations. Thirdly, based on the outputs of DKM-TRL and DKR-GA, we propose a reasoning conditional random field (R-CRF) model to infer the corresponding optimal family tree by exploiting the common kinship knowledge of a nuclear family. To evaluate the effectiveness of our DKMR framework, we conduct extensive experiments and the results show that it can gain superior performance on Group-Face dataset, TSKinFace dataset and FIW dataset over state-of-the-arts. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107342	10.1016/j.patog.2020.10732													
J								BrainPrint: EEG biometric identification based on analyzing brain connectivity graphs	PATTERN RECOGNITION										EEG biometrics; Brain functional connectivity; Person identification	FUNCTIONAL CONNECTIVITY; ARCHITECTURE	Research on brain biometrics using electroencephalographic (EEG) signals has received increasing attentions in recent years. In particular, it has been recognized that the brain functional connectivity reflects individual variability. However, many questions need to be answered before we can properly use distinctive characteristics of brain connectivity for biometric applications. This paper proposes a graph-based method for EEG biometric identification. It consists of a network estimation module to generate brain connectivity networks and a graph analysis module to generate topological features based on brain networks. Specifically, we investigate seven different connectivity metrics for the network estimation module, each of which is characterized by a certain signal interaction mechanism, defining a peculiar subjective brain network. A new connectivity metric is proposed based on the algorithmic complexity of EEG signals from a information-theoretic perspective. Meanwhile, six nodal features and six global features are proposed and studied for the graph analysis module. A comprehensive evaluation is carried out to assess the impact of different connectivity metrics, graph features, and EEG frequency bands on biometric identification performance. The results demonstrate that the graph-based method proposed in this study is effective in improving the recognition rate and inter-state stability of EEG-based biometric identification systems. Our findings about the network patterns and graph features bring a further understanding of distinctiveness of humans' EEG functional connectivity and provide useful guidance for the design of graph-based EEG biometric systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107381	10.1016/j.patcog.2020.107381													
J								Learning EEG topographical representation for classification via convolutional neural network	PATTERN RECOGNITION										Motor imagery; Electroencephalography topographical representation; Convolutional neural network; Machine learning; Signal pre-processing	BRAIN; FREQUENCY; TASKS; P300	Electroencephalography (EEG) topographical representation (ETR) can monitor regional brain activities and is emerging as a successful technique for causally exploring cortical mechanisms and connections. However, it is a challenge to find a robust method supporting high-dimensional EEG data with low signal-to-noise ratios from multiple objects and multiple channels. To address this issue, a new ETR energy calculation method for learning the EEG patterns of brain activities using a convolutional neural network is reported. It is able to customize temporal ETR training and recognize multiple objects within a common learning model. Specifically, an open-access dataset from the 2008 Brain-Computer Interface (BCI) Competition IV-2a is used for classification of five classes containing four Motor Imagery actions and one relax action. The proposed classification framework outperforms the best state-of-the-art classification method by 10.11% in average subject accuracy. Furthermore, by studying the ETR parameter optimization, a user interface for BCI applications is obtained and a real-time method implemented. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107390	10.1016/j.patcog.2020.107390													
J								Graph-based neural networks for explainable image privacy inference	PATTERN RECOGNITION										Image privacy protection; Graph neural networks; Image classification		With the development of social media and smartphones, people share their daily lives via a large number of images, but the convince also raises a problem of privacy leakage. Therefore, effective methods are needed to infer the privacy risk of images and identify images that may disclose privacy. Several works have tried to solve this problem with deep learning models. However, we know little about how the models infer the privacy label of an image, thus it is not easy to understand why the image may disclose privacy. Inspired by recent research on graph neural networks, we introduce prior knowledge to the deep models to make the inference more explainable. We propose the Graph-based neural networks for Image Privacy (GIP) to infer the privacy risk of images. The GIP mainly focuses on objects in an image, and the knowledge graph is extracted from the objects in the dataset without reliance on extra knowledge. Experimental results show that the GIP achieves higher performance compared with the object-based methods and comparable performance even compared with the multi-modal fusion method. The results show that the introduction of the knowledge graph not only makes the deep model more explainable but also makes better use of the information of objects provided by the images. Combing the knowledge graph with deep learning is a promising way to help protect image privacy that is worth exploring. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107360	10.1016/j.patcog.2020.107360													
J								Gated CNN: Integrating multi-scale feature layers for object detection	PATTERN RECOGNITION										Gated CNN; object detection; multi-scale feature layers; explainable CNN		Different convolutional layers in an explainable CNN usually encode different kinds of semantic information for an image, thus the feature fusion approaches like SSD, DSSD, and FPN are widely employed to enhance the detection performance by integrating different results based on multiple convolutional layers. However, the typical fusion approaches first need to independently detect objects based on one convolutional layer before fusion, and this single layer may exist noises or be irrelevant to objects, resulting in detection failure. To tackle the above problem, this paper proposes "Gated CNN" (short for "G-CNN") to introduce a "gate" structure to integrate multiple convolutional layers for object detection. Injected by multi-scale feature layers, a gate employs several filters to extract useful information and block noises by executing one more convolutional or deconvolutional operation simultaneously, thus a gate -based feature layer is more effective and efficient as compared to the convolutional one. Besides, GCNN employs a detector with two branches to predict the locations and categories of objects, respectively, as well as an inter-class loss to help detectors learn discrepant information among categories. Therefore, the learned detectors could better differentiate similar objects of different categories. Extensive experiments are conducted on two image datasets (PASCAL VOC and COCO), and the results demonstrate that G-CNN outperforms the state-of-the-art approaches, with a mAP of 40.9% at 10.6 FPS. (C)2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107131	10.1016/j.patcog.2019.107131													
J								Deep transductive network for generalized zero shot learning	PATTERN RECOGNITION										Generalized zero shot learning (GZSL); Transductive ZSL; KL Divergence; Deep transductive network (DTN)		Zero Shot Learning (ZSL) aims to learn projective functions on labeled seen data and transfer the learned functions to unseen classes by discovering their relationship with semantic embeddings. However, the mapping process often suffers from the domain shift problem caused by only using the labeled seen data. In this paper, we propose a novel explainable Deep Transductive Network (DTN) for the task of Generalized ZSL (GZSL) by training on both labeled seen data and unlabeled unseen data, with subsequent testing on both seen classes and unseen classes. The proposed network exploits a KL Divergence constraint to iteratively refine the probability of classifying unlabeled instances by learning from their high confidence assignments with the assistance of an auxiliary target distribution. Besides, to avoid the meaningless ascription assumption of unseen data on GZSL, we also propose an experimental paradigm by splitting the unseen data into two equivalent parts for training and testing respectively. Extensive experiments and detailed analysis demonstrate that our DTN can efficiently handle the problems and achieve the state-of-the-art performance on four popular datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107370	10.1016/j.patcog.2020.107370													
J								Video anomaly detection and localization using motion-field shape description and homogeneity testing	PATTERN RECOGNITION										Abnormal activity; Anomaly detection; Anomaly localization; Shape description; K-NN similarity-based outlier detection	ABNORMAL EVENT DETECTION; ONLINE; SCENES	Detection and localization of abnormal behaviors in surveillance videos of crowded scenes is challenging, where high-density people and various objects performing highly unpredictable motions lead to severe occlusions, making object segmentation and tracking extremely difficult. We associate the optical flows between multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor to describe such short-term trajectories, which reflects faithfully the motion trend and details in local patches. Furthermore, we propose a method to detect anomalies over time and space by judging whether the similarities between the testing sample and the retrieved K-NN samples follow the pattern distribution of homogeneous intra-class similarities, which is unsupervised one-class learning requiring no clustering nor prior assumption. Such a scheme can adapt to the whole scene, since the probability is used to judge and the calculation of probability is not affected by motion distortions arising from perspective distortion, which gains advantage over the existing solutions. We conduct experiments on real-world surveillance videos, and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in video sequences, outperforming the state-of-the-art approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107394	10.1016/j.patcog.2020.107394													
J								Spectral bounding: Strictly satisfying the 1-Lipschitz property for generative adversarial networks	PATTERN RECOGNITION										Generative adversarial networks; 1-Lipschitz constraint; Spectral bounding; Image generation	OBJECT DETECTION	Imposing the 1-Lipschitz constraint is a problem of key importance in the training of Generative Adversarial Networks (GANs), which has been proved to productively improve stability of GAN training. Although some interesting alternative methods have been proposed to enforce the 1-Lipschitz property, these existing approaches (e.g., weight clipping, gradient penalty (GP), and spectral normalization (SN)) are only partially successful. In this paper, we propose a novel method, which we refer to as spectral bounding (SB) to strictly enforce the 1-Lipschitz constraint. Our method adopts very cost-effective terms of both 1-norm and infinity-norm, and yet allows us to efficiently approximate the upper bound of spectral norms. In this way, our method provide important insights to the relationship between an alternative of strictly satisfying the Lipschitz property and explainable training stability improvements of GAN. Our proposed method thus significantly enhances the stability of GAN training and the quality of generated images. Extensive experiments are conducted, showing that the proposed method outperforms GP and SN on both CIFAR-10 and ILSVRC2015 (ImagetNet) dataset in terms of the standard inception score. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107179	10.1016/j.patcog.2019.107179													
J								Heterogenous output regression network for direct face alignment	PATTERN RECOGNITION										Direct face alignment; Multi-output regression network; Random Fourier features	REGULARIZATION; ROBUST; MODEL	Face alignment has gained great popularity in computer vision due to its wide-spread applications. In this paper, we propose a novel learning architecture, i.e., heterogenous output regression network (HORNet), for face alignment, which directly predicts facial landmarks from images. HORNet is based on kernel approximations and establishes a new compact multi-layer architecture. A nonlinear layer with cosine activations disentangles nonlinear relationships between representations of images and shapes of facial landmarks. A linear layer with identity activations explicitly encodes landmark correlations by low-rank learning via matrix elastic nets. HORNet is highly flexible and can work either with pre-built feature representations or with convolutional architectures for end-to-end learning. HORNet leverages the strengths of both kernel methods in modeling nonlinearities and of neural networks in structural prediction. This combination renders it effective and efficient for direct face alignment. Extensive experiments on five inthe-wild datasets show that HORNet delivers high performance and consistently exceeds state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107311	10.1016/j.patcog.2020.107311													
J								Unsupervised feature selection with adaptive multiple graph learning	PATTERN RECOGNITION										Feature selection; Multiple graph learning; Consensus learning	STRUCTURE PRESERVATION; RECOGNITION; REGRESSION	Unsupervised feature selection methods try to select features which can well preserve the intrinsic structure of data. To represent such structure, conventional methods construct various graphs from data. In most cases, those different graphs often contain some consensus and complementary information. To make full use of such information, we construct multiple base graphs and learn an adaptive consensus graph from these base graphs for feature selection. In our method, we integrate the multiple graph learning and the feature selection into a unified framework, which can jointly characterize the structure of the data and select the features to preserve such structure. The underlying optimization problem is hard to solve, and we solve it via a block coordinate descent schema, whose convergence is guaranteed. The extensive experiments well demonstrate the effectiveness of our proposed framework. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107375	10.1016/j.patcog.2020.107375													
J								Spectral rotation for deep one-step clustering	PATTERN RECOGNITION										Similarity matrix learning; Spectral clustering; One-step clustering; Alternating direction method of multipliers		Previous spectral clustering methods sequentially conduct three steps, i.e., similarity matrix learning from original data, spectral representation learning, and K-means clustering on spectral representation, respectively, to difficultly output robust clustering result even though each of three steps achieves individual optimization. The reason is that each goal of former two steps is not focused on achieving optimal clustering result. Moreover, original data usually contains noise to affect the clustering result, as well as has high-dimensional representation to easily result in the curse of dimensionality. In this paper, we propose a deep spectral clustering method which embeds four parts (i.e., similarity matrix learning, spectral representation learning, optimized K-means clustering, and transformation matrix learning) in a unified framework with the following advantages: 1) similarity matrix is obtained from the low-dimensional feature space of original data where the influence of both noise and high-dimensional data are considered; 2) optimized K-means clustering rotates original result of K-means clustering to search optimized clustering hyperplane which partitions data points into clusters; and 3) each of four parts is iteratively updated so that the clustering result is obtained based on the feedback of other three parts. As a result, our proposed framework develops a two-task deep clustering model with linear activation functions to output effective clustering result. Experimental results on real data sets show the effectiveness of our method in terms of four clustering evaluation metrics, compared to state-of-the-art clustering methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				SEP	2020	105								107175	10.1016/j.patcog.2019.107175													
J								On the robustness of consensus-based behaviors for robot swarms	SWARM INTELLIGENCE										Swarm robotics; Autonomous systems; Formal verification; Statistical model checking	MODEL; ALGORITHMS	In swarm robotics, behaviors requiring consensus, meaning having the robots agree on a set of variables, have attracted great attention over the years. Determining the robustness and applicability of these behaviors in harsh communication environments is an open area of research. In this paper, we propose the use of a formal software engineering technique, statistical model checking, to model and assess the robustness of consensus-based behaviors from a communication standpoint. We validate our approach on two common scenarios for a robot swarm: the election of a leader and the allocation of a set of tasks. With the proposed model, we verify the functional correctness of these consensus-based algorithms, as well as assessing their robustness to communication loss and robot failures.																	1935-3812	1935-3820				SEP	2020	14	3					205	231		10.1007/s11721-020-00183-1													
J								Optimal group selection model for large-scale group decision making	INFORMATION FUSION											PERSONALIZED INDIVIDUAL SEMANTICS; CONSENSUS-BASED MODEL; SOCIAL NETWORK; PREFERENCE-RELATION; SUPPORT; EXPERTS; INFORMATION; CONSISTENCY; SIMILARITY; WEIGHTS																		1566-2535	1872-6305				SEP	2020	61						1	12		10.1016/j.inffus.2020.03.002													
J								Feature distillation network for aspect -based sentiment analysis	INFORMATION FUSION											EXTRACTION; IMPLICIT																		1566-2535	1872-6305				SEP	2020	61						13	23		10.1016/j.inffus.2020.03.003													
J								Learning to fuse local geometric features for 3D rigid data matching	INFORMATION FUSION											OBJECT RECOGNITION; REGISTRATION; REPRESENTATION; ALGORITHM; FUSION																		1566-2535	1872-6305				SEP	2020	61						24	35		10.1016/j.inffus.2020.03.008													
J								A multiple k -means clustering ensemble algorithm to find nonlinearly separable clusters	INFORMATION FUSION											FRAMEWORK; CONSENSUS																		1566-2535	1872-6305				SEP	2020	61						36	47		10.1016/j.inffus.2020.03.009													
J								Knowledge graph fusion for smart systems: A Survey	INFORMATION FUSION											SEMANTIC WEB; LARGE-SCALE; FRAMEWORK; BASE; DBPEDIA; COMMON																		1566-2535	1872-6305				SEP	2020	61						56	70		10.1016/j.inffus.2020.03.014													
J								Fusion of short-wave infrared and visible near -infrared WorldView-3 data	INFORMATION FUSION											PAN-SHARPENING METHOD; MULTIVARIATE REGRESSION; PANSHARPENING METHODS; RESOLUTION IMAGES; CONTRAST; QUALITY; MS; CONSISTENCY; BATHYMETRY; MODULATION																		1566-2535	1872-6305				SEP	2020	61						71	83		10.1016/j.inffus.2020.03.012													
J								The fusion of Internet of Intelligent Things (IoIT) in remote diagnosis of obstructive A and a new model	INFORMATION FUSION											SLEEP-APNEA; BIG DATA; HEALTH-CARE; IOT; CLOUD; CLASSIFICATION; SYSTEM; SEGMENTATION; CHALLENGES																		1566-2535	1872-6305				SEP	2020	61						84	100		10.1016/j.inffus.2020.03.010													
J								DeciTrustNET: A graph based trust and reputation framework for social networks	INFORMATION FUSION											GROUP DECISION-MAKING; WORD-OF-MOUTH; CONSENSUS MODEL; LARGE-SCALE; SIMILARITY; CENTRALITY; PROPAGATION; DYNAMICS; SYSTEMS																		1566-2535	1872-6305				SEP	2020	61						101	112		10.1016/j.inffus.2020.03.006													
J								Stacked penalized logistic regression for selecting views in multi -view learning	INFORMATION FUSION											VARIABLE SELECTION; DISEASE; CLASSIFICATION; REGULARIZATION; MODELS; HEALTH; LASSO																		1566-2535	1872-6305				SEP	2020	61						113	123		10.1016/j.inffus.2020.03.007													
J								Explainable decision forest: Transforming a decision forest into an interpretable tree	INFORMATION FUSION											ROC CURVE; CLASSIFIERS; SELECTION; MODELS; AREA																		1566-2535	1872-6305				SEP	2020	61						124	138		10.1016/j.inffus.2020.03.013													
J								A low latency sequential model and its user -focused evaluation for automatic punctuation of ASR closed captions	COMPUTER SPEECH AND LANGUAGE										Punctuation; Recurrent neural network; LSTM; Maximum entropy; Low latency; Real-time modelling; User-focused evaluation; Mean opinion score; Closed captioning	CAPITALIZATION	In Automatic Speech Recognition (ASR), inserting the punctuation marks into the word chain hypothesis has long been given low priority, as efforts were concentrated on minimizing word error rates. Punctuation, however, also has a high impact on the transcription quality perceived by the users. Prosody, textual context and their combination have since been used successfully for automatic punctuation of ASR outputs. The recently proposed RNN based solutions show encouraging performance. We believe that current bottlenecks of punctuation technology are on one hand the complex punctuation models, which, having high latency, are not suitable for use-cases with real-time requirements; and on the other hand, punctuation efforts have not been validated against human perception and user impression. The ambition of this paper is to propose a lightweight, yet powerful RNN punctuation model for on-line (real-time including low latency) environment, and also to assess user opinion, in general and also for target users living with hearing loss or impairment. The proposed online RNN punctuation model is evaluated against a Maximum Entropy (MaxEnt) baseline, for Hungarian and for English, whereas subjective assessment tests are carried out on real broadcast data subtitled with ASR (closed captioning). As it can be expected, the RNN outperforms the MaxEnt baseline system, but of course not the off-line systems: limiting the future context to minimize latency results only in a slighter performance drop, but ASR errors obviously influence punctuation performance considerably. A genre analysis is also carried out w.r.t. the punctuation performance showing that both recognition and punctuation of more spontaneous speech styles is challenging. Overall, the subjective tests confirmed that users perceive a significant quality improvement when punctuation is added, even in presence of word errors and even if punctuation is automatic and hence itself may contain further errors. For users living with hearing loss or deafness, an even higher, clear preference for the punctuated captions could be confirmed. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101076	10.1016/j.csl.2020.101076													
J								End-to-end response selection based on multi-level context response matching	COMPUTER SPEECH AND LANGUAGE										Retrieval systems; Chatbots; Neural networks; Goal-oriented dialogue systems; DSTC		This paper presents our work on the Dialog System Technology Challenges 7 (DSTC7). We took part in Track 1 on sentence selection which evaluates response retrieving in dialog systems on more realistic test scenarios compared to the state-of-the-art evaluations. Our proposed dialog system matches the context with the best response by computing their semantic similarity on word and sequence levels. Evaluation results on the datasets provided show the effectiveness of our system by achieving higher performance compared to the provided baseline system. Our system enjoys the advantages of its simple and end-to-end architecture making its training and adaptation to other domains easier. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101080	10.1016/j.csl.2020.101080													
J								Deep generative variational autoencoding for replay spoof detection in automatic speaker verification	COMPUTER SPEECH AND LANGUAGE										Anti-spoofing; Presentation attack detection; Replay attack; Countermeasures; Deep generative models	REPRESENTATIONS; CLASSIFICATION	Automatic speaker verification (ASV) systems are highly vulnerable to presentation attacks, also called spoofing attacks. Replay is among the simplest attacks to mount - yet difficult to detect reliably. The generalization failure of spoofing countermeasures (CMs) has driven the community to study various alternative deep learning CMs. The majority of them are supervised approaches that learn a human-spoof discriminator. In this paper, we advocate a different, deep generative approach that leverages from powerful unsupervised manifold learning in classification. The potential benefits include the possibility to sample new data, and to obtain insights to the latent features of genuine and spoofed speech. To this end, we propose to use variational autoencoders (VAEs) as an alternative backend for replay attack detection, via three alternative models that differ in their class-conditioning. The first one, similar to the use of Gaussian mixture models (GMMs) in spoof detection, is to train independently two VAEs - one for each class. The second one is to train a single conditional model (C-VAE) by injecting a one-hot class label vector to the encoder and decoder networks. Our final proposal integrates an auxiliary classifier to guide the learning of the latent space. Our experimental results using constant-Q cepstral coefficient (CQCC) features on the ASVspoof 2017 and 2019 physical access subtask datasets indicate that the C-VAE offers substantial improvement in comparison to training two separate VAEs for each class. On the 2019 dataset, the C-VAE outperforms the VAE and the baseline GMM by an absolute 9 - 10% in both equal error rate (EER) and tandem detection cost function (t-DCF) metrics. Finally, we propose VAE residuals - the absolute difference of the original input and the reconstruction as features for spoofing detection. The proposed frontend approach augmented with a convolutional neural network classifier demonstrated substantial improvement over the VAE backend use case. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101092	10.1016/j.csl.2020.101092													
J								Learning Multi -Level Information for Dialogue Response Selection by Highway Recurrent Transformer	COMPUTER SPEECH AND LANGUAGE										Response selection; Transformer; Attention mechanism; Dialogue; DSTC		With increasing research interests in dialogue modeling, there is an emerging branch that formulates this task as next sentence selection, where given the partial dialogue context, the goal is to determine the most probable next sentence. To model natural language information, recurrent models have been applied to sequence modeling and shown promising results in various NLP tasks (Sutskever et al., 2014). Recently, the Transformer (Vaswani et al., 2017) has advanced modeling semantics for natural language sentences via attention, achieving improvement for sequence modeling. However, the Transformer focuses on modeling the intra-sentence attention but ignores inter-sentence information. In terms of dialogue modeling, the cross-sentence information is salient to understand dialogue content, so that the response selection can be better determined. Therefore, this paper proposes a novel attention mechanism based on multi-head attention, called highway attention, in order to allow the model to pass information through multiple sentences, and then builds a recurrent model based on the Transformer and the proposed highway attention. We call this model Highway Recurrent Transformer. This model focuses on not only intra-sentence dependency, but also inter-sentence dependency in the structure of dialogues. Experiments on the response selection task of the seventh Dialog System Technology Challenge (DSTC7) demonstrate that the proposed Highway Recurrent Transformer is capable of modeling both utterance-level and dialogue-level information for achieving better performance than the original Transformer in the single positive response scenario. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101073	10.1016/j.csl.2020.101073													
J								RAP -Net: Recurrent Attention Pooling Networks for Dialogue Response Selection	COMPUTER SPEECH AND LANGUAGE										Attention; Dialogue modeling; Response selection; DSTC		The response selection has been an emerging research topic due to the growing interest in dialogue modeling, where the goal of the task is to select an appropriate response for continuing dialogues. To further push the end-to-end dialogue model toward real-world scenarios, the seventh Dialog System Technology Challenge (DSTC7) proposed a challenge track based on real chatlog datasets. The competition focuses on dialogue modeling with several advanced characteristics: (1) natural language diversity, (2) capability of precisely selecting a proper response from a large set of candidates or the scenario without any correct answer, and (3) knowledge grounding. This paper introduces recurrent attention pooling networks (RAP-Net), a novel framework for response selection, which can well estimate the relevance between the dialogue contexts and the candidates. The proposed RAP-Net is shown to be effective and can be generalize across different datasets and settings in the DSTC7 experiments. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101079	10.1016/j.csl.2020.101079													
J								Leveraging Linguistic Context in Dyadic Interactions to Improve Automatic Speech Recognition for Children	COMPUTER SPEECH AND LANGUAGE										Child speech; Automatic speech recognition; Autism spectrum disorder; Forensic interviews	VOCAL-TRACT LENGTH; SEXUAL-ABUSE; LANGUAGE; ADAPTATION; NETWORKS; PROTOCOL; FEATURES; ENGLISH; SYSTEMS; AUTISM	Automatic speech recognition for child speech has been long considered a more challenging problem than for adult speech. Various contributing factors have been identified such as larger acoustic speech variability including mispronunciations due to continuing biological changes in growth, developing vocabulary and linguistic skills, and scarcity of training corpora. A further challenge arises when dealing with spontaneous speech of children involved in a conversational interaction, and especially when the child may have limited or impaired communication ability. This includes health applications, one of the motivating domains of this paper, that involve goal-oriented dyadic interactions between a child and clinician/adult social partner as a part of behavioral assessment. In this work, we use linguistic context information from the interaction to adapt speech recognition models for children speech. Specifically, spoken language from the interacting adult speech provides the context for the child's speech. We propose two methods to exploit this context: lexical repetitions and semantic response generation. For the latter, we make use of sequence-to-sequence models that learn to predict the target child utterance given context adult utterances. Long-term context is incorporated in the model by propagating the cell-state across the duration of conversation. We use interpolation techniques to adapt language models at the utterance level, and analyze the effect of length and direction of context (forward and backward). Two different domains are used in our experiments to demonstrate the generalized nature of our methods - interactions between a child with ASD and an adult social partner in a play-based, naturalistic setting, and in forensic interviews between a child and a trained interviewer. In both cases, context-adapted models yield significant improvement (upto 10.71% in absolute word error rate) over the baseline and perform consistently across context windows and directions. Using statistical analysis, we investigate the effect of source-based (adult) and target-based (child) factors on adaptation methods. Our results demonstrate the applicability of our modeling approach in improving child speech recognition by employing information transfer from the adult interlocutor. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101101	10.1016/j.csl.2020.101101													
J								Multilingual stance detection in social media political debates	COMPUTER SPEECH AND LANGUAGE										Stance detection; Multilingual; Contextual features; Political debates; Twitter		Stance Detection is the task of automatically determining whether the author of a text is in favor, against, or neutral towards a given target. In this paper we investigate the portability of tools performing this task across different languages, by analyzing the results achieved by a Stance Detection system (i.e. MultiTACOS) trained and tested in a multilingual setting. First of all, a set of resources on topics related to politics for English, French, Italian, Spanish and Catalan is provided which includes: novel corpora collected for the purpose of this study, and benchmark corpora exploited in Stance Detection tasks and evaluation exercises known in literature. We focus in particular on the novel corpora by describing their development and by comparing them with the benchmarks. Second, MultiTACOS is applied with different sets of features especially designed for Stance Detection, with a specific focus to exploring and combining both features based on the textual content of the tweet (e.g., style and affective load) and features based on contextual information that do not emerge directly from the text. Finally, for better highlighting the contribution of the features that most positively affect system performance in the multilingual setting, a features analysis is provided, together with a qualitative analysis of the misclassified tweets for each of the observed languages, devoted to reflect on the open challenges. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101075	10.1016/j.csl.2020.101075													
J								Hierarchical multimodal attention for end -to -end audio-visual scene -aware dialogue response generation	COMPUTER SPEECH AND LANGUAGE										Dialogue system; Audio-visual scene-aware dialogue; Neural network; Multimodal attention; Response generation		This work is extended from our participation in the 7th Dialogue System Technology Challenge (DSTC7), where we participated in the Audio Visual Scene-aware Dialogue System (AVSD) track. The AVSD track evaluates how dialogue systems understand video scenes and responds to users about the video visual and audio content. We propose a hierarchical attention approach on user queries, video caption, audio and visual features that contribute to improved evaluation results. We also apply a nonlinear feature fusion approach to combine the visual and audio features for better knowledge representation. Our proposed model shows superior performance in terms of both objective evaluation and human rating as compared to the baselines. In this extended work, we also provide a more extensive review of the related work, conduct additional experiments with word-level and context-level pretrained embeddings, and investigate different qualitative aspects of the generated responses. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101095	10.1016/j.csl.2020.101095													
J								13 years of speaker recognition research at BUT, with longitudinal analysis of NIST SRE	COMPUTER SPEECH AND LANGUAGE										Speaker recognition; NIST; Evaluations; GMM; Eigen-channel compensation; JFA; I-vectors; DNN Embedding; X-vectors	SCORE NORMALIZATION	In this paper, we present a brief history and a "longitudinal study" of all important milestone modelling techniques used in text independent speaker recognition since Brno University of Technology (BUT) first participated in the NIST Speaker Recognition Evaluation (SRE) in 2006-GMM MAP, GMM MAP with eigen-channel adaptation, Joint Factor Analysis, i-vector and DNN embedding (x-vector). To emphasize the historical context, the techniques are evaluated on all NIST SRE sets since 2004 on a time-machine principle, i.e. a system is always trained using all data available up till the year of evaluation. Moreover, as user-contributed audiovisual content dominates nowadays' Internet, we representatively include the Speakers In The Wild (SITW) and VOiCES challenge datasets in the evaluation of our systems. Not only we present a comparison of the modelling techniques, but we also show the effect of sampling frequency. (c) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101035	10.1016/j.csl.2019.101035													
J								Optimization of the area under the ROC curve using neural network supervectors for text -dependent speaker verification	COMPUTER SPEECH AND LANGUAGE										Text dependent speaker verification; Supervectors; Alignment; Triplet neural network; AUC	RECOGNITION; SCORES	This paper explores two techniques to improve the performance of text-dependent speaker verification systems based on deep neural networks. Firstly, we propose a general alignment mechanism to keep the temporal structure of each phrase and obtain a supervector with the speaker and phrase information, since both are relevant for a text-dependent verification. As we show, it is possible to use different alignment techniques to replace the global average pooling providing significant gains in performance. Moreover, we also present a novel Backend approach to train a neural network for detection tasks by optimizing the Area Under the Curve (AUC) as an alternative to the usual triplet loss function, so the system is end-to-end, with a cost function close to our desired measure of performance. As we can see in the experimental section, this approach improves the system performance, since our triplet neural network based on an approximation of the AUC (aAUC) learns how to discriminate between pairs of examples from the same identity and pairs of different identities. The different alignment techniques to produce supervectors in addition to the new Back-end approach were tested on the RSR2015-Part I and RSR2015-Part II database for text-dependent speaker verification, providing competitive results compared to similar size networks using the global average pooling to extract supervectors and using a simple Back-end or triplet loss training. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								101078	10.1016/j.csl.2020.101078													
J								Generalized end -to -end detection of spoofing attacks to automatic speaker recognizers	COMPUTER SPEECH AND LANGUAGE										Voice biometrics; Presentation attacks detection; Speaker verification; Convolutional neural networks	NEURAL-NETWORK; RECOGNITION	As automatic speaker recognizer systems become mainstream, voice spoofing attacks are on the rise. Common attack strategies include replay, the use of text-to-speech synthesis, and voice conversion systems. While previously-proposed end-to-end detection frameworks have shown to be effective in spotting attacks for one particular spoofing strategy, they have relied on different models, architectures, and speech representations, depending on the spoofing strategy. In practice, however, one does not have a priori information regarding the strategy an attacker might employ to fool a speaker recognizer, thus it is necessary to devise approaches which are able to detect attacks regardless of the strategy employed to generate them. In this work, we introduce an end-to-end ensemble based approach such that two models - previously shown to perform well on each considered attack strategy - are trained jointly, while a third model learns how to mix their outputs yielding a single score. Experimental results with replay and text-to-speech/voice conversion attacks show the proposed ensemble method achieving similar or superior performance when compared to systems specialized on each spoofing strategy separately. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101096	10.1016/j.csl.2020.101096													
J								Transfer learning from adult to children for speech recognition: Evaluation, analysis and recommendations	COMPUTER SPEECH AND LANGUAGE										Analysis of children's speech; Children speech recognition; Automatic speech recognition; Deep learning; Transfer learning; Deep neural network	MFCC FEATURES	Children speech recognition is challenging mainly due to the inherent high variability in children's physical and articulatory characteristics and expressions. This variability manifests in both acoustic constructs and linguistic usage due to the rapidly changing developmental stage in children's life. Part of the challenge is due to the lack of large amounts of available children speech data for efficient modeling. This work attempts to address the key challenges using transfer learning from adult's models to children's models in a Deep Neural Network (DNN) framework for children's Automatic Speech Recognition (ASR) task evaluating on multiple children's speech corpora with a large vocabulary. The paper presents a systematic and an extensive analysis of the proposed transfer learning technique considering the key factors affecting children's speech recognition from prior literature. Evaluations are presented on (i) comparisons of earlier GMM-HMM and the newer DNN Models, (ii) effectiveness of standard adaptation techniques versus transfer learning, (iii) various adaptation configurations in tackling the variabilities present in children speech, in terms of (a) acoustic spectral variability, and (b) pronunciation variability and linguistic constraints. Our Analysis spans over (i) number of DNN model parameters (for adaptation), (ii) amount of adaptation data, (iii) ages of children, (iv) age dependent-independent adaptation. Finally, we provide Recommendations on (i) the favorable strategies over various aforementioned - analyzed parameters, and (ii) potential future research directions and relevant challenges/problems persisting in DNN based ASR for children's speech. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101077	10.1016/j.csl.2020.101077													
J								Recurrent neural network language generation for spoken dialogue systems	COMPUTER SPEECH AND LANGUAGE										Dialogue systems; Recurrent neural networks; Natural language generation; Domain adaptation; Discriminative training; Human evaluation		Natural Language Generation (NLG) is a critical component of spoken dialogue systems and it has a significant impact both on usability and perceived quality. Most existing NLG approaches in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. Moreover, these limitations also add significantly to development costs and make the delivery of cross-domain, cross-lingual dialogue systems especially complex and expensive. The first contribution of this paper is to present RNNLG, a Recurrent Neural Network (RNN)-based statistical natural language generator that can learn to generate utterances directly from dialogue act - utterance pairs without any predefined syntaxes or semantic alignments. The presentation includes a systematic comparison of the principal RNN-based NLG models available. The second contribution, is to test the scalability of the proposed system by adapting models from one domain to another. We show that by pairing RNN-based NLG models with a proposed data counterfeiting method and a discriminative objective function, a pretrained model can be quickly adapted to different domains with only a few examples. All of the findings presented are supported by both corpus-based and human evaluations. (C) 2019 Published by Elsevier Ltd.																	0885-2308	1095-8363				SEP	2020	63								UNSP 101017	10.1016/j.csl.2019.06.008													
J								Knowledge-Grounded Response Generation with Deep Attentional Latent-Variable Model	COMPUTER SPEECH AND LANGUAGE										Knowledge-grounded; Response generation; Variational model		End-to-end dialogue generation has achieved promising results without using handcrafted features and attributes specific to each task and corpus. However, one of the fatal drawbacks in such approaches is that they are unable to generate informative utterances, so it limits their usage from some real-world conversational applications. In order to tackle this issue, this paper attempts to generate diverse and informative responses with a variational generation model, which contains a joint attention mechanism conditioning on the information from both dialogue contexts and extra knowledge. The experiments on benchmark DSTC7 data show that the proposed method generates responses with more grounded knowledge and improve the diversity of generated language. (c) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				SEP	2020	63								101069	10.1016/j.csl.2020.101069													
J								An NLP-Powered Human Rights Monitoring Platform	EXPERT SYSTEMS WITH APPLICATIONS										Crowdsourcing; Human Rights Monitoring; Machine Learning; Natural Llanguage Pprocessing; Social Mmedia; Twitter; Ceasefire; Applications	SENTIMENT ANALYSIS	Effective information management has long been a problem in organisations that are not of a scale that they can afford their own department dedicated to this task. Growing information overload has made this problem even more pronounced. On the other hand we have recently witnessed the emergence of intelligent tools, packages and resources that made it possible to rapidly transfer knowledge from the academic community to industry, government and other potential beneficiaries. Here we demonstrate how adopting state-of-the-art natural language processing (NLP) and crowdsourcing methods has resulted in measurable benefits for a human rights organisation by transforming their information and knowledge management using a novel approach that supports human rights monitoring in conflict zones. More specifically, we report on mining and classifying Arabic Twitter in order to identify potential human rights abuse incidents in a continuous stream of social media data within a specified geographical region. Results show deep learning approaches such as LSTM allow us to push the precision close to 85% for this task with an Fl-score of 75%. Apart from the scientific insights we also demonstrate the viability of the framework which has been deployed as the Ceasefire Iraq portal for more than three years which has already collected thousands of witness reports from within Iraq. This work is a case study of how progress in artificial intelligence has disrupted even the operation of relatively small-scale organisations. Crown Copyright (C) 2020 Published by Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113365	10.1016/j.eswa.2020.113365													
J								Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms	EXPERT SYSTEMS WITH APPLICATIONS										Speech recognition; Phoneme classification; Applied hyperheuristics; Multi-objective evolutionary computation	FORESTS	Recent advances in the availability of computational resources allow for more sophisticated approaches to speech recognition than ever before. This study considers Artificial Neural Network and Hidden Markov Model methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet rather than the classical approach of the classification of whole words and phrases, with a specific focus on both single and multi-objective evolutionary optimisation of bioinspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico and the recordings are transformed into a static dataset of statistics by way of their Mel-Frequency Cepstral Coefficients (MFCC) at sliding window length of 200ms as well as a reshaped MFCC timeseries format for forecast-based models. An deep neural network with evolutionary optimised topology achieves 90.77% phoneme classification accuracy in comparison to the best HMM that achieves 86.23% accuracy with 150 hidden units, when only accuracy is considered in a single-objective optimisation approach. The obtained solutions are far more complex than the HMM taking around 248 seconds to train on powerful hardware versus 160 for the HMM. A multi-objective approach is explored due to this. In the multi-objective approaches of scalarisation presented, within which real-time resource usage is also considered towards solution fitness, far more optimal solutions are produced which train far quicker than the forecast approach (69 seconds) with classification ability retained (86.73%). Weightings towards either maximising accuracy or reducing resource usage from 0.1 to 0.9 are suggested depending on the resources available, since many future IoT devices and autonomous robots may have limited access to cloud resources at a premium in comparison to the GPU used in this experiment. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113402	10.1016/j.eswa.2020.113402													
J								A hybrid heuristic algorithm for cyclic inventory-routing problem with perishable products in VMI supply chain	EXPERT SYSTEMS WITH APPLICATIONS										Hybrid heuristic algorithm; Cyclic inventory-routing problem; Perishable products; Vendor managed inventory supply chain	VENDOR-MANAGED INVENTORY; CUCKOO SEARCH; OPTIMIZATION; TIME; DELIVERY; SYSTEM; DEMAND	The VMI supply chain can bring benefit to concerned parties. The paper studies cyclic inventory-routing problem (IRP) under VMI policy. Cyclic IRP as a variant of the IRP belong to long-term decision. It means once the replenishment policy and vehicle routing are determined, they will stay the same in the following periods. This paper considers the loss cost caused by perishability of perishable products and assumes the demand is dependent on price and stock. Based on the above consideration, three different cyclic IRP models for perishable products with stock and price dependent demand in VMI supply chain are put forward. They are IRP model ending with shortage, IRP model starting with shortage, and IRP model with no shortage. These models are composed of a single manufacturer and multiple retailers. The objective is to minimize the average total cost. The total cost includes not only fixed and transportation cost of vehicles, inventory (order and holding) and shortage cost of retailers, but also startup and holding cost of manufacturer. The proposed models are nonlinear mixed integer programming models and NP-hard problem. In order to solve these models, a hybrid heuristic algorithm that is developed by combining cuckoo algorithm with improved Clarke-Wright savings algorithm is developed. In computational experiments, the proposed algorithm is compared with the optimization solver. The results demonstrate the proposed algorithm outperforms the optimization solver. In addition, the results show the average total cost may be reduced by using the strategy of stockout in some cases. Sensitivity analysis is also implemented to study the influence of parameters on optimal solution. The results of computational experiments validate the applicability of the proposed models and the effectiveness of hybrid heuristic algorithm. The study also provides policymakers with management implications. Finally, the conclusions and future research directions are given. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				SEP 1	2020	153								113322	10.1016/j.eswa.2020.113322													
J								An ensemble-based approach to the security-oriented classification of low-level log traces	EXPERT SYSTEMS WITH APPLICATIONS										Business process analysis; Process mining; Security; Classification	INFORMATION-SYSTEMS	Traditionally, Expert Systems have found a natural application in the behavioral analysis of processes. In fact, they have proved effective in the tasks of interpreting the data collected during the process executions and of analyzing these data with the aim of diagnosing/detecting anomalies. In this context, we focus on log data generated by executions of business processes, and consider the issue of detecting "insecure" process instances, involving some kind of security breach (e.g. attacks, frauds). We propose a hybrid framework for accomplishing a security-oriented classification of activity-unaware traces, i.e., traces consisting of "low-level" events with no explicit reference to the "high-level" activities the analysts are typically familiar with. The framework integrates two classification approaches traditionally used as alternative ways to decide on the "secureness" of process traces: (i) a model-driven approach, using knowledge of behavioral models expressed at the abstraction level of the activities, and (ii) an example driven approach, exploiting the availability of event sequences labeled by experts as symptomatic of "secure" or "in-secure" behavior. The core of our solution is a meta-classifier combining (i) and (ii) thanks to a probabilistic Montecarlo mechanism that allows the traces to be simultaneously viewed as sequences of low-level events and of high-level activities. The framework has been empirically proved effective in jointly exploiting the two aforementioned forms of knowledge/expertise, typically coming from different experts, and in acting as a sort of "super-expert" classification tool. Its accuracy and efficiency make it a solid basis for implementing a novel kind of expert system for the security-oriented monitoring/analysis of business processes. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113386	10.1016/j.eswa.2020.113386													
J								Hybrid flow shop with multiple servers: A computational evaluation and efficient divide-and-conquer heuristics	EXPERT SYSTEMS WITH APPLICATIONS										Scheduling; Hybrid flowshop; Secondary resource; Multiple-server; setup operator; Memory-based; Divide-and-conquer; Makespan; Heuristics	SEQUENCE-DEPENDENT SETUP; HIGH-PERFORMING HEURISTICS; SCHEDULING PROBLEMS; PERMUTATION FLOWSHOP; MINIMIZE MAKESPAN; MACHINE; ALGORITHMS; TIMES; CLASSIFICATION; OPTIMIZATION	This paper focuses on the minimisation of the makespan in a hybrid flow shop layout with multiple servers and identical machines. Servers are renewable secondary resources responsible of executing the setup times of the jobs. Although the use of human supervision is very extensive in real manufacturing scenarios, its study in academia is still very scarce. In fact, to the best of our knowledge, the hybrid flow shop with servers has not been addressed in the literature so far. Hence, we first analyse the problem and identify a number of problem properties. By using these properties, we design two constructive heuristics based on a divide-and-conquer mechanism and four composite heuristics based on memory-based procedures and local search that use an efficient representation of the solutions. In addition, in order to picture the state-of-the-art of the most efficient heuristics for this problem, we re-implement and adapt the most promising heuristics from related scheduling problems. All these heuristics, a total of 31, are compared in an extensive computational evaluation with 1620 instances. The results show the excellent performance of the heuristics proposed. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113462	10.1016/j.eswa.2020.113462													
J								Adaptive variable neighborhood search solution methods for the fleet size and mix pollution location-inventory-routing problem	EXPERT SYSTEMS WITH APPLICATIONS										Green logistics optimization; Metaheuristics; Location; Inventory; Routing; Fleet composition	SUPPLY CHAIN SUSTAINABILITY; JUST-IN-TIME; HETEROGENEOUS FLEET; OPTIMIZATION MODEL; HEURISTIC METHOD; DEPOT LOCATION; ALGORITHM; NETWORK; DESIGN; UNCERTAINTY	This work introduces the Fleet-size and Mix Pollution Location-Inventory-Routing Problem with Just-inTime replenishment policy and Capacity Planning. This problem extends the strategic-level decisions of classic LIRP by considering capacity selection decisions and heterogeneous fleet composition. An MIP formulation of this new complex combinatorial optimization problem is proposed and small-sized problem instances are solved using the CPLEX solver. For the solution of more realistic-sized problem instances, a General Variable Neighborhood Search (GVNS)-based framework is adopted. Novel adaptive shaking methods are proposed as intelligent components of the developed GVNS algorithms to further improve their performance. To evaluate the proposed GVNS schemes, several problem instances are randomly generated by following specific instructions from the literature and adopting real vehicles' parameters. Comparisons between these solutions and the corresponding ones achieved by CPLEX are made. The computational results indicate the efficiency of the proposed GVNS-based algorithms, with the best GVNS scheme to produce 7% better solutions than CPLEX for small problems. Finally, the economic and environmental impacts of using either homogeneous or heterogeneous fleet of vehicles are examined. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113444	10.1016/j.eswa.2020.113444													
J								An improved differential evolution algorithm with dual mutation strategies collaboration	EXPERT SYSTEMS WITH APPLICATIONS										Differential evolution; Elite guidance; Dual mutation strategies; Trade-off strategy	PARTICLE SWARM OPTIMIZATION; CONTROL PARAMETERS; SELECTION; ENSEMBLE; SEARCH	To reduce the effect of the selections of mutation strategies and control parameters on the performance of differential evolution (DE), this paper proposes an improved differential evolution algorithm with dual mutation strategies collaboration (DMCDE), in which two main improvements are presented. First, DMCDE introduces an elite guidance mechanism to propose two new variants of the classical DE/rand/2 and DE/best/2 mutation strategies, which we call DE/e-rand/2 and DE/e-best/2 respectively. They use the individuals randomly chosen from superior elite population as the base vector and the first vector of difference vectors, thereby providing clearer guidance for individual mutation without losing randomness. Second, a mechanism of dual mutation strategies collaboration is utilized to obtain a trade-off between global exploration and local exploitation of the algorithm. The performance of DMCDE is evaluated by using the commonly used test functions as well as a real-world optimization problem. The results show that DMCDE can significantly improve the optimization performance of DE, and is superior to the comparative competitors. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113451	10.1016/j.eswa.2020.113451													
J								GRAM: An efficient (k, l) graph anonymization method	EXPERT SYSTEMS WITH APPLICATIONS										Data privacy; Graph anonymity; Data publishing; (k,l) Anonymization; Social networks	DISCLOSURE RISK; ANONYMITY; UTILITY	There are plenty of applications that use graphs for representing the association between different entities. Many research communities are interested in publishing such graphs to study the knowledge contained in them while the privacy of involved individuals is a challenging issue. A successful computational privacy model to address the problem of data privacy in microdata publishing is k-anonymity. A relaxed version of it for graphs is called (k, l)-anonymity: identifying at most l neighbors of a vertex v in the graph, an attacker cannot limit v in a group of less than k vertices. This paper introduces the GRAM: an efficient (k, l) GRaph Anonymization Method based on edge addition. At first, the GRAM adds enough edges to the graph to meet the privacy requirement. Then, the algorithm removes some redundant added edges to minimize changes in the original graph. The necessary and sufficient conditions for removing an added edge while maintaining the privacy requirement are proposed in the paper. This makes it possible to realize the model efficiently. An extensive set of experiments shows that the GRAM usually achieves a good trade-off between data utility and privacy, while it is more efficient than similar techniques. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113454	10.1016/j.eswa.2020.113454													
J								H2-SLAN: A hyper-heuristic based on stochastic learning automata network for obtaining, storing, and retrieving heuristic knowledge	EXPERT SYSTEMS WITH APPLICATIONS										Hyper-heuristics; Meta-heuristics; On-line learning; Operations research	SELECTION; OPTIMIZATION	Over the years, the meta-heuristics have been adapted and based on metaphors, whose proposals show effective solutions. Nevertheless, these abstractions are proving to be simple camouflage processes. This study presents a hyper-heuristic based on stochastic automata networks with learning, controlling a set of meta-heuristics. In this sense, this work investigates the moving through the search space performed by heuristic mechanisms, free of abstractions used by meta-heuristic. Or, in the conceptual terms proposed, we built up a hyper-heuristic model based on stochastic automata networks with learning, for selection and parameterization of low-level heuristics. The approach works on eight known meta-heuristic, exploiting features, and strengths of each algorithm. This identification allied to the theory of stochastic automata networks with learning guided the construction of their representations. These representations are consolidated in a meta-space, a part of the architecture of the hyper-heuristic proposed in work, named H2-SLAN Model. The results illustrate the effectiveness of our hyper-heuristic approach, regardless of heuristic composition, when compared to each meta-heuristic. Besides, hyper-heuristic performs better than individual meta-heuristics, thus significantly increasing optimization opportunities. As a result of this work, we got a system capable of selecting and parameterize low-level heuristics, with the ability to learn the heuristic movements employed by the model in the search space. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113426	10.1016/j.eswa.2020.113426													
J								An integrated early warning system for stock market turbulence	EXPERT SYSTEMS WITH APPLICATIONS										Early warning system; LSTM; SWARCH; Two-peak method; Dynamic prediction	FUZZY INFERENCE SYSTEM; FINANCIAL CRISES; BANKING CRISES; FORECASTING-MODEL; PREDICTING STOCK; CURRENCY CRISES; PRICE INDEX; VOLATILITY; DEEP; MACHINE	This study constructs an integrated early warning system (EWS) that identifies and predicts stock market turbulence. Based on switching ARCH (SWARCH) filtering probabilities of the high volatility regime, the proposed EWS first classifies stock market crises according to an indicator function with thresholds dynamically selected by the two-peak method. An hybrid algorithm is then developed in the framework of a long short-term memory (LSTM) network to make daily predictions that alert turmoils. In the empirical evaluation based on ten-year Chinese stock data, the proposed EWS yields satisfying results with test-set accuracy of 96.4% and an average of 2.8 days forewarned period. The model's stability and practical value in the real-time decision-making are also proven by the cross-validation, back-testing and reality check. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113463	10.1016/j.eswa.2020.113463													
J								Automatic segmentation model combining U-Net and level set method for medical images	EXPERT SYSTEMS WITH APPLICATIONS										Image segmentation; Level set formulation; Constrained term; U-Net; Split Bregman method	SCALABLE FITTING ENERGY; SPLIT BREGMAN METHOD; ACTIVE CONTOUR; MINIMIZATION	We introduce a new method that combines a constrained term and level set method for the automated segmentation of medical image. There are two types of constrained terms, fully automatic and semiautomatic. It is fully automatic to use the U-Net's segmentation result as a constrained term, and the manual segmentation result as a constrained term is semi-automatic. The level set method does not require a large training set and is theoretically very explanatory, but is usually sensitive to the initial contour. The U-Net can segment more complex medical images, but requires a large number of manually labeled images and usually needs to be normalized to produce a good generalization. Therefore, the combination of these methods combines the advantages of both methods, resulting in a method that requires a small training set and produces accurate segmentation results. We test our method on the melanoma and left ventricle images. Among them, when segmenting melanoma images, our semi-automatic segmentation and full-automatic segmentation results are better than the U-Net and RSF segmentation results alone. When segmenting the left ventricle images, our semi-automatic segmentation result is better than the RSF segmentation result. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				SEP 1	2020	153								113419	10.1016/j.eswa.2020.113419													
J								Protein function prediction is improved by creating synthetic feature samples with generative adversarial networks	NATURE MACHINE INTELLIGENCE											GAN	Protein function prediction is a challenging but important task in bioinformatics. Many prediction methods have been developed, but are still limited by the bottleneck on training sample quantity. Therefore, it is valuable to develop a data augmentation method that can generate high-quality synthetic samples to further improve the accuracy of prediction methods. In this work, we propose a novel generative adversarial networks-based method, FFPred-GAN, to accurately learn the high-dimensional distributions of protein sequence-based biophysical features and also generate high-quality synthetic protein feature samples. The experimental results suggest that the synthetic protein feature samples are successful in improving the prediction accuracy for all three domains of Gene Ontology through augmentation of the original training protein feature samples. Training machine learning models to predict the function of proteins is limited by the availability of only a small amount of labelled training data. Training can be improved by employing generative adversarial networks to generate additional synthetic protein samples.																		2522-5839				SEP	2020	2	9					540	+		10.1038/s42256-020-0222-1		AUG 2020											
J								Deep Learning Technique Based Surveillance Video Analysis for the Store	APPLIED ARTIFICIAL INTELLIGENCE											ACTIVITY RECOGNITION; CROWD ANALYSIS; TRACKING; MULTIPLE; PEOPLE; MODEL	AI technology has developed so fast, and it has been applied to the commercial area. In order to predict the customer preference and adjust the placement of product or advertisement, etc., the intelligent surveillance video analysis technique has been proposed to gather the sufficient customer information and realize crowd counting and density map drawing. In this paper, a series of deep learning techniques are adopted to realize surveillance video analysis. This work covers different subproblems such as object detection, tracking and human identification. A skeleton recognition algorithm is adopted instead of object detection algorithm to overcome the severe occlusion problem. A multiple human tracking algorithm combing the human re-identification technology is adopted to realize the human tracking and counting. Finally, the density map and statistics information are obtained which can be used to evaluate and adjust the current business plan. A real store surveillance video is analyzed by the algorithm, and the results show the advantage of the algorithm.																	0883-9514	1087-6545															10.1080/08839514.2020.1784611		AUG 2020											
J								Multitasking scheduling with batch distribution and due date assignment	COMPLEX & INTELLIGENT SYSTEMS										Multitasking; Scheduling; Due date assignment; Batch distribution	INTEGRATED PRODUCTION; ASSIGNABLE COMMON; DELIVERY; TARDINESS; EARLINESS; MINIMIZE; OPERATIONS; ALGORITHMS; INVENTORY; TIME	This study addresses the multitasking scheduling problems with batch distribution and due date assignment (DDA). Compared with classical scheduling problems with due date-related optimization functions, the job due dates are decision variables rather than given parameters. The jobs completed are distributed in batches, and the sizes of all batches are identical, which may be bounded or unbounded. The jobs in every batch are scheduled one by one. Each batch incurs a fixed cost. Under multitasking environment, it allows the machine to put an uncompleted job on hold and turn to another uncompleted job. The goal is to identify the optimal primary job sequence, the optimal job due dates, and the optimal batch production and distribution strategy so that one of the following two optimization functions is minimised: the total cost composed of the earliness penalty, DDA cost, tardiness penalty and batch distribution cost, and the total cost composed of the earliness penalty, weighted number of late jobs, DDA cost and batch distribution cost. We devise efficient exact algorithms for the problems we consider, and perform numerical experiments to check how multitasking affects the scheduling cost or value, the results of which can assist decision-makers to justify the extent to put to use or refrain from multitasking.																	2199-4536	2198-6053															10.1007/s40747-020-00184-x		AUG 2020											
J								Adsorption control of a pipeline robot based on improved PSO algorithm	COMPLEX & INTELLIGENT SYSTEMS										Particle swarm optimization; Pipeline robot; Parameter optimization; PID control; Surface adsorption; H infinity theory	PARTICLE SWARM OPTIMIZATION; TIME-DELAY; FUZZY-SYSTEMS; STABILIZATION; STABILITY	Particle swarm optimization (PSO) is a widely used method that can provide good parameters for the motion controller of mobile robots. In this paper, an improved PSO algorithm that optimize the control PID parameters of a specific robot have been proposed. This paper first presents a brief review of recently proposed PSO methods, and then presents a detailed analysis of the PID optimization algorithm, which uses H infinity theory to reduce the search space and fuses the information entropy to ensure the diversity of particles. Simulations in Matlab show that the algorithm can improve the convergence speed and get a better global optimization ability than the standard PSO algorithm. Experimental results present a sound effects for the control of the negative pressure adsorption motor in the power grid pipeline robot during its adsorption along the circular movements, which verifies the effectiveness of the proposed method.																	2199-4536	2198-6053															10.1007/s40747-020-00190-z		AUG 2020											
J								Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory	INTERNATIONAL JOURNAL OF COMPUTER VISION										Vision-and-language navigation; Long-range navigation; Spatial memory; Dual attention	LANDMARK	The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with 10, 714 routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions-one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our project page..																	0920-5691	1573-1405															10.1007/s11263-020-01374-3		AUG 2020											
J								An improved reversible and secure patient data hiding algorithm for telemedicine applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electronic patient information (EPI); Privacy protection; Homomorphic encryption; Variable size secret message; Reversible data hiding (RDH)	ENCRYPTED IMAGES; DIFFERENCE; SIGNALS	In the telemedicine framework, a standout among the most significant issues is the exchange of electronic patient information (EPI) between patient and a doctor that are remotely connected. A minute change to EPI may result in a wrong diagnosis for the patient. To ensure secure and safe communication for telemedicine applications, an enhanced reversible data hiding method in the encrypted domain has been presented in this paper. All compared reversible data hiding methods have been seen to show predominant outcomes, yet just on natural images not on medical images because underflow problem may arise in medical images due to a large number of pixels have low-intensity values. Thus, an enhanced reversible data hiding method in the encrypted domain has been introduced here that gives a higher embedding rate than all the looked at reversible data hiding methods by embedding k, (k >= 1) binary bits of a secret message at every pixel of a cover image without any occurrence of underflow and overflow problem. The proposed method has not been suffering from underflow and overflow problem so that empowering it to embed and recover information precisely from low-intensity pixels too. This property makes our proposed method truly reasonable for its utilization on medical images. For all test images, the proposed method altogether beat all the compared methods in its ability to embed secret information and precisely recover it with maintaining the visual quality of stego images too.																	1868-5137	1868-5145															10.1007/s12652-020-02449-2		AUG 2020											
J								Friendly ad hoc routing in cloud scenario using CH selection and transmission head to enrich the security mechanism	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ad-hoc networks; Self formation; Cluster head; Transmission head; Traffic table		The approach of innovation has led the expanded enthusiasm for oneself arranging Ad-hoc Networks. Amongst the various constraints that relates to mobile infrastructure-less networking condition with reduced storage utility as the power storage facilities, influence utilization and vitality advancement stays a very significant issue. In our proposed the self formation nodes in the cloud scenario has been channelized properly through the cluster head (CH) and transmission head (TH) selection. Moreover the proposed technique Friendly Ad-hoc Routing in Cloud Scenario (FARICS) is introduced to enrich the security features of Mobile Ad hoc Network (MANET) architecture. The malicious nodes were found out sincerely through various traffic tables to enrich the security of the MANET. The first preliminary approach of the nodes combination will be vary from cluster to cluster. The CH, TH combination from one cluster to another cluster will be varied according to the sense of the node. The cluster may be a cloud form of organisation to represent the mobile ad hoc nodes. The unique identity of each node were monitored and mapped through the traffic table and the connectivity of the nodes will be strengthening. The simulation will be done with SAODV, SZRP and DMR protocols to get a successful result.																	1868-5137	1868-5145															10.1007/s12652-020-02477-y		AUG 2020											
J								A self-organizing recurrent fuzzy neural network based on multivariate time series analysis	NEURAL COMPUTING & APPLICATIONS										Self-organizing recurrent fuzzy neural network; Multivariate time series analysis; Prediction; Wastewater	SYSTEMS; ALGORITHM; CLASSIFICATION; CONTROLLER; MODEL; DESIGN	Fuzzy neural networks (FNNs) have attracted considerable interest for modeling nonlinear dynamic systems in recent years. However, the recurrent design and the self-organizing design of FNNs generally lack adaptability, and their analyses on the change rule of networks in continuous time are insufficient. To solve these problems, a self-organizing recurrent fuzzy neural network based on multivariate time series analysis (SORFNN-MTSA) is proposed in this paper. First, a recurrent mechanism, based on wavelet transform fuzzy Markov chain algorithm, is introduced to obtain adaptive recurrent values and accelerate convergence speed of the network. Second, a self-organization mechanism, based on weighted dynamic time warping algorithm and sensitivity analysis algorithm, is presented to optimize the network structure. Third, the convergence of SORFNN-MTSA is theoretically analyzed to show the efficiency in both fixed structure and self-organizing structure cases. Finally, several benchmark nonlinear systems and a real application of wastewater treatment are used to verify the effectiveness of SORFNN-MTSA. Compared with other existing methods, the proposed SORFNN-MTSA performs better in terms of both high accuracy and compact structure.																	0941-0643	1433-3058															10.1007/s00521-020-05276-w		AUG 2020											
J								Execution examination of chaotic S-box dependent on improved PSO algorithm	NEURAL COMPUTING & APPLICATIONS										Substitution boxes (S-boxes); Chaotic maps; Particle swarm optimization; Nonlinearity; Mono-test frequency; Performance analysis	HIERARCHY; DESIGN; MAPS; SCHEME; OPTIMIZATION; FAMILIES; SYSTEM	Achieving proper nonlinear properties and autocorrelation in the S-box structure is an open challenge in cryptography. Besides, there have been numerous articles on the optimization of S-box, using two types of fitness functions for optimization. This study investigated both types of functions and compares their performance. In addition, this study used ergodic chaotic maps. First, the performance of particle swarm optimization (PSO) was improved using these maps. Then, the new chaotic S-boxes were designed based on the ergodic maps. After that, the improved PSO was used for optimization to obtain the best S-boxes. This optimization was performed once by selecting nonlinearity as a fitness function. At the second optimization, the entropy source was selected as a fitness function for optimization by examining theP-value of the mono-test frequency. Finally, the related results for the introduced chaotic S-boxes were compared to the optimized chaotic S-boxes with two types of fitness functions. The introduced S-boxes were safe due to the use of ergodic maps with high keyspace length. Furthermore, the simulation performance was analyzed and compared with other relevant approaches.																	0941-0643	1433-3058															10.1007/s00521-020-05304-9		AUG 2020											
J								State-transition simulated annealing algorithm for constrained and unconstrained multi-objective optimization problems	APPLIED INTELLIGENCE										Multi-objective optimization; Pareto dominance; State transition; Simulated annealing algorithm	STRENGTH	In this article, a novel multi-objective optimization algorithm based on a state-transition simulated annealing algorithm (MOSTASA) is proposed, in which four state-transition operators for generating candidate solutions and the Pareto optimal solution is obtained by combining it with the concept of Pareto dominance and then storing it in a Pareto archive. To ensure the uniform distribution of the Pareto optimal solution, we define a crowded comparison operator to update the Pareto archive. Simulation experiments were conducted on several standard constrained and unconstrained multi-objective problems, in which convergence and spacing metrics were used to assess the performance of the MOSTASA. The test results manifest that the MOSTASA can converge to the true Pareto-optimal front, and the solution distribution is uniform. Compared to the performance of other multi-objective optimization algorithms, the proposed algorithm is more efficient and reliable.																	0924-669X	1573-7497															10.1007/s10489-020-01836-8		AUG 2020											
J								Hybrid approach based on cuckoo optimization algorithm and genetic algorithm for task scheduling	EVOLUTIONARY INTELLIGENCE										Evolutionary algorithms; Task scheduling; Genetic algorithm; Cuckoo optimization algorithm; Meta-heuristic algorithms; Spiral search	MULTIPROCESSOR SYSTEM; DUPLICATION; PRECEDENCE; OPERATORS; SELECTION; MAKESPAN; PRIORITY; STRATEGY; GRAPHS	One of the most important issues in designing efficient scheduling algorithms in heterogeneous distribution systems is the reduction of execution time. In the proposed algorithm, the modified operators of the cuckoo optimization algorithm and the genetic algorithm are used to achieve a relatively optimal solution with fewer repetitions of the genetic algorithm and less execution time than the cuckoo optimization algorithm. The most important innovation in the proposed algorithm is the introduction of a new operator called spiral search, which increases the variety among the samples produced in each generation. The main idea of this operator is to replace linear search with the spiral search, which allows local search between similar schedules and accelerates the achievement of a relatively optimal answer. Also the multi objective function in the proposed algorithm is used to minimize makespan and maximize parallelization. The results obtained from the proposed algorithm on a large number of standard graphs with a various range of attributes show that it is superior to the other task scheduling algorithms.																	1864-5909	1864-5917															10.1007/s12065-020-00471-z		AUG 2020											
J								Searching Multiple Approximate Solutions in Configuration Space to Guide Sampling-Based Motion Planning	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Motion planning; Rapidly-exploring random tree	EXPLORATION; PATH	High-dimensional configuration space is usually searched using sampling-based motion planning methods. The well-known issue of sampling-based planners is the narrow passage problem caused by small regions of the configuration space that are difficult to cover by random samples. Practically, the presence of narrow passages decreases the probability of finding a solution, and to cope with it, the number of random samples has to be significantly increased, which also increases the planning time. By dilating the free space, e.g., by scaling-down or thinning the robot (or obstacles), narrow passages become wider, which allows us to compute an approximate solution. Then, the configuration space can be sampled densely around the approximate solution to find the solution of the original problem. However, this process may fail if the final solution is too far from the approximate one. In this paper, we propose a method to find multiple approximate solutions in the configuration space to increase the chance of finding the final solution. The approximate solutions are computed by repeated search of the configuration space while avoiding, if possible, the already discovered solutions. This enables us to search for distinct solutions leading through different parts of the configuration space. The number of approximate solutions is automatically determined based on their similarity. All approximate solutions are then used to guide the sampling of the configuration space. The performance of the proposed approach is verified in scenarios with multiple narrow passages and the benefits of the method are demonstrated by comparing the results with the state-of-the-art planners.																	0921-0296	1573-0409															10.1007/s10846-020-01247-4		AUG 2020											
J								Head and camera rotation invariant eye tracking algorithm based on segmented group method of data handling	MACHINE VISION AND APPLICATIONS										Eye tracking; Haar-like features; Group method of data handling; Segmented regression	NEURAL-NETWORK ARCHITECTURE; GAZE; PREDICTION; IMAGE; RECOGNITION; PERCEPTION; ATTENTION; SYSTEM; GMDH	Eye-gaze tracking through camera is commonly used in a number of areas, such as computer user interface systems, sports science, psychology, and biometrics. The robustness of the head and camera rotation tracking algorithm has been a critical problem in recent years. In this paper, Haar-like features and a modified version of the group method of data handling, as well as segmented regression, are used together to find the base points of the eyes in a facial image. Then, a geometric transformation is applied to detect precise eye-gaze direction. The proposed algorithm is tested on GI4E and Columbia Gaze datasets and compared to other algorithms. The results show adequate accuracy, especially when the head/camera is rotated.																	0932-8092	1432-1769				AUG 31	2020	31	7-8							59	10.1007/s00138-020-01112-2													
J								Predicting users' behavior using mouse movement information: an information foraging theory perspective	NEURAL COMPUTING & APPLICATIONS										Users' behavior analysis; Users' behavior prediction; Mouse movements; Information foraging theory	INTERNET; LIFE	The prediction of users' behavior is essential for keeping useful information on the web. Previous studies have used mouse cursor information in web usability evaluation and designing user-oriented search interfaces. However, we know fairly to a small extent pertaining to user behavior, specifically clicking and navigating behavior, for prolonged search session illustrating sophisticated search norms. In this study, we perform extensive analysis on a mouse movement activities dataset to capture every users' movement pattern using the effects of information foraging theory (IFT). The mouse cursor movement information dataset includes the timing and positioning information of mouse cursors collected from several users in different sessions. The tasks vary in two dimensions: (1) to determine the interactive elements (i.e., information episodes) of user interaction with the site; (2) adopt these findings to predict users' behavior by exploiting the LSTM model. Our model is developed to find the main patterns of the user's movement on the site and simulate the behavior of users' mouse movement on any website. We validate our approach on a mouse movement dataset with a rich collection of time and position information of mouse pointers in which searchers and websites are annotated by web foragers and information patches, respectively. Our evaluation shows that the proposed IFT-based effects provide an LSTM model a more accurate interpretative exposition of all the patterns in the movement of the users' mouse cursors across the screen.																	0941-0643	1433-3058															10.1007/s00521-020-05306-7		AUG 2020											
J								An improved vertical fragmentation, allocation and replication for enhancinge-learningin distributed database environment	COMPUTATIONAL INTELLIGENCE										data replication; dynamic programming; e-learning; synergetic learning		E-learning is the indispensable technique to educate huge number of people and students in short period of time with optimized usage of different kind of required resources. It is employed as a crucial teaching approach by almost all kind of educational institutions all around the world. Since e-learning involves significant amount of resource utilization and cost, it requires some essential methodology to enhance the current system of e-learning more efficient. The mere publication of the educational content in websites is not enough. It is very clear that, without applying suitable strategic models and concepts and establishing appropriate communication channels between contributors of e-learning system, the educational goals cannot be achieved as we desired. Distributed database involves greater contribution in the field of cloud based e-learning process. Basically, data replication is crucial decision of companies as database distribution can be achieved effectively by the method of database replication which generates the same copies of information called replicas. In this article, we analyze the supremacy of synergetic learning and concentrates on data replication's significance in cloud based learning system. Here we propose an excellent mechanism for data replication and enhancing the performance in terms optimized access and update of data by the determination of exact location of data through dynamic programming. The efficiency of proposed mechanism is clearly illustrated by experimental results.																	0824-7935	1467-8640															10.1111/coin.12401		AUG 2020											
J								A new colour image copyright protection approach using evolution-based dual watermarking	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Multiple watermarking; information fusion; copyright protection; optimisation	SVD	Multiple watermarking techniques for images is receiving more attention in recent years for its wide variety of applications in different fields such as piracy of digital data, and copyrights protection. Current approaches rely on adding many watermarks in different bands or channels utilising scaling factor, and embedding locations that are mainly defined by experts. This brought many challenges in achieving equilibrium between security, robustness, and quality. Aiming to fill the gap of determining stationary scaling factor and embedding locations, the work proposed in this paper adapts genetic algorithm to find the optimality of these issues; that will enhance both of watermarking capacity and imperceptibility in colour images. To increase security, the suggested model encodes the watermarks using 2-D Walsh transform (WHT). Furthermore, Singular Value Decomposition (SVD) as a dimension reduction tool is utilised to factorise Wavelet coefficients to determine the most salient embedding locations. Experimental results show that the proposed model is more robust against common image manipulation attacks in terms of PSNR and NCC.																	0952-813X	1362-3079															10.1080/0952813X.2020.1801853		AUG 2020											
J								Ant colony optimization for variable selection in discriminant linear analysis	JOURNAL OF CHEMOMETRICS										ant colony optimization; edible oil and tea; linear discriminant analysis; near-infrared and ultraviolet-visible spectrometry; variable selection	LEAST-SQUARES; CLASSIFICATION; ALGORITHM; SPECTROSCOPY	A new algorithm using ant colony optimization (ACO) for selection of variables in linear discriminant analysis (LDA) is presented. The role of ACO is explored in the context of LDA classification in which spectral variable multicollinearity is a known cause of generalization problems. The proposed ACO-LDA presents a metaheuristic that mimics the ant's cooperative behavior, randomly depositing pheromones at vector elements corresponding to the most relevant variables. Such cooperative ant-like behavior, which is absent in the genetic algorithm, increases the probability of discarding noninformative variables, favoring construction of more parsimonious models than genetic algorithm-linear discriminate analysis (GA-LDA). The classification performance of ACO-LDA is assessed in two case studies: (i) classification of edible vegetable oils (with respect to base oil) via ultraviolet-visible (UV-Vis) spectrometry and (ii) simultaneous classification of tea samples with respect to type and geographic origin via near-infrared (NIR) spectrometry. In the first study, ACO-LDA was tested in a data set involving wide absorption bands in the UV region with low-resolution and strong spectral overlapping. In the second study, its capacity to manage a data matrix with high dimensionality was evaluated. In both studies, ACO-LDA selected a small subset of variables, which led to correct classifications for almost all of the samples, achieving a performance level similar to the well-established partial least squares-discriminant analysis (PLS-DA), and considerably better than GA-LDA. The use of ACO to select LDA classification variables can minimize generalization problems commonly associated with multicollinearity.																	0886-9383	1099-128X														e3292	10.1002/cem.3292		AUG 2020											
J								Multi-objective microservice deployment optimization via a knowledge-driven evolutionary algorithm	COMPLEX & INTELLIGENT SYSTEMS										Multi-objective optimization; NSGA-III; MGR-NSGA-III; Microservice	SERVICE COMPOSITION; OPTIMAL SELECTION	For the deployment and startup of microservice instances in different resource centres, we propose an optimization problem model based on the evolutionary multi-objective theory. The objective functions of the model consider the computation and storage resource utilization rate, load balancing rate, and actual microservice usage rate in resource service centres. The constraints of the model are the completeness of service, total amount of storage resources, and total number of microservices. In this study, a knowledge-driven evolutionary algorithm (named MGR-NSGA-III) is proposed to solve the problem model and seek the optimal deployment and startup strategy of microservice instances in different resource centres. The proposed model and solution have been evaluated via real data experiments. The results show that our approach is better than the traditional microservice instance deployment and startup strategy. The average computation rate, storage idle rate, and actual microservice idle rate were 13.21%, 5.2%, and 16.67% lower than those in NSGA-III, respectively. After 50, 100, and 150 evolutionary generations in serval operations, the population members in NGR-NSGA-III dominated the population members in NSGA-III 6,270, 3,581, and 7,978 times in average, respectively, which means that NGR-NSGA-III can converge to the optimal solution much quicker than NSGA-III.																	2199-4536	2198-6053															10.1007/s40747-020-00180-1		AUG 2020											
J								Consistency-index-driven group decision making under the environment of triangular fuzzy numbers	SOFT COMPUTING										Group decision making (GDM); Triangular fuzzy multiplicative reciprocal matrix (TFMRM); Generalized consistency index; Acceptable consistency; Aggregation operator	ANALYTIC HIERARCHY PROCESS; PREFERENCE RELATIONS; INCONSISTENCY INDEXES; AGGREGATION OPERATORS; GEOMETRIC CONSISTENCY; JUDGMENT; DERIVATION; MATRICES; MODEL; SETS	Group decision making (GDM) under a fuzzy environment is one of the research focuses recently. Triangular fuzzy number can be used as an effective tool to capture the vagueness encountered by decision makers (DMs). In this study, a novel GDM model is proposed when triangular fuzzy multiplicative reciprocal matrices (TFMRMs) are adopted to express the opinions of DMs. A generalized consistency index is constructed to quantify the inconsistency degree of TFMRMs, which reflects the basic idea of fuzzy set theory that everything has some elasticity. The interesting properties of the new consistency index are studied, and acceptable consistency of TFMRMs is discussed. Then, based on the proposed consistency index, an operator is proposed to aggregate the individual TFMRMs. The properties of the collective TFMRM are further investigated. Finally, a new algorithm for solving a GDM problem with TFMRMs is elaborated on. Numerical results are reported to illustrate the advantages and novelty of the proposed consistency-index-driven GDM model.																	1432-7643	1433-7479															10.1007/s00500-020-05278-9		AUG 2020											
J								Assessment of performance of telecom service providers using intuitionistic fuzzy grey relational analysis framework (IF-GRA)	SOFT COMPUTING										Intuitionistic fuzzy sets; Divergence measure; GRA; Telecom service providers (TSPs); VIKOR	DECISION-MAKING; INFORMATION; BENCHMARKING; VIKOR; SETS; EFFICIENCY; MODEL	Over two decades, telecommunication market has been continuing a huge growth in urban, semi-urban and rural regions of India. Investigating and choosing the desirable cellular mobile telephone service providers (TSPs) based on operative constraint can facilitate to obtain the ideal of TSPs selection. Here, the evaluation commonly encompasses different TSP options under various operational factors; thus, the assessment of TSPs can be considered as a rigorous multi-criteria decision-making (MCDM) issue. This paper initiates a structure for the exploration of the TSPs in the Madhya Pradesh region, India. To do this, a framework associated with grey relational analysis (GRA) on intuitionistic fuzzy sets (IFSs) is planned to obtain the performance of various telecom participants. The developed approach is based on the conception of the best and worst solutions. To find the attribute weights, a divergence measure is developed and employed by relative comparisons. Additionally, to show the proficiency and practicality, a selection problem of TSPs options of Madhya Pradesh circle, India is presented within the IFSs context. By employing the developed model, an expert can enlarge tactics to enhance the performance by standard operational factors. Comparison and sensitivity analysis are considered to validate the developed approach in the prioritization of the TSPs options.																	1432-7643	1433-7479															10.1007/s00500-020-05269-w		AUG 2020											
J								Event-triggered adaptive dynamic programming for multi-player zero-sum games with unknown dynamics	SOFT COMPUTING										Adaptive dynamic programming; Event-triggered control; Neural network; Multi-player zero-sum game	SYSTEMS	In this paper, a novel event-triggered optimal control approach is developed to solve zero-sum game problems for continuous-time multi-player nonlinear systems with unknown dynamics. To begin with, a model neural network (NN) is employed to reconstruct the unknown multi-player nonlinear system by measured input and output data. Then, a critic NN is used to solve the event-triggered Hamilton-Jacobi-Isaacs (HJI) equation for multi-player zero-sum game. Meanwhile, the optimal control law and the worst disturbance law are approximated with the help of critic NN only, respectively. Compared with time-triggered method, the developed control law and the disturbance law are updated only when the triggering condition is violated; thus, the computational and communication burden are reduced. The Lyapunov stability analysis shows that the closed-loop system can be guaranteed to be stable. Finally, two simulation examples are provided to validate the effectiveness of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05293-w		AUG 2020											
J								Bayesian mean-parameterized nonnegative binary matrix factorization	DATA MINING AND KNOWLEDGE DISCOVERY										Matrix factorization; Latent variable models; Bayesian inference; Binary data	DISCRETE; ALGORITHMS; MODELS	Binary data matrices can represent many types of data such as social networks, votes, or gene expression. In some cases, the analysis of binary matrices can be tackled with nonnegative matrix factorization (NMF), where the observed data matrix is approximated by the product of two smaller nonnegative matrices. In this context, probabilistic NMF assumes a generative model where the data is usually Bernoulli-distributed. Often, a link function is used to map the factorization to the [0, 1] range, ensuring a valid Bernoulli mean parameter. However, link functions have the potential disadvantage to lead to uninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this problem. We propose a unified framework for Bayesian mean-parameterized nonnegative binary matrix factorization models (NBMF). We analyze three models which correspond to three possible constraints that respect the mean-parameterization without the need for link functions. Furthermore, we derive a novel collapsed Gibbs sampler and a collapsed variational algorithm to infer the posterior distribution of the factors. Next, we extend the proposed models to a nonparametric setting where the number of used latent dimensions is automatically driven by the observed data. We analyze the performance of our NBMF methods in multiple datasets for different tasks such as dictionary learning and prediction of missing data. Experiments show that our methods provide similar or superior results than the state of the art, while automatically detecting the number of relevant components.																	1384-5810	1573-756X				NOV	2020	34	6					1898	1935		10.1007/s10618-020-00712-w		AUG 2020											
J								Arbitrary oriented multilingual text detection and segmentation using level set and Gaussian mixture model	EVOLUTIONARY INTELLIGENCE										Gaussian low pass filter; Single level 2D DWT; Level set method; k-Means algorithm; LOG filter; Gaussian mixture model(GMM)	CHARACTER SEGMENTATION; DOCUMENTS; SYSTEM; PCA	A pioneer conceptual combination of level set method and Gaussian Mixture Model (GMM) is presented in the described multilingual, arbitrary-oriented character segmentation. The method is a serial accomplishment processes of Gaussian low pass filter, single level 2 Dimensional Discrete Wavelet Transform (2D DWT) for better feature extraction and implemented level set method, k-means clustering algorithm with GMM to achieve a veracious character segmentation results by detecting great measure of true text region of an image. The proposed method segments a character chiefly by distinguishing the touching character constituents and also deals discontinuities presence in a character by using Laplacian of Gaussian filter and morphological bridge function in a intellectual way. The exhibited method was explored on Multi-script Robust Reading Competition dataset and on our privately collected graphical and handwritten multilingual, arbitrarily-oriented text images. The suggested method is compared with the well known multilingual and arbitrarily-oriented charter segmentation methods. The described method attains better segmentation outcomes when compared to the familiar functioning methods. Hence, the suggested method is highly suitable to consider as an improved, standard and procedural technique.																	1864-5909	1864-5917															10.1007/s12065-020-00472-y		AUG 2020											
J								Pixel-Wise Crowd Understanding via Synthetic Data	INTERNATIONAL JOURNAL OF COMPUTER VISION										Crowd analysis; Pixel-wise understanding; Crowd counting; Crowd segmentation; Synthetic data generation	ANOMALY DETECTION	Crowd analysis via computer vision techniques is an important topic in the field of video surveillance, which has wide-spread applications including crowd monitoring, public safety, space design and so on. Pixel-wise crowd understanding is the most fundamental task in crowd analysis because of its finer results for video sequences or still images than other analysis tasks. Unfortunately, pixel-level understanding needs a large amount of labeled training data. Annotating them is an expensive work, which causes that current crowd datasets are small. As a result, most algorithms suffer from over-fitting to varying degrees. In this paper, take crowd counting and segmentation as examples from the pixel-wise crowd understanding, we attempt to remedy these problems from two aspects, namely data and methodology. Firstly, we develop a free data collector and labeler to generate synthetic and labeled crowd scenes in a computer game, Grand Theft Auto V. Then we use it to construct a large-scale, diverse synthetic crowd dataset, which is named as "GCC Dataset". Secondly, we propose two simple methods to improve the performance of crowd understanding via exploiting the synthetic data. To be specific, (1) supervised crowd understanding: pre-train a crowd analysis model on the synthetic data, then fine-tune it using the real data and labels, which makes the model perform better on the real world; (2) crowd understanding via domain adaptation: translate the synthetic data to photo-realistic images, then train the model on translated data and labels. As a result, the trained model works well in real crowd scenes.Extensive experiments verify that the supervision algorithm outperforms the state-of-the-art performance on four real datasets: UCF_CC_50, UCF-QNRF, and Shanghai Tech Part A/B Dataset. The above results show the effectiveness, values of synthetic GCC for the pixel-wise crowd understanding. The tools of collecting/labeling data, the proposed synthetic dataset and the source code for counting models are available at.																	0920-5691	1573-1405															10.1007/s11263-020-01365-4		AUG 2020											
J								Privacy-aware PKI model with strong forward security	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										anonymous transactions; blockchain; PKI; ring signature; RSA		With the development of network technology, privacy protection and users anonymity become a new research hotspot. The existing blockchain privacy-aware public key infrastructure (PKI) model can ensure the privacy of users in the authentication process to a certain extent, but there are still problems of the storage and leakage of users' keys. This paper first proposes a strong forward-secure ring signature scheme based on RSA, which ensures the anonymity of the signing users and the forward-backward security of the keys. Then, by introducing the ring signature technology into the privacy-aware PKI model, this paper proposes a privacy-aware PKI model with strong forward security based on block chains, which not only ensures the users' identity privacy, but also solves the problem of the storage and leakage of the users' keys, greatly improving the success rate and security of the users' identity authentication. Finally, this paper applies the proposed PKI model to anonymous transactions, designs a privacy-aware anonymous transaction model with strong forward security, realizing anonymous transactions without relying on trusted third parties, and implementing users' privacy protection.																	0884-8173	1098-111X															10.1002/int.22283		AUG 2020											
J								Impact of Business Intelligence Adoption on performance of banks: a conceptual framework	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Business intelligence; Business Intelligence Adoption; Bank Performance; CRM; Conceptual framework	CRM	With the mounting prominence of inter-disciplinary methodologies in management, it was appropriate to study the impact of business intelligence on the performance of an organization. In the present study, the author attempted to create a conceptual framework to measure the impact of Business Intelligence Adoption on bank performance in order to add value to the existing views on Business Intelligence Adoption (BIA). Further the literature review approach was carried out to realize the definite gap that exists in the area of BIA. Also in lieu of the strong customer base of modern banks, the study has included Customer Relationship Management as a moderating variable of the proposed framework. This would enhance the focus of BIA in relationship with all the included variables which will enable a bank to lay policies based on the identified relationship between the variables of the study. Literature was assessed on all the variables and the research gap was identified paving way for conceptualisation of a model which can be used in future to measure the impact of BIA on Bank Performance in purview of Customer Relationship Management. This study would be an initial preparatory tool to arrive at a model so as to assess and quantify the impact of BIA on performance of banks in future.																	1868-5137	1868-5145															10.1007/s12652-020-02473-2		AUG 2020											
J								An efficient secure k nearest neighbor classification protocol with high-dimensional features	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										fully homomorphic scheme; honest-but-curious model; machine learning; oblivious transfer; privacy-preserving Euclidean distance; secure kNN; secure multi-party computation	MACHINE	k Nearest neighbor (kNN) classification algorithm is a prediction model which is widely used for real-life applications, such as healthcare, finance, computer vision, personalization recommendation and precision marketing. The arrival of data explosion era results in the significant increase of feature dimension, which also makes for the increase of privacy concern over the available samples and unlabeled data in the applications of machine learning. In this paper, we present a secure low communication overhead kNN classification protocol that is able to deal with high-dimensional features given in real numbers. First, to deal with feature values given in real numbers, we develop a specific data conversion algorithm, which is used in the chosen fully homomorphic scheme. This conversion algorithm is generic and applicable to other algorithms that need to handle real numbers using the fully homomorphic scheme. Second, we present a privacy-preserving euclidean distance protocol (PPEDP), which works with the Euclidean distance computation between two points given in real numbers in a high-dimensional space. Then, based on the novelty PPEDP and oblivious transfer, we propose a new classification approach, efficient secure kNN classification protocol, (ESkNN) with low communication overhead, which is appropriate for a sample set with high-dimensional features and real number feature values. Moreover, we implement ESkNN in C++. Experimental results show that ESkNN is several orders of magnitude faster in performance than existing works, and scales up to 18 000 feature dimension in a memory limited environment.																	0884-8173	1098-111X				NOV	2020	35	11					1791	1813		10.1002/int.22272		AUG 2020											
J								Operations of power in autonomous weapon systems: ethical conditions and socio-political prospects	AI & SOCIETY										Artificial intelligence; Autonomous weapon systems; Campaign to stop killer robots; Ethics; International security regimes; Power analysis	HUMAN-RIGHTS; ARMS-CONTROL; RETHINKING; DECISION; ROBOTS; LAW	The purpose of this article is to provide a multi-perspective examination of one of the most important contemporary security issues: weaponized, and especially lethal, artificial intelligence. This technology is increasingly associated with the approaching dramatic change in the nature of warfare. What becomes particularly important and evermore intensely contested is how it becomes embedded with and concurrently impacts two social structures: ethics and law. While there has not been a global regime banning this technology, regulatory attempts at establishing a ban have intensified along with acts of resistance and blocking coalitions. This article aims to reflect on the prospects and limitations, as well as the ethical and legal intensity, of the emerging regulatory framework. To allow for such an investigation, a power-analytical approach to studying international security regimes is utilized.																	0951-5666	1435-5655															10.1007/s00146-020-01048-1		AUG 2020											
J								Understanding the hermeneutics of digital materiality in contemporary architectural modelling: a material engagement perspective	AI & SOCIETY										Postphenomenology; Material engagement theory; Architecture; Design; ICT; Parametric; Material culture	THINKING; DESIGN	This article develops a framework for analysing how digital software and models become mediums for creative imagination in architectural design. To understand the hermeneutics of these relationships, we develop key concepts from Material Engagement Theory (MET) and Postphenomenology (PP). To push these frameworks into the realm of digital design, we develop the concept of Digital Materiality. Digital Materiality describes the way successive layers of mathematics, code, and software come to mediate enactive perception, and the possibilities of creative material engagement actualised in work with software. Just as molecular materials come to transform action with material objects, so digital materiality comes to enable and transform creative practices with computers. Digital architectural design form a new space for ongoing enactive discovery and creativity through manipulation of digital models and their underlying software environments. By shifting relationships within their digital models, architects can direct their attention, intention, and imagination towards widely different aspects of the model. Here, creative imagination becomes a fundamentally situated activity where mind emerges through dynamic interaction between a variety of embodied, material, and cultural domains.																	0951-5666	1435-5655															10.1007/s00146-020-01044-5		AUG 2020											
J								Uncertain insurance risk process with single premium and multiple classes of claims	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Uncertain insurance model; Uncertain premium; Ruin index; Ruin time; Uncertainty theory	RUIN PROBABILITIES; MODEL; TIME	Traditionally an insurance risk process is considered under the framework of probability theory with a prerequisite that the estimated distribution function is close enough to the real frequency. However, due to economic or technological reasons, sometimes data are unavailable or difficult to obtain, such as when we consider a new insurance product or insurance for valuable weapons. Under these situations, reimbursement policies are based on experts' belief degree, which has a much wider range than the real frequency. As a result, we should employ uncertain insurance risk models to better deal with human uncertainty in running an insurance company. Noticing the fact that an insurance company pays for different kinds of risks and the uncertainty of the customer's arrivals and payments, we investigate an uncertain insurance risk process with multiple classes of claims where the premium process follows an uncertain renewal process. Then we derive expressions for the ruin index and the uncertainty distribution of the ruin time. Some numerical examples and a real data example are performed to capture more insights.																	1868-5137	1868-5145															10.1007/s12652-020-02486-x		AUG 2020											
J								Synthesizing Precise and Useful Commutativity Conditions	JOURNAL OF AUTOMATED REASONING										Commutativity; Abstraction refinement; Synthesis		Reasoning about commutativity between data-structure operations is an important problem with many applications. In the sequential setting, commutativity can be used to reason about the correctness of refactoring, compiler transformations, and identify instances of non-determinism. In parallel contexts, commutativity dates back to the database (Weihl in IEEE Trans Comput 37(12):1488-1505, 1988) and compilers (Rinard and Diniz in ACM Trans Program Lang Syst 19(6):942-991, 1997) communities and, more recently, appears in optimistic parallelization (Herlihy and Koskinen in Proceedings of the 13th ACM SIGPLAN symposium on principles and practice of parallel programming, 2008), dynamic concurrency (Tripp et al. in Proceedings of the 33rd ACM SIGPLAN conference on programming language design and implementation, PLDI '12, New York, NY, USA, ACM, pp 145-156, 2012; Dimitrov et al. in Proceedings of the 35th ACM SIGPLAN conference on programming language design and implementation, 2014), scalable systems (Clements et al. in ACM Trans Comput Syst 32(4):10, 2015) and even smart contracts (Dickerson et al. in Proceedings of the ACM symposium on principles of distributed computing, PODC '17, New York, NY, USA, ACM, pp 303-312, 2017). There have been research results on automatic generation of commutativity conditions, yet we are unaware of any fully automated technique to generate conditions that are both sound and effective. We have designed such a technique, driven by an algorithm that iteratively refines a conservative approximation of the commutativity (and non-commutativity) condition for a pair of methods into an increasingly precise version. The algorithm terminates if/when the entire state space has been considered, and can be aborted at any time to obtain a partial yet sound commutativity condition. We have generalized our work to left-/right-movers (Lipton in Commun ACM 8(12):717-721, 1975) and proved relative completeness. We describe aspects of our technique that lead tousefulcommutativity conditions, including how predicates are selected during refinement and heuristics that impact the output shape of the condition. We have implemented our technique in a prototype open-source toolServois. Our algorithm produces quantifier-free queries that are dispatched to a back-end SMT solver. We evaluateServoisfirst by synthesizing commutativity conditions for a range of data structures including Set, HashTable, Accumulator, Counter, and Stack. We then show several applications of our work including reasoning about memories and locks, finding vulnerabilities in Ethereum smart contracts, improving transactional memory performance, distributed applications, code refactoring, verification, and synthesis.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1333	1359		10.1007/s10817-020-09573-w		AUG 2020											
J								Investigation on industrial dataspace for advanced machining workshops: enabling machining operations control with domain knowledge and application case studies	JOURNAL OF INTELLIGENT MANUFACTURING										Industrial dataspace; Machining knowledge; Machining operations control; Knowledge representation; Knowledge graph	CYBER-PHYSICAL SYSTEMS; BIG DATA; DIGITAL TWIN; VIRTUALIZATION; ARCHITECTURE; TECHNOLOGY	The machining processes on the advanced machining workshop floor are becoming more sophisticated with the interdependent intrinsic processes, generation of ever-increasing in-process data and machining domain knowledge. To manage and utilize those above effectively, an industrial dataspace for machining workshop (IDMW) is presented with a three-layer framework. The IDMW architecture isSchema Centralized-Data Distributed, which relies on Process-Workpiece-Centric knowledge schema description and data storage in decentralized data silos. Subsequently, the pre-processing method for the data silos driven by RFID event graphical deduction model is elaborated to associate decentralized data with knowledge schema. Furthermore, through two industrial case studies, it is found that IDMW is effective in managing heterogeneous data, interconnecting the resource entities, handling domain knowledge, and thereby enabling machining operations control on the machining workshop floor particularly.																	0956-5515	1572-8145															10.1007/s10845-020-01646-2		AUG 2020											
J								Online monitoring of resistance spot welding electrode wear state based on dynamic resistance	JOURNAL OF INTELLIGENT MANUFACTURING										Electrode wear state; Online monitoring; Dynamic resistance; Time series	QUALITY; EXPULSION	The electrode state is an important factor affecting the welding quality in the process of resistance spot welding, but there is still lack of an effective method to monitor the electrode wear state. In this paper, a novel online monitoring method of electrode wear state is proposed by exploring the variation pattern of dynamic resistance. The evaluation method of time series similarity is important for dynamic resistance data processing, and two types of evaluation methods including static evaluation method and dynamic evaluation method are proposed in this paper. The static evaluation methods include shape change factor, dynamic resistance decrease ratio and peak time delay, and the dynamic evaluation method refers to the trend change factor. The welding process parameters and the welding material are kept unchanged during the experiment, the newly polished electrode is used to weld 1300 times continuously, and dynamic resistance is collected. According to the results of data processing, the change of the electrode state can be divided into three stages: The electrode state is stable when the welding number is less than 360. When the welding number is in the range of 360-800, the electrode state is in the transition stage, and the electrode state rapidly deteriorates with the increase of the welding number. When the welding number is more than 800, the electrode state is completely deteriorated, and the electrode needs to be dressed. This study may pave the way for online monitoring of electrode wear state.																	0956-5515	1572-8145															10.1007/s10845-020-01650-6		AUG 2020											
J								A New Adaptive RISE Feedforward Approach based on Associative Memory Neural Networks for the Control of PKMs	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Delta parallel robot; RISE control; B-spline neural network; Trajectory tracking; On-line learning	DELTA; DESIGN; PSO	In this paper, a RISE (Robust Integral of the Sign Error) controller with adaptive feedforward compensation terms based on Associative Memory Neural Network (AMNN) type B-Spline is proposed to regulate the positioning of a Delta Parallel Robot (DPR) with three degrees of freedom. Parallel Kinematic Manipulators (PKMs) are highly nonlinear systems, so the design of a suitable control scheme represents a significant challenge given that these kinds of systems are continually dealing with parametric and non-parametric uncertainties and external disturbances. The main contribution of this work is the design of an adaptive feedforward compensation term using B-Spline Neural Networks (BSNNs). They make an on-line approximation of the DPR dynamics and integrates it into the control loop. The BSNNs' functions are bounded according to the extreme values of the desired joint space trajectories that are the BSNNs' inputs, and their weights are on-line adjusted by gradient descend rules. In order to evaluate the effectiveness of the proposed control scheme with respect to the standard RISE controller, numerical simulations for different case studies under different scenarios were performed.																	0921-0296	1573-0409															10.1007/s10846-020-01242-9		AUG 2020											
J								Application of GA-BP neural network algorithm in killing well control system	NEURAL COMPUTING & APPLICATIONS										Killing well; Bottom-hole pressure control; Control model; GA-BP neural network		Killing operation is an effective measure to restore bottom-hole pressure balance after unbalanced bottom-hole pressure shut-in. In the traditional well killing operation, the opening of the hydraulic throttle valve is manually adjusted by the throttle control box, and the manual control has the problems of uncertainty and low control precision, which makes the stability control of well killing operation a difficult problem. This paper presents a feedback control model based on a large number of real-time bottom-hole data, historical data and GA-BP neural network prediction. Through the intelligent control of throttle valve opening in the process of well killing operation, the fast, accurate and stable self-feedback control of bottom-hole pressure prediction and prediction output is realized. The analysis results show that the control model predicted by GA-BP neural network can effectively adjust the throttle opening and realize the stable and effective control of bottom-hole pressure.																	0941-0643	1433-3058															10.1007/s00521-020-05298-4		AUG 2020											
J								Mendelian evolutionary theory optimization algorithm	SOFT COMPUTING										Mendelian evolutionary theory; Rehabilitation; Binary coded optimizer; Pollination; Meta-heuristic optimization; Multi-species; Artificial DNA	DESIGN OPTIMIZATION; GENETIC ALGORITHM; BAT ALGORITHM; STRATEGY; MEMORY	This study presented a new multi-species binary coded algorithm, Mendelian evolutionary theory optimization (METO), inspired by the plant genetics. This framework mainly consists of three concepts: first, the "denaturation" of DNA's of two different species to produce the hybrid "offspring DNA". Second, the Mendelian evolutionary theory of genetic inheritance, which explains how the dominant and recessive traits appear in two successive generations. Third, theEpimutation, through which organism resist for natural mutation. The above concepts are reconfigured in order to design the binary meta-heuristic evolutionary search technique. Based on this framework, four evolutionary operators-(1) Flipper, (2) Pollination, (3) Breeding, and (4) Epimutation-are created in the binary domain. In this paper, METO is compared with well-known evolutionary and swarm optimizers: (1) binary hybrid GA, (2) bio-geography-based optimization, (3) invasive weed optimization, (4) shuffled frog leap algorithm, (5) teaching-learning-based optimization, (6) cuckoo search, (7) bat algorithm, (8) gravitational search algorithm, (9) covariance matrix adaptation evolution strategy, (10) differential evolution, (11) firefly algorithm and (12) social learning PSO. This comparison is evaluated on 30 and 100 variables benchmark test functions, including noisy, rotated, and hybrid composite functions. Kruskal-Wallis statistical rank-based nonparametric H-test is utilized to determine the statistically significant differences between the output distributions of the optimizer, which are the result of the 100 independent runs. The statistical analysis shows that METO is a significantly better algorithm for complex and multi-modal problems with many local extremes.																	1432-7643	1433-7479				OCT	2020	24	19					14345	14390		10.1007/s00500-020-05239-2		AUG 2020											
J								Pedestrian overpass utilization modeling based on mobility friction, safety and security, and connectivity using machine learning techniques	SOFT COMPUTING										Pedestrian overpass; Utilization; User perception; Machine learning	ENVIRONMENT; PREDICTION; PERCEPTIONS; BEHAVIOR; IMPACT; TIMES	The identification, understanding, and treatment of predictors that drive the use of pedestrian overpass or Foot Over Bridge (FOB) are very much essential for city planners and policymakers. There is paucity of studies that uses modern soft computing techniques to understand the factors driving the use of FOBs. The aim of the work presented here was to identify the important predictors that determine usability of FOBs. The study utilized both questionnaire survey (perception in terms of satisfaction/dissatisfaction) and field data collected across fourteen locations in six different Indian cities. The goal was to identify the essential features that drive the usability of FOBs under four different contexts, i.e., mobility friction, safety and security, and vertical connectivity and horizontal connectivity. Three soft computing algorithms such as generalized linear model (GLM), random forest (RF), and gradient boosting machine (GBM) were trained to predict the future usability of pedestrians. The modelling approach involved data collection from 14 FOB locations, preprocessing involving data input in spreadsheets, removing missing values and normalization for model training. The next stage involved splitting the data into training and testing set, followed by model training and hyper-parameter optimization using tenfold cross-validation. Finally, the developed models were evaluated for test dataset for generalization. The study results revealed that GBM algorithm showed highest classification accuracy on test dataset over the other two techniques at various scenarios. GBM helped in identifying the essential parameters that drive the usability of FOBs under the four different contexts. Sensitivity analysis supported the fact that gender and age had significant impact on the choice of pedestrians under different contexts. Further, the respondents' feedbacks regarding existing problems were used to validate the findings. The safety and security, walk environment, frequency of daily use, comfort, location type, length of travel, stairway dimensions and reduced walkable width affected the choice of using the FOBs. Therefore, provision of CCTV cameras and security personnel, removal of obstruction, provision of proper lighting and all-weather shade, and regular maintenance of the facilities will significantly improve the pedestrians' choice to use the FOBs. The identification of important variables not only provides better insight of factors that affects the choice of pedestrians using the elevated facilities but also provides a valuable source of information to researchers, planners and policymakers to construct a better-planned pedestrian friendly infrastructure.																	1432-7643	1433-7479				NOV	2020	24	22					17467	17493		10.1007/s00500-020-05277-w		AUG 2020											
J								New algebraic and geometric constructs arising from Fibonacci numbers In honor of Masami Ito	SOFT COMPUTING										Approximate constructions; Computing on words; Fibonacci numbers; Sturmian words; Mechanical sequences; Limit of words; Isogonal polygons	INFINITIES	Fibonacci numbers are the basis of a new geometric construction that leads to the definition of a family {C-n : n is an element of N} of octagons that come very close to the regular octagon. Such octagons, in some previous articles, have been given the name of Carboncettus octagons for historical reasons. Going further, in this paper we want to introduce and investigate some algebraic constructs that arise from the family {C-n : n is an element of N} and therefore from Fibonacci numbers: From each Carboncettus octagon C-n, it is possible to obtain an infinite (right) word Wn on the binary alphabet {0, 1}, which we will call the nth Carboncettus word. The main theorem shows that all the Carboncettus words thus defined are Sturmian words except in the case n = 5. The fifth Carboncettus word W-5 is in fact the only word of the family to be purely periodic: It has period 17 and periodic factor 000 100 100 010 010 01. Finally, we also define a further word W-infinity named the Carboncettus limit word and, as second main result, we prove that the limit of the sequence of Carboncettus words is W-infinity itself.																	1432-7643	1433-7479															10.1007/s00500-020-05256-1		AUG 2020											
J								Improving deep forest by ensemble pruning based on feature vectorization and quantum walks	SOFT COMPUTING										Deep forest; Ensemble pruning; Feature vectorization; Quantum walks	CLASSIFIER; DIVERSITY; MARGIN	Recently, a deep learning model, the deep forest (DF), was designed as an alternative to deep neural networks. Each cascade layer of the DF contains a set of random forests (RFs) with a large number of decision trees, some of which are of high redundancy and poor performance. To avoid the negative impacts of such decision trees, this paper proposes to optimize RFs in each cascade layer of the DF so as to realize a pruned deep forest (PDF) with higher performance and smaller ensemble size. In this paper, a new ordering-based ensemble pruning method is proposed based on feature vectorization and quantum walks. This method simultaneously considers the accuracy and the diversity of base classifiers, and it provides an integrated evaluation criterion for ordering base classifiers in the ensemble system. The effectiveness of the proposed method is verified by experiments and discussions.																	1432-7643	1433-7479															10.1007/s00500-020-05274-z		AUG 2020											
J								Higher-Order Quantifier Elimination, Counter Simulations and Fault-Tolerant Systems	JOURNAL OF AUTOMATED REASONING										2nd order quantifier elimination; Satisfiability Modulo theories; Verification of parameterized distributed systems; Counter abstractions	MODEL CHECKING; VERIFICATION; AGREEMENT; CONSENSUS	We developquantifier elimination proceduresfor fragments of higher order logic arising from the formalization of distributed systems (especially of fault-tolerant ones). Such procedures can be used in symbolic manipulations like the computation of pre/post images and of projections. We show in particular that our procedures are quite effective in producingcounter abstractionsthat can be model-checked using standard SMT technology. In fact, very often in the current literature verification tasks for distributed systems are accomplished via counter abstractions. Such abstractions can sometimes be justified via simulations and bisimulations. In this work, we supply logical foundations to this practice, by our technique for second order quantifier elimination. Weimplementedour procedure for a simplified (but still expressive) subfragment and we showed that our method is able to successfully handle verification benchmarks from various sources with interesting performances.																	0168-7433	1573-0670															10.1007/s10817-020-09578-5		AUG 2020											
J								On the performance of empirical mode decomposition-based replay spoofing detection in speaker verification systems	PROGRESS IN ARTIFICIAL INTELLIGENCE										Automatic speaker verification; Replay spoofing; Antispoofing; Empirical mode decomposition; Countermeasures	AUDIO WATERMARKING; SOURCE SEPARATION; ATTACK DETECTION; ALGORITHM; SPEECH	Automatic speaker verification (ASV) systems have maximum threat from replay spoofing attacks. High frequency regions of the underlying audio signal exhibit the phenomenon about their presence. It is therefore useful to decompose the underlying audio signal into frequency bands or regions for possible analysis. In this paper, an empirical mode decomposition (EMD)-based replay spoofing detection system is presented. Using EMD, each signal is decomposed into several monotonic intrinsic mode functions (IMFs). The signal is reconstructed and represented using one or more subsets of these IMFs by performing different combinations for spoofing detection. Results on ASVspoof 2017 version 2.0 and AVspoof benchmark replay attack datasets indicate that there is a potential in initial IMFs to carry replay attack patterns, and that is sufficient rather than processing the entire signal. The proposed approach can also serve as a preprocessing technique by employing dimension reduction strategy. Cross-corpus experiments on the systems indicate the limitations of ASV antispoofing systems due to mismatched conditions.																	2192-6352	2192-6360															10.1007/s13748-020-00216-0		AUG 2020											
J								Pearson correlation coefficient-based pheromone refactoring mechanism for multi-colony ant colony optimization	APPLIED INTELLIGENCE										Pearson correlation coefficient; Multi-colony; TSP; Information entropy; Minimum spanning tree	GENETIC ALGORITHM	To solve the problem of falling into local optimum and poor convergence speed in large Traveling Salesman Problem (TSP), this paper proposes a Pearson correlation coefficient-based Pheromone refactoring mechanism for multi-colony Ant Colony Optimization (PPACO). First, the dynamic guidance mechanism is introduced to dynamically adjust the pheromone concentration on the path of the maximum and minimum spanning tree, which can effectively balance the diversity and convergence of the algorithm. Secondly, the frequency of communication between colonies is adjusted adaptively according to a criterion based on the similarity between the minimum spanning tree and the optimal solution. Besides, the pheromone matrix of the colony is reconstructed according to the Pearson correlation coefficient or information entropy to help the algorithm jump out of the local optimum, thus improving the accuracy of the solution. These strategies greatly improve the adaptability of the algorithm and ensure the effectiveness of the interaction. Finally, the experimental results indicate that the proposed algorithm could improve the solution accuracy and accelerate the convergence speed, especially for large-scale TSP instances.																	0924-669X	1573-7497															10.1007/s10489-020-01841-x		AUG 2020											
J								A Brain-Inspired Model of Theory of Mind	FRONTIERS IN NEUROROBOTICS										theory of mind; false-belief task; brain inspired model; self-experience; connection maturation; inhibitory control	SELF-PERSPECTIVE INHIBITION; TEMPORO-PARIETAL JUNCTION; FALSE-BELIEF TASK; NEURAL BASIS; OTHERS; METAANALYSIS; PERCEPTION; UNDERSTAND; COGNITION; AUTISM	Theory of mind (ToM) is the ability to attribute mental states to oneself and others, and to understand that others have beliefs that are different from one's own. Although functional neuroimaging techniques have been widely used to establish the neural correlates implicated in ToM, the specific mechanisms are still not clear. We make our efforts to integrate and adopt existing biological findings of ToM, bridging the gap through computational modeling, to build a brain-inspired computational model for ToM. We propose a Brain-inspired Model of Theory of Mind (Brain-ToM model), and the model is applied to a humanoid robot to challenge the false belief tasks, two classical tasks designed to understand the mechanisms of ToM from Cognitive Psychology. With this model, the robot can learn to understand object permanence and visual access from self-experience, then uses these learned experience to reason about other's belief. We computationally validated that the self-experience, maturation of correlate brain areas (e.g., calculation capability) and their connections (e.g., inhibitory control) are essential for ToM, and they have shown their influences on the performance of the participant robot in false-belief task. The theoretic modeling and experimental validations indicate that the model is biologically plausible, and computationally feasible as a foundation for robot theory of mind.																	1662-5218					AUG 28	2020	14								60	10.3389/fnbot.2020.00060													
J								Towards Photo-Realistic Facial Expression Manipulation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial network; Graphics	DATABASE	We present a method for photo-realistic face manipulation. Given a single RGB face image with an arbitrary expression, our method can synthesize another arbitrary expression of the same person. To achieve this, we first fit a 3D face model and disentangle the face into its texture and shape. We then train separate networks in each of these spaces. In texture space, we use a conditional generative network to change the appearance, and carefully design the input format and loss functions to achieve the best results. In shape space, we use a fully connected network to predict an accurate face shape. When available, the shape branch uses depth data for supervision. Both networks are conditioned on expression coefficients rather than discrete labels, allowing us to generate an unlimited number of expressions. Furthermore, we adopt spatially adaptive denormalization on our texture space representation to improve the quality of the synthesized results. We show the superiority of this disentangling approach through both quantitative and qualitative studies. The proposed method does not require paired data, and is trained using an in-the-wild dataset of videos consisting of talking people. To achieve this, we present a simple yet efficient method to select appropriate key frames from these videos. In a user study, our method is preferred in 83.2% of cases when compared to state-of-the-art alternative approaches.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2744	2761		10.1007/s11263-020-01361-8		AUG 2020											
J								DeepVS2.0: A Saliency-Structured Deep Learning Method for Predicting Dynamic Visual Attention	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep neural networks; Saliency prediction; Convolutional LSTM; Eye-tracking database; Video; Video database	DETECTION MODEL; GAZE	Deep neural networks (DNNs) have exhibited great success in image saliency prediction. However, few works apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method, called DeepVS2.0. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of LEDOV, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) in DeepVS2.0 to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that human attention has a temporal correlation with a smooth saliency transition across video frames. Therefore, a saliency-structured convolutional long short-term memory network (SS-ConvLSTM) is developed in DeepVS2.0 to predict inter-frame saliency, using the extracted features of OM-CNN as the input. Moreover, the center-bias dropout and sparsity-weighted loss are embedded in SS-ConvLSTM, to consider the center-bias and sparsity of human attention maps. Finally, the experimental results show that our DeepVS2.0 method advances the state-of-the-art video saliency prediction.																	0920-5691	1573-1405															10.1007/s11263-020-01371-6		AUG 2020											
J								Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions	INTERNATIONAL JOURNAL OF COMPUTER VISION										Localization; Image retrieval; Side modality learning; Depth from monocular; Global image descriptor	REPRESENTATIONS; RECOGNITION	Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.																	0920-5691	1573-1405															10.1007/s11263-020-01363-6		AUG 2020											
J								Automatic design of dispatching rules for static scheduling conditions	NEURAL COMPUTING & APPLICATIONS										Genetic programming; Dispatching rules; Look-ahead; Rollout heuristic; Iterative dispatching rules; Unrelated machines environment; Static conditions	GENETIC ALGORITHM; INDEPENDENT TASKS; TUTORIAL SURVEY; HEURISTICS; SEARCH; MACHINES; HYBRID; OPTIMIZATION; ENSEMBLES; EVOLUTION	Dispatching rules (DRs) represent heuristic methods designed for solving various scheduling problems. Since it is hard to manually design new DRs, genetic programming is used to design them automatically. Most DRs are designed in a way that they can be applied under dynamic conditions. On the other hand, static problems are usually solved using various metaheuristic methods. However, situations exist in which metaheuristics might not be the best choice for static problems. Such situations can occur when the schedule needs to be constructed quickly so that the system starts executing as soon as possible, or when it is possible that certain changes happen during the execution of the system. For these cases, DRs are more suitable since they execute faster and can adapt to dynamic changes in the system. However, as most research is focused on developing DRs for dynamic conditions, they would perform poorly under static conditions, since they would not use all the information that is available. Therefore, there is a need to enable automatic development of DRs suitable for static and offline conditions. The objective of this paper is to analyse several methods by which automatically generated DRs can be adapted for static and offline scheduling conditions. In addition to look-ahead and iterative DRs which were studied previously, this paper proposes new terminal nodes, as well as the application of the rollout algorithm to adapt DRs for static conditions. The performance and execution time of all methods are compared with the results achieved by automatically generated DRs for dynamic conditions and genetic algorithms. The tested methods obtain a wide range of results and prove to be competitive both in their performance and execution speed with other approaches. As such, they are a viable alternative to metaheuristics since they can be used in situations where metaheuristics could not, but can offer either a better execution time or even competitive results.																	0941-0643	1433-3058															10.1007/s00521-020-05292-w		AUG 2020											
J								Skills prediction based on multi-label resume classification using CNN with model predictions explanation	NEURAL COMPUTING & APPLICATIONS										Skill-gap; Resume; Skills extraction; Multi-label classification; Convolutional neural network; Model explanation		Skills extraction is a critical task when creating job recommender systems. It is also useful for building skills profiles and skills knowledge bases for organizations. The aim of skills extraction is to identify the skills expressed in documents such as resumes or job postings. Several methods have been proposed to tackle this problem. These methods already perform well when it comes to extracting explicitly mentioned skills from resumes. But skills have different levels of abstraction: high-level skills can be determined by low-level ones. Instead of just extracting skill-related terms, we propose a multi-label classification architecture model based on convolutional neural networks to predict high-level skills from resumes even if they are not explicitly mentioned in these resumes. Experiments carried out on a set of anonymous IT resumes collected from the Internet have shown the effectiveness of our method reaching 98.79% of recall and 91.34% of precision. In addition, features (terms) detected by convolutional filters are projected on the input resumes in order to present to the user, the terms which contributed to the model decision.																	0941-0643	1433-3058															10.1007/s00521-020-05302-x		AUG 2020											
J								Multiclass classification of nutrients deficiency of apple using deep neural network	NEURAL COMPUTING & APPLICATIONS										Apple nutrients deficiency; Convolutional neural network; Deep learning; Image recognition; Image classification	COMPUTER VISION; QUALITY EVALUATION; SYSTEM	Agriculture industry is the foundation of Indian economy where quality fruit production plays an important role. Apple or pome fruits are always in demand because of rich nutrients in it. Hence, to analyze and recognize the nutrients deficiency in fruits, a deep neural-based model is being proposed. This model automatically classifies and recognizes the type of deficiency present in apple. In this paper, a database has been created for four major types of nutrients deficiency in apples and used for training and validation of the proposed deep convolutional network. The model is tuned withk-fold cross-validation. The hyper-parameters such as epoch are set at 100 and batch size kept at 5. Finally, the model is tested with the testing data and achieved an average accuracy of 98.24% withk-fold cross-validation set to 15. The model accuracy depends on the hyper-parameters. The process of features optimization reduces the risk of overfitting of the model. Hence, careful selection of hyper-parameters is important for the convergence of cost function to the global minima that results in minimum misclassification.																	0941-0643	1433-3058															10.1007/s00521-020-05310-x		AUG 2020											
J								Seeming autonomy, technology and the uncanny valley	AI & SOCIETY										Machines and robots; Uncanny valley; Heideggerian phenomenology; Autonomy and heteronomy	MIND PERCEPTION	This paper extends Mori's (IEEE Robot Autom Mag 19:98-100, 2012) uncanny valley-hypothesis to include technologies that fail its basic criterion that uncanniness arises when the subject experiences a discrepancy in a machine's human likeness. In so doing, the paper considers Mori's hypothesis about the uncanny valley as an instance of what Heidegger calls the 'challenging revealing' nature of modern technology. It introduces seeming autonomy and heteronomy as phenomenological categories that ground human being-in-the-world including our experience of things and people. It is suggested that this categorical distinction is more foundational than Heidegger's existential structures and phenomenological categories. Having introduced this novel phenomenological distinction, the paper considers the limits of Mori's hypothesis by drawing on an example from science fiction that showcases that uncanniness need not only be caused by machines that resemble human beings. In so doing, it explores how the seeming autonomy-heteronomy distinction clarifies (at least some of) the uncanniness that can arise when humans encounter advanced technology which is irreducible to the anthropocentrism that shapes Mori's original hypothesis.																	0951-5666	1435-5655															10.1007/s00146-020-01040-9		AUG 2020											
J								Machine and person: reconstructing Harry Collins's categories	AI & SOCIETY										Artificial intelligence; Harry Collins; AI vs; human intelligence; Michael Polanyi; Tacit knowing; Meaning construction; Embodiment; Responsibility	TACIT	Are there aspects of human intelligence that artificial intelligence cannot emulate? Harry Collins uses a distinction between tacit aspects of knowing, which cannot be digitized, and explicit aspects, which can be, to formulate an answer to this question. He postulates three purported areas of the tacit and argues that only "collective tacit knowing" cannot be adequately digitized. I argue, first, that Collins's approach rests upon problematic Cartesian assumptions-particularly his claim that animal knowing is strictly deterministic and, thus, radically different from human knowing. I offer evidence that human linguistic intelligence depends upon embodied forms of animal intelligence. Second, I suggest the development of deep-learning algorithms means that Collins's mimesis assumption (that successfully realized explicit instructions to machines are needed to confirm human-artificial intellectual equivalence) is no longer appropriate; equivalent accomplishment of goals is what counts (as he also concedes). However, persons must realize and integrate many goals and also deal with failures; a general-purpose AI capable of integrating all the needs and goals of human existence resists development. Third, I explain how Michael Polanyi's understanding of tacit knowing, quite different than Collins's concept of the tacit, exemplifies features that are missing in contemporary AI. I make use of evolutionary theory, studies of animal intelligence, biological insights, individual construction of meaning, and notions of human responsibility to argue for the existence of three categories that distinguish human from artificial intelligence.																	0951-5666	1435-5655															10.1007/s00146-020-01046-3		AUG 2020											
J								From computerised thing to digital being: mission (Im)possible?	AI & SOCIETY										Artificial intelligence; Electronic personhood; Algorithms; Liability	LIABILITY	Artificial intelligence (AI) is one of the main drivers of what has been described as the "Fourth Industrial Revolution", as well as the most innovative technology developed to date. It is a pervasive transformative innovation, which needs a new approach. In 2017, the European Parliament introduced the notion of the "electronic person", which sparked huge debates in philosophical, legal, technological, and other academic settings. The issues related to AI should be examined from an interdisciplinary perspective. In this paper, we examine this legal innovation-that has been proposed by the European Parliament-from not only legal but also technological points of view. In the first section, we define AI and analyse its main characteristics. We argue that, from a technical perspective, it appears premature and probably inappropriate to introduce AI personhood now. In the second section, justifications for the European Parliament's proposals are explored in contrast with the opposing arguments that have been presented. As the existing mechanisms of liability could be insufficient in scenarios where AI systems cause harm, especially when algorithms of AI learn and evolve on their own, there is a need to depart from traditional liability theories.																	0951-5666	1435-5655															10.1007/s00146-020-01051-6		AUG 2020											
J								On variational cross-examination: a method for postphenomenological multistability	AI & SOCIETY										Multistability; Ihde; Postphenomenology; Philosophy of technology; Homelessness; Design; Hostile design	HERMENEUTICS; TECHNOLOGY; ARTIFACTS; QUESTION; AGENCY	How should we understand postphenomenological methodology? Postphenomenology is a research perspective which builds on phenomenological and pragmatist philosophy to explore human-technology relations, but one with open methodological questions. Here, I offer some thoughts on the epistemological processes that should be (and often implicitly may be) at work in this research. In particular, I am concerned with postphenomenological research on technological "multistability," i.e., a device's ever-present capacity to be used for a variety of purposes, and to always be meaningful in multiple ways. I develop a methodology called "variational cross-examination," which entails the critical contrast of a device's various stabilities. As a set of instructive examples, I draw on my own line of research on the politics of public spaces, and especially the critique of anti-homeless design.																	0951-5666	1435-5655															10.1007/s00146-020-01050-7		AUG 2020											
J								Real-time solving of computationally hard problems using optimal algorithm portfolios	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Algorithm portfolios; NP-optimization; Real-time	CONFIGURATION	Various hard real-time systems have a desired requirement which is impossible to fulfill: to solve a computationally hard optimization problem within a short and fixed amount of timeT, e.g.,T= 0.5 seconds. For such a task, the exact, exponential algorithms, as well as various Polynomial-Time Approximation Schemes, are irrelevant because they can exceedT. What is left in practice is to combine various anytime algorithms in a parallel portfolio. The question is how to build such an optimal portfolio, given a budget ofKcomputing cores. It is certainly not as simple as choosing theKbest performing algorithms, because their results are possibly correlated (e.g., there is no point in choosing two good algorithm for the portfolio if they win on a similar set of instances). We prove that the decision variant of this problem is NP-complete, and furthermore that the optimization problem is approximable. On the practical side, our main contribution is a solution of the optimization problem of choosingKalgorithms out ofn, for a machine withKcomputing cores, and the related problem of detecting the minimum number of required cores to achieve an optimal portfolio, with respect to a given training set of instances. As a benchmark, we took instances of a hard optimization problem that is prevalent in the real-time industry, in which the challenge is to decide on the bestactionwithin timeT. We include the results of numerous experiments that compare the various methods. Hence, a side effect of our tests is that it gives the first systematic empirical evaluation of the relative success of various known stochastic-search algorithms in coping with a hard combinatorial optimization problems under a very short and fixed timeout.																	1012-2443	1573-7470															10.1007/s10472-020-09704-4		AUG 2020											
J								Tightly-coupled ultra-wideband-aided monocular visual SLAM with degenerate anchor configurations	AUTONOMOUS ROBOTS										Simultaneous localization and mapping; Ultra-wideband; Sensor fusion	LOCALIZATION; VERSATILE; ODOMETRY; ROBUST	This paper proposes an enhanced tightly-coupled sensor fusion scheme using a monocular camera and ultra-wideband (UWB) ranging sensors for the task of simultaneous localization and mapping. By leveraging UWB data, the method can achieve metric-scale, drift-reduced odometry and a map consisting of visual landmarks and UWB anchors without knowing the anchor positions. Firstly, the UWB configuration accommodates any degenerate cases with an insufficient number of anchors for 3D triangulation ( N <= 3 and no height data). Secondly, a practical model for UWB measurement is used, ensuring more accurate estimates for all the states. Thirdly, selected prior range measurements including the anchor-world origin and anchor-anchor ranges are utilized to alleviate the requirement of good initial guesses for anchor position. Lastly, a monitoring scheme is introduced to appropriately fix the scale factor to maintain a smooth trajectory as well as the UWB anchor position to fuse camera and UWB measurement in the bundle adjustment. Extensive experiments are carried out to showcase the effectiveness of the proposed system.																	0929-5593	1573-7527				NOV	2020	44	8					1519	1534		10.1007/s10514-020-09944-7		AUG 2020											
J								Intelligent Adaptive PID Control Using Fuzzy Broad Learning System: An Application to Tool-Grinding Servo control Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive PID control; Fuzzy broad learning system (FBLS); Identifier; Predictive control; Process control; Tool-grinding servo control (TGSC) systems	WAVELET NEURAL-NETWORKS	This paper presents an intelligent adaptive proportional-integral-derivative (PID) control method using fuzzy broad learning system (FBLS) and investigates how the method can be applied to control a tool-grinding servo control (TGSC) system. Due to accuracy, quality and geometric errors which are often difficult to capture the dynamics of the controlled plants or systems, fixed-gain PID controllers without good three-term parameters cannot meet the stringent control performance specifications of nonlinear industrial systems and servomechanisms. To accomplish better control, an adaptive PID control strategy based on the FBLS, or abbreviated as FBLS-APPID, is rigorously proposed by integrating an online parameter learning FBLS identifier together with an adaptive predictive PID control law using FBLS, to eliminate tracking error and achieve fast-tracking and disturbance rejection. Numerical simulations on the two existing discrete-time nonlinear time-delay processes are performed to show the merits and superiority of the constructed FBLS-APPID by comparing to three existing adaptive PID methods. Finally, the applicability of the proposed method is well exemplified by conducting comparatively experimental results on a servo control loop of a real TGSC machine with fixed PID gains tuned by the proposed FBLS-APPID method.																	1562-2479	2199-3211				OCT	2020	22	7					2149	2162		10.1007/s40815-020-00913-x		AUG 2020											
J								Robust Model-Free Adaptive Interval Type-2 Fuzzy Sliding Mode Control for PEMFC System Using Disturbance Observer	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										PEMFC system; Model-free control; Adaptive interval type-2 fuzzy; Nonlinear disturbance observer; Nonsingular fast terminal sliding mode control	TIME-DELAY ESTIMATION; PID CONTROL; AIR-FEED	This paper proposes a novel robust model-free adaptive interval type-2 fuzzy sliding mode control (MF-AIT2FSMC) for the proton exchange membrane fuel cell (PEMFC) system using nonlinear disturbance observer. The main control objective is to adjust the oxygen stoichiometry at its reference trajectory under load disturbances and parameter uncertainties, to avert the oxygen starvation problem and to obtain maximum net power. The referred MF-AIT2FSMC control strategy is composed of three parts: first, a nonlinear disturbance observer is proposed to estimate both oxygen and nitrogen partial pressures which are considered as unmeasurable variables. Second, a nonlinear disturbance observer-based intelligent proportional-integral (NDBO-iPI) control is derived, whereas the NDBO is used to estimate the unmodeled system dynamics via the knowledge of input and output signals. Third, adaptive interval type-2 fuzzy nonsingular fast terminal sliding mode control is inserted to NDBO-iPI control to compensate the NDBO estimation error, enhance the control performance and ensure the global controlled system stability. Furthermore, the entire controlled system stability is verified via the Lyapunov approach. Finally, the corresponding numerical simulation on the PEMFC system is obtained to demonstrate the effectiveness and efficiency of the proposed MF-AIT2FSMC technique by comparing with the NDBO-iPI and standard PID controller.																	1562-2479	2199-3211				OCT	2020	22	7					2188	2203		10.1007/s40815-020-00916-8		AUG 2020											
J								A New Nussbaum-Type Function and its Application in the Control of Uncertain Strict-Feedback Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Nussbaum-type function; Adaptive fuzzy control; Unknown control direction; Backstepping	ADAPTIVE FUZZY CONTROL; FAULT-TOLERANT CONTROL; DYNAMIC SURFACE CONTROL; MIMO NONLINEAR-SYSTEMS; HYPERSONIC VEHICLES; CONTROL DIRECTIONS; TRACKING CONTROL; FUNNEL CONTROL; INPUT; CONSENSUS	In this study, an adaptive fuzzy output tracking control scheme is proposed for uncertain strict-feedback systems with unknown control directions. Nussbaum-type functions are designed, with which the effects of multiple unknown control directions can be handled. Compared with the existing Nussbaum-type functions dealing with the same problem, the proposed Nussbaum-type functions have smaller amplitudes that are more conducive to the design of the controller. Fuzzy functions are used to estimate the unknown terms by combining adaptive laws with backstepping procedure. Adaptive fuzzy controller is constructed to guarantee that the output tracking error system is asymptotically stable and all states in the closed-loop system are bounded. Finally, numerical simulation studies are presented to illustrate the effectiveness of the proposed criteria.																	1562-2479	2199-3211				OCT	2020	22	7					2284	2299		10.1007/s40815-020-00909-7		AUG 2020											
J								Binary beta-hill climbing optimizer with S-shape transfer function for feature selection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; beta-hill climbing optimizer; S-shape transfer function; Optimization; Dimensionality reduction	GENETIC ALGORITHM; RECOGNITION; CLASSIFICATION	Feature selection is an essential stage in many data mining and machine learning and applications that find the proper subset of features from a set of irrelevant, redundant, noisy and high dimensional data. This dimensional reduction is a vital task to increase classification accuracy and thus reduce the processing time. An optimization algorithm can be applied to tackle the feature selection problem. In this paper, a beta-hill climbing optimizer is applied to solve the feature selection problem. beta-hill climbing is recently introduced as a local-search based algorithm that can obtain pleasing solutions for different optimization problems. In order to tailor beta-hill climbing for feature selection, it has to be adapted to work in a binary context. The S-shaped transfer function is used to transform the data into the binary representation. A set of 22 de facto benchmark real-world datasets are used to evaluate the proposed algorithm. The effect of the beta-hill climbing parameters on the convergence rate is studied in terms of accuracy, the number of features, fitness values, and computational time. Furthermore, the proposed method is compared against three local search methods and ten metaheuristics methods. The obtained results show that the proposed binary beta-hill climbing optimizer outperforms other comparative local search methods in terms of classification accuracy on 16 out of 22 datasets. Furthermore, it overcomes other comparative metaheuristics approaches in terms of classification accuracy in 7 out of 22 datasets. The obtained results prove the efficiency of the proposed binary beta-hill climbing optimizer.																	1868-5137	1868-5145															10.1007/s12652-020-02484-z		AUG 2020											
J								Two phase cluster validation approach towards measuring cluster quality in unstructured and structured numerical datasets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distance metric; Probability of inter closeness; Probability of inter separation; Representative cluster centroid; Two phase cluster validation; Unsupervised clustering		This paper presents an improved cluster validation scheme called two phase cluster validation (TPCV) and aims to estimate the inter closeness and inter separation among the clusters in the cluster set of unsupervised clustering schemes based on probability measure for validating the cluster quality without prior identification. First phase, the TPCV computes the representative cluster centroid of each individual cluster in the cluster set based on standard mean operation and then it estimates the probability of inter closeness of each cluster with other clusters in the cluster set based on cluster centroid. Next phase, it calculates the probability of separation among the clusters in the cluster set based on cluster centroid by distance measure. Experimental results show that the TPCV scheme is simple and effective to estimate the cluster quality by measuring the probability of closeness and separation between the clusters in the result of unsupervised clustering scheme.																	1868-5137	1868-5145															10.1007/s12652-020-02487-w		AUG 2020											
J								Prediction of fluid interface between dispersed and matrix phases by Lattice Boltzmann-adaptive network-based fuzzy inference system	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Artificial Intelligence; ANFIS; CFD; fluid interface; dispersed phase; numerical simulation	BUBBLE-COLUMN; CFD SIMULATION; NEURAL-NETWORK; REACTORS DEVELOPMENT; GAS PERMEATION; ANFIS; MEMBRANES; MODEL; CO2; COMBINATION	The integration of Lattice Boltzmann (LB) and adaptive-network-based fuzzy inference system was utilised to simulate mesoscale fluid interface in a multiphase fluid system. This method uses data generated by LB and based on the local population and density data. The interface between dispersed and matrix phases on the neural points was simulated. The neural mesh of interface was created by the ANFIS method and locally was compared by the CFD results. The results showed that there is a great agreement between ANFIS and numerical data when a particularly more number of rules are used in the learning step of simulations. Since this new overview of modelling can predict fluid flow based on existing data, less number of rules cause over prediction for the ANFIS method; however, by enhancing the number of rules, ANFIS can solve this over-prediction far from the curvature of the dispersed phase. This algorithm can be a promising tool for simulation of macroscopic parameters in the large-scale multi-phase reactors.																	0952-813X	1362-3079															10.1080/0952813X.2020.1808081		AUG 2020											
J								A novel fuzzy mathematical model for an integrated supply chain planning using multi-objective evolutionary algorithm	SOFT COMPUTING										Supply chain planning; Build-to-order; Multi-objective meta-heuristic algorithm; Fuzzy programming; Fuzzy VIKOR; Taguchi method	ROBUST OPTIMIZATION MODEL; PROGRAMMING APPROACH; GENETIC ALGORITHM; NSGA-II; BUILD; AGGREGATE; DEMAND; UNCERTAINTY; FRAMEWORK; PROCUREMENT	Due to the high competition in today's global market, design of an efficient supply chain is necessary to achieve competitive advantages. Having concerned this issue, scheduling is one of critical concepts in supply chain management that leads to increase efficiency and customer satisfaction as well as decrease costs. Recently, build-to-order supply chain (BTO-SC) system has attracted considerable attention because of its successful implementation in high-tech companies such as Dell, BMW, Compaq, and Gateway. Despite several studies in the context of BTO-SC, there has been revealed several gaps that this study attempts to eliminate them. In this paper, a fuzzy mixed-integer linear programming optimization model is proposed for an integrated supply, production, and distribution supply chain of multi-product, multi-level, multi-plant. Because of uncertainty, imprecision and variability associated with real data, fuzzy approach is utilized for processing such information in modeling the problem. Usually, supply chain problems have conflicting objectives that should be optimized simultaneously. Thus, the proposed model aims to optimize customer satisfaction as well as chain supply costs, simultaneously. Due to high complexity and lack of proper benchmark for this problem, the model is solved by some popular multi-objective meta-heuristic algorithms. Also, Taguchi method is deployed to calibrate as well as control the parameters in four mentioned algorithms. Finally, a new framework is proposed for ranking of these algorithms use of fuzzy VIKOR method. The presented framework is an excellent comparison dashboard, and it is applicable not only for comparing multi-objective genetic algorithms but also for comparing all of multi-objective meta-heuristic algorithms.																	1432-7643	1433-7479															10.1007/s00500-020-05251-6		AUG 2020											
J								The utilization of rough set theory and data reduction based on artificial intelligence in recommendation system	SOFT COMPUTING										Education environmental resources; College music; Environmental education; Rough set; Data reduction	EINSTEIN AGGREGATION OPERATORS	To improve the effectiveness of allocating the education environmental resources in college music, an allocation model for education environmental resource based on rough set and data reduction is proposed. First, an allocation model of risk preference for education environmental resource based on rough set and data reduction classifier is developed. This model divides risk preference into the existing risk preference and non-existent risk preference. Also, it trains and optimizes the data set for the allocation of education environmental resources by rough set and data reduction classifier. Second, as the rough set and data reduction classifier may fail sometimes in predicting the allocation of education environmental resource, the fuzzy clustering based on rough set is optimized, and combined with the data reduction theory to enhance the effectiveness of allocating the education environmental resources. Finally, based on the simulation research on the allocation model of education environmental resources, the proposed algorithm can obtain a more reasonable allocation of education environmental resources, which embodies the effectiveness of the algorithm. The data reduction classifier of a rough set is used to train and optimize the data set of the allocation of education environmental resources, and visual c++ is used to realize the allocation of education environmental resources. The simulation results show that the algorithm can effectively allocate education environmental resources. The model takes into account the characteristics of the incompatibility between the evaluation of historical data and the complex index system in the evaluation decision-making. It can divide the evaluation decision-making table reasonably, and realize the objective calculation of the weights of each evaluation index based on the rough set condition through hierarchical calculation.																	1432-7643	1433-7479															10.1007/s00500-020-05286-9		AUG 2020											
J								Early diagnosis of COVID-19-affected patients based on X-ray and computed tomography images using deep learning algorithm	SOFT COMPUTING										COVID-19; X-ray images; CT scan; CNN; VGG-16; Inception_V2; Decision tree	NEURAL-NETWORKS; FUZZY-SYSTEMS; PNEUMONIA	The novel coronavirus infection (COVID-19) that was first identified in China in December 2019 has spread across the globe rapidly infecting over ten million people. The World Health Organization (WHO) declared it as a pandemic on March 11, 2020. What makes it even more critical is the lack of vaccines available to control the disease, although many pharmaceutical companies and research institutions all over the world are working toward developing effective solutions to battle this life-threatening disease. X-ray and computed tomography (CT) images scanning is one of the most encouraging exploration zones; it can help in finding and providing early diagnosis to diseases and gives both quick and precise outcomes. In this study, convolution neural networks method is used for binary classification pneumonia-based conversion of VGG-19, Inception_V2 and decision tree model on X-ray and CT scan images dataset, which contains 360 images. It can infer that fine-tuned version VGG-19, Inception_V2 and decision tree model show highly satisfactory performance with a rate of increase in training and validation accuracy (91%) other than Inception_V2 (78%) and decision tree (60%) models.																	1432-7643	1433-7479															10.1007/s00500-020-05275-y		AUG 2020											
J								An improved deep forest for alleviating the data imbalance problem	SOFT COMPUTING										Deep forest; AdaBoost; SMOTE; Imbalanced data	CLASSIFIERS; SMOTE	Most deep learning methods have inherent defects and are rarely applied in the classification task of small-sized imbalanced datasets. On the one hand, data imbalance causes the classification results of the model to be biased toward the majority class. On the other hand, limited training data results in over-fitting. Deep forest (DF) is an interesting deep learning model that can perfectly work on small-sized datasets, and its performance is highly competitive with deep neural networks. In the present study, a variant of the DF called the imbalanced deep forest (IMDF) is proposed to effectively improve the classification performance of the minority class. It aims to explore the application of deep learning on small-sized imbalanced datasets. The IMDF is the cascade of multiple layers, where each layer is the ensemble of multiple units. The main idea behind the proposed method is to enable each unit of the IMDF to handle imbalanced data so that the classification results of the entire IMDF are biased toward minority class. Performed experiments demonstrate the effectiveness of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05279-8		AUG 2020											
J								Equilibrium analysis of marketing strategies in supply chain with marketing efforts induced demand considering free riding	SOFT COMPUTING										Marketing efforts; Free riding; Power structure; Supply chain	CARBON EMISSION REDUCTION; SALES EFFORT; PRICING DECISIONS; COORDINATION; COMPETITION; COST; MANUFACTURER; CONTRACTS; RETAILERS; IMPACT	Retailers can make marketing efforts to increase the market demand, but the results from their activities are generally uncertain and influenced by free riding. This paper considers marketing strategies in a two-echelon supply chain under free riding, where a manufacturer sells products through two competitive retailers who have different powers. The dominant retailer will decide whether to make marketing efforts, and the following retailer will choose whether to follow the decision of the dominant retailer. We establish our demand functions relying on the price and marketing efforts, and then build six decentralized game models to examine how marketing strategies and power structures (manufacturer-dominant and retailer-dominant) affect supply chain members' performances. It is found that, for the dominant retailer, he will make marketing efforts if free riding is not severe. As for the following retailer, in retailer-dominant structure, he will also make marketing efforts if the dominant retailer makes that, while his strategy varies with the degree of free riding in manufacturer-dominant structure. We also show that if the dominant retailer wants to make marketing efforts, he will make the same level of marketing efforts regardless of his market base and competitor's decision.																	1432-7643	1433-7479															10.1007/s00500-020-05281-0		AUG 2020											
J								A novel local search method for LSGO with golden ratio and dynamic search step	SOFT COMPUTING										Large-scale global optimization; Local search; Golden ratio; Memetic algorithm; CEC'2013 LSGO benchmark	EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; OPTIMIZATION	Depending on the developing technology, large-scale problems have emerged in many areas such as business, science, and engineering. Therefore, large-scale optimization problems and solution techniques have become an important research field. One of the most effective methods used in this research field is memetic algorithm which is the combination of evolutionary algorithms and local search methods. The local search method is an important part that greatly affects the memetic algorithm's performance. In this paper, a novel local search method which can be used in memetic algorithms is proposed. This local search method is named as golden ratio guided local search with dynamic step size (GRGLS). To evaluate the performance of proposed local search method, two different performance evaluations were performed. In the first evaluation, memetic success history-based adaptive differential evolution with linear population size reduction and semi-parameter adaptation (MLSHADE-SPA) was chosen as the main framework and comparison is made between three local search methods which are GRGLS, multiple trajectory search local search (MTS-LS1) and modified multiple trajectory search. In the second evaluation, the improved MLSHADE-SPA (IMLSHADE-SPA) framework which is a combination of MLSHADE-SPA framework and proposed local search method (GRGLS) was compared with some recently proposed nine algorithms. Both of the experiments were performed using CEC'2013 benchmark set designed for large-scale global optimization. In general terms, the proposed method achieves good results in all functions, but it performs superior on overlapping and non-separable functions.																	1432-7643	1433-7479															10.1007/s00500-020-05284-x		AUG 2020											
J								Hybrid particle swarm optimization with particle elimination for the high school timetabling problem	EVOLUTIONARY INTELLIGENCE										Particle swarm optimization; Hill climbing; Hybridisation; School timetabling		In this paper, a PSO-based algorithm that hybridized Particle Swarm Optimization (PSO) and Hill Climbing (HC) is applied to high school timetabling problem. This hybrid has two features, a novel solution transformation and particle elimination. The proposed methodologies are tested on the XHSTT-2014 dataset (which is relatively new for the school timetabling problem) plus other additional instances. The experimental results show that the proposed algorithm is effective in solving small and medium instances compared to standalone HC and better than the conventional PSO for most instances. In a comparison to the state of the art methods, it achieved the lowest mean of soft constraint violations for 7 instances and the lowest mean of hard constraint violations for 1 instance.																	1864-5909	1864-5917															10.1007/s12065-020-00473-x		AUG 2020											
J								Intelligent setting of process parameters for injection molding based on case-based reasoning of molding features	JOURNAL OF INTELLIGENT MANUFACTURING										Injection molding; Process parameter; Intelligent setting; Molding feature; Pressure profile	ARTIFICIAL NEURAL-NETWORK; MULTIOBJECTIVE OPTIMIZATION; THERMOPLASTIC MATERIALS; PLASTIC MATERIALS; SYSTEM; DESIGN; CLASSIFICATION; FLOW	Process parameters of injection molding are the key factors affecting the final quality and the molding efficiency of products. In the traditional automatic setting of process parameters based on case-based reasoning, only the geometric features of molds are considered, which may not be the representative feature of products and cause the reasoning process to fail. This problem of failure manifests itself in that the molding process parameters inferred by the reasoning system may be very different between molds with similar geometric features or very similar between molds with different geometric features. Therefore, this paper proposes a case-based-reasoning method based on molding features in order to overcome this problem by a method of dimensionality reduction, composed of three stages which (1) obtain the injection pressure profile data through actual injection molding or filling simulation analysis, (2) calculate the similarity of the pressure profiles between target case and each of source cases in case database using the nearest neighbor method, and sort according to the value of similarity, (3) find the case with a maximum of similarity out as the one closest to the target case, and take the process parameters of the most similar case as the solution of the target case according to case modification strategies. This method simplifies the high-dimensional molding features to the pressure profile at the injection location with two-dimensional data features. Experiments show that the new method has a high retrieval accuracy and sensitivity. Moreover, even slight differences in molding can be captured easily.																	0956-5515	1572-8145															10.1007/s10845-020-01658-y		AUG 2020											
J								Control in the presence of manipulators: cooperative and competitive cases	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Computational social choice; Control; Manipulation; Complexity	COMPLEXITY; BRIBERY; ATTACKS; HARD	Control and manipulation are two of the most studied types of attacks on elections. In this paper, we study the complexity of control attacks on elections in which there are manipulators. We study both the case where the "chair" who is seeking to control the election is allied with the manipulators, and the case where the manipulators seek to thwart the chair. In the latter case, we see that the order of play substantially influences the complexity. We prove upper bounds, holding over every election system with a polynomial-time winner problem, for all standard control cases, and some of these bounds are at the second or third level of the polynomial hierarchy, and we provide matching lower bounds to prove these tight. Nonetheless, for important natural systems the complexity can be much lower. We prove that for approval and plurality elections, the complexity of even competitive clashes between a controller and manipulators falls far below those high bounds, even as low as polynomial time. Yet for a Borda-voting case we show that such clashes raise the complexity unless NP = coNP.																	1387-2532	1573-7454				AUG 28	2020	34	2							52	10.1007/s10458-020-09475-6													
J								Partial least squares discrimination applied to a few samples dataset: A case for predicting the presence of pesticide in lettuce	JOURNAL OF CHEMOMETRICS										discrimination; food authentication; mass spectrometry; model reliability; permutation test; pesticides	VARIABLE SELECTION; MODEL VALIDATION; CLASSIFICATION; SPECTROSCOPY	To perform discriminant analysis through partial least squares (PLS-DA), it is important to note that the smaller the set of samples and the higher the variable to sample ratio, the higher the chance of a too optimistic classification model. In this sense, it is necessary to use strategies to check for the possibility that the classification achieved is done by chance. In metabolomics studies, it is not uncommon to work with a reduced number of samples, in which discrimination approaches must be evaluated according to its reliability. Considering this issue, this study aimed to show a case study to deal with few samples using PLS-DA to discriminate lettuce cultivated in the absence and presence of imidacloprid (IMI). The data were acquired by using ultra-high-performance liquid chromatography coupled to a quadrupole-time of flight mass spectrometry, and the model prediction ability was evaluated by permuting the classes. The performance of the PLS-DA model built using all the variables reached 100% correct classification. Nonetheless, the reliability tests (Wilcoxon, sign test, and Randttest) indicated that the model has been build choosing variables by chance. By using the variable importance in projection, it was possible to build a model with reliable specificity and sensitivity equals 1. The study showed the need to check the classification ability in PLS-DA models through strategies such as variable selection and the permutation test in order to allow for the evaluation of the reliability of the results, even in cases in which the classification reaches 100% in the target.																	0886-9383	1099-128X														e3299	10.1002/cem.3299		AUG 2020											
J								Analysis of the EEG Rhythms Based on the Empirical Mode Decomposition During Motor Imagery When Using a Lower-Limb Exoskeleton. A Case Study	FRONTIERS IN NEUROROBOTICS										brain-machine interface; frequency analysis; electroencephalography; empirical mode decomposition; exoskeleton; motor imagery	FEATURE-EXTRACTION; MU-RHYTHM; (DE)SYNCHRONIZATION; EMD	The use of brain-machine interfaces in combination with robotic exoskeletons is usually based on the analysis of the changes in power that some brain rhythms experience during a motion event. However, this variation in power is frequently obtained through frequency filtering and power estimation using the Fourier analysis. This paper explores the decomposition of the brain rhythms based on the Empirical Mode Decomposition, as an alternative for the analysis of electroencephalographic (EEG) signals, due to its adaptive capability to the local oscillations of the data, showcasing it as a viable tool for future BMI algorithms based on motor related events.																	1662-5218					AUG 27	2020	14								48	10.3389/fnbot.2020.00048													
J								An Incremental Learning Framework to Enhance Teaching by Demonstration Based on Multimodal Sensor Fusion	FRONTIERS IN NEUROROBOTICS										incremental learning network; teaching by demonstration; teleoperation; data fusion; robot learning	MULTISENSOR DATA FUSION; HUMAN-ROBOT INTERACTION; MACHINE; SYSTEM	Though a robot can reproduce the demonstration trajectory from a human demonstrator by teleoperation, there is a certain error between the reproduced trajectory and the desired trajectory. To minimize this error, we propose a multimodal incremental learning framework based on a teleoperation strategy that can enable the robot to reproduce the demonstration task accurately. The multimodal demonstration data are collected from two different kinds of sensors in the demonstration phase. Then, the Kalman filter (KF) and dynamic time warping (DTW) algorithms are used to preprocessing the data for the multiple sensor signals. The KF algorithm is mainly used to fuse sensor data of different modalities, and the DTW algorithm is used to align the data in the same timeline. The preprocessed demonstration data are further trained and learned by the incremental learning network and sent to a Baxter robot for reproducing the task demonstrated by the human. Comparative experiments have been performed to verify the effectiveness of the proposed framework.																	1662-5218					AUG 27	2020	14								55	10.3389/fnbot.2020.00055													
J								Ethical and legal challenges of informed consent applying artificial intelligence in medical diagnostic consultations	AI & SOCIETY										Medical diagnostics; Informed consent; Opacity; Trust; Artificial intelligence; Medical ethics; Right to explanation	DECISION-MAKING; HEALTH-CARE; FUTURE	This paper inquiries into the complex issue of informed consent applying artificial intelligence in medical diagnostic consultations. The aim is to expose the main ethical and legal concerns of the New Health phenomenon, powered by intelligent machines. To achieve this objective, the first part of the paper analyzes ethical aspects of the alleged right to explanation, privacy, and informed consent, applying artificial intelligence in medical diagnostic consultations. This analysis is followed by a legal analysis of the limits and requirements for the explainability of artificial intelligence. Followed by this analysis, recommendations for action are given in the concluding remarks of the paper.																	0951-5666	1435-5655															10.1007/s00146-020-01008-9		AUG 2020											
J								Artificial intelligence in cyber physical systems	AI & SOCIETY										Artificial cognition; Industrial internet of things; Cyber physical systems; Industry 4; 0; Artificial intelligence; Anomaly detection	INDUSTRIE 4.0	This article conducts a literature review of current and future challenges in the use of artificial intelligence (AI) in cyber physical systems. The literature review is focused on identifying a conceptual framework for increasing resilience with AI through automation supporting both, a technical and human level. The methodology applied resembled a literature review and taxonomic analysis of complex internet of things (IoT) interconnected and coupled cyber physical systems. There is an increased attention on propositions on models, infrastructures and frameworks of IoT in both academic and technical papers. These reports and publications frequently represent a juxtaposition of other related systems and technologies (e.g. Industrial Internet of Things, Cyber Physical Systems, Industry 4.0 etc.). We review academic and industry papers published between 2010 and 2020. The results determine a new hierarchical cascading conceptual framework for analysing the evolution of AI decision-making in cyber physical systems. We argue that such evolution isinevitable and autonomousbecause of the increased integration of connected devices (IoT) in cyber physical systems. To support this argument, taxonomic methodology is adapted and applied for transparency and justifications of concepts selection decisions through building summary maps that are applied for designing the hierarchical cascading conceptual framework.																	0951-5666	1435-5655															10.1007/s00146-020-01049-0		AUG 2020											
J								End-to-end CNN plus LSTM deep learning approach for bearing fault diagnosis	APPLIED INTELLIGENCE										Intelligent fault diagnosis; Bearing fault; Intelligent controller; CNN plus LSTM; Deep learning; IMS bearing dataset; CWRU bearing dataset	DAMAGE DETECTION	Fault diagnostics and prognostics are important topics both in practice and research. There is an intense pressure on industrial plants to continue reducing unscheduled downtime, performance degradation, and safety hazards, which requires detecting and recovering potential faults in its early stages. Intelligent fault diagnosis is a promising tool due to its ability to rapidly and efficiently processing collected signals and providing accurate diagnosis results. Although many studies have developed machine leaning (M.L) and deep learning (D.L) algorithms for detecting the bearing fault, the results have generally been limited to relatively small train and test datasets and the input data has been manipulated (selective features used) to reach high accuracy. In this work, the raw data, collected from accelerometers (time-domain features) are taken as the input of a novel temporal sequence prediction algorithm to present an end-to-end method for fault detection. We use equivalent temporal sequences as the input of a novel Convolutional Long-Short-Term-Memory Recurrent Neural Network (CRNN) to detect the bearing fault with the highest accuracy in the shortest possible time. The method can reach the highest accuracy in the literature, to the best knowledge of the authors of the present paper, voiding any sort of pre-processing or manipulation of the input data. Effectiveness and feasibility of the fault diagnosis method are validated by applying it to two commonly used benchmark real vibration datasets and comparing the result with the other intelligent fault diagnosis methods.																	0924-669X	1573-7497															10.1007/s10489-020-01859-1		AUG 2020											
J								An integrated method for multi-criteria decision-making based on the best-worst method and Dempster-Shafer evidence theory under double hierarchy hesitant fuzzy linguistic environment	APPLIED INTELLIGENCE										Double hierarchy hesitant fuzzy linguistic term set; Dempster-Shafer evidence theory; Best-worst method; Multi-criteria decision-making; Weight-determination method	TERM SETS; AGGREGATION OPERATORS; REPRESENTATION MODEL; SIMILARITY MEASURES; REASONING APPROACH; FRAMEWORK; PROBABILITIES	Double hierarchy hesitant fuzzy linguistic term set (DHHFLTS) is a newly developed complex linguistic expression model, and has been well applied to multi-criteria decision-making (MCDM) problems. However, the determination of criteria weights and the innovation of decision-making methods are still two issues that worth exploring in this field. At present, conventional weight-determination methods sometimes have the disadvantages of complicated calculation and low consistency of the obtained results. On the other hand, the existing methods for linguistic information sometimes cannot consider the uncertainty of information caused by ignorance. Considering that the best-worst method (BWM) is a weight-determination method, which can not only greatly simplify the calculation process, but also improve the consistency degree of the results. Dempster-Shafer evidence theory (DSET) can better deal with information uncertainty caused by ignorance. Therefore, this paper extends the BWM and DSET to double hierarchy hesitant fuzzy linguistic environment to solve the above two problems respectively, and the DHHFL-BWM-DSET method is proposed. First, the weight of each criterion is derived based on the BWM-based weight-determination method. Then, inspired by DSET, we propose a DSET-based MCDM method which can not only obtain the decision results of a single decision maker, but also integrate the decision information of multiple decision makers to obtain more rational results. Therefore, decision makers can choose the method based on the specific situation. Finally, taking the selection of financial products as an example, it shows that the method proposed in this paper has some breakthroughs and advantages.																	0924-669X	1573-7497															10.1007/s10489-020-01777-2		AUG 2020											
J								On the evaluation and combination of state-of-the-art features in Twitter sentiment analysis	ARTIFICIAL INTELLIGENCE REVIEW										Sentiment analysis; Meta-features; Word embeddings; Ensemble learning; Twitter	ENSEMBLE; CLASSIFIERS	Sentiment analysis of short informal texts, such as tweets, remains a challenging task due to their particular characteristics. Much effort has been made in the literature of Twitter sentiment analysis to achieve an effective and efficient representation of tweets. In this context, distinct types of features have been proposed and employed, from the simplen-gram representation to meta-features to word embeddings. Hence, in this work, using a relevant set of twenty-two datasets of tweets, we present a thorough evaluation of features by means of different supervised learning algorithms. We evaluate not only a rich set of meta-features examined in state-of-the-art studies, but also a significant collection of pre-trained word embedding models. Also, we evaluate and analyze the effect of combining those distinct types of features in order to detect which combination may provide core information in the polarity detection task in Twitter sentiment analysis. For this purpose, we exploit different strategies for combination, such as feature concatenation and ensemble learning techniques, and show that the sentiment detection of tweets benefits from combining different types of features proposed in the literature.																	0269-2821	1573-7462															10.1007/s10462-020-09895-6		AUG 2020											
J								An approach to object-level stiffness regulation of hand-arm systems subject to under-actuation constraints	AUTONOMOUS ROBOTS										Grasping; Dexterous manipulation; Compliance control; Impedance control	ROBOTIC HANDS; GRASP; MANIPULATION; OPTIMIZATION; POSTURE; DESIGN; JOINT	When using a tool with a robotic hand-arm system, the stiffness at the grasped object plays a key role in the interaction with the environment, allowing the successful execution of the task. However, the rapidly increasing use of under-actuated hands in robotic systems due to their robustness and simplicity of control, pose limitations to the achievable object-level stiffness. Indeed, due to the serial coupling of the hand and the arm, the resulting object-level stiffness is determined by the most compliant of both elements. To address this problem, we propose a novel controller that takes into account the limited achievable geometry of the object stiffness ellipsoid given by a hand with under-actuation constraints, and exploits the contribution of the robotic arm in reshaping the final stiffness towards the desired profile. The under-actuation is illustrated by a coordinated stiffening of the hand fingers. The proposed method is experimentally validated by a hand-arm system performing a peg-in-hole task.																	0929-5593	1573-7527				NOV	2020	44	8					1505	1517		10.1007/s10514-020-09942-9		AUG 2020											
J								Scene Text Detection and Recognition: The Deep Learning Era	INTERNATIONAL JOURNAL OF COMPUTER VISION										Scene text; Optical character recognition; Detection; Recognition; Deep learning; Survey	OBJECT DETECTION; NEURAL-NETWORK; IMAGES; LOCALIZATION; EXTRACTION; VIDEO	With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, methodology and performance. This survey is aimed at summarizing and analyzing the major changes and significant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Specifically, we will emphasize the dramatic differences brought by deep learning and remaining grand challenges. We expect that this review paper would serve as a reference book for researchers in this field. Related resources are also collected in our Github repository (https://github.com/Jyouhou/SceneTextPapers).																	0920-5691	1573-1405															10.1007/s11263-020-01369-0		AUG 2020											
J								Exploring the collective human behavior in cascading systems: a comprehensive framework	KNOWLEDGE AND INFORMATION SYSTEMS										Collective human behavior; Information cascades; Generative framework; Point process		The collective behavior describing spontaneously emerging social processes and events is ubiquitous in both physical society and online social media. The knowledge of collective behavior is critical in understanding and predicting social movements, fads, riots, and so on. However, detecting, quantifying, and modeling the collective behavior in cascading systems at large scale are seldom explored. In this paper, we examine a real-world online social media with more than 1.7 million information spreading records. We observe evident collective behavior in information cascading systems and then propose metrics to quantify the collectivity. We find that previous information cascading models cannot capture the collective behavior in the real-world data and thus never utilize it. Furthermore, we propose a comprehensive generative framework with a latent user interest layer to capture the collective behavior. Our framework accurately models the information cascades with respect to dynamics, popularity, structure, and collectivity. By leveraging the knowledge behind collective behavior, our model successfully predicts the popularity and participants of information cascades without temporal features or early stage information. Our framework may serve as a more generalized one in modeling cascading systems, and, together with empirical discovery and applications, advance our understanding of human behavior.																	0219-1377	0219-3116															10.1007/s10115-020-01506-8		AUG 2020											
J								Automatic role identification for research teams with ranking multi-view machines	KNOWLEDGE AND INFORMATION SYSTEMS										Research roles; Teamwork networks; Role identification; Multi-view machines	CENTRALITY; CLASSIFICATION	Research teams have been well recognized as the norm in modern scientific discovery. Rather than a loose collection of researchers, a well-performing research team is composed of a number of researchers, each of them playing a particular role (i.e., principal investigator, sub-investigator or research staff) for a short- or long-term effort. Role analysis for research teams would help gain insights into the dynamics of teams and the behavior of their members. In this paper, we address the problem of research role identification for large research institutes in which similar yet separated teams coexist. In particular, we represent a research team as teamwork networks and generate the feature representation of each member using a number of network metrics. Afterward, we propose RankMVM, short for Ranking Multi-View Machines, to learn the role identification model. Compared with traditional predictive models, RankMVM is advantageous in exploring high-order feature interactions in an efficient way, as well as handling the specific challenges of the research role identification task, including partially ordered learning targets and sparse feature interactions. In the experiments, we assess the performance on a real-world research team dataset. Extensive experimental evaluations verify the advantages of our proposed research role identification approach.																	0219-1377	0219-3116															10.1007/s10115-020-01504-w		AUG 2020											
J								Rethinking Turing's Test and the Philosophical Implications	MINDS AND MACHINES										Turing; Turing test; Response-dependence; Free will; Computational theory of mind; Computationalism	MIND; COMPUTATION; BRAINS; VIEW	In the 70 years since Alan Turing's 'Computing Machinery and Intelligence' appeared inMind, there have been two widely-accepted interpretations of the Turing test: the canonical behaviourist interpretation and the rival inductive or epistemic interpretation. These readings are based on Turing'sMindpaper; few seem aware that Turing described two other versions of the imitation game. I have argued that both readings are inconsistent with Turing's 1948 and 1952 statements about intelligence, and fail to explain the design of his game. I argue instead for aresponse-dependenceinterpretation (Proudfoot 2013). This interpretation has implications for Turing's view of free will: I argue that Turing's writings suggest a new form of free will compatibilism, which I call response-dependence compatibilism (Proudfoot 2017a). The philosophical implications of rethinking Turing's test go yet further. It is assumed by numerous theorists that Turing anticipated the computational theory of mind. On the contrary, I argue, his remarks on intelligence and free will lead to a new objection to computationalism.																	0924-6495	1572-8641															10.1007/s11023-020-09534-7		AUG 2020											
J								Coronavirus herd immunity optimizer (CHIO)	NEURAL COMPUTING & APPLICATIONS										Coronavirus; COVID-19; Herd immunity; Optimization; Nature inspired; Metaheuristic	KRILL HERD; ALGORITHM; DESIGN	In this paper, a new nature-inspired human-based optimization algorithm is proposed which is called coronavirus herd immunity optimizer (CHIO). The inspiration of CHIO is originated from the herd immunity concept as a way to tackle coronavirus pandemic (COVID-19). The speed of spreading coronavirus infection depends on how the infected individuals directly contact with other society members. In order to protect other members of society from the disease, social distancing is suggested by health experts. Herd immunity is a state the population reaches when most of the population is immune which results in the prevention of disease transmission. These concepts are modeled in terms of optimization concepts. CHIO mimics the herd immunity strategy as well as the social distancing concepts. Three types of individual cases are utilized for herd immunity: susceptible, infected, and immuned. This is to determine how the newly generated solution updates its genes with social distancing strategies. CHIO is evaluated using 23 well-known benchmark functions. Initially, the sensitivity of CHIO to its parameters is studied. Thereafter, the comparative evaluation against seven state-of-the-art methods is conducted. The comparative analysis verifies that CHIO is able to yield very competitive results compared to those obtained by other well-established methods. For more validations, three real-world engineering optimization problems extracted from IEEE-CEC 2011 are used. Again, CHIO is proved to be efficient. In conclusion, CHIO is a very powerful optimization algorithm that can be used to tackle many optimization problems across a wide variety of optimization domains.																	0941-0643	1433-3058															10.1007/s00521-020-05296-6		AUG 2020											
J								A combined deep learning method for internet car evaluation	NEURAL COMPUTING & APPLICATIONS										Internet car evaluation; Topic feature extraction; LSTM; Convolutional neural network	DECISION-MAKING; VEHICLE; OPTIMIZATION; NETWORK; MODEL	In recent years, the Internet has become a trend in the development of the global automotive industry. Numerous Internet companies have joined the automobile manufacturing industry. At the same time, people generally search for information about cars on the Internet as an important reference to purchase decisions before buying them. As a high-value commodity, almost all consumers use search engines to find out the price, reputation and other information about their favorite models before they buy. On the other hand, online reviews contain a large amount of information about what consumers are saying about products, and they influence the purchasing decisions of potential consumers. It is observed that current reviews of automobiles can include several dimensions: corporate brand attention, corporate development and user reputation. In order to provide reference for users and car manufacturers, this paper established a systematic model of Internet car evaluation system based on topic feature extraction, the long short-term memory (LSTM) and the deep convolutional generative adversarial networks (DCGAN). Firstly, the model uses feature extraction and LSTM for sentiment analysis of user evaluations; secondly, considering anomalies in the sample processing, which makes it difficult to cover the distribution of the entire review sample, we proposed a way to train without using too many anomalous samples using DCGAN. The results show that this method can achieve an effective systematic evaluation of Internet cars using only a large sample of normal review events. The results can be used as a reference for people to buy a car and for car companies to optimize their products.																	0941-0643	1433-3058															10.1007/s00521-020-05291-x		AUG 2020											
J								Optimal design of nonlinear model predictive controller based on new modified multitracker optimization algorithm	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										multitracker optimization algorithm; nonlinear model predictive control; nonlinear system; robot manipulator	PID CONTROLLER; TRAJECTORY TRACKING; SYSTEMS; MANIPULATOR	The controller design for the robotic manipulator faces different challenges such as the system's nonlinearities and the uncertainties of the parameters. Furthermore, the tracking of different linear and nonlinear trajectories represents a vital role by the manipulator. This paper suggests an optimal design for the nonlinear model predictive control (NLMPC) based on a new improved intelligent technique and it is named modified multitracker optimization algorithm (MMTOA). The proposed modification of the MTOA is carried out based on opposition-based learning (OBL) and quasi OBL approaches. This modification improves the exploration behavior of the MTOA to prevent it from becoming trapped in a local optimum. The proposed method is applied on the robotic manipulator to track different linear and nonlinear trajectories. The NLMPC parameters are tuned by the MMTOA rather than the trial and error method of the designer. The proposed NLMPC based on MMTOA is compared with the original MTOA, genetic algorithm, and cuckoo search algorithm in literature. The superiority and effectiveness of the proposed controller are confirmed to track different linear and nonlinear trajectories. Furthermore, the robustness of the proposed method is emphasized against the uncertainties of the parameters.																	0884-8173	1098-111X				NOV	2020	35	11					1857	1878		10.1002/int.22275		AUG 2020											
J								Light chainconsensus reinforcement machine learning: An effective blockchain model for Internet of Things using for its advancement and challenges	COMPUTATIONAL INTELLIGENCE										blockchain; industrial Internet of Things; light chain; reinforcement learning	SECURE	Recently, blockchain intersected the Internet of Things (IoT) has come up with an integrated opportunity for different applications such as industries, medical diagnosis, and the education sector. Several conflicts have risen during the intersection, where the purpose of addressing the enormous resource utilization of blockchain, efficiency, and security issues of massive IoT has not been tackled in the present scenario. Presently, Ruff-chain, blockchain consortium basis, mobile cloud blockchain (MCBC), probed IoT, and proof of work deployed to overcome the drawback of blockchain intersected IoT demands high resource utilization and power consumption. To address this issue, a light chain consensus reinforcement machine learning (LCC-RML) method has been developed to optimize the blockchain effectively intersected IoT system and it assists in providing a learning methodology from the aspects of resource utilization, data security decentralization, scalability, and latency. In LCC-RML, without affecting the decentralization system, security, and latency, scalability has been improved with the underlying blockchain approach. Here, a lighter model is designed especially for the blockchain intersected IoT platform, which contains optimized learning procedures, reduced block size, lightweight consensus data structure, and related effective block interval to streamline the data processing. The experimental analysis has been evaluated in the learning framework to improve the performance of the blockchain intersected IoT system with a computational speed of 84.89% and resource utilization reduction of 85.88%. Further in the power consumption has been reduced up to 57.55% with the computation cost of 29.55% with the scalability ratio of 86.88%.																	0824-7935	1467-8640															10.1111/coin.12395		AUG 2020											
J								Algorithms for complex interval-valued q-rung orthopair fuzzy sets in decision making based on aggregation operators,AHP,andTOPSIS	EXPERT SYSTEMS										complex interval-valued intuitionistic fuzzy sets; complex interval-valued q-rung orthopair fuzzy sets; interval-valued Pythagorean fuzzy sets; interval-valued q-rung orthopair fuzzy sets; q-rung orthopair fuzzy sets	OPERATORS; INFORMATION; DISTANCES; ENTROPY	The interval-valued q-rung orthopair fuzzy set (IVq-ROFS) and complex fuzzy set (CFS) are two generalizations of the fuzzy set (FS) to cope with uncertain information in real decision making problems. The aim of the present work is to develop the concept of complex interval-valued q-rung orthopair fuzzy set (CIVq-ROFS) as a generalization of interval-valued complex fuzzy set (IVCFS) and q-rung orthopair fuzzy set (q-ROFS), which can better express the time-periodic problems and two-dimensional information in a single set. In this article not only basic properties of CIVq-ROFSs are discussed but also averaging aggregation operator (AAO) and geometric aggregation operator (GAO) with some desirable properties and operations on CIVq-ROFSs are discussed. The proposed operations are the extension of the operations of IVq-ROFS, q-ROFS, interval-valued Pythagorean fuzzy, Pythagorean fuzzy (PF), interval-valued intuitionistic fuzzy, intuitionistic fuzzy, complex q-ROFS, complex PF, and complex intuitionistic fuzzy theories. Further, the Analytic hierarchy process (AHP) and technique for order preference by similarity to ideal solution (TOPSIS) method are also examine based on CIVq-ROFS to explore the reliability and proficiency of the work. Moreover, we discussed the advantages of CIVq-ROFS and showed that the concepts of IVCFS and q-ROFS are the special cases of CIVq-ROFS. Moreover, the flexibility of proposed averaging aggregation operator and geometric aggregation operator in a multi-attribute decision making (MADM) problem are also discussed. Finally, a comparative study of CIVq-ROFSs with pre-existing work is discussed in detail.																	0266-4720	1468-0394														e12609	10.1111/exsy.12609		AUG 2020											
J								Grasping Ability and Motion Synergies in Affordable Tendon-Driven Prosthetic Hands Controlled by Able-Bodied Subjects	FRONTIERS IN NEUROROBOTICS										3D printing; anthropomorphic hand; experimental analysis; functional testing; grasping; prosthetic hand; synergies	ADULT NORMS; DESIGN; TOOL	Affordable 3D-printed tendon-driven prosthetic hands are a rising trend because of their availability and easy customization. Nevertheless, comparative studies about the functionality of this kind of prostheses are lacking. The tradeoff between the number of actuators and the grasping ability of prosthetic hands is a relevant issue in their design. The analysis of synergies among fingers is a common method used to reduce dimensionality without any significant loss of dexterity. Therefore, the purpose of this study is to assess the functionality and motion synergies of different tendon-driven hands using an able-bodied adaptor. The use of this adaptor to control the hands by means of the fingers of healthy subjects makes it possible to take advantage of the human brain control while obtaining the synergies directly from the artificial hand. Four artificial hands (IMMA, Limbitless, Dextrus v2.0, InMoov) were confronted with the Anthropomorphic Hand Assessment Protocol, quantifying functionality and human-like grasping. Three subjects performed the tests by means of a specially designed able-bodied adaptor that allows each tendon to be controlled by a different human finger. The tendon motions were registered, and correlation and principal component analyses were used to obtain the motion synergies. The grasping ability of the analyzed hands ranged between 48 and 57% with respect to that of the human hand, with the IMMA hand obtaining the highest score. The effect of the subject on the grasping ability score was found to be non-significant. For all the hands, the highest tendon-pair synergies were obtained for pairs of long fingers and were greater for adjacent fingers. The principal component analysis showed that, for all the hands, two principal components explained close to or more than 80% of the variance. Several factors, such as the friction coefficient of the hand contact surfaces, limitations on the underactuation, and impairments for a correct thumb opposition need to be improved in this type of prostheses to increase their grasping stability. The principal components obtained in this study provide useful information for the design of transmission or control systems to underactuate these hands.																	1662-5218					AUG 26	2020	14								57	10.3389/fnbot.2020.00057													
J								Forbidden knowledge in machine learning reflections on the limits of research and publication	AI & SOCIETY										Forbidden knowledge; Machine learning; Artificial intelligence; Governance; Dual-use; Publication norms	PERSONALITY; ACCURATE; SCIENCE	Certain research strands can yield "forbidden knowledge". This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance, with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up till now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline deliberations on how to deal with questions concerning the dissemination of such information. It proposes a tentative ethical framework for the machine learning community on how to deal with forbidden knowledge and dual-use applications.																	0951-5666	1435-5655															10.1007/s00146-020-01045-4		AUG 2020											
J								CEEMDAN-IMFx-PCA-CICA: an improved single-channel blind source separation in multimedia environment for motion artifact reduction in ambulatory ECG	COMPLEX & INTELLIGENT SYSTEMS										Empirical mode decomposition; Motion artifact; Single-channel blind source separation; Wearable ECG monitoring system	OBJECT DETECTION; ELECTRODE	Long-term monitoring of ECG via wearable monitoring systems has already been widely adopted to detect and prevent heart diseases. However, one of the main issues faced by wearable ECG monitoring systems is that motion artifacts significantly affect the systems' stability and reliability. Therefore, motion artifact reduction is a very challenging task in filtering and processing physiological signals. Based on the existing algorithms and ECG prior knowledge, in this paper, we propose an algorithm, CEEMDAN-IMFx-PCA-CICA, for motion artifact reduction in ambulatory ECG signals using single-channel blind source separation technique. Our algorithm first utilizes CEEMDAN to decompose the mixed signals into IMFs (intrinsic mode function) containing different source signal features, thereby forming new multi-dimensional signals. Using the correlation between IMFx (IMF component with the most ECG features) and each IMF, and PCA are then applied to reduce the dimension of each IMF. Finally, the blind separation of the source ECG signals is achieved by using CICA with IMFx as the constraint reference component. The results of our experiments indicate that our algorithm outperformed CEEMDAN-CICA, CEEMDAN-PCA-CICA, and improved CEEMDAN-PCA-CICA. Besides, the number of iterations of the CICA is significantly reduced; the separated source signal is better; the obtained result is stable. Furthermore, the separated ECG signal has a higher correlation with the source ECG signal and a lower RRMSE, especially in the case of high noise-to-signal ratios.																	2199-4536	2198-6053															10.1007/s40747-020-00188-7		AUG 2020											
J								Co-eye: a multi-resolution ensemble classifier for symbolically approximated time series	MACHINE LEARNING										Time series classification; Symbolic representation; Ensemble classification; Random Forest	SIMILARITY; SEARCH; FOREST	Time series classification (TSC) is a challenging task that attracted many researchers in the last few years. One main challenge in TSC is the diversity of domains where time series data come from. Thus, there is no "one model that fits all" in TSC. Some algorithms are very accurate in classifying a specific type of time series when the whole series is considered, while some only target the existence/non-existence of specific patterns/shapelets. Yet other techniques focus on the frequency of occurrences of discriminating patterns/features. This paper presents a new classification technique that addresses the inherent diversity problem in TSC using a nature-inspired method. The technique is stimulated by how flies look at the world through "compound eyes" that are made up of thousands of lenses, called ommatidia. Each ommatidium is an eye with its own lens, and thousands of them together create a broad field of vision. The developed technique similarly uses different lenses and representations to look at the time series, and then combines them for broader visibility. These lenses have been created through hyper-parameterisation of symbolic representations (Piecewise Aggregate and Fourier approximations). The algorithm builds a random forest for each lens, then performs soft dynamic voting for classifying new instances using the most confident eyes, i.e., forests. We evaluate the new technique, coined Co-eye, using the recently released extended version of UCR archive, containing more than 100 datasets across a wide range of domains. The results show the benefits of bringing together different perspectives reflecting on the accuracy and robustness of Co-eye in comparison to other state-of-the-art techniques.																	0885-6125	1573-0565															10.1007/s10994-020-05887-3		AUG 2020											
J								Hybrid encryption algorithm (HEA) based on chaotic system	SOFT COMPUTING										Encryption; Block and stream cipher; Chaotic systems; Substitution-permutation network	IMAGE ENCRYPTION; MAP; 2D	In this paper, we present a very simple and efficient hybrid encryption algorithm based on block and stream ciphers using chaotic systems. Due to the specific characteristics of chaotic systems which are described by a set of nonlinear deterministic dynamic equations, chaos-based encryption achieves a very high level of security. The chaotic system used in this algorithm is Chirikov Standard Map, which is chosen in order to further minimize the encryption time. The proposed scheme adopts two main operations one to generate pseudorandom data block that will be used for stream cipher, and the second to create substitution and permutation tables in initial step and perform rounds for confusion and diffusion processes in block cipher. Some cryptographic tests and metrics are applied to measure the degree of security and analyze the performance of the encryption scheme. The evaluation and simulation analysis indicate that our proposal possesses excellent cryptographic properties; it is extremely sensitive to the small change in secret key, resists against common cryptanalytic attacks, has a high speed and easy to implement.																	1432-7643	1433-7479															10.1007/s00500-020-05258-z		AUG 2020											
J								An information-based score function of interval-valued intuitionistic fuzzy sets and its application in multiattribute decision making	SOFT COMPUTING										Score function; Ranking method; Interval-valued intuitionistic fuzzy set; Multiattribute decision making	IMPROVED ACCURACY FUNCTION; RANKING; RELIABILITY	The score functions are often used to rank the interval-valued intuitionistic fuzzy sets (IVIFSs) in multiattribute decision making (MADM). The purpose of this paper is to develop an information-based score function of the IVIFS and apply it to MADM. Considering the information amount, the reliability, the certainty information, and the relative closeness degree, we propose an information-based score function of the IVIFS. Comparing the information-based score function with existing ranking methods, we find that the information-based score function can overcome the drawbacks of the existing ranking methods and can rank the IVIFSs well. Three illustrative examples of MADM with linear programming are examined to demonstrate the applicability and feasibility of the information-based score function. It is shown that the information-based score function is well defined and can be applied to MADM.																	1432-7643	1433-7479															10.1007/s00500-020-05265-0		AUG 2020											
J								Single server multiple vacation queue with discouragement solve by confluent hypergeometric function	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multiple vacations; Balking; Reneging; Confluent hypergeometric function; Generating function; Modified Bessel function	MACHINE REPAIR PROBLEM; RETRIAL QUEUE; TRANSIENT SOLUTION; DISCRETE-TIME; BALKING; SYSTEM; FEEDBACK; PERFORMANCE; RETENTION; ARRIVALS	Waiting line problems with server vacation have envisaged with increasing complexities and their explicit transient solutions are rigorous in computations, at the same time such solutions are valued for studying the dynamical behaviour of queuing systems over a finite period predominantly utilizes within the state-of-art design process for a real time system. Keeping this fact in mind we adopt continued fractions and generating function to derive explicit expressions for transient state probabilities. In this paper, we consider the waiting line problem with a single server which adopts the multi vacations policy. We analyzed the transient part for a single server multi vacations queue with discouragement . It is also obtained the expected value of the state of the system using stationary queue size distribution, which gives a quick glance of a system performance.																	1868-5137	1868-5145															10.1007/s12652-020-02467-0		AUG 2020											
J								Scheduling multiple scientific workflows using containers on IaaS cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Workflow as a service; Scheduling; Resource provisioning; Deadline and cost minimization	AS-A-SERVICE; ALGORITHMS; SIMULATION; DRIVEN	With the adaptable paradigm of cloud computing and obtainable of data accumulated from largely high-powered scientific devices, workflows have turn into an occurring aim to execute considerable scientific advances at an enhanced speed. Occurring Workflow as a Service (WaaS) frameworks provide scientists an effortless, simply accessible and cost-efficient manner of using their applications from anywhere and at anytime in the cloud. They are multitenant platforms and are developed to handle the execution of heterogeneous workflows continuous workload. To fulfill this, they utilize the compute, network and storage services provided by Infrastructure as a Service (IaaS) vendors. Therefore, at any considerable particular moment, a WaaS framework should be proficient of effectively schedule these continuous workload of workflows with various features and quality of service (QoS). Therefore, we propose a strategy of scheduling and resource provisioning planned particularly for WaaS platforms. The algorithm is dynamic and scalable to adjust to improve in the workload and platform. It supports containers to deal the inefficiency of resource utilization and targets to reduce the overall execution cost of infrastructure resources as fulfilling each single workflow deadline constraint. To our information, this approach that explicitly deals VM sharing in the subject of WaaS by devising the utilization of containers in the heuristics of scheduling and resource provisioning. Our experimental results shows its responsiveness to the uncertainties of the environment, its potential to achieve deadlines, and its cost-effectiveness when compared to other recent algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02483-0		AUG 2020											
J								Nanoscale imaging technique for accurate identification of brain tumor contour using NBDS method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Brain tumor MRI; Image segmentation; Image filtering; MR images; Morphological filtering; Boosted non-local means filter; Nanotechnology based detection scheme (NBDS); Novel region based segmentation (NRBS); Deep neural network classification (DNNC)	NANOTECHNOLOGY; ENHANCEMENT	Brain tumor identification is a difficult task in the processing of diagnostic images and a great deal of research is being performed. Normally, the doctor can evaluate their condition through an MRI scan for irregular brain tissue growth. In this research work, the Kaggle brain MRI database image is used. The boosted non-local means filter is used to reduce noise in the image acquired and to improve the quality of the proposed image technique. Identifying the exact location of the tumor is a challenge, and a novel segmentation method is proposed to evaluate the position of the tumor precisely here. Recently, the nanoscale imaging technique increases tumor detection sensitivity, accuracy, and classification into a benign situation. This segmentation activity is carried out using the nanotechnology detection system using our new region-based segmentation approach. The system suggested used for the identification of the brain tumor seed pixel in the range of nanoscale (nm) is the nanotechnology basis detection scheme. Ultimately, the 97.3% deep-neural network classification method helps to identify brain cancer by automatically extracting the function. The computation time was found to be less when compared to the existing method which is 12 s.																	1868-5137	1868-5145															10.1007/s12652-020-02485-y		AUG 2020											
J								An introduction to papers from workshops on the evolution of cultural complexity	ADAPTIVE BEHAVIOR										Cultural complexity; cultural evolution; evolution of technology; information; tipping points	POPULATION-SIZE; DEMOGRAPHY; BEHAVIOR; MODELS	Organized as a satellite to the annual Conference on Complex Systems, the Evolution of Cultural Complexity held its inaugural workshop in Tempe, Arizona, in 2015, a second in Cancun, Mexico, in 2017, and a third in Thessaloniki, Greece, in 2018. The goal of those satellite sessions was to bring together a community of researchers from different fields who had interests in the evolution of cultural complexity. This special issue includes three papers that grew out of those workshops and one paper that was solicited. Each brings a unique perspective on myriad issues that surround how and why culture evolves. One issue that is key to understanding the evolution of cultural complexity-and it is addressed in the papers included here-is how humans learn and transmit information and the rate at which it is learned and transmitted.																	1059-7123	1741-2633				OCT	2020	28	5			SI		317	322	1059712320950483	10.1177/1059712320950483		AUG 2020											
J								A combined multiple action recognition and summarization for surveillance video sequences	APPLIED INTELLIGENCE										Video summarization; Human action recognition; CNN; HOG; TDMap		Human action recognition and video summarization represent challenging tasks for several computer vision applications including video surveillance, criminal investigations, and sports applications. For long videos, it is difficult to search within a video for a specific action and/or person. Usually, human action recognition approaches presented in the literature deal with videos that contain only a single person, and they are able to recognize his action. This paper proposes an effective approach to multiple human action detection, recognition, and summarization. The multiple action detection extracts human bodies' silhouette, then generates a specific sequence for each one of them using motion detection and tracking method. Each of the extracted sequences is then divided into shots that represent homogeneous actions in the sequence using the similarity between each pair frames. Using the histogram of the oriented gradient (HOG) of the Temporal Difference Map (TDMap) of the frames of each shot, we recognize the action by performing a comparison between the generated HOG and the existed HOGs in the training phase which represents all the HOGs of many actions using a set of videos for training. Also, using the TDMap images we recognize the action using a proposed CNN model. Action summarization is performed for each detected person. The efficiency of the proposed approach is shown through the obtained results for mainly multi-action detection and recognition.																	0924-669X	1573-7497															10.1007/s10489-020-01823-z		AUG 2020											
J								Speech big data cluster modeling based on joint neural network and Spark-SVM with evolutionary intelligence	EVOLUTIONARY INTELLIGENCE										Big data; Data clustering; Speech information; Neural network; Spark-SVM		Emerging development of artificial intelligence application scenarios have caused the bursting requirement for the speech analysis tools, the efficient speech data processing model is urgently needed. This paper analyzes the speech big data cluster modeling based on joint neural network and Spark-SVM. As one of the important functions of graph data mining and analysis applications, graph clustering mainly implements classification operations on each node in the graph model based on clusters, and at the same time increases the association of the object entities, we then use this feature to design the model. In practical applications, subject to the size of the training data set, the system recognition rate does not show a steady upward trend with the increase of Gaussian mixture. In the case of limited speech data, the model parameters that can be then reliably estimated are limited. Because the input vector in the neural network structure is mostly abstract data, the BN layer in the hidden layer must be located in the back of the network structure, which makes the hierarchical performance results more profound. Hence, the joint neural network model is designed. The Spark structure is implemented to improve the systematic efficiency. We simulate the model and compare with the state-of-the-art models.																	1864-5909	1864-5917															10.1007/s12065-020-00462-0		AUG 2020											
J								Covering problem on fuzzy graphs and its application in disaster management system	SOFT COMPUTING										Fuzzy graph; Fuzzy paths; Covering set; Strong covering set; Facility location problem	ALGORITHMS; LOCATION	Nowadays the fuzzy graphs gained popularity due to their wide applications in different areas of science, engineering, social sciences, etc.In this paper, we consider covering problem in fuzzy graph. Different types of covering problems have been defined in this paper. Almost all problems are new. For each of these problems separate study is required. Also, the covering set, strong covering set, minimal covering set, etc., are defined and explained with examples. In a graph, each vertex can cover only those vertices that lie within its covering radius along shortest path and no vertex can cover itself. Here, an algorithm is designed to find the different types of covering sets on a fuzzy graph. An imprecise number, instead of a real number, namely interval number and triangular fuzzy number are considered as arc length of a fuzzy graph. To the best of our knowledge no works are available for covering problem on fuzzy graphs/networks. An application of covering set in natural disaster management is discussed to highlight the importance of the covering problem. In this application, an algorithm is designed to find all the facility vertices (or supply vertices) for given vertex and covering radius.																	1432-7643	1433-7479															10.1007/s00500-020-05263-2		AUG 2020											
J								Big high-dimension data cube designs for hybrid memory systems	KNOWLEDGE AND INFORMATION SYSTEMS										Multidimensional database; Multidimensional query; Big Data; Data cube; Holistic measure; High dimension	COMPUTATION	In Big Data cubes with hundreds of dimensions and billions of tuples, the indexing and query operations are a challenge and the reason is the time-space exponential complexity when a full cube is computed. Therefore, solutions based on RAM may not be practical and the solutions based on hybrid memory (RAM and disk) become viable alternatives. In this paper, we propose a hybrid approach, named bCubing, to index and query high-dimension data cubes with high number of tuples in a single machine and using RAM and disk memory systems. We evaluated bCubing in terms of runtime and memory consumption, comparing it with the Frag-Cubing, HIC and H-Frag approaches. bCubing showed to be faster and used less RAM than Frag-Cubing, HIC and H-Frag. bCubing indexed and allowed to query a data cube with 1.2 billion tuples and 60 dimensions, consuming only 84 GB of RAM, which means 35% less memory than HIC. The complex holistic measures mode and median were computed in multidimensional queries, and bCubing was, on average, 50% faster than HIC.																	0219-1377	0219-3116															10.1007/s10115-020-01505-9		AUG 2020											
J								A novel entropy-based weighted attribute selection in enhanced multicriteria decision-making using fuzzy TOPSIS model for hesitant fuzzy rough environment	COMPLEX & INTELLIGENT SYSTEMS										Uncertainity; Hesitant fuzzy sets; Decision-making; Correlation; TOPSIS	SETS; REDUCTION	The existing approaches of multicriteria decision-making (MCDM) process might yield unreliable and questionable results. The notable challenges of MCDM approaches are rank reversal paradox and uncertainty. The prime inspiration for researchers is the MCDM for hesitant fuzzy sets (HFSs). In some scenarios, the decision-makers could not choose one from numerous values while expressing their preferences. HFS which is the extension of fuzzy sets (FS) is found to be helpful in solving such decision-making (DM) problems. The DM process is revolutionized with the commencement of powerful and efficient tools of data representation for expressing vagueness and uncertainty in data sets as FSs (both generalized and hesitant ones). This paper copes with one such novel approach that involves entropy-based attribute weighting, followed by an evaluation of approximate sets in the fuzzy rough framework. Correlation of the input alternatives in respect of evaluation criteria and the output class is evaluated. With the fuzzy technique for ordered preference by similarity to ideal solutions (FTOPSIS), the generated correlation matrix is utilized for calculating the degree of closeness (delta) of the output classes to the input alternatives. This paper made a novel contribution of performance indicator centered on FTOPSIS for the hesitant fuzzy rough domain. The proposed method's efficiency is established through comprehensive and systematic experimentation on datasets utilized by researchers globally. The proposed algorithms prove its ability to handle datasets that involve human-like hesitant thinking in the MCDM system by contrasting with the existing ones.																	2199-4536	2198-6053															10.1007/s40747-020-00187-8		AUG 2020											
J								A game-theoretic approach of mixing different qualities of coins	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										anonymity; Bitcoin; game theory; policy; smart contract		Perpetrators leverage the untraceable feature to conduct illegal behaviors leading security issues with respect to mixing coins. Generally, bad coins are blocked based on a common blacklist. However, the blacklist may not be updated in time, which results in that bad coins escape the blocking. Consequently, perpetrators can still conduct illicit behaviors such as money laundering. In this paper, we apply game theory under imperfect information to study how coins' quality restrain these illicit behaviors under the incomplete scenario. More specifically, we propose a strategy for participants to submit deposits if they hope to mix coins with others even if they are not in blacklist at this time. The deposits will not be refunded when participants are included in the blacklist after mixing. Therefore, no participants have incentives to mix with bad coins. At the last part of this paper, we also simulate the incomes for participants, which indicates that deposits strategy is effective to prevent illicit behaviors.																	0884-8173	1098-111X				DEC	2020	35	12					1899	1911		10.1002/int.22277		AUG 2020											
J								An efficient parameter optimization of software reliability growth model by using chaotic grey wolf optimization algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software reliability growth model; Testing effort functions; Chaotic grey wolf optimization algorithm; Mean square error; Relative error; Coefficient of determination	PREDICTION	Software reliability growth model (SRGM) with modified testing-effort function (TEF) is a function to evaluate and foresee the parameters of the data. Reliability of software is portrayed as the distinct possibility that for a predefined time, a software package will continue to run on an advance domain without frustration. SRGM utilized a few optimization procedure algorithms to advance the parameters by bifurcating them into a few stages however to upgrade the technique by using all of the parameters at the same time, the algorithm utilized is the chaotic grey wolf optimization algorithm (CGWO). CGWO is an advanced heuristic system for portraying the execution by achieving complex parameter optimization and designing application issues. Different parametric reliabilities rely upon the attributes or characteristics of the data. The parameters are predicted using the Pham-Zhang (PZ) model. Tandem computer software dataset DS1 and DS2 are used to compare the predicted parameter of SRGM obtained by Pham-Zhang (PZ) model using testing effort functions (TEFs) based on the evaluation metrics mean square error (MSE), relative error (RE) and coefficient of determination (R-2). To enhance the reliability of SRGM, the parameters of SRGM estimated using TEF and enhanced using chaotic maps to improve search performance. By using the constrained benchmark functions the results of chaotic maps are obtained. Based on the chaotic graph results, the Chebyshev graph shows a good convergence rate of 78%. Overall, 86% of the results revealed an association between the choice variable and fitness criteria for CGWO. In the SRGM using CGWO, the expected result is completely mechanized and does not require any client necessity.																	1868-5137	1868-5145															10.1007/s12652-020-02476-z		AUG 2020											
J								3D building fabrication with geometry and texture coordination via hybrid GAN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										GAN chain; Hybrid GAN; 3D building generation; multi-properties generation		3D building plays the essential role in digital city construction, city augmented reality and smart urban planning & design. Conventional building construction is accomplished by modeling software which requires significant human intervention. In this paper, a method of 3D building fabrication via Hybrid generative adversarial network (GAN) is proposed, in which a loss function with the introduction of cycle consistency loss and perceptual loss is given, a multi-properties GAN chain is built to create the building with complex architectures. Additionally, a mixed GAN network to generate the geometry and texture coordination is put forward. The discussed method can refine rough architectural models for outputting realistic buildings. Experiments show that generated 3D buildings utilizing the presented method are realistic, with geometry and textural consistency, which improves performance by 20% over traditional methods.																	1868-5137	1868-5145															10.1007/s12652-020-02488-9		AUG 2020											
J								Color distribution of three drawn balls from Ellsberg urn	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ellsberg urn; Uncertainty theory; Chance theory; Uncertain urn problem	UNCERTAIN; RISK	Ellsberg urn is a complicated system with uncertainty (the unknown numbers of the colored balls) and randomness (the randomly drawn balls). By supposing that two numbers of colored balls are unknown in an Ellsberg urn, this paper applies uncertainty theory, probability theory and chance theory as rigorous mathematical tools to formulating the color distribution of the drawn balls when three balls are randomly drawn from the urn. Furthermore, an intuitive example is given to illustrate the results obtained by the mathematical method.																	1868-5137	1868-5145															10.1007/s12652-020-02475-0		AUG 2020											
J								A weak clique based multi objective genetic algorithm for overlapping community detection in complex networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Complex networks; Genetic algorithm; Community detection; Overlapping community detection	GRAPH-THEORY; CENTRALITY	Community detection is an important technique to find hidden information, pattern, and relation in a complex network. All the traditional community detection algorithms focus on the separated community but in real-world networks, most of the community is overlapping. Detecting overlapping community thus becomes essential. Although overlapping community detection techniques based on clique, has exposed hopeful performance but suffers from the serious curse of dimensionality due to its high computational complexity, when comes to large-scale networks. To deal with this drawback this paper proposes a weak clique based multi objective evolutionary algorithm for overlapping community detection. In this algorithm, a novel gene pattern is designed using a weak_clique graph. The weak cliques in the original graph are the nodes in the weak clique graph. The weak clique is formulated by selecting two neighbours which are very similar. Any two nodes in the weak clique are differed at most two node distances. The weak cliques comprise of more than one actual node thus weak cliques share actual nodes. This property of weak clique graph allows finding overlapping communities with high accuracy and low computational cost. Experiments on a variety of real-world and artificial networks validate the performance of the proposed algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02301-7		AUG 2020											
J								Enhanced butterfly optimization algorithm for reliability optimization problems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bidirectional search; Butterfly optimization algorithm; Reliability optimization problems; Constrained optimization; Optimization	REDUNDANCY ALLOCATION PROBLEM; HARMONY SEARCH ALGORITHM; BEE COLONY ALGORITHM; CUCKOO SEARCH; SYSTEM-RELIABILITY	In this study a recently introduced algorithm, based on foraging process of butterflies known as butterfly optimization algorithm (BOA) is explored and a modified variant is introduced. The framework of BOA is based on the fragrance emitted by the butterflies, which helps other butterflies in searching food as well as in identifying a mating partner. BOA performs both the local and global search while seeking for the global optimal solution for the problem. Despite this BOA sometime stuck in a local optima which results in a slow or poor convergence. This study embeds the bidirectional search in the structure of BOA. This helps to perform the local search in forward as well as backward direction. Greedy selection is performed, while selecting the direction. If the solution improves while traversing backward then backward traverse is adapted otherwise forward. The proposed variant is termed as bidirectional butterfly optimization algorithm (BBOA). This modification facilitate in accelerating the convergence rate of BOA, which is verified and validated by statistical and comparative results on a set of CEC2006 and CEC2014 benchmark problems. Non-parametric statistical tests are performed to analyze the results. Further the method is investigated to solve eight reliability optimization problems. Experimental results demonstrate the competitiveness of BBOA.																	1868-5137	1868-5145															10.1007/s12652-020-02481-2		AUG 2020											
J								An edge computing method using a novel mode component for power transmission line fault diagnosis in distribution network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Edge computing; Information fusion; Power transmission line; Fault diagnosis; Distribution network	LOCATION ALGORITHM; TRAVELING-WAVE	The commonly used fault diagnosis method of the power transmission line (PTL) is the traveling wave method which uses the traditional mode components alpha, beta and o to realize the fault type recognizing and the fault location in distribution network. However, the traditional mode components of the traveling wave method may take the value of zero that cannot provide the effective fault feature for the A-phase ground fault, or the A and B double phases short-circuit fault. Moreover, a single traditional mode component cannot recognize all the types of the faults, and the fault recognition has to use the multiple mode components that increases the complexity. To address the aforementioned issues, this paper proposes an edge computing scheme based on a novel mode component gamma for PTL fault diagnosis in distribution network. Firstly, the novel mode component gamma is fused according to the traditional mode components alpha and beta, then the propagation property of the traveling wave-gamma is derived. Secondly, the edge computing scheme and the PTL fault diagnosis method are designed for distribution network by using the only novel mode component gamma. Thirdly, the relationship table of the fault type and the boundary conditions as well as the component gamma expressions is derived for the fault recognition using the only traveling wave-gamma. In addition, the fault distance computing is explored by using the traveling wave-gamma. The computing is composed with the wave-gamma head identification using the optimal Wavelet vanishing moment parameter, and the arrival time determination, and the fault location method. To evaluate the efficiency of the proposed edge computing scheme, the simulation experiment and the laboratory tests are conducted. The simulation experiment results show that, the novel mode component gamma not only provides the effective fault features for all the ten types of the power transmission line faults, but also increases fault locating accuracy by the average 2.08 % compared with the methods using the traditional mode component alpha, beta and o. Meanwhile, the laboratory tests show that the edge computing method is practical.																	1868-5137	1868-5145															10.1007/s12652-020-02466-1		AUG 2020											
J								Optimal bandwidth allocation for web crawler systems with time constraints	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bandwidth allocation; Web crawler; Time constraint; Optimization	DESIGN	Web crawler is an important tool to obtain information from the Internet in a timely manner. In a typical web crawler system with limited bandwidth, many websites are crawled with different time constraints. Existing studies regarding web crawler systems do not consider the bandwidth allocation in such a complex environment; hence, the time constraints may not be satisfied. In this study, we investigate the bandwidth allocation approaches for such a web crawler system. The approaches are designed for two scenarios, i.e., when the number of websites exceeds or does not exceed the maximum number of web crawlers that the system can execute simultaneously. For the latter situation, we propose approaches to control the bandwidth for web crawlers to minimize the maximum complete time or minimize the sum of execution times of all web crawlers, considering assumptions of both sufficient and insufficient bandwidths. For the former situation, we propose a round-based reallocation approach to schedule both the sequence and bandwidth allocation of the web crawlers. Extensive simulations are conducted to validate the proposed approaches, and the results show that our approaches satisfy the time constraints well and achieve desirable execution performances in various scenarios.																	1868-5137	1868-5145															10.1007/s12652-020-02377-1		AUG 2020											
J								PDCAT: a framework for fast, robust, and occlusion resilient fiducial marker tracking	JOURNAL OF REAL-TIME IMAGE PROCESSING										Fiducial marker; Detection; Tracking; Embedded; Real-time; AprilTag	REAL-TIME DETECTION; AUGMENTED REALITY; CAMERA TRACKING	Square binary patterns have become the de facto fiducial marker for most computer vision applications. Existing tracking solutions suffer a number of limitations, such as the low frame-rate and sensitivity to partial occlusions. This work aims at overcoming these limitations, by exploiting temporal information in video-sequences. We propose a parallel detection, compensation and tracking (PDCAT) framework, which can be integrated into any binary marker system. Our solution is capable of recovering markers even when they become mostly occluded. Furthermore, the low processing time of the tracking task makes PDCAT more than an order of magnitude faster than a track-by-detect solution. This is particularly important for embedded computer vision applications, wherein the detection run at a very low frame rate. In the experiments conducted on an embedded computer, the processing frame rate of the track-by-detect solution was merely 11 FPS. Our solution, on the other hand, was capable of processing more than 100 FPS.																	1861-8200	1861-8219															10.1007/s11554-020-01010-w		AUG 2020											
J								An Assortment of Evolutionary Computation Techniques (AECT) in gaming	NEURAL COMPUTING & APPLICATIONS											GAME	Real-time strategy (RTS) games differ as they persist in varying scenarios and states. These games enable an integrated correspondence of non-player characters (NPCs) to appear as an autodidact in a dynamic environment, thereby resulting in a combined attack of NPCs on human-controlled character (HCC) with maximal damage. This research aims to empower NPCs with intelligent traits. Therefore, we instigate an assortment of ant colony optimization (ACO) with genetic algorithm (GA)-based approach to first-person shooter (FPS) game, i.e., Zombies Redemption (ZR). Eminent NPCs with best-fit genes are elected to spawn NPCs over generations and game levels as yielded by GA. Moreover, NPCs empower ACO to elect an optimal path with diverse incentives and less likelihood of getting shot. The proposed technique ZR is novel as it integrates ACO and GA in FPS games where NPC will use ACO to exploit and optimize its current strategy. GA will be used to share and explore strategy among NPCs. Moreover, it involves an elaboration of the mechanism of evolution through parameter utilization and updation over the generations. ZR is played by 450 players with varying levels having the evolving traits of NPCs and environmental constraints in order to accumulate experimental results. Results revealed improvement in NPCs performance as the game proceeds.																	0941-0643	1433-3058															10.1007/s00521-020-05295-7		AUG 2020											
J								Large-area damage image restoration algorithm based on generative adversarial network	NEURAL COMPUTING & APPLICATIONS										Generative adversarial network; Damage image; Generator; Discriminator; Contextual information; Perceptual information; Restoration	CLASSIFICATION	Given that the traditional image restoration algorithm cannot generate high-quality false images and the restoration accuracy for the large-area damaged images is low, this study proposed the restoration algorithm of large-area damaged images based on the generative adversarial network. First of all, this study extracted multi-scale edge detailed information in image damage area through building the smooth function. Secondly, this study built the generative adversarial network model by using the multi-scale edge detailed information as the original information of large-area damaged images and then trained the model to generate the best false images through the continuous game between the generator and discriminator. Finally, this study obtained the existing information of the false images by combining the contextual information and the perceptual information, calculated the priority of information and restored the information according to the optimal sequencing results and continuously updated the damaged edge information until the large-area damaged image restoration is completed. The results show that the accuracy of extracting the detailed information with the proposed algorithm is high, and peak signal-to-noise ratio of the false images generated by the generative adversarial network model is high, the structural similarity index with the real image is not less than 0.93, the quality of the false images is high, the accuracy rate of priority calculation of the restoration information is high, and the restoration accuracy for large-area damaged images is far ahead of other algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-05308-5		AUG 2020											
J								Comparison of two deep learning methods for ship target recognition with optical remotely sensed data	NEURAL COMPUTING & APPLICATIONS										Full convolutional network; Ship target recognition; Pixel level; Mask R-CNN; Faster R-CNN; Optical remote sensing images	NEURAL-NETWORK; OPTIMIZATION	As an important part of modern marine monitoring systems, ship target identification has important significance in maintaining marine rights and monitoring maritime traffic. With the development of artificial intelligence technology, image detection and recognition based on deep learning methods have become the most popular and practical method. In this paper, two deep learning algorithms, the Mask R-CNN algorithm and the Faster R-CNN algorithm, are used to build ship target feature extraction and recognition models based on deep convolutional neural networks. The established models were compared and analyzed to verify the feasibility of target detection algorithms. In this study, 5748 remote sensing maps were selected as the dataset for experiments, and two algorithms were used to classify and extract warships and civilian ships. Experiments showed that for the accuracy of ship identification, Mask R-CNN and Faster R-CNN reached 95.21% and 92.76%, respectively. These results demonstrated that the Mask R-CNN algorithm achieves pixel-level segmentation. Compared with the Faster R-CNN algorithm, the obtained target detection effect is more accurate, and the performance in target detection and classification is better, which reflects the great advantage of pixel-level recognition.																	0941-0643	1433-3058															10.1007/s00521-020-05307-6		AUG 2020											
J								Isomorphism on generalized fuzzy graphs and image visualizations	SOFT COMPUTING										Fuzzy graphs; Generalized fuzzy graphs; Isomorphism; Network; Image visualization		The graph theory is being used for representation in networks and chemical atomic structures very frequently. However, these days, uncertainties are imposed on such networks. Isomorphism in generalized fuzzy graphs has been introduced here to capture the similarity of uncertainties in different networks. Homomorphism, weak isomorphism, co-weak isomorphism and nearly isomorphism are defined with examples. Also, an application of image visualization is described.																	1432-7643	1433-7479				OCT	2020	24	19					14401	14409		10.1007/s00500-020-05260-5		AUG 2020											
J								A methodology for automatic parameter-tuning and center selection in density-peak clustering methods	SOFT COMPUTING										Density peaks clustering; Automatic parameter tuning; Optimal rules; Cluster validity index; Differential entropy	FAST SEARCH; FIND; ALGORITHM	The density-peak clustering algorithm, which we refer to as DPC, is a novel and efficient density-based clustering approach. The method has the advantage of allowing non-convex clusters, and clusters of variable size and density, to be grouped together, but it also has some limitations, such as the visual location of centers and the parameter tuning. This paper describes an optimization-based methodology for automatic parameter/center selection applicable both to the DPC and to other algorithms derived from it. The objective function is an internal/external cluster validity index, and the decisions are the parameterization of the algorithm and the choice of centers. The internal validation measures lead to an automatic parameter-tuning process, and the external validation measures lead to the so-calledoptimal rules, which are a tool to bound the performance of a given algorithm from above on the set of parameterizations. A numerical experiment with real data was performed for the DPC and for the fuzzy weightedk-nearest neighbor (FKNN-DPC) which validates the automatic parameter-tuning methodology and demonstrates its efficiency compared to the state of the art.																	1432-7643	1433-7479															10.1007/s00500-020-05244-5		AUG 2020											
J								Using modified metaheuristic algorithms to solve a hazardous waste collection problem considering workload balancing and service time windows	SOFT COMPUTING										Hazardous waste; Location routing problem; Workload balancing; Multi-objective optimization; Metaheuristic algorithms	LOCATION-ROUTING-PROBLEM; MATHEMATICAL-MODEL; TRANSPORTATION; OPTIMIZATION	Hazardous wastes' volume produced by human activities has increased in recent years. Consequently, associated risks involved in the treatment, recycling, disposing, and transportation of these hazardous materials have become more attractive for the researchers. In this study, we propose a new model for hazardous waste location routing problem. Appending the service time window and workload balance to the previous mathematical models can be taken into account as the major contributions of this study. Three objective functions including two systematic goals (cost and risk) and one social goal (workload balancing) have been considered for the model. Compatibility between wastes and a heterogeneous fleet of vehicles, which are rarely investigated in the literature, is discussed in this paper. Since the proposed model is classified as a multi-objective model, three multi-objective evolutionary algorithms, namely Non-dominated Sorting Genetic Algorithm II (NSGA-II), Pareto Envelope-based Selection Algorithm II (PESA-II), and Strength Pareto Evolutionary Algorithm II (SPEA-II) are employed. As two other innovations, an adaptive penalty function is developed and the PESA-II is modified by removing replicated solutions from its archive and their obtained results are discussed. Finally, by experimenting a number of test problems in different sizes, it is demonstrated that proposed modified PESA-II and SPEA-II perform better than NSGA-II in most of comparison metrics including feasible answers exploration, CPU time, spacing metric, inverted generational distance, quality metric, etc., whereas, NSGA-II creates more spread Pareto frontiers which are suitable for decision-maker to choose, from among a range of different options.																	1432-7643	1433-7479															10.1007/s00500-020-05261-4		AUG 2020											
J								psda: A tool for extracting knowledge from symbolic data with an application in Brazilian educational data	SOFT COMPUTING										Polygonal data; psda; Symbolic data analysis; Regression; Descriptive measures; R	INTERVAL; REGRESSION	Symbolic polygonal data analysis is a new type of framework to extract valuable knowledge from a new structure of data using regular polygon built from data in class, big data, and complex data. This paper introduces a toolbox for symbolic polygonal data, namedpsda, that contains the main descriptive measures for this type of variable, e.g., mean, variance, correlation, and a polygonal linear regression model (plr). It is applied at the Brazilian Basic Education Assessment System (SAEB), giving a new perspective to the managers of the counties to realize the public policy in the Brazilian educational system. The hypothesis test showed that the polygonal linear regression model presented the best performance compared to some symbolic interval regression models in the SAEB application.																	1432-7643	1433-7479															10.1007/s00500-020-05252-5		AUG 2020											
J								Iterated graph cut method for automatic and accurate segmentation of finger-vein images	APPLIED INTELLIGENCE										Finger-vein image; Segmentation; Graph cut; Fully automatic; Iterated	RECOGNITION; REGION	Recent advances in computer vision and machine intelligence have facilitated biometric technologies, which increasingly rely on image data in security practices. As an important biometric identifier, the near-infrared (NIR) finger-vein pattern is favoured by non-contact, high accuracy, and enhanced security systems. However, large stacks of low-contrast and complex finger-vein images present barriers to manual image segmentation, which locates the objects of interest. Although some headway in computer-aided segmentation has been made, state-of-the-art approaches often require user interaction or prior training, which are tedious, time-consuming and prone to operator bias. Recognizing this deficiency, the present study exploits structure-specific contextual clues and proposes an iterated graph cut (IGC) method for automatic and accurate segmentation of finger-vein images. To this end, the geometric structures of the image-acquisition system and the fingers provide the hard (centreline along the finger) and shape (rectangle around the finger) constraints. A node-merging scheme is applied to reduce the computational burden. The Gaussian probability model determines the initial labels. Finally, the maximum a posteriori Markov random field (MAP-MRF) framework is tasked with iteratively updating the data models of the object and the background. Our approach was extensively evaluated on 4 finger-vein databases and compared with some benchmark methods. The experimental results indicate that the proposed IGC method outperforms the state-of-the-practice approaches in finger-vein image segmentation. Specifically, the IGC method, relative to its level set deep learning (LSDL) counterpart, can increase the average F-measure value by 5.03%, 6.56%, 49.91%, and 22.89% when segmenting images from four different finger-vein databases. Therefore, this work can provide a feasible path towards fully automatic image segmentation.																	0924-669X	1573-7497															10.1007/s10489-020-01828-8		AUG 2020											
J								Preference relation based collaborative filtering with graph aggregation for group recommender system	APPLIED INTELLIGENCE										Group recommendation; Preference relation; Matrix factorization; Graph aggregation	MATRIX-FACTORIZATION; TRUST	Most of the group recommender systems (GRS) apply some aggregation strategy to the ratings given by the group members for generating recommendations. But this can be highly influenced by a few members of the group, which can lead to poor group recommendation. Further, rating based aggregation strategies do not provide efficient ranking of items. Keeping these things in mind, this paper proposes a preference relation (PR) based GRS, that uses matrix factorization (MF) for predicting unknown PRs for group members. The aggregation of preferences is done using a novel virtual user based weight aggregation strategy. The weight aggregation concept is derived from the graph aggregation process. The advantage of this process is that it does not ignore weak preferences and also contributes towards group recommendation. The proposed model is evaluated and compared using standard ranking measures for MovieLens and NetFlix datasets. Experimental results obtained using Top-K recommendation task indicates the superiority of the proposed GRS method over the others. The proposed GRS model provides the best performance when we balance the number of member in a group and the number of recommended items.																	0924-669X	1573-7497															10.1007/s10489-020-01848-4		AUG 2020											
J								Non-blind and Blind Deconvolution Under Poisson Noise Using Fractional-Order Total Variation	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Blind deconvolution; Poisson noise; Expectation-maximization; Fractional-order total variation	ALTERNATING DIRECTION METHOD; IMAGE-RESTORATION; REGULARIZATION; ALGORITHMS	In a wide range of applications such as astronomy, biology, and medical imaging, acquired data are usually corrupted by Poisson noise and blurring artifacts. Poisson noise often occurs when photon counting is involved in such imaging modalities as X-ray, positron emission tomography, and fluorescence microscopy. Meanwhile, blurring is also inevitable due to the physical mechanism of an imaging system, which can be modeled as a convolution of the image with a point spread function. In this paper, we consider both non-blind and blind image deblurring models that deal with Poisson noise. In the pursuit of high-order smoothness of a restored image, we propose a fractional-order total variation regularization to remove the blur and Poisson noise simultaneously. We develop two efficient algorithms based on the alternating direction method of multipliers, while an expectation-maximization algorithm is adopted only in the blind case. A variety of numerical experiments have demonstrated that the proposed algorithms can efficiently reconstruct piecewise smooth images degraded by Poisson noise and various types of blurring, including Gaussian and motion blurs. Specifically for blind image deblurring, we obtain significant improvements over the state of the art.																	0924-9907	1573-7683				NOV	2020	62	9					1238	1255		10.1007/s10851-020-00987-0		AUG 2020											
J								Multi-objective Bonobo Optimizer (MOBO): an intelligent heuristic for multi-criteria optimization	KNOWLEDGE AND INFORMATION SYSTEMS										Multi-objective optimization; Bonobo Optimizer; Intelligent algorithm; Multi-objective Bonobo Optimizer; Multi-criteria optimization	EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHM	Non-traditional optimization tools have proved their potential in solving various types of optimization problems. These problems deal with either single objective or multiple/many objectives. Bonobo Optimizer (BO) is an intelligent and adaptive metaheuristic optimization algorithm inspired from the social behavior and reproductive strategies of bonobos. There is no study in the literature to extend this BO to solve multi-objective optimization problems. This paper presents a Multi-objective Bonobo Optimizer (MOBO) to solve different optimization problems. Three different versions of MOBO are proposed in this paper, each using a different method, such as non-dominated sorting with adaptation of grid approach; a ranking scheme for sorting of population with crowding distance approach; decomposition technique, wherein the solutions are obtained by dividing a multi-objective problem into a number of single-objective problems. The performances of all three different versions of the proposed MOBO had been tested on a set of thirty diversified benchmark test functions, and the results were compared with that of four other well-known multi-objective optimization techniques available in the literature. The obtained results showed that the first two versions of the proposed algorithms either outperformed or performed competitively in terms of convergence and diversity compared to the others. However, the third version of the proposed techniques was found to have the poor performance.																	0219-1377	0219-3116				NOV	2020	62	11					4407	4444		10.1007/s10115-020-01503-x		AUG 2020											
J								Video super -resolution via dense non -local spatial -temporal convolutional network	NEUROCOMPUTING											SUPERRESOLUTION																		0925-2312	1872-8286				AUG 25	2020	403						1	12		10.1016/j.neucom.2020.04.039													
J								Fast video crowd counting with a Temporal Aware Network	NEUROCOMPUTING											CONVOLUTIONAL NEURAL-NETWORK																		0925-2312	1872-8286				AUG 25	2020	403						13	20		10.1016/j.neucom.2020.04.071													
J								HieNN-DWE: A hierarchical neural network with dynamic word embeddings for document level sentiment classification	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						21	32		10.1016/j.neucom.2020.04.084													
J								CACNet: Salient object detection via context aggregation and contrast emb e dding	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						33	44		10.1016/j.neucom.2020.04.032													
J								Variances-constraine d weighte d extreme learning machine for imbalanced classification	NEUROCOMPUTING											ELM; PERFORMANCE																		0925-2312	1872-8286				AUG 25	2020	403						45	52		10.1016/j.neucom.2020.04.052													
J								Multi -view image clustering based on sparse coding and manifold consensus	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						53	62		10.1016/j.neucom.2020.03.052													
J								Prediction of early stabilization time of electrolytic capacitor based on ARIMA-Bi_LSTM model	NEUROCOMPUTING											SHORT-TERM-MEMORY; USEFUL LIFE PREDICTION; BAYESIAN OPTIMIZATION; FORECASTING-MODEL; FAULT-DETECTION; NEURAL-NETWORK; DEEP; SYSTEM; DEGRADATION; ARCHITECTURE																		0925-2312	1872-8286				AUG 25	2020	403						63	79		10.1016/j.neucom.2020.03.054													
J								Fully memristive spiking -neuron learning framework and its applications on pattern recognition and edge detection	NEUROCOMPUTING											NETWORK; CLASSIFICATION																		0925-2312	1872-8286				AUG 25	2020	403						80	87		10.1016/j.neucom.2020.04.012													
J								Object and background disentanglement for unsupervised cross -domain person re -identification	NEUROCOMPUTING											NEURAL-NETWORK																		0925-2312	1872-8286				AUG 25	2020	403						88	97		10.1016/j.neucom.2020.04.088													
J								An effective maximum entropy exploration approach for deceptive game in reinforcement learning	NEUROCOMPUTING											NETWORKS; GO																		0925-2312	1872-8286				AUG 25	2020	403						98	108		10.1016/j.neucom.2020.04.068													
J								PMA-DRL: A parallel model -augmented framework for deep reinforcement learning algorithms	NEUROCOMPUTING											GO																		0925-2312	1872-8286				AUG 25	2020	403						109	120		10.1016/j.neucom.2020.04.091													
J								Variants of DropConnect in Learning vector quantization networks for evaluation of classification stability	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						121	132		10.1016/j.neucom.2019.12.131													
J								Outlier -resistant H . filtering for a class of networked systems under Round -Robin protocol *	NEUROCOMPUTING											TIME-VARYING SYSTEMS; FAULT-DETECTION; SENSOR NETWORKS; KALMAN FILTER; SUBJECT; NONLINEARITIES; OPTIMIZATION; NOISES; DESIGN; DELAYS																		0925-2312	1872-8286				AUG 25	2020	403						133	142		10.1016/j.neucom.2020.04.058													
J								Integrated adaptive dynamic programming for data -driven optimal controller design	NEUROCOMPUTING											TIME NONLINEAR-SYSTEMS; TRACKING CONTROL; SCHEME																		0925-2312	1872-8286				AUG 25	2020	403						143	152		10.1016/j.neucom.2020.04.095													
J								A hierarchical temporal attention -based LSTM encoder -decoder model for individual mobility prediction	NEUROCOMPUTING											VARIABILITY																		0925-2312	1872-8286				AUG 25	2020	403						153	166		10.1016/j.neucom.2020.03.080													
J								A network model of speaker identification with new feature extraction methods and BLSTM	NEUROCOMPUTING											RECOGNITION; VERIFICATION; COMPENSATION; MISMATCH																		0925-2312	1872-8286				AUG 25	2020	403						167	181		10.1016/j.neucom.2020.04.041													
J								Monocular 3D vehicle detection with multi -instance depth and geometry reasoning for autonomous driving	NEUROCOMPUTING											OBJECT DETECTION																		0925-2312	1872-8286				AUG 25	2020	403						182	192		10.1016/j.neucom.2020.03.076													
J								Non -fragile l 2-l . synchronization for switched inertial neural networks with random gain fluctuations: A persistent dwell -time switching law q	NEUROCOMPUTING											GENETIC REGULATORY NETWORKS; STATE ESTIMATION; ROBUST STABILITY; H-INFINITY; DISCRETE; SYSTEMS; STABILIZATION; SUBJECT																		0925-2312	1872-8286				AUG 25	2020	403						193	202		10.1016/j.neucom.2020.03.112													
J								Reachable set estimation for genetic regulatory networks with time -varying delays and bounded disturbances	NEUROCOMPUTING											EXPONENTIAL STABILITY; SWITCHED SYSTEMS; LINEAR-SYSTEMS																		0925-2312	1872-8286				AUG 25	2020	403						203	210		10.1016/j.neucom.2020.03.113													
J								Weighted sum synchronization of memristive coupled neural networks q	NEUROCOMPUTING											EXPONENTIAL SYNCHRONIZATION; QUASI-SYNCHRONIZATION; LAG SYNCHRONIZATION; STABILIZATION; RECOGNITION; DELAYS																		0925-2312	1872-8286				AUG 25	2020	403						211	223		10.1016/j.neucom.2020.04.087													
J								Deep balanced discrete hashing for image retrieval	NEUROCOMPUTING											CODES																		0925-2312	1872-8286				AUG 25	2020	403						224	236		10.1016/j.neucom.2020.04.037													
J								An SAE -based resampling SVM ensemble learning paradigm for pipeline leakage detection	NEUROCOMPUTING										Deep learning; Pipeline leakage detection; Sparse autoencoder; Leader-follower; Particle swarm optimization; Support vector machine	ALGORITHM; CLASSIFICATION; OPTIMIZATION; NETWORK; PSO; RECOGNITION; PARAMETERS; SELECTION	In this paper, an ensemble learning framework combining the sparse autoencoder (SAE) network with an improved support vector machine (SVM) is established to enhance the accuracy of classification in detecting the pipeline leakage. First of all, the SAE network is introduced to extract the discriminative and robust features of the pipeline leakage data. Then, a kind of leader-follower particle swarm optimization (LFPSO) algorithm is put forward to optimize the parameters of the SVM algorithm such that the probability trapping into the local optimum is effectively reduced. Next, the proposed LFPSO-SVM approach is employed to further classify and recognize the features extracted from the pipeline leakage data via the SAE network. Moreover, the performance of the SAE-LFPSO-SVM approach is quantitatively evaluated by three performance indicators, i.e., sensitivity, positive predictive value and total classification accuracy. Finally, some simulation examples are given to demonstrate the effectiveness of the proposed SAE-LFPSO-SVM approach, which exhibits a higher classification accuracy than that of the other traditional classification algorithms. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 25	2020	403						237	246		10.1016/j.neucom.2020.04.105													
J								Revisiting Nystr?m extension for hypergraph clustering	NEUROCOMPUTING											GRAPH; EIGENVECTORS; CUTS																		0925-2312	1872-8286				AUG 25	2020	403						247	256		10.1016/j.neucom.2020.04.063													
J								Adaptive fuzzy output feedback inverse optimal control for vehicle active suspension systems q	NEUROCOMPUTING											ROBUST-CONTROL; DESIGN																		0925-2312	1872-8286				AUG 25	2020	403						257	267		10.1016/j.neucom.2020.04.096													
J								Blur kernel estimation of noisy -blurred image via dynamic structure prior	NEUROCOMPUTING											BLIND DECONVOLUTION; MOTION																		0925-2312	1872-8286				AUG 25	2020	403						268	281		10.1016/j.neucom.2020.03.067													
J								Multi-level semantic representation enhancement network for relationship extraction	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						282	293		10.1016/j.neucom.2020.04.056													
J								Learning multi -level domain invariant features for sketch re -identification	NEUROCOMPUTING											IMAGE RETRIEVAL; NEURAL-NETWORK																		0925-2312	1872-8286				AUG 25	2020	403						294	303		10.1016/j.neucom.2020.04.060													
J								Adaptive resilient control of a class of nonlinear systems based on event -triggered mechanism ?	NEUROCOMPUTING											DYNAMIC SURFACE CONTROL; NEURAL-NETWORK CONTROL; SECURITY CONTROL; FEEDBACK CONTROL; PERFORMANCE; ATTACKS																		0925-2312	1872-8286				AUG 25	2020	403						304	313		10.1016/j.neucom.2020.04.061													
J								Robust trajectory tracking control of an underactuated control moment gyroscope via neural network?based feedback linearization ?	NEUROCOMPUTING											INERTIA WHEEL PENDULUM; SLIDING MODE CONTROL; ATTITUDE-CONTROL; ADAPTIVE-CONTROL; STEERING LAW; VEHICLE; SYSTEM																		0925-2312	1872-8286				AUG 25	2020	403						314	324		10.1016/j.neucom.2020.04.019													
J								Learning channel -wise spatio-temporal representations for video salient object detection	NEUROCOMPUTING											OPTIMIZATION																		0925-2312	1872-8286				AUG 25	2020	403						325	336		10.1016/j.neucom.2020.04.015													
J								Interpretable spatio-temporal attention LSTM model for flood forecasting	NEUROCOMPUTING											PREDICTION; RAINFALL																		0925-2312	1872-8286				AUG 25	2020	403						348	359		10.1016/j.neucom.2020.04.110													
J								Boosting label weighted extreme learning machine for classifying multi -label imbalanced data	NEUROCOMPUTING											SUPPORT VECTOR MACHINE; CLASSIFICATION; ENSEMBLE; FRAMEWORK; CLASSIFIERS; SMOTE; ELM																		0925-2312	1872-8286				AUG 25	2020	403						360	370		10.1016/j.neucom.2020.04.098													
J								Automated diagnosis of multi -plane breast ultrasonography images using deep neural networks	NEUROCOMPUTING											COMPUTER-AIDED DIAGNOSIS; ULTRASOUND; CLASSIFICATION; CANCER; MAMMOGRAPHY; FEATURES; LESIONS; MASSES; WOMEN																		0925-2312	1872-8286				AUG 25	2020	403						371	382		10.1016/j.neucom.2020.04.123													
J								Online anomaly detection with sparse Gaussian processes	NEUROCOMPUTING											CONCEPT DRIFT; APPROXIMATIONS; CLASSIFIER; SELECTION																		0925-2312	1872-8286				AUG 25	2020	403						383	399		10.1016/j.neucom.2020.04.077													
J								Localization -aware channel pruning for object detection	NEUROCOMPUTING																													0925-2312	1872-8286				AUG 25	2020	403						400	408		10.1016/j.neucom.2020.03.056													
J								Domain generalization in rotating machinery fault diagnostics using deep neural networks	NEUROCOMPUTING											BEARINGS																		0925-2312	1872-8286				AUG 25	2020	403						409	420		10.1016/j.neucom.2020.05.014													
J								Adaptive NN -based finite -time tracking control robots with time -varying full state constraints for wheeled mobile	NEUROCOMPUTING											BARRIER LYAPUNOV FUNCTIONS; NONLINEAR-SYSTEMS; FUZZY CONTROL; STABILIZATION																		0925-2312	1872-8286				AUG 25	2020	403						421	430		10.1016/j.neucom.2020.04.104													
J								Enhanced sequence labeling based on latent variable conditional random fields	NEUROCOMPUTING											PROBABILISTIC FUNCTIONS; MODEL																		0925-2312	1872-8286				AUG 25	2020	403						431	440		10.1016/j.neucom.2020.04.102													
J								Parameter estimation based-FDI method enhancement with mixed particle filter	NEUROCOMPUTING											FAULT-DETECTION; MOBILE ROBOT; DIAGNOSIS																		0925-2312	1872-8286				AUG 25	2020	403						441	451		10.1016/j.neucom.2020.04.048													
J								A CNN -based comparing network for the detection of steady-state visual evoked potential responses	NEUROCOMPUTING											CANONICAL CORRELATION-ANALYSIS; BRAIN; INTERFACE; REHABILITATION; CLASSIFICATION; RECOGNITION; COMPONENTS; SPEED																		0925-2312	1872-8286				AUG 25	2020	403						452	461		10.1016/j.neucom.2020.03.048													
J								Autonomous reboot: Aristotle, autonomy and the ends of machine ethics	AI & SOCIETY										Autonomy; Artificial moral agent; AMA; Machine ethics		Tonkens (Mind Mach, 19, 3, 421-438, 2009) has issued a seemingly impossible challenge, to articulate a comprehensive ethical framework within which artificial moral agents (AMAs) satisfy a Kantian inspired recipe-"rational" and "free"-while also satisfying perceived prerogatives of machine ethicists to facilitate the creation of AMAs that are perfectly and not merely reliably ethical. Challenges for machine ethicists have also been presented by Anthony Beavers and Wendell Wallach. Beavers pushes for the reinvention of traditional ethics to avoid "ethical nihilism" due to the reduction of morality to mechanical causation. Wallach pushes for redoubled efforts toward a comprehensive account of ethics to guide machine ethicists on the issue of artificial moral agency. Options, thus, present themselves: reinterpret traditional ethics in a way that affords a comprehensive account of moral agency inclusive of both artificial and natural agents, or give up on the possibility and "muddle through" regardless. This series of papers pursues the first option, meets Tonkens' "challenge" and pursues Wallach's ends through Beavers' proposed means, by "landscaping" traditional moral theory in resolution of a comprehensive account of moral agency. This first paper sets out the challenge and establishes the tradition that Kant had inherited from Aristotle, briefly entertains an Aristotelian AMA, fields objections, and ends with unanswered questions. The next paper in this series responds to the challenge in Kantian terms, and argues that a Kantian AMA is not only a possibility for Machine ethics research, but a necessary one.																	0951-5666	1435-5655															10.1007/s00146-020-01039-2		AUG 2020											
J								Meta-heuristic algorithms for resource Management in Crisis Based on OWA approach	APPLIED INTELLIGENCE										Resource Management in Crisis (RMC); Dynamic resource allocation (DRA); Multi-objective optimization (MOO); Non-dominated sorting genetic algorithm-II (NSGA-II); Strength Pareto evolutionary algorithm-II (SPEA-II); Maximum Bayesian entropy OWA (MBEOWA)	WEAPON-TARGET ASSIGNMENT; BEE COLONY ALGORITHM; ALLOCATION PROBLEM; GENETIC ALGORITHM; OPERATOR WEIGHTS; MISSILE DEFENSE; OPTIMIZATION; MODEL; SYSTEMS	In crisis management, Threat Evaluation (TE) and Resource Allocation (RA) are two key components. To build an automated system in this area after modelling Threat Evaluation and Resource Allocation processes, solving these models and finding the optimal solution are further important issues. In this paper, Non-dominated Sorting Genetic Algorithm-II (NSGA-II) and Strength Pareto Evolutionary Algorithms (SPEA-II) are employed to solve a multi-objective multi-stage Resource Allocation problem. These Algorithms have been compared using normalized values of the objectives by generational distance, spread, hyper-volume, cardinality and actual computational times. It is found that the non-dominated solutions obtained by SPEA-II are better than NSGA-II both in terms of convergence and diversity but at the expense of computational time. Here, the fuzzy inference systems and the decision tree have been used to conduct threat evaluation process. Finally, Ordered Weighted Averaging (OWA) with maximum Bayesian entropy method for determining the operator weights has been used to pick the final choice among optimal options. We plan to use the proposed method in this paper for crisis management in Iranian Red Crescent organization during fire fighting. Two real studies have been done and results have been presented.																	0924-669X	1573-7497															10.1007/s10489-020-01808-y		AUG 2020											
J								A comparative analysis of gradient boosting algorithms	ARTIFICIAL INTELLIGENCE REVIEW										XGBoost; LightGBM; CatBoost; Gradient boosting; Random forest; Ensembles of classifiers		The family of gradient boosting algorithms has been recently extended with several interesting proposals (i.e. XGBoost, LightGBM and CatBoost) that focus on both speed and accuracy. XGBoost is a scalable ensemble technique that has demonstrated to be a reliable and efficient machine learning challenge solver. LightGBM is an accurate model focused on providing extremely fast training performance using selective sampling of high gradient instances. CatBoost modifies the computation of gradients to avoid the prediction shift in order to improve the accuracy of the model. This work proposes a practical analysis of how these novel variants of gradient boosting work in terms of training speed, generalization performance and hyper-parameter setup. In addition, a comprehensive comparison between XGBoost, LightGBM, CatBoost, random forests and gradient boosting has been performed using carefully tuned models as well as using their default settings. The results of this comparison indicate that CatBoost obtains the best results in generalization accuracy and AUC in the studied datasets although the differences are small. LightGBM is the fastest of all methods but not the most accurate. Finally, XGBoost places second both in accuracy and in training speed. Finally an extensive analysis of the effect of hyper-parameter tuning in XGBoost, LightGBM and CatBoost is carried out using two novel proposed tools.																	0269-2821	1573-7462															10.1007/s10462-020-09896-5		AUG 2020											
J								Nature inspired optimization algorithms or simply variations of metaheuristics?	ARTIFICIAL INTELLIGENCE REVIEW										Nature-inspired intelligent (NII) algorithms; Guidelines for nature-inspired algorithms; AI and optimization; Evaluation of algorithm's innovation	GLOBAL OPTIMIZATION; SWARM OPTIMIZATION; HEURISTIC OPTIMIZATION; BLACK-HOLE; SEARCH; COLONY; BEHAVIOR	In the last decade, we observe an increasing number of nature-inspired optimization algorithms, with authors often claiming their novelty and their capabilities of acting as powerful optimization techniques. However, a considerable number of these algorithms do not seem to draw inspiration from nature or to incorporate successful tactics, laws, or practices existing in natural systems, while also some of them have never been applied in any optimization field, since their first appearance in literature. This paper presents some interesting findings that have emerged after the extensive study of most of the existing nature-inspired algorithms. The need for irrationally introducing new nature inspired intelligent (NII) algorithms in literature is also questioned and possible drawbacks of NII algorithms met in literature are discussed. In addition, guidelines for the development of new nature-inspired algorithms are proposed, in an attempt to limit the misleading appearance of variation of metaheuristics as nature inspired optimization algorithms.																	0269-2821	1573-7462															10.1007/s10462-020-09893-8		AUG 2020											
J								Accelerating ELM training over data streams	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Extreme learning machine; Offline training; Online training; Flink	EXTREME LEARNING-MACHINE; CLASSIFICATION; APPROXIMATION; FRAMEWORK; KERNELS	In the field of machine learning, offline training and online training occupy the same important position because they coexist in many real applications. The extreme learning machine (ELM) has the characteristics of fast learning speed and high accuracy for offline training, and online sequential ELM (OS-ELM) is a variant of ELM that supports online training. With the explosive growth of data volume, running these algorithms on distributed computing platforms is an unstoppable trend, but there is currently no efficient distributed framework to support both ELM and OS-ELM. Apache Flink is an open-source stream-based distributed platform for both offline processing and online data processing with good scalability, high throughput, and fault-tolerant ability, so it can be used to accelerate both ELM and OS-ELM. In this paper, we first research the characteristics of ELM, OS-ELM and distributed computing platforms, then propose an efficient stream-based distributed framework for both ELM and OS-ELM, named ELM-SDF, which is implemented on Flink. We then evaluate the algorithms in this framework with synthetic data on distributed cluster. In summary, the advantages of the proposed framework are highlighted as follows. (1) The training speed of FLELM is always faster than ELM on Hadoop and Spark, and its scalability behaves better as well. (2) Response time and throughput of FLOS-ELM achieve better performance than OS-ELM on Hadoop and Spark when the incremental training samples arrive. (3) The response time and throughput of FLOS-ELM behave better in native-stream processing mode when the incremental data samples are continuously arriving.																	1868-8071	1868-808X															10.1007/s13042-020-01158-8		AUG 2020											
J								Developing and evaluating a proposed health security framework in IoT using fuzzy analytic network process method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things (IoT); Health Internet of Things; Security framework; Analytic network process (ANP); Cloud services; Fuzzy ANP; Privacy	MULTICRITERIA DECISION-MAKING; TRUST MANAGEMENT; THINGS SECURITY; ACCESS-CONTROL; INTERNET; SYSTEM; CHALLENGES; SCHEME; SMART	Internet of Things (IoT) played a significant role in Healthcare. One of the major challenges in IoT health is security. For this reason, regarding the past studies, there is a lack of a comprehensive security framework that encompasses all layers of IoT. This article offers a security framework with four layers of sensor, network, service, and application. These layers help to develop security mechanisms within the proposed framework by providing security solutions. Also, security improvement has been influenced by the Multi Criteria Decision Making (MCDM) technique. To evaluate this framework, a conceptual model was defined with six critical criteria including security, network, services, interoperability, privacy, and reliability. The validity and reliability of this conceptual model were analyzed using SPSS 24 and AMOS 18. After proving the important criteria in the proposed model, we used the MCDM process of fuzzy analytic network process to prioritize the important criteria. The evaluation results of the conceptual model in the proposed framework, after analyzing the routes, show the validity of the proposed criteria. Therefore, in this model, 8 out of 12 communication routes between the criteria have a positive and significant relationship and 4 routes do not. The results of prioritizing important criteria in the research model, using analytic network process method, show that the network criterion, and authentication and validation sub-criteria have the highest priority among the proposed criteria. Thus, it can be decided that security solutions in the network layer and authentication and validation sub-criteria help to develop the proposed security framework.																	1868-5137	1868-5145															10.1007/s12652-020-02472-3		AUG 2020											
J								A reactive search optimization algorithm for scientific workflow scheduling using clustering techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Workflow scheduling; NP-complete problem; Metaheuristic algorithm; Shuffled frog leaping algorithm (SFLA)	FROG-LEAPING ALGORITHM	Cloud computing is the style that can give plenty of shared pool resources such as hardware or software to clients based on requests from the internet. These resources are then scaled up automatically based on the specifications of the clients. Workflow scheduling optimization is an area of research activities in infrastructure as a service (IaaS) of the cloud. This problem is NP-complete. Thus, building a workflow scheduler that is optimum, having a reasonable level of performance and speed of computation, can be quite challenging in a distributed cloud environment. Metaheuristic algorithms may be improved in terms of their solution and its quality and speed of convergence utilizing combining it with other metaheuristic algorithms or any other algorithms that are metaheuristic based on local search. Shuffled frog leaping algorithm (SFLA) was acknowledged a metaheuristic performing heuristic search with a heuristic function (mathematical function) seeking solutions to combinatorial optimization problems. An optimization ratio on makespan %, resource utilization and computational cost performs better for SFLA-RSO with clustering when the number of tasks are increased.																	1868-5137	1868-5145															10.1007/s12652-020-02480-3		AUG 2020											
J								BeeM-NN: An efficient workload optimization using Bee Mutation Neural Network in federated cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud; Workload; Prediction; Optimization; BeeM-NN	QUANTITATIVE-ANALYSIS; LOAD PREDICTION; ALGORITHM; SERVICE	Cloud computing is an extensively implemented technique to handle enormous amount of data as it provides flexibility and scalability features. In an established cloud environment, users process their request to share the data that are stored in it. Under the dynamic cloud environment, multiple requests are processed in a short time, which leads to the problem of resource allocation. Virtual Machines or servers aid the cloud in maintaining the workflow active through proper distribution of resources. However, the accurate workload prediction model is necessary for effective resource management. In the present paper, a novel BeeM-NN framework is proposed through the integration of Workload Neural Network Algorithm (WNNA) and Novel Bee Mutation Optimization Algorithm (NBMOA) for optimized workload prediction in a cloud environment. The proposed model encloses the Fitness Feature Extraction Algorithm initially to extract the feature dataset from Azure public dataset and is provided to train the WNNA. The predicted workloads are optimized with the NBMOA in the cloud. The generated model is tested using the workload data traces from the federated cloud service provider and is evaluated and compared with the existing models. The outcome showed the prediction model achieved an accuracy of 99.98% better than the current models with optimum performance in the consumption of resources and cost. The future work is to use the predicted workloads for scheduling in the cloud.																	1868-5137	1868-5145															10.1007/s12652-020-02474-1		AUG 2020											
J								Ensemble echo network with deep architecture for time-series modeling	NEURAL COMPUTING & APPLICATIONS										Echo state network; Deep reservoir; Bayesian optimization; Ensemble mechanism; Multivariate time series	NEURAL-NETWORK; STATE NETWORKS; CLASSIFICATION; REPRESENTATION; LSTM	Echo state network belongs to a kind of recurrent neural networks that have been extensively employed to model time-series datasets. The function of reservoir in echo state network is expected to extract the feature context from time-series datasets. However, generalization of echo state networks is limited in real-world application because the architectures of the network are fixed and the hyper-parameters are hard to be automatically determined. In the present study, the ensemble Bayesian deep echo network (EBDEN) model with deep and flexible architecture is proposed. Such networks with deep architecture progressively extract more dynamic echo states through multiple reservoirs than those with the shallow reservoir. To enhance the flexibility of the configuration for the network, this study investigates the Bayesian optimization procedure of hyper-parameters and ensures the suitable hyper-parameters to activate the network. In addition, when dealing with more complex time-series datasets, ensemble mechanism of EBDEN can measure the redundancy for the channels of the time series without sacrificing the algorithm's performance. In this paper, the deep, optimization and ensemble architectures of EBDEN are verified by experiments benchmarked on multivariate time-series repositories and realistic tasks such as chaotic series representation and Dansgaard-Oeschger estimation tasks. According to the results, EBDEN achieves high level of the goodness-of-fit and classification performance in comparison with state-of-the-art models.																	0941-0643	1433-3058															10.1007/s00521-020-05286-8		AUG 2020											
J								An optimal extreme learning-based classification method for power quality events using fractional Fourier transform	NEURAL COMPUTING & APPLICATIONS										Time-frequency analysis; Signal processing techniques; PQ events; Fractional Fourier transform (FRFT); Extreme learning machine (ELM); Teaching-learning-based optimization	WAVELET; SYSTEM; MACHINE	Mitigation of power quality events (PQEs) needs an accurate, faster, and efficient detection and classification technique for designing the compensating devices as a remedial measure to the problem. This study motivates on this issue to formulate a better technique based on fractional Fourier transform (FRFT) and extreme learning machine (ELM). FRFT is considered for relevant feature extraction due to its characteristics like enhanced order control and capability to provide time, frequency, and intermediate time-frequency depictions for any non-stationary power signals. The possibility of multi-domain feature extraction capability due to its easy order control arrives at a robust feature matrix, which makes the classification more accurate. An optimal ELM-based classifier is designed by tuning its system parameters applying modified teaching-learning-based optimization (MTLBO). This optimal ELM is implemented in this study along with FRFT to formulate the proposed approach denoted as FRFT-MTLBO-ELM. To give a good reason for the enhanced performance of the proposed technique, noisy and hybrid synthetic signals of ten PQEs are generated and tested considering fully all real-time condition cases. At last, a comparative result is presented with other applied signal processing-based approaches and it is found that the proposed FRFT-MTLBO-ELM approach is very effective and comparatively better justifying its real-time implementation in the monitoring systems.																	0941-0643	1433-3058															10.1007/s00521-020-05282-y		AUG 2020											
J								Convolutional Sparse Coded Dynamic Brain Functional Connectivity	NEURAL PROCESSING LETTERS										Functional magnetic resonance images; Convolutional sparse coding; Dynamic brain network; Computer assisted diagnosis	FMRI	Functional brain network has been widely studied in many previous work for brain disorder diagnosis and brain network analysis. However, most previous work focus on static dynamic brain network research. Lots of recent work reveals that the brain shows dynamic activity even in resting state. Such dynamic brain functional connectivity reveals discriminative patterns for identifying many brain disorders. Current sliding window based dynamic brain connectivity framework are not easy to be applied to real clinical applications due to many issues: First, how to set up the optimal sliding window size and how to determine the threshold for the brain connectivity patterns. Secondly, how to represent the high dimensional dynamic brain connectivity pattern in a low dimensional representations for diagnosis purpose. Last, how to deal with the different length dynamic brain network patterns especially when the raw data are of different length. In order to address all those above issues, we proposed a new framework, which employs multiple scale sliding windows and automatically learns a sparse and low ran dynamic brain functional connectivity patterns from raw fMRI data. Furthermore, we are able to measure different length dynamic brain functional connectivity patterns in an equal space by learning a sparse coded convolutional filters. We have evaluated our method with state of the art dynamic brain network methods and the results demonstrated the strong potential of our methods for brain disorder diagnosis in real clinical applications.																	1370-4621	1573-773X															10.1007/s11063-020-10295-8		AUG 2020											
J								Spatial-temporal fuzzy information granules for time series forecasting	SOFT COMPUTING										Fuzzy information granule; Time series; Interval type-2 fuzzy set; Long-term forecasting	ALGORITHM; INTERVALS; SYSTEMS	The prediction of time series in multi-steps is of significance in reality. However, considering the uncertainty and high noise existing in time series, the long-term forecasting is still an open problem. By means of granular computing, in this article, a novel spatial-temporal fuzzy information granule (STFIG) model is proposed to achieve the multi-step forecasting of time series. From the perspective of time dimension, by using unequal division method, time series is converted into generalized time-varying fuzzy information granules, where the trend information and dispersion degree of sequence data can be quantitatively described. Moreover, in terms of spatial dimension, the fluctuation information of time series is also calculated and involved into information granules, which can further enhance the semantic representation of sequential data. In order to improve the ability of dealing with uncertainties and fuzziness in time series, the interval type-2 fuzzy set is applied in the granules model. By using synthetic data and real-life time series, experiments are carried out to verify the effectiveness of the proposed scheme, where abundant semantic information and better long-term predictive performance can be obtained.																	1432-7643	1433-7479															10.1007/s00500-020-05268-x		AUG 2020											
J								Aco-training-based approach for the hierarchical multi-label classification of research papers	EXPERT SYSTEMS										co-training; hierarchical multi-label classification; imbalanced data; research papers classification; semi-supervised learning	GENE-FUNCTION; DECISION TREES	This paper focuses on the problem of the hierarchical multi-label classification of research papers, which is the task of assigning the set of relevant labels for a paper from a hierarchy, using reduced amounts of labelled training data. Specifically, we study leveraging unlabelled data, which are usually plentiful and easy to collect, in addition to the few available labelled ones in a semi-supervised learning framework for achieving better performance results. Thus, in this paper, we propose a semi-supervised approach for the hierarchical multi-label classification task of research papers based on the well-known Co-training algorithm, which exploit content and bibliographic coupling information as two distinct papers' views. In our approach, two hierarchical multi-label classifiers, are learnt on different views of the labelled data, and iteratively select their most confident unlabelled samples, which are further added to the labelled set. The success of our suggested Co-training-based approach lies in two main components. The first is the use of two suggested selection criteria (i.e., Maximum Agreement and Labels Cardinality Consistency) that enforce selecting confident unlabelled samples. The second is the appliance of an oversampling method that rebalances the labels distribution of the initial labelled set, which reduces the reinforcement of the label imbalance issue during the Co-training learning. The proposed approach is evaluated using a collection of scientific papers extracted from the ACM digital library. Performed experiments show the effectiveness of our approach with regards to several baseline methods.																	0266-4720	1468-0394														e12613	10.1111/exsy.12613		AUG 2020											
J								A benchmark for clothes variation in person re-identification	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										clothes variation; deep learning; metric learning; person re-identification	ADAPTATION	Person re-identification (re-ID) has drawn attention significantly in the computer vision society due to its application and research significance. It aims to retrieve a person of interest across different camera views. However, there are still several factors that hinder the applications of person re-ID. In fact, most common data sets either assume that pedestrians do not change their clothing across different camera views or are taken under constrained environments. Those constraints simplify the person re-ID task and contribute to early development of person re-ID, yet a person has a great possibility to change clothes in real life. To facilitate the research toward conquering those issues, this paper mainly introduces a new benchmark data set for person re-identification. To the best of our knowledge, this data set is currently the most diverse for person re-identification. It contains 107 persons with 9,738 images, captured in 15 indoor/outdoor scenes from September 2019 to December 2019, varying according to viewpoints, lighting, resolutions, human pose, seasons, backgrounds, and clothes especially. We hope that this benchmark data set will encourage further research on person re-identification with clothes variation. Moreover, we also perform extensive analyses on this data set using several state-of-the-art methods. Our dataset is available at .																	0884-8173	1098-111X				DEC	2020	35	12					1881	1898		10.1002/int.22276		AUG 2020											
J								Mixture analysis using non-negative elastic net for Raman spectroscopy	JOURNAL OF CHEMOMETRICS										mixture analysis; non-negative elastic net; Raman spectroscopy; spectral database	PATTERN-RECOGNITION; FLOW CYTOMETER; STANDARDIZATION; SPECTROMETERS; EXPLOSIVES; REGRESSION; SELECTION; DATABASE	Raman spectrum is wealth of structural information, which can be used as molecular fingerprint to identify compounds. There are relations between the intensities of Raman peaks and the concentrations of compound, and they can be used to estimate relative concentrations of the compounds in mixture. However, it is still challenging to interpret the Raman spectrum of mixture. The non-negative lasso (NN-LASSO) has been used to solve this problem, but it failed to identify highly correlated compounds. The quadratic term of non-negative elastic net (NN-EN) can ensure the stability of the fitted model. Therefore, a novel mixture analysis method was developed based on NN-EN in this study. It has been applied to analyze the simulated, liquid, powder, tablet, and quantitative mixture datasets. Results showed that NN-EN can identify the compounds in mixture with high accuracy and estimate their relative concentrations with small deviation. Furthermore, NN-EN was more stable than NN-LASSO when the spectra of some compounds are highly correlated. It is a promising approach for analyzing Raman spectra of mixtures.																	0886-9383	1099-128X														e3293	10.1002/cem.3293		AUG 2020											
J								Use of data mining in a two-step process of profiling student preferences in relation to the enhancement of English as a foreign language teaching	STATISTICAL ANALYSIS AND DATA MINING										higher education; Kohonen map; market basket analysis; qualitative research; survey	EFL LEARNERS; EDUCATION	The paper pursues a twofold goal. The first goal refers to the identification of university students' needs regarding such modifications to English language courses that would improve English as a foreign language (EFL) teaching outcomes. The other goal refers to the methodical issue of achieving the first one. In this aspect, the use of selected data mining techniques in a hierarchical way in real data processing is proposed. These are: (a) Self-Organizing Map (SOM) dataset segmentation and then (b) market basket analysis applied to the individual SOM segments. The research data were collected from the students' survey concerning their opinion of the EFL teaching process; 347 students of a faculty of a technical university in Poland completed the questionnaire. The use of SOM allowed the identification of homogeneous groups of students, while market basket analysis allowed indicating, within each group, the relationships between student opinions of effective methods of teaching English. In such a way, satisfactory student preference profiles as regards their approach to the improvement of English language competences were developed. On this basis, EFL teaching methods appropriate for the specific profile can be adapted.																	1932-1864	1932-1872				OCT	2020	13	5					482	498		10.1002/sam.11478		AUG 2020											
J								Garra Rufa-inspired optimization technique	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										algorithm; firefly algorithm; Garra Rufa; optimization; swarm optimization	BIG-BANG; FIREFLY ALGORITHMS; GENETIC ALGORITHM; COLONY ALGORITHM; PSO ALGORITHM	Natural selection has inspired researchers to develop and apply several intelligent optimization techniques in the past few decades. Generally, in artificial intelligence optimization, the particles follow a local or global best particle until finding an acceptable solution. In well- developed optimization techniques, such as swarm optimization (PSO) and the firefly algorithm (FA), getting around the initial optimal value of the group and randomly checking the effect of the surrounding points may lead to a better solution than the initial optimal value. The present work was inspired by the fascinating movement of Garra Rufa fish between two immersed legs during a regular "fish massage session." A new optimization approach is proposed and modeled based on the movements of Garra Rufa fish, in which the particles are separated into groups, and the best optimal value leads each group for the group. Also, some of these particles are allowed to change groups depending on the fitness of the leaders of the groups. The suggested strategy is then compared with PSO and FA using multiple test optimization functions, such as the Ackley, Hartmann, Michalewicz, Shubert, Easom, Bohachevsky, and Rastrigin functions. Also, a multiobjective real issue in power system is tested using the proposed methods where the objectives were cumulative voltage deviation and power losses of three weight sets during the selection allocation of distribution generators. The results show that the proposed method provides good data and greater convergence to the optimal point compared with the classical methods for most of the functions tested.																	0884-8173	1098-111X				NOV	2020	35	11					1831	1856		10.1002/int.22274		AUG 2020											
J								Bonsai: diverse and shallow trees for extreme multi-label classification	MACHINE LEARNING										Large-scale multi-label classification; Extreme multi-label classification; Large label space		Extreme multi-label classification (XMC) refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. In this paper, we develop a suite of algorithms, called Bonsai, which generalizes the notion of label representation in XMC, and partitions the labels in the representation space to learn shallow trees. We show three concrete realizations of this label representation space including: (i) the input space which is spanned by the input features, (ii) the output space spanned by label vectors based on their co-occurrence with other labels, and (iii) the joint space by combining the input and output representations. Furthermore, the constraint-free multi-way partitions learnt iteratively in these spaces lead to shallow trees. By combining the effect of shallow trees and generalized label representation, Bonsai achieves the best of both worlds-fast training which is comparable to state-of-the-art tree-based methods in XMC, and much better prediction accuracy, particularly on tail-labels. On a benchmark Amazon-3M dataset with 3 million labels, Bonsai outperforms a state-of-the-art one-vs-rest method in terms of prediction accuracy, while being approximately 200 times faster to train. The code for Bonsai is available at.																	0885-6125	1573-0565															10.1007/s10994-020-05888-2		AUG 2020											
J								An Analysis of Fast Learning Methods for Classifying Forest Cover Types	APPLIED ARTIFICIAL INTELLIGENCE												Proper mapping and classification of Forest cover types are integral in understanding the processes governing the interaction mechanism of the surface with the atmosphere. In the presence of massive satellite and aerial measurements, a proper manual categorization has become a tedious job. In this study, we implement three different modest machine learning classifiers along with three statistical feature selectors to classify different cover types from cartographic variables. Our results showed that, among the chosen classifiers, the standard Random Forest Classifier together with Principal Components performs exceptionally well, not only in overall assessment but across all seven categories. Our results are found to be significantly better than existing studies involving more complex Deep Learning models.																	0883-9514	1087-6545				AUG 23	2020	34	10					691	709		10.1080/08839514.2020.1771523													
J								Effective Context-Aware Recommendations Based on Context Weighting Using Genetic Algorithm and Alleviating Data Sparsity	APPLIED ARTIFICIAL INTELLIGENCE											SYSTEMS; ACCURACY	Context-aware collaborative filtering (CACF) is an effective approach for adapting recommendations under users' specific contextual situations and aims to improve predictive accuracy for Context-aware recommender systems (CARSs). Incorporating context in recommender systems (RSs) considering the equal importance to all contextual dimensions is not appropriate for seeking an intelligent and useful recommendation. In this paper, we propose a Real-coded Genetic Algorithm (RCGA) based CARS framework that exploits contextual pre-filtering and contextual modeling paradigms into CACF with appropriate context feature weights for enhancing accuracy as well as the diversity of the recommendation list. Further to alleviate the data sparsity, an effective missing value prediction (EMVP) algorithm is applied into proposed framework. The accuracy based on RCGA is compared with other two schemes: Support Vector Machine (SVM) and Particle Swarm Optimization (PSO), and RCGA has shown better results. Experimental results based on real-world datasets have clearly established the effectiveness of our proposed CARS schemes.																	0883-9514	1087-6545				AUG 23	2020	34	10					730	753		10.1080/08839514.2020.1775011													
J								A new user similarity measure in a new prediction model for collaborative filtering	APPLIED INTELLIGENCE										Collaborative filtering; Prediction; Proximity-impact-popularity; Similarity measure; Recommender systems	RECOMMENDER SYSTEMS; STATISTICAL TESTS; GENERATION; ALLEVIATE; ALGORITHM	The Recommender Systems (RSs) based on the performance of Collaborative filtering (CF) depends on similarities among users or items obtained by a user-item rating matrix. The conventional measures such as the Pearson correlation coefficient (PCC), cosine (COS), and Jaccard (JACC) provide a varied and dissimilar value when the ratings between the users lie in the positive and negative side of the rating scale. These measures are also not very effective when there is sparsity in the rating matrix of the user-item. These problems are addressed by the Proximity-Impact-Popularity (PIP) similarity measure. Even though thePIPmethod provides an improved solution for this problem, the range of values for each component inPIPis very high. To address this issue and to improve the performance of a CF-based RS, a modified proximity-impact-popularity (MPIP) similarity measure is introduced. The expression is designed to getPIPvalues within the range of 0 to 1. A modified prediction expression is proposed to predict the available and unavailable ratings by combining user- and item-related components. The proposed method is tested by using various benchmark datasets. The size of the user-item sparse matrix varies to compare the performance of the methods in terms of mean absolute error, root mean squared error, precision, recall, and F-1-measure. The performance of the proposed method is statistically tested through the Friedman and McNemer test. The results obtained by using the evaluation criteria indicate that the proposed method provides a better solution than the conventional methods. The statistical analysis reveals that the proposed method provides minimumMAEandRMSEvalues. Similarly, it also provides a maximum F-1-measure for all the sub-problems.																	0924-669X	1573-7497															10.1007/s10489-020-01811-3		AUG 2020											
J								EnsP(KDE)&IncL(KDE): a hybrid time series prediction algorithm integrating dynamic ensemble pruning, incremental learning, and kernel density estimation	APPLIED INTELLIGENCE										Time series prediction (TSP); One-step-ahead prediction; Incremental learning (IL); Kernel density estimation (KDE); Dynamic ensemble pruning (DEP)	NEURAL-NETWORKS; MACHINE; COMPETITION; MODEL	Ensemble pruning can effectively overcome several shortcomings of the classical ensemble learning paradigm, such as the relatively high time and space complexity. However, each predictor has its own unique ability. One predictor may not perform well on some samples, but it will perform very well on other samples. Blindly underestimating the power of specific predictors is unreasonable. Choosing the best predictor set for each query sample is exactly what dynamic ensemble pruning techniques address. This paper proposes a hybrid Time Series Prediction (TSP) algorithm to implement one-step-ahead prediction task, integrating Dynamic Ensemble Pruning (DEP), Incremental Learning (IL), and Kernel Density Estimation (KDE), abbreviated as the EnsP(KDE)&IncL(KDE)algorithm. It dynamically selects proper predictor sets based on the kernel density distribution of all base learners' prediction values. Due to the characteristic of TSP problems that samples arrive in chronological order, the idea of IL is naturally introduced into EnsP(KDE)&IncL(KDE), while DEP is a common technology to address the concept drift issue inherent in IL. The algorithm is divided into three subprocesses: 1) Overproduction, which generates the original ensemble learning system; 2) Dynamic Ensemble Pruning (DEP), achieved by one subalgorithm called EnsP(KDE); 3) Incremental Learning (IL), realized by one subalgorithm termed IncL(KDE). Benefited from the advantages of integrating Dynamic Ensemble Pruning scheme, Incremental Learning paradigm and Kernel Density Estimation, in the experimental results, EnsP(KDE)&IncL(KDE)demonstrates superior prediction performance to several other state-of-the-art algorithms in fulfilling time series forecasting tasks.																	0924-669X	1573-7497															10.1007/s10489-020-01802-4		AUG 2020											
J								Surveying the Impact of Authentic and Pragmatic Marketing by Paradigm Shift on Brand Authenticity: A Case Study of Selected Islamic Azad Universities (Anzali, Tehran Markaz, and Tonekabon)	ADVANCES IN HUMAN-COMPUTER INTERACTION											FRAMEWORK	The contextual changes of the present age have altered the former order of personal and social relations in such a way that the creation of a new order is accompanied by an epistemic crisis; the crisis of knowledge of new relationships has rendered past valuations inefficient and invalid. In this regard, Kotler considers authenticity as the most influential element in this era of achieving sustainable development and customer trust, and since the production of university knowledge is the key to sustainable development, today it is faced with quality issues. This study aimed to investigate the effect of two marketing strategies, namely, authentic marketing and pragmatic marketing, through paradigm shifts as a solution to the epistemic crisis of brand authenticity. The research method of this study is based on quantitative and descriptive survey. The case study consists of Iranian students of Islamic Azad Universities (IAUs) stratified random sampling and sample size based on Morgan table were 385 people. Data were collected and distributed by questionnaire. The structural equation modeling technique with partial least squares approach and SmartPLS2 software were used for data analysis. The findings indicate that both authentic marketing and pragmatic marketing are more effective on brand authenticity through paradigm shift.																	1687-5893	1687-5907				AUG 21	2020	2020								4612589	10.1155/2020/4612589													
J								Spatio-temporal attention on manifold space for 3D human action recognition	APPLIED INTELLIGENCE										Skeleton-based; Action recognition; Spatial attention; Temporal attention; Manifold space	NETWORK; LSTM	Recently, skeleton-based action recognition has become increasingly prevalent in computer vision due to its wide range of applications, and many approaches have been proposed to address this task. Among these methods, manifold space is widely used to deal with the relative geometric relationships between different body parts in human skeletons. Existing studies treat all geometric relationships as having the same degree of importance; thus, they cannot focus on significant information. In addition, the traditional attention mechanism aims mostly to solve the attention problems in Euclidean space, and is not applicable in manifold space. To investigate these issues, we propose a spatial and temporal attention mechanism on Lie groups for 3D human action recognition. We build our network architecture with a generalized attention mechanism that extends the scope of attention from traditional Euclidean space to manifold space. In addition, our model can learn to identify the significant spatial features and temporal stages with effective attention modules, which focus on discriminative transformation relationships between different rigid bodies within each frame and allocate different levels of attention to different frames. Extensive experiments are conducted on standard datasets and the experimental results demonstrate the effectiveness of the proposed network architecture.																	0924-669X	1573-7497															10.1007/s10489-020-01803-3		AUG 2020											
J								Ergonomic Impact of Multi-rotor Unmanned Aerial Vehicle Noise in Warehouse Environments	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned Aerial Vehicle (UAV); Noise; Human subject; MANOVA	EXPOSURE; HEARING; COMMUNICATION; TECHNOLOGY; ANNOYANCE; MANOVA	Small multi-rotor unmanned aerial vehicles (UAVs) are poised to revolutionize commercial and logistics sectors through their versatility, maneuverability, and rapidly increasing sophistication and decreasing costs. However, these robotic systems also produce a substantial and overpowering level of acoustic noise that can potentially distract or harm humans who are working in close proximity to these UAVs. The aim of this study is to investigate the acoustic signature of quadcopter UAVs under various operating conditions, and its impact on human communication and psychological well-being. A unique design of experiments is developed allowing efficient usage of space and reducing the number of sessions required to complete the experiment. The human study has been conducted with popular UAV platforms in a workshop environment. Participants completed various listening tasks and their scores were compared with the noise signature of the UAV to identify operating factors with significant impact on human hearing and perceived annoyance.																	0921-0296	1573-0409															10.1007/s10846-020-01238-5		AUG 2020											
J								Deep transfer learning-based automated detection of COVID-19 from lung CT scan slices	APPLIED INTELLIGENCE										COVID-19; Transfer learning; Wavelets; CT scan; ResNet18		Lung abnormality is one of the common diseases in humans of all age group and this disease may arise due to various reasons. Recently, the lung infection due to SARS-CoV-2 has affected a larger human community globally, and due to its rapidity, the World-Health-Organisation (WHO) declared it as pandemic disease. The COVID-19 disease has adverse effects on the respiratory system, and the infection severity can be detected using a chosen imaging modality. In the proposed research work; the COVID-19 is detected using transfer learning from CT scan images decomposed to three-level using stationary wavelet. A three-phase detection model is proposed to improve the detection accuracy and the procedures are as follows; Phase1- data augmentation using stationary wavelets, Phase2- COVID-19 detection using pre-trained CNN model and Phase3- abnormality localization in CT scan images. This work has considered the well known pre-trained architectures, such as ResNet18, ResNet50, ResNet101, and SqueezeNet for the experimental evaluation. In this work, 70% of images are considered to train the network and 30% images are considered to validate the network. The performance of the considered architectures is evaluated by computing the common performance measures. The result of the experimental evaluation confirms that the ResNet18 pre-trained transfer learning-based model offered better classification accuracy (training = 99.82%, validation = 97.32%, and testing = 99.4%) on the considered image dataset compared with the alternatives.																	0924-669X	1573-7497															10.1007/s10489-020-01826-w		AUG 2020											
J								Exploring local spatial features in hyperspectral images	JOURNAL OF CHEMOMETRICS										gray-level co-occurrence matrix; hyperspectral images; multivariate image analysis; spatial features; wavelet transform	EXTRACTION; INFORMATION; RESOLUTION; QUALITY	We propose a methodological framework to extract spatial features in hyperspectral imaging data and establish a link between these features and the spectral regions, capturing the observed structural patterns. The proposed approach consists of five main steps: (i) two-dimensional stationary wavelet transform (2D-SWT) is applied to a hyperspectral data cube, decomposing each single-channel image with a selected wavelet filter up to the maximum decomposition level; (ii) a gray-level co-occurrence matrix is calculated for every 2D-SWT image resulting from stage (i); (iii) distinctive spatial features are determined by computing morphological descriptors from each gray-level co-occurrence matrix; (iv) the morphological descriptors are rearranged in a two-dimensional data array; and (v) this data matrix is subjected to principal component analysis (PCA) for exploring the variability of the aforementioned descriptors across spectral channels. As a result, groups of spectral wavelengths associated to specific spatial features can be pointed out yielding a better understanding and interpretation of the data. In principle, this information can also be further exploited, for example, to improve the separation of pure spectral profiles in a multivariate curve resolution context.																	0886-9383	1099-128X														e3295	10.1002/cem.3295		AUG 2020											
J								GraspFusionNet: a two-stage multi-parameter grasp detection network based on RGB-XYZ fusion in dense clutter	MACHINE VISION AND APPLICATIONS										Deep learning; Multi-parameter grasp; Grasp detection; RGB-XYZ fusion; Dense clutter	PICKING; LESSONS	Robotic grasping of diverse range of novel objects is a great challenge in dense clutter, which is also critical to many applications. However, current methods are vulnerable to perception uncertainty for dense stacked objects, resulting in limited accuracy of multi-parameter grasp prediction. In this paper, we propose a two-stage grasp detection pipeline including sampling and predicting stages. The first sampling stage applies fully convolutional network to generate grasp proposal regions, which contain potential graspable objects. Among grasp proposal region, the second prediction stage predicts complete grasp parameters based on fusion of RGB-XYZ heightmaps, which are converted from color and depth images. To perceive essential structures of stable grasping, 2D CNN and 3D CNN are used to learn color and geometric features to predict multi-parameter grasp, respectively. The direct mapping from heightmaps to grasp parameters is realized based on a multi-task loss. Experiments on a self-built dataset and an open dataset are conducted to analyze the network performance. The results indicate that the proposed two-stage method achieves the best performance among other grasp detection algorithms. Robotic experiments demonstrate generalization ability and robustness in dense clutter for novel objects, and the proposed method achieves average grasp success rate of 82.4%, which is also better than other state-of-the-art methods. Our self-built dataset and robotic grasping video are available at https://github.com/liuwenhai/toteGrasping.git.																	0932-8092	1432-1769				AUG 20	2020	31	7-8							58	10.1007/s00138-020-01108-y													
J								A Novel Basketball Result Prediction Model Using a Concurrent Neuro-Fuzzy System	APPLIED ARTIFICIAL INTELLIGENCE												Including uncertainties such as the performance of the teams, player performance indicators, and the quality of the competitors, there are numerous factors affecting the result of a game. Therefore, prediction of the game results is quite a complicated and a conspicuous research problem. Various artificial intelligence models were developed in order to solve this problem. By drawing together the advantageous sides of various artificial methods, this study aims to develop a hybrid intelligent system in order to better predict the result of a basketball game. Firstly, a prediction model was developed via artificial neural network (ANN), which is frequently used in game result predictions. The success of this developed ANN model in predicting the result of the game was 70.8%. In order to increase this success rate, a new concurrent neuro fuzzy system (CNFS) was suggested which was combined with fuzzy logic system that determined whether the team was favorite. The accurate prediction rate increased to 79.2% via this suggested CNFS model. Moreover, the results of the models developed were compared with each other and previous studies predicting the game results. As the conclusion of the comparisons, it was observed that CNFS model had a remarkable talent in predicting the game results.																	0883-9514	1087-6545				NOV 9	2020	34	13					1038	1054		10.1080/08839514.2020.1804229		AUG 2020											
J								A stable image steganography: a novel approach based on modified RSA algorithm and 2-4 least significant bit (LSB) technique	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cipher; Data security; Canny edge detection; LSB		Nowadays, due to the rapid technological growth of information technology billions of data transferred through the network around the world in a fraction of seconds. To protect the transferred information from unauthorized access, security mechanism is required. At this point, significant role is performed by a technology called information security. So, to provide security, various techniques have been evolved such as cryptography, steganography and digital watermarking. Cryptography converts the message into scrambled format whereas in steganography the secret message is buried in the digital media. In this paper, both cryptography and steganography is combined to safeguard the information. First, the message is encrypted with the help of revised RSA algorithm to convert the message into secret message. Second, the cover image is segmented into edge and non-edge pixels using Canny edge detection technique and then N1(two) and N2 (four) bits of the secret messages are buried into the edge and non-edge pixel areas using LSB technique. Finally, the essential parameters like N1, N2 and the length values are implanted into the last four pixels of the cover-image to obtain the stego-image. At the recipient side, the hidden data are extracted from the stego-image after extracting N1, N2 and length information. Then, the extracted secret data is decrypted using revised RSA algorithm. Modified RSA algorithm improves the security by using two pairs of keys. At last, the various performance metrics parameters such as entropy, MSE, peak signal to noise ratio and histogram are measured. It is observed that the proposed system attains improved efficiency, security and imperceptibility properties of information hiding from the results.																	1868-5137	1868-5145															10.1007/s12652-020-02398-w		AUG 2020											
J								Effective segmentation and classification of brain tumor using rough K means algorithm and multi kernel SVM in MR images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MR image; Brain tumor; Rough k means clustering; Support vector machine; Segmentation		From the classifications, an effective brain tumor classification and segmentation is the curious part for identifying the tumor and non-tumor cells in brain and the cell levels are evaluated. The brain tumor segmentation and classification is established on their experiences. The accuracy of tumor segmentation is very crucial to diagnosis accuracy. So, in our work we are align and improve an approach for tumor identification applying brain MR image segmentation. With an efficient, accurate and reproducible manner, the aim of our suggested method is to evaluate the tumor. Then the brain tumor is separated by using the effective techniques. For segmentation process, first the MRI image must be preprocessed. Next, the process of feature extraction is done by using preprocessed images. In feature extraction process, a raised Gabor wavelet transform (IGWT) is applied. In this research, the means of optimization technique is changed from the traditional Gabor wavelet transform. And the effectiveness of that optimization technique is aligned by using an oppositional fruit fly algorithm. At the end of the process, feature values are transferred in to the clustering process for segmentation. In this article we are introduced an algorithm called as rough k means clustering algorithm for segmentation. Here, we are applying an oppositional fruit fly algorithm to develop an effectiveness of the Gabor filter. Further to raise the classification accuracy of brain tumor we are introduced a multi kernel support vector machine algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02300-8		AUG 2020											
J								Significant directed walk framework to increase the accuracy of cancer classification using gene expression data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cancer classification; Microarray; Significant directed walk; Gene expression	IDENTIFICATION; REVEALS; NETWORK	Early diagnosis methods in cancer diagnosis studies are making great challenge as they require the involvement of different fields. Deoxyribonucleic acid (DNA) microarray analysis is one of the modern cancer diagnosis techniques used by scientists to measure the gene expression level changes in gene expression data. From the perspective of computing, an algorithm can be developed to identify more difficult cases. Numerous cancer studies have combined different machine learning techniques for the cancer diagnosis. This study is conducted to improve the cancer diagnosis technique, directed random walk (DRW) from the direction of framework. Improved directed random walk framework is proposed with the new introduced sub-algorithms, a larger directed graph and a different classifier. It is named as significant directed walk (SDW). In this study, six gene expression datasets are applied to study the effectiveness of the sub-algorithm, directed graph and classifier in SDW in terms of cancer prediction and cancer classification. Sub-algorithms of SDW can be further divided into data pre-processing phase, specific tuning parameter selection, weight as additional variable, and exclusion of unwanted adjacency matrix. Besides that, SDW also incorporated four directed graphs to study the usability of the directed graph. The best directed graph among the four is chosen to be part of the structure in SDW. The experimental results showed that the combination of SDW with walker network and linear regression is the best among all. SDW is achieves accuracy of 95.03% in average which is higher by 8.97% compare to conventional DRW for all cancer datasets. This study provides a foundation for further studies and research on early diagnosis of cancer with machine learning technique. It is found that these findings would improve the early diagnosis methods of cancer classification.																	1868-5137	1868-5145															10.1007/s12652-020-02404-1		AUG 2020											
J								Deep learning based facial expression recognition using improved Cat Swarm Optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image retrieval; Facial expression; Deep convolution neural network; Improved cat swarm optimization; Ensemble classifiers; SVM; Neural network		Human emotional facial expressions play a vital role in interpersonal relations. Automated facial expression recognition has always remained a challenging problem in real-life applications as people vary significantly in the way of showing their expressions. Recently various approaches have been proposed for automatically analyzing the facial expression of a person. In this paper, a novel approach to human facial expression recognition by applying a modified version of the Cat Swarm Optimization (CSO) algorithm, called Improved Cat Swarm Optimization (ICSO) algorithm is proposed. The input image given to the proposed system retrieves similar images from the dataset as well as identifies the person's emotional state through facial expressions. Deep features present in the face image are extracted using Deep Convolution Neural Network (DCNN) approach. ICSO is proposed to select optimal features from the face image that can uniquely distinguish the facial expression of a person. Employing DCNN with ICSO improves the retrieval performance of the proposed system. Ensemble classifiers that employ Neural Network (NN) and Support Vector Machine (SVM) are implemented to classify facial expressions such as normal, happy, sad, surprise, fear and angry. The performance of the proposed system is evaluated using JAFFE, CK+, Pie datasets and some real-world images. The proposed system outperforms the existing system, thus achieving superior accuracy and reduced computation time.																	1868-5137	1868-5145															10.1007/s12652-020-02463-4		AUG 2020											
J								TacticToe: Learning to Prove with Tactics	JOURNAL OF AUTOMATED REASONING										Machine learning; Tactic; Policy; Interactive theorem prover; HOL4; Tree search; Theorem; Formalization	LANGUAGE	We implement an automated tactical prover TacticToe on top of the HOL4 interactive theorem prover. TacticToe learns from human proofs which mathematical technique is suitable in each proof situation. This knowledge is then used in a Monte Carlo tree search algorithm to explore promising tactic-level proof paths. On a single CPU, with a time limit of 60 s, TacticToe proves 66.4% of the 7164 theorems in HOL4's standard library, whereas E prover with auto-schedule solves 34.5%. The success rate rises to 69.0% by combining the results of TacticToe and E prover.																	0168-7433	1573-0670															10.1007/s10817-020-09580-x		AUG 2020											
J								Optical character recognition with neural networks and post-correction with finite state methods	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										OCR; Historical periodicals; Finnish; Swedish		The optical character recognition (OCR) quality of the historical part of the Finnish newspaper and journal corpus is rather low for reliable search and scientific research on the OCRed data. The estimated character error rate (CER) of the corpus, achieved with commercial software, is between 8 and 13%. There have been earlier attempts to train high-quality OCR models with open-source software, like Ocropy (https://github.com/tmbdev/ocropy)) and Tesseract (https://github.com/tesseract-ocr/tesseract),), but so far, none of the methods have managed to successfully train a mixed model that recognizes all of the data in the corpus, which would be essential for an efficient re-OCRing of the corpus. The difficulty lies in the fact that the corpus is printed in the two main languages of Finland (Finnish and Swedish) and in two font families (Blackletter and Antiqua). In this paper, we explore the training of a variety of OCR models with deep neural networks (DNN). First, we find an optimal DNN for our data and, with additional training data, successfully train high-quality mixed-language models. Furthermore, we revisit the effect of confidence voting on the OCR results with different model combinations. Finally, we perform post-correction on the new OCR results and perform error analysis. The results show a significant boost in accuracy, resulting in 1.7% CER on the Finnish and 2.7% CER on the Swedish test set. The greatest accomplishment of the study is the successful training of one mixed language model for the entire corpus and finding a voting setup that further improves the results.																	1433-2833	1433-2825				DEC	2020	23	4					279	295		10.1007/s10032-020-00359-9		AUG 2020											
J								Spatial division networks for weakly supervised detection	NEURAL COMPUTING & APPLICATIONS										Deep learning; Learning systems; Convolutional neural networks; Predictive models	OBJECT	With only global image-level annotations, weakly supervised learning of deep convolutional neural networks has shown enough capacity in classification and localization but lack of ability to present the detection explicitly. In this work, we propose a novel spatial division network, which is applied to detect bounding boxes only with weak supervision. The essence of our model is two innovative differentiable modules, determination network and parameterized division, which perform the spatial division in feature maps of classification networks. After training, the learned parameters of the spatial division would correspond to a set of predicted bounding box coordinates. To demonstrate the effectiveness of our model for multi-label classification and weakly supervised detection, we conduct extensive experiments on the multi-MNIST dataset. Experimental results show our spatial division networks can (1) help improve the accuracy of multi-label classification, (2) implement in an end-to-end way only with the image-level annotations, and (3) output accurate bounding box coordinate, thereby achieving multi-digits detection.																	0941-0643	1433-3058															10.1007/s00521-020-05257-z		AUG 2020											
J								Adaptive inverse multilayer fuzzy control for uncertain nonlinear system optimizing with differential evolution algorithm	APPLIED INTELLIGENCE										Adaptive inverse multilayer T-S fuzzy controller (AIMFC); Uncertain nonlinear system; Hybrid adaptive optimal control; Differential evolution (DE) algorithm; Lyapunov stability principle; Spring-mass-damper (SMD) benchmark system; Coupled-liquid tank system	POWER POINT TRACKING; OPTIMIZATION ALGORITHM; DESIGN	This paper introduces a novel adaptive inverse multilayer T-S fuzzy controller (AIMFC) optimally identified with an optimization soft computing algorithm available for a class of robust control applied in uncertain nonlinear SISO systems. The parameters of multilayer T-S fuzzy model are optimally identified by the differential evolution (DE) algorithm to create offline the inverse nonlinear plant with uncertain coefficients. Then, the adaptive fuzzy-based sliding mode surface is applied to ensure that the closed-loop system is asymptotically stable in which the stability is satisfied using Lyapunov stability concept. The control quality of the proposed AIMFC algorithm is compared with the three recent advanced control algorithms applied in the Spring-Mass-Damper (SMD) benchmark system. Simulation and experiment results with different control parameters show that the proposed algorithm is better than the inverse fuzzy controller and the conventional adaptive fuzzy controller comparatively applied in both SMD system and the coupled-liquid tank system with the performance index using the least mean squares (LMS) error, which is investigated to demonstrate the efficiency and the robustness of the proposed AIMFC control approach.																	0924-669X	1573-7497															10.1007/s10489-020-01819-9		AUG 2020											
J								E-FCNN for tiny facial expression recognition	APPLIED INTELLIGENCE										Facial expression recognition; Super-resolution; Tiny face; Residual network; Edge extraction; Feedback networks	MACHINE	As a hot issue in recent years, facial expression recognition(FER) has been widely applied in many fields, but it still faces great challenges in tiny facial expression recognition. Currently, most of the FER networks only consider images of ideal sizes. Their recognition accuracy would significantly decrease as the image resolution decreases. However, images captured by surveillance cameras often contain tiny faces with low-resolution. This paper proposes an edge-aware feedback convolutional neural network(E-FCNN) for tiny FER, which associates image super-resolution and facial expression recognition together. To effectively leverage the texture information of faces, we design a novel three-stream super-resolution network, which is embedded with an edge-enhancement block as one branch. The other two branches are the up-sampling branch and SR(Super-Resolution) primary branch. Specifically, visual features are extracted from tiny images based on a hierarchical strategy, and then put into a feedback block with fused results of the three branches. Experiments are performed on down-sampled images in four facial expression datasets: CK+, FER2013, BU-3DFE, RAF-DB. The results demonstrate the favorable performance of our network.																	0924-669X	1573-7497															10.1007/s10489-020-01855-5		AUG 2020											
J								Electric Charged Particles Optimization and its application to the optimal design of a circular antenna array	ARTIFICIAL INTELLIGENCE REVIEW										Metaheuristics; Optimization; Electric charged particles; Circular antenna array	ALGORITHM	In this paper, a new metaheuristic, Electric Charged Particles Optimization (ECPO) algorithm, is developed. This algorithm is inspired by the interaction (forces exerted) between electric charged particles. It this algorithm not all the particles interact with each other, only selected ones. Then the way they interact with each other is defined by the selected strategy among the three available strategies. Therefore, there are several combinations possible between the number of interacting particles and strategies to find the most suitable one for the problem in hand which will help the algorithm to solve a wide range of optimization problems. The performance of the developed algorithm is first tested on the set of problems used for single objective real parameter algorithm competition that was held in the congress on evolutionary computation 2014. Then, the ECPO has been applied to optimal design of circular antenna array for sidelobe level reduction. The obtained results are then compared with other well-known metaheuristics using statistical tools. The analysis of the experimental results shows that the ECPO is a very competitive optimization algorithm.																	0269-2821	1573-7462															10.1007/s10462-020-09890-x		AUG 2020											
J								Joint feature and instance selection using manifold data criteria: application to image classification	ARTIFICIAL INTELLIGENCE REVIEW										Feature selection; Instance selection; Feature and instance selection; Data reduction; Linear discriminant analysis (LDA); Local discriminant embedding (LDE); Classification	SPARSITY PRESERVING PROJECTION; BANKRUPTCY PREDICTION; GENETIC ALGORITHMS; OPTIMIZATION; ENSEMBLES	In many pattern recognition applications feature selection and instance selection can be used as two data preprocessing methods that aim at reducing the computational cost of the learning process. Moreover, in some cases, feature subset selection can improve the classification performance. Feature selection and instance selection can be interesting since the choice of features and instances greatly influence the performance of the learnt models as well as their training costs. In the past, unifying both problems was carried out by solving a global optimization problem using meta-heuristics. This paradigm not only does not exploit the manifold structure of data but can be computationally expensive. To the best of our knowledge, the joint use of sparse modeling representative and feature subset relevance have not been exploited by the joint feature and selection methods. In this paper, we target the joint feature and instance selection by adopting feature subset relevance and sparse modeling representative selection. More precisely, we propose three schemes for the joint feature and instance selection. The first is a wrapper technique while the two remaining ones are filter approaches. In the filter approaches, the search process adopts a genetic algorithm in which the evaluation is mainly given by a score that quantify the goodness of the features and instances. An efficient instance selection technique is used and integrated in the search process in order to adapt the instances to the candidate feature subset. We evaluate the performance of the proposed schemes using image classification where classifiers are the nearest neighbor classifier and support vector machine classifier. The study is conducted on five public image datasets. These experiments show the superiority of the proposed schemes over various baselines. The results confirm that the filter approaches leads to promising improvement on classification accuracy when both feature selection and instance selection are adopted.																	0269-2821	1573-7462															10.1007/s10462-020-09889-4		AUG 2020											
J								ARP: asexual reproduction programming	CONNECTION SCIENCE										Meta-heuristic techniques; asexual reproduction optimisation; genetic programming; asexual reproduction programming; convergence	OPTIMIZATION; ALGORITHMS	Recently meta-heuristic techniques have attracted more attention. Algorithms based on Bio-inspired problems are among the most popular techniques of this field. In meta-heuristic algorithms, Genetic algorithm is one of the most useful. GA uses chromosome representation and operates on the chromosome with crossover and mutation operators. Genetic programming is a form of GA with tree representation for its chromosomes. GP was developed to evolve programming in computers and is a population-based algorithm. But GP is very slow and needs a long time for converging. On the other hand, asexual reproduction optimisation (ARO) is another variant of meta-heuristic algorithms in which convergence to the global optima is done at a fast time. In this paper, we introduced a new method, which is inducted by asexual reproduction with combination to GP. This algorithm is named Asexual Reproduction Programming (ARP). ARP has advantages of both ARO and GP together i.e. the fast convergence time of ARO and the power and flexibility of GP. ARP has fast convergence to global optimum while its error is less than GP. By mathematically analysing and proving, we show the ARP convergence to the global optimum. To assay the efficiency of the ARP, two algorithms were compared on some real-valued symbolic regression problems. Perusing the experimental results demonstrate that ARP outperforms GP in performance and convergence time.																	0954-0091	1360-0494															10.1080/09540091.2020.1807465		AUG 2020											
J								Fault diagnosis of biological systems using improved machine learning technique	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Cad System inE; coli(CSEC) process; Fault detection and isolation (FDI); Gaussian process regression (GPR); Partial GPR (PGPR); Generalized likelihood ratio test (GLRT)	LIKELIHOOD RATIO TEST; FEATURE-EXTRACTION; NEURAL-NETWORK; BIOPHYSICAL PARAMETERS; GAUSSIAN-PROCESSES; REGRESSION; CLASSIFICATION; PERFORMANCE; PREDICTION; MODEL	Fault detection and isolation (FDI) is considered as one of the most critical problems in biological processes. Therefore, in this paper, we consider a new FDI framework that aims to improve the monitoring of biological processes. To do that, a machine learning-based statistical hypothesis approach, which can identify the model, detect and isolate the faults, will be developed. In the developed approach, so-called partial Gaussian process regression (PGPR)-based generalized likelihood ratio test (GLRT), first, the GPR model that can accurately model biological processes is presented. Then, the fault detection phase is performed using the GLRT chart. Finally, the PGPR-based GLRT, which can effectively isolate the faults, is developed. The FDI performances of the developed PGPR-based GLRT approach are compared with partial support vector regression (SVR), extreme learning machines (ELM), Kernel ridge regression (KRR) and relevance vector machines (RVM)-based GLRT methods in terms of missed detection rate (MDR), false alarm rate (FAR), root mean square error (RMSE), execution time (ET) and isolation accuracy. The obtained results show that the proposed technique can reliably detect and isolate various faults using two examples: a synthetic data and a biological process representing a Cad System inE. coli(CSEC) model.																	1868-8071	1868-808X															10.1007/s13042-020-01184-6		AUG 2020											
J								Underspecification, Parsing Mismatches and Routinisation: The Historical Development of the Clitic Systems of Greek Dialects	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Dynamic Syntax; Diachronic change; Routinization; Parsing; production assymetries	CONVERSATION	In this paper, the historical development of the clitic systems of Standard Modern, Cypriot and Pontic Greek is discussed. These three varieties not only present the whole range of variation one can find across clitic systems in Greek but, furthermore, derive from a common linguistic ancestor, i.e. Koine Greek. This paper argues that the transition from Koine Greek to the Medieval varieties and from the Medieval varieties to the respective modern ones can be explained by making the assumption that routinisation (in the sense of Pickering and Garrod in Behav Brain Sci 27:169-226, 2004) and parsing/hearer assymetries are two important factors behind syntactic change. The claim is that the transition from Koine to the Medieval Greek varieties involves the emergence of a clitic system with encoded syntactic constraints out of a freer one, where clitic positioning was regulated by pragmatic preferences rather than syntactic constraints. Then, the transition to the modern varieties from the respective medieval ones is explained, at least partly, on the assumption that production/parsing mismatches are capable of triggering syntactic change. This last assumption combined with: (a) the tendency to obtain more generalised parsing triggers for parsing the individual clitics and (b) the fact that the Medieval varieties in question differ in minimal but crucial ways, provides us an explanation for the transition to the modern varieties.																	0925-8531	1572-9583															10.1007/s10849-020-09319-2		AUG 2020											
J								Multiple clustering and selecting algorithms with combining strategy for selective clustering ensemble	SOFT COMPUTING										Selective clustering ensemble; Clustering solution; Multiple clustering and selecting algorithms; Combining strategy	IMPROVE; MODEL	Clustering ensemble can overcome the instability of clustering and improve clustering performance. With the rapid development of clustering ensemble, we find that not all clustering solutions are effective in their final result. In this paper, we focus on selection strategy in selective clustering ensemble. We propose a multiple clustering and selecting approach (MCAS), which is based on different original clustering solutions. Furthermore, we present two combining strategies, direct combining and clustering combining, to combine the solutions selected by MCAS. These combining strategies combine results of MCAS and get a more refined subset of solutions, compared with traditional selective clustering ensemble algorithms and single clustering and selecting algorithms. Experimental results on UCI machine learning datasets show that the algorithm that uses multiple clustering and selecting algorithms with combining strategy performs well on most datasets and outperforms most selective clustering ensemble algorithms.																	1432-7643	1433-7479				OCT	2020	24	20					15129	15141		10.1007/s00500-020-05264-1		AUG 2020											
J								An improved image denoising technique using differential evolution-based salp swarm algorithm	SOFT COMPUTING										Denoising; Optimization; Salp swarm algorithm; Differential evolution; Mutation; Crossover; Visual quality analysis	CONTOURLET TRANSFORM; WAVELET; FILTER; NOISE; PARAMETERS	This paper proposes an improved denoising method based on the cascaded arrangement of filters. The different combinations of filters are obtained optimally through the improved performance of the salp swarm algorithm and cascading four filters out of twelve different types of filters. The searching ability of standard salp swarm algorithm is enhanced following the strategies in differential evolution, and hence the algorithm is named as differential evolution-based salp swarm algorithm (DESSA). Most of the existing image denoising algorithms are suitable to remove either Gaussian, Salt & Pepper, or Speckle noise. Alternatively, due to the optimal combination of filters in the cascaded arrangement, the proposed denoising method exhibits its effectiveness in the removal of all three noises and the denoised images are better in terms of both quantitative analysis and visual quality. The denoising performance of the proposed method is also tested on the mixed noise which demonstrates the significant improvements compared to state-of-the-art algorithms. Further, the experiment on CEC 2014 benchmark functions indicates that the proposed DESSA achieves better optimal solutions than existing algorithms.																	1432-7643	1433-7479															10.1007/s00500-020-05267-y		AUG 2020											
J								Joint set-up of parameters in genetic algorithms and the artificial bee colony algorithm: an approach for cultivation process modelling	SOFT COMPUTING										E; colifed-batch cultivation; Parameter identification; Genetic algorithm; Artificial bee colony; Parameter set-up; Crossover probability; Mutation probability; Population size; Limit parameter	TYPE-2 FUZZY-LOGIC; INTERCRITERIA ANALYSIS; GLOBAL OPTIMIZATION; CROSSOVER; IMPLEMENTATION; ADAPTATION; STRATEGIES; SOFTWARE; DESIGN	In this paper, aJoint set-upprocedure for tuning metaheuristic algorithms' parameters is proposed. The approach is applied to a genetic algorithm (GA) and tested further on the artificial bee colony (ABC) algorithm. The joint influence of parameters (the crossover and mutation probabilities for GA and the number of population andlimitfor ABC) on the performance of the algorithms is investigated. As a case study, a model parameter identification of anE. colifed-batch cultivation process is considered.E. coliis one of the most commonly used bacteria for producing medical substances in the pharmaceutical industry. The development of an effective model of a fed-batch cultivation process is very important. The processes in a bioreactor are usually described by a system of parametric nonlinear differential equations. The model parameter identification is a difficult optimization problem, which cannot be solved by applying traditional numerical methods. Feasibilities of GA and ABC for a model parameter identification of a nonlinear fed-batch cultivation process based on real experimental data are presented. The application of the proposedJoint set-upapproach leads to a significant improvement in the performance of GA and ABC. As a result, a reasonable enhancement of theE. colicultivation model accuracy is achieved. The main advantage of the tuning procedure, which searches an optimal set of values of GA and ABC control parameters, focusing on promising intervals of variation of the parameter values and refining their ranges, is that the computational efforts are reduced by more than 60% for the ABC algorithm and more than 90% for GA.																	1432-7643	1433-7479															10.1007/s00500-020-05272-1		AUG 2020											
J								Chemosensory aerosol assessment of key attributes for tobacco products	JOURNAL OF CHEMOMETRICS										chemometrics; chemosensory; cigarette; sensometrics; sensory analysis; tobacco	DESCRIPTIVE SENSORY ANALYSIS; REGRESSION; PREDICTION	Human sensory evaluation plays an important role in assessing quality and supports commercial product development in many industries. However, sensory evaluation of tobacco products often requires highly trained specialists, is thus expensive, time-consuming with low throughput capacity. To overcome such limitations, analytical platforms based on chemical fingerprints were proposed and evaluated in this work based on two different tobacco matrices. Using cigarette smoke as an example, the method was capable of predicting sensory attributes through chemical fingerprinting key tobacco and cigarette mainstream aerosol compositions. To achieve this, tobacco samples (comprising flue-cured Virginia and air-cured Burley types) and cigarette mainstream smoke samples were evaluated using high-resolution mass spectrometry. The chemical fingerprint of each sample matrix was related to its respective reference sensory attributes, validated by highly trained panellists, to create predictive models based on partial least squares algorithm. These methodologies were further validated through a blind test with suitable prediction of all sensory attributes for single grade tobacco leaf and commercial blended cigarette smoke. The proposed methodology demonstrated satisfactory accuracy, repeatability and robustness, with prediction errors less than 20% for single grade tobaccos, and less than 11% for commercial products. When compared with human sensory evaluation, it significantly improved analytical capacity (over 100 samples per day) at comparable or improved accuracy. This method could be applied to evaluate other novel tobacco products such as heated tobacco products.																	0886-9383	1099-128X														e3297	10.1002/cem.3297		AUG 2020											
J								The flexible and real-time commute trip sharing problems	CONSTRAINTS										Robust planning; Ride sharing; Scenario sampling; Vehicle routing problem	ALGORITHM; PICKUP	The Commute Trip Sharing Problem (CTSP) was introduced to remove parking pressure on cities, as well as corporate and university campuses. Its goal is to reduce the number of vehicles being used for daily commuting activities. Given a set of inbound and outbound requests, which consists of origin and destination pairs and their departure and return times, the CTSP assigns riders and a driver, as well as inbound and outbound routes, to each vehicle in order to satisfy time-window, capacity, and ride-duration constraints. The CTSP guarantees a ride back for each rider, which is a critical aspect of such a ride-sharing system. This paper generalizes the CTSP to account for uncertainties about the return trip. Each rider is assumed to have a return time specified by a distribution (learned from historical data) and, each day, a percentage of riders will want to preprone or postpone their return trip to accommodate some schedule changes. The paper proposes two generalizations of the CTSP: the Flexible CTSP (FCTSP) and the Real-Time CTSP (RT-CTSP). In the FCTSP, riders must confirm their final return times by a fixed deadline. In the RT-CTSP, riders confirm their new return times in real time with some prior notice. The paper proposes a two-step approach to address the FCTSP and the RT-CTSP. The first step uses a scenario-based stochastic program to choose the drivers and the morning routes in order to maximize the robustness of the driver assignment. The second step reoptimizes the plan at the fixed deadline or in real time once the return times are confirmed. Experiments on a real-world dataset of commute trips demonstrate the effectiveness of the algorithm in generating robust plans and reveal a trade-off between vehicle reduction and plan robustness as the robust plans tend to be conservative. A method is then proposed to evaluate this trade-off using the per-unit price ratio of vehicle increase to uncovered riders.																	1383-7133	1572-9354															10.1007/s10601-020-09310-5		AUG 2020											
J								A clinical support system for classification and prediction of depression using machine learning methods	COMPUTATIONAL INTELLIGENCE										classification; depression; healthcare; machine learning; prediction; random-forest		The health sector collects a very large amount of data, hence the diagnostic process processes a very large and varied amount of data type which makes the process of analyzing these data very complicated, specifically the healthcare sector, mental health is very composed and varied by various data criteria. However, the forecast of health in modern life becomes very important. To this end, the proposed work aims to analyze patient data based on their represented symptoms, in order to help clinicians and mental health practitioners classify and refine the type of depression disorder "characterized" in patients intelligently, in order to make a relevant decision. In this context, the proposed system called CP-DDC is based on machine learning algorithms supervised more precisely by the random-forest algorithm. The dataset used in the case study contains 150 instances and 11 attributes, which define the different patient criteria, obtained from the Mohammed VI University Hospital Center of Marrakech "CHU." The results of the experiment show that the proposed system offers the highest performance.																	0824-7935	1467-8640															10.1111/coin.12377		AUG 2020											
J								Analysing terminology translation errors in statistical and neural machine translation	MACHINE TRANSLATION										Terminology translation; Machine translation; Phrase-based statistical machine translation; Neural machine translation		Terminology translation plays a critical role in domain-specific machine translation (MT). Phrase-based statistical MT (PB-SMT) has been the dominant approach to MT for the past 30 years, both in academia and industry. Neural MT (NMT), an end-to-end learning approach to MT, is steadily taking the place of PB-SMT. In this paper, we conduct comparative qualitative evaluation and comprehensive error analysis on terminology translation in PB-SMT and NMT in two translation directions: English-to-Hindi and Hindi-to-English. To the best of our knowledge, there is no gold standard available for evaluating terminology translation quality in MT. For this reason we select an evaluation test set from a legal domain corpus and create a gold standard for evaluating terminology translation in MT. We also propose an error typology taking the terminology translation errors in MT into consideration. We translate sentences of the test set with our MT systems and terminology translations are manually classified as per the error typology. We evaluate the MT system's performance on terminology translation, and demonstrate our findings, unraveling strengths, weaknesses, and similarities of PB-SMT and NMT in the area of term translation.																	0922-6567	1573-0573				SEP	2020	34	2-3					149	195		10.1007/s10590-020-09251-z		AUG 2020											
J								A new framework for sign language alphabet hand posture recognition using geometrical features through artificial neural network (part 1)	NEURAL COMPUTING & APPLICATIONS										Sign language alphabet; Hand posture recognition; Depth-based geometrical sign language; Geometrical features sign language		Hand pose tracking is essential in sign languages. An automatic recognition of performed hand signs facilitates a number of applications, especially for people with speech impairment to communication with normal people. This framework which is called ASLNN proposes a new hand posture recognition technique for the American sign language alphabet based on the neural network which works on the geometrical feature extraction of hands. A user's hand is captured by a three-dimensional depth-based sensor camera; consequently, the hand is segmented according to the depth analysis features. The proposed system is called depth-based geometrical sign language recognition as named DGSLR. The DGSLR adopted in easier hand segmentation approach, which is further used in segmentation applications. The proposed geometrical feature extraction framework improves the accuracy of recognition due to unchangeable features against hand orientation compared to discrete cosine transform and moment invariant. The findings of the iterations demonstrate the combination of the extracted features resulted to improved accuracy rates. Then, an artificial neural network is used to drive desired outcomes. ASLNN is proficient to hand posture recognition and provides accuracy up to 96.78% which will be discussed on the additional paper of this authors in this journal.																	0941-0643	1433-3058															10.1007/s00521-020-05279-7		AUG 2020											
J								Emotion cause detection with enhanced-representation attention convolutional-context network	SOFT COMPUTING										Emotion cause detection; Enhanced-Representation; Synonyms; Attention; Context; Bi-LSTM; CNN	CLASSIFICATION	Emotion cause detection is mainly to identify the emotion cause with an emotion expression text, which plays a critical role in building NLP applications. This task is much more difficult than other emotion classification and emotion extraction problems. However, most existing methods only focus on partial information to extract the emotion cause. In this study, we present a new approach to combine the emotion word with its synonyms in order to discover the deep and semantic information by enhanced-representation and attention mechanism. Meanwhile, we propose a new mechanism to introduce the hierarchical context behind emotion word information for extracting emotion cause inspired by a convolution operation. Our proposed framework can extract both enhanced represented emotion level features and context level features to better detect the emotion cause. We have conducted extensive experiments on the emotion cause dataset. Experimental results demonstrate the effectiveness of our proposed model, outperforming a number of competitive baselines by at least 3.39% in F1-measure.																	1432-7643	1433-7479															10.1007/s00500-020-05223-w		AUG 2020											
J								Visual analysis framework for network abnormal data based on multi-agent model	SOFT COMPUTING										Network data; Intrusion detection; Visual analysis; Multi-agent model; Intelligent algorithm; Collaboration	BIG DATA	With the development of Internet, the amount of data increases exponentially. With the development of network technology, human beings have entered the era of big data. Therefore, it is very important to find valuable information from massive network data. At present, most of the network security products in the market record the system test results in the form of logs. Users can view and analyze the log information one by one to find out the suspicious behavior and carry out the next diagnosis. Finally, they can defend the identified attacks. Intrusion detection system usually stores detection results in the form of alert files. There may be a large number of redundant or false alarm information in the alarm file, so the heavy cognitive burden of users has become one of the disadvantages of traditional intrusion detection system. In consideration of the above situation, visualization technology has been introduced into the field of network security by researchers. The network visualization analysis displays the massive network data and log information in the way of graph and image, and uses the developed human vision to process the massive graphical data. Data visualization is a method of transforming abstract symbols into concrete geometric figures, which presents the results of simulation and calculation in the form of figures and images. In this paper, multi-agent model is used to build a network data visualization analysis model. The experimental results show that the method in this paper can effectively visualize a large number of network data, and the experimental results are easy to understand.																	1432-7643	1433-7479															10.1007/s00500-020-05257-0		AUG 2020											
J								Soft clustering by convex electoral model	SOFT COMPUTING										Soft clustering; Fuzzy clustering; C-means; Polynomial-time complexity bounds; Electoral models	FUZZY	In this paper, we suggest a new technique for soft clustering of multidimensional data. It is based on a new convex voting model, where each voter chooses a party with certain probability depending on the divergence between his/her preferences and the position of the party. The parties can react on the results of polls by changing their positions. We prove that under some natural assumptions this system has a unique fixed point, providing a unique solution for soft clustering. The solution of our model can be found either by imitation of the sequential elections, or by direct minimization of a convex potential function. In both cases, the methods converge linearly to the solution. We provide our methods with worst-case complexity bounds. To the best of our knowledge, these are the first polynomial-time complexity results in this field.																	1432-7643	1433-7479															10.1007/s00500-020-05148-4		AUG 2020											
J								Piecewise linear bounding functions in univariate global optimization	SOFT COMPUTING										Univariate global optimization; Piecewise linear functions; Estimators; Deterministic methods	MINIMIZATION; CONSTRAINTS; MODELS; CONVEX	The paper addresses the problem of constructing lower and upper estimators for univariate functions. This problem is of crucial importance in global optimization, where such bounds are used to reduce the search area. We propose to use piecewise linear estimators for bounding univariate functions and show how such estimators can be derived from the function's algebraic expression. The basic properties of such estimators are formulated and proved. We implemented the algorithms for the automated construction of lower and upper piecewise linear estimators and experimentally compared the proposed approach with the first-order interval bounds, Pijavskij method, and slope arithmetic. Numerical examples demonstrate that the piecewise linear estimators are more accurate with respect to the mentioned approaches. We also show that global optimization algorithms can significantly benefit from using piecewise linear estimators. Another advantage of the proposed approach is that the objective function does not have to be differentiable. This feature can favorably distinguish this method from other methods where the first and second derivatives are used.																	1432-7643	1433-7479															10.1007/s00500-020-05254-3		AUG 2020											
J								Deep learning based energy efficient novel scheduling algorithms for body-fog-cloud in smart hospital	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things (IoT); BAN (body area networks); Fog architecture; WORN-DEAR; L-NO-DEAF	AWARE	Recent innovative development in Internet of Things, usage of wearable devices in body area networks has become smarter and has reached new perception, in terms of connectivity and diagnosis. Energy consumption, latency and network coverage are some of the research issues occurred in IoT based body area network. To address latency issue, in this work, networks could adopt to the Fog architectures to perform computation, data analysis and storage near to the users. To improve battery life period of sensor nodes an intelligent proactive routing algorithms for body-fog-cloud area networks are needed. In this research a novel algorithm called as modified WORN-DEAR algorithms for BAN-IoT networks is proposed to achieve energy efficient routing and scheduling using the principle of deep learning based adaptive distance-energy features. This work is simulated on Cooja-Contiki network simulator and implemented on different test beds with ESP8266 WIFI SoC interface. Final results were compared with existing WORN-DEAR algorithm and achieved higher accuracy of 98% in LSTM compare to other machine learning algorithms such as logistic regression, naive bias, SVM and KNN.																	1868-5137	1868-5145															10.1007/s12652-020-02421-0		AUG 2020											
J								ANN and fuzzy based household energy consumption prediction with high accuracy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ARIMA; Fuzzy time series; ANN; SVR; Energy consumption	SUPPORT VECTOR MACHINES; TIME-SERIES; FORECASTING TECHNIQUES; BOX	Timeline Data is gathered according to different time intervals, which are day after day after week or month after month, for updating properties and rationing institutional resilience it is important to consider the usage of systems and lead to reduced lifespan Such details illustrate the use of the system as well as its interaction with time, like day, week, month and time of year, and the relation between the equipment and a relative, essential factors for the effects of the usage of their potency and the expected movement by customers. This is because it is not significant to determine the various relations between specific devices utilizing concurrent data. In addition, precise relations between interval-based instances in which specific system usage continue for certain duration cannot be calculated. To address these difficulties, we propose supervised energy time series data clustering and frequent pattern mining analysis as well as a Bayesian network forecast for energy use. However, the AI model is a univariate construct based on past use-values. Neural Networks have the favored position that can estimate nonlinear limits. Everything together they have an approximate usage of vitality, the ANN adds in a planning knowledge table between the use of vitality (EC) and its determinants. SVM is capable of reliably calculating knowledge on time structure while the basic system mechanism is frequently nonlinear and not set. Also, certain nonlinear mechanisms such as multilayer perceptron have been shown to flank SVM. The single data has been converted into a multivariate and the ANFIS has been selected as it transmits both the AI (ANN) and Fuzzy Inference Method (FIS) points of concern. ANFIS yields the accuracy, RMSE, and MAPE among genuine and anticipated power utilization of 91.19%, 0.4076 and 0.9049 which is moderately low.																	1868-5137	1868-5145															10.1007/s12652-020-02455-4		AUG 2020											
J								Combination of contrast enhanced fuzzy c-means (CEFCM) clustering and pixel based voxel mapping technique (PBVMT) for three dimensional brain tumour detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Axial plane; Magnetic resonance; 3D space; Fuzzy c-means; Accuracy; Sensitivity	SEGMENTATION; ALGORITHM	Brain tumour is a lump of tissue produced by uncontrolled growth of cells in the human brain. Automated and accurate detection of brain tumour is important for robotics based surgery operation. The proposed method enhances the accuracy, sensitivity and specificity of automated brain tumour detection by adopting an accurate two phase method of detection. The method segments the Axial plane slices of Magnetic Resonance image in its first phase. In the second phase, binary decision values representing the presence or absence of tumor cells on each pixel locations are projected into 3D space to obtain the 3D tumor. Contrast Enhanced fuzzy c-means (CEFCM) clustering method is used to segment the 2D tumor regions from MR image slices due to its high accuracy. The decision values obtained from the segmented image for each pixel locations are mapped into 3D space using Pixel based voxel mapping technique (PBVMT). Average accuracy (Dice overlap coefficient) and average sensitivity of detection are measured with respect to the given ground truth of the image of BRATS 2013 dataset. The overall accuracy, sensitivity and specificity of detection are found to be 0.948, 92.14% and 96.97% respectively.																	1868-5137	1868-5145															10.1007/s12652-020-02366-4		AUG 2020											
J								Threshold based DDoS mitigation with fog layer in cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed denial of service; Fog computing; Third-party auditor; Privacy identifies forming; Sequence integrity data algorithm		Security is an important to process our data in public network. Most of the have the problem of distributed denial of service attacks, the cloud also a witness of facing distributed denial of service attack. The malicious user generates the malicious request to the service provided towards spoiling the performance of the cloud environment. The throughput of any cloud service is upon successful completion of service invocation. If the malicious user initiates the service if is their service without completing, it would be considered as a distributed denial of service attack. Even though there exists the number of trust verification measures enforced like TPA (third-party auditor) in the cloud environment, there is no sufficient improvement for the performance of the cloud. We propose a sequence integrity data algorithm (SIDA) to perform continuous workload scheduling for various attributes with addition privacy identifies forming (PIF-SIDA) algorithm is to enhance the data are split based on domain and stored in cloud storage. From this, the data are shown based on the domain in which the user searches these are implemented the current research trends, and to help stakeholders in the industry the developing field of edge computing. Verifying the identity of the user is not enough to safeguard the service performance. Distributed environment depends the storage security system by verifying the authenticity and auditing to improve the security.																	1868-5137	1868-5145															10.1007/s12652-020-02369-1		AUG 2020											
J								SLA-aware load balancing using risk management framework in cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing; Load balancing; Service level agreement (SLA); SLA-violation; SLA-aware risk management framework (SA-RMF)	MACHINE; ALLOCATION; PLACEMENT; VIOLATION	In cloud computing environment, load balancing is an important task. So many of the researchers had focused on load balanced scheduling technique. Those provides better load balancing in cloud but there are some issues like resource allocation and cost maintenance. One of the major issue in load balancing techniques is service level agreement (SLA) management because many of them are affected by this SLA-violation. Many researchers have proposed various risk based framework but few of them has guides the service provider to take steps for SLA violation abatement and they also need some improvements. To tackle this problem, a new SLA-aware risk management framework (SA-RMF) is proposed in this work for efficient load balancing in cloud. A new technique is presented here based on CPU parameter for generating efficient dynamic threshold. A better quality of service values prediction is achieved by using hybrid approach in SA-RMF. Through experiments, the appropriateness of proposed system is demonstrated and validated that it helps cloud providers to mitigate future violations of services and consequences.																	1868-5137	1868-5145															10.1007/s12652-020-02458-1		AUG 2020											
J								Fundus image lesion detection algorithm for diabetic retinopathy screening	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diabetic retinopathy; Hemorrhages; Micro-aneurysms; Exudates; Luminosity and contrast enhancement	AUTOMATIC DETECTION; RETINAL IMAGES; MICROANEURYSMS; PREVALENCE	An eye disease that damages the retina of diabetic patients is known as diabetic retinopathy (DR). The severity of the disease is found by different lesions such as hemorrhages, microaneurysms, exudates etc., these are the early stage symptoms of non-proliferative DR for early analysis of DR. A single framework for instinctive Lesion Detection used for diagnosis of the disease easily by screening is proposed. It consists of four steps: luminosity and contrast enhancement, removal of extracted blood vessels and optic disc (OD), lesion detection and classification based on lesions. Gamma correction and CLAHE for luminosity and contrast enhancement. Principle component analysis for vessel extraction and using convex hull transform for OD detection. After background subtraction, lesions are detected using morphological operations and classification based on count of lesions. The proposed algorithm is analyzed using the publically available datasets and evaluated using the metrics of specificity, sensitivity and accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02417-w		AUG 2020											
J								Drift compensation of commercial water quality sensors using machine learning to extend the calibration lifetime	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-sensor system; Drift correction; Calibration; Machine learning; Artificial neural network	ELECTRONIC TONGUE; TEMPERATURE; CLASSIFICATION; NETWORKS	There are specific issues in the multi-sensor systems used for water quality monitoring, which prevents these systems for routine measurement of water samples. An important issue is drift; related to sensor readings, which may refute the calibration of sensors leads to the necessity of frequent recalibration of the sensors that required effort as well as shut down the system. An alternative approach for drift correction is based on the mathematical correction method. The paper proposed a regression calibration method and implemented by the machine learning approach. In this paper, we have used a feed-forward artificial neural network based regression model to extend the calibration lifetime of sensors. The evaluation of the model was performed based on the root mean square error and the root mean square error for cross-validation. The proposed model is also compared with the traditional statistical method and proved to be superior to the traditional one. The experimental results demonstrate the best performance with a negligible error rate. Based on the results of the current study, ANN appears to be more adaptive for data analysis in environmental monitoring applications.																	1868-5137	1868-5145															10.1007/s12652-020-02469-y		AUG 2020											
J								Hybrid model for tasks scheduling in distributed real time system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Scheduling; Fuzzy c-means clustering; Genetic algorithm; Branch and bound method; Response time; System reliability	MAXIMIZING RELIABILITY; ALLOCATION ALGORITHMS; GENETIC-ALGORITHM; ASSIGNMENT; OPTIMIZATION	Resource allocation and their scheduling to optimize performance measures in heterogeneous environments are famous such as an NP-hard issue, not only for the resource heterogeneity, but also for the possibility of applying allocation to take advantage of idle resource. This article proposes a scheduling technique for communicating tasks by using two hybrid genetic algorithms (HGAs) which minimizes system cost and response time and maximizes the reliability of the distributed real time system. In the present technique, convergence of genetic algorithm (GA) is made better by offering new encoding and population initialization method and genetic operations. This technique is completed in two phases: Phase I develops hybrid c-mean genetic algorithm (HCMGA) which is a fusion of fuzzy c-means (FCM) technique and genetic algorithm (GA) and Phase II develops hybrid branch and bound genetic algorithm (HBBGA) which is a fusion of branch and bound (B&B) technique and genetic algorithm (GA). HCMGA, makes 's' clusters of 'r' tasks by using FCM clustering technique then these clusters are updated by using GA to get final clusters of tasks. HBBGA, initially allocates clusters of tasks onto processors by B&B technique then their allocations are updated by using GA to get the final allocation. To check the performance of the proposed technique, several examples are considered from different research articles and results of the numerical examples have compared with well-regard existing models. The proposed technique is able to outperform all comparative techniques established in the literature; thus, superior results are obtained. This technique is suitable for arbitrary number of processors and tasks.																	1868-5137	1868-5145															10.1007/s12652-020-02445-6		AUG 2020											
J								Validation of four designed motion features for recognizing computer user activities	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Motion features; Intelligent human-computer interaction; Human activity recognition; Artificial intelligence; RNN; LSTM; GRU	EYES	Toward the age of ambient intelligence, in which contactless devices are widely applied to recognize human states. This study aims at designing critical motion features to build artificial intelligence (AI) models for identifying user activities in front of the computer. Eight participants were recruited in the study to perform four daily computer activities, including playing games, surfing the web, typing words, and watching videos. While performing the experimental tasks, the participants' upper body were videotaped, and the recorded videos were processed to obtain four designed features, comprising (1) the eye-opening size, (2) the mouth-opening size, (3) the number of optical-fence pixels, and (4) the standard deviation of optical-fence pixels. After feature importance confirmation, these obtained motion features were used to establish three recurrent neural network (RNN) models using simple RNN, gated recurrent unit (GRU), and long short-term memory (LSTM). The comparison of the model predictions showed that the GRU model had the best performance (accuracy = 76%), compared to the Simple RNN model (accuracy = 59%) and the LSTM model (accuracy = 70%). This study showed that the four tested computer activities had significant effects on the four designed features, and hence the features could be applied to build AI models for recognizing activities in front of a computer. Limitations are discussed for directing future studies in extending the methodology to other applications.																	1868-5137	1868-5145															10.1007/s12652-020-02479-w		AUG 2020											
J								Automatic generation control with fuzzy logic controller incorporating tandem and cross compound turbine	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Load frequency control (LFC); Automatic generation control (AGC); Static synchronous series compensator (SSSC); Redox flow battery (RFB); Governor dead band (GDB); Generation rate constraint (GRC); Fuzzy logic controller (FLC)	MAGNETIC ENERGY-STORAGE	This dissertation characterize the Automatic Generation Control (AGC) knit together with tandem reheat and cross reheat thermal system wielding Proportional Integral (PI) and Fuzzy Logic Controller (FLC). Multi area thermal system is designed with non-linearities such as governor dead band in addition with generation rate constraint. This study is come up with a knowledgeable application of Redox flow batteries (RFB) correlative along Static Synchronous Series Compensator. In order to enhance dynamic reaction of the load frequency control, redox flow battery has been appended in couple of the units as a result of being active rejoinder and nether time persistent. Ziegler Nichols method is wielded to regulate the PI controllers. During enormous load distraction within areas, conventional control methods alone would be ineffective in diminishing frequency deflection and power of tie-line swinging as a result of medium reaction by speed governor process. In order prevail this hindrance; fuzzy logic controller has been executed within the plant. Simulation studies affirm the efficacy of the fuzzy logic controller, depends upon the parameters like overshoots, under shoots and settling time, hence AGC functioning during abnormality in power system has been ameliorated. System is also studied for hydro thermal power system with simulation outcome.																	1868-5137	1868-5145															10.1007/s12652-020-02372-6		AUG 2020											
J								Computational intelligence on image classification methods for microscopic image data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image classification; SEM image; Random forest; Confusion matrix; Decision tree	RANDOM FOREST CLASSIFICATION	Technology play a substantial function in exemplifying technical potentials that are unconsidered in the past to include intelligent behaviour. Computers in machine learning involve the modelling of intelligent behaviour with the least human mediation. Computational intelligence signifies a set of vital information processing methods for information managing and decision accomplishing. Major critical trials in technical exploration is the supervision of data obtained from various resources. Presently, Scanning Electron Microscopy (SEM) images are categorized as per user's measures, which greatly restrict misuse and reliability, owing for the need in definite classification algorithm. The foremost objective is to evaluate the exactness with each algorithm in terms of performance measures such as accuracy, precision, sensitivity, F-measure, kappa, computational time and confusion matrix. In this work, a recital assessment among three classification algorithms on SEM image dataset was conducted. Experimental results revealed that the Random forest algorithm gives the highest accuracy comparing to other two approach. Hence, the researchers would be able to perform automatic SEM image classification evading the necessity to categorize image and for providing a searchable database to identify a definite group of images.																	1868-5137	1868-5145															10.1007/s12652-020-02406-z		AUG 2020											
J								Improving spectral efficiency and low latency in 5G framework utilizing FD-MIMO	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										FD-MIMO; Dynamic device-specific single downtilt beam-forming (DD-SSDB); Signal to interference ratio (SIR); Incremental redundancy shifting; HARQ; Regression monitoring; NS3		Multiple input-multiple output (MIMO), massive MIMO, full-dimension MIMO (FD-MIMO) are the most recent remote advancements, which structure the purpose of takeoff for long term evolution systems. FD-MIMO is the promising technology proposed in the Third Generation Partnership Project to the transformation for the fifth-generation wireless communication schemes. FD-MIMO tramples several practical challenges imposed by massive MIMO systems such as interference, hardware limitations by the use of 2D active antenna array system which provides beam-forming in both azimuths as well as elevation domains in three-dimensional space. The traditional construction of big-scale planar arrays maximizes the spatial correlation is addressed using the dynamic device specific single down tilt beam-forming which minimizes the signal to interference ratio. That is further reduced by using FD-MIMO with incremental Redundancy and Regression monitoring techniques formulated as soft combining strategies. In this paper, the simulation results show maximizing of the minimum signal-to-interference ratio, spectral efficiency improvement and minimizing packet transmission time between eNodeB. Also, user equipment utilizing incremental redundancy shifting, hybrid automatic repeat request and regression monitoring strategies using network simulator 3.																	1868-5137	1868-5145															10.1007/s12652-020-02341-z		AUG 2020											
J								Deep CNN framework for retinal disease diagnosis using optical coherence tomography images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Convolutional neural networks; Retinal optical; Coherence tomography; Denoising	CLASSIFICATION	Accurate and robust diagnosis of retinal diseases using OCT imaging is considered an essential part for clinical utility. We propose a deep learning based, a fully automated diagnosis system for detecting retinal disorders namely, Drusen macular degeneration (DMD) and diabetic macular edema (DME) using optical coherence tomography (OCT) Images. If it is not diagnosed and treated, these degenerative abnormalities may result in moderate to severe vision loss. Early detection of these diseases reduces the risk of further complications and expedites the treatment process. We propose a deep convolutional neural network (CNN) framework for the diagnosis and classification into Normal, DMD and DME effectively. First, despeckling of the input OCT images is performed using the Kuan filter to remove inherent speckle noise. Further, the CNN network is tuned using hyperparameter optimization techniques. Additionally, K-fold validation is performed to ensure complete usage of the dataset. We evaluate the proposed model with number of performance metrics using Mendeley database consisting of labelled OCT images. The resulting classification accuracy of the proposed model is 95.7%. Further, an authoritative study is performed between the pre-trained models and proposed framework using the acquired performance metrics to demonstrate the efficacy of our model.																	1868-5137	1868-5145															10.1007/s12652-020-02460-7		AUG 2020											
J								Multi objective dragonfly algorithm for congestion management in deregulated power systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Congestion; Generator rescheduling; System operator (SO); Congestion management (CM); Voltage security margin (lambda); Corrected transient energy margin (CTEM); Multi objective dragonfly Algorithm(MODA); Fuzzy decision maker	PARTICLE SWARM OPTIMIZATION; TRANSIENT STABILITY; VOLTAGE; GENERATORS; REAL	Congestion in transmission corridors are the major bother for deregulated power system's operation. Generator rescheduling along with demand alteration is a traditional remedy for transmission line congestion. According to market clearing process, the system operator (SO) has to pay a certain amount of cost to the market participants for rescheduling the generation and demand. This kind of redispatch related congestion management (CM) procedure is mainly carried out to reduce the congestion cost, but they are failing to provide an attention in power systems security. The risky generator's power shifts may diminish the voltage and transient stability of the power system. So power system security should be included in the congestion management procedure. In this proposed multi objective congestion management procedure, rescheduling of active power is carried out to improve/retain the power systems security along with a congestion cost reduction. Voltage security margin (lambda) and corrected transient energy margin (CTEM) provides a measure for power system security level. Multi Objective Dragonfly Algorithm (MODA) is employed to trace the non dominated solutions for three conflicting objectives. Fuzzy decision making principle is applied to select the best Pareto solution depends on the objective's significances. The goodness of the MODA optimization approaches is experimented in congestion alleviation of New England 39 bus systems and solutions are compared with some reputed methods.																	1868-5137	1868-5145															10.1007/s12652-020-02440-x		AUG 2020											
J								An intelligent Context Based Multi-layered Bayesian Inferential predictive analytic framework for classifying machine states	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MisMatch negativity (MMN); Predictive coding; Bayes theorem; Condition monitoring; Rotating machinery	BEARING FAULT-DIAGNOSIS; MISMATCH NEGATIVITY; NETWORK	Proactive fault diagnosis is a burning issue in condition monitoring of machines. Intelligent methods prove to be promising solutions for designing predictive analytic frameworks for fault diagnosis and machine state classification. The competency of machine learning algorithms in handling large voluminous data has marked them as a natural solution for developing intelligent framework that proactively classifies the machine states. The paper proposes a novel Context Based Multi-layered Bayesian Inferential (CBMBI) predictive analytic framework, which is motivated by MisMatch Negativity (MMN) and Predictive Coding. The CBMBI framework is augmented with a new hyperparameter (context) that greatly reduces the misclassification rate. The performance of the framework is analysed with Case Western Reserve University 6205-2RS JEM SKF dataset. The profound results reveal that the proposed framework shows 97% accuracy and 94% F1-score which is relatively higher than the state of art technique.																	1868-5137	1868-5145															10.1007/s12652-020-02411-2		AUG 2020											
J								HERDE-MSNB: a predictive security architecture for IoT health cloud system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; Security; Prediction; Homomorphic; Naive bayes		The application of IoT in the several fields is extending due to its simple and robust functionality. Especially, with the advent of the IoT-cloud based devices, IoT are established in the field that process very high amount of data. The health care system is one of the emerging applications of the IoT-Cloud. Many research works are carried out in ensuring the privacy of the patient data. The main issues in the IoT-cloud based health system remains on the security of data along with computation overheads. Predicting disease using patient data from the IoT device is another demanding aspect of health systems. In this research work, a novel Homomorphic Encryption with Random Diagonal Elliptical curve cryptography integrated with Multi-nomial smoothing Naive Bayes (HERDE-MSNB) is proposed to provide the effective security and predict the disease over patient data in the IoT Health Cloud system. The cryptic framework in the proposed architecture involves the encryption and decryption of the patient data along with key words through the HERDE algorithm. The Medicinal person deciphers the encrypted data and performs the prediction through the MSNB model. The UCI repository dataset is employed to predict the performance of the security and prediction model. From the analysis it is observed that the proposed architecture is effective in providing security and disease prediction than the existing models with less processing time, computational cost and increased accuracy. The future work may include all aspects of the dataset with robust prediction model.																	1868-5137	1868-5145															10.1007/s12652-020-02408-x		AUG 2020											
J								Satellites, war, climate change, and the environment: are we at risk for environmental deskilling?	AI & SOCIETY										Remote sensing; Warfighting; Phenomenology; Multistability; Epistemological perspectivalism; Hermeneutics; Moral deskilling		Currently, we find ourselves in a paradigm in which we believe that accepting climate change data will lead to a kind of automatic action toward the preservation of our environment. I have argued elsewhere (Fried 2020) that this lack of civic action on climate data is significant when placed in the historical, military context of the technologies that collect this data--Earth remote sensing technologies. However, I have not yet discussed the phenomenological or moral implications of this context, which are deeply interconnected. In this paper, I assert that Earth remote sensing technologies can, if we are not careful, lead us to a kind of environmental deskilling. This assertion comes in four parts. First, the military context of Earth remote sensing technologies--which collect important data on climate change--acts as a kind of stability, as defined by Don Ihde and others. Second, I invoke Sir Patrick Heelan to argue that the theoretical underpinnings of Earth systems science do not translate from military to environmental praxes as we imagine they do. Third, Hannah Arendt makes the case that a state's trust in simplifying narratives like that of climate data, meant to create "islands" of certainty in an uncertain world, can be self-defeating. That is to say, they can silence public action. I extend these arguments through Vallor's analysis of moral deskilling, in which she points out that an overemphasis on autonomous data collection--and trust in a kind of automated decision-making on that data--can deskill us from important questions relevant to our collective flourishing. In all of these examples, the lines between environmental and military research are blurry.																	0951-5666	1435-5655															10.1007/s00146-020-01047-2		AUG 2020											
J								A multi-valued and sequential-labeled decision tree method for recommending sequential patterns in cold-start situations	APPLIED INTELLIGENCE										Classification; Sequential pattern mining; Personalized recommendation; Data mining; Machine learning	MARKOV MODEL; CLASSIFICATION; ONLINE	We plan to recommend some initial suitable single-itemed sequences like a flight itinerary based on a preference pattern in the form of personalized sequential pattern to each cold-start user. However, sequential pattern mining has never treated a conventional sequential pattern as a personalized pattern. Besides, as a cold-start user lacks the personalized sequential pattern, collaborative filtering cannot recommend one any single-itemed sequences. Thus, we first design such a preference pattern, namely representative sequential pattern, which reflects one's main frequently recurring buying behavior mined from the item-sequences during a time period. After sampling a training-set from non-cold-start users who prefer similar items, we propose an auxiliary algorithm to mine the representative sequential pattern as the sequential class labels of each training instance. A multi-label classifier seems therefore be trained to predict the sequential-label for each cold-start user based on one's features. However, most multi-label classification methods are designed to classify data whose class labels are non-sequential. Besides, some of the predictor attributes would be multi-valued in the real world. Aiming to handle such data, we have developed a novel algorithm, named MSDT (Multi-valued and Sequential-labeled Decision Tree). Experimental results indicate it outperforms all the baseline multi-label algorithms in accuracy even if three of them are deep learning algorithms.																	0924-669X	1573-7497															10.1007/s10489-020-01806-0		AUG 2020											
J								A hybrid algorithm for the university course timetabling problem using the improved parallel genetic algorithm and local search	APPLIED INTELLIGENCE										Genetic algorithm; Local search; University course timetabling problem; Distance to feasibility	NONLINEAR GREAT DELUGE; NEIGHBORHOOD SEARCH; CONSTRUCTION	Scheduling is one of the problems that has attracted the attention of many researchers over the years. The University Course Timetabling Problem (UCTP) is a highly constrained real-world combinatorial optimization task. Designing course timetables for academic institutions has always been challenging, because it is a non-deterministic polynomial-time hardness (NP-hard) problem. This problem attempts to assign specific timeslots and rooms to the events considering a number of hard and soft constraints. All hard constraints must be satisfied to achieve a feasible solution, whereas satisfying all soft constraints is not necessary. Although the quality of the solution is directly related to the number of soft constraints that are satisfied. One of the recent innovative methodologies for solving UCTP is the hybrid algorithm, which attempts to automate the timetabling design process so that it would be able to work with different instances of problem domains. In this paper, we present a hybrid method based on the Improved Parallel Genetic Algorithm and Local Search (IPGALS) to solve the course timetabling problem. The Local Search (LS) approach is used to strengthen the Genetic Algorithm (GA). The IPGALS has applied a representation of the timetable, which ensure the hard constraints would never be violated. Hard constraints are measured by Distance to Feasibility (DF) criterion. In fact, applying the DF criterion leads to achieving feasible solutions and promotes the performance of our algorithm. Due to the wide range of problem constraints, the proposed algorithm is performed in parallel to improve the GA searching process. The IPGALS algorithm is tested over BenPaechter and ITC-2007 standard benchmarks and compared with the state-of-the-art techniques in this literature. The experimental results confirm the effectiveness and the superiority of the proposed algorithm compared to other prominent methods for solving UCTP.																	0924-669X	1573-7497															10.1007/s10489-020-01833-x		AUG 2020											
J								Autoencoder-based unsupervised clustering and hashing	APPLIED INTELLIGENCE										Information retrieval; Unsupervised hashing; Deep learning; Clustering	IMAGE; QUANTIZATION; ALGORITHM	Faced with a large amount of data and high-dimensional data information in a database, the existing exact nearest neighbor retrieval methods cannot obtain ideal retrieval results within an acceptable retrieval time. Therefore, researchers have begun to focus on approximate nearest neighbor retrieval. Recently, the hashing-based approximate nearest neighbor retrieval method has attracted increasing attention because of its small storage space and high retrieval efficiency. The development of neural networks has also promoted progress in hash learning. However, these methods are mostly supervised. In practical applications, annotating large amounts of data is a very time-consuming and laborious task. Furthermore, efficiently using a large amount of unlabeled data for hash learning is challenging. In this paper, we create a new autoencoder variant to efficiently capture the features of high-dimensional data, and propose an unsupervised deep hashing method for large-scale data retrieval, named as Autoencoder-based Unsupervised Clustering and Hashing (AUCH). By constructing a hashing layer as a hidden layer of the autoencoder, hash learning is performed together with unsupervised clustering by minimizing the overall loss. AUCH can unify unsupervised clustering and retrieval tasks into a single learning model. In addition, the method can use a deep neural network to simultaneously learn feature representations, hashing functions and cluster assignments. Experimental results on standard datasets indicate that AUCH achieves competitive results compared to state-of-the-art models for retrieval and clustering tasks.																	0924-669X	1573-7497															10.1007/s10489-020-01797-y		AUG 2020											
J								Deep learning techniques for rating prediction: a survey of the state-of-the-art	ARTIFICIAL INTELLIGENCE REVIEW										Recommender systems; Rating prediction; Deep learning; Approaches; Survey	RECOMMENDER SYSTEMS; MODEL; SENTIMENT; NETWORK; USER; SIMILARITY; PREFERENCES; REVIEWS; MACHINE	With the growth of online information, varying personalization drifts and volatile behaviors of internet users, recommender systems are effective tools for information filtering to overcome the information overload problem. Recommender systems utilize rating prediction approaches i.e. predicting the rating that a user will give to a particular item, to generate ranked lists of items according to the preferences of each user in order to make personalized recommendations. Although previous recommendation systems are effective in creating attired recommendations, however, they still suffer from different types of challenges such as accuracy, scalability, cold-start, and data sparsity. In the last few years, deep learning has attained substantial interest in various research areas such as computer vision, speech recognition, and natural language processing. Deep learning based approaches are vigorous in not only performance improvement but also to feature representations learning from the scratch. The impact of deep learning is also prevalent, recently validating its efficacy on information retrieval and recommender systems research. In this study, a comprehensive review of deep learning-based rating prediction approaches is provided to help out new researchers interested in the subject. More concretely, the classification of deep learning-based recommendation/rating prediction models is provided and articulated along with an extensive summary of the state-of-the-art. Lastly, new trends are exposited with new perspectives pertaining to this novel and exciting development of the field.																	0269-2821	1573-7462															10.1007/s10462-020-09892-9		AUG 2020											
J								Automatic Arabic Text Summarization Using Analogical Proportions	COGNITIVE COMPUTATION										Arabic text summarization; Extractive summarization; Analogical proportions; Analogical relevance	CLASSIFIERS	Automatic text summarization is the process of generating or extracting a brief representation of an input text. There are several algorithms for extractive summarization in the literature tested by using English and other languages datasets; however, only few extractive Arabic summarizers exist due to the lack of large collection in Arabic language. This paper proposes and assesses new extractive single-document summarization approaches based on analogical proportions which are statements of the form "ais tobascis tod". The goal is to study the capability of analogical proportions to represent the relationship between documents and their corresponding summaries. For this purpose, we suggest two algorithms to quantify the relevance/irrelevance of an extracted keyword from the input text, to build its summary. In the first algorithm, the analogical proportion representing this relationship is limited to check the existence/non-existence of the keyword in any document or summary in a binary way without considering keyword frequency in the text, whereas the analogical proportion of the second algorithm considers this frequency. We have assessed and compared these two algorithms with some language-independent summarizers (LexRank, TextRank, Luhn and LSA (Latent Semantic Analysis)) using our large corpus ANT (Arabic News Texts) and a small test collection EASC (Essex Arabic Summaries Corpus) by computing ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (BiLingual Evaluation Understudy) metrics. The best-achieved results are ROUGE-1 = 0.96 and BLEU-1 = 0.65 corresponding to educational documents from EASC collection which outperform the best LexRank algorithm. The proposed algorithms are also compared with three other Arabic extractive summarizers, using EASC collection, and show better results in terms of ROUGE-1 = 0.75 and BLEU-1 = 0.47 for the first algorithm, and ROUGE-1 = 0.74 and BLEU-1 = 0.49 for the second one. Experimental results show the interest of analogical proportions for text summarization. In particular, analogical summarizers significantly outperform three among four language-independent summarizers in the case of BLEU-1 for ANT collection and they are not significantly outperformed by any other summarizer in the case of EASC collection.																	1866-9956	1866-9964				SEP	2020	12	5					1043	1069		10.1007/s12559-020-09748-y		AUG 2020											
J								An extensive experimental evaluation of automated machine learning methods for recommending classification algorithms	EVOLUTIONARY INTELLIGENCE										Evolutionary algorithms; Algorithm recommendation; Automated machine learning; Classification; Meta-learning	DECISION-TREE ALGORITHMS; EVOLUTIONARY; COMPLEXITY; DESIGN	This paper presents an experimental comparison among four automated machine learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on evolutionary algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the combined algorithm selection and hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level.																	1864-5909	1864-5917															10.1007/s12065-020-00463-z		AUG 2020											
J								GADE: A Generative Adversarial Approach to Density Estimation and its Applications	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative models; GANs; Flow-based generative models; Deep learning	INFERENCE	Density estimation is a challenging unsupervised learning problem. Current maximum likelihood approaches for density estimation are either restrictive or incapable of producing high-quality samples. On the other hand, likelihood-free models such as generative adversarial networks, produce sharp samples without a density model. The lack of a density estimate limits the applications to which the sampled data can be put, however. We propose agenerative adversarial density estimator(GADE), a density estimation approach that bridges the gap between the two. Allowing for a prior on the parameters of the model, we extend our density estimator to a Bayesian model where we can leverage the predictive variance to measure our confidence in the likelihood. Our experiments on challenging applications such as visual dialog or autonomous driving where the density and the confidence in predictions are crucial shows the effectiveness of our approach.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2731	2743		10.1007/s11263-020-01360-9		AUG 2020											
J								Recursive Context Routing for Object Detection	INTERNATIONAL JOURNAL OF COMPUTER VISION										Object detection; Context modeling; Computer vision; Deep learning	IMPLICIT; IDENTIFICATION; INFORMATION; VISION; SCENES	Recent studies have confirmed that modeling contexts is important for object detection. However, current context modeling approaches still have limited expressive capacity and dynamics to encode contextual relationships and model contexts, deteriorating their effectiveness. In this paper, we instead seek to recast the current context modeling framework and perform more dynamic context modeling for object detection. In particular, we devise a novel Recursive Context Routing (ReCoR) mechanism to encode contextual relationships and model contexts more effectively. The ReCoR progressively models more contexts through a recursive structure, providing a more feasible and more comprehensive method to utilize complicated contexts and contextual relationships. For each recursive stage, we further decompose the modeling of contexts and contextual relationships into a spatial modeling process and a channel-wise modeling process, avoiding the need for exhaustive modeling of all the potential pair-wise contextual relationships with more dynamics in a single pass. The spatial modeling process focuses on spatial contexts and gradually involves more spatial contexts according to the recursive architecture. In the channel-wise modeling process, we introduce a context routing algorithm to improve the efficacy of modeling channel-wise contextual relationships dynamically. We perform a comprehensive evaluation of the proposed ReCoR on the popular MS COCO dataset and PASCAL VOC dataset. The effectiveness of the ReCoR can be validated on both datasets according to the consistent performance gains of applying our method on different baseline object detectors. For example, on MS COCO dataset, our approach can respectively deliver around 10% relative improvements for a Mask RCNN detector on the bounding box task, and 7% relative improvements on the instance segmentation task, surpassing existing context modeling approaches with a great margin. State-of-the-art detection performance can also be accessed by applying the ReCoR on the Cascade Mask RCNN detector, illustrating the great benefits of our method for improving context modeling and object detection.																	0920-5691	1573-1405															10.1007/s11263-020-01370-7		AUG 2020											
J								Improved VGG model-based efficient traffic sign recognition for safe driving in 5G scenarios	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Traffic sign recognition; Deep learning; Convolutional neural network; GTSRB		The rapid development and application of AI in intelligent transportation systems has widely impacted daily life. The application of an intelligent visual aid for traffic sign information recognition can provide assistance and even control vehicles to ensure safe driving. The field of autonomous driving is booming, and great progress has been made. Many traffic sign recognition algorithms based on convolutional neural networks (CNNs) have been proposed because of the fast execution and high recognition rate of CNNs. However, this work addresses a challenging question in the autonomous driving field: how can traffic signs be recognized in real time and accurately? The proposed method designs an improved VGG convolutional neural network and has significantly superior performance compared with existing schemes. First, some redundant convolutional layers are removed efficiently from the VGG-16 network, and the number of parameters is greatly reduced to further optimize the overall architecture and accelerate calculation. Furthermore, the BN (batch normalization) layer and GAP (global average pooling) layer are added to the network to improve the accuracy without increasing the number of parameters. The proposed method needs only 1.15 M when using the improved VGG-16 network. Finally, extensive experiments on the German Traffic Sign Recognition Benchmark (GTSRB) Dataset are performed to evaluate our proposed scheme. Compared with traditional methods, our scheme significantly improves recognition accuracy while maintaining good real-time performance.																	1868-8071	1868-808X															10.1007/s13042-020-01185-5		AUG 2020											
J								A survey of 5G network systems: challenges and machine learning approaches	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										5G cellular network; 5G services; 5G key technologies; 5G architectures; 5G challenges; ML solutions; Intelligence; SON	TO-DEVICE COMMUNICATIONS; BIG DATA ANALYTICS; OF-THE-ART; ARTIFICIAL-INTELLIGENCE; RESOURCE-ALLOCATION; CELLULAR NETWORKS; FUNCTION VIRTUALIZATION; ENABLING TECHNOLOGIES; WIRELESS NETWORKS; ANOMALY DETECTION	5G cellular networks are expected to be the key infrastructure to deliver the emerging services. These services bring new requirements and challenges that obstruct the desired goal of forthcoming networks. Mobile operators are rethinking their network design to provide more flexible, dynamic, cost-effective and intelligent solutions. This paper starts with describing the background of the 5G wireless networks then we give a deep insight into a set of 5G challenges and research opportunities for machine learning (ML) techniques to manage these challenges. The first part of the paper is devoted to overview the fifth-generation of cellular networks, explaining its requirements as well as its key technologies, their challenges and its forthcoming architecture. The second part is devoted to present a basic overview of ML techniques that are nowadays applied to cellular networks. The last part discusses the most important related works which propose ML solutions in order to overcome 5G challenges.																	1868-8071	1868-808X															10.1007/s13042-020-01178-4		AUG 2020											
J								Clinical quantitative information recognition and entity-quantity association from Chinese electronic medical records	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Clinical quantitative information; Information extraction; Chinese clinical text; Deep learning; Machine learning	EXTRACTION SYSTEM; TEXT; QUALITY	Clinical quantitative information contains crucial measurable expressions of patients' diseases and treatment conditions, which are commonly exist in free-text electronic medical records. Although the clinical quantitative information is of considerable significance in assisting the analysis of health care, few researches have yet focused on the topic and it remains an ongoing challenge. Focusing on Chinese electronic medical records, this paper proposed an extended Bi-LSTM-CRF model, which integrated domain knowledge information and position characteristics of quantitative information as external features to improve the effectiveness of clinical quantitative information recognition. In addition, to associate the extracted entities and quantities more effectively, this paper presented an automatic approach for entity-quantity association using machine learning strategy. Based on 1359 actual Chinese electronic medical records from burn department of a domestic public hospital, we compared our model with a number of widely-used baseline methods. The evaluation results showed that our model outperformed the baselines with an F1-measure of 94.27% for quantitative information recognition and an accuracy of 94.60% for entity-quantity association, demonstrating its effectiveness.																	1868-8071	1868-808X															10.1007/s13042-020-01160-0		AUG 2020											
J								On the designing a secure biometric-based remote patient authentication scheme for mobile healthcare environments	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										RFID; Security analysis; Authentication; Healthcare; Biometric	LIGHTWEIGHT; CRYPTANALYSIS; PROTOCOL	Internet of medical things (IoMT) is bringing many opportunities for healthcare and our personal lives. For example, using this technology a healthcare provider can remotely monitor, collect and analyze data of patients using smart sensors that are connected to them. With this trend on the rise, data protection and information security in healthcare environments are now major concerns. Authentication before starting the data transmission is a common approach to provide data security. Recently, Mohammedi et al. have proposed a lightweight biometric-based authentication scheme for mobile healthcare environments and have claimed that their scheme is secure against known attacks in the context of RFID authentication protocols. However, in this paper, we provide a more detailed analysis of the this scheme and show that their protocol is vulnerable to a man-in-the-middle attack. Furthermore, we demonstrate that their protocol does not provide other security requirements such as forward secrecy, anonymity, and untraceability. To remedy these weaknesses, we propose an improved scheme and demonstrate that the proposed scheme can withstand common attacks while it requires approximately 23% less computation time and 50% less communication overhead than the Mohammediet al.scheme. We also formally evaluate the security of the proposed protocol by Scyther tool, which is a widely accepted automated tool for this purpose.																	1868-5137	1868-5145															10.1007/s12652-020-02465-2		AUG 2020											
J								A new Fuzzy C-Means and AHP-based three-phased approach for multiple criteria ABC inventory classification	JOURNAL OF INTELLIGENT MANUFACTURING										ABC analysis; Analytical hierarchy process (AHP); Fuzzy C-Means (FCM) algorithm; Multi-Criteria Inventory Classification (MCIC)	ARTIFICIAL NEURAL-NETWORK; SUPPLIERS	ABC analysis is an efficient and easy-to-use methodology to classify inventory based on a single or multi-criteria basis that may consist of thousands of items. The first study by Dickie (Fact Manag Maint 109(7):92-94, 1951), based on a single criterion, is considered to be limited now. New studies focus on Multi-Criteria-Inventory Classification (MCIC) since such an extension of the criteria fits the realities of modern business decisions. The proposed approach in this study uses three-phased MCIC incorporating analytical hierarchy process (AHP), Fuzzy C-Means (FCM) algorithm, and a newly proposed Revised-Veto (Rveto) phase to meet the ABC classification principles and increase its applicability and flexibility. This new approach is called AHP-FCM-Rveto and proposed in this study for the first time. A numerical example taken from the literature is used to compare AHP-FCM-Rveto with other methods, and the results also show that the proposed methodology performs better. In the real-life example, the main advantage of compliance with the Pareto principle of the proposed method is shown.																	0956-5515	1572-8145															10.1007/s10845-020-01633-7		AUG 2020											
J								Mechanism and Position Tracking Control of a Robotic Manipulator Actuated by the Tendon-Sheath	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Tendon-sheath transmission; Robotic manipulator; Position tracking; Fuzzy PID control; Time-delay estimation	COMPENSATION CONTROL; TRANSMISSION MODEL; DISTAL-END; EXOSKELETON; DESIGN; SYSTEM; FORCE	To achieve a manipulator that is lighter and more anthropomorphic, a 7-DOF robotic manipulator driven by the tendon-sheath is designed. Its mechanical system and control system are both introduced in this paper. An encoder is installed on both the driver side and the joint side to realize double closed-loop control. Considering the transmission characteristics of the reducer and the tendon-sheath, a position transmission model of a single joint is constructed. Furthermore, the correctness of the transmission model is verified based on the prototype joint four. A proportional-integral-differential (PID) controller and fuzzy PID controller with time-delay estimation (TDE) are designed based on double encoders, and some experiments on the position tracking control are carried out for the robotic joints. The experimental results show that each joint can rotate normally and there is no interference between the different tendon-sheaths. When the drive motor moves to follow the position command of the sinusoidal signal, the TDE-based fuzzy PID controller can reduce the maximum tracking error compared with the traditional PID controller. According to the experimental results, it is feasible to apply the tendon-sheath transmission to the manipulator, which provides a reference for the development of other equipment based on the tendon-sheath.																	0921-0296	1573-0409															10.1007/s10846-020-01245-6		AUG 2020											
J								Building an expert system for printer forensics: A new printer identification model based on niching genetic algorithm	EXPERT SYSTEMS										feature selection; forensics-based expert system; genetic algorithm; KNN; printer identification; texture analysis	DIGITAL FORENSICS; SELECTION	Inside digital forensic science, expert systems are utilized to clarify suspicions where normally one or more human experts would need to be consulted. Expert systems-based printer identification is provided with the objective of distinguishing the printer that produced a suspicious or questioned document. The arising problem is that the extraction of many features of the printed document for printer forensics sometimes increases the time and decreases the classification accuracy as many of the printed document descriptors may emanate to be recurring and non-valuable. Therefore, the distinct combinatorial collection of features (knowledge base) will demand to be acquired in order to preserve the essence of operative features' fusion to accomplish the maximum accuracy. This paper presents a bio-inspired expert system for printer forensics that integrates both texture features conveyed from the grey level co-occurrence matrix of the printed letter 'WOO' and niching genetic search to select the good enough reduced feature set. This combination intends to realize high classification precision relies on a trivial collection of discriminative descriptors. Niching methods extend genetic algorithms to domains that require the location and maintenance of multiple solutions based on adjusting the crossover ratio and occurrence of mutation of each individual and employs the slope of the individuals to choose their mutation value. For categorization, the scheme exploits k-nearest neighbours (KNN) to distinguish the brand of the printer for its simplicity. Results confirm that the suggested approach has high classification accuracy and needs less computation time.																	0266-4720	1468-0394														e12624	10.1111/exsy.12624		AUG 2020											
J								A deep learning approach for specular highlight removal from transmissive materials	EXPERT SYSTEMS										Inpainting; partial convolution; semantic masking; specular highlight; transmissive material	COLOR; SEPARATION	The appearance of specular highlights in images is one main factor affecting accurate material or object recognition tasks. Such an appearance has a misleading effect on the true gradient information found in transmissive material images. Certain methods use specular highlights as an intrinsic feature of transparency to detect transparent objects. However, this process reduces the robustness of methods in applications with opaque and shiny materials and in the classification of tasks among related features, such as transparency and translucency. Thus, correcting this artefact can enhance texture- or gradient-based image and video analyses. However, the correction of small or large regions with specular highlights from transmissive materials, such as glass, plastic and water, is complex and ambiguous. These materials are sensitive to specular highlights and exhibit high degrees of reflection. In this study, we propose a deep learning framework to address the problem. A partial convolution-based inpainting method is integrated with automatic semantic mask generation by using a simple adaptive binarization to detect highlight spots during training and inference. The proposed framework improves the learning process by capturing the semantic nature of specular highlights. Moreover, the framework eliminates the use of image-mask pairs during inference and avoids predefined irregular random mask training. We qualitatively and quantitatively evaluate the proposed framework by using new and existing publicly available datasets that contain specular images. Experimental results show that our framework registers competitive performance and considerably reduces computational time.																	0266-4720	1468-0394														e12598	10.1111/exsy.12598		AUG 2020											
J								Multiclass machine learning classification of functional brain images for Parkinson's disease stage prediction	STATISTICAL ANALYSIS AND DATA MINING										deep neural network; functional brain image; machine learning; supervised classification	AUTOMATIC CLASSIFICATION; SHAPE-ANALYSIS; SPECT; DIAGNOSIS; TOMOGRAPHY	We analyzed a data set containing functional brain images from 6 healthy controls and 196 individuals with Parkinson's disease (PD), who were divided into five stages according to illness severity. The goal was to predict patients' PD illness stages by using their functional brain images. We employed the following prediction approaches: multivariate statistical methods (linear discriminant analysis, support vector machine, decision tree, and multilayer perceptron [MLP]), ensemble learning models (random forest [RF] and adaptive boosting), and deep convolutional neural network (CNN). For statistical and ensemble models, various feature extraction approaches (principal component analysis [PCA], multilinear PCA, intensity summary statistics [IStat], and Laws' texture energy measure) were employed to extract features, the synthetic minority over-sampling technique was used to address imbalanced data, and the optimal combination of hyperparameters was found using a grid search. For CNN modeling, we applied an image augmentation technique to increase and balance data sizes over different disease stages. We adopted transfer learning to incorporate pretrained VGG16 weights and architecture into the model fitting, and we also tested a state-of-the-art machine learning model that could automatically generate an optimal neural architecture. We found that IStat consistently outperformed other feature extraction approaches. MLP and RF were the analytic approaches with the highest prediction accuracy rate for multivariate statistical and ensemble learning models, respectively. Overall, the deep CNN model with pretrained VGG16 weights and architecture outperformed other approaches; it captured critical features from imaging, effectively distinguished between normal controls and patients with PD, and achieved the highest classification accuracy.																	1932-1864	1932-1872				OCT	2020	13	5					508	523		10.1002/sam.11480		AUG 2020											
J								Surface pattern-enhanced relation extraction with global constraints	KNOWLEDGE AND INFORMATION SYSTEMS										Relation classification; Surface pattern; Deep learning; Self-attention; Global constraints		Relation extraction is one of the most important tasks in information extraction. The traditional works either use sentences or surface patterns (i.e., the shortest dependency paths of sentences) to build extraction models. Intuitively, the integration of these two kinds of methods will further obtain more robust and effective extraction models, which is, however, ignored in most of the existing works. In this paper, we aim to learn the embeddings of surface patterns to further augment the sentence-based models. To achieve this purpose, we propose a novel pattern embedding learning framework with the weighted multi-dimensional attention mechanism. To suppress noise in the training dataset, we mine the global statistics between patterns and relations and introduce two kinds of prior knowledge to guide the pattern embedding learning. Based on the learned embeddings, we present two augmentation strategies to improve the existing relation extraction models. We conduct extensive experiments on two popular datasets (i.e., NYT and KnowledgeNet) and observe promising performance improvements.																	0219-1377	0219-3116															10.1007/s10115-020-01502-y		AUG 2020											
J								An efficient, dense and long-range marker system for the guidance of the visually impaired	MACHINE VISION AND APPLICATIONS										Computer vision; Mobile vision; Visual markers; Visually impaired		In this paper, we address the problem of making a mobile/smartphone camera sensitive to distant fiducial markers. To this end, we carefully design a novel visual marker that is both dense and readable from large distances. The main novelty of the proposed marker is the combination of a quaternary color-based coding system with robust methods for reading the color patterns included in each frame once it is detected. These patterns include a CRC whose length grows linearly, whereas that of the message grows quadratically. Our experiments show that the proposed bundle marker-vision algorithm outperforms the alternatives in terms of distance and angle and also that it is very robust to changes in lighting conditions, thus making it a good intelligent system for guiding people with visual impairments in their day to day use of public transportation systems.																	0932-8092	1432-1769				AUG 18	2020	31	7-8							57	10.1007/s00138-020-01097-y													
J								Drone culture: perspectives on autonomy and anonymity	AI & SOCIETY										Drone; UAV; Culture; Parallax; Simulation; Assemblage; Relational; Socio-technical	SIMULATION; WAR	This article addresses the problematic perspectives of drone culture. In critiquing focus on the drone's apparent 'autonomy', it argues that such devices function as part of a socio-technical network. They are relational parts of human-machine interaction that, in our changing geopolitical realities, have a powerful influence on politics, reputation and warfare. Drawing on zizek's conception of parallax, the article stresses the importance of culture and perception in forming the role of the drone in widening power asymmetries. It examines how perceptions of autonomy are evoked by drones, to claim that this misperception is a smokescreen that obscures the relational socio-technical realities of the drone. The article therefore argues that a more critical culture of the drone emerges by shifting the focus and perception from autonomy to anonymity. This allows us to engage more fully with the distributed agency and decision-making that define how drones are developed and deployed. Rather than focusing on the drone as a singular, fetishised, technical object, a relational approach to the drone assemblage is proposed that highlights the competing human interests that define and resist drones in global politics and culture.																	0951-5666	1435-5655															10.1007/s00146-020-01042-7		AUG 2020											
J								Guidance systems: from autonomous directives to legalsensor-bilities	AI & SOCIETY										Sensors; Behavioural biometrics; Legal personhood; Autonomous vehicles; Robotics; Algorithms	PEOPLE	The design of collaborative robotics, such as driver-assisted operations, engineer a potential automation of decision-making predicated on unobtrusive data gathering of human users. This form of 'somatic surveillance' (Hayles, Unthought: the power of the cognitive nonconscious. University of Chicago Press, Chicago, 2017, p. 11) increasingly relies on behavioural biometrics and sensory algorithms to verify the physiology of bodies in cabin interiors. Such processes secure cyber-physical space, but also register user capabilities for control that yield data as insured risk. In this technical re-formation of human-machine interactions for control and communication 'a dissonance of attribution' (Hancock et al., Proc Natl Acad Sci 116(16):7684, 2019. https://doi.org/10.1073/pnas.1805770115) is created between perceptions of phenomena, materials and decision-making. This reconfigures relations not only between humans and machines, objects and subjects, but possibly disrupts attributive functions in the social system of Law. What it requires is shifting a legal accountability for action from a sovereignty of the human to a new materialist account based on a 'cognitive assemblage' between physiological data, computation and algorithmic sensing. This paper investigates the function of law as a guidance system to acknowledge this account of sensory and algorithmic computation as autonomous 'sensing agents' (Hansen, Feed-forward: on the future of twenty-first-century media. University of Chicago Press, Chicago, 2015) that may be accountable in situations of risk. This assemblage of robotic computation and sensory determination requires a clearer legal differentiation across the current static terminologies of person, property, liability and rights that maintain strict separations of object from subject. To neglect this, we argue, law will solely impute attributions of error to humans despite evidence of operation via mutual control.																	0951-5666	1435-5655															10.1007/s00146-020-01012-z		AUG 2020											
J								Empirical research of accounting conservatism, corporate governance and stock price collapse risk based on panel data model	CONNECTION SCIENCE										Accounting conservatism; corporate governance; stock price collapse risk; mixed utility model; substitution effect		This paper selects the panel data of the main board listed companies in Shenzhen and Shanghai from 2011 to 2016 as the research object and constructs the intersection of corporate governance comprehensive index and accounting conservatism and corporate governance index. Based on this, the mixed utility model is used to empirically test the relationship between accounting conservatism, corporate governance, and stock price collapse risk. The results show that: accounting conservatism, corporate governance and stock price collapse risk are negatively correlated, and conservative accounting policies can effectively restrain the occurrence of the stock price collapse, and higher levels of corporate governance can effectively reduce the possibility of the future stock price crash, and accounting conservatism and corporate governance have substitution effects in preventing the risk of the stock price collapse, and higher corporate governance levels can significantly reduce the negative impact of accounting conservatism on future stock price collapse risks. This paper also puts forward some suggestions to reduce the risk of stock price collapse from two aspects of improving accounting conservatism and strengthening corporate governance, which can be used as the reference for the sustainable and stable development of the capital market.																	0954-0091	1360-0494															10.1080/09540091.2020.1806204		AUG 2020											
J								Error prediction and structure determination forCMACneural network based on the uniform design method	EXPERT SYSTEMS										CMAC; location number analysis; structure determination; uniform design method	CMAC; CONTROLLER; INJECTION; SYSTEMS	Insufficient study on error bound of cerebellar model articulation controller (CMAC) severely limits its application. To investigate the error prediction and structure determination of CMAC for multi-dimensional and data-generation objects, this paper builds a 10-input 2-output model for a desulfurization system to test 44,640 sets of operation data. Four test groups and one prediction group are designed and performed based on uniform design method. Regression analysis and curve fitting methods are applied for error analyses. The focus of regression analysis method is the influence of uniform table's level on its prediction formulas' accuracies, whereas curve fitting's is the impact of theoretical memory space (location number) and actual storage space (address number) of CMAC on output error. Based on the results, the following conclusions are obtained. (a) The prediction accuracy of the linear regression equation is not monotonous with the level of the uniform design table, but there is a distinct region with local high precision. (b) Compared with regression analysis and address number fitting methods, location number analysis method has distinct advantages of prediction range, accuracy and flexibility. (c) In terms of location number analysis, different intervals may correspond to different optimal fitting functions, but only power function maintains high prediction accuracy in the whole range where the method works. Besides, the scope of location number analysis is also studied, of which the lower borders are 10(9)and 10(10)approximately for model's two outputs, respectively.																	0266-4720	1468-0394														e12614	10.1111/exsy.12614		AUG 2020											
J								Vehicle routing for the urgent delivery of face shields during the COVID-19 pandemic	JOURNAL OF HEURISTICS										Coronavirus; Humanitarian logistics; Heuristic optimization; Tabu search	SEARCH; RELIEF	The speed by which the COVID-19 pandemic spread throughout the world caught some national and local governments unprepared. Healthcare systems found themselves struggling to increase capacity and procure key supplies, such as personal protective equipment. Protective face shields became essential for healthcare professionals. However, most hospitals and healthcare facilities did not have them in adequate quantities. The urgency of producing and delivering face shields increased as the number of COVID-19 cases rapidly multiplied. This was the situation that we encountered in the city and province of Burgos (Spain). Since there was no time to wait for a large manufacturer to produce face shields, private citizens and small companies volunteered to make them using technologies such as 3D printers. Nonprofits, citizens, and governments agencies volunteered to deliver materials to the face shield makers and to pick up and deliver the face shields to health centers and other locations where they were needed. This resulted in a vehicle routing problem with some special characteristics that made it different from models used for commercial purposes. We describe the development of a heuristic to find feasible and efficient routes for this problem. We highlight the advantages of using heuristics in an emergency context like the one triggered by the COVID-19 pandemic. In particular, the heuristic approach allowed us to design, implement, test, and delivery a routing system in less than 1 week from the time that the local government contacted us with what they described as a logistics nightmare.																	1381-1231	1572-9397				OCT	2020	26	5					619	635		10.1007/s10732-020-09456-8		AUG 2020											
J								A solution to statistical and multidisciplinary design optimization problems using hGWO-SA algorithm	NEURAL COMPUTING & APPLICATIONS										Benchmark test functions; Engineering optimization; Grey wolf optimizer; Meta-heuristic search	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; NANOFLUID FLOW; MAGNETIC-FIELD; SWARM	Recently developed grey wolf optimizer (GWO) algorithm has evident behaviour for verdict of global optima, without getting ensnared in premature convergence. However, the exploitation phase of the existing grey wolf optimizer is underprivileged. In the proposed research, a hybrid version of grey wolf optimizer algorithm combined with simulated annealing (named as hGWO-SA) algorithm has been developed for the solution of various nonlinear, highly constrained, non-convex engineering design and optimization problems. In the proposed research, the exploitation phase of the existing grey wolf optimizer has been further enhanced using simulated annealing algorithm, which improves the local search capability of the existing grey wolf optimizer. In order to indorse the results of the proposed algorithm, 65 benchmark problems including CEC2017, CEC2018 and five multidisciplinary design optimization problems are taken into consideration. Experimentally, it has been found that the results of the proposed hybrid GWO-SA algorithm are better than standard grey wolf optimizer algorithm, ant lion optimizer algorithm, moth-flame optimization algorithm, sine-cosine optimization algorithm and other recently reported heuristics, meta-heuristic and hybrid search algorithm and the proposed algorithm indorses its effectiveness in the field of nature-inspired meta-heuristic algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-05229-3		AUG 2020											
J								A novel two-model local search algorithm with a self-adaptive parameter for clique partitioning problem	NEURAL COMPUTING & APPLICATIONS										Heuristic; Local search; Clique partitioning problem; Self-adaptive parameter	FACETS	Given a complete edge-weighted undirected graphG(V, E, W), clique partitioning problem (CPP) aims to cluster all vertices into an unknown number of disjoint groups and the objective is to maximize the sum of the edge weights of the induced subgraphs. CPP is an NP-hard combinatorial optimization problem with many real-world applications. In this paper, we propose a novel two-model local search algorithm with a self-adaptive parameter (TMLS_SA) to solve CPP. First, a simple solution is presented, that is, one vertex per group is used. Then, we present a two-model local search that is used to improve the solution which comprises a move operator model and an exchange operator model. In the local search phase, a gain function is used to guide the search toward a possible best solution, and a lock mechanism is also applied to prevent the local search from immediately returning to visited solutions. Finally, we execute a perturbation procedure to increase the diversity. The perturbation strength is updated self-adaptively according to the solutions obtained. Our algorithm TMLS_SA is compared with several representative algorithms, and the experimental results show that TMLS_SA is superior to competitors on almost all test instances with respect to the solution quality.																	0941-0643	1433-3058															10.1007/s00521-020-05289-5		AUG 2020											
J								Eccentricity based kinship verification from facial images in the wild	PATTERN ANALYSIS AND APPLICATIONS										Kinship verification; Facial section; Ellipse; Eccentricity	FACE; FEATURES; PCA	Kinship verification from facial images in the wild is a promising research aiming to identify whether a facial image pair shares kinship relation by analyzing face structures. This paper proposes a novel eccentricity-based kinship verification (EKV) method to demonstrate efficacy of dominant facial sections for kinship verification. The proposed EKV method uses eccentricity of ellipse-approximated dominant facial sections as discriminative parameter to perform kinship verification. It presents two different schemes, namely single eccentricity (SE) and fused eccentricity (FE). SE scheme for EKV method employs single formulation by considering single facial section. Each selected facial section is approximated as an ellipse to compute eccentricity parameter and perform verification. Next, FE scheme for EKV method employs multiview formulation by analyzing two or more facial sections. Eccentricity of different ellipse-approximated facial sections is computed and fused to form a transformed parameter and perform verification. The proposed EKV method is demonstrated on different available kinship databases. Experimental results showcase effectiveness of EKV method with the best and competitive accuracy obtained for FE scheme on different databases.																	1433-7541	1433-755X															10.1007/s10044-020-00906-4		AUG 2020											
J								Feature optimization by discrete weights for heart disease prediction using supervised learning	SOFT COMPUTING										Coronary Heart Disease (CHD); Decision Trees (DT); Nearest Neighbor Algorithm (K-NN); Support vector machines (SVM); Decision support system (DSS)	DECISION-SUPPORT-SYSTEM	The topic predictive analytics is the ray that lightning the way to patch the gap between accuracy in decision-making by the expertise and the inexperience. In particular, the health domain is more crucial about disease prediction accuracy. The disease diagnosis by clinical practitioner correlates to his exposer toward the clinical observations of the disease. However, the perceptions of an experienced clinical practitioner on a medical record often fail to identify the premature states of the disease, which costs patient life in the sector of critical diseases such as heart diseases. Hence, contemporary computer science engineering research has more attention to define substantial predictive analytics built by machine learning toward heart disease prediction. The critical objective to define predictive analytics with minimal false alarming is centric to potential training data corpus, and the optimal feature selection. In order to these arguments, the contribution of this manuscript aimed to portray the feature selection approach to perform supervised learning and label the given patient record is prone to heart disease or not with minimal false alarming. The contribution is a dynamicn-gram Features Optimization by Discrete Weights of the feature correlation. The experimental study signified the performance of the proposed model compared to the contemporary methods of feature selection for heart disease prediction.																	1432-7643	1433-7479															10.1007/s00500-020-05253-4		AUG 2020											
J								Ritz-like values in steplength selections for stochastic gradient methods	SOFT COMPUTING										Stochastic gradient methods; Learning rate selection rule; Ritz-like values; Adaptive subsampling strategies; Reduction variance techniques	OPTIMIZATION METHODS; ALGORITHMS	The steplength selection is a crucial issue for the effectiveness of the stochastic gradient methods for large-scale optimization problems arising in machine learning. In a recent paper, Bollapragada et al. (SIAM J Optim 28(4):3312-3343, 2018) propose to include an adaptive subsampling strategy into a stochastic gradient scheme, with the aim to assure the descent feature in expectation of the stochastic gradient directions. In this approach, theoretical convergence properties are preserved under the assumption that the positive steplength satisfies at any iteration a suitable bound depending on the inverse of the Lipschitz constant of the objective function gradient. In this paper, we propose to tailor for the stochastic gradient scheme the steplength selection adopted in the full-gradient method knows as limited memory steepest descent method. This strategy, based on the Ritz-like values of a suitable matrix, enables to give a local estimate of the inverse of the local Lipschitz parameter, without introducing line search techniques, while the possible increase in the size of the subsample used to compute the stochastic gradient enables to control the variance of this direction. An extensive numerical experimentation highlights that the new rule makes the tuning of the parameters less expensive than the trial procedure for the efficient selection of a constant step in standard and mini-batch stochastic gradient methods.																	1432-7643	1433-7479															10.1007/s00500-020-05219-6		AUG 2020											
J								Detection and evaluation of Keratoconus (corneal topography) by using the image classifier techniques	SOFT COMPUTING										Corneal topography; Keratoconus; Laser refractive surgery; Naive Bayes classifier		In recent years, more patients were detected with corneal abnormalities. It is critical to perform diagnosis of clinical or subclinical Keratoconus accurately. The main diagnostic tool for Keratoconus is corneal topography; leading to more Laser refractive surgeries. In this paper, we recommend a novel approach which can detect Keratoconus more efficiently. The parameters derived for feature extraction are corneal volume, corneal thickness, thinnest corneal thickness, corneal area, corneal perimeter. Hybrid feed forward network along with Naive Bayes classifier is used for classification. This proposed method detects and classifies Keratoconus and the performance metrics are also evaluated using MATLAB-based simulations and finally the performance of proposed method is compared with some existing techniques.																	1432-7643	1433-7479															10.1007/s00500-020-05255-2		AUG 2020											
J								An improved evolution algorithm using population competition genetic algorithm and self-correction BP neural network based on fitness landscape	SOFT COMPUTING										Genetic algorithm; Population competition mechanism; Fitness landscape; BP neural network; Learning rate	MECHANISM	The genetic algorithm-backpropagation neural network algorithm (GA-BP) makes full use of the advantages of genetic algorithm (GA) and BP neural network (BPNN). It has been widely used in practical problems, but it also has shortcomings such as slow convergence. Inspired by the generative adversarial network, a population competition mechanism (PCM) is proposed to improve the search ability of the GA, two populations are generated to compete, and the winning population can obtain the optimal individual from the failed population to ensure that the winning population has a sustainable advantage. The failed population will randomly generate new individuals and add new genes so as to get better individuals, through such a mechanism to ensure the rapid optimization of the entire population, avoid the risk of premature convergence, speed up the iterative speed and improve the stability of the GA. According to the characteristics of the fitness landscape, the learning rate of the BPNN is optimized, and it can change adaptively, which can effectively improve the network convergence speed and greatly reduce the time cost. We define the GA-BP that improves the learning rate based on fitness landscape as FL-GA-BP algorithm. On the basis of the FL-GA-BP algorithm, adding GA improved with PCM, we define this new algorithm as I-GA-BP algorithm, namely the I-GA-BP algorithm that combines PCM-improved GA and fitness landscape-improved BPNN. In this paper, we use two types of test functions with different characteristics and complexity to conduct experiments to verify the effectiveness of the I-GA-BP algorithm. By comparing the experimental data of the three algorithms GA-BP, FL-GA-BP and I-GA-BP, it is obtained that the I-GA-BP algorithm can better escape from the local optimal solution, which is more conducive to finding the global optimal solution. It also greatly improves the convergence speed of the neural network. Finally, we briefly discussed the effect of adjusting the number of neurons on the stability of the I-GA-BP algorithm.																	1432-7643	1433-7479															10.1007/s00500-020-05250-7		AUG 2020											
J								Temporally Coherent General Dynamic Scene Reconstruction	INTERNATIONAL JOURNAL OF COMPUTER VISION										Dynamic 4D reconstruction; Segmentation; Reconstruction; 3D; Temporal coherence; Dynamic scenes	ENERGY MINIMIZATION; STEREO	Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: an automatic method for initial coarse reconstruction to initialize joint estimation; sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to various applications such as free-view rendering and virtual reality.																	0920-5691	1573-1405															10.1007/s11263-020-01367-2		AUG 2020											
J								Emergency decision support modeling for COVID-19 based on spherical fuzzy information	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										AHP technique; COVID-19; emergency decision making algorithms; spherical fuzzy Einstein aggregation operators; spherical fuzzy entropy measure; spherical fuzzy sets; TOPSIS and grey techniques	AGGREGATION OPERATORS; SETS	Significant emergency measures should be taken until an emergency event occurs. It is understood that the emergency is characterized by limited time and information, harmfulness and uncertainty, and decision-makers are always critically bound by uncertainty and risk. This paper introduces many novel approaches to addressing the emergency situation of COVID-19 under spherical fuzzy environment. Fundamentally, the paper includes six main sections to achieve appropriate and accurate measures to address the situation of emergency decision-making. As the spherical fuzzy set (FS) is a generalized framework of fuzzy structure to handle more uncertainty and ambiguity in decision-making problems (DMPs). First, we discuss basic algebraic operational laws (AOLs) under spherical FS. In addition, elaborate on the deficiency of existing AOLs and present three cases to address the validity of the proposed novel AOLs under spherical fuzzy settings. Second, we present a list of Einstein aggregation operators (AgOp) based on the Einstein norm to aggregate uncertain information in DMPs. Thirdly, we are introducing two techniques to demonstrate the unknown weight of the criteria. Fourthly, we develop extended TOPSIS and Gray relational analysis approaches based on AgOp with unknown weight information of the criteria. In fifth, we design three algorithms to address the uncertainty and ambiguity information in emergency DMPs. Finally, the numerical case study of the novel carnivorous (COVID-19) situation is provided as an application for emergency decision-making based on the proposed three algorithms. Results explore the effectiveness of our proposed methodologies and provide accurate emergency measures to address the global uncertainty of COVID-19.																	0884-8173	1098-111X				NOV	2020	35	11					1601	1645		10.1002/int.22262		AUG 2020											
J								A method for combining conflicting evidences with improved distance function and Tsallis entropy	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										conflicting evidences; Dempster-Shafer evidence theory; information fusion; improved distance function; Tsallis entropy	BELIEF FUNCTIONS; DECISION-MAKING; PREFERENCE RELATIONS; UNCERTAINTY; FRAMEWORK; SELECTION	For the sake of great ability of handling uncertain information, Dempster-Shafer evidence theory is extensively used in information fusion. Nevertheless, when there exists highly inconsistent evidences, using classical Dempster's combination rule may lead to counter-intuitive results. To address this issue, a new conflicting evidences combination method based on distance function and Tsallis entropy is proposed. Numerical examples are used to illustrate the feasibility and efficiency of the proposed method. Further, an fault diagnosis problem is used as an example to show the effectiveness and superiority of the proposed method. The proposed method outperforms other methods that the proposed method recognize the target by the probability 99.49%, which is higher than other methods.																	0884-8173	1098-111X				NOV	2020	35	11					1814	1830		10.1002/int.22273		AUG 2020											
J								Improving answer selection with global features	EXPERT SYSTEMS										answer selection; deep global features; statistical global features	QUESTIONS	Given a question and its answer candidates (named QA corpus), answer selection is the task of identifying the most relevant answers to the question. Answer selection is widely used in question answering, web search, and so on. Current deep neural network models primarily utilize local features extracted from input question-answer pairs (QA pairs). However, the global features contained in QA corpora are under-utilized, and we argue that these global features substantially contribute to the answer selection task. To verify this point of view, we propose a novel model that combines local and global features for answer selection. In our model, two different global feature extractors are employed to extract statistical global features and deep global features from a QA corpus, respectively. Furthermore, we investigate the integration of these global features with local features in various experimental settings: statistical global features, deep global features, and a combination of statistical and deep global features. Our experimental results show that the global features are effective for answer selection. Our model obtains new state-of-the-art results on two public answer selection datasets and performs especially well on YahooCQA, where it achieves 9.2 and 6% higher precision@1 (P@1) and mean reciprocal rank (MRR) scores than previously published models.																	0266-4720	1468-0394														e12603	10.1111/exsy.12603		AUG 2020											
J								Fuzzy Non-singular Terminal Sliding Mode Controller Design for Nonlinear Systems with Input Saturation	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Finite-time convergence; Fuzzy logic controller; Terminal sliding mode; Input saturation	TRACKING CONTROL	The current work has proposed an approach to the controller design of a fuzzy non-singular terminal sliding mode (NTSM) for a type of planar systems with input saturation. On the basis of a modified version of NTSM, a category of saturated NTSM controller is first constructed to ensure that the states can reach the sliding surface and finite-time converge to the origin. On this basis, a fuzzy logic controller including two fuzzy input variables and a fuzzy output variable is developed to adaptively adjust the control gain such that the gain of the NTSM controller can be automatically minimized. This also implies that the chattering phenomenon encountered by most conventional sliding mode control (SMC) schemes can be significantly attenuated without sacrificing inherent properties. Finally, in comparison with a traditional SMC method, the superiority of the presented algorithm is confirmed by the comparative simulation results in terms of chattering alleviation and robustness.																	1562-2479	2199-3211				OCT	2020	22	7					2271	2283		10.1007/s40815-020-00915-9		AUG 2020											
J								Research on the Intelligent Fault Diagnosis of Medical Devices Based on a DEMATEL-Fuzzy Concept Lattice	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Medical devices; Intelligent fault diagnosis; Concept lattice; Fuzzy sets; DEMATEL	DECISION-MAKING; HEALTH-CARE; BIG DATA; SYSTEM; IMPLEMENTATION; PERFORMANCE; FRAMEWORK	Hospitals' daily operations have become increasingly dependent on medical devices. However, the occurrence of faults is inevitable. Therefore, it is crucial for hospitals to make timely fault diagnoses and enact the corresponding measures and improvements. This paper proposes a novel concept lattice method for the intelligent diagnosis of medical device faults. To minimize the influence of uncertain factors, fuzzy sets are used to accurately express relationships between concepts. First, the occurrence frequency and severity of each fault type are extracted based on the collected information. Then, the fuzzy formal context of occurrent faults and known faults can be constructed. Next, the corresponding fuzzy concept lattice is established and visualized using a Hasse diagram. Finally, the similarity between the concept lattices is calculated and used for fault diagnosis. Here, the weight factors are determined using the decision-making trial and evaluation laboratory (DEMATEL) method. A comparative analysis is performed to show that the proposed method uses simple calculations and is highly accurate.																	1562-2479	2199-3211				OCT	2020	22	7					2369	2384		10.1007/s40815-020-00859-0		AUG 2020											
J								Tracking objects with partial occlusion by background alignment	NEUROCOMPUTING										Object tracking; Subspace learning; Partial occlusion; Background alignment		Visual object tracking is a challenging and fundamental research topic in the field of computer vision. Recent years, many subspace-learning based methods have been proposed for visual object tracking with promising results. These methods reconstruct candidate states by a set of basis vectors and select the best state with minimum reconstruction error. It is well known that the accuracy of reconstructed results is seriously affected by partial occlusion. Besides, updating with occlusions is likely to mislead the tracker to drift away. Existing methods either do not consider these situations or find occlusion regions only by current observation and reconstruction. In fact, occlusion regions usually come from the background regions in previous frames, which are neglected in existing methods. Under this assumption, a novel object tracking algorithm called Partial Occlusion by Background Alignment (POBA) is proposed, which aims to find the best candidate state with an accurate occlusion mask. The POBA tracker treats current observation as a combination of object appearance and occlusion regions. The object appearance is modelled by basis vectors through incremental PCA over grayscale images. Then the occlusion region is reconstructed from last frame under the assumption that the backgrounds between two consecutive frames are almost identical. Besides, most candidate states are different from the object obviously, which can be filtered by some predefined occlusion masks so that computational complexity can be further reduced. Finally, the POBA tracker was analyzed on 8 challenge sequences and evaluated on two challenging datasets including OTB2015 and Temple Color. It gets an AUC of 0.456, a success rate score of 0.538 and a precision score of 0.626 in OPE on OTB2015 dataset. All indicators are increased by more than 23% compared with the 6 classical trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						1	13		10.1016/j.neucom.2020.03.026													
